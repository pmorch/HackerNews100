<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 13 Jul 2024 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["Firefox added [ad tracking] and has already turned it on without asking you" (162 pts)]]></title>
            <link>https://mastodon.social/@mcc/112775362045378963</link>
            <guid>40954535</guid>
            <pubDate>Sat, 13 Jul 2024 15:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@mcc/112775362045378963">https://mastodon.social/@mcc/112775362045378963</a>, See on <a href="https://news.ycombinator.com/item?id=40954535">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[For the Colonel, It Was Finger-Lickin‚Äô Bad (1976) (116 pts)]]></title>
            <link>https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad</link>
            <guid>40952650</guid>
            <pubDate>Sat, 13 Jul 2024 08:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad">https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad</a>, See on <a href="https://news.ycombinator.com/item?id=40952650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-container">
<p>

posted <time datetime="2016-08-26T17:26:06Z">Aug 26 @ 01:26 PM</time> by <a href="http://www.kottke.org/">Jason Kottke</a><span> ‚ÄÇ¬∑‚ÄÇ <span>gift link</span></span>



</p>




<p><img src="https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=1200,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1" srcset="https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=500,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1 500w, https://kottke.org/cdn-cgi/image/format=auto,fit=scale-down,width=1200,metadata=none/plus/misc/images/kfc-finger-lickin-bad.jpg?1 1200w" sizes="(max-width: 500px) 500px, 1200px" loading="lazy" width="1200" height="650" alt="KFC Finger Lickin Bad"></p>

<p>Here‚Äôs a gem from the archive of the NY Times. One day in September 1976, NY Times food critic Mimi Sheraton and Colonel Harland Sanders <a href="http://www.nytimes.com/1976/09/09/archives/for-the-colonel-it-was-fingerlickin-bad.html">stopped into a Manhattan Kentucky Fried Chicken</a>. The Colonel, then estranged from the company he founded, strolled into the kitchen after glad-handing some patrons and proceeded to tear into the quality of the food:</p>

<blockquote><p>Once in the kitchen, the colonel walked over to a vat full of frying chicken pieces and announced, ‚ÄòThat‚Äôs much too black. It should be golden brown. You‚Äôre frying for 12 minutes ‚Äî that‚Äôs six minutes too long. What‚Äôs more, your frying fat should have been changed a week ago. That‚Äôs the worst fried chicken I‚Äôve ever seen. Let me see your mashed potatoes with gravy, and how do you make them?‚Äù</p>
	
<p>When Mr. Singleton explained that he first mixed boiling water into the instant powdered potatoes, the colonel interrupted. ‚ÄúAnd then you have wallpaper paste,‚Äù he said. ‚ÄúNext suppose you add some of this brown gravy stuff and then you have sludge.‚Äù ‚ÄúThere‚Äôs no way anyone can get me to swallow those potatoes,‚Äù he said after tasting some. ‚ÄúAnd this cole slaw. This cole slaw! They just won‚Äôt listen to me. It should he chopped, not shredded, and it should be made with Miracle Whip. Anything else turns gray. And there should be nothing in it but cabbage. No carrots!‚Äù</p></blockquote>

<p>Sanders sold his company to an investment group in 1964, which took the company public two years later and eventually sold to a company called Heublein. After selling, Sanders officially still worked for the company as an advisor but grew more and more dissatisfied with it, as evidenced by the story above. When the company HQ moved to Tennessee, the Colonel was quoted as saying:</p>

<blockquote><p>This ain‚Äôt no goddam Tennessee Fried Chicken, no matter what some slick, silk-suited son-of-a-bitch says.</p></blockquote>

<p>And he got sued by a KFC franchisee after he <a href="https://books.google.com/books?id=ZskrsscU7HkC&amp;pg=SA4-PA17&amp;lpg=SA4-PA17&amp;dq=My+God,+that+gravy+is+horrible!+sanders&amp;source=bl&amp;ots=zfKAaLx1Rx&amp;sig=iZUPCcCs3LlSlsdnEGKkbQzEJx0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjTpZn_3t3OAhVMCsAKHaviCY8Q6AEILTAC#v=onepage&amp;q=My%20God%2C%20that%20gravy%20is%20horrible!%20sanders&amp;f=false">commented</a>:</p>

<blockquote><p>My God, that gravy is horrible. They buy tap water for 15 to 20 cents a thousand gallons and then mix it with flour and starch and end up with pure wallpaper paste. And I know wallpaper paste, by God, because I‚Äôve seen my mother make it.</p>

<p>To the ‚Äúwallpaper paste‚Äù they add some sludge and sell it for 65 or 75 cents a pint. There‚Äôs no nutrition in it and the ought not to be allowed to sell it.</p>

<p>And another thing. That new crispy chicken is nothing in the world but a damn fried doughball stuck on some chicken.</p></blockquote>

<p>Colonel Sanders: serving up chicken and sick burns with equal spiciness. (via <a href="https://twitter.com/mccanner">@mccanner</a>)</p>

<ul><li><a href="https://kottke.org/tag/food">food</a></li><li><a href="https://kottke.org/tag/Harland%20Sanders">Harland Sanders</a></li><li><a href="https://kottke.org/tag/KFC">KFC</a></li><li><a href="https://kottke.org/tag/Mimi%20Sheraton">Mimi Sheraton</a></li><li><a href="https://kottke.org/tag/restaurants">restaurants</a></li></ul>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a drag and drop CSS grid generator (178 pts)]]></title>
            <link>https://cssgridgenerator.io/</link>
            <guid>40952509</guid>
            <pubDate>Sat, 13 Jul 2024 07:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cssgridgenerator.io/">https://cssgridgenerator.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40952509">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>CSS grid generator is a tool that helps developers create custom CSS grid layouts more easily. The generator allows users to specify the number of columns, rows, the gutter size.</p><div><h2>How to use CSS grid generator?</h2><ul><li><span>1.</span> Customize the number of columns, rows, and gaps to fit your needs.</li><li><span>2.</span> Click the square with + sign to add a new element to the grid.</li><li><span>3.</span> Resize the DIV using the<!-- --> <b>handle in the bottom right corner.</b></li><li><span>4.</span> Drag and drop the DIV to reposition it as desired.</li><li><span>5.</span> Finally, copy the generated HTML and CSS code and paste it into your project.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ad-tech setting 'Privacy-Preserving Attribution' is opt-out in Firefox 128 (167 pts)]]></title>
            <link>https://gladtech.social/@cuchaz/112775302929069283</link>
            <guid>40952330</guid>
            <pubDate>Sat, 13 Jul 2024 06:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gladtech.social/@cuchaz/112775302929069283">https://gladtech.social/@cuchaz/112775302929069283</a>, See on <a href="https://news.ycombinator.com/item?id=40952330">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[WebContainers: Dev environments. In your web app (104 pts)]]></title>
            <link>https://webcontainers.io/</link>
            <guid>40952248</guid>
            <pubDate>Sat, 13 Jul 2024 06:31:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webcontainers.io/">https://webcontainers.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40952248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-669faec9="" id="VPContent" data-v-5a346dfe=""><!--[--><div data-v-6d7c391d=""><header data-v-6d7c391d="" data-v-0856bbef=""><div data-v-0856bbef=""><h2 data-v-0856bbef=""><strong data-v-0856bbef="">Dev environments.</strong><br data-v-0856bbef="">In your <strong data-v-0856bbef="">web app.</strong></h2><p data-v-0856bbef="">From interactive tutorials to full-blown IDEs, build instant, interactive coding experiences backed by WebContainers: the trusted, browser-based runtime from <a href="https://stackblitz.com/" target="_blank" data-v-0856bbef="">StackBlitz</a>.</p><p data-v-0856bbef=""><a href="https://webcontainers.io/guides/introduction" data-v-0856bbef="" data-v-11452584=""><!--[-->Get started<!--]--></a><a href="https://forms.default.com/360757" data-v-0856bbef="" data-v-11452584=""><!--[-->Book a demo<!--]--></a></p></div></header><h3 data-v-6d7c391d="">Battle-tested by cutting-edge teams</h3><!--[--><div data-v-be173c96="" data-v-60437d36=""><!--[--><!--[--><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/theme/sveltekit-light.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89="">On the SvelteKit team, we've fantasized for years about being able to build fully interactive learning material for full stack frameworks.<strong><br>With WebContainers it went from 'impossible' to 'easy' almost overnight.</strong></p></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/logos/egghead-light.svg" data-v-de3caf89=""><img src="https://webcontainers.io/img/logos/egghead-dark.svg" data-v-de3caf89=""></p><p data-v-de3caf89=""><strong>As a team working on educational products, StackBlitz WebContainers has been an invaluable tool for us.</strong> The ability to embed full-stack applications with customisable, interactive coding environment directly into our products has greatly enhanced the learning experience for our users.</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/vojta_holik.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Vojta Holik</p><p data-v-de3caf89="">Designer &amp; Developer, Egghead.io</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><!----><!----><p data-v-de3caf89="">WebContainers solve the final frontier in JavaScript developer experience - making full-stack Node.js projects run in the browser as lightweight and disposable and secure as frontend REPLs. <br><strong>Every PR, every npm library maintainer, every devtool company with a Node.js SDK, can benefit from this!</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/swyx_shawn_wang.jpg" data-v-de3caf89=""></p><p data-v-de3caf89="">swyx</p></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/testimonials/scrimba.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89="">I have worked with Web container API for a couple of weeks at Scrimba to make a pooc of backend support. And I can say it's a solid piece of technology. Things just work, and it's also quite fast. <strong>I'm super excited about the GA since it will unlock so much opportunities for OSS projects and the industry at large.</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/abdellah_alaoui.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Abdellah Alaoui</p><p data-v-de3caf89="">Fullstack hacker, Scrimba</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/xata.png" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/xata.png" data-v-de3caf89=""></p><p data-v-de3caf89=""><strong>The WebContainer API is a landmark on the way we think docs.</strong> Creating interactive docs and snippets just became so much more feasible! With Server-side code running on the browser, setting up a playground to securely learn Node.js SDKs and compilers  became feasible and even fun!</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/atila_fassina.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Atila Fassina</p><p data-v-de3caf89="">DX Engineer at Xata</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><p><img src="https://webcontainers.io/img/testimonials/astro-full-light.svg" data-v-de3caf89=""></p><!----><!----><p data-v-de3caf89=""><strong>WebContainers represent a fundamental shift in what is possible in the browser. I'm incredibly excited about the potential this tech unlocks,</strong> from secure, browser-based development environments to highly interactive educational content. </p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/nate_moore.jpg" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Nate Moore</p><p data-v-de3caf89="">Senior Software Engineer, The Astro Technology Company</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/suborbital.svg" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/suborbital.svg" data-v-de3caf89=""></p><p data-v-de3caf89="">For such a powerful piece of tech I was so impressed by how clear to use the API is. Also running WebContainers inside WebContainers had me ü§Ø</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/ramon_huidobro.png" data-v-de3caf89=""></p><div data-v-de3caf89=""><p data-v-de3caf89="">Ram√≥n Huidobro</p><p data-v-de3caf89="">Developer Advocate at Suborbital Software Systems</p></div></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><p><img src="https://webcontainers.io/img/testimonials/retune.png" data-v-de3caf89=""><img src="https://webcontainers.io/img/testimonials/retune.png" data-v-de3caf89=""></p><p data-v-de3caf89="">At re:tune, we have been building the missing frontend for GPT-3, on a mission to empower everyone to build AI-first software at the speed of thought. <strong>WebContainers set the stage for our AI-native IDE</strong> - with a copilot that can not only read and write code, but can also understand and operate in the full runtime context across server and client!</p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/retune-dj.jpg" data-v-de3caf89=""></p></div></div><div data-v-de3caf89="" data-v-be173c96="" data-v-681eb775=""><!----><!----><!----><p data-v-de3caf89="">Running chess in a terminal, running a terminal in the browser, check mate!<br><strong>The best position to be in is a creative one and the StackBlitz WebContainers allow that.</strong></p><div data-v-de3caf89=""><p><img src="https://webcontainers.io/img/testimonials/manus_nijhoff.png" data-v-de3caf89=""></p></div></div><!--]--><!--]--></div><!--]--></div><div data-v-6d7c391d=""><p><img src="https://webcontainers.io/img/features/elements-01-nodeLIGHT-02-DARK.png" data-v-6d7c391d=""></p><h2 data-v-6d7c391d="">Power your web app with<br data-v-6d7c391d="">the <strong data-v-6d7c391d="">WebContainer API</strong></h2><p data-v-6d7c391d="">Create unmatched user experiences by integrating Node.js directly into your web app.<br data-v-6d7c391d="">Build fully-branded products without connecting to external servers or directing users away to third-party apps.</p><div data-v-bd3c3473="" data-v-60437d36="" data-v-6d7c391d=""><!--[--><!--[--><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Run native package managers</h4><p data-v-e2782662="">Run the native versions of <code>npm</code>, <code>pnpm</code>, and <code>yarn</code>, all in the browser, all in your app, up to 10x faster than local.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Full browser support</h4><p data-v-e2782662="">Run WebContainer in all major browsers, from Chromium-based, to Firefox or Safari TP.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">All major frameworks</h4><p data-v-e2782662="">Instantly spin up disposable environments running any major modern framework.</p></div><div data-v-e2782662="" data-v-bd3c3473="" data-v-681eb775=""><h4 data-v-e2782662="">Run Wasm out of the box</h4><p data-v-e2782662="">Port your favorite language or framework to Wasm to run it in WebContainers. Yes, really.</p></div><!--]--><!--]--></div></div><div data-v-6d7c391d=""><h3 data-v-6d7c391d="">Set up in only a few steps</h3></div><div data-v-6d7c391d=""><h3 data-v-6d7c391d="">Build rich development experiences not possible before</h3><div data-v-09544a0d="" data-v-60437d36="" data-v-6d7c391d=""><!--[--><!--[--><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">Interactive tutorials</p><p data-v-cf776150="">Learn SvelteKit, a full stack framework, within their custom editor, running on WebContainers, all in the browser.</p><p><a href="https://learn.svelte.dev/tutorial/introducing-sveltekit" target="_blank" rel="nofollow" data-v-cf776150="">learn.svelte.dev</a></p></div><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">Low code / no code</p><p data-v-cf776150="">A stress-free editor enabling non-technical contributors to make their own PRs with a live, disposable preview to confirm an error-free build.</p><p><a href="https://developer.stackblitz.com/codeflow/integrating-web-publisher#what-is-web-publisher" target="_blank" rel="nofollow" data-v-cf776150="">Web Publisher by StackBlitz</a></p></div><div data-v-cf776150="" data-v-09544a0d="" data-v-681eb775=""><p data-v-cf776150="">AI applications</p><p data-v-cf776150="">re:tune is setting the stage for AI-native IDEs - with a copilot that can understand and operate in the full runtime context across server and client.</p><p><a href="https://retune.so/" target="_blank" rel="nofollow" data-v-cf776150="">retune.so</a></p></div><!--]--><!--]--></div></div><div data-v-6d7c391d=""><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Support for every team</h2><p data-v-6d7c391d="">Small startups, open source maintainers, and Fortune 500 enterprises all enjoy access to StackBlitz's committed product support, features and improvements.</p></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Slash server costs</h2><p data-v-6d7c391d="">WebContainer API only requires the compute power of your local CPU and browser, eliminating the cost and overhead of managing remote servers.</p></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Ship faster</h2><p data-v-6d7c391d="">No additional teams to build or maintain. Leave the technical support to us and focus on actually shipping your product.</p></div></div><div data-v-6d7c391d=""><h2 data-v-6d7c391d="">Leverage the tech <strong data-v-6d7c391d="">we use<br data-v-6d7c391d=""> in our own products.</strong></h2><p data-v-6d7c391d="">Years of development by our full-time engineering team, front-line feedback from leading community partners, and funded R&amp;D into future technological possibilities make WebContainer more robust by the day.</p><a href="https://webcontainers.io/guides/quickstart" data-v-6d7c391d="" data-v-11452584=""><!--[-->Get started!<!--]--></a></div><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[gpu.cpp: A lightweight library for portable low-level GPU computation (104 pts)]]></title>
            <link>https://github.com/AnswerDotAI/gpu.cpp</link>
            <guid>40952182</guid>
            <pubDate>Sat, 13 Jul 2024 06:12:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AnswerDotAI/gpu.cpp">https://github.com/AnswerDotAI/gpu.cpp</a>, See on <a href="https://news.ycombinator.com/item?id=40952182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">gpu.cpp</h2><a id="user-content-gpucpp" aria-label="Permalink: gpu.cpp" href="#gpucpp"></a></p>
<p dir="auto">gpu.cpp is a lightweight library that makes portable GPU compute with C++ simple.</p>
<p dir="auto">It focuses on general purpose native GPU computation, leveraging the WebGPU
specification as a portable low-level GPU interface. This means we can drop in
GPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs.
The same C++ code can work on a wide variety of laptops, workstations, mobile
devices or virtually any hardware with Vulkan, Metal, or DirectX support.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate</h2><a id="user-content-technical-objectives-lightweight-fast-iteration-and-low-boilerplate" aria-label="Permalink: Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate" href="#technical-objectives-lightweight-fast-iteration-and-low-boilerplate"></a></p>
<p dir="auto">With gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:</p>
<ul dir="auto">
<li>High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.</li>
<li>Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be &lt;5 seconds on a modern laptop.</li>
<li>Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.</li>
</ul>
<p dir="auto">The implementation aims for a small API surface area with minimum boilerplate. There are a small number of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it's needed.</p>
<p dir="auto">In this spirit of fast experimentation, we also want near- instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.</p>
<p dir="auto">The core library implementation in the header-only <code>gpu.h</code> source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high.
We also pre-build Google's Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google's powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles.</p>
<p dir="auto">For more advanced users and release deployments, we include <code>cmake</code> examples for building both Dawn with gpu.cpp end-to-end, but this is not required nor recommended for most users to get started.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start: Building and Running</h2><a id="user-content-quick-start-building-and-running" aria-label="Permalink: Quick Start: Building and Running" href="#quick-start-building-and-running"></a></p>
<p dir="auto">To build a gpu.cpp project, you will need to have installed on your system:</p>
<ul dir="auto">
<li><code>clang++</code> compiler installed with support for C++17.</li>
<li><code>python3</code> and above, to run the script which downloads the Dawn shared library.
make to build the project.</li>
<li><code>make</code> to build the project.</li>
<li>Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run <code>sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools</code> to install them.</li>
</ul>
<p dir="auto">The only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).</p>
<p dir="auto">Optionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.</p>
<p dir="auto">After cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:</p>

<p dir="auto">The first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see <code>libdawn.dylib</code> on MacOS or <code>libdawn.so</code> on Linux. This download only occurs once.</p>
<p dir="auto">The build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:</p>
<div data-snippet-clipboard-copy-content="Hello gpu.cpp!
--------------

  gelu(0.00) = 0.00
  gelu(0.10) = 0.05
  gelu(0.20) = 0.12
  gelu(0.30) = 0.19
  gelu(0.40) = 0.26
  gelu(0.50) = 0.35
  gelu(0.60) = 0.44
  gelu(0.70) = 0.53
  gelu(0.80) = 0.63
  gelu(0.90) = 0.73
  gelu(1.00) = 0.84
  gelu(1.10) = 0.95
  ...

Computed 10000 values of GELU(x)"><pre><code>Hello gpu.cpp!
--------------

  gelu(0.00) = 0.00
  gelu(0.10) = 0.05
  gelu(0.20) = 0.12
  gelu(0.30) = 0.19
  gelu(0.40) = 0.26
  gelu(0.50) = 0.35
  gelu(0.60) = 0.44
  gelu(0.70) = 0.53
  gelu(0.80) = 0.63
  gelu(0.90) = 0.73
  gelu(1.00) = 0.84
  gelu(1.10) = 0.95
  ...

Computed 10000 values of GELU(x)
</code></pre></div>
<p dir="auto">If you need to clean up the build artifacts, you can run:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Hello World Tutorial: A GELU Kernel</h2><a id="user-content-hello-world-tutorial-a-gelu-kernel" aria-label="Permalink: Hello World Tutorial: A GELU Kernel" href="#hello-world-tutorial-a-gelu-kernel"></a></p>
<p dir="auto">As a real-world example for how to use gpu.cpp, let's start with a practical-but-simple example of a GPU kernel from neural networks.</p>
<p dir="auto">GELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.</p>
<p dir="auto">It takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.</p>
<p dir="auto">The GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:</p>
<ol dir="auto">
<li>
<p dir="auto">The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute opporation.</p>
</li>
<li>
<p dir="auto">The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.</p>
</li>
<li>
<p dir="auto">The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.</p>
</li>
</ol>
<p dir="auto">Here's a GELU kernel implemented (based on the CUDA implementation in <a href="https://github.com/karpathy/llm.c">llm.c</a>) as an on-device WGSL shader and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):</p>
<div data-snippet-clipboard-copy-content="#include <array>
#include <cstdio>
#include <future>

#include &quot;gpu.h&quot;

using namespace gpu; // createContext, createTensor, createKernel,
                     // createShader, dispatchKernel, wait, toCPU
                     // Bindings, Tensor, Kernel, Context, Shape, kf32

static const char *kGelu = R&quot;(
const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)
@group(0) @binding(0) var<storage, read_write> inp: array<{{precision}}>;
@group(0) @binding(1) var<storage, read_write> out: array<{{precision}}>;
@compute @workgroup_size({{workgroupSize}})
fn main(
    @builtin(global_invocation_id) GlobalInvocationID: vec3<u32>) {
    let i: u32 = GlobalInvocationID.x;
    if (i < arrayLength(&amp;inp)) {
        let x: f32 = inp[i];
        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR 
                 * (x + .044715 * x * x * x))), x, x > 10.0);
    }
}
)&quot;;

int main(int argc, char **argv) {
  Context ctx = createContext();
  static constexpr size_t N = 10000;
  std::array<float, N> inputArr, outputArr;
  for (int i = 0; i < N; ++i) {
    inputArr[i] = static_cast<float>(i) / 10.0; // dummy input data
  }
  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());
  Tensor output = createTensor(ctx, Shape{N}, kf32);
  std::promise<void> promise;
  std::future<void> future = promise.get_future();
  Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32),
                           Bindings{input, output},
                           /* number of workgroups */ {cdiv(N, 256), 1, 1});
  dispatchKernel(ctx, op, promise);
  wait(ctx, future);
  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));
  for (int i = 0; i < 16; ++i) {
    printf(&quot;  gelu(%.2f) = %.2f\n&quot;, inputArr[i], outputArr[i]);
  }
  return 0;
}"><pre><code>#include &lt;array&gt;
#include &lt;cstdio&gt;
#include &lt;future&gt;

#include "gpu.h"

using namespace gpu; // createContext, createTensor, createKernel,
                     // createShader, dispatchKernel, wait, toCPU
                     // Bindings, Tensor, Kernel, Context, Shape, kf32

static const char *kGelu = R"(
const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)
@group(0) @binding(0) var&lt;storage, read_write&gt; inp: array&lt;{{precision}}&gt;;
@group(0) @binding(1) var&lt;storage, read_write&gt; out: array&lt;{{precision}}&gt;;
@compute @workgroup_size({{workgroupSize}})
fn main(
    @builtin(global_invocation_id) GlobalInvocationID: vec3&lt;u32&gt;) {
    let i: u32 = GlobalInvocationID.x;
    if (i &lt; arrayLength(&amp;inp)) {
        let x: f32 = inp[i];
        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR 
                 * (x + .044715 * x * x * x))), x, x &gt; 10.0);
    }
}
)";

int main(int argc, char **argv) {
  Context ctx = createContext();
  static constexpr size_t N = 10000;
  std::array&lt;float, N&gt; inputArr, outputArr;
  for (int i = 0; i &lt; N; ++i) {
    inputArr[i] = static_cast&lt;float&gt;(i) / 10.0; // dummy input data
  }
  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());
  Tensor output = createTensor(ctx, Shape{N}, kf32);
  std::promise&lt;void&gt; promise;
  std::future&lt;void&gt; future = promise.get_future();
  Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32),
                           Bindings{input, output},
                           /* number of workgroups */ {cdiv(N, 256), 1, 1});
  dispatchKernel(ctx, op, promise);
  wait(ctx, future);
  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));
  for (int i = 0; i &lt; 16; ++i) {
    printf("  gelu(%.2f) = %.2f\n", inputArr[i], outputArr[i]);
  }
  return 0;
}
</code></pre></div>
<p dir="auto">Here we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a> for a demonstration of live WGSL code re-loading).</p>
<p dir="auto">The CPU code in main() sets up the host coordination for the GPU computation.
We can think of the use of gpu.cpp library as a collection of GPU nouns and
verbs.</p>
<p dir="auto">The "nouns" are GPU resources modeled by the type definitions of the library
and the "verbs" actions on GPU resources, modeled by the functions of the
library. The ahead-of-time resource acquisition functions are prefaced with
<code>create*</code>, such as:</p>
<ul dir="auto">
<li><code>createContext()</code> - constructs a reference to the GPU device context (<code>Context</code>).</li>
<li><code>createTensor()</code> - acquires a contiguous buffer on the GPU (<code>Tensor</code>).</li>
<li><code>createShader()</code> - constructs WGSL code string to run on the GPU) (<code>ShaderCode</code>)</li>
<li><code>createKernel()</code> - constructs a handle to resources for the GPU computation (<code>Kernel</code>), which combines bindings to GPU buffers from <code>createTensor()</code> with the computation definition from <code>createShader()</code>.</li>
</ul>
<p dir="auto">These resource acquisition functions are tied to resource types for interacting with the GPU:</p>
<ul dir="auto">
<li><code>Context</code> - a handle to the state of resources for interacting with the GPU device.</li>
<li><code>Tensor</code> - a buffer of data on the GPU.</li>
<li><code>ShaderCode</code> - the code for a shader program that can be dispatched to the
GPU. This is a thin wrapper around a WGSL string but also includes the
workgroup size the code is designed to run with.</li>
<li><code>Kernel</code> - a GPU program that can be dispatched to the GPU. This accepts a
<code>ShaderCode</code> and a list of <code>Tensor</code> resources to bind for the dispatch
computation. This takes an argument <code>Bindings</code> that is a list of <code>Tensor</code> instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the <code>input</code> buffer on the GPU and the <code>ouptut</code> buffer on the GPU.</li>
</ul>
<p dir="auto">In this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.</p>
<p dir="auto">Besides the <code>create*</code> resource acquisition functions, there are a few more "verbs" in the gpu.cpp library for handling dispatching execution to the GPU and data movement:</p>
<ul dir="auto">
<li><code>dispatchKernel()</code> - dispatches a <code>Kernel</code> to the GPU for computation. This is an asynchronous operation that returns immediately.</li>
<li><code>wait()</code> - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.</li>
<li><code>toCPU()</code> - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.</li>
<li><code>toGPU()</code> - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, <code>toGPU()</code> is not used because there's only one data movement from CPU to GPU in the program and that happens when the <code>createTensor()</code> function is called.</li>
</ul>
<p dir="auto">This example is available in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp">examples/hello_world/run.cpp</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering</h2><a id="user-content-other-examples-matrix-multiplication-physics-sim-and-sdf-rendering" aria-label="Permalink: Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering" href="#other-examples-matrix-multiplication-physics-sim-and-sdf-rendering"></a></p>
<p dir="auto">You can explore the example projects in
<a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> which
illustrate how to use gpu.cpp as a library.</p>
<p dir="auto">After you have run <code>make</code> in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running <code>make</code> from the example's directory.</p>
<p dir="auto">An example of tiled matrix multiplication is in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/">examples/matmul</a>. This implements a WebGPU version of the first few kernels of Simon Boehm's <a href="https://siboehm.com/articles/22/CUDA-MMM" rel="nofollow">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a> post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point.</p>
<p dir="auto">A parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics">examples/physics</a>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/matmul.png"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/matmul.png" alt="matmul example output" width="40%"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/pendulum.gif"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/pendulum.gif" alt="physics example animated gif" width="42%" data-animated-image=""></a>
</p>
<p dir="auto">We also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render%5D">examples/render</a> and a shadertoy-like live-reloading example is in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a>.</p>
<p dir="auto">Interestingly, with a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/docs/images/shadertui.gif"><img src="https://github.com/AnswerDotAI/gpu.cpp/raw/main/docs/images/shadertui.gif" alt="shadertui example animated gif" width="88%" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Who is gpu.cpp for?</h2><a id="user-content-who-is-gpucpp-for" aria-label="Permalink: Who is gpu.cpp for?" href="#who-is-gpucpp-for"></a></p>
<p dir="auto">gpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:</p>
<ul dir="auto">
<li>Development of GPU algorithms to be run on personal computing devices</li>
<li>Direct standalone implementations of neural network models</li>
<li>Physics simulations and simulation environments</li>
<li>Multimodal applications - audio and video processing</li>
<li>Offline graphics rendering</li>
<li>ML inference engines and runtimes</li>
<li>Parallel compute intensive data processing applications</li>
</ul>
<p dir="auto">Although gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.</p>
<p dir="auto">To date, AI research has primarily been built with CUDA as the priveledged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.</p>
<p dir="auto">GPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastrcture scale efforts - game engines, production ML inference engines, large software packages.</p>
<p dir="auto">We want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.</p>
<p dir="auto">gpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What gpu.cpp is not</h2><a id="user-content-what-gpucpp-is-not" aria-label="Permalink: What gpu.cpp is not" href="#what-gpucpp-is-not"></a></p>
<p dir="auto">gpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.</p>
<p dir="auto">Second, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable <em>native</em> GPU API first and foremost, with the possibility of running in the browser being being a convenient additional benefit in the future.</p>
<p dir="auto">If you find it counerintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk <a href="https://www.youtube.com/watch?v=qHrx41aOTUQ" rel="nofollow">"WebGPU is Not Just About the Web"</a>.</p>
<p dir="auto">Finally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations and Upcoming Features</h2><a id="user-content-limitations-and-upcoming-features" aria-label="Permalink: Limitations and Upcoming Features" href="#limitations-and-upcoming-features"></a></p>
<p dir="auto"><em>API Improvements</em> - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.</p>
<p dir="auto"><em>Browser Targets</em> - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority.</p>
<p dir="auto"><em>Reusable Kernels and Shader Library</em> - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in <code>examples/</code>. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.</p>
<p dir="auto"><em>More Use Case Examples and Tests</em> - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from <a href="https://github.com/karpathy/llm.c">llm.c</a> in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you run into issues building the project, please open an issue.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">gpu.cpp makes use of:</p>
<ul dir="auto">
<li><a href="https://dawn.googlesource.com/dawn" rel="nofollow">Dawn</a> as the WebGPU implementation</li>
<li><a href="https://github.com/jspanchu/webgpu-dawn-binaries">webgpu-dawn-binaries</a> by
@jspanchu to build a binary artifact of Dawn.</li>
<li><a href="https://github.com/eliemichel/WebGPU-distribution">webgpu-distribution</a> by
@eliemichel for cmake builds.</li>
</ul>
<p dir="auto">Thanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discord Community and Contributing</h2><a id="user-content-discord-community-and-contributing" aria-label="Permalink: Discord Community and Contributing" href="#discord-community-and-contributing"></a></p>
<p dir="auto">Join our community in the <code>#gpu-cpp</code> channel on the <a href="https://discord.gg/zmJVhXsC7f" rel="nofollow">AnswerDotAI Discord with this invite link</a>. Feel free to get in touch via X <a href="https://twitter.com/austinvhuang" rel="nofollow">@austinvhuang</a> as well.</p>
<p dir="auto">Feedback, issues and pull requests are welcome.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Calvin and Hobbes: Sunday Pages 1985-1995 (2001) (207 pts)]]></title>
            <link>http://timhulsizer.com/cwords/cintro.html</link>
            <guid>40951800</guid>
            <pubDate>Sat, 13 Jul 2024 04:37:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://timhulsizer.com/cwords/cintro.html">http://timhulsizer.com/cwords/cintro.html</a>, See on <a href="https://news.ycombinator.com/item?id=40951800">Hacker News</a></p>
<div id="readability-page-1" class="page">

<b>
<center>
Back To the <a href="http://timhulsizer.com/cwords/calvin.html">Calvin &amp; Hobbes Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/cbooks.html">C&amp;H Books Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/index.html">Main Page</a><br>

<hr>

<h3>Introduction</h3>
<b>By Bill Watterson (C)2001
</b><p>From the book, "Calvin and Hobbes - Sunday Pages 1985 - 1995"</p></center>

</b>

<p>It's been five years since the end of <i>Calvin and Hobbes</i>, 
 the longest time I can remember in which I haven't drawn cartoons. 
 <i>Calvin and Hobbes</i> was a wonderful experience, but it was an 
 all-consuming career. When I quit the strip, I put my cartoons in 
 boxes, and jumped into other interests. I haven't really considered 
 the strip since, so at the invitation to do this show, I thought 
 it might be time to look back at some of my work. 

</p><p>My first reaction in going through my old cartoons was some 
 amazement at the size and weight of the pile. For most successful 
 comic strips, ten years is just a drop in the bucket, but even that 
 amount of time yields a huge amount of material. It's no wonder 
 that decade seems like a blur. 

</p><p>Going through my old strips is sort of like looking at old 
 photographs of myself: they're personal and familiar, yet somewhat 
 bizarre at the same time. There are cartoons I've drawn that are 
 the equivalent of pictures of my younger self wearing yellow 
 pants: I know I'm responsible for that, but what on earth was I 
 thinking? As my tastes have changed, and as I've learned more, I 
 imagine that I would do many strips quite differently today. Not 
 better necessarily, but certainly differently. I was twenty-eight 
 when <i>Calvin and Hobbes</i> was first published, and, of course, I would 
 make other choices now at age forty-three. 

</p><p>It's also sort of strange to see a record of my own learning 
 curve. Pick up a given strip, and I see how I struggled with 
 various writing and drawing problems, or how I finally surmounted 
 one. I remember sometimes feeling that the strip was better written 
 than I could actually write, and better drawn than I could actually 
 draw. I learned a great deal over the years by trying to push the 
 strip beyond my own abilities, and I'm very proud that <i>Calvin and 
 Hobbes</i> explored and developed all the way to the end. By the final 
 years, I see naturalness or a sense of inevitability to the drawing 
 and writing that is very satisfying. 


</p><p>I'm more appreciative of this kind of grace since returning to 
 the awkward stages of new learning curves. Of course, I'd also say 
 the times have caught up with some of my strips. It's frankly a 
 little discouraging to see how ordinary some of them look now. When 
 <i>Calvin and Hobbes</i> first appeared, it was somewhat surprising to 
 treat reality as subjective, and to draw a strip with multiple 
 viewpoints, juxtaposing Calvin's vision with what others saw. I did  
 this simply as a way to put the reader in Calvin's head and to 
 reveal his imaginative personality. Now these juxtapositions are a 
 visual game for many comic strips, and after all these years, I 
 suspect readers know where this sort of joke is headed as soon as 
 they see it. The novelty cannot be recaptured. 

</p><p>Novelty, however, is probably overrated anyway. The <i>Calvin and 
 Hobbes</i> strips that hold up best, to my eye anyway, are the ones 
 where the characters seem big, vivid, and full of life, and where 
 the strip's world seems genuine and inviting. Punchlines come and 
 go, but something in the friendship between Calvin and Hobbes seems  
 to hold a small piece of truth. Expressing something real and 
 honest is, for me, the joy and the importance of cartooning. 

</p><p>The Sunday strips were usually the cartoons I had the most fun 
 with, and for this show I've chosen a few Sunday strips from each 
 year that I think show off the strip's strengths. 

</p><p>I have fond memories of reading the Sunday comics when I was a 
 kid. As far as I was concerned, the Sunday comics were the whole 
 reason for newspapers to exist. On weekdays, I read only the strips 
 I liked; but on Sundays, I read them all, and often several times. 
 The Sunday comics were always the most fun to look at, so when I 
 finally got the chance to draw my own comic strip, I knew I wanted 
 to make the Sunday Calvin and Hobbes something special. It took me 
 a little while to learn to use the larger Sunday space effectively. 
 It requires a somewhat different pace for the humor, and, of 
 course, a big color panel is no place to find out that you don't 
 know how to draw the back of your character's head. The Sunday 
 strip shows off both strengths and weaknesses. 

</p><p>Occasionally I would see that an idea I'd written for a Sunday 
 strip was not as substantial as I'd hoped it would be, and I'd 
 realize that some of the panels and dialogue weren't adding 
 anything significant to the story. If that were the case, I'd 
 remove everything extraneous and use the trimmed idea for a daily 
 strip instead. I held the Sundays to a different standard: any idea 
 for the Sunday strip had to need the extra space. I felt a Sunday 
 strip should do something that was impossible the rest of the week. 

</p><p>Over the years, I learned that daily strips are better suited for 
 certain kinds of ideas, while Sunday strips are better for others. 
 The daily strip is quick and to the point, perfect for a simple 
 observation, or a short exchange between characters. Daily strips 
 are also better for long stories, where a certain suspense can be 
 fostered by continuing the story day after day, and the reader can 
 remember what happened previously. 

</p><p>Extended conversations with real back and forth dialogue, however, 
 don't work very well in four tiny panels - the dialogue balloons 
 crowd out the drawings and the strip loses its grace. In a Sunday 
 strip, you can spread out, and let the characters yap a bit. This 
 is often funny in itself, and it's a wonderful way to let the 
 characters' personalities emerge. It also lets you explore a topic 
 a bit more fully. 

</p><p>You can talk about things without reducing them to one-liners 
 right away. And, of course, in today's minuscule comics, if an idea 
 requires any real drawing, the Sunday strip is the only possible 
 place for it. Likewise, any complex storytelling problem-a strip 
 illustrating a long expanse of time, for example, or an event 
 depicted in a succession of very tiny moments-is futile in the 
 daily format. Calvin's fantasies generally migrated to the Sunday 
 page for this reason. 

</p><p>In short, the Sunday page offered unique opportunities, and I 
 deliberately tried to come up with ideas that could take advantage 
 of them. 

</p><p>I usually wrote the Sunday strips separately from the dailies. 
 For the daily strips, I tried to write an entire month's worth of 
 ideas before inking any of them. This allowed a long period for 
 editing and rewriting. I was less able to do this for the Sunday 
 strips because the Sundays need to be drawn weeks further in 
 advance and because the strips took so much longer to draw. If at 
 all possible, however, I would try to keep two or three Sunday 
 ideas ahead of the deadlines. I always wanted to reserve the option 
 of abandoning an idea that didn't stand up to a few weeks of 
 scrutiny. 

</p><p>For those who are interested in technical matters, the early 
 strips were drawn on any cheap pad of Bristol board the local art 
 supply store happened to stock. The paper was usually rather thin 
 and sometimes the sheet wouldn't accept the ink consistently (bad 
 sizing or something), which would make drawing aggravating and time 
 consuming. Eventually I switched to heavier Strathmore Bristol 
 board, which was much nicer. I used a 2H pencil to rough in the 
 drawing, and then inked with a small sable brush and India ink. I 
 did as little pencil work as possible in order to keep the inking 
 more spontaneous, although the more elaborate panels required more 
 preliminary drawing. For lettering, I used a Rapidograph cartridge 
 pen. I drew the dialogue balloons and a few odds and ends with a 
 crow quill pen. To cover up unwanted marks, I used various brands 
 of Wite-Out, and in the early days, typewriter correction fluid. 
 (Remember typewriters?) No doubt this stuff will eat through the 
 paper or turn green in a few years, but as the original cartoons 
 were intended for reproduction, not picture frames and gallery 
 walls, I did not overly concern myself with archival issues or, for 
 that matter, neatness. At some point along the way, however, I did 
 ask the syndicate to send the printers a quality reproduction of 
 the Sunday cartoon, rather than the original drawing, in order to 
 reduce the amount of tape, registration marks, and general 
 crunchings and manglings to which the drawings had previously been 
 subjected. 

</p><p>Coloring the strips was a slow and tedious process. My 
 syndicate gave me a printed sheet showing numbered squares of color, 
 each a mixture of various percentages of red, yellow, and blue. 
 Using this sheet as a guide, I taped some tracing paper over the 
 finished cartoon, and painted watercolor approximations of the 
 available colors in the areas I wanted. This would give me a very 
 rough idea of what the newspaper version might look like. Then I 
 numbered each little spot of color. As the Sunday strips became 
 more visually complex, and as I started to use color more 
 deliberately for effects, this process became a real chore. These 
 days, I believe much of it can be done with a few clicks of a mouse. 

</p><p>Colors take on different characteristics when placed next to 
 other colors (a neutral-seeming gray might look greenish and dark 
 next to one color, but brownish and pale in relation to another). 
 Because of this, I came up with one little trick for coloring the 
 strip. I cut out each of the color squares provided by the printer, 
 so I had a stack of colors (like paint chips), rather than a sheet. 
 By laying out the cut squares and physically placing one color next 
 to the others I expected to use, I could see exactly how each color 
 behaved in that particular context. As I got better at this, I was 
 able to choose approprial "palettes" for each strip, and create 
 moods with color. One strip might call for contrasting, bright 
 colors; another strip might be done with a limited group of soft, 
 warm colors; another idea might call for a close range of grays and 
 darks, and so on. If I made Calvin's skin a dull pink-gray to 
 suggest dim lighting at night, I would have to find a dull 
 yellow-gray that would suggest his hair in the same light. These 
 challenges took an inordinate amount of time for work on deadline, 
 but I was often quite proud of the results. A comic strip should 
 always be fun to look at, and good use of color can contribute to 
 that appeal More than that, color creates its own emotional impact, 
 which can make the drawing more expressive. 

</p><p>The half-page Sunday format required certain guaranteed panel 
 divisions. The strip had to be drawn in three rows of equal height, 
 and there was one unmovable panel division within each row. This 
 allowed editors to reduce and reconfigure the strip to suit their 
 particular space needs. The same strip could run in several shapes 
 by restacking the panels. 

</p><p>Editors commonly removed the entire top row altogether, so in 
 essence, a third of the strip had to be wasted on "throwaway 
 panels" that many readers would never see. The fixed panel divisions 
 were also annoying because they limited my ability to compose the 
 strip to best suit the idea. For example, they often forced a small 
 panel where I needed more space for words. 

</p><p>Of course, a big part of cartooning is learning to work 
 effectively within tight space constraints. Much of cartooning's 
 power comes from its ability to do more with less: when the 
 drawings and ideas are distilled to their essences, the result can 
 be more beautiful and powerful for having eliminated the clutter. 
 That said, there is a point at which simplification thwarts good 
 storytelling. You can't condense Moby Dick into a paragraph and get 
 the same effect. Over the years, my frustration increased and I 
 became convinced that I could draw a better comic strip than the 
 current newspaper format was permitting. Looking at examples of 
 comics from the 1930s, when a Sunday strip could fill an entire 
 page, I was amazed by the long-forgotten possibilities out there. 

</p><p>I took a sabbatical after resolving a long and emotionally 
 draining fight to prevent <i>Calvin and Hobbes</i> from being 
 merchandised. Looking for a way to rekindle my enthusiasm for the 
 duration of a new contract term, I proposed a redesigned Sunday 
 format that would permit more panel flexibility. To my surprise and 
 delight, Universal responded with an offer to market the strip as 
 an unbreakable half page (more space than I'd dared to ask for), 
 despite the expected resistance of editors. 

</p><p>To this day, my syndicate assures me that some editors liked the 
 new format, appreciated the difference, and were happy to run the 
 larger strip, but I think it's fair to say that this was not the 
 most common reaction. The syndicate had warned me to prepare for 
 numerous cancellations of the Sunday feature, but after a few weeks 
 of dealing with howling, purple-faced editors, the syndicate 
 suggested that papers could reduce the strip to the size tabloid 
 newspapers used for their smaller sheets of paper. Another strip 
 could then run vertically down the side. Consequently, while some 
 papers, primarily in larger markets, ran the strip as a half page, 
 other papers reduced it. In some of the latter papers (including 
 the one I read at the time), I actually lost ground: the new Sunday 
 strip was printed even smaller than before. I was in no mood to 
 take on new fights, so I focused on the bright side: I had complete 
 freedom of design and there were virtually no cancellations. 

</p><p>For all the yelling and screaming by outraged editors, I remain 
 convinced that the larger Sunday strip gave newspapers a better 
 product and made the comics section more fun for readers. Comics 
 are a visual medium. A strip with a lot of drawing can be exciting 
 and add some variety. Proud as I am that I was able to draw a 
 larger strip, I don't expect to see it happen again any time soon. 
 In the newspaper business, space is money, and I suspect most 
 editors would still say that the difference is not worth the cost. 
 Sadly, the situation is a vicious circle: because there's no room 
 for better artwork, the comics are simply drawn; because they're 
 simply drawn, why should they have more room? 

</p><p>Business controversies aside, the new format opened up new ways 
 to tell stories, and I drew different kinds of strips as a result. 
 I could write and draw the strip exactly as I imagined it, so it 
 truly challenged my abilities. Whereas Sunday strips had previously 
 taken me a full day to draw and color, a complex strip would now 
 take me well into a second day to finish. Deadlines discourage this 
 kind of indulgence, and I had to steal that extra time from what 
 would have been some semblance of an ordinary life, but I was 
 thrilled to expand the strip's world. 

</p><p>Laying out the panels became a job in itself, now that I was no 
 longer confined to horizontal rows. could place boxes anywhere and 
 any size, but the reader's eye needs to flow naturally to the 
 proper panels without confusion, and big panels need to be designed 
 in such a way that they don't divert attention and spoil surprises. 
 The graphic needs of each panel must be accommodated and the panels 
 themselves should form a pleasing arrangement so the entire page is 
 attractive, balanced, and unified as well. Here again I looked for 
 guidance in the gorgeous Sunday pages of George Herriman's <i>Krazy Kat.</i>
 
</p><p>The new Sunday format necessitated a change in the format of my 
 book collections as well. Having won a bigger strip in newspapers, 
 I wanted the book reproductions to reflect the strip's new impact 
 as much as possible by printing the Sunday strips large. This 
 resulted in the rather awkward horizontal format of my later books. 
 They stick out of bookshelves, but the strips look nice. From this 
 point on, the Sunday strips were reproduced in color with each 
 collection, not just in the "treasury" collections, as before. 
 (Here's a piece of trivia: because of the timing of the book format 
 change, the cartoons from the <i>Snow Goons</i> collection were 
 never put in a treasury book, so those Sunday strips have been 
 reprinted only in black-and-white.)
 
</p><p>Ten years after starting <i>Calvin and Hobbes</i>, I ended the 
 strip. As much as I knew I'd miss the characters, the decision was 
 long anticipated on my part. Professionally, I had accomplished far 
 more than I'd ever set out to do and there were no more mountains I 
 wanted to climb. Creatively, my interests were shifting away from 
 cartooning toward painting, where I could develop my drawing skills 
 further. And personally, I wanted to restore some balance to my 
 life. I had given the strip all my time and energy for a decade 
 (and was happy to do so), but now I was that much older and I 
 wanted to work at a more thoughtful pace, out of the limelight, and 
 without the pressures and restrictions of newspapers. 

</p><p>The final Calvin and Hobbes strip was a Sunday strip. The 
 deadline for Sunday strips being early, I drew it well before 
 writing the daily strips that would eventually precede it in the 
 newspaper. I very much wanted to hit the right note for this final 
 strip. I think it worked, but it was a bittersweet strip to draw. 

</p><p>Since <i>Calvin and Hobbes</i>, I've been teaching myself how to 
 paint, and trying to learn something about music. I have no 
 background in either subject, and there are certainly days when I 
 wonder what made me trade proficiency and understanding in one 
 field for clumsiness and ignorance in these others. On better 
 days, I enjoy having so many new challenges and surprises. Even so, 
 these new endeavors have only deepened my appreciation for comics. 
 I no longer take quite so much for granted the versatility of 
 comics and their ability to depict complex ideas in a beautiful, 
 accessible, and entertaining form. For all their seeming simplicity, 
 the expressive possibilities of comics rival those of any other art 
 form. Five years after <i>Calvin and Hobbes</i>, I love the comics 
 as much as ever. 

</p><p>Bill Watterson
<br>Summer 2001 

</p><center><hr><b>
Back To the <a href="http://timhulsizer.com/cwords/calvin.html">Calvin &amp; Hobbes Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/cbooks.html">C&amp;H Books Page</a><br>
Back To the <a href="http://timhulsizer.com/cwords/index.html">Main Page</a><br>

</b>
<hr>



</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Use a Work Journal to Recover Focus Faster and Clarify Your Thoughts (754 pts)]]></title>
            <link>https://fev.al/posts/work-journal/</link>
            <guid>40950584</guid>
            <pubDate>Sat, 13 Jul 2024 00:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fev.al/posts/work-journal/">https://fev.al/posts/work-journal/</a>, See on <a href="https://news.ycombinator.com/item?id=40950584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    
    <p>You‚Äôre working on the most complex problem in computer science: fixing permissions on a deployment pipeline. It‚Äôs been 4 days you started on that simple task already. Your manager explained to you in no uncertain terms that your performance on the subject is well below the expectations she has from a midterm intern. Your colleagues stay as far away as possible from you to avoid getting tainted by your shameful failure. 4 days of sleepless afternoons, seeing that freaking status turning to ‚Äúbuild failed‚Äù everytime, bringing you to tears. The weather is shit, rain taping on the window of your overpriced basement suite, reflecting the state of your soul. You never felt so alone. Even your partner left you, you loser!</p>

<p>But this time you got it. This time you have a strategy beyond clicking on ‚Äúretry failed steps‚Äù again and again. You finally read the logs, and you have an idea. You think you finely grasped what might be wrong. It‚Äôs a long shot: you‚Äôre gonna clear the credential cache, get an elevation you‚Äôre missing, force a permission sync, reset the service connection to the cluster, then downgrade the stupid auth library you‚Äôre using to a version that was hacked 6y ago but still works, setup everything again, then rollback. Your brain is making the connections, you got it, you‚Äôre better than that. You‚Äôre sorting through 23 different documentation tabs, waiting for the elevation to succeed, summoning all precious focus you got.</p>

<p>A red bubble shows up on IM. Conditioned by years of desperately reading your texts as soon as they arrived to create yourself what passes for a social life, your hand doesn‚Äôt even consults the sentient part of your cortex, and just moves to the little icon. <em>click</em>. It‚Äôs Mitch, your PM. He‚Äôs asking the url for a doc he wrote, and complains that it‚Äôs so complex to find doc in this organization.</p>

<p>This is a trap meant to get your focus out. Not this time. <strong>You‚Äôre better than that</strong>. You ignore his message, look at the elevation command, trigger it. Copy the id of the request that you need to preciously keep to finish the elevation to your clipboard.</p>

<p>4 minutes later you get a call request from your manager. You answer.</p>

<blockquote>
  <p>Hey, Mitch is saying he needs a doc urgently and you‚Äôre not answering his IM. Can you get to it?</p>
</blockquote>

<p><em>poof</em></p>

<p>Where the freak was I?</p>

<p><img src="https://fev.al/img/2024/focus.png" alt="poof"> 
<em>Credit: <a href="https://monkeyuser.com/">monkeyuser.com</a>. notice how much better it conveys my story.</em></p>

<p>Like everyone, I sometimes struggle to maintain focus. This was particularly true when I was a manager switching context all day long, this is also true as a  dev working on three antagonistic projects, including one that‚Äôs very consultory in nature, and working with processes that sometimes take hours to complete.</p>

<p>The typical situation is that I start something, switch to something else, get into a meeting, forget the very essence of what I was doing. Turn on autopilot, read all my emails, all my IMs. Then it‚Äôs 5PM, I‚Äôm exhausted, and I realize I‚Äôm at the same stage I was at 8AM, tell myself I should really achieve something that day, and open HN.</p>

<p>It was so bad at the beginning of this year that I seriously started wondering if I had ADHD<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. I started working on something badly documented. As in: there‚Äôs no documentation, the people who built that thing are halfway across the world and they don‚Äôt even really work for the company anymore. It wasn‚Äôt even just interruption, but also procrastination. I felt so frustrated by the situation that I started writing that in my daily notes on Obsidian.</p>

<blockquote>
  <p>I asked Berna how to turn on super-compression 5000, and she‚Äôs not answering again! I tried it with the <code>--yo-compress-shit really-well-like-5000-or-something</code> and it didn‚Äôt work. It still spits me that <code>yo is not proper English, be civilized</code> error. What the hell can I do about that.</p>
</blockquote>

<p>Mitch called me to ask for that doc again, which I gave him, not without mentioning that little ‚Äústar‚Äù icon in the url bar. And then got back at it.</p>

<p>And boy oh boy did that help! I just re-read what I was doing, and boom! I was back in.</p>

<p>I started listing all the commands I was running, and their results. Writing down my train of thoughts, the things I was doing and what I wanted to do next. And I have been doing that for the past 3-4 months. I feel like I invented something new. It helps me think more clearly, and restore the context so, so much faster when I switch between things. I‚Äôm almost looking forward to an interruption to get a chance to marvel again at my genius!</p>

<p>Except that it‚Äôs nothing new, right? ‚ÄúWriting helps you organize your thoughts more clearly‚Äù: everyone and their grandmother know that! Writing a plan, writing a diary? People keep listing how transformative that‚Äôs been for them. I‚Äôm not proposing a new framework. I‚Äôm just saying - every movie has a scientist recording themselves on one of these shitty little cassette recorders. They might be onto something. Write notes of what you‚Äôre doing and what you‚Äôre thinking. When you drop the pen and get back at it, read the last bit. That‚Äôs it.</p>

<p>I‚Äôve just been too lazy to ever do it. Or not necessarily lazy, but more: I didn‚Äôt trust the tool enough to think it was a good use of my time, and instead just mash on the keyboard till it works. After all, I‚Äôm writing pages of text, of which I will never read more than a fraction. But that‚Äôs not the point. The point is structure, and the point is caching.</p>

<p>I guess that‚Äôs kind of it: if you‚Äôre having trouble switching between things or getting focused, try writing what you‚Äôre doing, and read the last couple sentences when you resume. Maybe it will help you. Maybe it won‚Äôt. Or maybe I‚Äôm an idiot who needs crutches. But hey, who knows!</p>

<h2 id="notes">notes</h2>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crafting Interpreters (371 pts)]]></title>
            <link>https://craftinginterpreters.com/</link>
            <guid>40950235</guid>
            <pubDate>Fri, 12 Jul 2024 23:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://craftinginterpreters.com/">https://craftinginterpreters.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40950235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><em>Crafting Interpreters</em> contains everything you need to implement a
full-featured, efficient scripting language. You‚Äôll learn both high-level
concepts around parsing and semantics and gritty details like bytecode
representation and garbage collection. Your brain will light up with new ideas,
and your hands will get dirty and calloused. It‚Äôs a blast.</p>

<p>Starting from <code>main()</code>, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.</p>

<p>The book is available in four delectable formats:</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing the Lichess Tablebase Server (177 pts)]]></title>
            <link>https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd</link>
            <guid>40949943</guid>
            <pubDate>Fri, 12 Jul 2024 22:14:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd">https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd</a>, See on <a href="https://news.ycombinator.com/item?id=40949943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Recently our <a href="https://lichess.org/@/lichess/blog/7-piece-syzygy-tablebases-are-complete/W3WeMyQA">our 7 piece Syzygy tablebase server</a> was struggling to complete its periodic RAID integrity check while being hammered with tablebase requests. We decided to try a new approach, using <a href="https://man7.org/linux/man-pages/man7/lvmraid.7.html#DATA_INTEGRITY">dm-integrity on LVM</a>. Now, instead of periodically checking every data block, we passively check blocks whenever they are read.</p>
<p>17 TiB of tablebases are unwieldy, so to do this migration without hours of downtime, we set up a second server with the new approach. This also allowed us to run controlled benchmarks on the full set of tablebases, before finally doing the switch and retiring the old server.</p>
<p>We're trying to get the most out of the following new hardware:</p>
<ul>
<li>32 GiB RAM unchanged</li>
<li>2 x 201 GiB NVMe, where the previous server didn't have any SSD space. The rest of the 476 GiB disks is reserved for OS and working space</li>
<li>6 x 5.46 TiB HDD, where the previous server had only 5 disks</li>
</ul>
<p>The current operating system is Debian bookworm with default I/O schedulers:</p>
<pre><code>root@bwrdd:~# uname -a
Linux bwrdd 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux
root@bwrdd:~# cat /sys/class/block/nvme0n1/queue/scheduler
[none] mq-deadline
root@bwrdd:~# cat /sys/class/block/sda/queue/scheduler
[mq-deadline] none
</code></pre>
<h2>Monitoring important as ever</h2>
<p>RAID 5 is a good fit here, allowing recovery from any single disk failure, and distributing random reads across all disks. My first attempt was:</p>
<pre><code>$ lvcreate --type raid5 --raidintegrity y --raidintegrityblocksize 512 --name tables --size 21T vg-hdd # Oops
</code></pre>
<p>Peformance numbers in initial tests were decent, but we would have left a lot on the table if we didn't have monitoring to catch that not all disks were indeed participating equally.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:0yd54XyjtR2R:VUTWHQqX.png&amp;w=800&amp;sig=3fc8b923017ef07ac43342d38a06c1665b34d092" alt="Reads not distributed evenly"><br>Physical disk read activity with bad raid setup</p>
<p>That's because omitting <code>--stripes</code> does <em>not</em> default to use all physical volumes.</p>
<h2>Benchmark results (overview)</h2>
<p>In normal conditions the server receives between 10 and 35 requests per second. We record 1 million requests in the production environment to replay them in a controlled benchmark. In the chosen scenario, 12 parallel clients each sequentially submit requests from the production log.</p>
<p>Tables are lazily opened, and application and OS caches are lazily populated. So the first 800k response times are ignored as a warmup. We analyse the response times for the remaining 200k requests.</p>
<p>On average, response times are plenty fast, but tail latencies are high. So this is our focus for any optimizations. We'll unpack the results in a moment, but here are the empirical distribution functions (ECDFs) with 30ms added to each response time for an overview.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:KfbJv0yt3hyH:W2HFO1ha.png&amp;w=800&amp;sig=324cebd8631a951e7a67b9ec74831391dfcbca1e" alt="ECDFs with 30ms offset"><br>ECDFs</p>
<p>For a given response time on the x axis (log scale!) you can see which proportion of requests is faster. Or for a given proportion on the y axis (think percentile), you can read off the corresponding response time on the x axis.</p>
<p>The added constant seems artificial, but it's just viewing the results from the point of view of a client with 30ms ping time. Otherwise the log scaled x-axis would overemphasize the importance of a few milliseconds at the low end.</p>
<h2>mmap with higher tail latencies than pread</h2>
<p>Our Syzygy tablebase implementation <a href="https://github.com/niklasf/shakmaty-syzygy">shakmaty-syzygy</a> now offers an interface to plug in different ways of opening and reading from table files. The main contenders are:</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man2/mmap.2.html">Map</a> table files into memory. After the file has been mapped, disk reads happen transparently when accessing the respective memory region, so no further system calls are needed. Unfortunately, that also means reads look just as infallible as normal memory accesses, so that errors can only be handled out of band, via signals.</li>
<li><a href="https://man7.org/linux/man-pages/man2/pwrite.2.html">pread(2)</a>, one system call per read, with read error reporting via the return value.</li>
</ul>
<p>More robust error handling would probably be enough to justify using <code>pread</code> for a server implementation, but surprisingly, the diagram above shows that <code>pread</code> also peforms <em>better</em> in the scenario we care about. Perhaps that is because sometimes transparently reading a single memory-mapped data block across page boundaries may end up issuing two disk reads</p>
<pre><code>while (...)
{
    uint8_t d = *ptr++;
}
</code></pre>
<p>whereas</p>
<pre><code>uint8_t buf[MAX_BLOCK_SIZE];
ssize_t res = pread(fd, buf, block_size, offset);
</code></pre>
<p>immediately reveals how much data will be read.</p>
<p>Now, before you change your chess engine to use <code>pread</code>: Tablebases in engine matches are typically used only if enough fast storage for all WDL tables is available. The typical range of response times is not even visible in the graph above. Here, the saved syscall overhead is significant, so that memory mapping performs better.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:TLBrF0isfFgq:FbA5f9fq.png&amp;w=800&amp;sig=8aa103c6da4a742e53ed5e78949542a0231afb79" alt="ECDFs without offset"><br>Zoom on ECDFs: mmap saves syscall overhead</p>
<h2><code>MADV_RANDOM</code> / <code>POSIX_FADV_RANDOM</code> counter-productive</h2>
<p>The next surprise looking at the results above, is that <a href="https://man7.org/linux/man-pages/man2/posix_fadvise.2.html"><code>posix_fadvise(fd, 0, 0, POSIX_FADV_RANDOM)</code></a> or its equivalent for memory maps are actually mostly counter-productive. <code>POSIX_FADV_RANDOM</code> is intended to alleviate pressure on the page cache, by hinting to the operating system that file accesses are going to be random and automatic read-ahead is likely pointless.</p>
<p>Perhaps tablebase access patterns when people are analysing endgames are not so random afterall.</p>
<p>Again, this may differ for chess engines, where probes may be more likely to be scattered across different possible endgames.</p>
<h2>Table prefixes on limited SSD space</h2>
<p>To decide how to use the limited SSD space, let's have a look at the anatomy of a single table probe. The position will be encoded as an integer index, based on encoding information from the table header. Then we need to find the compressed data block that contains the result for the particular index. Syzygy provides a "sparse" block length list, which points close to the correct entry in the block length list, which is then used to find the relevant data block.</p>
<div>
<table>
<thead>
<tr><th>Table section sizes</th><th>WDL</th><th>DTZ</th><th>Total</th></tr>
</thead>
<tbody>
<tr><td>Headers and sparse block length lists</td><td>38 GiB</td><td>9 GiB</td><td>47 GiB</td></tr>
<tr><td>Block length lists</td><td>274 GiB</td><td>64 GiB</td><td>339 GiB</td></tr>
<tr><td>Compressed data blocks</td><td>8433 GiB</td><td>8458 GiB</td><td>16891 GiB</td></tr>
</tbody>
</table>
</div>
<p>We could certainly use the SSD space for an additional layer of adaptive caching, to cache hot list entries and data blocks. But since we're trying to improve tail latencies in particular, it makes sense to think about the worst case. By putting the sparse block length lists and the block length lists on SSD storage, hot or cold, we can guarantee a maximum of 1 slow disk read per table probe.</p>
<p>In our case that doesn't quite fit when using the SSD space in RAID 1 (mirrored), but since this optimization is optional, we can give up redundancy and use RAID 0.</p>
<h2>Parallelizing reads</h2>
<p>In chess engines, a typical tablebase request will be for a single WDL value. But for the user interface we instead want to display DTZ values for all moves.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:JGE5DJVrlUwR:VQu8XPZu.png&amp;w=800&amp;sig=66061784df25478d0e120a193664ce6dbd2ee481" alt="Screenshot of tablebase response"><br>Tablebase explorer</p>
<p>That, together with Syzygy's internal resolution of captures, will cause the average request to issue 23 WDL probes and 70 DTZ probes. In the initial implementation handling of requests was parallelized, but probes within each request were executed sequentially.</p>
<p>In the benchmark results we can see that using more fine grained parallelism has some overhead at the low end, but significantly reduces tail latencies. Of course the disks can not really physically handle that many parallel reads, but now the I/O scheduler is more likely to plan them in a way that will finish each request as soon as possible, and can better plan the order of all involved disk accesses (minimizing time until the disk's read head is at the next requested sector).</p>
<h2>Performance in production</h2>
<p>Finally, it's good to confirm that optimizations in the benchmark scenario actually help in production. Here are response time charts sliced together.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=revoof:ublogBody:hil5PjrHZMoy:Hc1Y8NLW.png&amp;w=800&amp;sig=fd4b864738e2060844341e192d20cfd51fa86efd" alt="Standard chess response times significantly reduced in production"><br>Standard chess response times</p>
<hr>
<p>Raw data at <a href="https://github.com/niklasf/lila-tablebase-bench-tool">https://github.com/niklasf/lila-tablebase-bench-tool</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I know about Git commits (108 pts)]]></title>
            <link>https://www.jvt.me/posts/2024/07/12/things-know-commits/</link>
            <guid>40949229</guid>
            <pubDate>Fri, 12 Jul 2024 20:42:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2024/07/12/things-know-commits/">https://www.jvt.me/posts/2024/07/12/things-know-commits/</a>, See on <a href="https://news.ycombinator.com/item?id=40949229">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Written by</span>
Jamie Tanna<br><span>on&nbsp;</span><time datetime="2024-07-12T20:01:09+0100">July 12, 2024</time><br><span><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"><i></i> CC-BY-NC-SA-4.0</a>
<a href="http://www.apache.org/licenses/LICENSE-2.0"><i></i> Apache-2.0</a></span><br><span>7 mins</span></p></div><div><p>This is an article that's been swimming around in my head for ~5 weeks now, and may become a "living post" that I keep updated over time.</p><p>In no particular order, some things I've learned about Git commits and commit history, over the last 12 years. This is a mix of experience in companies with teams of 2-12 people, as well as in Open Source codebases with a vast range of contributors.</p><ol><li>Git has different uses - a collaboration tool, a backup tool, a documentation tool</li><li>Git commit messages are excellent</li><li>I've never met anyone who likes reading commit messages as much as me</li><li>Finding out why a change was made is easier sorting through commits than it is through an issue/bug tracker</li><li>It's better to have a commit that says "Various fixes. DEV-123" than it is to have "Various fixes"</li><li>It's worse to have a commit that says "Various fixes. DEV-123" when the issue itself has no useful information</li><li>Rebase-merging is my preference. Then squash-merge, then merge</li><li>If you don't learn how to rebase, you're missing out on a good skill</li><li>People who say "just delete the repo" when things go wrong really frustrate me</li><li>Learn how to use <code>git reflog</code>, and it'll save you deleting a repo and work that was recoverable</li><li>Learn how to use <code>git reflog</code>, and you'll be able to save yourself from mistakes that aren't that bad</li><li>No amount of learning fancy tools and commands saves you from fucking up every once in a while</li><li>My most recent botched rebase was last week, and I needed <code>git reflog</code> to help un-fuck it</li><li>Learn how to <a href="https://www.jvt.me/posts/2021/10/23/undo-force-push/">undo a force push</a> and then how to <a href="https://www.jvt.me/posts/2018/09/18/safely-force-git-push/">more safely force push</a> (remember the <code>=ref</code>!!)</li><li>Squashing is a waste of well-written atomic commits</li><li>Squashing is better than 100 crap commits</li><li>Squashing, and writing a good commit message at the time of merge is good</li><li>Squashing, and then not re-editing the message is the worst</li><li>Squashing, when you have 100 crap commits, and then not re-editing the message is <strong>a crime</strong></li><li>Squashing, and then not re-editing the message is worse than a merge commit from a branch with 100 crap commits</li><li>Writing a well documented PR/MR description but not using that to inform the squash-merge message is a waste of time</li><li>Writing commit messages have helped me pick up on missing test cases, missing documentation, or invalid thought processes, as it helps me rewrite <em>why</em> the changes are being made</li><li>Using your <code>git log</code> as an indication for standup updates is valid</li><li>I can't be bothered to sign my commits (unless I'm forced to)</li><li>If I have to sign my commits, SSH key signing makes it almost not awful</li><li>If you're moving files between repos, you need to keep the history intact, <a href="https://www.jvt.me/posts/2018/06/01/git-subtree-monorepo/">using <code>git subtree</code></a></li><li>Commits should be atomic - all the code and tests, and configuration changes should all be in there</li><li>I spend a good chunk of time ensuring that each commit passes CI checks atomically</li><li>Some people do horrific things, like split their implementation and test code from each other</li><li>It's OK to put documentation in a separate commit - we don't have to have a whole end-to-end feature delivery in a single commit</li><li>Repos that use squash-merges suck</li><li>As a maintainer of Open Source projects, I like squash-merge, so I can rewrite contributors' commit messages</li><li>Sometimes it's not worth coaching how to write a given commit message</li><li>The people around you shape the way you write commits</li><li>Do the work up front to make your history atomic</li><li>It's much more painful to split a mega-commit into atomic commits after the fact</li><li>Splitting work atomically can be good for improving your reward drive - you can get many more things done</li><li>Atomic commits work really well with <a href="https://www.jvt.me/posts/2022/04/12/prefactor/">prefactoring</a></li><li>Sometimes prefactor commits can go into separate PRs (especially if squash-merge is used)</li><li>Writing commit messages can take longer than the implementation</li><li>The commit message can be an order of magnitude larger than the number of lines changed in a commit</li><li>If you end up writing a lot of "and"s or "also" in a commit message, you may be trying to do too many things</li><li>Trawling through Git commit history in the past has helped unlock a number of cases where I could then understand the <em>why</em> without the original authors there to answer my questions</li><li>Commit messages are a great point to reflect on not just what you've done, but <strong>why</strong></li><li>Why is more important than what - anyone can look at the diff and generally work out what changes were made, but the intent behind it is the special sauce</li><li>If you only write what changed, you're annoying, and I dislike you</li><li>A commit that explains what is better than a commit that just has "fixes"</li><li><span><a href="https://cbea.ms/">Chris Beams</a></span>' article, <a href="https://cbea.ms/git-commit/"><em>How to Write a Git Commit Message</em></a> is still an excellent post and a great place to start, just under 10 years later!</li><li>Commits are a point-in-time explanation of the assumptions and the state of the world for the committer. Don't be too hard on them</li><li>I don't want to read AI/LLM rewrites of your changes - either write it yourself, or call it <code>Various fixes</code></li><li>There needs to be a way to add an annotation (maybe using <code>git notes</code>) to a previous message to correct assumptions</li><li>I won't write perfect commit messages up front - they'll sometimes be as much as <code>rew! add support for SBOMs</code> or <code>sq</code>, or <a href="https://www.jvt.me/posts/2019/01/10/git-commit-fixup/">use <code>git commit --fixup</code></a></li><li>I will, generally, break off very good atomic chunks of work into commits</li><li>I will split atomic commits into multiple commits, sometimes</li><li>Make sure you <a href="https://www.jvt.me/posts/2019/01/12/self-code-review/">review your own code changes</a> before you send it out for review to your collaborators</li><li>Reviewing your commit messages should be as important as reviewing the code changes</li><li>Getting all your contributors to invest the same amount of care into the commit history is a losing battle</li><li>Trying to police commit history is going to be painful</li><li>Trying to mandate reviews of commit messages as part of code review is going to be painful</li><li>Trying to police commit history does lead to a greater level of documentation and consideration around changes int he codebase</li><li>Making implicit assumptions explicit is really useful</li><li>Introducing <code>commitlint</code> can be useful, but also frustrating</li><li>It's nicer to have your collaborators want to write good commit messages than you having to force them</li><li>Some people don't write, and that's alright</li><li>Writing is a skill</li><li>I'm not perfect at writing (commit messages)</li><li>Sometimes I can't be arsed to write the perfect message</li><li>Sometimes I write some really great commit messages, and impress myself</li><li>Using a <a href="https://www.jvt.me/posts/2017/04/17/commit-templates/">template for your Git commit messages</a> is a good nudge to doing it right</li><li><a href="https://www.jvt.me/posts/2019/01/10/git-commit-fixup/"><code>fixup</code> commits</a> and <code>git rebase --autosquash</code> has been one of the best Git tips I've learned</li><li>I value working on a team with a diverse set of perspectives, skills and approaches to work</li><li>But I also really value having a team who writes atomic commits with well-written commit messages</li><li>Commit message writing is as useful as writing well-refined user stories/tickets</li><li><code>git commit -m sq</code> is probably my most-run command</li><li>Using <code>git add -p</code> and <code>git commit -p</code> are hugely important for atomic commits</li><li>Never use <code>git add -u</code> or <code>git add .</code></li><li>Learn when you can use <code>git add -u</code> or <code>git add .</code></li><li>I really need to look into tools like Graphite, <code>git-branchless</code> and other means to provide a stacked PR setup</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> with <a href="https://github.com/semantic-release/semantic-release">semantic-release</a> or <a href="https://github.com/go-semantic-release/semantic-release">go-semantic-release</a> can make a huge difference when wanting to release automagically and often</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> as a framework for your commits can be really useful</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a>, as someone with ADHD, reduces the need for thinking at times and can allow you to focus more on what the changes are</li><li>Using <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a> helps you work out when you're trying to do too much in a commit</li><li>I <a href="https://www.jvt.me/posts/2023/10/04/blogging-neurodiversity/">think through writing</a> so commit messages help understand <a href="https://www.youtube.com/watch?v=1qdyNoe3q2A">why I did the thing</a></li><li>It can be better to write a good commit message than a piece of documentation, stored elsewhere</li><li>It can be better to write a good commit message than a code comment</li><li>Give people space to learn</li><li>Give people space to fail</li><li>Remember you weren't so great at one point in time</li><li>Documentation is rad. Do more of it</li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["GitHub" Is Starting to Feel Like Legacy Software (116 pts)]]></title>
            <link>https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/</link>
            <guid>40949034</guid>
            <pubDate>Fri, 12 Jul 2024 20:19:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/">https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/</a>, See on <a href="https://news.ycombinator.com/item?id=40949034">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I‚Äôve used a lot of tools over the years, which means I‚Äôve seen a lot of tools hit a plateau. That‚Äôs not always a problem; sometimes something is just ‚Äúdone‚Äù and won‚Äôt need any changes. Often, though, it‚Äôs a sign of what‚Äôs coming. Every now and then, something will pull back out of it and start improving again, but it‚Äôs often an early sign of long-term decline. I can‚Äôt always tell if something‚Äôs just coasting along or if it‚Äôs actually started to get worse; it‚Äôs easy to be the boiling frog. That changes for me when something that <em>really</em> matters to me breaks.</p>

<p>To me, one of GitHub‚Äôs killer power user features is its <code>blame</code> view. <code>git blame</code> on the commandline is useful but hard to read; it‚Äôs not the interface I reach for every day. GitHub‚Äôs web UI is not only convenient, but the ease by which I can click through to older versions of the blame view on a line by line basis is uniquely powerful. It‚Äôs one of those features that anchors me to a product: I stopped using offline graphical git clients because it was just that much nicer.</p>

<p>The other day though, I tried to use the blame view on a large file and ran into an issue I don‚Äôt remember seeing before: I just <em>couldn‚Äôt find</em> the line of code I was searching for. I threw various keywords from that line into the browser‚Äôs command+F search box, and nothing came up. I was stumped until a moment later, while I was idly scrolling the page while doing the search again, and it finally found the line I was looking for. I realized what must have happened.</p>

<p>I‚Äôd heard rumblings that GitHub‚Äôs in the middle of shipping a frontend rewrite in React, and I realized this must be it. The problem wasn‚Äôt that the line I wanted wasn‚Äôt on the page‚Äîit‚Äôs that the whole document wasn‚Äôt being rendered at once, so my browser‚Äôs builtin search bar just <em>couldn‚Äôt find it</em>. On a hunch, I tried disabling JavaScript entirely in the browser, and suddenly it started working again. GitHub is <em>able</em> to send a fully server-side rendered version of the page, which actually works like it should, but doesn‚Äôt do so unless JavaScript is completely unavailable.</p>

<p>I‚Äôm hardly anti-JavaScript, and I‚Äôm not anti-React either. Any tool‚Äôs perfectly fine when used in the right place. The problem: this <em>isn‚Äôt the right place</em>, and what is to me personally a key feature suddenly doesn‚Äôt work right all the time anymore. This isn‚Äôt the only GitHub feature that‚Äôs felt subtly worse in the past few years‚Äîthe once-industry-leading status page no longer reports minor availability issues in an even vaguely timely manner; Actions runs randomly drop network connections to GitHub‚Äôs own APIs; hitting the merge button sometimes scrolls the page to the wrong position‚Äîbut this is the first moment where it really hit me that GitHub‚Äôs probably not going to get better again from here.</p>

<p>The corporate branding, the new ‚ÄúAI-powered developer platform‚Äù slogan, makes it clear that what I think of as ‚ÄúGitHub‚Äù‚Äîthe traditional website, what are to me the core features‚Äîsimply isn‚Äôt Microsoft‚Äôs priority at this point in time. I know many talented people at GitHub who care, but the company‚Äôs priorities just don‚Äôt seem to value what I value about the service. This isn‚Äôt an anti-AI statement so much as a recognition that the tool I still need to use every day is past its prime. Copilot isn‚Äôt navigating the website for me, replacing my need to the website as it exists today. I‚Äôve had tools hit this phase of decline and turn it around, but I‚Äôm not optimistic. It‚Äôs still plenty usable now, and probably will be for some years to come, but I‚Äôll want to know what other options I have <em>now</em> rather than when things get worse than this.</p>

<p>And in the meantime, well‚Ä¶ I still need to use GitHub everyday, but maybe it‚Äôs time to start exploring new platforms‚Äîand find a good local <code>blame</code> tool that works as well as the GitHub web interface used to. (Got a fave? Send it to me at <a href="https://digipres.club/@misty">misty@digipres.club</a> / <a href="https://bsky.app/profile/cdrom.ca">@cdrom.ca</a>. Please!)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Responsive bar charts in HTML and CSS (120 pts)]]></title>
            <link>https://9elements.com/blog/responsive-bar-charts-in-html-and-css/</link>
            <guid>40949021</guid>
            <pubDate>Fri, 12 Jul 2024 20:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9elements.com/blog/responsive-bar-charts-in-html-and-css/">https://9elements.com/blog/responsive-bar-charts-in-html-and-css/</a>, See on <a href="https://news.ycombinator.com/item?id=40949021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Building flexible data visualizations for international sites</strong></p><p>For our international clients, we have created dynamic charts and data visualizations for the web. Charts typically render shapes like lines and paths, rectangles and circles. They contain text for titles, axis labels, numerical values and legends.</p><p>SVG is the good fit for this purpose. It embeds directly into HTML and pairs well with CSS. However, for dynamic data visualizations on the web, SVG poses a challenge.</p><h2 id="Responsive-charts-and-the-problems-of-SVG">Responsive charts and the problems of SVG</h2><p>The websites we build feature responsive layouts and fluid typography. We employ CSS Flexbox and Grid together with media and container queries to fit in the content. There is not one single fixed presentation, but many possible presentations depending on the content and the reading environment.</p><p>In contrast, SVG does not have layout techniques like Flexbox, Grid or even <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_flow_layout">Normal Flow</a>. In SVG, all shapes are absolutely positioned. Text does not wrap automatically. The shapes and text need to be laid out manually by the code that generates the SVG.</p><p>SVG does scale continuously, as the name says ‚Äì but for charts on the web, we usually do not want that. A small chart should not look like a downscaled big chart. Text would become unreadable, shapes would become tiny pixel mush ‚Äì even with techniques that <a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/vector-effect#non-scaling-stroke">prevent the scaling of some graphical features</a>.</p><p>For charts on the web, we want quantitative and qualitative responsive scaling. A small and a large chart should be designed and laid out differently. A small chart should focus on clear, distinguishable marks that represent the data. A large chart should take advantage of the screen estate to show more items and details as well as provide context.</p><p>For example, a line chart with multiple lines may switch to small multiples on smaller viewports or containers.</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.avif 352w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-704w-embedded.avif 704w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Line chart with six lines representing six world regions (Western Pacific, Europe, Americas, South-East Asia, Eastern Mediterranean, Africa). Lines are colored differently and sometimes overlap." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/6GQrENkrcOjoCMIY8TZIQO-352w-embedded.jpeg" width="1408" height="1051"></picture></figure><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.avif 352w, https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Two-column grid of six small line charts, one line chart for each world region. All lines are colored blue. The y axes are aligned so the lines are comparable." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/1IqU5V32yN34rKj2RNREFi-352w-embedded.jpeg" width="704" height="960"></picture></figure><p>We have typically implemented this responsiveness with client-side JavaScript logic. JavaScript is able to read the container size and measure text in order to compute all shape coordinates and sizes. This often involves decollision with <a href="https://d3js.org/d3-force">force simulations</a>.</p><p>This approach has severe disadvantages. The cycle of forcing the browser to compute the style, reading sizes and setting positions leads to <a href="https://web.dev/articles/avoid-large-complex-layouts-and-layout-thrashing">layout thrashing</a> and slows down the chart rendering.</p><p>When the container size changes, for example due to a browser resize or orientation change, the JavaScript needs to compute all SVG positions and sizes from scratch. Assuming this takes 50-100ms per chart, a page with 20 charts freezes the browser for 1-2 seconds.</p><h2 id="HTML-CSS-and-SVG-hybrid">HTML, CSS and SVG hybrid</h2><p>Horizontal bar charts are simple yet effective, intuitive and accessible visualizations. They are versatile regarding the bar design, labeling, value placement and axes. And they can be highly flexible regarding the container size.</p><p>We have a pretty solid implementation of a responsive bar chart. In narrow containers, the row label is shown on top of the bar. In wide containers, it is shown next to the bar.</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.avif 352w, https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Horizontal bar chart for four countries. The country names are placed on top of the bars. The vertical x axis lines span the whole height. Next to each bar, there is a label with the value for the country." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/5Sulbp7D4VfW4YXX4s1hKD-352w-embedded.jpeg" width="704" height="357"></picture></figure><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.avif 352w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-704w-embedded.avif 704w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Horizontal bar chart for four countries. The country names are placed in a column on the left side. The bars as well as the x axis ticks and labels are placed in a column on the right side. Next to each bar, there is a label with the value for the country." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/22MPkZRNittq3duJ5JAx2X-352w-embedded.jpeg" width="1408" height="359"></picture></figure><p>This chart is a hybrid of HTML, CSS and SVG. We wanted to use essential CSS layout methods like Flexbox instead of re-implementing layout algorithms in JavaScript. However, the synchronization with the SVG parts is still slow, complex client-side JavaScript code.</p><h2 id="lessstronggreaterBar-chart-in-plain-HTML-andamp-CSSlessstronggreater"><strong>Bar chart in plain HTML &amp; CSS</strong></h2><p>We were wondering: Can we achieve this with HTML and CSS alone, preferably without SVG and with less JavaScript logic? We fiddled around, but never finished this idea.</p><p>Then we saw the <a href="https://2023.stateofjs.com/en-US/features/#syntax_features">beautiful responsive bar charts of State of JS</a>, made with HTML &amp; CSS only. On narrow viewports, they use a two-column grid:</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.avif 352w, https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-704w-embedded.avif 704w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-704w-embedded.jpeg 704w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Bar chart from &quot;State of JS&quot; showing how many people have used a certain JavaScript features. X axis values on the top, lines spanning the whole chart. Three rows, one for each JavaScript feature. Feature label and number of users on top of the bar." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/7xQa0or3SZ1lXzbFdIfGs8-352w-embedded.jpeg" width="704" height="506"></picture></figure><p>On wide viewports, this is a three-column grid with subgrids that inherit the column setup:</p><figure><picture><source type="image/avif" srcset="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.avif 352w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-704w-embedded.avif 704w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-1408w-embedded.avif 1408w" sizes="(min-width: 176em) 176rem, 100vw"><source type="image/jpeg" srcset="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.jpeg 352w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-704w-embedded.jpeg 704w, https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-1408w-embedded.jpeg 1408w" sizes="(min-width: 176em) 176rem, 100vw"><img alt="Bar chart from &quot;State of JS&quot; showing usage percentage of JavaScript features. X axis values on the top and bottom, lines spanning the whole height. Left column contains feature name, middle column the bar, right column the absolute user number." loading="lazy" decoding="async" src="https://9elements.com/images/ctfl/3Psc17pbAY5Zyd8WZZ61dx-352w-embedded.jpeg" width="1408" height="588"></picture></figure><p>These well-made charts encouraged us to try to migrate our bar charts to HTML &amp; CSS.</p><h2 id="lessstronggreaterGrid-setuplessstronggreater"><strong>Grid setup</strong></h2><p>For a start, we <a href="https://codepen.io/molily/pen/gOJVQgB?editors=1100">rebuild the basic structure</a>:</p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="gOJVQgB" data-pen-title="Responsive bar chart with ticks" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/gOJVQgB">Responsive bar chart with ticks</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><p>In the narrow version, the each row (<code>li</code> element) is a two-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> <span>minmax</span><span>(</span>0<span>,</span> 1fr<span>)</span> min-content<span>;</span>
<span>grid-template-areas</span><span>:</span>
  <span>"dimension value"</span>
  <span>"bar bar"</span><span>;</span>
<span>position</span><span>:</span> relative<span>;</span></code></pre><p>In the wide version, the wrapper (<code>ol</code> element) is a three-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-areas</span><span>:</span> <span>"dimension bar value"</span><span>;</span>
<span>grid-template-columns</span><span>:</span> <span>fit-content</span><span>(</span>10rem<span>)</span> 1fr min-content<span>;</span></code></pre><p>The row (<code>li</code>) is a subgrid that spans all columns:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> subgrid<span>;</span>
<span>grid-template-areas</span><span>:</span> none<span>;</span>
<span>grid-column</span><span>:</span> 1 / -1<span>;</span></code></pre><h2 id="lessstronggreaterReal-world-requirementslessstronggreater"><strong>Real-world requirements</strong></h2><p>Our real bar chart, however, is much more complex and has the following requirements:</p><ul><li><strong>Internationalization with bidirectional text</strong>: We're building charts for sites in six languages and two text directions: Left-to-right (LTR, like English and Russian) and right-to-left (RTL, like Arabic and Hebrew).</li><li><strong>Positive and negative values</strong>. Bars grow to both sides.</li><li><strong>Row labels</strong> may have an arbitrary length and should wrap and align nicely.</li><li><strong>Value labels</strong> should be positioned at the end of the bars, not inside them or in a separate column.</li><li><strong>Do not repeat the axis tick lines</strong> for each row if it's avoidable.</li></ul><p>This is the solution we came up with:</p><p><strong></strong><a href="https://codepen.io/molily/pen/JjqgxVR?editors=1100"><strong>Responsive bar chart in HTML &amp; CSS</strong></a></p><p><i>This is version 2 which implements essential feedback from </i><a href="https://vesa.piittinen.name/"><i>Vesa Piitiinen</i></a><i>.</i></p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="JjqgxVR" data-pen-title="Bar chart version 2" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/JjqgxVR">Bar chart</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><p>Let's dive into the implementation.</p><h3><strong>Responsive grid setup</strong></h3><p>The HTML structure looks like this:</p><pre><span>html</span><code><span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar-chart<span>"</span></span><span>&gt;</span></span>
  <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>ticks<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>
    <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>tick<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>inset-inline-start:</span> <span>{</span>percent<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>{tick value}<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span>&lt;!-- ‚Ä¶ more ticks ‚Ä¶ --&gt;</span>
  <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
  <span><span><span>&lt;</span>ol</span><span>&gt;</span></span>
    <span><span><span>&lt;</span>li</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>dimension<span>"</span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>dimension-label<span>"</span></span><span>&gt;</span></span>{dimension label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>margin-inline-start:</span> <span>{</span>bar start<span>}</span>%<span>;</span> <span>width:</span> <span>{</span>bar width<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>bar-label<span>"</span></span><span>&gt;</span></span>{ bar label }<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>value<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>{ bar label again }<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span><span><span>&lt;/</span>li</span><span>&gt;</span></span>
    <span>&lt;!-- Bars with negative values require a class is-negative: --&gt;</span>
    <span><span><span>&lt;</span>li</span> <span>class</span><span><span>=</span><span>"</span>is-negative<span>"</span></span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>dimension<span>"</span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>dimension-label<span>"</span></span><span>&gt;</span></span>{dimension label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span>&lt;!-- And the value need to placed before the bar: --&gt;</span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>value<span>"</span></span> <span>aria-hidden</span><span><span>=</span><span>"</span>true<span>"</span></span><span>&gt;</span></span>{bar label again}<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
      <span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>bar<span>"</span></span> <span><span>style</span><span><span>=</span><span>"</span><span><span>margin-inline-start:</span> <span>{</span>bar start<span>}</span>%<span>;</span> <span>width:</span> <span>{</span>bar width<span>}</span>%</span><span>"</span></span></span><span>&gt;</span></span>
        <span><span><span>&lt;</span>span</span> <span>class</span><span><span>=</span><span>"</span>bar-label<span>"</span></span><span>&gt;</span></span>{bar label}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
      <span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
    <span><span><span>&lt;/</span>li</span><span>&gt;</span></span>
    <span>&lt;!-- ‚Ä¶ more li elements ‚Ä¶ --&gt;</span>
  <span><span><span>&lt;/</span>ol</span><span>&gt;</span></span>
<span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>In the narrow version, the <code>.bar-chart</code> wrapper is a three-column grid:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-template-columns</span><span>:</span> min-content 1fr min-content<span>;</span>
<span>grid-template-areas</span><span>:</span>
  <span>"dimension dimension dimension"</span>
  <span>"valuePaddingStart bar valuePaddingEnd"</span><span>;</span></code></pre><p>The <code>ol</code> element and <code>li</code> elements are subgrids:</p><pre><span>css</span><code><span>display</span><span>:</span> grid<span>;</span>
<span>grid-column</span><span>:</span> 1 / -1<span>;</span>
<span>grid-template-columns</span><span>:</span> subgrid<span>;</span></code></pre><p>In the wide version, the wrapper becomes a four-column grid:</p><pre><span>css</span><code><span>grid-template-areas</span><span>:</span> <span>"dimension valuePaddingStart bar valuePaddingEnd"</span><span>;</span>
<span>grid-template-columns</span><span>:</span> <span>fit-content</span><span>(</span>10rem<span>)</span> min-content 1fr min-content<span>;</span>
</code></pre><p>Each row remains a subgrid.</p><h2 id="lessstronggreaterBidirectional-textlessstronggreater"><strong>Bidirectional text</strong></h2><p>Internationalization is where HTML and CSS shine compared to SVG.</p><p>Our JavaScript code that generates SVG charts is full or <code>if (isLTR) {‚Ä¶} else {‚Ä¶}</code> conditionals. In SVG, the origin of the coordinate system is always top left. X coordinates need to be calculated using those LTR/RTL switches.</p><p>In HTML and CSS, we can simply use <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_logical_properties_and_values">logical properties</a> like <code>inset-inline-start/-end</code>, <code>margin-inline-start/-end</code> as well as <code>padding-inline-start/-end</code> to solve most left-to-right vs. right-to-left differences. When laying out the boxes in CSS, we can work with the text direction.</p><p>For example, each bar is a Flexbox container with the value label nested inside. Then the label is positioned next to the bar: For positive values, we add a box with <code>::before</code> plus <code>content: ''</code> with a <code>padding-inline-start</code> of 100%. For negative values, we add a box with <code>::after</code> plus <code>content: ''</code> with a <code>padding-inline-end: 100%</code>. These boxes push the label out of the bar so it sits right next to it.</p><p>We still need to handle positive and negative values differently, but by using Flexbox, logical properties and the current text direction, we don't need to handle left-to-right and right-to-left differently.</p><p>The bar labels are <i>also</i> rendered into the columns named <code>valuePaddingStart</code> and <code>valuePaddingEnd</code>. These invisible placeholders ensure the columns have the correct width to accommodate the absolutely positioned value labels. So the labels appear twice in the DOM. The placeholders have <code>aria-hidden="true"</code> and <code>visibility: hidden</code> though.</p><h2 id="lessstronggreaterTick-lines-spanning-the-full-heightlessstronggreater"><strong>Tick lines spanning the full height</strong></h2><p>Our goal to put the axis tick lines in the DOM only once instead of repeating them for each row complicates the grid. The challenge is to constrain the tick lines in the bar column horizontally, but let them span the whole grid vertically.</p><p>This is possible with <code>grid-row: 1 / -1</code> given the grid has <strong>explicit rows</strong>. It does not work with an arbitrary number of implicitly-created rows.</p><p>So we defined an outer grid that <strong>has two fixed rows</strong>. The ticks are then positioned in the first row, spanning two rows.</p><pre><span>css</span><code><span>.ticks</span> <span>{</span>
  <span>grid-column</span><span>:</span> bar<span>;</span>
  <span>grid-row</span><span>:</span> 1 / span 2<span>;</span>
<span>}</span></code></pre><p>The list of bars is then positioned in the second row and spans all columns of the parent grid. It creates a subgrid that inherits the grid configuration from the parent grid.</p><pre><span>css</span><code><span>ol</span> <span>{</span>
  <span>display</span><span>:</span> grid<span>;</span>
  <span>grid-row</span><span>:</span> 2<span>;</span>
  <span>grid-column</span><span>:</span> 1 / -1<span>;</span>
  <span>grid-template-columns</span><span>:</span> subgrid<span>;</span>
<span>}</span></code></pre><p>The subgrid may then create an arbitrary number of implicit rows. It remains nested in the second row of the outer grid.</p><p><a href="https://codepen.io/molily/pen/wvbVNbY?editors=1100">Minimal example on CodePen</a>:</p><figure><p data-height="300" data-default-tab="html,result" data-slug-hash="wvbVNbY" data-pen-title="Grid: Span whole grid" data-preview="true" data-editable="true" data-user="molily"><span>See the Pen <a href="https://codepen.io/molily/pen/wvbVNbY">Grid: Span whole grid</a> by molily (<a href="https://codepen.io/molily">@molily</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p></figure><h2 id="lessstronggreaterAccessibility-considerationslessstronggreater"><strong>Accessibility considerations</strong></h2><p>Accessibility of data visualizations is a top priority for us and our clients. In our SVG charts and HTML / SVG hybrids, we have assigned ARIA roles and accessible labels so graphical shapes have proper semantics and textual representation. In the accessibility tree, these charts appear either as lists (like <code>ul</code> or <code>ol</code> elements) or tables (like the <code>table</code> element) so users can read and navigate the chart in a familiar way.</p><p>While we have made SVG charts accessible, it is simpler and more robust to use semantic HTML directly. The shown HTML &amp; CSS bar chart uses plain <code>ol</code> and <code>li</code> elements with built-in ARIA roles. Screen readers and other assistive tools read out the labels and values.</p><p>Edge with JAWS on Windows:</p><p><video controls=""><source src="https://videos.ctfassets.net/bo3k2i3mbg0o/2vLU4rEGRrRvviFPMB0GqU/b9a0d2171b17072ca3995c4c95532e11/jaws-edge.mp4" type="video/mp4">Reading the bar chart with JAWS and Edge. Navigating through the labels and values by keyboard. <a href="https://videos.ctfassets.net/bo3k2i3mbg0o/2vLU4rEGRrRvviFPMB0GqU/b9a0d2171b17072ca3995c4c95532e11/jaws-edge.mp4">jaws-edge.mp4</a></video></p><p>Chrome with VoiceOver on MacOS:</p><p><video controls=""><source src="https://videos.ctfassets.net/bo3k2i3mbg0o/1TaiDq3atkiGPaZqVFWSFc/6aafd1a35f5931061420cbd171daa94b/voiceover-chrome.mp4" type="video/mp4">Reading the bar chart with VoiceOver and Chrome. Navigating through the labels and values by keyboard. <a href="https://videos.ctfassets.net/bo3k2i3mbg0o/1TaiDq3atkiGPaZqVFWSFc/6aafd1a35f5931061420cbd171daa94b/voiceover-chrome.mp4">voiceover-chrome.mp4</a></video></p><h2 id="lessstronggreaterRecaplessstronggreater"><strong>Recap</strong></h2><p>Today's websites feature responsive layout and fluid typography. Data visualizations should adapt these design techniques.</p><p>While responsive and accessible SVGs are possible, they require manual client-side JavaScript logic. HTML and CSS allow us to create charts using declarative layouts and bidirectional positioning without computing positions and preventing overlap manually.</p><p>We've demonstrated this for a bar chart. We've also created HTML, CSS and SVG hybrids where each technology does what it is good at.</p><h2 id="Building-your-next-data-visualizations">Building your next data visualizations</h2><p>At 9elements, we have been visualizing data for our clients for more than 10 years. In 2013, we developed <a href="https://9elements.com/blog/ged-viz-an-html5-data-visualization-tool/">GED VIZ</a> for the Bertelsmann Foundation, visualizing global economic relations. From 2014 on, we developed the front-end and the chart rendering of the <a href="https://9elements.com/blog/new-project-oecd-data-portal/">OECD Data Portal</a>. In 2015, we contributed to the <a href="https://9elements.com/blog/project-launched-wef-inclusive-growth-report-2015/">World Economic Forum Inclusive Growth Report</a>. The bar charts described in this article are part of a long-term work for an international organization in the public health sector.</p><p>Let us discuss how we can help you to explore, present and visualize the data of your organization or business! <a href="https://9elements.com/contact/">Contact us</a>.</p><h2 id="Acknowledgements">Acknowledgements</h2><p>Thanks to my colleagues <a href="https://9elements.com/blog/author/nils-binder/">Nils Binder</a>, <a href="https://9elements.com/blog/author/julian-laubstein/">Julian Laubstein</a> and Matthias von Schmettow for this collaboration.</p><p>Thanks to <a href="https://vesa.piittinen.name/">Vesa Piittinen</a> for substantial feedback and many valuable ideas on how to improve and simplify the HTML and CSS. Please have a look at <a href="https://codepen.io/Merri/pen/RwzwbdV">Vesa's version of the bar chart</a> which demonstrates more clever optimizations.</p><p>Thanks to the data visualization designers <a href="https://www.alicethudt.de/">Alice Thudt</a>, <a href="https://christianlaesser.com/">Christian Laesser</a> and <a href="https://truth-and-beauty.net/">Moritz Stefaner</a> for their stellar work on the <a href="https://truth-and-beauty.net/projects/who">Data Design Language</a>.</p><p>Thanks to the <a href="https://www.devographics.com/">Devographics</a> team behind the ‚ÄúState of HTML/CSS/JS‚Äú surveys for the inspiration.</p><p>Thanks to our client for the opportunity to work on ambitious data visualizations.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Goldman Sachs: AI Is overhyped, expensive, and unreliable (129 pts)]]></title>
            <link>https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/</link>
            <guid>40948971</guid>
            <pubDate>Fri, 12 Jul 2024 20:12:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/">https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/</a>, See on <a href="https://news.ycombinator.com/item?id=40948971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Investment giant Goldman Sachs published a research paper about the economic viability of generative AI which notes that there is ‚Äúlittle to show for‚Äù the huge amount of spending on generative AI infrastructure and questions ‚Äúwhether this large spend will ever pay off in terms of AI benefits and returns.‚Äù&nbsp;</p><p>The paper, called ‚Äú<a href="https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf?ref=404media.co"><u>Gen AI: too much spend, too little benefit?</u></a>‚Äù is based on a series of interviews with Goldman Sachs economists and researchers, MIT professor Daron Acemoglu, and infrastructure experts. The paper ultimately questions whether generative AI will ever become the transformative technology that Silicon Valley and large portions of the stock market are currently betting on, but says investors may continue to get rich anyway. ‚ÄúDespite these concerns and constraints, we still see room for the AI theme to run, either because AI starts to deliver on its promise, or because bubbles take a long time to burst,‚Äù the paper notes.&nbsp;</p><p>Goldman Sachs researchers also say that AI optimism is driving large growth in stocks like Nvidia and other S&amp;P 500 companies (the largest companies in the stock market), but say that the stock price gains we‚Äôve seen are based on the assumption that generative AI is going to lead to higher productivity (which necessarily means automation, layoffs, lower labor costs, and higher efficiency). These stock gains are already baked in, Goldman Sachs argues in the paper: ‚ÄúAlthough the productivity pick-up that AI promises could benefit equities via higher profit growth, we find that stocks often anticipate higher productivity growth before it materializes, raising the risk of overpaying. And using our new long-term return forecasting framework, we find that a very favorable AI scenario may be required for the S&amp;P 500 to deliver above-average returns in the coming decade.‚Äù&nbsp;(Ed Zitron also has a <a href="https://www.wheresyoured.at/pop-culture/?ref=404media.co" rel="noreferrer">thorough writeup of the Goldman Sachs report</a> over at Where's Your Ed At.)</p><p>It adds that ‚Äúoutside of the most bullish AI scenario that includes a material improvement to the structural growth/inflation mix and peak US corporate profitability, we forecast that S&amp;P 500 returns would be below their post-1950 average. AI‚Äôs impact on corporate profitability will matter critically.‚Äù</p><blockquote>"Despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks"</blockquote><p>What this means in plain English is that one of the largest financial institutions in the world is seeing what people who are paying attention are seeing with their eyes: Companies are acting like generative AI is going to change the world and are acting as such, while the reality is that this is a technology that is currently deeply unreliable and may not change much of anything at all. Meanwhile, their stock prices are skyrocketing based on all of this hype and investment, which may not ultimately change much of anything at all.</p><p>Acemoglu, the MIT professor, told Goldman that the industry is banking on the idea that largely scaling the amount of AI training data‚Äîwhich may not actually be possible given the massive amount of training data already ingested‚Äîis going to solve some of generative AI‚Äôs growing pains and problems. But there is no evidence that this will actually be the case: ‚ÄúWhat does a doubling of data really mean, and what can it achieve? Including twice as much data from Reddit into the next version of GPT may improve its ability to predict the next word when engaging in an informal conversation, but it won't necessarily improve a customer service representative‚Äôs ability to help a customer troubleshoot problems with their video service,‚Äù he said. ‚ÄúThe quality of the data also matters, and it‚Äôs not clear where more high-quality data will come from and whether it will be easily and cheaply available to AI models.‚Äù He also posits that large language models themselves ‚Äúmay have limitations‚Äù and that the current architecture of today‚Äôs AI products may not get measurably better.&nbsp;</p><p>Jim Covello, who is Goldman Sachs‚Äô head of global equity research, meanwhile, said that he is skeptical about both the cost of generative AI and its ‚Äúultimate transformative potential.‚Äù&nbsp;</p><p>‚ÄúAI technology is exceptionally expensive, and to justify those costs, the technology must be able to solve complex problems, which it isn‚Äôt designed to do,‚Äù he said. ‚ÄúPeople generally substantially overestimate what the technology is capable of today. In our experience, even basic summarization tasks often yield illegible and nonsensical results. This is not a matter of just some tweaks being required here and there; despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks.‚Äù He added that Goldman Sachs has tested AI to ‚Äúupdate historical data in our company models more quickly than doing so manually, but at six times the cost.‚Äù&nbsp;</p><p>Covello then likens the ‚ÄúAI arms race‚Äù to ‚Äúvirtual reality, the metaverse, and blockchain,‚Äù which are ‚Äúexamples of technologies that saw substantial spend but have few‚Äîif any‚Äîreal world applications today.‚Äù&nbsp;</p><p>The Goldman Sachs report comes on the heels of a piece by David Cahn, partner at the venture capital firm Sequoia Capital, which is one of the largest investors in generative AI startups, titled ‚Äú<a href="https://www.sequoiacap.com/article/ais-600b-question/?ref=404media.co"><u>AI‚Äôs $600 Billion Question</u></a>,‚Äù which attempts to analyze how much revenue the AI industry as a whole needs to make in order to simply pay for the processing power and infrastructure costs being spent on AI right now.&nbsp;</p><p>To break even on what they‚Äôre spending on AI compute infrastructure, companies need to vastly scale their revenue, which Sequoia argues is not currently happening anywhere near the scale these companies need to break even. OpenAI‚Äôs annualized revenue has doubled from $1.6 billion in late 2023 to $3.4 billion, but Sequoia‚Äôs Cahn asks in his piece: ‚ÄúOutside of ChatGPT, how many AI products are consumers really using today? Consider how much value you get from Netflix for $15.49/month or Spotify for $11.99. Long term, AI companies will need to deliver significant value for consumers to continue opening their wallets.‚Äù</p><p>This is all to say that journalists, artists, workers, and even people who <em>use</em> generative AI are not the only ones who are skeptical about the transformative potential of it. The very financial institutions that have funded and invested in the AI frenzy, and are responsible for billions of dollars in investment decisions are starting to wonder what this is all for. </p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->

                    <div>
    <div>
      <p>About the author</p>
      <p>Jason is a cofounder of 404 Media. He was previously the editor-in-chief of Motherboard. He loves the Freedom of Information Act and surfing.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-jason-01-copy.jpeg" alt="Jason Koebler" src="https://www.404media.co/content/images/2023/08/404-jason-01-copy.jpeg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free-threaded CPython is ready to experiment with (438 pts)]]></title>
            <link>https://labs.quansight.org/blog/free-threaded-python-rollout</link>
            <guid>40948806</guid>
            <pubDate>Fri, 12 Jul 2024 19:52:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.quansight.org/blog/free-threaded-python-rollout">https://labs.quansight.org/blog/free-threaded-python-rollout</a>, See on <a href="https://news.ycombinator.com/item?id=40948806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>First, a few announcements:</p>
<p>Yesterday, <a href="https://py-free-threading.github.io/">py-free-threading.github.io</a> launched!
It's both a resource with documentation around adding support for free-threaded
Python, and a status tracker for the rollout across open source projects in the
Python ecosystem. We hope and expect both of these to be very useful, with the
status tracker providing a one-stop-shop to check the support status of the
dependencies of your project (e.g., "what was the first release of a package on
PyPI to support free-threaded Python?" or "are there nightly wheels and where
can I find them?") and get an overview of ecosystem-wide progress:</p>
<p><img alt="Tracking website for package compatibility with free-threaded CPython." src="https://labs.quansight.org/posts/free-threaded-python-rollout/py_free_threading_tracker.png" width="60%"></p>
<p>Later today, the Birds-of-a-Feather session
<a href="https://cfp.scipy.org/2024/talk/HDR7WZ/">"Supporting free-threaded Python"</a>
will be held at the SciPy 2024 conference (co-organized by one of our team
members, Nathan Goldbaum, together with Madicken Munck), focusing on knowledge
and experience sharing.</p>
<h2 id="free-threaded-cpython---what-why-how">Free-threaded CPython - what, why, how?</h2>
<p>You may be wondering by now what "free threading" or "free-threaded CPython"
is, and why you should care. In summary: it is a major change to CPython that
allows running multiple threads in parallel within the same interpreter. It is
becoming available as an experimental feature in CPython 3.13. A free-threaded
interpreter can run with the global interpreter lock (GIL) disabled - a
capability that is finally arriving as a result of the efforts that went into
<a href="https://peps.python.org/pep-0703/">PEP 703 - Making the Global Interpreter Lock Optional in CPython</a>.</p>
<p>Why? Performance. Multi-threaded performance. It makes it significantly easier
to write code that efficiently runs in parallel and will utilize multiple CPU
cores effectively. The core counts in modern CPUs continue to grow, while clock
speeds do not grow, so multi-threaded performance will continue to grow in
importance.</p>
<p>How? It's now <a href="https://py-free-threading.github.io/installing_cpython/">easy to get started by installing a free-threaded interpreter</a>:
macOS/Linux/Windows &amp; python.org/pyenv/apt/yum/conda - your preferred option is
probably available now.</p>
<h2 id="sounds-awesome---whats-the-catch">Sounds awesome - what's the catch?</h2>
<p>Implementing free-threading in CPython itself is a massive effort already, and
worthy of its own (series of) blog post(s). For the wider ecosystem, there's
also a ton of work involved, mainly due to two problems:</p>
<ol>
<li>Thread-safety. While pure Python code should work unchanged, code written in
other languages or using the CPython C API may not. The GIL was implicitly
protecting a lot of thread-unsafe C, C++, Cython, Fortran, etc. code - and
now it no longer does. Which may lead to all sorts of fun outcomes (crashes,
intermittent incorrect behavior, etc.).</li>
<li>ABI incompatibility between the default and free-threaded CPython builds.
The result of a free-threaded interpreter having a different ABI is that
each package that has extension modules must now build extra wheels.</li>
</ol>
<p>Out of these two, the thread-safety one is the more hairy problem. Having to
implement and maintain extra wheel build jobs is not ideal, but the work itself
is well-understood - it just needs doing for each project with extension
modules. Thread-safety on the other hand is harder to understand, improve,
and even test reliably. Because multithreaded code is usually sensitive to the
timing of how multiple threads run and access shared state, bugs may manifest
rarely. And a crash or failure that is hard to reproduce locally is
harder to fix then one that is always reproducible.</p>
<p>Here are a couple of examples of such intermittent failures:</p>
<p><a href="https://github.com/numpy/numpy/issues/26690">numpy#26690</a> shows an example
where a simple call to the <code>.sum()</code> method of a numpy array fails with a
fairly mysterious</p>
<div data-ch-theme="solarized-dark"><p><code><br><div><p><span>RuntimeError: Identity cache already includes the item.</span></p></div><br></code></p></div>
<p>when used with the Python <code>threading</code> and <code>queue</code> modules. This was noticed
in a scikit-learn CI job - it never failed in NumPy's own CI (scikit-learn has
more tests involving parallelism). After the bug report with a reproducer was
submitted, the fix to a numpy-internal cache wasn't that hard.</p>
<p><a href="https://github.com/PyWavelets/pywt/issues/758">pywavelets#758</a> was a report
of another fairly obscure failure in a test using <code>concurrent.futures</code>:</p>
<div data-ch-theme="solarized-dark"><p><code><br><div><p><span>TypeError: descriptor '__enter__' for '_thread.RLock' objects doesn't apply to a '_thread.lock' object</span></p></div><br></code></p></div>
<p>That looked a lot like a problem in CPython, and after some investigating it
was found there as well <a href="https://github.com/python/cpython/issues/121368">cpython#121368</a>
and fixed fairly quickly (the fix required some deep expertise in both CPython
internals and multithreaded programming in C though).</p>
<p>There are a fair amount of examples like that, e.g. <a href="https://github.com/PyWavelets/pywt/pull/753#issuecomment-2190335170">undefined behavior in
Cython code</a> that
no longer worked due to changes in CPython 3.13, a <a href="https://github.com/scipy/scipy/issues/21142">crash from C code in
<code>scipy.signal</code></a> that hadn't been
touched for 24 years (it was always buggy, but the GIL offered enough
protection), and a <a href="https://github.com/hugovk/Pillow/pull/123">crash in Pillow</a>
due to <a href="https://github.com/python/cpython/issues/121403">Python C API usage that wasn't
supported</a>.</p>
<p>It's encouraging though that issues like the ones above do get understood and
resolved fairly quickly. With a good test strategy, and over time also test
suites of libraries that cover Python-level threading better (such tests are
largely non-existent now in most packages), detecting or guarding against
thread-safety issues does seem doable. That test strategy will have to be
multi-pronged: from writing new tests and running tests in loops with <code>pytest-repeat</code> &amp;
co., to getting <a href="https://clang.llvm.org/docs/ThreadSanitizer.html">ThreadSanitizer</a>
to work in CI and doing integration-level and real-world testing with users.</p>
<h2 id="the-road-ahead--what-our-team-will-be-working-on">The road ahead &amp; what our team will be working on</h2>
<p>Free-threaded CPython becoming the default, and eventually the only, build of
CPython is several years away. What we're hoping to see, and help accomplish,
is that for Python 3.13 many projects will work on compatibility and start
releasing <code>cp313t</code> wheels on PyPI (and possibly nightly builds too, for projects
with a lot of dependencies), so users and packages further downstream can start
experimenting as well. After a full year of maturing support in the ecosystem
and further improvements in performance in CPython itself, we should have a
good picture of both the benefits and the remaining challenges with robustness.</p>
<p>Our team (currently <a href="https://github.com/ngoldbaum">Nathan</a>,
<a href="https://github.com/Fidget-Spinner">Ken Jin</a>,
<a href="https://github.com/lysnikolaou/">Lysandros</a>,
<a href="https://github.com/andfoy/">Edgar</a>, and
<a href="https://github.com/rgommers/">myself</a>) has now been working on this topic for
a few months, starting at the bottom of the PyData stack (most effort so far
has gone to NumPy, Cython, and CPython), and slowly working our way up from
there.</p>
<p>For each package, the approach has been similar so far - and a lot of that can
be used as a template by others we think. The steps are roughly:</p>
<ol>
<li>Add a first CI job, usually Linux x86-64 with the latest Python 3.13
pre-release candidate, and ensure the test suite passes,</li>
<li>Based on knowledge from maintainers, fix known issues with thread-safety and
shared/global state in native code,</li>
<li>Add free-threaded support to the wheel build CI jobs, and start uploading
nightly wheels (if appropriate for the project),</li>
<li>Do some stress testing locally and monitor CI jobs, and fix failures that
are observed (take the opportunity to add regression tests using <code>threading</code>
or <code>concurrent.futures.ThreadPoolExecutor</code>)</li>
<li>Mark extension modules as supporting running without the GIL</li>
<li>Move on to a next package (e.g., a key dependency) and using its test suite
to exercise the first package more, circling back to fix issues or address
follow-up actions as needed.</li>
</ol>
<p>Our main takeaway so far: it's challenging, but tractable! And fun as well:)</p>
<p>We've only just scratched the surface, there'll be a lot to do - from key
complex packages like PyO3 (important for projects using Rust) and
PyTorch, to the sheer volume of smaller packages with extension modules.
The lessons we are learning, as far as they are reusable, are going into the
documentation at
<a href="https://py-free-threading.github.io/">py-free-threading.github.io</a>.
The <a href="https://github.com/Quansight-Labs/free-threaded-compatibility/">repository</a>
that contains the sources for that website also has an issue tracker that is used
to link to the relevant project-specific tracking issues for free-threaded support,
as well as for ecosystem-wide issues and tasks (contributions and ideas are
very welcome here!).</p>
<p>Furthermore, we'd like to spend time on whatever may be impactful in helping
the ecosystem adopt free-threaded CPython, from answering questions
to helping with debugging - please don't hesitate to reach out or ping one of
us directly on GitHub!</p>
<h2 id="conclusion--acknowledgements">Conclusion &amp; acknowledgements</h2>
<p>We're really excited about what is becoming possible with free-threaded CPython!
While our team is busy with implementing CI jobs and fixing thread-safety issues,
we are as curious as anyone to see what performance improvements and
interesting experiments are going to show up with real-world code soon.</p>
<p>It's hard to acknowledge and thank everyone involved in moving free-threaded
CPython forward, because so much activity is happening. First of all we have
to thank Meta for funding the efforts of our team to help the ecosystem adopt
free-threaded CPython at the pace that will be needed to make this whole
endeavour a success, and Sam Gross and the whole Python Runtime team at Meta
for the close collaboration. Then the list is long - from the Python Steering
Council, for its thoughtful approach to (and acceptance of) PEP 703, to the
many library maintainers and community members who are proactively adding
support to their own projects or guide and review our contributions whenever we
work on projects we are not ourselves maintainers of.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common Expression Language interpreter written in Rust (111 pts)]]></title>
            <link>https://github.com/clarkmcc/cel-rust</link>
            <guid>40948566</guid>
            <pubDate>Fri, 12 Jul 2024 19:19:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/clarkmcc/cel-rust">https://github.com/clarkmcc/cel-rust</a>, See on <a href="https://news.ycombinator.com/item?id=40948566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Common Expression Language (Rust)</h2><a id="user-content-common-expression-language-rust" aria-label="Permalink: Common Expression Language (Rust)" href="#common-expression-language-rust"></a></p>
<p dir="auto"><a href="https://github.com/clarkmcc/cel-rust/actions/workflows/rust.yml"><img src="https://github.com/clarkmcc/cel-rust/actions/workflows/rust.yml/badge.svg" alt="Rust"></a></p>
<p dir="auto">The <a href="https://github.com/google/cel-spec">Common Expression Language (CEL)</a> is a non-Turing complete language designed
for simplicity, speed, safety, and
portability. CEL's C-like syntax looks nearly identical to equivalent expressions in C++, Go, Java, and TypeScript. CEL
is ideal for lightweight expression evaluation when a fully sandboxed scripting language is too resource intensive.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Check whether a resource name starts with a group name.
resource.name.startsWith(&quot;/groups/&quot; + auth.claims.group)"><pre><span>// Check whether a resource name starts with a group name.</span>
<span>resource</span>.<span>name</span>.<span>startsWith</span>(<span>"/groups/"</span> + <span>auth</span>.<span>claims</span>.<span>group</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="// Determine whether the request is in the permitted time window.
request.time - resource.age < duration(&quot;24h&quot;)"><pre><span>// Determine whether the request is in the permitted time window.</span>
<span>request</span>.<span>time</span> <span>-</span> <span>resource</span>.<span>age</span> <span>&lt;</span> <span>duration</span>(<span>"24h"</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="// Check whether all resource names in a list match a given filter.
auth.claims.email_verified &amp;&amp; resources.all(r, r.startsWith(auth.claims.email))"><pre><span>// Check whether all resource names in a list match a given filter.</span>
<span>auth</span><span>.</span><span>claims</span><span>.</span><span>email_verified</span> <span>&amp;&amp;</span> <span>resources</span><span>.</span><span>all</span><span>(</span><span>r</span><span>,</span> <span>r</span><span>.</span><span>startsWith</span><span>(</span><span>auth</span><span>.</span><span>claims</span><span>.</span><span>email</span><span>)</span><span>)</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">This project includes a CEL-parser and an interpreter which means that it can be used to evaluate CEL-expressions. The
library aims to be very simple to use, while still being fast, safe, and customizable.</p>
<div dir="auto" data-snippet-clipboard-copy-content="fn main() {
    let program = Program::compile(&quot;add(2, 3) == 5&quot;).unwrap();
    let mut context = Context::default();
    context.add_function(&quot;add&quot;, |a: i64, b: i64| a + b);
    let value = program.execute(&amp;context).unwrap();
    assert_eq!(value, true.into());
}"><pre><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> program = <span>Program</span><span>::</span><span>compile</span><span>(</span><span>"add(2, 3) == 5"</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
    <span>let</span> <span>mut</span> context = <span>Context</span><span>::</span><span>default</span><span>(</span><span>)</span><span>;</span>
    context<span>.</span><span>add_function</span><span>(</span><span>"add"</span><span>,</span> |<span>a</span><span>:</span> <span>i64</span><span>,</span> <span>b</span><span>:</span> <span>i64</span>| a + b<span>)</span><span>;</span>
    <span>let</span> value = program<span>.</span><span>execute</span><span>(</span><span>&amp;</span>context<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
    <span>assert_eq</span><span>!</span><span>(</span>value, <span>true</span>.into<span>(</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Check out these other examples to learn how to use this library:</p>
<ul dir="auto">
<li><a href="https://github.com/clarkmcc/cel-rust/blob/master/example/src/simple.rs">Simple</a> - A simple example of how to use the library.</li>
<li><a href="https://github.com/clarkmcc/cel-rust/blob/master/example/src/variables.rs">Variables</a> - Passing variables and using them in your program.</li>
<li><a href="https://github.com/clarkmcc/cel-rust/blob/master/example/src/functions.rs">Functions</a> - Defining and using custom functions in your program.</li>
<li><a href="https://github.com/clarkmcc/cel-rust/blob/master/example/src/threads.rs">Concurrent Execution</a> - Executing the same program concurrently.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating the Compiler (176 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2024-07-12-interpreter/</link>
            <guid>40948353</guid>
            <pubDate>Fri, 12 Jul 2024 18:54:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2024-07-12-interpreter/">https://www.mattkeeter.com/blog/2024-07-12-interpreter/</a>, See on <a href="https://news.ycombinator.com/item?id=40948353">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>In modern times, everyone knows that writing assembly is a fool's errand:
compilers are the result of literal engineer-centuries of work, and they know
the processor much better than you do.</p>
<p>And yet ‚Äì one hears <em>rumors</em>.</p>
<p>Written in <a href="https://jilp.org/vol5/v5paper12.pdf">ancient tomes</a>,
muttered in <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">quiet watering holes</a>,
scrawled on the walls of
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">bygone temples</a>,
hinted at by
<a href="https://llvm.org/docs/LangRef.html#calling-conventions">mysterious texts</a>;
the rumors paint a specific picture:</p>
<blockquote>
<p>Compilers are bad at generating code for interpreters, and it's possible to
outperform them by writing your interpreter in assembly.</p>
</blockquote>
<p>I recently <a href="https://www.mattkeeter.com/projects/raven">wrote a fast interpreter</a> for the
<a href="https://wiki.xxiivv.com/site/uxn.html">Uxn CPU</a>,
a stack-based architecture with 256 opcodes.  The interpreter is a simple loop
which reads a byte from RAM then selects the appropriate instruction:</p>
<pre><code>impl Uxn {
    /// Runs the VM starting at the given address until it terminates
    #[inline]
    pub fn run&lt;D: Device&gt;(&amp;mut self, dev: &amp;mut D, mut pc: u16) {
        loop {
            let op = self.ram[usize::from(pc)];
            pc = pc.wrapping_add(1);
            let Some(next) = self.op(op, dev, pc) else {
                break;
            };
            pc = next;
        }
    }

    /// Executes a single operation
    #[inline]
    fn op&lt;D: Device&gt;(&amp;mut self, op: u8, dev: &amp;mut D, pc: u16) -&gt; Option&lt;u16&gt; {
        match op {
            0x00 =&gt; op::brk(self, dev, pc),
            0x01 =&gt; op::inc::&lt;0b000&gt;(self, dev, pc),
            0x02 =&gt; op::pop::&lt;0b000&gt;(self, dev, pc),
            0x03 =&gt; op::nip::&lt;0b000&gt;(self, dev, pc),
            0x04 =&gt; op::swp::&lt;0b000&gt;(self, dev, pc),
            0x05 =&gt; op::rot::&lt;0b000&gt;(self, dev, pc),
            0x06 =&gt; op::dup::&lt;0b000&gt;(self, dev, pc),
            0x07 =&gt; op::ovr::&lt;0b000&gt;(self, dev, pc),
            0x08 =&gt; op::equ::&lt;0b000&gt;(self, dev, pc),
            0x09 =&gt; op::neq::&lt;0b000&gt;(self, dev, pc),
            0x0a =&gt; op::gth::&lt;0b000&gt;(self, dev, pc),
            0x0b =&gt; op::lth::&lt;0b000&gt;(self, dev, pc),
            0x0c =&gt; op::jmp::&lt;0b000&gt;(self, dev, pc),
            0x0d =&gt; op::jcn::&lt;0b000&gt;(self, dev, pc),
            0x0e =&gt; op::jsr::&lt;0b000&gt;(self, dev, pc),
            // ... etc
        }
    }
}
</code></pre>
<p>All of the opcode implementations end up monomorphized and inlined into the body
of <code>Uxn::run(..)</code>, and the compiler is smart enough to keep key values in
registers.  This makes it relatively fast; I see 10-20% speedup over the
<a href="https://git.sr.ht/%7Erabbits/uxn/">reference implementation</a>.</p>
<p>Let's look at the assembly and see what the compiler is doing ‚Äì and whether we
can do any better.  For context, the Uxn CPU has four different memories:</p>
<ul>
<li>The data stack, which is a <code>[u8; 256]</code> along with a <code>u8</code> index</li>
<li>The return stack, which has the same format</li>
<li>RAM, which is a <code>[u8; 65535]</code></li>
<li>Device memory, which we'll ignore for the moment (along with the <code>D: Device</code>
argument)</li>
</ul>
<p>During evaluation, we also track the program counter <code>pc</code>, which is a <code>u16</code> used
to index into the RAM.  In each cycle, we load a byte from RAM, then call the
appropriate opcode.  Some opcodes can also read and write to RAM, so
self-modifying code is possible!</p>
<p>By examining the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">assembly</a>, we can
reverse-engineer which values are stored where.  Consider the <code>INC</code> operation,
which loads a value from the top of the data stack and increments it:</p>
<pre><code>; INC
0x100002d4c: ldrb w8, [x25]     ; read the current data stack index
0x100002d50: ldrb w9, [x24, x8] ; read a byte from the data stack
0x100002d54: add w9, w9, #1     ; increment that byte
0x100002d58: strb w9, [x24, x8] ; write that byte back to the stack
0x100002d5c: b 0x100002d1c      ; jump back to the dispatch loop
</code></pre>
<p>From this assembly, we learn the following:</p>
<ul>
<li><code>x25</code> is the <em>address</em> of the data stack index (not its value!)</li>
<li><code>x24</code> is the address of the data stack array</li>
<li><code>w9</code> is used as a temporary register</li>
</ul>
<p>Similarly, <code>INCr</code> ‚Äì increment the top value in the <strong>return</strong> stack ‚Äì teaches us
that <code>x22</code> and <code>x23</code> are the return stack's data and index addresses.</p>
<p><code>JMP</code> shows that our program counter is stored in <code>w27</code>:</p>
<pre><code>; JMP
0x100002eac: ldrb w8, [x25]      ; read the current data stack index
0x100002eb0: ldrsb w9, [x24, x8] ; read a signed jump offset from the data stack
0x100002eb4: sub w8, w8, #1      ; decrement the data stack index
0x100002eb8: strb w8, [x25]      ; write back the data stack index
0x100002ebc: add w27, w27, w9    ; apply the jump to our program counter
0x100002ec0: b 0x100002d1c       ; jump back to the dispatch loop
</code></pre>
<p>Finally, the dispatch loop itself is worth examining:</p>
<pre><code>0x100002d1c: and x10, x27, #0xffff          ; mask pc to a u16
0x100002d20: ldr x8, [x20, #256]            ; load RAM base from *mut Uxn
0x100002d24: ldrb w10, [x8, x10]            ; load opcode byte from RAM
0x100002d28: add w27, w27, #1               ; increment pc
0x100002d2c: adr x11, #-96                  ; load base for jump
0x100002d30: ldrh w12, [x27, x10, lsl  #1]  ; load per-opcode jump amount
0x100002d34: add x11, x11, x12, lsl #2      ; compute jump location
0x100002d38: br x11                         ; jump into opcode implementation
</code></pre>
<p>The compiler has generated a jump table of 256 offsets (each a 2-byte value,
indicated by <code>lsl #1</code>).  It reads an opcode-specific value from this table to
compute a jump target, then performs an indirect branch to jump into the
opcode's implementation.</p>
<p>We can run this in a debugger and dump the actual jump table:</p>
<pre><code>(lldb) disas -p -c3
raven-cli`raven_uxn::Uxn::run::had9dba0d7d1b5105:
-&gt;  0x100002d30 &lt;+236&gt;: ldrh   w12, [x27, x10, lsl  #1]
    0x100002d34 &lt;+240&gt;: add    x11, x11, x12, lsl #2
    0x100002d38 &lt;+244&gt;: br     x11
(lldb) reg read x27
     x27 = 0x0000000100170b10
(lldb) memory read -s2 -fu -c256 0x0000000100170b10
0x100170b10: 2923
0x100170b12: 31
0x100170b14: 36
0x100170b16: 40
0x100170b18: 44
0x100170b1a: 52
0x100170b1c: 28
0x100170b1e: 64
0x100170b20: 70
0x100170b22: 78
0x100170b24: 86
0x100170b26: 94
0x100170b28: 119
0x100170b2a: 102
0x100170b2c: 110
0x100170b2e: 125
; etc...
</code></pre>
<p>(indeed, this is how I generated the <a href="https://www.mattkeeter.com/projects/raven/disassembly.html">per-opcode instruction listing</a>)</p>
<hr>
<p>Having looked at the assembly, there are two things that stick out as possible
inefficiencies:</p>
<ul>
<li>Some critical values (stack indices, the base address of RAM) are kept in
memory instead of registers; for example, <code>INC</code> has an extra load operation to
get the current data stack index.</li>
<li>The dispatch loop takes a single indirect branch to the opcode-specific
implementation.  This means that the branch will be nigh unpredictable!</li>
</ul>
<p>Profiling the code, the hottest instructions are all in the dispatch loop; the
<code>ldrh</code> takes over 1/3 of the total runtime!</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/profile.png" alt="screenshot of profiling info"></p>
<p>(I'm not confident that the profiler is attributing the time to the correct
specific instruction here, but the vibes definitely indicate that dispatch is
expensive)</p>
<hr>
<p><a href="http://luajit.org/">LuaJIT</a> is the fast interpreter <em>par excellence</em>, and it's
written in assembly.  Mike Pall
<a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">specifically calls out</a>
keeping state in registers and indirect threading as two contributors to its
speed, which can only be
<a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">accomplished reliably</a>
in assembly.</p>
<p>Since persuading our compiler to generate extremely specific patterns is hard,
let's get started writing some assembly of our own.  My home machine is an M1
Macbook, so all of the assembly will be AArch64-flavored. The implementation
uses general-purpose registers; be aware that <code>w*</code> and <code>x*</code> refer to 32-bit and
64-bit views of the same register.</p>
<hr>
<h3>Register assignment</h3>
<p>Our first optimization is to store <strong>all</strong> important data in registers, to avoid
superfluous loads and stores.  My implementation ends up using 9 registers
(<code>x0-x8</code>), along with a handful of scratch registers:</p>
<pre><code>; x0 - stack pointer (&amp;mut [u8; 256])
; x1 - stack index (u8)
; x2 - return stack pointer (&amp;mut [u8; 256])
; x3 - return stack index (u8)
; x4 - RAM pointer (&amp;mut [u8; 65536])
; x5 - program counter (u16), offset of the next value in RAM
; x6 - VM pointer (&amp;mut Uxn)
; x7 - Device handle pointer (&amp;DeviceHandle)
; x8 - Jump table pointer
; x9-15 - scratch registers
</code></pre>
<p>The <a href="https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)">AArch64 calling convention</a>
only gives you 8 input arguments, so we can't call a function directly with all
of these values in registers;
we'll need a C ABI-flavored entry point (<a href="#shims">discussed below</a>).</p>
<h3>Indirect threading</h3>
<p>Our second optimization is using
<a href="https://en.wikipedia.org/wiki/Threaded_code">threaded code</a>
to eliminate the dispatch loop.
Each opcode's implementation will end with a jump to the next opcode's
implementation.</p>
<p>Opcodes are stored as single bytes in VM RAM, with a base address of <code>x4</code>.  I'll
build a separate jump table of function pointers, then pass its address in
register <code>x8</code>.  On the Rust side, here's what that table looks like:</p>
<pre><code>extern "C" {
    fn BRK();
    fn INC();
    fn POP();
    fn NIP();
    fn SWP();
    fn ROT();
    fn DUP();
    fn OVR();
    fn EQU();
    // ...etc
}

const JUMP_TABLE: [unsafe extern "C" fn(); 256] = [
    (BRK as unsafe extern "C" fn()),
    (INC as unsafe extern "C" fn()),
    (POP as unsafe extern "C" fn()),
    (NIP as unsafe extern "C" fn()),
    (SWP as unsafe extern "C" fn()),
    (ROT as unsafe extern "C" fn()),
    (DUP as unsafe extern "C" fn()),
    (OVR as unsafe extern "C" fn()),
    (EQU as unsafe extern "C" fn()),
    (NEQ as unsafe extern "C" fn()),
    // ... etc
];
</code></pre>
<p>In assembly, we want to read the current byte from VM RAM (<code>x4</code>), use it to pick
an address in the jump table (<code>x8</code>), then jump to that address.  I defined a
macro to do this dispatch:</p>
<pre><code>.macro next
    ldrb w9, [x4, x5]          ; load the byte from RAM
    add x5, x5, #1             ; increment the program counter
    and x5, x5, #0xffff        ; wrap the program counter
    ldr x10, [x8, x9, lsl #3]  ; load the opcode implementation address
    br x10                     ; jump to the opcode's implementation
.endm
</code></pre>
<p>Notice that this is a <strong>macro</strong>, not a function; we'll add <code>next</code> to the end of
each opcode, which will expand into this text.</p>
<p>For example, here's <code>INC</code>:</p>
<pre><code>.global _INC
_INC:
    ldrb w9, [x0, x1]   ; read the byte from the top of the stack
    add w9, w9, #1      ; increment it
    strb w9, [x0, x1]   ; write it back
    next                ; jump to the next opcode
</code></pre>
<p>Unlike LuaJIT, there's no <strong>decoding</strong> step for instructions; there are no
register arguments, and the single-byte opcode uniquely defines program
behavior.</p>
<h3>Implementation</h3>
<p>Implementing the other 255 opcodes is mostly just turning the crank;
there's nothing particularly exotic here, just good honest assembly.</p>
<p>In many cases, I'll use helper macros to generate code for a group of
instructions:</p>
<pre><code>.macro binary_op op
    ldrb w10, [x0, x1]  ; read the top value from the data stack
    pop                 ; decrement the data stack index (this is a macro!)
    ldrb w11, [x0, x1]  ; read the next value from the data stack
    \op w10, w11, w10   ; do the actual math operation
    strb w10, [x0, x1]  ; write the result into the data stack
    next
.endm

.global _ADD
_ADD:
    binary_op add

.global _SUB
_SUB:
    binary_op sub

.global _MUL
_MUL:
    binary_op mul

.global _DIV
_DIV:
    binary_op udiv
</code></pre>
<p>The whole implementation ends up being about
<a href="https://github.com/mkeeter/raven/blob/main/raven-uxn/src/native/aarch64.s">2400 lines</a>.
It sounds like a lot, but only about half of that is unique:
most opcodes come in two flavors (with and without the <code>RET</code> flag),
which only differ in which stack is used.</p>
<h3>C Shims</h3>
<p>Of course, my whole program isn't hand-written in assembly.  We need a way to
call our assembly function from the rest of our (Rust) implementation.  This
looks like a (Rust) <code>entry</code> function, which calls into an (assembly)
<code>aarch64_entry</code> point (which is compatible with the C ABI):</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/entry.png" alt="diagram showing entry points"></p>
<p>What do we actually pass into <code>aarch64_entry</code>?  We have too much state to pass
in function argument registers (<code>x0-x7</code>), so I defined a helper object which
contains everything we need:</p>
<pre><code>#[repr(C)]
pub(crate) struct EntryHandle {
    stack_data: *mut u8,
    stack_index: *mut u8,
    ret_data: *mut u8,
    ret_index: *mut u8,
    ram: *mut u8,
    vm: *mut core::ffi::c_void,  // *Uxn
    dev: *mut core::ffi::c_void, // *DeviceHandle
}

struct DeviceHandle&lt;'a&gt;(&amp;'a mut dyn Device);
</code></pre>
<p>The <code>DeviceHandle</code> is needed because <code>&amp;mut dyn Device</code> is a fat pointer, and is
therefore not safe to pass into a C function.  Like all computer problems, we
solve this with an extra level of indirection: put the <code>&amp;mut dyn Device</code> into a
<code>DeviceHandle</code>, then pass <em>its</em> address instead.</p>
<p>Calling into assembly is a simple matter of populating an <code>EntryHandle</code> object,
then branching into the danger zone:</p>
<pre><code>// Declaration of our entry point, written in assembly
extern "C" {
    pub fn aarch64_entry(
        h: *const EntryHandle,
        pc: u16,
        table: *const unsafe extern "C" fn(),
    ) -&gt; u16;
}

pub fn entry(vm: &amp;mut Uxn, dev: &amp;mut dyn Device, pc: u16) -&gt; u16 {
    let mut h = DeviceHandle(dev);
    let mut e = EntryHandle {
        stack_data: vm.stack.data.as_mut_ptr(),
        stack_index: &amp;mut vm.stack.index as *mut _,
        ret_data: vm.ret.data.as_mut_ptr(),
        ret_index: &amp;mut vm.ret.index as *mut _,
        ram: (*vm.ram).as_mut_ptr(),
        vm: vm as *mut _ as *mut _,
        dev: &amp;mut h as *mut _ as *mut _,
    };

    // SAFETY: do you trust me?
    unsafe {
        aarch64::aarch64_entry(&amp;mut e as *mut _, pc, JUMP_TABLE.as_ptr())
    }
}
</code></pre>
<p><code>aarch64_entry</code> is a hand-written entry point in the assembly code.  It shuffles
around registers to put everything in the right place for our opcodes, then
begins execution with the usual <code>next</code> macro:</p>
<pre><code>.global _aarch64_entry
_aarch64_entry:
    sub sp, sp, #0x200  ; make room in the stack
    stp   x29, x30, [sp, 0x0]   ; store stack and frame pointer
    mov   x29, sp

    // Unpack from EntryHandle into registers
    mov x5, x1 ; move PC (before overwriting x1)
    mov x8, x2 ; jump table (before overwriting x2)
    ldr x1, [x0, 0x8]  ; stack index pointer
    ldr x2, [x0, 0x10] ; ret data pointer
    ldr x3, [x0, 0x18] ; ret index pointer
    ldr x4, [x0, 0x20] ; RAM pointer
    ldr x6, [x0, 0x28] ; *mut Uxn
    ldr x7, [x0, 0x30] ; *mut DeviceHandle
    ldr x0, [x0, 0x00] ; stack data pointer (overwriting *EntryHandle)

    ; Convert from index pointers to index values in w1 / w3
    stp x1, x3, [sp, 0x10]      ; save stack index pointers
    ldrb w1, [x1]               ; load stack index
    ldrb w3, [x3]               ; load ret index

    ; Jump into the instruction list
    next
</code></pre>
<p>Finally, when exiting (via the <code>BRK</code> opcode), we need to update the data and
return stack indices, moving values from registers into the appropriate memory
addresses:</p>
<pre><code>.global _BRK
_BRK:
    ; Write index values back through index pointers
    ldp x9, x10, [sp, 0x10]     ; restore stack index pointers
    strb w1, [x9]               ; save data stack index
    strb w3, [x10]              ; save return stack index

    ldp   x29, x30, [sp, 0x0]   ; restore stack and frame pointer
    add sp, sp, #0x200          ; undo our stack offset

    mov x0, x5 ; return PC from function
    ret
</code></pre>
<h3>Device IO</h3>
<p>The <code>DEI</code> and <code>DEO</code> opcodes perform "device I/O", which lets you attach
arbitrary peripherals to the system.  The most common set of peripherals is the
<a href="https://wiki.xxiivv.com/site/varvara.html">Varvara system</a>,
which adds everything you need to make the CPU into an actual computer: a
screen, keyboard and mouse input, audio, etc.</p>
<p>To keep the Uxn implementation generic, I defined a trait for a device:</p>
<pre><code>/// Trait for a Uxn-compatible device
pub trait Device {
    /// Performs the `DEI` operation for the given target
    ///
    /// This function must write its output byte to `vm.dev[target]`; the CPU
    /// evaluation loop will then copy this value to the stack.
    fn dei(&amp;mut self, vm: &amp;mut Uxn, target: u8);

    /// Performs the `DEO` operation on the given target
    ///
    /// The input byte will be written to `vm.dev[target]` before this function
    /// is called, and can be read by the function.
    ///
    /// Returns `true` if the CPU should keep running, `false` if it should
    /// exit.
    #[must_use]
    fn deo(&amp;mut self, vm: &amp;mut Uxn, target: u8) -&gt; bool;
}
</code></pre>
<p>The opcode implementation takes a <code>&amp;mut dyn Device</code>, i.e. something implementing
this trait, and calls trait methods on it:</p>
<pre><code>pub fn deo&lt;const FLAGS: u8&gt;(
    vm: &amp;mut Uxn,
    dev: &amp;mut dyn Device,
    pc: u16,
) -&gt; Option&lt;u16&gt; {
    let mut s = vm.stack_view::&lt;FLAGS&gt;();
    let i = s.pop_byte();
    let mut run = true;
    match s.pop() {
        Value::Short(v) =&gt; {
            let [lo, hi] = v.to_le_bytes();
            let j = i.wrapping_add(1);
            vm.dev[usize::from(i)] = hi;
            run &amp;= dev.deo(vm, i);
            vm.dev[usize::from(j)] = lo;
            run &amp;= dev.deo(vm, j);
        }
        Value::Byte(v) =&gt; {
            vm.dev[usize::from(i)] = v;
            run &amp;= dev.deo(vm, i);
        }
    }
    if run {
        Some(pc)
    } else {
        None
    }
}
</code></pre>
<p>However, this function is not compatible with the C ABI ‚Äì it's both generic
<em>and</em> takes a trait object ‚Äì so it can't be called directly from the <code>DEO</code>
opcode in assembly.</p>
<p>To let my opcodes call <code>DEO</code> and <code>DEI</code> functions, I again wrote a bunch of
shims:</p>
<pre><code>#[no_mangle]
extern "C" fn deo_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b000&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern "C" fn deo_2_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b001&gt;(dev.0, 0).is_some()
}

#[no_mangle]
extern "C" fn deo_r_entry(vm: &amp;mut Uxn, dev: &amp;mut DeviceHandle) -&gt; bool {
    vm.deo::&lt;0b010&gt;(dev.0, 0).is_some()
}

// etc, 16 functions in total for all DEI / DEO variants
</code></pre>
<p>The full path of the function looks something like this:</p>
<p><img src="https://www.mattkeeter.com/blog/2024-07-12-interpreter/deo.png" alt="diagram showing deo calls"></p>
<p>On the assembly side, there's one subtlety: during our normal opcode processing,
we keep data and return stack index values in <code>x1</code> and <code>x3</code> (leaving the
original values in the <code>&amp;mut Uxn</code> unchanged).  We have to write those registers
back into the appropriate memory locations in the <code>&amp;mut Uxn</code> <em>before</em> calling a
function that expects those values to be correct.</p>
<p>Here's the assembly code to call into our shim functions:</p>
<pre><code>.global _DEI
_DEI:
    ; We have to write our stack index pointers back into the &amp;mut Uxn
    ldp x11, x12, [sp, 0x10] ; restore stack index pointers
    strb w1, [x11]   ; modify stack index pointer
    strb w3, [x12]   ; modify return stack index pointer

    ; We're using caller-saved registers, so we have to back them up
    stp x0, x1, [sp, #0x20] ; store register state
    stp x2, x3, [sp, #0x30]
    stp x5, x4, [sp, #0x40]
    stp x6, x7, [sp, #0x50]
    str x8,     [sp, #0x60]

    ; set up our arguments, then call the shim function:
    mov x0, x6 ; x0 = Uxn pointer
    mov x1, x7 ; x1 = DeviceHandle pointer
    bl _dei_entry

    ldp x0, x1, [sp, #0x20] ; restore register state
    ldp x2, x3, [sp, #0x30]
    ldp x5, x4, [sp, #0x40]
    ldp x6, x7, [sp, #0x50]
    ldr x8,     [sp, #0x60]

    ; The DEO operation may have changed stack pointers, so reload them here 
    ldp x11, x12, [sp, 0x10]
    ldrb w1, [x11]  ; update stack index pointer
    ldrb w3, [x12]  ; update return stack index pointer
    next
</code></pre>
<h3>Performance</h3>
<p>I used two CPU-heavy workloads to test interpreter performance:</p>
<ul>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/exercises/fib.tal"><code>fib.tal</code></a>,
modified to print the first <strong>35</strong> numbers of the Fibonacci sequence</li>
<li><a href="https://git.sr.ht/%7Erabbits/uxn/tree/main/item/projects/examples/demos/mandelbrot.tal"><code>mandelbrot.tal</code></a>,
with <code>%SCALE</code> set to <code>#0020</code> (rendering a 672 √ó 512 image)</li>
</ul>
<p>Both of these programs do all of their computation at startup, so I added
instrumentation to print time spent in the entry vector (at <code>0x100</code>).</p>
<p>There are four different implementations being tested here:</p>
<ul>
<li>The <a href="https://git.sr.ht/%7Erabbits/uxn/"><code>uxnemu</code></a> reference implementation,
running natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running natively on my laptop</li>
<li>The optimized <code>raven-uxn</code> interpreter (hand-written in assembly), running
natively on my laptop</li>
<li>The baseline <code>raven-uxn</code> interpreter, running in my browser (compiled to
WebAsembly)</li>
</ul>
<p>Here are the performance numbers that you've been waiting for:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>uxnemu</code> (reference)</td><td><code>AArch64</code></td><td>1.57 s</td><td>2.03 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>wasm32</code></td><td>2.54 s</td><td>2.82 s
</td></tr></tbody></table>
<p>There are three clear trends:</p>
<ul>
<li><code>raven-uxn</code>'s baseline interpreter (written in safe Rust) is faster than the
reference implementation; we already knew that from previous work</li>
<li>The assembly implementation is about <strong>30% faster</strong> than the baseline!</li>
<li>WebAssembly encurs a roughly 1.8√ó slowdown compared to the baseline</li>
</ul>
<h3>Ablation testing</h3>
<p>It's not obvious whether the speedup is due to keeping values in registers, or
adding dispatch to the end of each opcode (instead of a central branch).</p>
<p>We can easily test for the latter by changing our <code>next</code> macro:</p>
<pre><code>.macro next
    b next_dispatch
.endm
next_dispatch:
    ldrb w9, [x4, x5]
    add x5, x5, #1
    and x5, x5, #0xffff
    ldr x10, [x8, x9, lsl #3]
    br x10
</code></pre>
<p>Adding these new results to the chart( as "assembly*"), here's what I see:</p>
<table>
    <tbody><tr><th>Interpreter</th><th>Target</th><th>Fibonacci</th><th>Mandelbrot
    </th></tr><tr><td><code>raven-uxn</code> (baseline)</td><td><code>AArch64</code></td><td>1.38 s</td><td>1.56 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly)</td><td><code>AArch64</code></td><td>1.00 s</td><td>1.10 s
    </td></tr><tr><td><code>raven-uxn</code> (assembly*)</td><td><code>AArch64</code></td><td>1.34 s</td><td>1.41 s
</td></tr></tbody></table>
<p>Centralized dispatch is a significant slowdown, and is nearly as slow as the
baseline interpreter!
It just goes to show:
<a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">do not taunt happy fun branch predictor</a>.</p>
<h3>Things that didn't work</h3>
<p>I did a bunch of other experiments, which didn't make things faster:</p>
<ul>
<li>Expanding RAM to store both user bytes and the jump targets (i.e. making RAM a
<code>[u64; 65536]</code>).  The user byte is stored in bits 48-54 of the pointer, since
those are unused, and I added masking + shifting depending on whether we were
using the data or pointer component.  This was noticeably slower, probably
because it's less cache-friendly (512 KiB, rather than 64 KiB + 1 KiB of jump
table)</li>
<li>Making all of the opcode implementations the same size (padding to the size of
the largest opcode implementation with <code>.balign 256</code>), then removing the jump
table entirely.  This was also slower, also probably because of cache
friendliness: the opcode implementations go from 16.6 KiB total to 64 KiB.</li>
</ul>
<h3>Conclusion</h3>
<p>I've proven to my satisfaction that writing an interpreter in assembly is both
fun and performant!</p>
<p>There are strategies to get similar performance in high-level languages:
<a href="https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables">using computed goto</a>
and the <a href="https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md#m3-massey-meta-machine">Massey Meta Machine</a>
are both relevant prior art.</p>
<p>However, neither of these are feasible in Rust; to quote
<a href="https://pliniker.github.io/post/dispatchers/">this excellent writeup</a>.</p>
<blockquote>
<p>At this time there is no portable way to produce computed gotos or tail call
optimization in compiled machine code from Rust.</p>
</blockquote>
<p>On a brighter note, it should be relatively easy to port all of the assembly
code to x86-64, but I'll leave that as a challenge for someone else!</p>
<p>All of the relevant code is <a href="https://github.com/mkeeter/raven">on Github</a>, gated
by the <code>native</code> feature.  The <code>uxn-cli</code> and <code>uxn-gui</code> executables both accept a
<code>--native</code> flag to select the assembly interpreter backend.</p>
<p>Have fun!</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What could explain the gallium anomaly? (192 pts)]]></title>
            <link>https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/</link>
            <guid>40948202</guid>
            <pubDate>Fri, 12 Jul 2024 18:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/">https://www.quantamagazine.org/what-could-explain-the-gallium-anomaly-20240712/</a>, See on <a href="https://news.ycombinator.com/item?id=40948202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><p>Physicists have ruled out a mundane explanation for the strange findings of an old Soviet experiment, leaving open the possibility that the results point to a new fundamental particle.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/GalliumAnomaly-crNicoRoper-Lede-scaled.webp"></p></div><figcaption><div><p>Gallium occasionally converts into germanium, but not as often as expected.</p><p>Nico Roper/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Deep in the Caucasus Mountains, on the border between Russia and Georgia, an unusual experiment is taking place. In an underground lab shielded by a mountain of rock, highly radioactive material sits inside a vat of liquid gallium, blasting out particles called neutrinos that break the gallium down into atoms of germanium.</p>
<p>The goal is to resolve a little-known mystery of physics: the gallium anomaly. ‚ÄúI think it‚Äôs one of the most compelling anomalies in neutrino physics that we have today,‚Äù said <a href="https://www.uta.edu/academics/faculty/profile?username=jonesb">Ben Jones</a>, a neutrino physicist at the University of Texas, Arlington. Some three decades ago, in a previous version of the current experiment, scientists first detected a dearth of the expected germanium atoms that still can‚Äôt be explained.</p>
<p>Since then, physicists have worked to rule out possible mismeasurements or inaccuracies that could explain the anomaly. Now they‚Äôve eliminated another one. <a href="https://nuc.berkeley.edu/people/eric-norman/">Eric Norman</a>, a nuclear physicist at the University of California, Berkeley, and colleagues <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.109.055501#fulltext">have announced</a> that one possible solution, an incorrect calculation of the half-life of germanium, can‚Äôt be the cause.</p>
<p>‚ÄúThe half-life is correct,‚Äù Norman said. ‚ÄúThis is not the explanation for the gallium anomaly.‚Äù</p>
<p>That leaves few possibilities. One is that some still-unknown experimental defect caused the anomaly. Perhaps a different mismeasurement is throwing things off, or a misunderstanding of nuclear physics. Or maybe, just maybe, the anomaly points to a monumental discovery, the existence of a new type of elementary particle called a sterile neutrino. Sterile neutrinos were initially proposed to explain why the masses of the three known neutrinos are so tiny, but they could also account for at least some of the invisible ‚Äúdark matter‚Äù that fills the cosmos.</p>
<p>‚ÄúWe cannot find some huge uncertainty in our experimental procedures,‚Äù said Vladislav Barinov, a particle physicist at the Institute for Nuclear Research of the Russian Academy of Sciences who works on the experiment in the Caucasus. ‚ÄúIs it a new type of neutrino? We don‚Äôt know.‚Äù</p>
<h2><strong>Neutrino Village</strong></h2>
<p>At the height of the Cold War, before the fall of the Berlin Wall in 1989 and the subsequent dissolution of the Soviet Union, an unlikely partnership arose in the form of an experiment called SAGE, the Soviet-American Gallium Experiment. ‚ÄúThe Soviet Union had a phenomenal group of theoretical scientists,‚Äù said Steven Elliott, a nuclear physicist at Los Alamos National Laboratory who worked on the project. But they lacked money and access to certain technologies that would make SAGE possible, he said. ‚ÄúLos Alamos was able to provide those types of resources.‚Äù</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/BaksanNeutrinoObservatory-crMaximBabenko_TheNewYorkTimes_Diptych-scaled.webp"></p></div><figcaption><div><p>For the last half-century, physicists have studied neutrinos in experiments deep underground at the Baksan Neutrino Observatory in Russia‚Äôs Caucasus Mountains. In the lab that houses the BEST experiment, fish serve as an early warning system about any leaking radiation.</p><p>Maxim Babenko/The New York Times</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>SAGE was constructed at the Baksan Neutrino Observatory, a <a href="https://cerncourier.com/a/baksan-scales-new-neutrino-heights/">neutrino physics facility</a> built in the 1960s and 1970s inside a mountain in Russia‚Äôs Baksan Valley, about 3 miles from the Georgian border. The 13,000-foot-tall Mount Andyrchi shielded the facility from cosmic rays and other sources of noise, allowing precise neutrino experiments to take place.</p>
<p>A nearby residential area called Neutrino Village housed the families of the scientists who worked at the facility, as well as visiting international scientists like Elliott. ‚ÄúI did go out for a number of trips,‚Äù he said. ‚ÄúI found it an adventure.‚Äù</p>
<p>SAGE began in 1989 and continued for more than 20 years despite attempts by the Russian government to <a href="https://www.science.org/content/article/new-twist-gallium-struggle">sell its gallium</a>, a precious metal that‚Äôs liquid at room temperature. The project was designed to investigate the solar neutrino problem, a measured deficit of neutrinos streaming from the sun. Specifically, scientists were finding a shortage of electron neutrinos, one of three known types, or ‚Äúflavors.‚Äù That problem was ultimately resolved in the 2000s with the <a href="https://www.symmetrymagazine.org/article/nobel-prize-awarded-for-discovery-of-neutrino-oscillations">Nobel Prize-winning discovery</a> that neutrinos oscillate between flavors as they travel. By the time many of the electron neutrinos from the sun reach Earth, they have become something else.</p>
<p>SAGE used a tank of 57 metric tons of gallium. Incoming electron neutrinos would occasionally combine with a neutron inside a gallium atom and convert it into a proton, turning the gallium into germanium. The scientists counted the germanium atoms in a monthlong extraction process. They chose gallium for the experiment because it has a ‚Äúlow threshold for this reaction,‚Äù Elliott said. A similar experiment began in Italy in 1991, called Gallex.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/GALLAXExperimentAtGranSassoLab-crTommasoGuicciardini_INFN_ScienceSource.webp"></p></div><figcaption><div><p>A researcher with the Gallex experiment, which ran in the 1990s at Gran Sasso National Laboratory in Italy, is shown holding a device called a proportional counter that was used for detecting germanium atoms.</p><p>Tommaso Guicciardini/INFN/ScienceSource</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>In the mid-1990s, researchers tweaked both experiments to use neutrinos from radioactive elements. They hoped to avoid unknown errors related to the solar neutrino problem. But both experiments generated roughly 20% less germanium than expected ‚Äî surprise results that couldn‚Äôt have been caused by the solar neutrino problem. ‚ÄúThey exactly knew the source activity and how many neutrinos are produced,‚Äù said Inwook Kim, a nuclear physicist at Los Alamos. Soon, the puzzling discrepancy had a name: the gallium anomaly. ‚ÄúIt was really surprising,‚Äù Barinov said.</p>
<p>A follow-up experiment that began at Baksan in 2014, called the Baksan Experiment on Sterile Transitions (BEST), uses two gallium chambers instead of one, to determine whether the anomaly could be explained by the distance from the source of the neutrinos. ‚ÄúBEST was constructed to resolve this tension,‚Äù said Barinov, who has worked on the experiment since 2015. But both chambers have continued to show a shortfall relative to what models predict. ‚ÄúIt‚Äôs a really unusual result,‚Äù he said.</p>
<h2><strong>Half-Life Theory</strong></h2>
<p>Repeated results from BEST continue to show the anomaly <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.128.232501">as recently as 2022</a>. One chamber contained only 79% of the expected amount of germanium, the other only 77%. ‚ÄúEverybody was hoping that anomaly would go away,‚Äù said Wick Haxton, a theoretical physicist at Berkeley. ‚ÄúThere is still not any clean understanding of what‚Äôs going on.‚Äù</p>
<p>A possible explanation was floated: that the half-life of germanium-71 (the specific isotope produced in the experiment), <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.31.666">measured</a> in 1985 to be 11.43 days, was actually longer. The same constant controls germanium-71‚Äôs decay rate and the rate at which gallium captures neutrinos to produce that germanium. That means a longer germanium-71 half-life would imply a lower rate of neutrino capture and hence germanium production, which could explain the lack of germanium seen by SAGE, Gallex and BEST.</p>
</div></div><figure><div><p><img alt="A young man wearing a red scarf stands in front of a mountain." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/VladislavBarinov-crSviatoslavBorisov.webp"></p></div><figcaption><div><p>Vladislav Barinov, a particle physicist at the Institute for Nuclear Research of the Russian Academy of Sciences, is part of the team that reported the anomalous results of the BEST experiment.</p><p>Sviatoslav Borisov</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Norman and colleagues published a reinvestigation of this half-life in <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.109.055501"><em>Physical Review C</em></a> in late May. Using a nuclear reactor at the McClellan Nuclear Research Center at the University of California, Davis, they irradiated ‚Äúvery pure germanium material,‚Äù Norman said, producing germanium-71. They then analyzed the samples over 80 days to see how long it took the atoms to decay.</p>
<p>They arrived at a half-life of 11.468 days, extremely close to the 1985 measurement, ruling the half-life out as the explanation for the gallium anomaly. While no one ever quite believed the original half-life measurement to be wildly incorrect, researchers still considered it worth checking. ‚ÄúIt was a measurement that needed to be done,‚Äù Jones said.</p>
<p>Another proposed explanation was that physicists had miscalculated the probability of neutrinos from the source interacting with the gallium. But in September 2023, Haxton and his colleagues <a href="https://journals.aps.org/prc/abstract/10.1103/PhysRevC.108.035502">also ruled out this possibility</a>. ‚ÄúYou can‚Äôt get rid of the anomaly,‚Äù he said.</p>
<p>That leaves physicists in an uncomfortable position. Either there is still some error that no one has thought of, or, as Haxton put it, ‚Äúsomething unusual is going on with neutrinos.‚Äù For instance, the experiments might point to a controversial additional type of neutrino, undetected by most other experiments, that might also help to explain dark matter.</p>
<h2><strong>Sterile Neutrinos</strong></h2>
<p>The three known flavors of neutrinos, which are all millions of times lighter than electrons, interact with other elementary particles via the weak force, which makes them detectable. Sterile neutrinos, on the other hand, would interact only via gravity. If they‚Äôre much heavier than the known neutrinos, their existence could explain why the known neutrinos are so light, through an inverse relationship hypothesized around 1980 called the seesaw mechanism.</p>
</div></div><figure><div><p><img alt="A yellow-lit laboratory crammed with equipment, including a set of 10 chemical reactors with red caps." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/07/Gallium%E2%80%93Germanium_Neutrino_Telescope_main_room_-_2010-07-19_-_DSC_0764.webp"></p></div><figcaption><div><p>The experimental apparatus used in the SAGE and BEST experiments at the Baksan Neutrino Observatory.</p><p>Konstantin Malanchev</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>The gallium anomaly, however, would point toward a lighter-weight sterile neutrino, with the electron neutrinos emitted by the radioactive source sometimes oscillating into a sterile neutrino that wouldn‚Äôt interact with the gallium.</p>
<p>In some models, lightweight sterile neutrinos could comprise a fraction of the universe‚Äôs dark matter, though not all of it because they would be too light to gravitationally shape the universe in the way dark matter does. ‚ÄúThey could be a small subset of it,‚Äù said <a href="https://physics.mit.edu/faculty/lindley-winslow/">Lindley Winslow</a>, an experimental nuclear and particle physicist at the Massachusetts Institute of Technology.</p>
<p>Other attempts to find sterile neutrinos by studying neutrino oscillation patterns, however, have been largely unsuccessful. The number of researchers who support the light sterile neutrino ‚Äúis sort of shrinking,‚Äù Winslow said. <a href="https://sites.uci.edu/abazajian/">Kevork Abazajian</a>, an astrophysicist at the University of California, Irvine, said they are the ‚Äúunderdogs of the particle physics community.‚Äù</p>
<p>If they do exist, light sterile neutrinos will ‚Äúwreak havoc‚Äù on our current understanding of cosmology, Abazajian said, including ideas of how atoms formed in the minutes following the Big Bang and the theory of the cosmic microwave background, the remnant heat from the initial expansion of the universe. ‚ÄúYou would expect to see the presence of this extra neutrino,‚Äù Abazajian said. However, he added that <a href="https://arxiv.org/abs/2205.09777">recent work has shown</a> that alternative models of the sequence of events in those first minutes ‚Äúcan accommodate light sterile neutrinos.‚Äù</p>

<p>In lieu of other explanations for the gallium anomaly, light sterile neutrinos remain a possibility that we just can‚Äôt eradicate. ‚ÄúI‚Äôve been a bit skeptical of the sterile neutrino hypothesis, but I can‚Äôt tell you why it‚Äôs not right,‚Äù Elliott said. ‚ÄúThere‚Äôs never been a convincing explanation of why the experiment might be wrong.‚Äù</p>
<p>While Russia‚Äôs invasion of Ukraine ‚Äúhas complicated things,‚Äù Elliott said, the collaboration between the U.S. and Russia on BEST is still ongoing, for now. Barinov says the team at Baksan is considering using a new source of neutrinos, such as zinc, to further test the result. They may even construct a third chamber of gallium around the source. For now, the anomaly remains unsolved, with no sign of a resolution on the horizon. ‚ÄúIt has us all puzzled,‚Äù Haxton said.</p>
</div></div></div><div><h2>Next article</h2><p>How America‚Äôs Fastest Swimmers Use Math to Win Gold</p></div></div>]]></description>
        </item>
    </channel>
</rss>