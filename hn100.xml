<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 30 Apr 2025 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Waymo partners with Toyota to bring autonomous driving to personal vehicles (359 pts)]]></title>
            <link>https://waymo.com/blog/2025/04/waymo-and-toyota-outline-strategic-partnership</link>
            <guid>43839123</guid>
            <pubDate>Tue, 29 Apr 2025 23:06:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/blog/2025/04/waymo-and-toyota-outline-strategic-partnership">https://waymo.com/blog/2025/04/waymo-and-toyota-outline-strategic-partnership</a>, See on <a href="https://news.ycombinator.com/item?id=43839123">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-labelledby="P0-7-title"><a href="https://waymo.com/blog/"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow.svg"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow-rollover.svg"><span>Back to all posts</span></a><section><div><picture><source srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?fm=webp&amp;q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1440&amp;fm=webp&amp;q=90" media="(min-width: 600px)" type="image/webp" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1440&amp;" media="(min-width: 600px)" type="image/png" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?fm=webp&amp;q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1024&amp;fm=webp&amp;q=90" media="(min-width: 600px) and (max-width: 1023px)" type="image/webp" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1024&amp;" media="(min-width: 600px) and (max-width: 1023px)" type="image/png" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?fm=webp&amp;q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1024&amp;fm=webp&amp;q=90" media="(max-width: 599px)" type="image/webp" width="1920" height="1080"><img alt="Waymo and Toyota logos" loading="lazy" srcset="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?q=90 2x, https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=1024&amp;" src="https://images.ctfassets.net/e6t5diu0txbw/5HgdQTkVl6YvZ99X4MoHxp/f5cbcc5eae5be56c53bc770b002597af/Waymo-Toyota_Logo_Lockup.png?w=420&amp;" width="1920" height="1080"></picture></div><div><p>Toyota Motor Corporation (“Toyota”) and Waymo reached a preliminary agreement to explore a collaboration focused on accelerating the development and deployment of autonomous driving technologies. Woven by Toyota will also join the potential collaboration as Toyota’s strategic enabler, contributing its strengths in advanced software and mobility innovation. This potential partnership is built on a shared vision of improving road safety and delivering increased mobility for all.</p><p>Toyota and Waymo aim to combine their respective strengths to develop a new autonomous vehicle platform. In parallel, the companies will explore how to leverage Waymo's autonomous technology and Toyota's vehicle expertise to enhance next-generation personally owned vehicles (POVs). The scope of the collaboration will continue to evolve through ongoing discussions.</p><p>Toyota has long advanced research and development in support of a zero-traffic-accident vision, guided by a three-pillar approach that integrates people, vehicles, and traffic infrastructure. Automated driving and advanced safety technologies play a central role, exemplified by the development and global deployment of Toyota Safety Sense (TSS) — a proprietary suite of advanced safety technologies. TSS reflects Toyota’s belief that technologies have the greatest impact when they are made widely accessible. Through this new collaboration, the companies aim to further accelerate the development and adoption of driver assistance and automated driving technologies for POVs, with a continued focus on safety and peace of mind.</p><p>Waymo, the global leader in autonomous driving technology, now serves more than a quarter of a million trips each week across the San Francisco Bay Area, Los Angeles, Phoenix, and Austin. With tens of millions of miles traveled, the data shows that Waymo is making roads safer where it operates, including being involved in 81% fewer injury-causing crashes compared to a human benchmark. Waymo is building a generalizable driver that can be applied to a variety of vehicle platforms and businesses over time. The company continues to scale its commercial ride-hailing service, Waymo One, and through this strategic partnership will now begin to incorporate aspects of its technology for personally owned vehicles.</p><p>Hiroki Nakajima, Member of the Board and Executive Vice President of Toyota Motor Corporation, emphasized the significance of this collaboration, stating, “Toyota is committed to realizing a society with zero traffic accidents and becoming a mobility company that delivers mobility for all. We share a strong sense of purpose and a common vision with Waymo in advancing safety through automated driving technology, and we are confident this collaboration can help bring our solutions to more people around the world, moving us one step closer to a zero-accident society. Our companies are taking an important step toward a future with greater safety and peace of mind for all.”</p><p>Tekedra Mawakana, co-CEO at Waymo, also emphasized the impact of this collaboration, stating, "Waymo's mission is to be the world's most trusted driver. This requires global partners like Toyota that share our commitment to improving road safety and expanding accessible transportation. We look forward to exploring this strategic partnership, incorporating their vehicles into our ride-hailing fleet and bringing the magic of Waymo's autonomous driving technology to Toyota customers."<br></p></div></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Only Teslas exempt from new auto tariffs thanks to 85% domestic content rule (373 pts)]]></title>
            <link>https://fuelarc.com/cars/only-tesla-exempt-from-new-auto-tariffs-thanks-to-85-domestic-content-rule/</link>
            <guid>43837993</guid>
            <pubDate>Tue, 29 Apr 2025 20:59:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fuelarc.com/cars/only-tesla-exempt-from-new-auto-tariffs-thanks-to-85-domestic-content-rule/">https://fuelarc.com/cars/only-tesla-exempt-from-new-auto-tariffs-thanks-to-85-domestic-content-rule/</a>, See on <a href="https://news.ycombinator.com/item?id=43837993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="27ad23c5" data-element_type="widget" data-widget_type="theme-post-content.default">
					
<p>In a major policy shift, U.S. Commerce Secretary <a href="https://www.theguardian.com/us-news/live/2025/apr/29/donald-trump-100-days-in-office-immigration-tariffs-canada-mark-carney-us-politics-live-updates?page=with:block-6810f9e08f08b6ffd73c19ae">Howard Lutnick announced</a> that vehicles with 85% or more domestic content will be fully exempt from new tariffs on automobiles.</p>



<p>That’s a steep threshold, and as of today, there’s only one automaker that qualifies for the exemption: Tesla.</p>



<h2>Tesla Alone Qualifies for Full Tariff Exemption</h2>



<p>Here are the top 10 vehicles according to the most recent data on automobiles ranked by domestic content percentage:</p>



<figure><table><thead><tr><th>Rank</th><th>Makes</th><th>Model</th><th>Total Domestic Content</th></tr></thead><tbody><tr><td>1</td><td>Tesla</td><td>Model 3 Performance</td><td><strong>87.5%</strong></td></tr><tr><td>2 (tied)</td><td>Tesla</td><td>Model Y Long Range</td><td><strong>85.0%</strong></td></tr><tr><td>2 (tied)</td><td>Tesla</td><td>Model Y</td><td><strong>85.0%</strong></td></tr><tr><td>3</td><td>Tesla</td><td>Cybertruck</td><td>82.5%</td></tr><tr><td>4 (tied)</td><td>Ford</td><td>Mustang GT AT</td><td>80.0%</td></tr><tr><td>4 (tied)</td><td>Ford</td><td>Mustang GT 5.0L</td><td>80.0%</td></tr><tr><td>4 (tied)</td><td>Ford</td><td>Mustang GT Coupe Premium</td><td>80.0%</td></tr><tr><td>4 (tied)</td><td>Tesla</td><td>Model S</td><td>80.0%</td></tr><tr><td>4 (tied)</td><td>Tesla</td><td>Model X</td><td>80.0%</td></tr><tr><td>5</td><td>Honda</td><td>Passport AWD</td><td>76.5%</td></tr></tbody></table></figure>



<p>According to <a href="https://kogod.american.edu/autoindex/2024">the 2024 data from the Kogod School of Business</a> at American University, there are only three vehicles that meet the exemption threshold under the new tariff policy. Those are one cut of the Tesla Model 3 and two cuts of the Tesla Model Y with domestic content above the 85% mark.</p>



<h2>Where Automotive Tariffs Stand Today</h2>



<p>The base tariff rate for all imports is set at 10%, but the standard for many automakers and automotive part suppliers will be <a href="https://www.cnn.com/2025/04/28/business/us-auto-tariffs-easing-deal-intl-hnk/index.html">a stiff 25%</a>, unless they qualify for a rebate program that will be available for the next two years only.</p>



<p>Clearing that new 85% threshold is a massive win for any automaker, simplifying their supply chain, regulatory requirements, and most importantly, pricing.</p>



<h2>Is This Favoritism?</h2>



<p>You probably can’t tell, but I have an eyebrow raised over here, it’s been stuck that way since these exemptions were announced. While domestic content rules sound neutral on paper, the real-world effect of this policy is to carve out an exemption that only Tesla benefits from today.</p>



<p>Watching Elon Musk hang out in the White House for weeks on end was initially puzzling. It certainly did nothing good for <a href="https://electrek.co/2025/04/22/tesla-tsla-q1-2025-financial-results/">Tesla’s revenue numbers</a>. It absolutely <em>does</em> make sense when you look at the newly emerging regulatory order, though.</p>



<p>Even for models that come close to that <em>oddly specific</em> 85% mark, like the Ford Mustang and the built-in-Alabama Honda Passport, a few percentage points make all the difference between scrambling to adapt to a steep tariff and skating by untouched.</p>



<p>Stay tuned, these changes are slated to be packaged in an executive order to be signed this evening. We may have further updates based on the specific verbiage in the order.</p>



<blockquote>
<p>Update: The White House released a “<a href="https://www.whitehouse.gov/fact-sheets/2025/04/fact-sheet-president-donald-j-trump-incentivizes-domestic-automobile-production/">Fact Sheet</a>” tonight that broadly confirms the tariff rules change that we reported on above. That promised <a href="https://www.whitehouse.gov/presidential-actions/2025/04/addressing-certain-tariffs-on-imported-articles/">Executive Order</a> arrived, but the impact is easier to understand from the Fact Sheet.</p>
</blockquote>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I can't pay rent because devs just don't care (149 pts)]]></title>
            <link>https://happyfellow.bearblog.dev/i-cant-pay-rent-because-devs-just-dont-care/</link>
            <guid>43836619</guid>
            <pubDate>Tue, 29 Apr 2025 18:57:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://happyfellow.bearblog.dev/i-cant-pay-rent-because-devs-just-dont-care/">https://happyfellow.bearblog.dev/i-cant-pay-rent-because-devs-just-dont-care/</a>, See on <a href="https://news.ycombinator.com/item?id=43836619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-02-11T16:20Z">
                    11 Feb, 2025
                </time>
            </i>
        </p>
    

    <p>Rent's due today, thank heavens I just got paid.</p>
<p>I don't set up standing orders, got burned once with unarranged overdraft fees, that wasn't a good month. Better to be late a day or two with rent payment. Not this month though! Just need to log in to HSBC and make the payment.</p>
<p>Shit, can't log in with without updating the app but I have no space on my phone, why's the new app bigger? I had to remove other apps and the HSBC app, then install it again.</p>
<p>It lost my Digital Secure Key, can't log in, crap. Call centres are closed, I need the chat. I can only keep the chat opened for a few minutes before Chrome gives me the "Aw, snap!" page. Stupid phone, I bought it just 2 years ago, it's not that old. I can't upgrade more often, who the hell can?</p>
<p>Rent's due today and computer said I can't pay it.</p>
<p>--</p>
<p>I made that up. Not all of it, parts of it happened. And worse stuff happens, like when I needed to pay to get my visa and the website just wouldn't work if your phone was a few years old. It likely didn't have enough RAM for it to work. How is 2GB not enough to send a few numbers describing the payment through the network?</p>
<p>I'm just ranting because it makes my blood boil. When someone smugly says "micro-optimisations don't matter, networks are great, CPUs fast and memory plentiful", proceeds to make an app one degree more complicated than Hello World use 4 GB of RAM and drain half my battery, I'm having dark thoughts.</p>
<p>Memory is not cheap. CPUs don't really get that much faster. Draining user's battery is not free, it's a consumable and you are forcing users to replace it earlier. The 17 tracking scripts you put on your websites which added 0.004 pence to your bottom line mean that someone needs to shell out a hundred quid for new battery earlier.</p>
<p>It's not "fast enough". It's shit. Benchmarks and guidelines you've read probably say "if you show any text under 5 seconds on newest iPhone on 5G connection you get 100% Performance Score, blue ribbon and your parents will be finally proud of you" because otherwise the biggest websites build by The Best Engineers in the Entire World will not get anywhere close to 100%.</p>
<p>THE LIMIT IS BASICALLY THE SPEED OF LIGHT, A LITERAL SPEED OF LIGHT AND YOU SAY TAKING 10 SECONDS TO DISPLAY 1000 WORDS OF TEXT AND TWO IMAGES IS SOMETHING TO BE PROUD OF?</p>
<p>And God forbid you create any sort of actual critical infrastructure and misquote Knuth to me one more goddamn time. People cannot afford iPhone 137, they need to access government services or pay their rent and if you make your payment processing website unusable on 3 year old mid-range Android because adding 10 MB more Javascript increases ✨Development Velocity✨, there's a special place in hell for you. It'll mostly consist of refactoring Javascript codebases with no test coverage.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Beatsync – perfect audio sync across multiple devices (233 pts)]]></title>
            <link>https://github.com/freeman-jiang/beatsync</link>
            <guid>43835584</guid>
            <pubDate>Tue, 29 Apr 2025 17:32:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/freeman-jiang/beatsync">https://github.com/freeman-jiang/beatsync</a>, See on <a href="https://news.ycombinator.com/item?id=43835584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Beatsync</h2><a id="user-content-beatsync" aria-label="Permalink: Beatsync" href="#beatsync"></a></p>
<p dir="auto">Beatsync is a high-precision web audio player built for multi-device playback.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description beatsync-demo.mov">beatsync-demo.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/56516912/438452788-2aa385a7-2a07-4ab5-80b1-fda553efc57b.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NjYxMDIsIm5iZiI6MTc0NTk2NTgwMiwicGF0aCI6Ii81NjUxNjkxMi80Mzg0NTI3ODgtMmFhMzg1YTctMmEwNy00YWI1LTgwYjEtZmRhNTUzZWZjNTdiLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDI5VDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2YjIxYWYzYTc1MGYxY2NlOTdlYjNiZDU3MzhjM2ZjODEwZWRkYjA5MmUwMjBlMzg3MzEzYzI5YTAwZjNjMzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-sLCQ_X16e_pvwbWJ87GKVp8Y2bsJ8JuRXLSC60RKZg" data-canonical-src="https://private-user-images.githubusercontent.com/56516912/438452788-2aa385a7-2a07-4ab5-80b1-fda553efc57b.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NjYxMDIsIm5iZiI6MTc0NTk2NTgwMiwicGF0aCI6Ii81NjUxNjkxMi80Mzg0NTI3ODgtMmFhMzg1YTctMmEwNy00YWI1LTgwYjEtZmRhNTUzZWZjNTdiLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDI5VDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2YjIxYWYzYTc1MGYxY2NlOTdlYjNiZDU3MzhjM2ZjODEwZWRkYjA5MmUwMjBlMzg3MzEzYzI5YTAwZjNjMzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-sLCQ_X16e_pvwbWJ87GKVp8Y2bsJ8JuRXLSC60RKZg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Millisecond-accurate synchronization</strong>: Abstracts <a href="https://en.wikipedia.org/wiki/Network_Time_Protocol" rel="nofollow">NTP-inspired</a> time synchronization primitives to achieve a high degree of accuracy</li>
<li><strong>Cross-platform</strong>: Works on any device with a modern browser (Chrome recommended for best performance)</li>
<li><strong>Spatial audio:</strong> Allows controlling device volumes through a virtual listening source for interesting sonic effects</li>
<li><strong>Polished interface</strong>: Smooth loading states, status indicators, and all UI elements come built-in</li>
<li><strong>Self-hostable</strong>: Run your own instance with a few commands</li>
</ul>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">Beatsync is in early development. Currently, only desktop Chrome browsers are fully supported.</p>
<p dir="auto">Mobile device synchronization is experimental and may be unstable.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">This project uses <a href="https://turbo.build/repo" rel="nofollow">Turborepo</a>.</p>
<p dir="auto">Fill in the <code>.env</code> file in <code>apps/client</code> with the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="NEXT_PUBLIC_API_URL=http://localhost:8080
NEXT_PUBLIC_WS_URL=ws://localhost:8080/ws"><pre>NEXT_PUBLIC_API_URL=http://localhost:8080
NEXT_PUBLIC_WS_URL=ws://localhost:8080/ws</pre></div>
<p dir="auto">Run the following commands to start the server and client:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bun install          # installs once for all workspaces
bun dev              # starts both client (:3000) and server (:8080)"><pre>bun install          <span><span>#</span> installs once for all workspaces</span>
bun dev              <span><span>#</span> starts both client (:3000) and server (:8080)</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Directory</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>apps/server</code></td>
<td>Bun HTTP + WebSocket server</td>
</tr>
<tr>
<td><code>apps/client</code></td>
<td>Next.js frontend with Tailwind &amp; Shadcn/ui</td>
</tr>
<tr>
<td><code>packages/shared</code></td>
<td>Type-safe schemas and functions shared between client &amp; server</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bamba: An open-source LLM that crosses a transformer with an SSM (150 pts)]]></title>
            <link>https://research.ibm.com/blog/bamba-ssm-transformer-model</link>
            <guid>43835495</guid>
            <pubDate>Tue, 29 Apr 2025 17:24:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.ibm.com/blog/bamba-ssm-transformer-model">https://research.ibm.com/blog/bamba-ssm-transformer-model</a>, See on <a href="https://news.ycombinator.com/item?id=43835495">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The transformer architecture behind today’s large language models has shown an uncanny ability to generate human-like text. Part of its effectiveness comes from its self-attention mechanism, which allows the model to weigh all the words in an input sequence when generating a response.</p><p>The problem comes as conversations get longer. Because the model holds the running sequence in memory as it responds, the cumulative cost of generation grows quadratically. If the size of the context window doubles, the cost of processing the context and generating a response doesn’t just double — it quadruples.</p><p>This “quadratic bottleneck” is often behind that frustrating lag between asking the model a question and getting an answer. It also creates a lot of redundant computing. By the time ChatGPT popularized the transformer in 2022, researchers were already searching for alternative architectures.</p><p>State-space models (SSMs), and transformers interleaved with SSM layers, have emerged as two possible solutions. IBM Research has just open-sourced its first hybrid experiment: <a href="https://huggingface.co/blog/ibm-ai-platform/bamba-9b-v2">Bamba</a>, a model that can run as quickly as an SSM and process long sequences as skillfully as a transformer. Many of Bamba’s innovations are part of IBM’s next-generation Granite 4.0 models coming in several months.</p><p>By significantly reducing the memory requirements of the transformer’s KV (key value) cache memory, Bamba-9B has shown it can run at least twice as fast as transformers of similar size while matching their accuracy. &nbsp;“Everything comes back to the KV cache reduction,” says Raghu Ganti, the IBM researcher leading the project. “More throughput, lower latency, longer context length.”</p><p><h2><a name="-the-most-important-model-youve-never-heard-of"></a>The most important model you’ve never heard of</h2></p><p>State-space models come nowhere close to matching the name recognition of transformers, but they’ve been used for decades to model dynamic systems.</p><p>“They are the bread and butter of electrical engineering — signal processing, robotics, and control theory,” says Ankit Gupta, an IBM researcher who has played a key role in adapting SSMs to deep learning. “Any field that uses time-series data would use state-space models to analyze it.”</p><p>The mathematical equations SSMs are built on can be applied to electrical activity in the brain, the weather, and even the stock market. From a series of observations, an SSM calculates a “hidden state” of fixed size, capturing the essential properties of the system. Think of the state as a summary of the past. When new data comes in, the hidden state gets updated, without increasing its size, along with predictions of what will happen next.</p><p>SSMs crossed over to neural networks in 2021, when Albert Gu and his collaborators at Stanford <a href="https://arxiv.org/pdf/2111.00396">released S4</a>, an SSM that applied state variables to language. Like the transformer, and recurrent neural network (RNNs), before it, the SSM was good at processing sequences of words. But it could process long sequences more skillfully than RNNs and much faster than transformers.</p><p>While a transformer attends to all the words in the context window when outputting a response, an SSM maintains a compressed hidden state that summarizes past information. This selective retention of information requires less memory overhead and leads to faster inference speeds.</p><p>S4 made a splash when it suddenly appeared on <a href="https://arxiv.org/abs/2011.04006">Long Range Arena</a>, a benchmark for comparing language models by their ability to handle long sequences, but it was difficult to implement. Then an AI resident at IBM, Gupta helped Gu and team simplify the model using <a href="https://arxiv.org/pdf/2206.11893">diagonal state spaces</a>. Their “diagonal” SSM shrank S4’s 1,000 lines of code to 10. Gupta later helped to introduce <a href="https://arxiv.org/abs/2206.13947">a gating mechanism</a> for filtering out irrelevant information, allowing SSMs for the first time to match the “expressivity,” or sequence-modeling skill, of transformers.</p><p>That team also unveiled what may be the first hybrid transformer. “Exploring hybrids made sense,” said Gupta, who now works on IBM’s <a href="https://research.ibm.com/blog/granite-vlm">Granite Vision models</a>. “We could use standard attention blocks to handle text with local dependencies while leveraging SSMs to do longer-range contextualization.”</p><p>In 2023, Gu, then a professor at CMU, and Tri Dao, at Princeton, unveiled a gated SSM variant — <a href="https://tridao.me/blog/2024/mamba2-part1-model/">Mamba2</a>, which helped to inspire a wave of hybrids, with names like <a href="https://arxiv.org/pdf/2406.07522">Samba</a> and <a href="https://arxiv.org/pdf/2402.04248">MambaFormer</a>. Last year, Nvidia confirmed that these new hybrids could outperform either architecture on their own while dramatically <a href="https://arxiv.org/pdf/2406.07887">speeding up inferencing</a>, culminating in their release of Nemotron-H.</p><p><h2><a name="-overcoming-the-kv-cache-bottleneck"></a>Overcoming the KV cache bottleneck</h2></p><p>From the start, IBM Research has made efficiency the cornerstone of its Granite LLMs for enterprise. As <a href="https://www.ibm.com/granite">IBM Granite</a> has become smaller and more capable, researchers aimed their sights on the quadratic bottleneck. When Nvidia’s results came out, IBM researchers validated them internally and moved forward with building their own hybrid — Bamba-9B.</p><p>They sought out Mamba’s creators, Gu and Dao, along with Minjia Zhang, a professor at the University of Illinois at Urbana-Champaign. Together, they selected Nvidia’s Mamba2 architecture and chose to make just about everything associated with Bamba open-source — the training recipes, the data, the data loader IBM designed for largescale distributed training, and a quantization framework aimed at shaving storage and inferencing costs.</p><p>They initially trained Bamba on 2 trillion tokens (words and parts of words). Encouraged by the results, they added another trillion tokens and shrank the model from 18 GB to 9 GB through quantization, reducing its bit width from Mamba2’s 16-bit floating-point precision to 8-bits.
On key benchmarks, Bamba has performed on par with Meta’s Llama-3.1 8B model, which was trained on seven times more data — an achievement Ganti attributes to Bamba’s design and high-quality training data.</p><p>Their next hurdle was optimizing vLLM to run SSMs. “Virtual” LLM has emerged as the go-to open-source inference server for LLMs, and the team behind Bamba worked closely with Red Hat to integrate the model into the platform. “SSMs are difficult to support, because you need bespoke state management,” said Tyler Smith, a technical staff member at Red Hat and a vLLM committer.</p><p>When Bamba was released late last year, Ganti invited the open community to help improve it. “Let’s overcome the KV-cache bottleneck together!” he wrote in Bamba’s intro on Hugging Face.</p><p>Trained on 4,000-token sequences, Bamba can capably handle 32,000-token conversations. But Ganti said he thinks it can go to 1 million tokens or more, and run up to five times faster than a transformer as vLLM incorporates more support for SSMs.</p><p>The refrain of La Bamba, the Mexican folk song that Ritchie Valens made famous, goes: <em>Para bailar La Bamba/Se necesita una poca de Gracia.</em> All you need to dance the beat is a little grace.</p><p>The same could be said for beating the transformer’s quadratic bottleneck.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made my AI think harder by making it argue with itself (397 pts)]]></title>
            <link>https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts</link>
            <guid>43835445</guid>
            <pubDate>Tue, 29 Apr 2025 17:19:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts">https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts</a>, See on <a href="https://news.ycombinator.com/item?id=43835445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CoRT (Chain of Recursive Thoughts) 🧠🔄</h2><a id="user-content-cort-chain-of-recursive-thoughts-" aria-label="Permalink: CoRT (Chain of Recursive Thoughts) 🧠🔄" href="#cort-chain-of-recursive-thoughts-"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TL;DR: I made my AI think harder by making it argue with itself repeatedly. It works stupidly well.</h2><a id="user-content-tldr-i-made-my-ai-think-harder-by-making-it-argue-with-itself-repeatedly-it-works-stupidly-well" aria-label="Permalink: TL;DR: I made my AI think harder by making it argue with itself repeatedly. It works stupidly well." href="#tldr-i-made-my-ai-think-harder-by-making-it-argue-with-itself-repeatedly-it-works-stupidly-well"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What is this?</h3><a id="user-content-what-is-this" aria-label="Permalink: What is this?" href="#what-is-this"></a></p>
<p dir="auto">CoRT makes AI models recursively think about their responses, generate alternatives, and pick the best one. It's like giving the AI the ability to doubt itself and try again... and again... and again.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does it actually work?</h3><a id="user-content-does-it-actually-work" aria-label="Permalink: Does it actually work?" href="#does-it-actually-work"></a></p>
<p dir="auto">YES. I tested it with Mistral 3.1 24B and it went from "meh" to "holy crap", especially for such a small model, at programming tasks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<ol dir="auto">
<li>AI generates initial response</li>
<li>AI decides how many "thinking rounds" it needs</li>
<li>For each round:
<ul dir="auto">
<li>Generates 3 alternative responses</li>
<li>Evaluates all responses</li>
<li>Picks the best one</li>
</ul>
</li>
<li>Final response is the survivor of this AI battle royale</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Mistral 3.1 24B non CoRT
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/155454343/437931497-acbcf1f9-4715-4d2c-a31c-38b349602380.PNG?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NTUzMDEsIm5iZiI6MTc0NTk1NTAwMSwicGF0aCI6Ii8xNTU0NTQzNDMvNDM3OTMxNDk3LWFjYmNmMWY5LTQ3MTUtNGQyYy1hMzFjLTM4YjM0OTYwMjM4MC5QTkc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNDI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDQyOVQxOTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yYWM5NGExZDM2MjFkNDQwNTJjNjU4ZWFjMjhmZjE0MTFmMDI0NWYwYTNhNmE3NjQwMmZiMGE5YTY3MDU3ZWU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.MCKzCjJu2WSO0eTyIyooBqtHE4t-nmWUBXpooUhmqas"><img src="https://private-user-images.githubusercontent.com/155454343/437931497-acbcf1f9-4715-4d2c-a31c-38b349602380.PNG?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NTUzMDEsIm5iZiI6MTc0NTk1NTAwMSwicGF0aCI6Ii8xNTU0NTQzNDMvNDM3OTMxNDk3LWFjYmNmMWY5LTQ3MTUtNGQyYy1hMzFjLTM4YjM0OTYwMjM4MC5QTkc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNDI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDQyOVQxOTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yYWM5NGExZDM2MjFkNDQwNTJjNjU4ZWFjMjhmZjE0MTFmMDI0NWYwYTNhNmE3NjQwMmZiMGE5YTY3MDU3ZWU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.MCKzCjJu2WSO0eTyIyooBqtHE4t-nmWUBXpooUhmqas" alt="rec"></a></p>
<p dir="auto">Mistral 3.1 24B + CoRT
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/155454343/437931494-9c4f6af9-0a8f-4c62-920c-f272fce225c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NTUzMDEsIm5iZiI6MTc0NTk1NTAwMSwicGF0aCI6Ii8xNTU0NTQzNDMvNDM3OTMxNDk0LTljNGY2YWY5LTBhOGYtNGM2Mi05MjBjLWYyNzJmY2UyMjVjMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNDI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDQyOVQxOTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03N2E2YjQwNTUxNjE4ZGM3ZTZjZDAxMzRiY2JmZWI4NWIwOGM2YWE4ZTE2MGJkMzFlNjcyNmJhYWRhZTg5YjYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.Ti1DUkjHPlHiG_QhTD1LEG34q0x7Mt_lwjcrsSqIZ-4"><img src="https://private-user-images.githubusercontent.com/155454343/437931494-9c4f6af9-0a8f-4c62-920c-f272fce225c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDU5NTUzMDEsIm5iZiI6MTc0NTk1NTAwMSwicGF0aCI6Ii8xNTU0NTQzNDMvNDM3OTMxNDk0LTljNGY2YWY5LTBhOGYtNGM2Mi05MjBjLWYyNzJmY2UyMjVjMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNDI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDQyOVQxOTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03N2E2YjQwNTUxNjE4ZGM3ZTZjZDAxMzRiY2JmZWI4NWIwOGM2YWE4ZTE2MGJkMzFlNjcyNmJhYWRhZTg5YjYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.Ti1DUkjHPlHiG_QhTD1LEG34q0x7Mt_lwjcrsSqIZ-4" alt="non-rec"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Try it yourself</h2><a id="user-content-try-it-yourself" aria-label="Permalink: Try it yourself" href="#try-it-yourself"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt
export OPENROUTER_API_KEY=&quot;your-key-here&quot;
python recursive-thinking-ai.py"><pre><span>pip</span> <span>install</span> <span>-</span><span>r</span> <span>requirements</span>.<span>txt</span>
<span>export</span> <span>OPENROUTER_API_KEY</span><span>=</span><span>"your-key-here"</span>
<span>python</span> <span>recursive</span><span>-</span><span>thinking</span><span>-</span><span>ai</span>.<span>py</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Secret Sauce</h3><a id="user-content-the-secret-sauce" aria-label="Permalink: The Secret Sauce" href="#the-secret-sauce"></a></p>
<p dir="auto">The magic is in:</p>
<ul dir="auto">
<li>Self-evaluation</li>
<li>Competitive alternative generation</li>
<li>Iterative refinement</li>
<li>Dynamic thinking depth</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Found a way to make it even better? PR's welcome!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT - Go wild with it</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything we announced at our first LlamaCon (158 pts)]]></title>
            <link>https://ai.meta.com/blog/llamacon-llama-news/?_fb_noscript=1</link>
            <guid>43835424</guid>
            <pubDate>Tue, 29 Apr 2025 17:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/llamacon-llama-news/?_fb_noscript=1">https://ai.meta.com/blog/llamacon-llama-news/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=43835424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Takeaways</h2><div><ul><li>We’re celebrating the first-ever LlamaCon by sharing some exciting new tools that will make it even easier to build with <a href="https://www.llama.com/" target="_blank" data-lnfb-mode="ie"><u>Llama</u></a> and create custom models to fit your use cases.</li><li>The Llama API, launching as a limited preview, combines the best features of closed models with open-source flexibility, offering easy one-click API key creation and interactive playgrounds to explore different Llama models.</li><li>We’re also sharing new Llama Protection Tools and announcing the <a href="https://www.llama.com/llama-protections/ai-defenders" target="_blank" data-lnfb-mode="ie"><u>Llama Defenders Program</u></a>, where select trusted partners can access AI-enabled tools to help them evaluate the security of their systems to safeguard against potential threats.</li><li>We’re announcing the 10 international recipients of the second <a href="https://about.fb.com/news/2025/04/llama-impact-grant-recipients/" target="_blank" data-lnfb-mode="ie"><u>Llama Impact Grants</u></a>. With over $1.5 million USD awarded, these grants support companies, startups, and universities using Llama to drive transformative change.</li></ul></div><div><p>LlamaCon has officially kicked off. Our inaugural event brings together developers from around the world who all have one thing in common: They love building with <a href="https://llama.com/" target="_blank" data-lnfb-mode="ie"><u>Llama</u></a>. It’s been a little more than two years since we <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/" target="_blank" data-lnfb-mode="ie"><u>launched Llama</u></a>. During that time, we’ve surpassed one billion downloads and, most importantly, established Llama as a leader in the open source AI ecosystem. Developers, startups, governments, and enterprises are finding success with Llama by leveraging its capabilities to drive innovation, improve efficiency, and solve complex problems.</p><p>Of course, we couldn’t bring together a group of developers without also sharing some open source tools that will make it easier for them to explore Llama models, build faster, and use the latest tools to defend and protect. Here’s a look at what we’re announcing today and how you can get started with our newest releases.</p></div></div><p><h2>Introducing Llama API in preview: The best features of closed model APIs, with the flexibility of open source</h2></p><div><div><p>We want to make it even easier for you to quickly start building with Llama, while also giving you complete control over your models and weights without being locked into an API. Today, we’re announcing Llama API—our upcoming developer platform for Llama application development, which is available as a limited free preview.</p><p>Llama API provides easy one-click API key creation and interactive playgrounds to explore different Llama models, including the <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" data-lnfb-mode="ie"><u>Llama 4 Scout and Llama 4 Maverick models</u></a> we announced earlier this month. When you’re ready to build your application, we provide a lightweight SDK in both Python and Typescript. Llama API is also compatible with the OpenAI SDK, making it easy to convert existing applications.</p><br></div><div><p>We know that it’s also important to have access to tools that help you customize and run your models more efficiently. As part of this release, we’re sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. We’re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model. Making evaluations more accessible and easier to run will help move from gut feelings to data, ensuring you have models that perform well to meet your needs. The security and privacy of your content and data is our top priority. We do not use your prompts or model responses to train our AI models. When you’re ready, the models you build on the Llama API are yours to take with you wherever you want to host them, and we don’t keep them locked on our servers. We’re introducing these features to select customers, with plans for a broader rollout in the coming weeks and months. This will open up new possibilities for developers to build custom models for every kind of use case.</p><p>This preview is just step one for the Llama API. We’re excited to have more people building with our API and will use their feedback to help us iterate on future versions. If you’re interested in applying for one of the early limited spots for our free preview, please <a href="https://llama.developer.meta.com/join_waitlist" target="_blank" data-lnfb-mode="ie"><u>fill out this form</u></a>.</p></div><h2>Fast inference with Llama API</h2><p>We’re excited to announce a collaboration with Cerebras and Groq that will help empower more Llama developers to achieve faster inference speeds using the Llama API. Early experimental access to Llama 4 models powered by Cerebras and Groq are now available by request, offering a convenient way for developers to experiment and prototype use cases before scaling with their chosen vendor. By simply selecting the Cerebras or Groq model names in the API, developers can leverage this capability and enjoy a streamlined experience with all usage tracked in one location. This collaboration reinforces our commitment to fostering a diverse ecosystem that offers flexibility and choice, and we look forward to expanding partnerships with additional providers to bring even more options to build on top of Llama.</p><h2>New Llama Stack integrations</h2><p>We heard from developers that we need to make it easier to deploy applications using different service providers. Last year, we collaborated with partners to build Llama Stack distributions for their downstream enterprise clients. This year, we’re expanding those collaborations, including a recently announced integration of Llama Stack with NVIDIA NeMo microservices, and working closely with partners like IBM, Red Hat, Dell Technologies, and others on new integrations that will be announced soon. Alongside our partners, we envision Llama Stack as the industry standard for enterprises looking to seamlessly deploy production-grade turnkey AI solutions.</p><h2>New Llama Protections and security for the open source community</h2><p>We’re committed to providing developers with the best tools and resources to build secure AI applications. Today, we’re releasing new Llama protection tools for the open source community, including Llama Guard 4, LlamaFirewall, and Llama Prompt Guard 2. We’re also sharing updates to help organizations evaluate the efficacy of AI systems in security operations with CyberSecEval 4, as well as announcing the <a href="https://www.llama.com/llama-protections/ai-defenders" target="_blank" data-lnfb-mode="ie"><u>Llama Defenders Program</u></a> for select partners. As more capable AI models become available, we believe this is an important effort to improve the robustness of software systems. <a href="http://ai.meta.com/blog/ai-defenders-program-llama-protection-tools" target="_blank" data-lnfb-mode="ie"><u>Read more</u></a> about our latest open source protection tools and AI security advancements news.</p><h2>Meet the Llama Impact Grant recipients</h2><p>The Llama Impact Grants are aimed at fostering innovation and creating economic opportunities through open source AI. Today, we’re excited to announce the 10 international recipients of the second Llama Impact Grants. With over $1.5 million USD awarded to 10 international recipients, these grants support companies, startups, and universities using Llama to drive transformative change. This year’s recipients showcase a diverse range of solutions, including E.E.R.S. from the US, which uses a Llama-enabled chatbot to enhance public access to civic services, and Doses AI in the UK, which transforms pharmacy operations with real-time error detection. Other notable winners include Solo Tech, which provides offline AI support in rural US communities, and <a href="https://ai.meta.com/blog/foondamate-study-aid-education-llama/" target="_blank" data-lnfb-mode="ie"><u>FoondaMate</u></a>, a multilingual study tool that aids millions of students across Africa and beyond. <a href="https://about.fb.com/news/2025/04/llama-impact-grant-recipients/" target="_blank" data-lnfb-mode="ie"><u>Read more</u></a> about the Llama Impact Grant recipients.</p><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/494065846_1032799098805163_8494634563183521593_n.png?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=8Zv-ja_QAfYQ7kNvwEPkpFT&amp;_nc_oc=AdmWonU8oqY-dZkpyMpcDcxRcCR4qCNRqgQljtlX0yIbSvOHAoqXQYRr4CkYxgxa8yH9HXek_IHSBAyPGHMQu8vM&amp;_nc_zt=14&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=zgeZzZOzVYkzczg_hsuHFQ&amp;oh=00_AfHyMPuPSS8AgJum7_kIb1kSmQvtDN9yec43MEanVUkyiQ&amp;oe=682B8C0D" alt="" id="u_0_8_ep"></p><h2>The future is open source</h2><div><p>We’re proud to be leaders in open source AI and have our Llama ecosystem at the forefront of this revolution. With Llama, developers and enterprises have the freedom to build whatever they want, without being tied down by proprietary systems or locked-in contracts. This flexibility, combined with its portability and accessibility, makes Llama the go-to choice for those looking to harness the power of AI.</p><p>We’re committed to being a long-term partner for enterprises and developers and providing a seamless transition path from closed models. Llama is affordable, easy-to-use, and enabling more people to access the benefits of AI regardless of their technical expertise or hardware resources. We believe in the potential of AI to transform industries and improve lives, which is why we’re excited to continue supporting the growth and development of the Llama ecosystem for the benefit of all. We can’t wait to see what you’ll build next.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[O3 beats a master-level GeoGuessr player, even with fake EXIF data (390 pts)]]></title>
            <link>https://sampatt.com/blog/2025-04-28-can-o3-beat-a-geoguessr-master</link>
            <guid>43835044</guid>
            <pubDate>Tue, 29 Apr 2025 16:48:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sampatt.com/blog/2025-04-28-can-o3-beat-a-geoguessr-master">https://sampatt.com/blog/2025-04-28-can-o3-beat-a-geoguessr-master</a>, See on <a href="https://news.ycombinator.com/item?id=43835044">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Indian court orders blocking of Proton Mail (329 pts)]]></title>
            <link>https://techcrunch.com/2025/04/29/indian-court-orders-blocking-of-proton-mail/</link>
            <guid>43834942</guid>
            <pubDate>Tue, 29 Apr 2025 16:41:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/04/29/indian-court-orders-blocking-of-proton-mail/">https://techcrunch.com/2025/04/29/indian-court-orders-blocking-of-proton-mail/</a>, See on <a href="https://news.ycombinator.com/item?id=43834942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">A court in India has ordered the blocking of encrypted email provider Proton Mail across the country.</p>

<p>On Tuesday, the Karnataka High Court directed the Indian government to block Proton Mail, a popular email service known for its enhanced security, following a legal complaint filed by New Delhi-based M Moser Design Associates. The local firm alleged that its employees had received emails containing obscene and vulgar content sent via Proton Mail.</p>







<p>In a <a href="https://www.youtube.com/live/cnCfjfWGo_E?si=IP6J6aP31eX7EDkJ&amp;t=10419" target="_blank" rel="noreferrer noopener nofollow">Tuesday hearing streamed on YouTube</a>, Justice M Nagaprasanna ordered the Indian government to “block Proton Mail, bearing in mind the observations made in the course of the order,” under the Information Technology Act 2008.</p>

<p>In its <a href="https://karnatakajudiciary.kar.nic.in/newwebsite/casestatushck.php?params=UFdkQmZjM3lKOGY2TFZkZjkvNkVUdDJwWnVUZTFyOW9sSEgzYjdNcDNOZ0ZKY1JidGxnendDeUwxUWdmdlI2ZWpWbjFiWnpoNlNidEViaE5QS1NnK3c9PQ==" target="_blank" rel="noreferrer noopener nofollow">complaint</a> filed in January, the New Delhi-based firm called for the regulation or blocking of Proton Mail in India, as the email service reportedly refused to share details about the sender of the allegedly offensive emails, despite a police complaint.</p>

<p>Additional solicitor general Aravind Kamath, representing the Indian government, had earlier told the high court that the government might have a limited role in addressing the petitioner’s concerns and suggested criminal courts could seek the required information from Swiss authorities since the petitioner had made a complaint.</p>

<p>The block of Proton Mail has yet to take effect, based on TechCrunch’s checks of Proton Mail’s website in India. TechCrunch contacted Proton Mail for comment and will update this story if we hear back.</p>

<p>This is the latest legal tussle facing Proton Mail in India, its second ruling in as many years aimed at blocking the encrypted email service from operating in the country.</p>

<p>Last year, the police department of the southern state of Tamil Nadu had <a href="https://techcrunch.com/2024/02/16/india-may-block-proton-mail/" target="_blank" rel="noreferrer noopener">sought to block</a> Proton Mail after the email service was found to have been used for sending hoax bomb threats to local schools. The Indian government’s IT ministry reportedly notified internet providers to block Proton Mail at the request of law enforcement. However, the Swiss federal authorities <a href="https://proton.me/blog/india-block-proton-mail" target="_blank" rel="noreferrer noopener nofollow">intervened to prevent</a> the blocking of Proton Mail taking effect.</p>

<p>“Blocking access to Proton Mail simply prevents law-abiding citizens from communicating securely and does not prevent cybercriminals from sending threats with another email service, especially if the perpetrators are located outside of India,” Proton said at the time.</p>

<p>Nonetheless, in October 2024, the Delhi High Court <a href="https://indianexpress.com/article/cities/delhi/delhi-hc-questions-proton-mail-india-directs-mha-police-9660111/" target="_blank" rel="noreferrer noopener nofollow">asked</a> state police and the Indian government’s home ministry to investigate the alleged use of Proton Mail across the country. Kamath, the additional solicitor general, had assured the Karnataka High Court that he would examine the Delhi High Court’s observations on Proton Mail’s use in India.</p>
</div><div>
	
	
	
	

	
<div>
	<p>
		Jagmeet covers startups, tech policy-related updates, and all other major tech-centric developments from India for TechCrunch. He previously worked as a principal correspondent at NDTV. You can reach out to him at mail[at]journalistjagmeet[dot]com.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/jagmeet-singh/" data-event="button" href="https://techcrunch.com/author/jagmeet-singh/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox tab groups are here (571 pts)]]></title>
            <link>https://blog.mozilla.org/en/firefox/tab-groups-community/</link>
            <guid>43834101</guid>
            <pubDate>Tue, 29 Apr 2025 15:37:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/firefox/tab-groups-community/">https://blog.mozilla.org/en/firefox/tab-groups-community/</a>, See on <a href="https://news.ycombinator.com/item?id=43834101">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-78908">
  

  <div>
    
<p>What happens when 4,500 people ask for the same feature? At Firefox, we build it.</p>



<figure><video controls="" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/ENG-Tab-groups-animation-v101.mp4"></video><figcaption><em>Firefox tab groups now available</em></figcaption></figure>



<p>Tab groups have long been the most <a href="https://connect.mozilla.org/t5/ideas/native-tab-grouping-more-customizable-tab-bar/idi-p/303">requested idea</a> on <a href="https://connect.mozilla.org/">Mozilla Connect</a> – our community platform – and thanks to thousands of votes, comments and passionate feedback, it’s finally here. 🎉</p>



<p>But this is more than just a feature launch. It’s the story of what happens when community insight, real-world pain points, and a whole lot of curiosity come together.</p>



<h3><strong>A feature the community asked for, loud and clear</strong></h3>



<p>Just one day after Mozilla Connect quietly <a href="https://blog.mozilla.org/en/firefox/about-mozilla-connect/">launched</a> in March 2022, a <a href="https://connect.mozilla.org/t5/ideas/native-tab-grouping-more-customizable-tab-bar/idi-p/303">request</a> for tab groups appeared. We hadn’t even promoted the platform. There were no announcements. But the community found Mozilla Connect and rallied behind the request for tab groups.</p>



<p>“It’s still the number one most upvoted post on Mozilla Connect,” said Jon Siddoway, who helps surface user insights to Firefox teams. “Even when the feature was in beta, people were still voting for it and saying, ‘We want this.’”</p>



<p>At Mozilla, we work hard to make Firefox the best browser for you. Last year, we <a href="https://blog.mozilla.org/en/mozilla/heres-what-were-working-on-in-firefox/">shared</a> what we were working on – features that help you stay organized, like our handy sidebar, <a href="https://blog.mozilla.org/en/firefox/vertical-tabs-and-the-firefox-community/">vertical tabs</a> and tab groups. As we noted then, community feedback directly shaped what came next.&nbsp;</p>



<blockquote>
<p>“It’s still the number one most upvoted post on Mozilla Connect.” </p>
<cite>Jon Siddoway, product manager of Mozilla Connect</cite></blockquote>



<p>That early request kicked off a collaboration between the Firefox team and community. Before any code was written, Jon summarized comments, tracked trends across 64+ pages of feedback and brought key themes to the team.</p>



<p>That enthusiasm spilled into beta testing. Before the official <a href="https://connect.mozilla.org/t5/discussions/help-shape-the-future-of-tab-groups-in-firefox/m-p/89026">invite</a> to community members went out, many of them discovered the hidden toggle in the Nightly release, turned it on themselves, and started sharing how to use it. The team watched, learned and iterated from the sidelines.</p>



<h3><strong>Listening and learning from thousands of voices</strong></h3>



<p>Stefan Smagula, product manager for the tabs group feature, didn’t just skim the posts – he dove in.</p>



<p>“I read Mozilla Connect every day for the first month,” he said. “Sometimes the ideas confirmed what we were already thinking. Other times they were totally new and unexpected, like requests for nested tab groups.”</p>



<p>But with over 1,000 comments and many differing opinions, how do you make decisions?</p>



<blockquote>
<p>“Sometimes the ideas confirmed what we were already thinking. Other times they were totally new and unexpected.”</p>
<cite>Stefan Smagula, product manager at Firefox</cite></blockquote>



<p>“You try to get to the <em>underlying needs</em> behind each request,” Stefan explained. “Instead of just implementing one person’s idea, you look for the broader pattern — the thing that could help the most people.”</p>



<p>This approach helped shape a feature that balances flexibility with simplicity. With tab groups, you can drag and drop tabs into organized groups, label them by name or color, and stay focused. Whether you’re a <a href="https://blog.mozilla.org/en/firefox/firefox-tips/transform-firefox-into-minimalist-workspace/">minimalist</a> with 10 tabs or a power user juggling 10,000 (seriously — one of our colleagues does this), tab groups can help.</p>


<div>
<figure><img decoding="async" fetchpriority="high" width="1024" height="578" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2-1024x578.png" alt="Browser window showing Firefox's tab grouping feature. A 'Create tab group' pop-up is open, with 'Thailand Trip' entered as the group name and a purple color selected. Tabs for 'Thailand Trip,' 'Google Flights,' 'Hotels.com,' and a 'New Tab' are visible, along with existing tab groups labeled 'Work,' 'Reading,' and 'Shopping.'" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2-1024x578.png 1024w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2-300x169.png 300w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2-768x433.png 768w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2-1000x564.png 1000w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/04/EN2.png 1136w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>Keep it together — group your tabs by trip, work, or whatever you need!</em></figcaption></figure></div>


<p>“Tab groups aren’t just about decluttering,” Stefan said. “It’s about reclaiming your flow and finding focus again.”</p>



<p>It also reinforced the team’s belief that done is never truly done.</p>



<blockquote>
<p>“Tab groups aren’t just about decluttering. It’s about reclaiming your flow and finding focus again.”</p>
<cite>Stefan Smagula, product manager at Firefox</cite></blockquote>



<h3><strong>What’s next: Make tab groups smarter</strong></h3>



<p>Once early testers began using tab groups in <a href="https://www.mozilla.org/en-US/firefox/channel/desktop/">Firefox Nightly and Beta</a>, feedback kept rolling in – both on Mozilla Connect and in places like <a href="https://blog.mozilla.org/en/firefox/firefox-subreddit/">Reddit</a> and X, where Stefan scouts for feedback. Many users wanted less friction and more flow when managing their tabs, which inspired the team to explore the next step: having the browser help organize things automatically.&nbsp;</p>



<p>Now, the team is experimenting with smart tab groups, a new AI-powered feature that suggests names and groups based on the tabs you have open. Other browsers might send your tab info to the cloud, but Firefox keeps it on your device. Your tabs stay private and never leave your device.</p>



<p>“I used to have 30 windows open, each with 30 or 40 tabs. Smart tab groups changed the way I work. It made it easier to find what I need and resume tasks faster,” said Stefan.</p>



<p>It’s just the beginning of what’s possible when you pair smart tech with real human needs.</p>



<blockquote>
<p>“I used to have 30 windows open, each with 30 or 40 tabs. Smart tab groups changed the way I work. It made it easier to find what I need and resume tasks faster.”</p>
<cite>Stefan Smagula, product manager at Firefox</cite></blockquote>



<h3><strong>Thank you – and keep the ideas coming</strong></h3>



<p>This feature wouldn’t exist without you. Your upvotes, comments, ideas and testing helped bring it to life.</p>



<p>As Stefan put it: “It’s extremely motivating to know how many people want this. It makes the hard work easier and more meaningful.”</p>



<p>So if you’ve ever felt tab overload — or if you just want your browser to feel a bit more like your own — try out tab groups. Share what you love and what you’d change.</p>



<blockquote>
<p>“It’s extremely motivating to know how many people want this. It makes the hard work easier and more meaningful.”</p>
<cite>Stefan Smagula, product manager at Firefox</cite></blockquote>



<p>You can join the conversation anytime on <a href="https://connect.mozilla.org/">Mozilla Connect</a>. 💬</p>



<a href="https://www.mozilla.org/en-US/firefox/new/?utm_source=blog.mozilla.org&amp;utm_medium=referral&amp;utm_campaign=blog-nav">
  <p><img width="800" height="800" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2024/05/fx_website_meta-image_tips-tricks_alt-04-1-800x800.webp" alt="" decoding="async" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2024/05/fx_website_meta-image_tips-tricks_alt-04-1-800x800.webp 800w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2024/05/fx_website_meta-image_tips-tricks_alt-04-1-150x150.webp 150w" sizes="(max-width: 800px) 100vw, 800px">  </p>
  <div>
     <h3>Get the browser that puts your privacy first — and always has </h3>      <p><span>Download Firefox</span>   </p></div>
</a>
  </div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ArkFlow: High-performance Rust stream processing engine (147 pts)]]></title>
            <link>https://github.com/arkflow-rs/arkflow</link>
            <guid>43833310</guid>
            <pubDate>Tue, 29 Apr 2025 14:38:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/arkflow-rs/arkflow">https://github.com/arkflow-rs/arkflow</a>, See on <a href="https://news.ycombinator.com/item?id=43833310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ArkFlow</h2><a id="user-content-arkflow" aria-label="Permalink: ArkFlow" href="#arkflow"></a></p>
<p dir="auto">English | <a href="https://github.com/arkflow-rs/arkflow/blob/main/README_zh.md">中文</a></p>
<p dir="auto"><a href="https://github.com/arkflow-rs/arkflow/actions/workflows/rust.yml"><img src="https://github.com/arkflow-rs/arkflow/actions/workflows/rust.yml/badge.svg" alt="Rust"></a>
<a href="https://github.com/arkflow-rs/arkflow/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/c355f200ea90fddaa407b6eaab303663a669248ea3ca7b1fcf77dbe04ff5f48c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202.0-blue.svg"></a></p>
<p dir="auto"><a href="https://www.producthunt.com/posts/arkflow?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-arkflow" rel="nofollow"><img src="https://camo.githubusercontent.com/85bc9a37fb504dfdad5bf0aba5e7d0fe211288afd9a6a9c7a64e632f1bffbd0d/68747470733a2f2f6170692e70726f6475637468756e742e636f6d2f776964676574732f656d6265642d696d6167652f76312f66656174757265642e7376673f706f73745f69643d393432383034267468656d653d6c6967687426743d31373433313336323632333336" alt="ArkFlow - High-performance rust stream processing engine | Product Hunt" width="250" height="54" data-canonical-src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=942804&amp;theme=light&amp;t=1743136262336"></a></p>
<p dir="auto">High-performance Rust stream processing engine, providing powerful data stream processing capabilities, supporting
multiple input/output sources and processors.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>High Performance</strong>: Built on Rust and Tokio async runtime, offering excellent performance and low latency</li>
<li><strong>Multiple Data Sources</strong>: Support for Kafka, MQTT, HTTP, files, and other input/output sources</li>
<li><strong>Powerful Processing Capabilities</strong>: Built-in SQL queries, JSON processing, Protobuf encoding/decoding, batch
processing, and other processors</li>
<li><strong>Extensible</strong>: Modular design, easy to extend with new input, output, and processor components</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from Source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from Source" href="#building-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/arkflow-rs/arkflow.git
cd arkflow

# Build the project
cargo build --release

# Run tests
cargo test"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/arkflow-rs/arkflow.git
<span>cd</span> arkflow

<span><span>#</span> Build the project</span>
cargo build --release

<span><span>#</span> Run tests</span>
cargo <span>test</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>Create a configuration file <code>config.yaml</code>:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="logging:
  level: info
streams:
  - input:
      type: &quot;generate&quot;
      context: '{ &quot;timestamp&quot;: 1625000000000, &quot;value&quot;: 10, &quot;sensor&quot;: &quot;temp_1&quot; }'
      interval: 1s
      batch_size: 10

    pipeline:
      thread_num: 4
      processors:
        - type: &quot;json_to_arrow&quot;
        - type: &quot;sql&quot;
          query: &quot;SELECT * FROM flow WHERE value >= 10&quot;

    output:
      type: &quot;stdout&quot;
    error_output:
      type: &quot;stdout&quot;"><pre><span>logging</span>:
  <span>level</span>: <span>info</span>
<span>streams</span>:
  - <span>input</span>:
      <span>type</span>: <span><span>"</span>generate<span>"</span></span>
      <span>context</span>: <span><span>'</span>{ "timestamp": 1625000000000, "value": 10, "sensor": "temp_1" }<span>'</span></span>
      <span>interval</span>: <span>1s</span>
      <span>batch_size</span>: <span>10</span>

    <span>pipeline</span>:
      <span>thread_num</span>: <span>4</span>
      <span>processors</span>:
        - <span>type</span>: <span><span>"</span>json_to_arrow<span>"</span></span>
        - <span>type</span>: <span><span>"</span>sql<span>"</span></span>
          <span>query</span>: <span><span>"</span>SELECT * FROM flow WHERE value &gt;= 10<span>"</span></span>

    <span>output</span>:
      <span>type</span>: <span><span>"</span>stdout<span>"</span></span>
    <span>error_output</span>:
      <span>type</span>: <span><span>"</span>stdout<span>"</span></span></pre></div>
<ol start="2" dir="auto">
<li>Run ArkFlow:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/arkflow --config config.yaml"><pre>./target/release/arkflow --config config.yaml</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Guide</h2><a id="user-content-configuration-guide" aria-label="Permalink: Configuration Guide" href="#configuration-guide"></a></p>
<p dir="auto">ArkFlow uses YAML format configuration files, supporting the following main configuration items:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Top-level Configuration</h3><a id="user-content-top-level-configuration" aria-label="Permalink: Top-level Configuration" href="#top-level-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="logging:
  level: info  # Log level: debug, info, warn, error

streams: # Stream definition list
  - input:      # Input configuration
    # ...
    pipeline:   # Processing pipeline configuration
    # ...
    output:     # Output configuration
    # ...
    error_output: # Error output configuration
    # ...
    buffer:     # Buffer configuration
    # ... "><pre><span>logging</span>:
  <span>level</span>: <span>info  </span><span><span>#</span> Log level: debug, info, warn, error</span>

<span>streams</span>: <span><span>#</span> Stream definition list</span>
  - <span>input</span>:      <span><span>#</span> Input configuration</span>
    <span><span>#</span> ...</span>
    <span>pipeline</span>:   <span><span>#</span> Processing pipeline configuration</span>
    <span><span>#</span> ...</span>
    <span>output</span>:     <span><span>#</span> Output configuration</span>
    <span><span>#</span> ...</span>
    <span>error_output</span>: <span><span>#</span> Error output configuration</span>
    <span><span>#</span> ...</span>
    <span>buffer</span>:     <span><span>#</span> Buffer configuration</span>
    <span><span>#</span> ... </span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Input Components</h3><a id="user-content-input-components" aria-label="Permalink: Input Components" href="#input-components"></a></p>
<p dir="auto">ArkFlow supports multiple input sources:</p>
<ul dir="auto">
<li><strong>Kafka</strong>: Read data from Kafka topics</li>
<li><strong>MQTT</strong>: Subscribe to messages from MQTT topics</li>
<li><strong>HTTP</strong>: Receive data via HTTP</li>
<li><strong>File</strong>: Reading data from files(Csv,Json, Parquet, Avro, Arrow) using SQL</li>
<li><strong>Generator</strong>: Generate test data</li>
<li><strong>Database</strong>: Query data from databases(MySQL, PostgreSQL, SQLite, Duckdb)</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="input:
  type: kafka
  brokers:
    - localhost:9092
  topics:
    - test-topic
  consumer_group: test-group
  client_id: arkflow
  start_from_latest: true"><pre><span>input</span>:
  <span>type</span>: <span>kafka</span>
  <span>brokers</span>:
    - <span>localhost:9092</span>
  <span>topics</span>:
    - <span>test-topic</span>
  <span>consumer_group</span>: <span>test-group</span>
  <span>client_id</span>: <span>arkflow</span>
  <span>start_from_latest</span>: <span>true</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Processors</h3><a id="user-content-processors" aria-label="Permalink: Processors" href="#processors"></a></p>
<p dir="auto">ArkFlow provides multiple data processors:</p>
<ul dir="auto">
<li><strong>JSON</strong>: JSON data processing and transformation</li>
<li><strong>SQL</strong>: Process data using SQL queries</li>
<li><strong>Protobuf</strong>: Protobuf encoding/decoding</li>
<li><strong>Batch Processing</strong>: Process messages in batches</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipeline:
  thread_num: 4
  processors:
    - type: json_to_arrow
    - type: sql
      query: &quot;SELECT * FROM flow WHERE value >= 10&quot;"><pre><span>pipeline</span>:
  <span>thread_num</span>: <span>4</span>
  <span>processors</span>:
    - <span>type</span>: <span>json_to_arrow</span>
    - <span>type</span>: <span>sql</span>
      <span>query</span>: <span><span>"</span>SELECT * FROM flow WHERE value &gt;= 10<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Output Components</h3><a id="user-content-output-components" aria-label="Permalink: Output Components" href="#output-components"></a></p>
<p dir="auto">ArkFlow supports multiple output targets:</p>
<ul dir="auto">
<li><strong>Kafka</strong>: Write data to Kafka topics</li>
<li><strong>MQTT</strong>: Publish messages to MQTT topics</li>
<li><strong>HTTP</strong>: Send data via HTTP</li>
<li><strong>Standard Output</strong>: Output data to the console</li>
<li><strong>Drop</strong>: Discard data</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="output:
  type: kafka
  brokers:
    - localhost:9092
  topic: 
    type: value
    value: test-topic
  client_id: arkflow-producer"><pre><span>output</span>:
  <span>type</span>: <span>kafka</span>
  <span>brokers</span>:
    - <span>localhost:9092</span>
  <span>topic</span>: 
    <span>type</span>: <span>value</span>
    <span>value</span>: <span>test-topic</span>
  <span>client_id</span>: <span>arkflow-producer</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Error Output Components</h3><a id="user-content-error-output-components" aria-label="Permalink: Error Output Components" href="#error-output-components"></a></p>
<p dir="auto">ArkFlow supports multiple error output targets:</p>
<ul dir="auto">
<li><strong>Kafka</strong>: Write error data to Kafka topics</li>
<li><strong>MQTT</strong>: Publish error messages to MQTT topics</li>
<li><strong>HTTP</strong>: Send error data via HTTP</li>
<li><strong>Standard Output</strong>: Output error data to the console</li>
<li><strong>Drop</strong>: Discard error data</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="error_output:
  type: kafka
  brokers:
    - localhost:9092
  topic: 
    type: value
    value: error-topic
  client_id: error-arkflow-producer"><pre><span>error_output</span>:
  <span>type</span>: <span>kafka</span>
  <span>brokers</span>:
    - <span>localhost:9092</span>
  <span>topic</span>: 
    <span>type</span>: <span>value</span>
    <span>value</span>: <span>error-topic</span>
  <span>client_id</span>: <span>error-arkflow-producer</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Buffer Components</h3><a id="user-content-buffer-components" aria-label="Permalink: Buffer Components" href="#buffer-components"></a></p>
<p dir="auto">ArkFlow provides buffer capabilities to handle backpressure and temporary storage of messages:</p>
<ul dir="auto">
<li><strong>Memory Buffer</strong>: Memory buffer, for high-throughput scenarios and window aggregation</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="buffer:
  type: memory
  capacity: 10000  # Maximum number of messages to buffer
  timeout: 10s  # Maximum time to buffer messages"><pre><span>buffer</span>:
  <span>type</span>: <span>memory</span>
  <span>capacity</span>: <span>10000</span>  <span><span>#</span> Maximum number of messages to buffer</span>
  <span>timeout</span>: <span>10s</span>  <span><span>#</span> Maximum time to buffer messages</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Kafka to Kafka Data Processing</h3><a id="user-content-kafka-to-kafka-data-processing" aria-label="Permalink: Kafka to Kafka Data Processing" href="#kafka-to-kafka-data-processing"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="streams:
  - input:
      type: kafka
      brokers:
        - localhost:9092
      topics:
        - test-topic
      consumer_group: test-group

    pipeline:
      thread_num: 4
      processors:
        - type: json_to_arrow
        - type: sql
          query: &quot;SELECT * FROM flow WHERE value > 100&quot;

    output:
      type: kafka
      brokers:
        - localhost:9092
      topic: processed-topic"><pre><span>streams</span>:
  - <span>input</span>:
      <span>type</span>: <span>kafka</span>
      <span>brokers</span>:
        - <span>localhost:9092</span>
      <span>topics</span>:
        - <span>test-topic</span>
      <span>consumer_group</span>: <span>test-group</span>

    <span>pipeline</span>:
      <span>thread_num</span>: <span>4</span>
      <span>processors</span>:
        - <span>type</span>: <span>json_to_arrow</span>
        - <span>type</span>: <span>sql</span>
          <span>query</span>: <span><span>"</span>SELECT * FROM flow WHERE value &gt; 100<span>"</span></span>

    <span>output</span>:
      <span>type</span>: <span>kafka</span>
      <span>brokers</span>:
        - <span>localhost:9092</span>
      <span>topic</span>: <span>processed-topic</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generate Test Data and Process</h3><a id="user-content-generate-test-data-and-process" aria-label="Permalink: Generate Test Data and Process" href="#generate-test-data-and-process"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="streams:
  - input:
      type: &quot;generate&quot;
      context: '{ &quot;timestamp&quot;: 1625000000000, &quot;value&quot;: 10, &quot;sensor&quot;: &quot;temp_1&quot; }'
      interval: 1ms
      batch_size: 10000

    pipeline:
      thread_num: 4
      processors:
        - type: &quot;json_to_arrow&quot;
        - type: &quot;sql&quot;
          query: &quot;SELECT count(*) FROM flow WHERE value >= 10 group by sensor&quot;

    output:
      type: &quot;stdout&quot;"><pre><span>streams</span>:
  - <span>input</span>:
      <span>type</span>: <span><span>"</span>generate<span>"</span></span>
      <span>context</span>: <span><span>'</span>{ "timestamp": 1625000000000, "value": 10, "sensor": "temp_1" }<span>'</span></span>
      <span>interval</span>: <span>1ms</span>
      <span>batch_size</span>: <span>10000</span>

    <span>pipeline</span>:
      <span>thread_num</span>: <span>4</span>
      <span>processors</span>:
        - <span>type</span>: <span><span>"</span>json_to_arrow<span>"</span></span>
        - <span>type</span>: <span><span>"</span>sql<span>"</span></span>
          <span>query</span>: <span><span>"</span>SELECT count(*) FROM flow WHERE value &gt;= 10 group by sensor<span>"</span></span>

    <span>output</span>:
      <span>type</span>: <span><span>"</span>stdout<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ArkFlow Plugin</h2><a id="user-content-arkflow-plugin" aria-label="Permalink: ArkFlow Plugin" href="#arkflow-plugin"></a></p>
<p dir="auto"><a href="https://github.com/arkflow-rs/arkflow-plugin-examples">ArkFlow Plugin Examples</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">ArkFlow is licensed under the <a href="https://github.com/arkflow-rs/arkflow/blob/main/LICENSE">Apache License 2.0</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">Discord: <a href="https://discord.gg/CwKhzb8pux" rel="nofollow">https://discord.gg/CwKhzb8pux</a></p>
<p dir="auto">If you like or are using this project to learn or start your solution, please give it a star⭐. Thanks!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon RDS for PostgreSQL 17.4 (320 pts)]]></title>
            <link>https://jepsen.io/analyses/amazon-rds-for-postgresql-17.4</link>
            <guid>43833195</guid>
            <pubDate>Tue, 29 Apr 2025 14:30:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/amazon-rds-for-postgresql-17.4">https://jepsen.io/analyses/amazon-rds-for-postgresql-17.4</a>, See on <a href="https://news.ycombinator.com/item?id=43833195">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://aws.amazon.com/rds/postgresql/">Amazon RDS for PostgreSQL</a> is an Amazon Web Services (AWS) service which provides managed instances of the <a href="https://www.postgresql.org/">PostgreSQL</a> database. We show that Amazon RDS for PostgreSQL multi-AZ clusters violate <a href="https://jepsen.io/consistency/models/snapshot-isolation">Snapshot Isolation</a>, the strongest consistency model supported across all endpoints. Healthy clusters occasionally allow <a href="https://jepsen.io/consistency/phenomena/long-fork">Long Fork</a> and other <a href="https://jepsen.io/consistency/phenomena/g-nonadjacent">G-nonadjacent</a> cycles. These phenomena occurred in every version tested, from 13.15 to 17.4. Amazon RDS for PostgreSQL may instead provide <a href="https://scispace.com/pdf/transactional-storage-for-geo-replicated-systems-2j5mhrj29h.pdf">Parallel Snapshot Isolation</a>. This work was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p><a href="https://www.postgresql.org/">PostgreSQL</a> is a popular open source general-purpose SQL database. It uses multiversion concurrency control (MVCC) to provide <a href="https://www.postgresql.org/docs/17/transaction-iso.html">three levels of transaction isolation</a>. PostgreSQL’s “Read Uncommitted” and “Read Committed” are both <a href="https://jepsen.io/consistency/models/read-committed">Read Committed</a>. The “Repeatable Read” level actually provides <a href="https://jepsen.io/consistency/models/snapshot-isolation">Snapshot Isolation</a>, not <a href="https://jepsen.io/consistency/models/repeatable-read">Repeatable Read</a>. “Serializable” provides <a href="https://jepsen.io/consistency/models/serializable">Serializability</a>.</p>
<p><a href="https://aws.amazon.com/rds/postgresql/">Amazon RDS for PostgreSQL</a> is an AWS service which provides managed PostgreSQL clusters. RDS automates provisioning, storage management, replication, backups, upgrades, and more. <a href="https://aws.amazon.com/rds/features/multi-az/">Multi-AZ deployments</a> distribute database nodes across multiple availability zones, reducing the probability of correlated failure. RDS uses synchronous replication to ensure that transactions are durable both on primary and (at least one) secondary instances before acknowledging.</p>
<p>From a user perspective, Amazon RDS for PostgreSQL provides a pair of URLs which speak the PostgreSQL wire protocol: a primary endpoint for read-write transactions, and a reader endpoint for read-only transactions. The primary endpoint supports all PostgreSQL isolation levels, while secondaries do not support Serializable. The strongest level supported across all nodes is therefore Snapshot Isolation (which PostgreSQL terms “Repeatable Read”).</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We adapted Jepsen’s <a href="https://github.com/jepsen-io/postgres/tree/225203dd64ad5e5e4fe481ccb8b180b7d0d99f9d/postgres">test library for PostgreSQL</a> for use with Amazon RDS for PostgreSQL with a <a href="https://github.com/jepsen-io/postgres/tree/225203dd64ad5e5e4fe481ccb8b180b7d0d99f9d/rds">small wrapper program</a>. For each round of tests, we <a href="https://github.com/jepsen-io/rds/blob/13cada8381b7cde00bca3adfb005a66613656039/src/jepsen/rds.clj#L284-L377">provisioned an RDS cluster</a> using AWS’s <a href="https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBCluster.html"><code>CreateDBCluster</code></a> API, using <code>gp3</code> storage and <code>db.m6id.large</code> instances. We then launched a single EC2 node to run our tests, and provided it with the main and read-only endpoints of the RDS cluster. We performed no fault injection, and triggered no failovers.</p>
<p>As in <a href="https://jepsen.io/analyses/postgresql-12.3#test-design">our previous work on PostgreSQL</a>, our <a href="https://github.com/jepsen-io/postgres/blob/225203dd64ad5e5e4fe481ccb8b180b7d0d99f9d/postgres/src/jepsen/postgres/workload/append.clj">primary workload</a> consisted of transactions over lists of unique integers. We stored each list in a single row, encoded as a <code>TEXT</code> field of comma-separated values. Transactions could either read a list by primary key, or append a unique integer to a list using <code>CONCAT</code>. This workload allowed our <a href="https://github.com/jepsen-io/elle">Elle checker</a> to verify a variety of isolation levels, mainly by inferring dataflow dependencies between transactions and finding cycles in the resulting graph.</p>
<h2 data-number="3" id="results"> Results</h2>
<p>Under healthy conditions, with moderate concurrency, Amazon RDS for PostgreSQL 17.4 exhibited <a href="https://jepsen.io/consistency/phenomena/g-nonadjacent">G-nonadjacent</a> cycles every few minutes. Consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/amazon-rds-for-postgresql-17.4/20250406T172435-long-fork.zip">this two-minute test run</a>, which performed approximately 150 write transactions per second, along with 1600 read-only transactions per second. It contains the following cycle of four transactions:</p>

<p>From top to bottom, call these transactions <span><em>T</em><sub>1</sub></span>, <span><em>T</em><sub>2</sub></span>, <span><em>T</em><sub>3</sub></span>, and <span><em>T</em><sub>4</sub></span>. <span><em>T</em><sub>1</sub></span> appended 9 to row 89, resulting in the list <code>[4 9]</code>, which <span><em>T</em><sub>2</sub></span> observed. <span><em>T</em><sub>3</sub></span> appended 11 to row 90, resulting in the list <code>[11]</code>. That version was overwritten by <span><em>T</em><sub>4</sub></span>, which appended <code>3</code> to row <code>90</code>, and read the resulting list <code>[11, 3]</code>. While <span><em>T</em><sub>2</sub></span> observed <span><em>T</em><sub>1</sub></span>’s append to row 89, it failed to observe <span><em>T</em><sub>3</sub></span>’s append to row 90. Symmetrically, <span><em>T</em><sub>4</sub></span> observed <span><em>T</em><sub>3</sub></span>’s append to row 90, but failed to observe <span><em>T</em><sub>1</sub></span>’s append to 89.</p>
<p>Since this cycle includes <a href="https://jepsen.io/consistency/dependencies">read-write dependencies</a> which are not adjacent to each other, this cycle is G-nonadjacent, a violation of Snapshot Isolation. This behavior should not occur in standard PostgreSQL at “Repeatable Read” and we have not observed it there.</p>
<p>To understand why this cycle is illegal, recall that in <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Snapshot Isolation</a>, every transaction (apparently) operates on a snapshot of the database taken at some start timestamp <span><em>s</em></span>. That transaction’s effects are made visible to others at some later commit timestamp <span><em>c</em></span>. In order for <span><em>T</em><sub>2</sub></span> to read <span><em>T</em><sub>1</sub></span>’s append, its start timestamp must have followed <span><em>T</em><sub>1</sub></span>’s commit timestamp: <span><em>c</em><sub>1</sub> &lt; <em>s</em><sub>2</sub></span>. Since <span><em>T</em><sub>2</sub></span> did not observe <span><em>T</em><sub>3</sub></span>’s append, <span><em>s</em><sub>2</sub></span> &lt; <span><em>c</em><sub>3</sub></span>. Since <span><em>T</em><sub>4</sub></span> overwrote (and observed) <span><em>T</em><sub>3</sub></span>, <span><em>c</em><sub>3</sub> &lt; <em>s</em><sub>4</sub></span>. But <span><em>T</em><sub>4</sub></span> did not observe <span><em>T</em><sub>1</sub></span>’s append, so <span><em>s</em><sub>4</sub></span> &lt; <span><em>c</em><sub>1</sub></span>. We have a contradiction! There is no way these timestamps can each precede each other.</p>
<p>This cycle is also an example of <a href="https://jepsen.io/consistency/phenomena/long-fork">Long Fork</a>. The first and second transactions compose one logical fork of the state. The third and fourth comprise a second. Each fork updates a different row, but neither fork observes the other’s effects. Curiously, we did <em>not</em> observe <a href="https://jepsen.io/consistency/phenomena/a5b">Short Fork</a>, also known as Write Skew. This suggests that Amazon RDS for PostgreSQL might provide <a href="https://www.cs.princeton.edu/courses/archive/fall13/cos518/papers/walter.pdf">Parallel Snapshot Isolation</a>, a slightly weaker consistency model.</p>
<p>We observed a variety of G-nonadjacent anomalies, including those linked only by write-read edges, as well as several with more than four transactions. They occurred in every PostgreSQL version we tested, from 13.15 (the oldest version which AWS supported) to 17.4 (the newest).</p>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>From the presence of Long Fork and other G-nonadjacent cycles, we conclude that Amazon RDS for PostgreSQL multi-AZ clusters do not ensure Snapshot Isolation. Instead, they may provide Parallel Snapshot Isolation, a slightly weaker model. In this respect Amazon RDS for PostgreSQL multi-AZ clusters offer weaker safety semantics than a single-node PostgreSQL system, which, <a href="https://jepsen.io/analyses/postgresql-12.3">in our previous testing</a>, appeared to provide Strong Snapshot Isolation.</p>
<p>Users of Amazon RDS for PostgreSQL may wish to examine their transaction structures with an eye towards Long Fork, or design experiments to verify whether their intended invariants are preserved. A read transactions may disagree with other transactions as to the order in which transactions were executed. Since these anomalies appear to involve queries against read-only secondaries, it may be possible to recover Snapshot Isolation by only using the writer endpoint, or ensuring that every safety-critical transaction includes at least one write.</p>
<p>This report is the product of a cursory exploration—we have not investigated Amazon RDS for PostgreSQL behavior in detail. As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<p><em>Our thanks to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This work was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[White House slams Amazon tariff price display "hostile and political" (153 pts)]]></title>
            <link>https://www.axios.com/2025/04/29/tariffs-amazon-prime-day-sellers-report</link>
            <guid>43832588</guid>
            <pubDate>Tue, 29 Apr 2025 13:49:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2025/04/29/tariffs-amazon-prime-day-sellers-report">https://www.axios.com/2025/04/29/tariffs-amazon-prime-day-sellers-report</a>, See on <a href="https://news.ycombinator.com/item?id=43832588">Hacker News</a></p>
Couldn't get https://www.axios.com/2025/04/29/tariffs-amazon-prime-day-sellers-report: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Performance optimization is hard because it's fundamentally a brute-force task (237 pts)]]></title>
            <link>https://purplesyringa.moe/blog/why-performance-optimization-is-hard-work/</link>
            <guid>43831705</guid>
            <pubDate>Tue, 29 Apr 2025 12:29:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://purplesyringa.moe/blog/why-performance-optimization-is-hard-work/">https://purplesyringa.moe/blog/why-performance-optimization-is-hard-work/</a>, See on <a href="https://news.ycombinator.com/item?id=43831705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><time>April 29, 2025</time><a href="https://www.reddit.com/r/programming/comments/1kam686/why_performance_optimization_is_hard_work/"> Reddit</a></p><p>I’m not talking about skill, knowledge, or convincing a world focused on radical acceleration that optimization is necessary. Performance optimization is hard because it’s fundamentally a brute-force task, and there’s nothing you can do about it.</p><p>This post is a bit of a rant on my frustrations with code optimization. I’ll also try to give actionable advice, which I hope enchants your experience.</p><p>Certain optimizations can only work together, while others lead to pessimizations when combined. To be an expert means to know what optimization avenues exist; to be a master means to know which ones to choose.</p><p>I have a post on integer formatting in the works, covering a very particular algorithm design – and I <em>still</em> haven’t finished it because there’s like five different choices to make, I have no idea how they impact each other, and I need to analyze <eq><math><msup><mn>2</mn><mn>5</mn></msup></math></eq> variants to claim which one’s the best in conscience. Several of my projects are similarly stuck because I don’t have the willpower to implement a dozen combinations.</p><p>Pruning “obviously” suboptimal approaches is all but a heuristic. I like to think I’m more in tune with an x86-64 CPU than most people, and it still manages to surprise me from time to time. Dumb algorithms can become more applicable due to vectorization, smart code can fail due to <a href="https://en.wikipedia.org/wiki/Branch_predictor">branch misprediction</a> or <a href="https://en.wikipedia.org/wiki/Memory_disambiguation#Store_to_load_forwarding">store-to-load forwarding</a> gone wrong.</p><p>Optimization takes a lot of trial and error. I dislike the “intuition doesn’t work, profile your code” mantra because it seemingly says profiling is a viable replacement for theoretical calculations, which it isn’t. But I can’t argue that profiling is avoidable. I often joke that <code>perf report</code> is my go-to disassembler.</p><p>Worse yet, you can’t trust “obviously” good code either. In <a href="https://purplesyringa.moe/blog/the-ram-myth/">a previous post</a>, I optimized a single linear pass by replacing it with a superlinear sort. This is by no means a unique experience: just yesterday, I saw someone optimize best-of-class <a href="https://en.wikipedia.org/wiki/Barrett_reduction">Barrett reduction</a> by dividing the numbers as <code>double</code>s, rounding them, and computing the reminder from the quotient. It’s so stupid that it can’t possibly work, yet it does.</p><p>The good news is that the work here can be split among multiple people trying different approaches. Open-source projects in particular benefit from this, because contributors typically have different strengths and focus on different ideas. Try to reuse work by consulting your teammates or reading about others’ experiences in solving similar tasks.</p><p>A variation on this is algorithms where a <em>cut-off boundary</em> is present. You no longer choose whether to apply an optimization: you also need to select parameters via more trial and error. For example:</p><ul><li>Hybrid sorting algorithms can switch between different implementations due to high big-O constants,</li><li><a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> can switch between recursive and iterative approaches to better utilize processor cache.</li><li>Depending on data density, the optimal set structure might be bitsets, hash sets, or complementary hash sets.</li></ul><p>Modifying either of the alternative algorithms requires rebenchmarking to update the optimal boundary. Small modifications here can lead to <em>drastic</em> end performance changes due to interactions with CPU cache, branch and memory access prediction, the discrete nature of recursive cut-offs, and floating-point precision (for <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm#Details">big integer multiplication via FFT</a>). Forgetting to rebenchmark and abandoning a prospective approach can easily leave <eq><math><mrow><mn>2</mn><mo>×</mo></mrow></math></eq> performance on the table.</p><p>For another example, consider a program that executes <eq><math><mi>n</mi></math></eq> times either action <eq><math><mi>A</mi></math></eq> or <eq><math><mi>B</mi></math></eq> depending on probability <eq><math><mi>p</mi></math></eq>. If <eq><math><mi>p</mi></math></eq> is far from <eq><math><mfrac><mn>1</mn><mn>2</mn></mfrac></math></eq>, branch prediction means it’s better to implement the switch with an <code>if</code>; if <eq><math><mi>p</mi></math></eq> is close to <eq><math><mfrac><mn>1</mn><mn>2</mn></mfrac></math></eq>, branch prediction will fail and a branchless approach will work better. Not only does the relative performance of <eq><math><mi>A</mi></math></eq> and <eq><math><mi>B</mi></math></eq> matter here, but the cost of branch misprediction matters as well, and that might depend not only on the CPU but on the precise code executed.</p><p>Ideally, you’d have a test bench that plots graphs and finds optimal parameter values automatically, even though getting this working can be draining. This way, running checks all the time becomes cheap and emotionally easier. Even if it takes half an hour, you can still work on something else in parallel.</p><p>The worst example of incompatible optimizations is those that fail due to external constraints.</p><p>One example is when two <a href="https://en.wikipedia.org/wiki/Lookup_table">LUTs</a> don’t fit in cache together, but do individually. You can sometimes fix this by splitting the computation into multiple passes, where each pass only needs to access helper data that does fit into cache. This does not necessarily mean two passes over <em>all</em> data, consuming <eq><math><mrow><mn>2</mn><mo>×</mo></mrow></math></eq> memory bandwidth – you can chunk the data and apply two passes on a chunk, which increases performance if the chunk fits into, say, L3. But sometimes that doesn’t work, and then I bash my head against the wall.</p><p>Register pressure is even worse because that is only a problem because of the ISA, not the <a href="https://en.wikipedia.org/wiki/Microarchitecture">microarchitecture</a>. The hardware has enough registers, they just aren’t exposed to user code. You can try to split data between general-purpose registers and vector registers, and that works as long as you seldom cross the GPR-SIMD boundary, but at that point, you might as well <a href="https://github.com/docker/cli/issues/267#issuecomment-695149477">change your profession</a>.</p><p>It doesn’t have to be that way. <a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGAs</a> enable you to design your own hardware (kind of, anyway), and alternative approaches like <a href="https://en.wikipedia.org/wiki/Interaction_nets">interaction nets</a> have a chance to make software-specified operations as optimal as operations that are usually implemented in hardware. But that’s not the world we live in, no, we live in the world where Intel keeps introducing useful instructions to AVX-512 only to abandon them later, so I need to choose between a CPU with <code>vp2intersect</code> or with <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">FP16</a>. So not only do you have to benchmark different code, you also have to test it on different CPUs to decide which EC2 instance to deploy it on.</p><p>The only advice I have for this is to try to achieve the best possible result, even if it’s worse than the theoretical optimum. Reduce the size of one of the LUTs by moving some calculations to runtime, rewrite a chunk of code in assembly to manage registers better, and when all else fails, accept that you have to make a choice.</p><p>“Compilers are smarter than humans” is a common mantra. It couldn’t be further from the truth. Any developer can see that the following two snippets are (supposed to be) equivalent:</p><pre><code><span>let</span> <span>condition1</span> = HashSet::<span>from</span>([a, b]).<span>contains</span>(&amp;c);
<span>let</span> <span>condition2</span> = a == c || b == c;
</code></pre><p>But compilers aren’t going to optimize the former into the latter (<a href="https://4comprehension.com/the-curious-case-of-jdk9-immutable-collections/">JVM’s JIT, in some cases, excluded</a>). They don’t reason in abstractions, and they certainly don’t reason in <em>your</em> auxiliary abstractions. This doesn’t just apply to high-level code: LLVM <a href="https://godbolt.org/z/j3ehhr3KT">does not even understand</a> that bitwise AND is an intersection.</p><p>No, compilers excel at something different from optimization: they turn higher-level languages into zero-cost abstractions, but there’s no ingenuity. Compilers are optimal transpilers – barring a few exceptions, they codegen exactly what you wrote in the source. They allow you to write assembly with the syntax and capabilities of Rust or C++, but don’t you dare forget that the <code>arr.map(|x| x / c)</code> you wrote will invoke <code>idiv</code> without performing obvious <a href="https://github.com/ridiculousfish/libdivide">libdivide</a>-style precalculations.</p><p>Sometimes I wonder if <code>-O2</code> should be renamed to <code>-fzero-cost-abstractions</code>.</p><p>This might make it sound like I’m arguing that compilers are only good at plumbing, but they aren’t even good at that. For example, they can be terrible at register allocation of all things. If a rarely executed chunk of code needs many registers, GCC satisfies that need by <a href="https://godbolt.org/z/53o1vdsfj">spilling variables accessed by the hot path</a> <em>on every iteration</em>, not only on entry to cold path. Clang handles this simple example better but fails in more complicated cases.</p><p>The lesson is never to trust the compiler blindly. Always check the disassembly, consult an instruction-level profiler like <code>perf</code>, and don’t be afraid to use this information to nudge the compiler to do the right thing if it leads to tangible improvements.</p><p>Despite obvious shortcomings, compilers don’t allow you to correct them on things they get wrong. There is no way to provide both optimized assembly and equivalent C code and let the compiler use the former in the general case and the latter in special cases. Custom calling conventions are mostly unsupported, and so is choosing between branchless and branchy code and any other assembly tricks. There are intrinsics, but LLVM and rustc <em>still</em> try to be smart and rewrite them, which sometimes causes pessimizations, leaving no alternative but to add an optimization barrier.</p><p><a href="https://egraphs-good.github.io/">e-graphs</a>, as popularized by <a href="https://cranelift.dev/">Cranelift</a>, try to tackle this problem, but to my knowledge, there hasn’t been much success in this field. I’m still hopeful, though.</p><p>For x86 processors, <a href="https://uops.info/table.html">uops.info</a> provides timing and port information for each instruction and many Intel and AMD CPUs. <a href="https://www.agner.org/optimize/">Agner Fog</a> wrote a manual on optimization for x86 processors and publishes his own tables. <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html">Intel Software Developer’s Manual</a> contains more than 5000 pages documenting not only the instruction set but many internal workings of their CPUs as well.</p><p>Apple Silicon has <em>nothing</em> like that. I have no goddamn idea how to work with M1+ processors. There’s <a href="https://dn721600.ca.archive.org/0/items/apple-silicon-cpu-optimization-guide/Apple-Silicon-CPU-Optimization-Guide.pdf">Apple Silicon CPU Optimization Guide</a>, which contains only 169 pages and reads like something people would write for novices, not experts. It reads like a tutorial you might find on HN, not something I would be interested in. It contains estimates of latencies and throughputs for some categories of instructions, but there’s frustratingly little tabular data, and it doesn’t mention <a href="https://stackoverflow.com/questions/56413517/what-is-instruction-fusion-in-contemporary-x86-processors">uop fusion</a> or provide port information. <a href="https://dougallj.github.io/applecpu/firestorm.html">Dougall Johnson’s research</a> is immensely valuable but only covers M1, not newer CPUs, and it still doesn’t answer many questions.</p><p>Even <a href="https://github.com/swiftlang/llvm-project/tree/next/llvm/lib/Target/AArch64">Apple’s LLVM fork</a> lacks scheduling annotations for Apple Silicon. How am I supposed to write efficient code when Apple doesn’t bother to tune their own compiler? Optimizing code for such a platform is 90% reverse engineering and 10% writing meaningful code – and writing meaningful code is already hard.</p><p>The right fix for this is to commit intellectual property theft, but I’m not allowed to say that, so I won’t. Oops.</p><p>Performance optimization is hard because you have to:</p><ul><li>Explore dozens of cases manually without losing your mind.</li><li>Iterate with inadequate tooling. (Profilers and <a href="https://llvm.org/docs/CommandGuide/llvm-mca.html">MCA</a> are useful, but they’re still toys that can’t match the underlying complexity.)</li><li><s>Jam squares into round holes until they fit.</s> Merge incompatible optimizations.</li><li>Deal with both corporate greed and cultural apathy.</li></ul><p>It’s not easy by any means, but it’s still something I enjoy doing, even though people often consider anything short of radical improvements a waste of time. To me, a 10% optimization is a form of art, but it’s not just that. Small improvements compound and help form a better user experience, even if no single optimization seems valuable on its own – much like improving data transfer rates has led to structural changes in how we process and utilize information.</p><p>Optimizations save time, and time is the one resource people don’t get enough of.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Programming languages should have a tree traversal primitive (202 pts)]]></title>
            <link>https://blog.tylerglaiel.com/p/programming-languages-should-have</link>
            <guid>43831628</guid>
            <pubDate>Tue, 29 Apr 2025 12:23:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.tylerglaiel.com/p/programming-languages-should-have">https://blog.tylerglaiel.com/p/programming-languages-should-have</a>, See on <a href="https://news.ycombinator.com/item?id=43831628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There should be a control flow construct in programming languages that can handle tree-like traversal in a nice way, similar to how for/foreach loops can handle linear traversal. It's a bit of a missing gap in the current set of control flow constructs most languages these days have settled on. Its a thing I end up having to do *all the time* and it seems like there should be some shortcuts for it.</p><p>I posted a thought about this recently and was thinking about how I would want something like that to look like. I can't find anything similar in any other languages. I'm most familiar with C++, so my sample code here is going to be C++-ish, but its general enough that you could fit it into basically anything else. I'm not a language design expert, so this is more of a loose idea than a formal proposal. </p><pre><code>for_tree(Node* N = mytreeroot; N != NULL; N : {N-&gt;left, N-&gt;right}){
	print(N-&gt;value);
}</code></pre><p>with the specifics removed, this is *almost* the same syntax as an existing for loop</p><pre><code>for_tree(init; condition; branch){
	//body
}</code></pre><p>It's basically a for loop that branches instead of continuing linearly. </p><p><span>"</span><strong>init</strong><span>" here runs when you enter the for_tree loop, exactly the same as a normal for loop</span><br><span>"</span><strong>condition</strong><span>" must be met to enter the body of the loop</span><br><span>"</span><strong>branch</strong><span>" evolves the initial value into multiple branches (it would have to compile down into recursive function calls here)</span></p><p>Well, this is significantly easier and less error prone than implementing recursive functions for every operation you'd want to do on every type of tree, plus all the relevant code ends up in-place and readable. This would likely compile down to the same code that a recursive function would anyway. It's just a nice shortcut, the same way for is a nice shortcut for gotos and labels.</p><pre><code>for_tree(Node* N = mytreeroot; N != NULL; N : {N-&gt;left, N-&gt;right}){
	print(N-&gt;value);
}</code></pre><p>would compile down into the equivalent of</p><pre><code>void for_tree_internal(Node* N){
	print(N-&gt;value);
	for(Node* n2 : {N-&gt;left, N-&gt;right}){
		if(n2 != NULL){
			for_tree_internal(n2);
		}
	}
}
for_tree_internal(mytreeroot);</code></pre><p>Doesn't for_tree(...) look a lot nicer and simpler and less error prone than needing to implement a recursive function for each operation you would want to do on a tree?</p><p>Additionally, for_tree would offer a few more benefits that you couldn't easily do with a recursive function, for instance being able to use break, continue, and return from within the function body. Those would behave similarly to how a for loop does.</p><pre><code>for_tree(Node* N = mytreeroot; N != NULL; N : {N-&gt;left, N-&gt;right}){
	if(N-&gt;value == 10) {
		found = true;
		break; //would break out of the entire for_tree block,
                       //unwinding the stack in the process
	}
}</code></pre><p>There is an additional flow construct that could be added here, "prune", which would prevent the loop from traversing into the branches off of the current node. </p><pre><code>for_tree(Node* N = mytreeroot; N != NULL; N : {N-&gt;left, N-&gt;right}){
	if(N-&gt;value == 10) {
		prune; //dont need to check any children of this node
	}
}</code></pre><p>Well a range based for loop requires that your tree exist in memory AND that you have an iterator defined for your tree. With for_tree you could operate on an entirely imperative tree, without needing to define any iterators or generator functions. Here's an example where I'm checking every single string composed of "a", "b", and "c" of length 8 or less.</p><pre><code>for_tree(string x = ""; x.size() &lt;= 8; x : {x+"a", x+"b", x+"c"}){
	print(x);
}</code></pre><p>This is an operation that requires "tree-like traversal" but is not iterating over a data structure that exists in memory. You can do that entire thing inside for_tree. It's entirely agnostic to any underlying data structures you are or aren't using. The "prune" flow described earlier is also something you can't get from a range based for loop. </p><p>There is a bit of a footgun here, which is that in that above string example its actually generating and rejecting all of the strings of length 9. A normal for loop *also* reaches the state after its last valid state before exiting, its just kind of not that much of an issue with a linear for loop where that will usually be like, one extra addition and conditional check. On the other hand, "one level deeper" in a tree traversal means checking the validity of multiple times as many leaf nodes as you need to. </p><p>You could resolve this manually in multiple ways, generate an empty branch list if you are at a leaf already</p><pre><code>for_tree(string x = ""; x.size() &lt;= 8; 
x : x.size() &lt; 8 ? {x+"a", x+"b", x+"c"} : {}){
	print(x);
}</code></pre><p>or just prune in the function body</p><pre><code>for_tree(string x = ""; x.size() &lt;= 8; x : {x+"a", x+"b", x+"c"}){
	print(x);
	if(x.size() == 8) prune;
}</code></pre><p>There may be a better way to resolve that, but would have to pull the syntax further away from a for loop I think. Anyway.</p><p><span>So, a depth first search is pretty simple, uses minimal extra memory, and works great with the stack that we are already using. BFS requires a lot of extra memory (enough that it would likely need to use the heap) and is more complex. If you really wanted to you could do an alternate function like for_tree_breadth_first(...), but I think the extra complexity needed for a BFS might be too much for a “primitive” construct.</span><br></p><p><span>Anyway, as a bonus here's a test C++ implementation that tries to get as close to this as possible with a bunch of templates and macros. The usage is uglier, and a few constructs do not work (ex, you cant return from within a for_tree here), but its kind of a neat proof of concept I think.</span><br></p><div itemprop="text" id="gist137690106" data-color-mode="light" data-light-theme="light" data-attrs="{&quot;innerHTML&quot;:&quot;<div id=\&quot;gist137690106\&quot; class=\&quot;gist\&quot;>\n    <div class=\&quot;gist-file\&quot; translate=\&quot;no\&quot; data-color-mode=\&quot;light\&quot; data-light-theme=\&quot;light\&quot;>\n      <div class=\&quot;gist-data\&quot;>\n        <div class=\&quot;js-gist-file-update-container js-task-list-container\&quot;>\n  <div id=\&quot;file-treetraversal-cpp\&quot; class=\&quot;file my-2\&quot;>\n    \n    <div itemprop=\&quot;text\&quot;\n      class=\&quot;Box-body p-0 blob-wrapper data type-c  \&quot;\n      style=\&quot;overflow: auto\&quot; tabindex=\&quot;0\&quot; role=\&quot;region\&quot;\n      aria-label=\&quot;treetraversal.cpp content, created by TylerGlaiel on 03:36AM today.\&quot;\n    >\n\n        \n<div class=\&quot;js-check-hidden-unicode js-blob-code-container blob-code-content\&quot;>\n\n  <template class=\&quot;js-file-alert-template\&quot;>\n  <div data-view-component=\&quot;true\&quot; class=\&quot;flash flash-warn flash-full d-flex flex-items-center\&quot;>\n  <svg aria-hidden=\&quot;true\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 16 16\&quot; version=\&quot;1.1\&quot; width=\&quot;16\&quot; data-view-component=\&quot;true\&quot; class=\&quot;octicon octicon-alert\&quot;>\n    <path d=\&quot;M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\&quot;></path>\n</svg>\n    <span>\n      This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.\n      <a class=\&quot;Link--inTextBlock\&quot; href=\&quot;https://github.co/hiddenchars\&quot; target=\&quot;_blank\&quot;>Learn more about bidirectional Unicode characters</a>\n    </span>\n\n\n  <div data-view-component=\&quot;true\&quot; class=\&quot;flash-action\&quot;>        <a href=\&quot;{{ revealButtonHref }}\&quot; data-view-component=\&quot;true\&quot; class=\&quot;btn-sm btn\&quot;>    Show hidden characters\n</a>\n</div>\n</div></template>\n<template class=\&quot;js-line-alert-template\&quot;>\n  <span aria-label=\&quot;This line has hidden Unicode characters\&quot; data-view-component=\&quot;true\&quot; class=\&quot;line-alert tooltipped tooltipped-e\&quot;>\n    <svg aria-hidden=\&quot;true\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 16 16\&quot; version=\&quot;1.1\&quot; width=\&quot;16\&quot; data-view-component=\&quot;true\&quot; class=\&quot;octicon octicon-alert\&quot;>\n    <path d=\&quot;M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\&quot;></path>\n</svg>\n</span></template>\n\n  <table data-hpc class=\&quot;highlight tab-size js-file-line-container\&quot; data-tab-size=\&quot;8\&quot; data-paste-markdown-skip data-tagsearch-path=\&quot;treetraversal.cpp\&quot;>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L1\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;1\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC1\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>include</span> <span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;lt;</span>vector<span class=\&quot;pl-pds\&quot;>&amp;gt;</span></span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L2\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;2\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC2\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>include</span> <span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;lt;</span>string<span class=\&quot;pl-pds\&quot;>&amp;gt;</span></span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L3\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;3\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC3\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>include</span> <span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;lt;</span>iostream<span class=\&quot;pl-pds\&quot;>&amp;gt;</span></span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L4\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;4\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC4\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L5\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;5\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC5\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-k\&quot;>enum</span> <span class=\&quot;pl-k\&quot;>class</span> <span class=\&quot;pl-en\&quot;>_tree_return</span> {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L6\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;6\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC6\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Continue = <span class=\&quot;pl-c1\&quot;>0</span>,</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L7\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;7\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC7\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Prune,</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L8\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;8\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC8\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Break</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L9\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;9\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC9\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>};</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L10\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;10\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC10\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L11\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;11\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC11\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-k\&quot;>template</span>&amp;lt;<span class=\&quot;pl-k\&quot;>typename</span> T, <span class=\&quot;pl-k\&quot;>typename</span> F1, <span class=\&quot;pl-k\&quot;>typename</span> F2, <span class=\&quot;pl-k\&quot;>typename</span> F3&amp;gt;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L12\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;12\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC12\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>_tree_return _for_tree(T initial, F1 condition, F2 branch, F3 visit) {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L13\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;13\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC13\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    _tree_return result = <span class=\&quot;pl-c1\&quot;>visit</span>(initial);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L14\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;14\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC14\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-k\&quot;>if</span>(result == _tree_return::Break) <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Break;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L15\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;15\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC15\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L16\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;16\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC16\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-k\&quot;>if</span>(result != _tree_return::Prune) {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L17\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;17\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC17\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>        <span class=\&quot;pl-k\&quot;>for</span>(T subnode : <span class=\&quot;pl-c1\&quot;>branch</span>(initial)) {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L18\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;18\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC18\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>            <span class=\&quot;pl-k\&quot;>if</span>(<span class=\&quot;pl-c1\&quot;>condition</span>(subnode)) {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L19\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;19\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC19\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>                _tree_return result = <span class=\&quot;pl-c1\&quot;>_for_tree</span>(subnode, condition, branch, visit);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L20\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;20\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC20\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>                <span class=\&quot;pl-k\&quot;>if</span>(result == _tree_return::Break) <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Break;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L21\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;21\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC21\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>            }</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L22\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;22\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC22\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>        }</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L23\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;23\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC23\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    }</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L24\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;24\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC24\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Continue;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L25\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;25\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC25\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>}</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L26\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;26\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC26\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L27\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;27\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC27\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>define</span> <span class=\&quot;pl-en\&quot;>tree_break</span> <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Break</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L28\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;28\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC28\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>define</span> <span class=\&quot;pl-en\&quot;>tree_prune</span> <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Prune</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L29\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;29\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC29\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>define</span> <span class=\&quot;pl-en\&quot;>tree_continue</span> <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Continue</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L30\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;30\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC30\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L31\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;31\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC31\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>                                                          <span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>v-- semicolon to not allow you to get the return value here</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L32\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;32\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC32\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>#<span class=\&quot;pl-k\&quot;>define</span> <span class=\&quot;pl-en\&quot;>for_tree</span>(<span class=\&quot;pl-v\&quot;>XName, Xinitial, Condition, Branch, Visit</span>) ;_for_tree(Xinitial, \\</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L33\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;33\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC33\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>[&amp;amp;](decltype(Xinitial) XName){ <span class=\&quot;pl-k\&quot;>return</span> Condition; }, \\</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L34\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;34\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC34\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>[&amp;amp;](decltype(Xinitial) XName){ <span class=\&quot;pl-k\&quot;>return</span> std::vector&amp;lt;<span class=\&quot;pl-c1\&quot;>decltype</span>(Xinitial)&amp;gt;Branch; }, \\</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L35\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;35\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC35\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>[&amp;amp;](decltype(Xinitial) XName){ Visit; <span class=\&quot;pl-k\&quot;>return</span> _tree_return::Continue; })</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L36\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;36\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC36\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>excuse the use of a std::vector in there, I guess you cant return an initialize_list from a lambda</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L37\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;37\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC37\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>that wouldn&amp;#39;t really be an issue if this was implemented at the language level instead of hacked together from lambdas and macros</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L38\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;38\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC38\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L39\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;39\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC39\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-k\&quot;>struct</span> <span class=\&quot;pl-en\&quot;>Node</span> {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L40\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;40\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC40\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Node* left = <span class=\&quot;pl-c1\&quot;>NULL</span>;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L41\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;41\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC41\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Node* right = <span class=\&quot;pl-c1\&quot;>NULL</span>;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L42\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;42\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC42\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    std::string value;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L43\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;43\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC43\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-en\&quot;>Node</span>(std::string value):value(value){}</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L44\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;44\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC44\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>};</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L45\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;45\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC45\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L46\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;46\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC46\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;><span class=\&quot;pl-k\&quot;>int</span> <span class=\&quot;pl-en\&quot;>main</span>() {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L47\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;47\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC47\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>syntax is a little uglier than it could be if it was native</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L48\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;48\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC48\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    </td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L49\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;49\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC49\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>imperative tree sample</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L50\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;50\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC50\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-c1\&quot;>for_tree</span>(x, <span class=\&quot;pl-c1\&quot;>std::string</span>(<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span><span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>), x.<span class=\&quot;pl-c1\&quot;>size</span>()&amp;lt;=<span class=\&quot;pl-c1\&quot;>8</span>, ({x+<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>a<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>, x+<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>b<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>, x+<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>c<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>}), {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L51\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;51\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC51\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>        std::cout &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L52\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;52\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC52\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    });</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L53\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;53\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC53\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    </td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L54\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;54\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC54\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-c\&quot;><span class=\&quot;pl-c\&quot;>//</span>tree structure sample</span></td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L55\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;55\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC55\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    Node <span class=\&quot;pl-smi\&quot;>mytree</span>(<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>root<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L56\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;56\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC56\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    mytree.<span class=\&quot;pl-smi\&quot;>left</span> = <span class=\&quot;pl-k\&quot;>new</span> <span class=\&quot;pl-c1\&quot;>Node</span>(<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>left<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L57\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;57\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC57\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    mytree.<span class=\&quot;pl-smi\&quot;>right</span> = <span class=\&quot;pl-k\&quot;>new</span> <span class=\&quot;pl-c1\&quot;>Node</span>(<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>right<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L58\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;58\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC58\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    mytree.<span class=\&quot;pl-smi\&quot;>left</span>-&amp;gt;<span class=\&quot;pl-smi\&quot;>left</span> = <span class=\&quot;pl-k\&quot;>new</span> <span class=\&quot;pl-c1\&quot;>Node</span>(<span class=\&quot;pl-s\&quot;><span class=\&quot;pl-pds\&quot;>&amp;quot;</span>leftleft<span class=\&quot;pl-pds\&quot;>&amp;quot;</span></span>);</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L59\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;59\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC59\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    </td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L60\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;60\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC60\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-c1\&quot;>for_tree</span>(x, &amp;amp;mytree, x != <span class=\&quot;pl-c1\&quot;>NULL</span>, ({x-&amp;gt;<span class=\&quot;pl-smi\&quot;>left</span>, x-&amp;gt;<span class=\&quot;pl-smi\&quot;>right</span>}), {</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L61\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;61\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC61\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>        std::cout &amp;lt;&amp;lt; x-&amp;gt;<span class=\&quot;pl-smi\&quot;>value</span> &amp;lt;&amp;lt; std::endl;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L62\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;62\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC62\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    });</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L63\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;63\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC63\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>\n</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L64\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;64\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC64\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>    <span class=\&quot;pl-k\&quot;>return</span> <span class=\&quot;pl-c1\&quot;>0</span>;</td>\n        </tr>\n        <tr>\n          <td id=\&quot;file-treetraversal-cpp-L65\&quot; class=\&quot;blob-num js-line-number js-blob-rnum\&quot; data-line-number=\&quot;65\&quot;></td>\n          <td id=\&quot;file-treetraversal-cpp-LC65\&quot; class=\&quot;blob-code blob-code-inner js-file-line\&quot;>}</td>\n        </tr>\n  </table>\n</div>\n\n\n    </div>\n\n  </div>\n</div>\n\n      </div>\n      <div class=\&quot;gist-meta\&quot;>\n        <a href=\&quot;https://gist.github.com/TylerGlaiel/7b6fa06628883148e4403b3cc616fdec/raw/f0d4bf0e8043ae6c6bdad2146daddbe9d0cb3b88/treetraversal.cpp\&quot; style=\&quot;float:right\&quot; class=\&quot;Link--inTextBlock\&quot;>view raw</a>\n        <a href=\&quot;https://gist.github.com/TylerGlaiel/7b6fa06628883148e4403b3cc616fdec#file-treetraversal-cpp\&quot; class=\&quot;Link--inTextBlock\&quot;>\n          treetraversal.cpp\n        </a>\n        hosted with &amp;#10084; by <a class=\&quot;Link--inTextBlock\&quot; href=\&quot;https://github.com\&quot;>GitHub</a>\n      </div>\n    </div>\n</div>\n&quot;,&quot;stylesheet&quot;:&quot;https://github.githubassets.com/assets/gist-embed-b1ee75c43dbe.css&quot;}" data-component-name="GitgistToDOM"><div data-view-component="true"><p><span>
  
    

    </span><span><span>
      This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      </span><a href="https://github.co/hiddenchars" target="_blank" rel="">Learn more about bidirectional Unicode characters</a><span>
    </span></span><span>


  </span></p></div><table data-hpc="" data-tab-size="8" data-paste-markdown-skip="" data-tagsearch-path="treetraversal.cpp"><tbody><tr><td id="file-treetraversal-cpp-L1" data-line-number="1"></td><td id="file-treetraversal-cpp-LC1"><span>#</span><span>include</span><span> </span><span><span>&lt;</span><span>vector</span><span>&gt;</span></span></td></tr><tr><td id="file-treetraversal-cpp-L2" data-line-number="2"></td><td id="file-treetraversal-cpp-LC2"><span>#</span><span>include</span><span> </span><span><span>&lt;</span><span>string</span><span>&gt;</span></span></td></tr><tr><td id="file-treetraversal-cpp-L3" data-line-number="3"></td><td id="file-treetraversal-cpp-LC3"><span>#</span><span>include</span><span> </span><span><span>&lt;</span><span>iostream</span><span>&gt;</span></span></td></tr><tr><td id="file-treetraversal-cpp-L4" data-line-number="4"></td><td id="file-treetraversal-cpp-LC4">
</td></tr><tr><td id="file-treetraversal-cpp-L5" data-line-number="5"></td><td id="file-treetraversal-cpp-LC5"><span>enum</span><span> </span><span>class</span><span> </span><span>_tree_return</span><span> {</span></td></tr><tr><td id="file-treetraversal-cpp-L6" data-line-number="6"></td><td id="file-treetraversal-cpp-LC6"><span>    Continue = </span><span>0</span><span>,</span></td></tr><tr><td id="file-treetraversal-cpp-L7" data-line-number="7"></td><td id="file-treetraversal-cpp-LC7">    Prune,</td></tr><tr><td id="file-treetraversal-cpp-L8" data-line-number="8"></td><td id="file-treetraversal-cpp-LC8">    Break</td></tr><tr><td id="file-treetraversal-cpp-L9" data-line-number="9"></td><td id="file-treetraversal-cpp-LC9">};</td></tr><tr><td id="file-treetraversal-cpp-L10" data-line-number="10"></td><td id="file-treetraversal-cpp-LC10">
</td></tr><tr><td id="file-treetraversal-cpp-L11" data-line-number="11"></td><td id="file-treetraversal-cpp-LC11"><span>template</span><span>&lt;</span><span>typename</span><span> T, </span><span>typename</span><span> F1, </span><span>typename</span><span> F2, </span><span>typename</span><span> F3&gt;</span></td></tr><tr><td id="file-treetraversal-cpp-L12" data-line-number="12"></td><td id="file-treetraversal-cpp-LC12">_tree_return _for_tree(T initial, F1 condition, F2 branch, F3 visit) {</td></tr><tr><td id="file-treetraversal-cpp-L13" data-line-number="13"></td><td id="file-treetraversal-cpp-LC13"><span>    _tree_return result = </span><span>visit</span><span>(initial);</span></td></tr><tr><td id="file-treetraversal-cpp-L14" data-line-number="14"></td><td id="file-treetraversal-cpp-LC14"><span>    </span><span>if</span><span>(result == _tree_return::Break) </span><span>return</span><span> _tree_return::Break;</span></td></tr><tr><td id="file-treetraversal-cpp-L15" data-line-number="15"></td><td id="file-treetraversal-cpp-LC15">
</td></tr><tr><td id="file-treetraversal-cpp-L16" data-line-number="16"></td><td id="file-treetraversal-cpp-LC16"><span>    </span><span>if</span><span>(result != _tree_return::Prune) {</span></td></tr><tr><td id="file-treetraversal-cpp-L17" data-line-number="17"></td><td id="file-treetraversal-cpp-LC17"><span>        </span><span>for</span><span>(T subnode : </span><span>branch</span><span>(initial)) {</span></td></tr><tr><td id="file-treetraversal-cpp-L18" data-line-number="18"></td><td id="file-treetraversal-cpp-LC18"><span>            </span><span>if</span><span>(</span><span>condition</span><span>(subnode)) {</span></td></tr><tr><td id="file-treetraversal-cpp-L19" data-line-number="19"></td><td id="file-treetraversal-cpp-LC19"><span>                _tree_return result = </span><span>_for_tree</span><span>(subnode, condition, branch, visit);</span></td></tr><tr><td id="file-treetraversal-cpp-L20" data-line-number="20"></td><td id="file-treetraversal-cpp-LC20"><span>                </span><span>if</span><span>(result == _tree_return::Break) </span><span>return</span><span> _tree_return::Break;</span></td></tr><tr><td id="file-treetraversal-cpp-L21" data-line-number="21"></td><td id="file-treetraversal-cpp-LC21">            }</td></tr><tr><td id="file-treetraversal-cpp-L22" data-line-number="22"></td><td id="file-treetraversal-cpp-LC22">        }</td></tr><tr><td id="file-treetraversal-cpp-L23" data-line-number="23"></td><td id="file-treetraversal-cpp-LC23">    }</td></tr><tr><td id="file-treetraversal-cpp-L24" data-line-number="24"></td><td id="file-treetraversal-cpp-LC24"><span>    </span><span>return</span><span> _tree_return::Continue;</span></td></tr><tr><td id="file-treetraversal-cpp-L25" data-line-number="25"></td><td id="file-treetraversal-cpp-LC25">}</td></tr><tr><td id="file-treetraversal-cpp-L26" data-line-number="26"></td><td id="file-treetraversal-cpp-LC26">
</td></tr><tr><td id="file-treetraversal-cpp-L27" data-line-number="27"></td><td id="file-treetraversal-cpp-LC27"><span>#</span><span>define</span><span> </span><span>tree_break</span><span> </span><span>return</span><span> _tree_return::Break</span></td></tr><tr><td id="file-treetraversal-cpp-L28" data-line-number="28"></td><td id="file-treetraversal-cpp-LC28"><span>#</span><span>define</span><span> </span><span>tree_prune</span><span> </span><span>return</span><span> _tree_return::Prune</span></td></tr><tr><td id="file-treetraversal-cpp-L29" data-line-number="29"></td><td id="file-treetraversal-cpp-LC29"><span>#</span><span>define</span><span> </span><span>tree_continue</span><span> </span><span>return</span><span> _tree_return::Continue</span></td></tr><tr><td id="file-treetraversal-cpp-L30" data-line-number="30"></td><td id="file-treetraversal-cpp-LC30">
</td></tr><tr><td id="file-treetraversal-cpp-L31" data-line-number="31"></td><td id="file-treetraversal-cpp-LC31"><span>                                                          </span><span><span>//</span><span>v-- semicolon to not allow you to get the return value here</span></span></td></tr><tr><td id="file-treetraversal-cpp-L32" data-line-number="32"></td><td id="file-treetraversal-cpp-LC32"><span>#</span><span>define</span><span> </span><span>for_tree</span><span>(</span><span>XName, Xinitial, Condition, Branch, Visit</span><span>) ;_for_tree(Xinitial, \</span></td></tr><tr><td id="file-treetraversal-cpp-L33" data-line-number="33"></td><td id="file-treetraversal-cpp-LC33"><span>[&amp;](decltype(Xinitial) XName){ </span><span>return</span><span> Condition; }, \</span></td></tr><tr><td id="file-treetraversal-cpp-L34" data-line-number="34"></td><td id="file-treetraversal-cpp-LC34"><span>[&amp;](decltype(Xinitial) XName){ </span><span>return</span><span> std::vector&lt;</span><span>decltype</span><span>(Xinitial)&gt;Branch; }, \</span></td></tr><tr><td id="file-treetraversal-cpp-L35" data-line-number="35"></td><td id="file-treetraversal-cpp-LC35"><span>[&amp;](decltype(Xinitial) XName){ Visit; </span><span>return</span><span> _tree_return::Continue; })</span></td></tr><tr><td id="file-treetraversal-cpp-L36" data-line-number="36"></td><td id="file-treetraversal-cpp-LC36"><span><span>//</span><span>excuse the use of a std::vector in there, I guess you cant return an initialize_list from a lambda</span></span></td></tr><tr><td id="file-treetraversal-cpp-L37" data-line-number="37"></td><td id="file-treetraversal-cpp-LC37"><span><span>//</span><span>that wouldn't really be an issue if this was implemented at the language level instead of hacked together from lambdas and macros</span></span></td></tr><tr><td id="file-treetraversal-cpp-L38" data-line-number="38"></td><td id="file-treetraversal-cpp-LC38">
</td></tr><tr><td id="file-treetraversal-cpp-L39" data-line-number="39"></td><td id="file-treetraversal-cpp-LC39"><span>struct</span><span> </span><span>Node</span><span> {</span></td></tr><tr><td id="file-treetraversal-cpp-L40" data-line-number="40"></td><td id="file-treetraversal-cpp-LC40"><span>    Node* left = </span><span>NULL</span><span>;</span></td></tr><tr><td id="file-treetraversal-cpp-L41" data-line-number="41"></td><td id="file-treetraversal-cpp-LC41"><span>    Node* right = </span><span>NULL</span><span>;</span></td></tr><tr><td id="file-treetraversal-cpp-L42" data-line-number="42"></td><td id="file-treetraversal-cpp-LC42">    std::string value;</td></tr><tr><td id="file-treetraversal-cpp-L43" data-line-number="43"></td><td id="file-treetraversal-cpp-LC43"><span>    </span><span>Node</span><span>(std::string value):value(value){}</span></td></tr><tr><td id="file-treetraversal-cpp-L44" data-line-number="44"></td><td id="file-treetraversal-cpp-LC44">};</td></tr><tr><td id="file-treetraversal-cpp-L45" data-line-number="45"></td><td id="file-treetraversal-cpp-LC45">
</td></tr><tr><td id="file-treetraversal-cpp-L46" data-line-number="46"></td><td id="file-treetraversal-cpp-LC46"><span>int</span><span> </span><span>main</span><span>() {</span></td></tr><tr><td id="file-treetraversal-cpp-L47" data-line-number="47"></td><td id="file-treetraversal-cpp-LC47"><span>    </span><span><span>//</span><span>syntax is a little uglier than it could be if it was native</span></span></td></tr><tr><td id="file-treetraversal-cpp-L48" data-line-number="48"></td><td id="file-treetraversal-cpp-LC48">    </td></tr><tr><td id="file-treetraversal-cpp-L49" data-line-number="49"></td><td id="file-treetraversal-cpp-LC49"><span>    </span><span><span>//</span><span>imperative tree sample</span></span></td></tr><tr><td id="file-treetraversal-cpp-L50" data-line-number="50"></td><td id="file-treetraversal-cpp-LC50"><span>    </span><span>for_tree</span><span>(x, </span><span>std::string</span><span>(</span><span><span>"</span><span>"</span></span><span>), x.</span><span>size</span><span>()&lt;=</span><span>8</span><span>, ({x+</span><span><span>"</span><span>a</span><span>"</span></span><span>, x+</span><span><span>"</span><span>b</span><span>"</span></span><span>, x+</span><span><span>"</span><span>c</span><span>"</span></span><span>}), {</span></td></tr><tr><td id="file-treetraversal-cpp-L51" data-line-number="51"></td><td id="file-treetraversal-cpp-LC51">        std::cout &lt;&lt; x &lt;&lt; std::endl;</td></tr><tr><td id="file-treetraversal-cpp-L52" data-line-number="52"></td><td id="file-treetraversal-cpp-LC52">    });</td></tr><tr><td id="file-treetraversal-cpp-L53" data-line-number="53"></td><td id="file-treetraversal-cpp-LC53">    </td></tr><tr><td id="file-treetraversal-cpp-L54" data-line-number="54"></td><td id="file-treetraversal-cpp-LC54"><span>    </span><span><span>//</span><span>tree structure sample</span></span></td></tr><tr><td id="file-treetraversal-cpp-L55" data-line-number="55"></td><td id="file-treetraversal-cpp-LC55"><span>    Node </span><span>mytree</span><span>(</span><span><span>"</span><span>root</span><span>"</span></span><span>);</span></td></tr><tr><td id="file-treetraversal-cpp-L56" data-line-number="56"></td><td id="file-treetraversal-cpp-LC56"><span>    mytree.</span><span>left</span><span> = </span><span>new</span><span> </span><span>Node</span><span>(</span><span><span>"</span><span>left</span><span>"</span></span><span>);</span></td></tr><tr><td id="file-treetraversal-cpp-L57" data-line-number="57"></td><td id="file-treetraversal-cpp-LC57"><span>    mytree.</span><span>right</span><span> = </span><span>new</span><span> </span><span>Node</span><span>(</span><span><span>"</span><span>right</span><span>"</span></span><span>);</span></td></tr><tr><td id="file-treetraversal-cpp-L58" data-line-number="58"></td><td id="file-treetraversal-cpp-LC58"><span>    mytree.</span><span>left</span><span>-&gt;</span><span>left</span><span> = </span><span>new</span><span> </span><span>Node</span><span>(</span><span><span>"</span><span>leftleft</span><span>"</span></span><span>);</span></td></tr><tr><td id="file-treetraversal-cpp-L59" data-line-number="59"></td><td id="file-treetraversal-cpp-LC59">    </td></tr><tr><td id="file-treetraversal-cpp-L60" data-line-number="60"></td><td id="file-treetraversal-cpp-LC60"><span>    </span><span>for_tree</span><span>(x, &amp;mytree, x != </span><span>NULL</span><span>, ({x-&gt;</span><span>left</span><span>, x-&gt;</span><span>right</span><span>}), {</span></td></tr><tr><td id="file-treetraversal-cpp-L61" data-line-number="61"></td><td id="file-treetraversal-cpp-LC61"><span>        std::cout &lt;&lt; x-&gt;</span><span>value</span><span> &lt;&lt; std::endl;</span></td></tr><tr><td id="file-treetraversal-cpp-L62" data-line-number="62"></td><td id="file-treetraversal-cpp-LC62">    });</td></tr><tr><td id="file-treetraversal-cpp-L63" data-line-number="63"></td><td id="file-treetraversal-cpp-LC63">
</td></tr><tr><td id="file-treetraversal-cpp-L64" data-line-number="64"></td><td id="file-treetraversal-cpp-LC64"><span>    </span><span>return</span><span> </span><span>0</span><span>;</span></td></tr><tr><td id="file-treetraversal-cpp-L65" data-line-number="65"></td><td id="file-treetraversal-cpp-LC65">}</td></tr></tbody></table></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After 53 years, a failed Soviet Venus spacecraft is crashing back to Earth (124 pts)]]></title>
            <link>https://gizmodo.com/after-53-years-a-failed-soviet-venus-spacecraft-is-crashing-back-to-earth-2000595234</link>
            <guid>43831602</guid>
            <pubDate>Tue, 29 Apr 2025 12:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/after-53-years-a-failed-soviet-venus-spacecraft-is-crashing-back-to-earth-2000595234">https://gizmodo.com/after-53-years-a-failed-soviet-venus-spacecraft-is-crashing-back-to-earth-2000595234</a>, See on <a href="https://news.ycombinator.com/item?id=43831602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>A 53-year-old Venus probe that failed to escape low Earth orbit is expected to make an uncontrolled reentry in the coming weeks. Built to withstand extreme heat, parts of the spacecraft could survive the descent and crash on Earth.</p> <p>The lander module from an old Soviet spacecraft is expected to reenter Earth’s atmosphere during the second week of May, according to Marco Langbroek, a satellite tracker based in Leiden, the Netherlands. “As this is a lander that was designed to survive passage through the Venus atmosphere, it is possible that it will survive reentry through the Earth atmosphere intact, and impact intact,” Langbroek wrote in a blog <a href="https://sattrackcam.blogspot.com/2025/04/kosmos-842-descent-craft-reentry.html">update</a>. “The risks involved are not particularly high, but not zero.”</p> <p>Kosmos 482 launched on March 31, 1972 from the Baikonur Cosmodrome spaceport in Kazakhstan. The mission was an attempt by the Soviet space program to reach Venus, but it failed to gain enough velocity to enter a transfer trajectory toward the scorching-hot planet. A malfunction resulted in an engine burn that wasn’t sufficient to reach Venus’ orbit and left the spacecraft in an elliptical Earth orbit, according to <a href="https://nssdc.gsfc.nasa.gov/nmc/spacecraft/display.action?id=1972-023A">NASA</a>. The spacecraft broke apart into four different pieces, with two of the smaller fragments reentering over Ashburton, New Zealand, two days after launch. Meanwhile, two remaining pieces, believed to be the payload and the detached upper-stage engine unit, entered a higher orbit measuring 130 by 6,089 miles (210 by 9,800 kilometers).</p> <p>The failed mission consisted of a carrier bus and a lander probe, which together form a spherical pressure vessel weighing more than 1,000 pounds (495 kilograms). Considering its mass, “risks are similar to that of a meteorite impact,” Langbroek wrote.</p>

 <p>As of now, it’s hard to determine exactly when the spacecraft will reenter. Langbroek estimates that the reentry will take place on May 10, but a more precise date will get clearer as the reentry date nears. It’s hard to pin down an exact date because the Sun, now in its active phase, is heating and expanding Earth’s atmosphere; it’s creating more atmospheric drag on orbiting objects, causing them to reenter sooner.</p> <p>It’s also difficult to determine where the spacecraft’s remains will end up on Earth, since that depends on when it reenters the atmosphere and breaks apart. Generally, the chances of spacecraft debris landing in an inhabited area are low, with a greater likelihood of it falling into a remote part of the ocean. Still, uncontrolled reentries do pose a small risk that shouldn’t be overlooked or ignored.</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly - A faster Python type checker written in Rust (123 pts)]]></title>
            <link>https://pyrefly.org/</link>
            <guid>43831524</guid>
            <pubDate>Tue, 29 Apr 2025 12:13:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pyrefly.org/">https://pyrefly.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43831524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><header><section><img src="https://pyrefly.org/img/Pyrefly-Brandmark-Invert.svg" alt="Pyrefly Logo"></section><p><span>A faster Python type checker written in Rust</span></p><section><a href="https://github.com/facebook/pyrefly/milestone/1"> <!-- -->Github<!-- --> </a><a href="https://pyrefly.org/try/"> <!-- -->Demo<!-- --> </a><a href="https://pyrefly.org/en/docs/"> <!-- -->Docs<!-- --> </a></section><section><p>Launching Spring 2025</p></section><section><li id="firefly"></li><li id="firefly"></li><li id="firefly"></li><li id="firefly"></li><li id="firefly"></li></section></header></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Chrome extension that will auto-reject non-essential cookies (220 pts)]]></title>
            <link>https://blog.bymitch.com/posts/reject-cookies/</link>
            <guid>43831298</guid>
            <pubDate>Tue, 29 Apr 2025 11:49:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.bymitch.com/posts/reject-cookies/">https://blog.bymitch.com/posts/reject-cookies/</a>, See on <a href="https://news.ycombinator.com/item?id=43831298">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><a href="https://chromewebstore.google.com/detail/bnbodofigkfjljnopfggfoecokhmhamc?utm_source=item-share-cb">Add the extension</a></p>
<p><img src="https://blog.bymitch.com/reject-cookies/reject-cookies-no.png" alt="Reject Cookies Logo" title="Logo"></p>
<h2 id="a-chrome-extension">A Chrome Extension</h2>
<p>Everyone can agree that cookie consent banners are frustrating. It might be one of the few unifying factors on the internet today. Even though it’s a couple clicks, the couple clicks are a pain, and the couple clicks can happen on many sites each day.</p>
<p>There are browser extensions out there that will auto-accept cookies like <a href="https://chromewebstore.google.com/detail/i-dont-care-about-cookies/fihnjjcciajhdojfnbdddfaoknhalnja">I don’t care about cookies</a> and it’s open source fork <a href="https://chromewebstore.google.com/detail/i-still-dont-care-about-c/edibdbjcniadpccecjdfdjjppcpchdlm">I still don’t care about cookies</a>. You can even chain this extension with another that will auto-clean up your cookies. This is an adequate solution and ascribes to <a href="https://en.wikipedia.org/wiki/Unix_philosophy">unix philosophy</a>.</p>
<p>Additionally, there are extensions like <a href="https://ublockorigin.com/">uBlock Origin</a> with additional filters to help ignore these annoying pop ups. Or <a href="https://privacybadger.org/">Privacy Badger</a> to block cookie trackers. Although there is space to provide an extension that just auto-rejects non essential cookies.</p>
<p>That’s what led to the “Reject Cookies” chrome extension. It will first attempt to reject the cookies on the page. If that is unsuccessful, it will then attempt to close the cookie pop up or banner. To comply with the regulations governing cookies under the <a href="https://gdpr.eu/cookies/">GDPR and the ePrivacy Directive you must</a></p>
<blockquote>
<p>Receive users’ consent before you use any cookies except strictly necessary cookies.</p>
</blockquote>
<p>So the omission of an acceptance should be on par with an explicit rejection. If you’re interested in how it works the code is <a href="https://github.com/mitch292/reject-cookies">open source and on github</a>, but let’s step through it at a high level.</p>
<h2 id="how-its-implemented">How it’s implemented</h2>
<p>Vibe coding is the answer. I leveraged Cursor and let it auto-select the model. This combination while extremely useful, did not serve me as well as recent past experience. On the project setup front, I had not previously written a Chrome extension. Having the Cursor agent set up the boilerplate was convenient. Although, it requested too liberal of permissions in the permissions to start and wouldn’t go and update them as the design of the app changed. Below is a snippet of the <code>manifest.json</code> to show what the permissions ended up looking like.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"permissions"</span>: [<span>"activeTab"</span>, <span>"sidePanel"</span>, <span>"tabs"</span>],
</span></span><span><span>  <span>"content_scripts"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"matches"</span>: [<span>"http://*/*"</span>, <span>"https://*/*"</span>],
</span></span><span><span>      <span>"js"</span>: [<span>"content.js"</span>]
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>Next on the implementation side of things, it started with a set of common selectors that could possibly be relevant to non-essential cookies. The problem was once again these selectors were extremely liberal things like elements with the class “accept”. I opted to take a more targeted approach and aim the logic at specific cookie consent vendors that most sites seem to leverage. Cursor’s agent was, as expected, not able to help much with this implementation.</p>
<p>The extension will go through the configured providers.</p>
<div><pre tabindex="0"><code data-lang="typescript"><span><span><span>const</span> <span>findAndClickRejectButtons</span> <span>=</span> () <span>=&gt;</span> {
</span></span><span><span>	<span>commonCookiePopupChecks</span>.<span>forEach</span>(({ <span>check</span>, <span>rejectOrClose</span> }) <span>=&gt;</span> {
</span></span><span><span>	  <span>if</span> (<span>check</span>()) {
</span></span><span><span>		<span>rejectOrClose</span>();
</span></span><span><span>		<span>// assume that there is only one cookie consent provider and we can exit
</span></span></span><span><span><span></span>		<span>return</span>;
</span></span><span><span>	  }
</span></span><span><span>	});
</span></span><span><span>  }
</span></span></code></pre></div><p>A check for a provider will look for a specific element that identifies it.</p>
<div><pre tabindex="0"><code data-lang="typescript"><span><span><span>const</span> <span>checkForOneTrust</span> <span>=</span> ()<span>:</span> <span>boolean</span> <span>=&gt;</span> <span>!!</span>document.<span>getElementById</span>(<span>'onetrust-consent-sdk'</span>);
</span></span></code></pre></div><p>Then attempt to reject the cookies and fallback to removing the consent banner or popup if it’s not able to reject the non-essential cookies.</p>
<div><pre tabindex="0"><code data-lang="typescript"><span><span><span>const</span> <span>closeOrRejectOneTrust</span> <span>=</span> () <span>=&gt;</span> {
</span></span><span><span>  <span>const</span> <span>rejectButton</span> <span>=</span> document.<span>getElementById</span>(<span>'onetrust-reject-all-handler'</span>);
</span></span><span><span>  <span>if</span> (<span>rejectButton</span>) {
</span></span><span><span>    <span>rejectButton</span>.<span>click</span>();
</span></span><span><span>    <span>return</span> <span>true</span>;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>const</span> <span>consentSDK</span> <span>=</span> document.<span>getElementById</span>(<span>'onetrust-consent-sdk'</span>);
</span></span><span><span>  <span>if</span> (<span>consentSDK</span>) {
</span></span><span><span>    <span>consentSDK</span>.<span>remove</span>();
</span></span><span><span>    <span>return</span> <span>true</span>;
</span></span><span><span>  }
</span></span><span><span>  <span>return</span> <span>false</span>;
</span></span><span><span>};
</span></span></code></pre></div><p>Not much more to it other than that.</p>
<p><img src="https://blog.bymitch.com/reject-cookies/reject-cookies-umbrella.png" alt="Reject Cookies Umbrella" title="Umbrella"></p>
<h2 id="help-it-get-better">Help it get better</h2>
<p><strong>Reject Cookies is still a work in progress.</strong> It can use your support to help cover more use cases and report bugs. As mentioned the design targets specific cookie consent implementations from different vendors. There are more vendors out there and different flavors of each vendors implementation. The side panel allows you to report sites where the cookie consent rejection was missed along with a place to report bugs or issues with the extension. The side panel can be accessed by clicking on the chrome extension’s menu in Chrome. You can also feel free to reach out to <a href="https://blog.bymitch.com/cdn-cgi/l/email-protection#81e8efe7eec1e3f8ece8f5e2e9afe2eeec"><span data-cfemail="5e373038311e3c2733372a3d36703d3133">[email&nbsp;protected]</span></a> with any feedback.</p>
<p><img src="https://blog.bymitch.com/reject-cookies/side-panel.png" alt="Side panel debugging screenshot" title="Side Panel"></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Heart disease deaths worldwide linked to chemical widely used in plastics (228 pts)]]></title>
            <link>https://medicalxpress.com/news/2025-04-heart-disease-deaths-worldwide-linked.html</link>
            <guid>43831142</guid>
            <pubDate>Tue, 29 Apr 2025 11:35:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2025-04-heart-disease-deaths-worldwide-linked.html">https://medicalxpress.com/news/2025-04-heart-disease-deaths-worldwide-linked.html</a>, See on <a href="https://news.ycombinator.com/item?id=43831142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/detergent-bottle.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/detergent-bottle.jpg" data-sub-html="Credit: Unsplash/CC0 Public Domain">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/detergent-bottle.jpg" alt="detergent bottle" title="Credit: Unsplash/CC0 Public Domain" width="800" height="530">
             <figcaption>
                Credit: Unsplash/CC0 Public Domain
            </figcaption>        </figure>
    </div><p>Daily exposure to certain chemicals used to make plastic household items could be linked to more than 365,000 global deaths from heart disease in 2018 alone, a new analysis of population surveys shows.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>While the chemicals, called phthalates, are in widespread use globally, Africa, South Asia, and the Middle East populations bore a much larger share of the death toll than others—about half the total.</p>
<p>For decades, experts have connected health problems to exposure to certain phthalates found in cosmetics, detergents, solvents, plastic pipes, bug repellents, and other products. When these chemicals break down into microscopic particles and are ingested, studies have linked them to an increased risk of conditions ranging from obesity and diabetes to fertility issues and cancer.</p>
<p>Led by researchers at NYU Langone Health, the current study focused on a kind of phthalate called di-2-ethylhexyl phthalate (DEHP), which is used to make food containers, medical equipment, and other plastic softer and more flexible. Exposure has been shown in other studies to prompt an overactive immune response (inflammation) in the heart's arteries, which, over time, is associated with an increased risk of heart attack or stroke.</p>
<p>In their new analysis, the authors estimated that DEHP exposure contributed to 368,764 deaths, or more than 10% of all global mortality from <a href="https://medicalxpress.com/tags/heart+disease/" rel="tag">heart disease</a> in 2018 among men and women aged 55 through 64. A report on the findings is published in the journal <i>eBioMedicine</i>.</p>
<p>"By highlighting the connection between phthalates and a leading cause of death across the world, our findings add to the vast body of evidence that these chemicals present a tremendous danger to human health," said study lead author Sara Hyman, BS, an associate research scientist at NYU Grossman School of Medicine.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>According to the authors, the resulting economic burden from the deaths identified in their study was estimated to be around $510 billion and may have reached as high as $3.74 trillion.</p>
<p>In a past study from 2021, the research team tied phthalates to more than 50,000 premature deaths each year, mostly from heart disease, among older Americans. Their latest investigation is believed to be the first global estimate to date of cardiovascular mortality—or indeed any health outcome—resulting from exposure to the chemicals, says Hyman, who is also a graduate student at NYU School of Public Global Health.</p>
<p>For the research, the team used health and environmental data from dozens of population surveys to estimate DEHP exposure across 200 countries and territories. The information included urine samples containing chemical breakdown products left by the plastic additive. Mortality data was obtained from the Institute for Health Metrics and Evaluation, a research group in the US that collects medical information worldwide to identify trends in public health.</p>
<p>Among the key findings, the study showed that losses in Africa and in the combined region of East Asia and the Middle East accounted, respectively, for 30% and 25% of the mortality from heart disease linked to DEHP. Specifically, India had the highest death count at 39,677 deaths, followed by Pakistan and Egypt.</p>
<p>The larger heart death risks in these populations held true even after the researchers adjusted their statistical analysis to take into account population size within the studied age group.</p>

                                                                                                                                            <p>A possible explanation, the authors say, is that these countries face higher rates of exposure to the chemicals, possibly because they are undergoing a boom in plastic production but with fewer manufacturing restrictions than other regions.</p>
<p>"There is a clear disparity in which parts of the world bear the brunt of heightened heart risks from phthalates," said study senior author Leonardo Trasande, MD, MPP.</p>
<p>"Our results underscore the urgent need for global regulations to reduce exposure to these toxins, especially in areas most affected by rapid industrialization and plastic consumption," added Trasande, the Jim G. Hendrick, MD, Professor of Pediatrics at NYU Grossman School of Medicine.</p>
<p>Trasande, who is also a professor in the Department of Population Health, cautions that the analysis was not designed to establish that DEHP directly or alone caused heart disease and that higher death risks did not take into account other types of phthalates. Nor did it include mortality among those in other age groups. As a result, the overall death toll from heart disease connected to these chemicals is likely much higher, he says.</p>
<p>Trasande says that the researchers next plan to track how reductions in <a href="https://medicalxpress.com/tags/phthalate/" rel="tag">phthalate</a> exposure may, over time, affect global mortality rates, as well as to expand the study to other health concerns posed by the chemicals, such as preterm birth. Trasande also serves as director of NYU Grossman School of Medicine's Division of Environmental Pediatrics and the Center for the Investigation of Environmental Hazards.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Phthalate exposure from plastics and cardiovascular disease: global estimates of attributable mortality and years life lost, <i>eBioMedicine</i> (2025). <a data-doi="1" href="https://dx.doi.org/10.1016/j.ebiom.2025.105730" target="_blank">DOI: 10.1016/j.ebiom.2025.105730</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Heart disease deaths worldwide linked to chemical widely used in plastics (2025, April 29)
                                                 retrieved 29 April 2025
                                                 from https://medicalxpress.com/news/2025-04-heart-disease-deaths-worldwide-linked.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon to display tariff costs for consumers (441 pts)]]></title>
            <link>https://punchbowl.news/article/tech/amazon-display-tariff-costs/</link>
            <guid>43831027</guid>
            <pubDate>Tue, 29 Apr 2025 11:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://punchbowl.news/article/tech/amazon-display-tariff-costs/">https://punchbowl.news/article/tech/amazon-display-tariff-costs/</a>, See on <a href="https://news.ycombinator.com/item?id=43831027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                        <p><img width="1024" height="683" src="https://punchbowl.news/wp-content/uploads/GettyImages-2209820827.jpg" alt="Amazon will soon display how much of an item’s cost is derived from tariffs — right next to the product’s total listed price." decoding="async" srcset="https://punchbowl.news/wp-content/uploads/GettyImages-2209820827.jpg 1024w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-300x200.jpg 300w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-768x512.jpg 768w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-50x33.jpg 50w" sizes="(max-width: 1024px) 100vw, 1024px">                        </p>

                        
                        
                        <div>
                                                            <p><strong>Amazon doesn’t want</strong>&nbsp;to shoulder the blame for the cost of President&nbsp;<strong>Donald Trump</strong>’<strong>s</strong>&nbsp;trade war.</p>
<p><strong>So the e-commerce giant</strong>&nbsp;will<strong>&nbsp;</strong>soon show how much Trump’s tariffs are adding to the price of each product, according to a person familiar with the plan.</p>
<p><strong>The shopping site&nbsp;</strong>will display how much of an item’s cost is derived from tariffs – right next to the product’s total listed price.</p>
                                
                                                                                        <div>
        <p><img src="https://punchbowl.news/wp-content/themes/punchbowl-news/assets/images/tech-icon.svg" alt="Subscripion logo">
        </p>
        <h4>You're seeing a preview of our <span>Premium Policy: Tech</span> coverage. Read the full story by <a href="https://punchbowl.news/pricing">subscribing here.</a></h4>
    </div>                            
                            
                            
                        </div>
                    </div><p>Editorial photos provided by Getty Images. Political ads courtesy of AdImpact.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generative AI is not replacing jobs or hurting wages at all, say economists (309 pts)]]></title>
            <link>https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/</link>
            <guid>43830613</guid>
            <pubDate>Tue, 29 Apr 2025 10:08:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/">https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/</a>, See on <a href="https://news.ycombinator.com/item?id=43830613">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Instead of depressing wages or taking jobs, generative AI chatbots like ChatGPT, Claude, and Gemini have had almost no wage or labor impact so far – a finding that calls into question the huge capital expenditures required to create and run AI models.</p>
<p>In <a target="_blank" rel="nofollow" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5219933">a working paper</a> released earlier this month, economists Anders Humlum and Emilie Vestergaard looked at the labor market impact of AI chatbots on 11 occupations, covering 25,000 workers and 7,000 workplaces in Denmark in 2023 and 2024.</p>
<p>Many of these occupations have been described as being vulnerable to AI: accountants, customer support specialists, financial advisors, HR professionals, IT support specialists, journalists, legal professionals, marketing professionals, office clerks, software developers, and teachers.</p>

    

<p>Yet after Humlum, assistant professor of economics at the Booth School of Business, University of Chicago, and Vestergaard, a PhD student at the University of Copenhagen, analyzed the data, they found the labor and wage impact of chatbots to be minimal.</p>
<blockquote>

<p>AI chatbots have had no significant impact on earnings or recorded hours in any occupation</p>
</blockquote>
<p>"AI chatbots have had no significant impact on earnings or recorded hours in any occupation," the authors state in their paper.</p>
<p>The report should concern the tech industry, which has hyped AI's economic potential while plowing billions into infrastructure meant to support it. Early this year, OpenAI <a target="_blank" href="https://www.theregister.com/2025/01/06/altman_gpt_profits/">admitted</a> that it loses money per query even on its most expensive enterprise SKU, while companies like <a target="_blank" href="https://www.theregister.com/2025/04/09/microsoft_puts_more_datacenter_builds/">Microsoft</a> and <a target="_blank" href="https://www.theregister.com/2025/04/22/aws_datacenter_leases/">Amazon</a> are starting to pull back on their AI infrastructure spending in light of <a target="_blank" href="https://www.theregister.com/2025/03/14/ai_running_out_of_juice/">low</a> business adoption past a few <a target="_blank" rel="nofollow" href="https://www.wsj.com/articles/johnson-johnson-pivots-its-ai-strategy-a9d0631f">pilots</a>.</p>

        


        

<p>The problem isn't that workers are avoiding generative AI chatbots - quite the contrary. But they simply aren't yet equating to actual economic benefits.</p>
<blockquote>

<p>The adoption of these chatbots has been remarkably fast ... But then when we look at the economic outcomes, it really has not moved the needle</p>
</blockquote>
<p>"The adoption of these chatbots has been remarkably fast," Humlum told <em>The Register</em>. "Most workers in the exposed occupations have now adopted these chatbots. Employers are also shifting gears and actively encouraging it. But then when we look at the economic outcomes, it really has not moved the needle."</p>
<p>The researchers looked at the extent to which company investment in AI has contributed to worker adoption of AI tools, and also how chatbot adoption affected workplace processes.</p>
<p>While firm-led investment in AI boosted the adoption of AI tools — saving time for 64 to 90 percent of users across the studied occupations — chatbots had a mixed impact on work quality and satisfaction.</p>

        

<p>The economists found for example that "AI chatbots have created new job tasks for 8.4 percent of workers, including some who do not use the tools themselves."</p>
<p>In other words, AI is creating new work that cancels out some potential time savings from using AI in the first place.</p>
<p>"One very stark example that it's close to home for me is there are a lot of teachers who now say they spend time trying to detect whether their students are using ChatGPT to cheat on their homework," explained Humlum.</p>

        

<p>He also observed that a lot of workers now say they're spending time reviewing the quality of AI output or writing prompts.</p>
<p>Humlum argues that can be spun negatively, as a subtraction from potential productivity gains, or more positively, in the sense that automation tools historically have tended to generate more demand for workers in other tasks.</p>
<p>"These new job tasks create new demand for workers, which may boost their wages, if these are more high value added tasks," he said.</p>
<ul>

<li><a href="https://www.theregister.com/2025/04/28/ibm/">Artist formerly known as Indian Business Machines pledges $150B for US ops, R&amp;D</a></li>

<li><a href="https://www.theregister.com/2025/04/27/darpa_expmath_ai/">DARPA to 'radically' rev up mathematics research. And yes, with AI</a></li>

<li><a href="https://www.theregister.com/2025/04/25/google_admits_depreciation_costs_soaring/">Google admits depreciation costs are soaring amid furious bit barn build</a></li>

<li><a href="https://www.theregister.com/2025/04/24/sustainability_still_not_a_high/">Sustainability still not a high priority for datacenter industry</a></li>
</ul>
<p>But overall, the time savings from using AI was less than expected. According to the study, "users report average time savings of just 2.8 percent of work hours" from using AI tools. That's a bit more than one hour per 40 hour work week.</p>
<p>The authors note that this finding differs from other randomized controlled trials that have found productivity benefits on the order of <a target="_blank" rel="nofollow" href="https://academic.oup.com/qje/article/140/2/889/7990658">15 percent</a>. And they explain this discrepancy by saying that other studies have focused on occupations with high AI productivity potential and that real-world workers don't operate under the same conditions.</p>
<p>"So I think there are two key reasons why the real economic gains are lower than [the cited studies]," said Humlum, noting that his study relies on actual tax data.</p>
<p>"First, most tasks do not fall into that category where ChatGPT can just automate everything. And then second, we're in this middle phase where employers are still waking up to the new reality, and we're trying to figure out how to best really realize the potential in these tools. And just at this stage, it's just not been that much of a game changer."</p>
<p>Where there are productivity gains to be had, Humlum and Vestergaard estimate that only a small portion of that benefit – between 3 and 7 percent – gets passed through to workers in the form of higher earnings.</p>
<p>Humlum said while there are gains and time savings to be had, "there's definitely a question of who they really accrue to. And some of it could be the firms – we cannot directly look at firm profitability. Some of it could also just be that you save some time on existing tasks, but you're not really able to expand your output and therefore earn more.</p>
<p>"So it's like it saves you time writing emails. But if you cannot really take on more work or do something else that is really valuable, then that will put a damper on how much we should actually expect those time savings to affect your earning ability, your total hours, your wages."</p>
<p>Humlum said the impact of using AI chatbots, in the form of productivity, time savings, and work quality, can be improved through company commitment to internal education and evangelism. He pointed in particular to how firm initiatives can reduce the tool-usage gender gap – fewer women use these tools than men.</p>
<p>But doing so at this point doesn't show much promise of payoff.</p>
<p>"In terms of economic outcomes, when we're looking at hard metrics – in the administrative labor market data on earnings, wages – these tools have really not made a difference so far," said Humlum. "So I think that that puts in some sense an upper bound on what return we should expect from these tools, at least in the short run.</p>
<p>"My general conclusion is that any story that you want to tell about these tools being very transformative, needs to contend with the fact that at least two years after [the introduction of AI chatbots], they've not made a difference for economic outcomes." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to build Intrinsic Motivation: a review of the science (127 pts)]]></title>
            <link>https://erringtowardsanswers.substack.com/p/intrinsic-motivation</link>
            <guid>43830544</guid>
            <pubDate>Tue, 29 Apr 2025 09:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erringtowardsanswers.substack.com/p/intrinsic-motivation">https://erringtowardsanswers.substack.com/p/intrinsic-motivation</a>, See on <a href="https://news.ycombinator.com/item?id=43830544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Soon after I entered into year 12, something crazy happened:&nbsp;</p><p>I started studying.&nbsp;</p><p>Like – a lot.&nbsp;</p><p>Between the ages of 12 and 16, I’d had no motivation for school whatsoever. I didn’t do my homework. I didn’t revise for my exams. I diligently ignored everything my teachers said to me.&nbsp;</p><p>While I was doing my GCSEs, my parents would force me to sit in my room with no distractions and study for two hours each day. Rather than put this time to good use, I would simply sit at my desk, stare out the window, and run down the clock.</p><p>But then suddenly, almost overnight, something changed:</p><p>I started to care about doing well. I started to find my subjects interesting. I started to like – and listen to – my teachers. I developed absolute tunnel vision for my studies, and I eventually managed some of the best A Level grades in my entire school.</p><p>From the outside, it looked like I had suddenly decided to knuckle down and start taking my studies seriously – but from the inside, this isn’t what happened at all.&nbsp;</p><p>There was no knuckling down going on; I wasn’t working hard in any real sense. I would often put in twelve hours of studying each day and at no point did I feel tired or strained or stressed. I felt curious, energised – excited.&nbsp;</p><p>This kind of experience – this sudden turning on (and off) – of motivation has happened to me a number of times throughout my life.&nbsp;&nbsp;</p><p>Sometimes – for reasons that I’ve been trying to understand – I’m able to work happily and without rest for many weeks or months at a time. But at others, it’s as if the supply of motivation has run dry and getting anything done feels next to impossible.&nbsp;</p><p>I don’t think I’m alone in this.&nbsp;</p><p>In fact, to some extent or another, I think the above description probably applies to everyone.&nbsp;</p><p>I also don’t think many of us have a good idea as to what’s going on here.&nbsp;</p><p>This post is my attempt to find out.&nbsp;</p><p><span>This is a topic I’ve been researching in and around for quite a while, and as far as I can see, all roads lead to the idea of </span><strong>Intrinsic Motivation.</strong></p><p>Playing sport. Writing. Painting. Travelling. Exploring. Hiking. Reading.&nbsp;</p><p>For the most part, we engage in these activities for their own sake&nbsp;– because they are inherently enjoyable.&nbsp;</p><p><span>When we do something for its own sake – without regard for rewards, punishments, or really outcomes of any kind – we can be said to be </span><em>intrinsically motivated</em><span>. In contrast, when we’re motivated by external pressures and outcomes, we can can be said to be extrinsically motivated.</span></p><p><span>There’s good evidence to suggest that the more intrinsically motivated we are to do a task, the more we enjoy it,  </span><a href="https://www.sciencedirect.com/science/article/pii/1041608090900148" rel="">the better we learn</a><span>, </span><a href="https://www.sciencedirect.com/science/article/abs/pii/0022103179900398" rel="">the better we perform</a><span>, and </span><a href="https://www.researchgate.net/publication/5640891_Does_Intrinsic_Motivation_Fuel_the_Prosocial_Fire_Motivational_Synergy_in_Predicting_Persistence_Performance_and_Productivity" rel="">the more likely we are to persevere</a><span> in the face of obstacles and setbacks.</span></p><p>The hypothesis that I’ll be pushing throughout this piece is that whenever I’ve found myself in one of these high-motivation life periods, I’ve unwittingly stumbled upon a rich vein of intrinsic motivation. It therefore stands to reason that if I want to understand these experiences – and how to create more of them – I need to understand intrinsic motivation.&nbsp;</p><p>As it turns out, intrinsic motivation looks to be an extremely delicate thing. Under the right conditions, it can be encouraged and drawn out of us; in the wrong conditions, it can be suffocated, stifled – maybe even killed.&nbsp;</p><p><span>Thankfully, there’s been an absolute mountain of research done in this area – mostly carried out under the rubric of </span><a href="https://en.wikipedia.org/wiki/Self-determination_theory" rel="">Self-Determination Theory</a><span> – and in this piece, I’m going to do a deep-dive on it all.&nbsp;</span></p><p>Here goes.</p><p>Main topics to be covered:&nbsp;</p><ol><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/what-is-intrinsic-motivation-a-quick-history-some-context" rel="">What is intrinsic motivation? A quick history + some context</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/how-do-psychologists-measure-intrinsic-motivation" rel="">How do psychologists measure intrinsic motivation?&nbsp;</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/what-causes-or-blocks-intrinsic-motivation-a-tour-through-the-research" rel="">What causes – and blocks – intrinsic motivation? A tour through the research</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/what-causes-or-blocks-intrinsic-motivation-a-tour-through-the-research" rel="">Discussion - concerns, criticisms, and miscellaneous ideas:</a></p><ul><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/an-alternative-theory-dopamines-impact-on-intrinsic-motivation" rel="">Dopamine: an alternative explanation of intrinsic motivation</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/distraction-vs-autonomy-an-alternative-explanation" rel="">Distraction vs. Autonomy: another possible alternative explanation</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/the-largest-meta-analysis-on-the-effects-of-rewards-and-reward-contingencies-on-intrinsic-motivation" rel="">The largest meta-analysis looking at the effects of rewards and reward contingencies on intrinsic motivation</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/mimetic-desire-a-missing-piece-of-the-puzzle-maybe" rel="">Mimetic desire: a missing piece of the puzzle maybe?</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/western-centrism-the-most-common-criticism-of-sdt" rel="">Western centrism: the most common criticism of SDT</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/is-the-free-choice-measure-actually-valid" rel="">Is the free-choice measure actually valid?</a></p></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/competition-should-it-be-outlawed" rel="">Should competition be outlawed?</a></p></li></ul></li><li><p><a href="https://erringtowardsanswers.substack.com/i/150476227/final-thoughts-parting-advice" rel="">Final thoughts + parting advice</a></p></li></ol><p>The general concept of intrinsic motivation is quite intuitive, so I’m hoping the sketch I’ve offered up so far will be enough to give you the general gist.&nbsp;</p><p>Saying that, there are a number of possible points of confusion here, so it’s worth spending a little bit of time fleshing out this definition and distinguishing intrinsic motivation from other closely related concepts.&nbsp;</p><p>To do this, a whistle-stop tour through the history and wider research context of the concept is going to be useful (and hopefully interesting, too!):</p><p>When the research on intrinsic motivation first started, behaviourism was still the dominant school of thought within psychology. Operant Psychology – B. F. Skinner’s development on the original behaviourist position – was primarily concerned with how reinforcement and reinforcement contingencies influence the frequency of different behaviours.&nbsp;</p><p>The upshot (with many qualifications) was that the more you reward a behaviour, the more frequently that behaviour will occur, e.g. give a mouse some cocaine for pulling a lever and over time the mouse will learn to start pulling that lever like it’s life depends on it.&nbsp;</p><p>Particularly relevant to our discussion here is the fact that there is no space in this theory for intrinsic motivation. Before receiving reinforcement, behaviour is more or less random (e.g. maybe the mouse moves around randomly and knocks the lever, resulting in cocaine being administered). Once reinforcement has been dished out, motivation is then present to the extent that the behaviour has been reinforced.&nbsp;</p><p>That’s pretty much it.&nbsp;&nbsp;</p><p>Credit where it’s due: behaviourism and operant psychology were able to shed light on many previously enshadowed phenomena –&nbsp; but over time, a number of experimental findings began to emerge that put this paradigm under increasing pressure.&nbsp;&nbsp;</p><p>Here are a couple of them:&nbsp;</p><ul><li><p><a href="https://www.degruyter.com/document/doi/10.7312/ward90352-012/html" rel="">Nissen (1930)</a><span> found that rats would cross an electrified grid in order to get to a novel maze area on the other side. Because neither the grid nor the novel space had been paired with a reinforcer, operant psychology would predict that the electrified grid would act as a negative reinforcer, deterring the exploratory behaviour. This obviously isn’t what happened.&nbsp;</span></p></li><li><p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0045931" rel="">Butler (1957)</a><span> found that rhesus monkeys would learn discrimination problems solely for the opportunity to visually explore the environment. Again, this exploratory behaviour had not been previously reinforced, so this left behaviourists their heads.&nbsp;</span></p></li><li><p><a href="https://psycnet.apa.org/record/1956-02388-001" rel="">Montgomery (1955)</a><span> gave rats a chance to either return to their home base or explore a novel environment (that had never been reinforced in any way). The rats showed a strong preference for the latter.&nbsp;</span></p></li><li><p>Harlow (1953b) found that rhesus monkeys would solve discrimination tasks with the sole reward of being able to manipulate novel objects. Interestingly, these manipulation drives were extremely difficult to eradicate via the usual processes of extinction (e.g. if you stop rewarding the rat with cocaine when it pulls the lever, it will eventually stop pressing the lever. Not so with the drive to manipulate).</p></li></ul><p>The trend here, as I’m sure you’ve noticed, is that there are certain kinds of behaviours that we – or, at least, animals generally – are reliably motivated to perform but that have never been reinforced. A certain class of these behaviours seems to be tied to exploration, and this class of behaviour is often far more resistant to extinction than the kinds of behaviours that have been learned via reinforcement, e.g. the lever pressing of a coke addicted mouse.&nbsp;</p><p><span>After a decade or so of discussion, nobody was able to find a way of convincingly integrating these findings into the dominant paradigms of the day. This culminated in a </span><a href="https://psycnet.apa.org/record/1961-04411-001" rel="">1959 paper by Robert White</a><span>, where he proposed that these behaviours were best viewed as being fuelled by innate psychological tendencies – tendencies associated with interest, curiosity, exploration, and play.&nbsp;&nbsp;</span></p><p>There’s a whole lot more to this story than is worth covering here, but the crux of this all is that White’s understanding of these behaviours ran in the face of the behaviourist theories of the day and ultimately paved the way for much of the research into intrinsic motivation that has happened since.&nbsp;</p><p>As it goes, almost all of the research into intrinsic motivation has been done under the rubric of Self-Determination Theory (SDT). Part of the reason for this is that SDT pretty much started out as THE study of intrinsic motivation (more on this shortly), before then expanding into a fully-fledged theory of human flourishing in its own right.&nbsp;</p><p>For good measure, here’s a fleshed out definition of intrinsic motivation from the founders of SDT themselves:</p><p>“[Intrinsic Motivation is] the primary and spontaneous propensity of some organisms, especially mammals, to develop through activity—to play, explore, and manipulate things and, in doing so, to expand their competencies and capacities. This natural inclination is an especially significant feature of human nature that affects people’s cognitive and emotional development, quality of performance, and psychological well-being. It is among the most important of the inner resources that evolution has provided (Deci &amp; Ryan, 2000; Ryan &amp; Hawley, 2016), and because it represents a prototypical manifestation of integrative organismic tendencies, SDT research began with it as a primary focus."</p><p>So, in other words: intrinsic motivation isn’t really about engaging in purely pleasurable activities. For example, I don’t think that SDT would say that eating delicious cookies would be an example of an intrinsically motivated activity. Rather, SDT’s conception of intrinsic motivation is more about engaging in activities that allow us to explore, expand, and develop our capacities (and that also happen to be deeply enjoyable!).&nbsp;</p><p>Up until fairly recently – when I started to make a habit of subsuming myself in different research literatures for many weeks at a time (as you do) – I had never come across SDT before. Given the amount and type of content I consume, I have no idea why this is the case.&nbsp;</p><p>SDT is a massive area of study.&nbsp;</p><p>There are thousands of researchers working in this space. Thousands (?) of studies have been conducted. Multiple millions of citations have been accrued.</p><p>As far as I can see, SDT has been developed slowly and patiently, with a sound scientific methodology. Its theories have been arrived at inductively, and they have been revised and refined to account for the best available empirical research. Many of its core findings do seem to replicate and show up consistently within large-scale meta-analyses.</p><p>What’s more, if you put any stake in h-indices – a measure of a researcher’s impact and productivity – the founders of SDT theory are some of the most influential scholars alive.</p><p>For reference, father of modern linguistics Noam Chomsky has a h-index of 196. Richard Dawkins has a h-index of 83.&nbsp;</p><p>The founders of SDT, E. L. Deci and Richard Ryan, have h-indices of 180 and 229, respectively.&nbsp;</p><p>So, like, point being: SDT does seem to be a legitimate scientific enterprise, and yet outside of certain academic circles, its ideas don’t receive any air time at all.&nbsp;</p><p>I don’t know why – and I’ve not been able to find anything online to explain this to my satisfaction.&nbsp;</p><p>But regardless:&nbsp;</p><p>The SDT view of motivation, which we’ll tentatively be adopting from here on out, views motivation as a spectrum ranging from most to least autonomous.&nbsp;</p><p>Here’s a diagram:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg" width="880" height="694" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:694,&quot;width&quot;:880,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc033fc7b-8245-4946-bcc6-d8cf3390b046_880x694.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>On the high autonomy end, you have intrinsic motivation, i.e. behaviours that are more or less entirely engaged in for their own sake.&nbsp;</p><p>Slightly lower down, you have behaviours that are extrinsically motivated but highly autonomous. For example, let’s say that I grew up in a Christian household and that I eventually came to endorse and enact a Christian ethic. By SDT’s lights, behaviours that flow from this ethic would be classed as extrinsically motivated, because they came from an external source – namely, my formative environment – but they would also be classed as highly autonomous, because I engage in them freely and willingly. They are self-endorsed.&nbsp;</p><p>A little further along the spectrum, you then get lower autonomy forms of extrinsic motivation. For example, let’s imagine that I grew up in a Christian household but that I did not come to assimilate the Christian ethic that I grew up around. Now, when I visit my family, I might find myself adhering to certain Christian norms, but I would only be doing so to avoid criticism or argument, rather than because I actively endorse these regulations.</p><p>And then all the way off to the left of the spectrum, you have amotivation, which is where you simply have zero motivation – neither intrinsic or extrinsic – to do something.&nbsp;</p><p>One point worth making before we move on is this:</p><p>SDT assumes that most behaviours are motivated by a mix of these motivation types. As a general rule, the more autonomous forms of motivation are better: they are experienced as being far less effortful, and they are associated with greater persistence and superior performance than controlled forms of motivation.&nbsp;</p><p>When I interpret my sudden turn to scholarliness through an SDT lens, here’s my best high-level guess as to what happened: I shifted from the far left side of the spectrum to somewhere much further to the right.&nbsp;</p><p>I’m a naturally curious person who enjoys learning. It makes sense that studying interesting subjects would tap into my intrinsic motivation.&nbsp;</p><p>But at the same time, I wasn’t simply studying for curiosity’s sake. I was also studying because I wanted to get good grades, I wanted to outperform my classmates, I wanted to get into a good university, etc.</p><p>So intrinsic and extrinsic forms of motivation were probably both in action here.&nbsp;</p><p>The question I’m trying to answer with this piece is: why did this happen when it did?&nbsp;</p><p>This question breaks down into two subquestions</p><ol><li><p>Why did I suddenly become intrinsically motivated to study?</p></li><li><p>Why did I suddenly shift from low to high-autonomy forms of extrinsic motivation?</p></li></ol><p>As you will have gathered by now, this piece is primarily about intrinsic motivation, so you may think that I’m only addressing the first part of this question.&nbsp;</p><p>But as it goes, the conditions that SDT posits as being necessary for intrinsic motivation are also the same conditions that it posits as being necessary for high autonomy forms of extrinsic motivation.&nbsp;&nbsp;</p><p>So by covering the intrinsic motivation research – which is, I should add, the seed from which all subsequent SDT research eventually sprung – we’ll be able to understand the conditions that are conducive to all forms of autonomous motivation.&nbsp;</p><p>I’ll circle back to this point later, but thought it was worth flagging now.&nbsp;</p><p>I promise: I will get into the meat of this piece shortly (next section), but before I do, I need to share a quick overview of the way intrinsic motivation is operationalised in lab experiments, because it’s interesting and also maybe not beyond criticism.</p><p>In the 70s, E. L. Deci – one of the pioneers of intrinsic motivation research – developed the free-choice paradigm to investigate the impact of various rewards and reward contingencies on intrinsic motivation.&nbsp;</p><p>I’ve read through a fairly wide range of papers in this space, and while the free-choice paradigm is often applied slightly differently in different contexts, there are a number of features that show up in pretty much all of them.&nbsp;&nbsp;</p><p>First, participants are broken up into at least two groups – the control group and the experimental group(s).&nbsp;</p><p>In the initial period, both groups are encouraged to engage in some kind of intrinsically interesting task – for example, a puzzle.&nbsp;</p><p>(Note: experimenters use a range of methods to make sure that these intrinsically interesting tasks are actually intrinsically interesting for the participants).&nbsp;</p><p>During this initial period, the control group are generally left to engage with the task uninterrupted, while the experimental groups are interfered with in some way.&nbsp; This is where the independent variable gets deployed. So, for example, maybe the experimental group are told that they’re going to be rewarded for engaging with the task (independent variable = expectation of reward) – or maybe they’re told that they’re going to be evaluated based on task performance (independent variable = expectation of evaluation).&nbsp;</p><p>Once the initial period is complete, all participants then enter what is known as the free-choice period. This is where the experimenter leaves the room and participants are left on their own, apparently unobserved, with the task materials.&nbsp;</p><p>Of note: there will also usually be a number of other activities or distractions intentionally left in the room – so, for example, in a study with preschoolers, a selection of toys were left in the room; in a study with adults, a selection of magazines.&nbsp;</p><p>Within this paradigm, then, intrinsic motivation is measured in two main ways:</p><ol><li><p>Free choice measure – the amount of time participant’s spend with the ‘intrinsically motivating’ task in the free-choice period. The idea here is that the more intrinsically motivated the participant is to engaged with the task, the longer they will choose to spend on that task when nobody is watching or evaluating them in any way. This is generally taken to be the primary metric of interest.</p></li><li><p>Intrinsic Motivation Inventory – basically just a self-report questionnaire to try and get a read on how much the participant finds the task enjoyable and interesting in the free-choice period. This is usually taken to be a sort of supplementary measure.&nbsp;</p></li></ol><p>As it turns out, these two measures are only modestly correlated with one another, which seems to have been a topic of some contention within this field. In the largest meta-analysis ever conducted on rewards and intrinsic motivation (see discussion section), these measures mostly moved in the same direction, but their effect sizes varied quite a lot. As far as I’ve been able to work out, the free-choice measure is generally viewed as the most important of the two because it avoids the well-documented shortcomings of self-report measures.</p><p>I have one or two concerns with the way SDT is measured, but i don’t want to get bogged down in the minutiae without first giving you the interesting stuff, so i’ve saved this for the discussion section. Read: is the free-choice measure actually valid? </p><p>This is probably all you need to know for now.&nbsp;</p><p>So – onto the research!</p><p><span>The earliest experiments on intrinsic motivation were </span><a href="https://psycnet.apa.org/record/1971-22190-001" rel="">run in 1971 by E. L. Deci</a><span>. He wanted to understand what would happen to a person’s intrinsic motivation for a task if you offered them a monetary reward for doing it.&nbsp;</span></p><p>As mentioned already, the behaviourist assumption at the time was that when you reinforce a behaviour, its frequency increases – and also, importantly, that when you remove said reinforcement, the frequency of the behaviour returns to baseline.&nbsp;</p><p>Deci’s experiment was intended to see if these assumptions – particularly the second one – hold up.&nbsp;</p><p><span>In his first free-choice experiment, he asked both groups – the control and the treatment – to work on a </span><a href="https://en.wikipedia.org/wiki/Soma_cube" rel="">Soma Cube</a><span> puzzle. The treatment group were given a $1 reward for every puzzle they completed, whereas the control group were given nothing – they worked on the puzzle without any expectation of a reward.&nbsp;</span></p><p>When all participants were then put in the free-choice condition and researchers measured the amount of time they spent with the puzzles, they found that the treatment group – i.e those that had been given the $1 rewards – actually engaged with the puzzles less.&nbsp;</p><p>So, in other words: when rewards were involved, intrinsic motivation for the task, as measured by the free-choice measure, went below the pre-reward baseline.</p><p>As I understand things, this was a pretty groundbreaking finding at the time&nbsp;– and behaviourists have been bitching about it ever since. Unfortunately for them – behaviourists, I mean – this was soon replicated with a whole host of different tasks, rewards, reward contingencies, participants, etc., so it does look to be a legitimate thing (although there are still some who hold out against it).&nbsp;</p><p><span>(FYI: the observation that extrinsic rewards tend to harm intrinsic motivation is known as the </span><a href="https://thedecisionlab.com/biases/overjustification-effect" rel="">overjustification effect</a><span> (also sometimes referred to as the undermining effect))</span></p><p><span>Before diving in any deeper, an interesting nuance should probably be drawn out here: SDT does not claim that rewards always harm motivation. In fact, the SDT view accepts that </span><em>at the time rewards are being administered</em><span>, they will often enhance motivation (with certain qualifications). This is why SDT generally holds that rewards can be an effective means of motivating menial, uninteresting tasks.&nbsp;</span></p><p><span>The key finding here was that rewards harmed </span><em>subsequent</em><span> intrinsic motivation for an intrinsically interesting task. This may seem like a pedantic point, but – if true – it has some pretty far reaching implications.&nbsp;</span></p><p>Take education: should we be rewarding kids for reading?&nbsp;</p><p><span>The prima facie conclusion of this research is that, no, we probably shouldn’t. Rewards may be an effective means of motivating reading </span><em>at the time they are present</em><span>, but when rewards are removed and kids are left to their own devices, they – the rewards – may actually have a deleterious impact on subsequent intrinsic motivation for reading.&nbsp;&nbsp;</span></p><p>Saying that: there are many qualifications and quirks to this picture, so let’s keep following the research.&nbsp;&nbsp;</p><p>(FYI: there have been a huge number of experiments run in this space, so I’m mostly just going to stick to research that’s in line with the general tenor. You’re getting the highlight reel, but rest assured: many replications and meta-analyses have been done here (some of which we’ll cover).)</p><p><span>In a bid to begin exploring the impact of different reward contingencies on intrinsic motivation, </span><a href="https://selfdeterminationtheory.org/SDT/documents/1972_Deci_OBHP.pdf" rel="">Deci (1972a)</a><span> ran a followup study – once again with the same ‘interesting puzzles’ – but this time he paid the treatment group for simply showing up, rather than for each puzzle completed. The control condition was the same as in the original experiment.&nbsp;</span></p><p>Here, it turned out that participants who received a reward for showing up exhibited just as much intrinsic motivation as those that received no reward at all. So, in other words: within this contingency, rewards did not harm intrinsic motivation at all.&nbsp;</p><p><a href="https://www.semanticscholar.org/paper/Undermining-children%27s-intrinsic-interest-with-A-of-Lepper-Greene/abbcacaa273b8fea38d142e795e968051fa368ea" rel="">Lepper, Green, and Nisbett (1973)</a><span> ran an experiment looking at the impact of different reward contingencies on preschoolers’ intrinsic motivation for a drawing task using ‘attractive materials.’ The study included three groups:&nbsp;</span></p><ol><li><p>Treatment group 1 were told that they would receive a ‘good player’ reward (a star and ribbon) if they completed the drawing task. All children in this group received a good player reward.&nbsp;</p></li><li><p>Treatment group 2 were not told about any rewards beforehand but were then all given a ‘good player’ reward after the drawing task.&nbsp;</p></li><li><p>The Control group were not told anything and did not receive a ‘good player’ reward.</p></li></ol><p>In line with the previous Deci experiment, treatment group 1 showed less intrinsic motivation in the free choice period than the control group – so, once again, rewards undermined intrinsic motivation. But here’s the interesting thing: treatment group 2 showed just as much (if not more) intrinsic motivation in the free choice period as the control group.&nbsp;</p><p>So, in other words: if the reward was dangled beforehand, it harmed intrinsic motivation; if it was unexpected, it didn’t.&nbsp;</p><p>The plot thickens.&nbsp;</p><p><span>This finding was built upon by </span><a href="https://psycnet.apa.org/record/1977-02501-001" rel="">Ross (1975)</a><span>, who compared the impact of salient vs. non-salient rewards on children’s intrinsic motivation to play with a drum kit.&nbsp;</span></p><p>One group of children was simply told that they would receive an unknown prize if they engaged with the drum (non-salient reward). The other group were given the same instructions, but they were told that the unknown prize was hidden inside a box that had been placed conspicuously in front of them (salient reward).&nbsp;</p><p>Turns out that the group with the salient reward showed significantly less intrinsic motivation in the free choice period than the group with the non-salient reward. Ross concluded that the salience of the reward seems to modulate its impact: the more salient, the stronger the deleterious effect on intrinsic motivation.&nbsp;</p><p>I’ll share one more experiment – because it adds texture to this picture, plus it touches on the topic of reading motivation – and then I’ll give a rundown of how SDT has accounted for all of these findings.&nbsp;</p><p><a href="https://www.tandfonline.com/doi/abs/10.1080/19388070701749546" rel="">Marinek and Cambrell (2008)</a><span> were interested in third-grade children’s reading motivation. They compared a no-reward condition with a token-reward condition – kids received something like a star or a sticker for reading – and found that the token rewards significantly reduced intrinsic motivation for reading.&nbsp;</span></p><p>Based on the other research above, this isn’t a major surprise – but here’s where it gets interesting:</p><p>They also ran a third condition where they rewarded children with a book rather than a token. This time round, the reward had no negative effect on intrinsic motivation at all. The explanation put forward in the paper was that rewards that are more proximal to the desired behaviour – e.g. books for reading or, say, paint brushes for painting – do not have the same negative impact on intrinsic motivation as token rewards like money or stickers.&nbsp;</p><p>OK , so, so far we’ve got:</p><ul><li><p>Rewards reduce intrinsic motivation</p></li><li><p>Rewards only reduce intrinsic motivation when they’re contingent on task completion but not when they’re contingent on simply showing up.</p></li><li><p>Salient rewards harm intrinsic motivation, but non-salient ones don’t (or, at least, not as much)</p></li><li><p>Token rewards harm intrinsic motivation, but rewards that relate closely to the task at hand don’t</p></li></ul><p>How do we reconcile these findings?&nbsp;</p><p><span>SDT’s answer, in a word: </span><em><strong>autonomy</strong></em><span>.&nbsp;</span></p><p>Rewards and reward contingencies that preserve or enhance an individual’s sense of autonomy have a positive impact on intrinsic motivation; those that diminish an individual’s sense of autonomy and that are experienced as an attempt to control their behaviour have a negative impact on intrinsic motivation.&nbsp;</p><p>Interpreting these findings through this lens, then:</p><ul><li><p>In the first Deci experiment, where individuals were rewarded with $1 for each puzzle completed, it’s argued that the monetary reward was experienced by participants (whether consciously or not) as a means of controlling their behaviour. This resulted in a reduction in autonomy, which subsequently led to a decline in intrinsic motivation.&nbsp;</p></li><li><p>In the second Deci study, where individuals were given a monetary reward for showing up but not for completing the puzzles, it’s argued that the monetary reward was not experienced by participants as a means of controlling their behaviour – since they’d get the reward regardless of whether or not they completed any puzzles – which is why it didn’t impact intrinsic motivation.&nbsp;</p></li><li><p>In the Lepper, Green, and Nisbett study, where individuals in treatment 1 were told they would receive a ‘good player’ reward if they performed well, it’s argued that the expectation of evaluation and the dangling of a reward was interpreted as a means of controlling their behaviour. This likewise resulted in a reduction in autonomy and intrinsic motivation.&nbsp;</p></li><li><p>But in the same study, for treatment 2, where individuals were not forewarned about the ‘good player’ reward – but where they received the reward nonetheless – it’s argued that the activity was engaged in autonomously – without any external control – and that intrinsic motivation was therefore unaffected.&nbsp;</p></li></ul><p><span>So, with the </span><em>autonomy</em><span> element of the theory in place&nbsp;– there are a few more key components that still need to be introduced – we can then begin taking a look at experiments that actively seek to manipulate this variable.&nbsp;</span></p><p>Here are two interesting examples, plus a meta-analysis for good measure:</p><ol><li><p><a href="https://journals.sagepub.com/doi/abs/10.1177/014616727800400317" rel="">Zuckerman, Porac, Lathin, Smith, and Deci (1978)</a><span> used the standard free-choice paradigm, but they allowed one group of participants to choose which puzzles they engaged with – plus how they allotted their time between them – while the other group were yoked to the choices of a randomly selected counterpart from the first group. Turns out that participants from the first group, i.e. those who got to choose, showed significantly more intrinsic motivation in the free choice period. I’ve come across a handful of other studies that show the same thing: </span><a href="https://selfdeterminationtheory.org/wp-content/uploads/2019/10/2010_PatallCooperWynn_JEP.pdf" rel="">introducing choice often seems to improve performance</a><span>.&nbsp;</span></p></li><li><p><span>This one isn’t strictly about the relationship between choice and intrinsic motivation, but I thought I’d throw it in anyway because it’s closely related and also interesting: </span><a href="https://scholar.google.co.uk/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Murayama+et+al+%282015%29&amp;btnG=" rel="">Murayama et al (2015)</a><span> had participants play a game where they were challenged to pause a stopwatch within 50 milliseconds of the 5-second mark. One group was allowed to choose which stopwatch they used (there were a number of different options); the other group were assigned a stopwatch at random. While this was going on, all participants were put in an fMRI to see what was going on in their brains. Turns out that performance was better in the choice condition. That participants in the choice condition were more resilient to negative feedback. That participants in the choice condition showed no drop in ventromedial PFC activity&nbsp; following failure, but there was a drop in vmPFC activity for participants in the no-choice condition.</span></p></li><li><p><a href="https://psycnet.apa.org/record/2008-01984-005" rel="">Patall, Cooper, and Robinson (2008)</a><span> ran a meta-analysis with 41 studies to examine the effects of choice on intrinsic motivation and related outcomes. They found robust effects showing that choice enhances intrinsic motivation in both adults and children.&nbsp;</span></p></li></ol><p>So, there you have it:</p><p>As predicted, when we give people more choice – when you increase their sense of autonomy – it boosts their intrinsic motivation (and performance too).&nbsp;</p><p>I have a few qualms with the whole autonomy angle – to be discussed in the discussion section (where else!) –&nbsp; but when I try to match this up against my own experiences with intrinsic motivation, it fits pretty well.</p><p>Take, once again, the monomaniacal study spree discussed in the intro to this piece.&nbsp;</p><p>During my GCSEs, i.e. before I started working like a madman, my parents and teachers exerted constant pressure on me to study. As a result, my studies never felt self-motivated – autonomy was in the gutter – and I had zero intrinsic motivation.&nbsp;</p><p>But when I moved into year 12 – which is when the monomania began – there was a big and definite uptick in the amount of autonomy I was being offered. To give a few examples:</p><ul><li><p><span>Uniform – we were no longer assigned a uniform. There </span><em>was</em><span> a sixth form dress code, but within this dress code there was a fair amount of scope to choose what we wore.</span></p></li><li><p>Exams – at the start of sixth form, there was an extended respite from the pressure that came with our GCSEs. This armistice might have given me enough space to begin generating my own motivation for school.&nbsp;</p></li><li><p>Ambient climate – teachers started to treat us like adults that could be listened to and respected, rather than drones that needed to be coerced into compliance. We were allowed to leave the school grounds at lunch. We had free periods, in which we could do more or less whatever we wanted. Some of us even started getting cars.</p></li></ul><p>So, yeah, in sum: my explosion in motivation does seem to coincide with a sharp uptick in autonomy.</p><p>This still doesn’t explain everything though: for example, why did I specifically become monomaniacal about school? Why not something else – like, say, video games or football or puzzles?&nbsp;</p><p>By introducing further elements in the SDT theory, I think we can start to explain some of this.&nbsp;</p><p>—</p><p>Assuming SDT is true, so far we’ve established that when rewards are experienced as controlling , i.e. when they infringe upon an individual’s sense of autonomy, they have a negative impact on intrinsic motivation.&nbsp;</p><p>But what about verbal rewards? Do they count?&nbsp;</p><p>This is important, since a massive percentage of the rewards we receive and dispense in life are of this kind.&nbsp;</p><p>The previous operant paradigm would have us believe that verbal rewards are likely to reinforce a behaviour and thereby increase its frequency. So, for example, if a kid does something good – like e.g. sharing their toys with their siblings – we ought to praise them. But maybe, as per the research above, this isn’t the case?&nbsp;</p><p>In much of the early research on verbal rewards and intrinsic motivation, participants were placed in the standard free-choice paradigm and given positive verbal feedback for working on an activity. For example, if they completed the activity, they were told ‘you did very well in completing the task; many people did not.’ If they failed to complete the activity, they were told ‘this was a very difficult task but you were progressing well with it.’</p><p><span>Turns out that participants who were given positive feedback </span><a href="https://pubmed.ncbi.nlm.nih.gov/10589297/" rel="">generally displayed MORE free-choice persistence</a><span>, i.e. intrinsic motivation, than those weren’t given any feedback at all.&nbsp;</span></p><p>We’ve just learned that token rewards have a negative impact on intrinsic motivation; now we’re finding out that verbal rewards apparently do the opposite. Again, given the research discussed so far, this isn’t what we’d expect at all.&nbsp;</p><p>Does SDT have a means of reconciling these findings?&nbsp;</p><p><span>You betcha – but it needed to introduce another component to its theory in order to do so: </span><em><strong>competence</strong></em><span>.</span></p><p>All things being equal, if a reward signals that its recipient is competent with respect to a given activity or task, it will have a positive impact on intrinsic motivation for that task/activity.&nbsp;</p><p>So, for example, let’s say I’m learning to paint. I spend an hour brushing away at my canvas, and at the end of the session, my instructor says something to the effect of ‘you have a wonderful eye for colour’ (or whatever a painting instructor might say to a pupil!).&nbsp;</p><p>This feedback serves as a signal that I am competent and therefore bolsters my intrinsic motivation for painting (which, I guess, is kind of what our folk psychology would predict?).</p><p>But here’s the thing: the vast majority of rewards and reward contingencies affect competence and autonomy at the same time. Take this experiment:&nbsp;</p><p><a href="https://search.worldcat.org/title/The-effects-of-anticipated-vs.-unanticipated-social-reward-on-subsequent-intrinsic-motivation/oclc/2896769" rel="">Smith (1975)</a><span> assigned three groups to a learning task about Art History. One group was told beforehand that they were going to receive a written evaluation; the other two weren’t. Of those who weren’t, one group received an unanticipated evaluation after the learning task – one didn’t. Unbeknownst to all participants, everybody that received an evaluation received a positive one. As per usual, they were all then observed in a free-choice period to see how much they engaged with the original task when left to their own devices.&nbsp;</span></p><p>The main finding here was that those in the first group – i.e. those who were told beforehand that they would be evaluated and who then received a positive evaluation – displayed significantly less intrinsic motivation than either of the other groups.&nbsp;</p><p>SDT would explain this result by saying that the anticipated evaluation negatively affected autonomy while positively affecting competence. This then netted out at a slight reduction in intrinsic motivation. In contrast, the unanticipated evaluation had no negative effect on autonomy, but a positive effect on competence – which netted out as an increase in intrinsic motivation.&nbsp;</p><p>This is generally how SDT understands most rewards and rewards contingencies: rewards that signal competence are conducive to intrinsic motivation, but unfortunately, many rewards also inadvertently impair autonomy – which is why they so often seem to harm intrinsic motivation.&nbsp;</p><p>Returning once again to the example of my monomaniacal study spree: this all tracks really nicely.&nbsp;</p><p>Why did I become obsessive about studying rather than other things?&nbsp;</p><p>Because I was good at it.</p><p>Once my teachers and parents stepped back, my sense of autonomy increased. This freed up some autonomous motivation for studying, and as soon as I started to put in the work, my results improved radically. Improved results were a strong competence signal, which freed up even more motivation. More motivation = even better results = even more motivation = even better results, etc.&nbsp;</p><p><span>But there was also another dynamic at play: the better my grades became, the more relaxed my teachers and parents became. This meant they gave me a lot more breathing space to do my own thing – read: more autonomy. </span><em>Even</em><span> </span><em>more</em><span> autonomy = even more intrinsic motivation = even more autonomy, etc.</span></p><p>As far as I can see, I was essentially locked inside multiple feedback loops.&nbsp;</p><p>—</p><p>If you’ve ever done any reading in and around SDT, you’ll know that the theory identifies three psychological needs – not two.&nbsp;</p><p>Autonomy and competence are the first and the second.</p><p><em><strong>Relatedness</strong></em><span> is the third.&nbsp;&nbsp;</span></p><p>Relatedness is all about how connected we feel to those around us.&nbsp;</p><p>Turns out that relatedness is another extremely important component in SDT’s model of intrinsic motivation. This particular element of the theory was stumbled upon accidentally, and the story’s pretty interesting, so I thought I’d tell it:</p><p><a href="https://www.semanticscholar.org/paper/The-undermining-and-enhancing-of-intrinsic-in-Anderson-Manoogian/afb7da115c090d0a4a0bfcf7b27514029935035d" rel="">Anderson, Manoogian, and Reznick (1976)</a><span> were examining the effects of rewards and feedback on young children’s intrinsic motivation. The usual fare.&nbsp;</span></p><p>In a bid to create a condition with neither positive nor negative feedback, they instructed researchers within this condition to remain silent and unresponsive to any overtures from the children. As a no-reward, no-feedback condition, we’d expect this to have a fairly neutral impact on intrinsic motivation – but this isn’t what happened at all:&nbsp;</p><p>Children in this group showed less intrinsic motivation than any group in the study – including those who received negative feedback!</p><p>It was theorised that these children felt rejected by the researchers, which thwarted their need for relatedness and thereby destroyed any intrinsic motivation they may have had for the task.</p><p>Plenty of later research supports this idea, e.g.&nbsp;</p><ul><li><p><a href="https://journals.sagepub.com/doi/10.1177/027243169401400207" rel="">Ryan, Stiller, and Lynch (1994)</a><span> surveyed a large group of junior high students. They found that students who felt security with teachers (and other figures in their lives) were more engaged with school work and exhibited higher levels of intrinsic motivation.&nbsp;</span></p></li><li><p><a href="https://selfdeterminationtheory.org/SDT/documents/2008_BaoLam_CD.pdf" rel="">Bao and Lam (2008)</a><span> looked at the importance of autonomy and relatedness for intrinsic motivation to study in Chinese students. They found that kids who make their own choices, rather than having them made for them show higher levels of intrinsic motivation, but that this was mediated by how close these children felt to their parents. If they were close, then choice was less important and they showed just as much intrinsic motivation as their higher autonomy peers.</span></p></li><li><p><span>This theory also lines up very neatly with </span><a href="https://en.wikipedia.org/wiki/Attachment_theory" rel="">Bowlby’s attachment theory</a><span>: babies that are more securely attached to their primary caregivers demonstrate greater curiosity and tendency to explore.&nbsp;</span></p></li></ul><p>So, there you go:</p><p>Relatedness = super important.&nbsp;</p><p>But one question you might still have is: how important?</p><p>After all, many activities that seem to be fuelled by intrinsic motivation – things like reading, hiking, etc. – are solitary. Solitary activities, by definition, do not involve relating to others, so how can relatedness be involved here?</p><p>SDT’s stance, so far as I can work it out, is that relatedness is often a background requirement for intrinsic motivation. Relatedness doesn’t necessarily have to be tied to each and every intrinsically motivated activity – although it sometimes helps. If your life generally meets your requirement for relatedness, then you will have readier access to intrinsic motivation across all activities.&nbsp;</p><p>So, bringing this back one last time to my own experience at school: I personally feel like autonomy and competence are more operative than relatedness in my case – but I would also say this:&nbsp;</p><p>During sixth form, almost everyone in my former friendship group left the school. This former friendship group, for reference, wasn’t great. If it was being scored on its ability to satisfy my need for relatedness, it would probably get a 3 out of 10.&nbsp;</p><p>As a result of this disbanding, I was then forced to join a new friendship group, and I would say that those new relationships did a much better job of meeting my need for relatedness. Maybe 7 or 8 out of 10.&nbsp;</p><p>There’s also just the general point to make, which is that when I was studying hard, I was in harmony with my parents and my teachers. My aspirations for my life lined up closely with theirs’, and this improved the quality and closeness of those relationships.&nbsp;</p><p>It seems, then, that competence may also have played an important role in my studies too – and that the feedback loop may have more layers to it than previously assumed.&nbsp;</p><p>—</p><p>For the record: all of the research and theory discussed in this section is actually taken solely from one of the six mini-theories that make up SDT. This particular mini-theory is called Cognitive Evaluation Theory (CET), and the other five are mostly just an extension of these core ideas.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png" width="728" height="486.4752941176471" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:568,&quot;width&quot;:850,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ae47398-980e-4828-8805-d1d00f07ea6b_850x568.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Thought this image might be useful, </span><a href="https://www.researchgate.net/figure/Overview-of-Self-Determination-Theory-SDT-The-Five-Mini-Theories_fig1_330307867" rel="">borrowed from here</a><span>. No idea why they’ve not included the 6</span><sup>th</sup><span> of these mini-theories, Relatedness Motivation Theory (RMT).&nbsp;</span></figcaption></figure></div><p>So: according to CET, intrinsic motivation is determined by the interplay of three key factors: autonomy, competence, and relatedness.&nbsp;</p><p>There are some finer points and concerns that we’ll dig into in the next section, but this is the big picture theory – and actually, all things considered, I think it does quite an impressive job of mapping onto some of my own experiences with intrinsic motivation.&nbsp;</p><p>As will hopefully be clear at this point, I’m fairly convinced that SDT is getting at something real – but there are a few parts of the theory that I’m not quite bought into – or that I think might need refining.&nbsp;</p><p>I’m no expert, so take these speculations with a hearty pinch of salt. It may well be the case that people have tackled and refuted these ideas and objections already.&nbsp;</p><p>There are also a couple of other things that I didn’t touch on during previous sections but that I think are either important or interesting, so I’ve thrown them into this section too.&nbsp;</p><p><span>As some of you will know, a while ago I did a bit of a </span><a href="https://erringtowardsanswers.substack.com/p/dopamine-everything-you-need-to-know" rel="">deep dive on dopamine</a><span>, the neurochemical linked to motivation, learning, and reward. I’m not a world authority on this subject by any means, so treat my speculation here with the level of skepticism it deserves. Nonetheless, I think the dopamine angle offers a slightly different – and, I think, quite compelling – way of interpreting some of the experimental findings from this literature.</span></p><p>First, a quick tl;dr on the dopamine piece:</p><p>When we receive a reward for completing a task, we experience a dopamine spike in certain ‘reward centres’ of the brain. Contrary to the lay view, this spike does not seem to be causing pleasure: it seems to be causing learning. In other words, dopamine seems to stamp in the relationship between stimulus, behaviour, and reward.&nbsp;</p><p>But dopamine is about more than just learning; it’s also about motivation. The next time we encounter the stimuli associated with a reward, dopamine spikes again, this time invigorating the behaviour that previously led to the reward.</p><p>So, to make this a bit more concrete: let’s say I walk past a sweet shop and I decide to go in and buy a bag of sherbet lemons. When I eat the sherbet lemons, there’s a dopamine spike in my brain, and this stamps in a relationship between the stimuli – the sight of the sweet shop – the behaviour – the act of going in there, buying the sweets, and eating them – and the reward – the enjoyment that came from eating the sherbet lemons.&nbsp;</p><p>Now imagine that a week later I’m walking past this sweetshop. My brain has come to associate these specific stimuli – the sight of the sweetshop – with reward, so it produces a dopamine spike. This dopamine spike then invigorates the behaviour that generated the reward last time – i.e. going into the shop and buying the sweets – and this ultimately results in the reward being obtained again.&nbsp;</p><p>But here’s the thing: let’s say that I encounter the stimuli, I perform the behaviour, but the reward is not forthcoming. Like, say: I see the sweetshop, I walk into it, but when I go to the sherbet lemon section, it turns out that there are none left.</p><p>What happens?&nbsp;</p><p>Instead of spiking or staying constant, dopamine goes below baseline.&nbsp;</p><p>As I understand things, when this occurs, it is in some sense reducing the strength of the relationship between the stimuli, the behaviour, and the reward. This means that the next time I see the sweet shop, there will be a smaller dopamine spike than before, which will reduce the strength of the motivation I feel to go inside and buy the sweets.&nbsp;</p><p>I think we can use this understanding of dopamine to explain many of the findings from the intrinsic motivation literature without recourse to the core SDT concepts.&nbsp;</p><p>Consider once again the original study run by E. L. Deci (1971).&nbsp;</p><p>I didn’t go into all of the details originally, because it wasn’t really needed, but it is now, so:</p><p>Deci actually had three periods in his experiment. In the first period, all participants did the same thing. From the original paper:</p><p>‘After they entered the experimental room, they were seated at a table with the puzzle pieces in front of them, three drawings of configurations to the right of them, the latest issues of New Yorker, Time, and Playboy to their left, and the experimenter on the opposite side of the table. They were told that they would spend all three sessions using the pieces of plastic to form various configurations, such as the same they were shown.’</p><p>All participants were then asked to reproduce, using the puzzle pieces, the configurations shown in the drawings.&nbsp;</p><p>In the second session, those in the experimental group were told that they would receive a $1 reward for every configuration completed within 13 minutes. Meanwhile, the control group did the exact same thing as in the first session.&nbsp;</p><p>And then in the third session, both groups returned to the same conditions as in the first period.&nbsp;</p><p>So for the control it was: no reward, no reward, no reward. For the experimental group is was: no reward, reward, no reward.&nbsp;</p><p><span>Now, in order to measure intrinsic motivation, Deci placed an eight minute free-choice period in the middle of </span><em>each</em><span> of these sessions. During this time, the experimenter left the room, telling participants ‘I shall be gone only a few minutes, you may do whatever you want while I am gone.’ They could work on the puzzles, read the magazines, stare around the room, etc.&nbsp;</span></p><p>Meanwhile, all participants were monitored to see how long they engaged with the puzzles during each of these free choice periods.&nbsp;</p><p>So, with all of that additional context, here are the results:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png" width="393" height="314" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:314,&quot;width&quot;:393,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1595f6bf-47a9-4454-ba49-d55e11494d81_393x314.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The interesting thing here – and the reason I think the dopamine explanation is plausible – is this: in the experimental group, free-choice engagement with the task goes way up in the second session, after rewards have first been introduced.&nbsp;</p><p>This is what the dopamine account would predict: an association between the puzzles and a monetary reward has been established. As a result, when the participants encounter the puzzle next time, dopamine levels rise to invigorate the behaviour that secured the reward last time, i.e. playing with the puzzles.</p><p>But in the following session, when the monetary rewards are removed, dopamine levels go below baseline, weakening the association between the behaviour (doing the puzzles) and the reward (receiving the money).&nbsp;</p><p>As a result, the individual feels even less motivated to engage in the task than before, which results in free-choice engagement going way down.&nbsp;</p><p>At least for this particular experiment, I find this explanation quite compelling. If anyone knows a reason as to why it’s wrong, I’m all ears!</p><p>I&nbsp; think there’s good evidence that higher autonomy forms of motivation are superior – in terms of subjective experience, performance, persistence, etc. – than lower forms of autonomy.&nbsp;</p><p>I’m completely on board with this</p><p>But when I look at some of the early experiments surrounding intrinsic motivation, I feel like autonomy as an explanation is a bit of a stretch.&nbsp;</p><p><span>Take the </span><a href="https://psycnet.apa.org/record/1977-02501-001" rel="">Ross (1975)</a><span> one, where salient rewards were compared to non-salient rewards.&nbsp;&nbsp;</span></p><p>This result is completely plausible to me: it makes sense that if you promise a kid an unknown reward for playing a drum, and then you hide the unknown reward in a box placed conspicuously in front of them, they will experience less enjoyment when they play the drum. And if they experience less enjoyment, it stands to reason that they will probably be less motivated to play with the drum the next time the opportunity presents itself, i.e. a reduction in intrinsic motivation.&nbsp;</p><p>But here’s the thing: SDT posits that the reason for this reduction in intrinsic motivation is that the highly salient reward is experienced as controlling – as impairing the individual’s sense of autonomy – and that this is ultimately what causes the downturn in intrinsic motivation.&nbsp;</p><p>But would these salient rewards really be experienced as a form of control?&nbsp;</p><p>I’m not convinced.&nbsp;</p><p>For me, a better explanation is simply that the reward functioned as a distraction, and that the distraction reduced the quality of the attention the kids brought to bear on the task.&nbsp;</p><p>Think of it like this: tasks that are intrinsically interesting are in some sense a reward unto themselves. But if you offer up highly salient extrinsic rewards for doing these tasks, then each participant’s attention is going to be divided between the task and the reward. This divided attention will impair the quality of the participant’s engagement with the task, which will in turn reduce the amount of enjoyment they’re able to extract from it.&nbsp;</p><p>Less enjoyment from the task = reduced time playing with the reward in the free-choice period.&nbsp;</p><p>This seems to me like a much more plausible explanation than the autonomy one.&nbsp;</p><p><span>For another example, consider the </span><a href="https://www.tandfonline.com/doi/abs/10.1080/19388070701749546" rel="">Marinek and Cambrell (2008)</a><span> experiment where they compared the effect of token-rewards, e.g. a gold star, with task-related rewards, e.g. a book, on reading motivation.&nbsp;</span></p><p>Would the reward of a book really be experienced as less controlling than the reward of a gold star?&nbsp;</p><p>Again, I struggle to believe this.&nbsp;</p><p>Instead, I think the token reward was probably more distracting than the task-related reward – which makes sense, since the task-related reward was really just a means of spending more time doing the task at hand anyway.</p><p>I was going to throw this into the previous section, but it all just got too cumbersome, so decided to save it for now.&nbsp;</p><p><span>Over the years, there have been mountains of experiments looking at the&nbsp; effects of different rewards and reward contingencies on intrinsic motivation. In 1999, </span><a href="https://pubmed.ncbi.nlm.nih.gov/10589297/" rel="">Deci et al ran a huge meta-analysis</a><span> on all of this research. Here are the headline results:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png" width="1600" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:369421,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F103d46f0-5f3a-4d03-9756-0241f56c7ac8_1600x800.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>(Note: k = number of individual effect sizes included in the composite effect size ; d = composite effect size. Also, the numbers in brackets at the bottom of each box are the 95% confidence interval for each composite.)</p><p>Some of these categories are self-explanatory – e.g. tangible vs. verbal rewards, expected vs. unexpected rewards – but others are not. To help you interpret all of this, here’s a quick run-through of the categories I didn’t think were obvious:</p><ol><li><p>Task-noncontingent rewards - rewards are given for simply being present; does not require engagement with the target activity.&nbsp;</p></li><li><p>Engagement-continent rewards - rewards given for spending time engaged with the target activity.&nbsp;</p></li><li><p>Completion-contingent rewards - rewards given for completing a target activity.&nbsp;</p></li><li><p>Performance-contingent - given for reaching a specific performance standard, e.g. doing better than 80% of people.&nbsp;</p></li></ol><p>Upshot: verbal rewards seem to increase intrinsic motivation; almost all tangible rewards seem to undermine it, other than unexpected tangible rewards, which don’t have much of an effect either way.&nbsp;</p><p>So:&nbsp;</p><p>SDT - 1; Behaviourism - Nil.&nbsp;</p><p>Here are a couple of bits in relation to all of this that I thought were interesting and worth drawing attention to:</p><ul><li><p>Children vs. Students – notice that for verbal praise, the effect size is way bigger for college students than for children. One quite convincing explanation for this is that that much of the praise children receive from adults in their daily lives actually is intended to control their behaviour. They are therefore much more sensitive to the controlling aspect of feedback than college students, who interpret feedback as more straightforwardly competence affirming.&nbsp;</p></li><li><p><span>Praise – in addition to the findings from this meta-analysis, I thought it was worth adding that </span><a href="https://psycnet.apa.org/record/2002-15487-005" rel="">Henderlong and Lepper (2002)</a><span> did a review of the praise research and found that when praise was viewed as informational, i.e. conveying genuine information about competence, it increased intrinsic motivation, whereas when praise was viewed as controlling, it harmed it.&nbsp;</span></p></li><li><p><span>Performance contingent rewards appear to have a negative impact on intrinsic motivation, but here’s the thing: a decent percentage of the experiments included in this category involved all participants receiving the reward. This means we can expect the competence affirming aspect of the reward to at least partially offset the controlling aspect. But in many real-world contexts, we would often only expect the upper X% of&nbsp; participants to meet the criteria for the reward. That means that the majority of people within this contingency would&nbsp; experience both the controlling AND the competence </span><em>disconfirming</em><span> aspects of the reward, which we would expect to absolutely hammer their intrinsic motivation for the activity. This whole point is interesting, because it suggests that performance contingent rewards might act as something like a gate-keeping function: anyone that performs well will receive enough competence affirming information to keep their intrinsic motivation for the activity alive. Meanwhile, everybody else’s intrinsic motivation will be completely crushed. More on this in the ‘Competition’ section below.&nbsp;</span></p></li><li><p>Delay time – in some of the experiments that were included in this meta-analysis, the free-choice measure was taken immediately after the reward period; in others, it was taken several days later. By comparing the two, we can get a read on whether the effects of rewards on intrinsic motivation are short-lived or whether they persist long after the rewards were administered. Turns out that the effect size for immediate and non-immediate rewards was more or less identical.</p></li></ul><p>When I first started writing this piece, I was planning to criticise SDT on the grounds that it doesn’t really explain why I suddenly became obsessed with school and studying rather than other things.&nbsp;</p><p>But on reflection, I actually do think it offers up a fairly plausible explanation.&nbsp;</p><p>One factor, which I’ve mentioned already, is competence: I have a knack for school and learning, and this meant that when I started working hard, I received a strong competence signal. But why did I suddenly care obsessively about going to a good university when this hadn’t been on my radar at any point prior to then?&nbsp;</p><p><span>My assumption has always been that this was a simple case of </span><a href="https://en.wikipedia.org/wiki/Mimetic_theory#:~:text=Girard%20called%20this%20phenomenon%20%22mimetic,because%20we%20imitate%20their%20desires.%22" rel="">mimetic desire</a><span>: all of my classmates suddenly conceived a desire to go to a good university; my desire was in some sense learned from – or an imitation of – the desire I was seeing in them.</span></p><p>I think this is partially right – but one piece of the picture is missing:</p><p>Up until this point, I’d been relatively resistant to this kind of mimesis. Prior to sixth form, many of my school friends wanted nothing more than to hang out in town, chase girls, and do the usual mindless teenager stuff – but I wasn’t really interested in any of it.</p><p>But then I enter year 12 and suddenly I become subject to the kinds of mimetic desire that I had been holding out against all this time.&nbsp;</p><p>What’s up with that?</p><p>As it turns out, one of SDT’s other mini-theories – Organismic Integrations Theory (OIT) – has something quite interesting to say here.&nbsp;</p><p>In essence, OIT says that I am much more likely to internalise the values and norms of those around me when two conditions are met:</p><ol><li><p>When my adoption of those values and norms is volitional (read: autonomous).</p></li><li><p>When I feel connected with those around me (read: relatedness).</p></li></ol><p>As mentioned already, when I was in sixth form, both of these conditions were met: I had a close group of friends, and my environment and relationships were much more supportive of autonomy than they had been at earlier stages of my schooling.&nbsp;</p><p>SDT’s explanation is that this uptick in relatedness and autonomy meant that I was much more likely to internalise the values of my surroundings – those presented by my parents, teachers, and peers – and this led to far more autonomous forms of motivation becoming active.&nbsp;</p><p>When I was trying to understand why SDT was so rarely mentioned in mainstream circles, I did a bit of digging around the most common criticisms of the theory.&nbsp;</p><p>Of those that I encountered, most weren’t particularly compelling, but one did make think: the theory – which claims to offer a universal account of human psychological needs – is excessively western-centric.&nbsp;&nbsp;</p><p>The autonomy component of the theory seems to be the main target of this criticism: in western culture, we place a strong emphasis on autonomy as a value, but in collectivist cultures, where the needs of the individual are generally subordinated to the needs of the group, this emphasis is not there.&nbsp;&nbsp;</p><p>Does this mean that SDT is only applicable in the west?</p><p>Proponents of SDT generally have a couple of responses to this criticism:</p><ol><li><p>Autonomy vs. independence – the western centricism criticism conflates the ideas of autonomy and independence. Autonomy is about acting volitionally and endorsing one’s own behaviours; independence is about being reliant upon oneself and separate from others. These are two different things. SDT is about the former&nbsp;–&nbsp;the criticism relates to the latter. It is possible to act in a highly autonomous way – that is, to endorse one’s own behaviours willingly – and to also conform to the needs of the collective. This idea is brought out in clear relief when SDT talks about morals: if I reluctantly adhere to my community’s moral rules, dragging my feet all the way, I can be said to have a low autonomy form of motivation. If I see the value in these moral laws and willingly adhere to them, I can be said to possess a high autonomy form of motivation.&nbsp;</p></li><li><p>Universality of relatedness – SDT’s proponents also respond to this criticism by pointing out that relatedness – a value that would usually be more closely associated with collectivist cultures – plays a central role in their theory too. To zero in on autonomy and ignore relatedness feels a bit like cherry picking. What’s more, there’s no reason why individualist cultures should be more supportive of autonomy and collectivist culture more supportive of relatedness. There may well be cultures where the reverse is the case.&nbsp;</p></li></ol><p>Don’t know about you, but I actually find both of these responses quite satisfactory.&nbsp;</p><p>There are two niggling concerns that I’ve had while writing this piece. I wouldn’t feel right putting it out into the wild without at least flagging them, so here they are:</p><p>First, does the free-choice measure actually measure intrinsic motivation? </p><p>Does it, in other words, achieve construct validity?</p><p>The measure basically relies on inferences to differentiate intrinsic and extrinsic motivation. In the free-choice period, we are inferring that the individual’s behaviour is fuelled by intrinsic motivation, but it seems perfectly conceivable that in some instances it may be fuelled by extrinsic factors like, e.g. expectation of future rewards. </p><p>This is a legitimate concern, and i’m far from the only one to raise it. </p><p>SDT’s proponents generally respond in two main ways:</p><ol><li><p>They emphasise the correlation between the free-choice measure and the self-report measure. As mentioned already, this correlation isn’t perfect, but it’s close enough to indicate that the free-choice measure probably is capturing some element of enjoyment and interest. From my perspective, i don’t think this really deals with the criticism, since the main objection here isn’t that the measure fails to capture intrinsic motivation – it’s that it captures both intrinsic and extrinsic motivation.</p></li><li><p>They also sometimes emphasise the importance of using multiple measures of intrinsic motivation to triangulate on this construct. The idea, as i understand it, is that taken alone none of these measures are perfect, but by raising the threshold of evidence – by saying that multiple measures need to point in the same direction for the result to be considered significant – we can more reliably zero in on intrinsic motivation. I find this response more convincing, though i’m not sure if anyone is actually putting it into effect.  </p></li></ol><p>My second concern relates to the free-choice paradigm’s external validity: are its findings generalisable to the real-world? </p><p>These experiments are very controlled and sanitised, but the rewards and reward contingencies of real life are messy and complex and multi-layered. So – like – do the findings from these simplistic experiments carry over?</p><p>I’ve been able to satisfy myself with two main replies to this one:</p><ol><li><p>This is just the nature of experimentation (or, at least, experimentation within the field of psychology). We need to simplify and sanitise in order to isolate our variables of interest. If we did not do so, it would be super difficult to detect a signal within all the noise. Also, as it goes: when SDT uses field experiments to test their findings from the lab, the results generally run in the same direction. </p></li><li><p>The findings and theory from this literature do a surprisingly good job of explaining my own real-life experiences with intrinsic motivaiton, e.g. my crazy study habits during A Levels. This obviously isn’t a particularly scientific way of evaluating the external validity of a paradigm, but for me at least, it does tilt me slightly in the direction of wanting to give the measure the benefit of the doubt. </p></li></ol><p>There’s been some tentative talk within this literature about the potential undermining effects of competition on intrinsic motivation.&nbsp;</p><p>Consider again, for example, the meta-analysis discussed above, where it was found that performance-contingent rewards are likely to undermine intrinsic motivation, even in those who actually fulfil the performance criteria. For those who don’t fulfil the criteria – the losers of the competition – this is expected to have an even greater undermining effect.&nbsp;</p><p>This line of reasoning has led some commentators to suggest that we ought to consider removing competition from certain domains, e.g. maybe schooling, exercise, work, etc.&nbsp;</p><p>On one level, this makes sense – but I think we need to be careful here.</p><p>In some areas, it’s important that we ensure everybody succeeds to some extent or another. Take, for example, reading: we don’t want a handful of reading rockstars, while everybody else comes out of school illiterate. We want every student to leave school with a high level of literacy.&nbsp;</p><p>But if we introduce competition here, we may run the risk of harming the intrinsic motivation of the worse performing kids. This will mean that they fall even further behind – and that they leave school with a significantly lower literacy level than they would have done otherwise, i.e. without competition.&nbsp;&nbsp;&nbsp;</p><p>Meanwhile, as per the meta-analysis, we might find that the competition only has a neutral – if not slight undermining – effect on the intrinsic motivation of the high-performing kids.&nbsp;</p><p>This is a clear lose-lose.</p><p>But i think a further point needs to be made: in certain instances, competition inheres within the activity itself. That means the individual is never going to end up in a 'free choice' situation in real life, because the reward contingency is baked into the activity itself.</p><p>Take tennis, for example.&nbsp;</p><p><span>Tennis </span><em>is</em><span> competition. The competition inheres within the activity.</span></p><p>Sure, you can rally for a while, but if you’re not playing points – if you’re not competing – you’re not really playing tennis at all: you’re just practicing.&nbsp;</p><p>In some sense, this is just semantics – what does the word ‘play’ actually mean? Or the word ‘tennis’? – but in another sense, it’s not: you’re never going to enter a free-choice period where rewards are off the table entirely; performance-contingent rewards, however low stakes, are embedded within the very fabric of the game.&nbsp;</p><p>This is a good thing, and it’s probably part of what makes sport – and games in general – so compelling.&nbsp;</p><p>When I play competitive tennis, I become completely absorbed in the game in front of me. As far as I’m concerned, this kind of absorption is one of life’s great joys – being wholly immersed in the present moment, in the challenge at hand, and temporarily forgetting about every other worry in my life.</p><p>Without competition, this kind of narrowing of attention just would not happen. The enjoyment of the activity – it’s ability to elicit motivation and narrow attention – is inextricably tied up with the competition itself.&nbsp;</p><p>As a result, I think that any blanket policy seeking to remove all forms of competition in order to militate against the potential alienating effects on poor performers would be a mistake.&nbsp;</p><p>Anyhow, there are lots of subtleties to this debate, and I’m not the guy to cut through all of the confusion. Hopefully this discussion at least gives a sense as to what’s at stake on both sides.&nbsp;</p><p>Throughout this piece, I’ve done my best to give a clear-eyed overview of the research on intrinsic motivation.&nbsp;</p><p>I think this is a really important subject, not just because greater intrinsic motivation means greater output, but also because I think that greater intrinsic motivation, all things being equal, means a better life.&nbsp;&nbsp;</p><p><span>I don’t need any research to tell me this – it’s kind of self-evident – but as you’d expect, there actually </span><em>are</em><span> a load of papers showing a strong relationship between wellbeing and intrinsic motivation, e.g. </span><a href="https://link.springer.com/article/10.1007/S11031-018-9733-Z" rel="">1</a><span>, </span><a href="https://psycnet.apa.org/record/2006-12810-012" rel="">2</a><span>, </span><a href="https://journals.sagepub.com/doi/abs/10.1177/01461672982412006" rel="">3</a><span>, </span><a href="https://psycnet.apa.org/record/1995-97696-020" rel="">4</a><span>, etc.</span></p><p>This makes sense.&nbsp;</p><p>We don’t want to be white knuckling everything we do. When we’re excited, inspired, energised, motivated – when the wind is in our sails – life is fun and wellbeing is abundant.&nbsp;</p><p>Unfortunately, if the research literature is to be believed, there are many things that we’re currently doing – both to ourselves and to others – that undermine intrinsic motivation badly.&nbsp;</p><p><span>What I’ve presented here is really only the tip of the iceberg – the research literature is </span><em>vast</em><span> – but I’m hoping this sketch will be enough to give you a broad-strokes understanding of the main findings, plus the theories that have emerged to account for them.&nbsp;</span></p><p>Maybe I’ll do a followup piece or two to try and flesh out this picture some more.&nbsp;</p><p>In the meantime, I’ll leave leave you with a parting piece of advice:</p><p>If you want your kids to study – if you want them to actually enjoy learning – then you’re going to have to do something and you’re probably not going to like it:</p><p>You need to get off their backs!</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Flowcode – Turing-complete visual programming platform (150 pts)]]></title>
            <link>https://app.getflowcode.io/playground/example1</link>
            <guid>43830193</guid>
            <pubDate>Tue, 29 Apr 2025 09:04:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.getflowcode.io/playground/example1">https://app.getflowcode.io/playground/example1</a>, See on <a href="https://news.ycombinator.com/item?id=43830193">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Try Switching to Kagi (519 pts)]]></title>
            <link>https://daringfireball.net/2025/04/try_switching_to_kagi</link>
            <guid>43829490</guid>
            <pubDate>Tue, 29 Apr 2025 07:08:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/2025/04/try_switching_to_kagi">https://daringfireball.net/2025/04/try_switching_to_kagi</a>, See on <a href="https://news.ycombinator.com/item?id=43829490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">



<p>Aaron Pressman, writing earlier this month in The Boston Globe, “<a href="https://www.bostonglobe.com/2025/04/01/business/google-search-problems-alternatives-kagi/">Why I Abandoned Google Search After 27 Years — and What I’m Using Instead</a>”:</p>

<blockquote>
  <p>The UK now requires travelers from America to obtain an electronic
travel authorization, or ETA. I wasn’t sure of the exact name of
the ETA, so I just searched “travel to UK.”</p>

<p>The results were all about obtaining an ETA and I picked a link
that looked like the official UK government site. It was not; the
official site was lower, below an AI summary, some sponsored
links, and other junk on the results page. Luckily for me, I did
get a legitimate travel pass — but the site I picked overcharged
me by about $70.</p>
</blockquote>

<p>I don’t know what the name for this sort of thing is, but it’s like a semi-scam. There are similar services to what Pressman ran into here for expedited passport renewals, for example — third-party companies that present themselves as official partners of the government that charge you extra for a service. But they just handle for you what you could just as easily do yourself, if you found the right place on the web to do it. A complete scam would be taking your money and giving you nothing (or a bogus document) in return. These semi-scams deliver the thing they’re promising, but charge you more than you should pay.</p>

<p>I just tried searching for “expedited passport renewal” <a href="https://www.google.com/search?q=expedited+passport+renewal">in Google</a> and <a href="https://kagi.com/search?q=expedited+passport+renewal">in Kagi</a>. Kagi presents as its first response the US State Department’s “<a href="https://travel.state.gov/content/travel/en/passports/get-fast.html">How to Get my U.S. Passport Fast</a>” page. Google has that same link listed 7th, below the fold even on a desktop browser window on a 27-inch display, behind four sponsored links (all of which look pretty official but aren’t), an AI Overview (which itself includes, in its own AI Overview sidebar, another link to the same “How to Get my U.S. Passport Fast” page), and another U.S. State Department webpage with general instructions for applying for a passport.</p>

<blockquote>
  <p>In the second case, last week, I needed to book a hotel for a
Passover trip to my brother’s in Connecticut. I knew there was a
cool hotel we had stayed at before near his house but I couldn’t
remember the name. I asked Google for hotels in the town where my
brother lives. Sure enough, one of the top results appeared on
first glance to be the official site of the hotel I wanted to
book. It was not. Once again, somewhat nefarious search engine
optimization techniques allowed a hotel aggregation site to jump
ahead in the results. And this time my error was even more costly,
to the tune of several hundred dollars in extra charges for two
hotel rooms.</p>

<p>Google has worked hard to eliminate truly fraudulent websites from
ending up in its results, and for that I am grateful. It is
undeniable that, in both instances, I should have been a more
careful consumer. But decades of relying on Google had taught me
that I didn’t have to be.</p>

<p>After I learned my lesson, I did some research in search of better
search. People I trust on the Internet, including the <a href="https://daringfireball.net/linked/2024/01/19/bray-google-kagi">Apple
blogger John Gruber</a> and <a href="https://pluralistic.net/2024/04/04/teach-me-how-to-shruggie/">novelist Cory Doctorow</a>,
recommended a new search engine called <a href="https://kagi.com/">Kagi</a>.</p>

<p>I gave it a few test runs. A search for “<a href="https://kagi.com/search?q=travel+to+uk">travel to UK</a>”
brought up the UK government page to apply for an ETA as the first
result. A search for a hotel in my brother’s town was topped by
the official site of the hotel I wanted. So I switched all my
default searches to Kagi.</p>
</blockquote>

<p>I keep trying to emphasize that I recommend switching to Kagi not because it’s more private (although it clearly is), not as a protest against Google (although for some, switching could be), not as a rejection of search ads dominating the top of Google’s results (although that’s true too), but simply because Kagi’s results are clearly better.</p>

<p>Like, even if I <a href="https://udm14.com/">use the magic <code>&amp;udm=14</code> parameter</a> with Google search, <a href="https://daringfireball.net/linked/2024/05/23/udm14">to get “disenshittified” results from Google</a>, I find I get <em>better</em> results from Kagi. When I know there’s one right answer (say, a specific article I remember reading and want to find again), Kagi is more likely than Google to list it first. If it’s a years-old article, Kagi is <em>way</em> more likely than Google to find it at all. For me, Google (and, alas, DuckDuckGo too) have largely stopped working reliably for finding not-recent stuff on the web. Not true with Kagi.</p>

<p>I used DuckDuckGo for years as my default search, and for those years, I found it largely on par with Google. But it felt like every once in a while — maybe, say, once or twice a month — DuckDuckGo would come up dry in its results. DuckDuckGo pioneered a trick <a href="https://duckduckgo.com/bangs">they call Bangs</a>. Include <code>!g</code> to any search terms, and instead of performing the search itself, DuckDuckGo will redirect that search to Google. They have a whole bunch of these Bangs — “!a” for Amazon search, “!nf” for Netflix. There are literally thousands of them (which of course they allow you to search for). The only one I ever really used though was <code>!g</code>, for redirecting my current search to Google because DuckDuckGo’s own results for the same terms was unsatisfying. My memory may not match with my actual usage, but like I said, I <em>feel</em> like I used this about once or twice a month for the several years I was using DuckDuckGo as my default search engine. Infrequently enough that it didn’t annoy me to the point of considering switching back to Google for default in-browser search, but frequently enough that I was annoyed enough to remember that I needed to use it at all.</p>

<p><a href="https://help.kagi.com/kagi/features/bangs.html">Kagi supports Bangs too</a>, including <code>!g</code> for Google web search. I can’t remember the last time I felt the need to try using it. It’s been months, many months. And, the last few times I’ve tried it, Google’s results were no more help than Kagi’s. Your mileage may vary, of course, but for me, unlike with DuckDuckGo, I effectively <em>never</em> find myself redirecting the same search to Google because I wasn’t happy with the results from Kagi. For context on my search usage, <a href="https://daringfireball.net/misc/2025/04/kagi-usage.png">my Kagi usage report</a> shows that I perform 400–800 web searches per month. (Kagi counts how often you search, for billing purposes, but <a href="https://kagi.com/privacy">does not keep a history</a> of <em>what</em> you searched for.)</p>

<p>Paying for Kagi today feels a <em>lot</em> like paying for HBO back in the cable TV heyday. Part of the deal is that you are paying for ad-free service, yes. But you’re also paying for noticeably higher quality. There were no shows like <em>The Sopranos</em>, <em>The Wire</em>, and <em>The Larry Sanders Show</em> on “free” TV channels, albeit with commercial interruptions. With HBO you got commercial-free entertainment <em>and</em> higher-quality shows and movies. Kagi is like that.<sup id="fnr1-2025-04-28"><a href="#fn1-2025-04-28">1</a></sup> It’s that good. No ads, no unwanted AI (but very good AI results if you want — just end your query with a question mark), <em>and</em> better search results.</p>





 <!-- PreviousNext -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spain is about to face the challenge of a "black start" (115 pts)]]></title>
            <link>https://arstechnica.com/science/2025/04/why-restarting-a-power-grid-is-so-hard/</link>
            <guid>43829356</guid>
            <pubDate>Tue, 29 Apr 2025 06:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2025/04/why-restarting-a-power-grid-is-so-hard/">https://arstechnica.com/science/2025/04/why-restarting-a-power-grid-is-so-hard/</a>, See on <a href="https://news.ycombinator.com/item?id=43829356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>Local conditions</h2>
<p>While the grids in Spain and Portugal are connected to each other, they have limited connections to elsewhere. The only sources of external power to the grid come from France and Morocco, which are small connections, but they could be used to help black start some plants. Both blacked-out countries have significant hydropower, with Spain seeing it cover 10 percent of its demand and Portugal 25 percent. That's useful because hydro plants need very little in the way of an external power supply to start operating.</p>
<p>Beyond that, both countries have invested heavily in renewables, with Portugal supplying about half of its power from wind and hydro, having closed its last coal plant in 2021. Spain receives about 40 percent of its power from renewables at present.</p>
<p>Solar is not an ideal power source for black-starting the grid, given that it's unavailable for a significant chunk of the day. But solar panels produce direct current, with electronic systems matching it to the alternating current of the grid. With the right electronics, it can play a key role in keeping frequencies stable as grid segments are repowered. In productive areas, wind can provide black start power to other plants and doesn't need much external power to begin operations. It's unclear, however, whether the local wind hardware is equipped for black starts or if the local weather will cooperate (a quick check of the weather in various cities suggests it's relatively calm there).</p>
<p>Batteries have the potential to be incredibly helpful, since they also provide direct current that can be converted to any frequency needed, and so used for both starting up power plants or for frequency stabilization as segments of the grid are brought back online. Unfortunately, neither country has installed much grid-scale battery hardware yet. That's expected to change over the next few years in parallel with dramatically expanded solar power. But, at the moment, batteries will not be a huge help.</p>
<p>Regardless of how precisely the grid operators manage to handle this task in Spain and Portugal, they face a monumental challenge at the moment. If you're seeing estimates of several days for the restoration of power, it's because failing to meet this challenge will leave things back in the state they're in now.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear "Security Researchers" (166 pts)]]></title>
            <link>https://ftp.bit.nl/pub/debian/</link>
            <guid>43829080</guid>
            <pubDate>Tue, 29 Apr 2025 05:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ftp.bit.nl/pub/debian/">https://ftp.bit.nl/pub/debian/</a>, See on <a href="https://news.ycombinator.com/item?id=43829080">Hacker News</a></p>
<div id="readability-page-1" class="page">
<span>Dear "Security Researchers",<p>

Welcome to our *PUBLIC* OPEN SOURCE SOFTWARE MIRROR SERVER.<br>
Please DO NOT report this under our responsible disclosure policy.<br>
This is a PUBLIC service, with OPEN SOURCE SOFTWARE, and NOT a security threat to our company.<br>
There is NO SENSITIVE INFORMATION on this server.</p><p>

Thanks.</p></span>
<hr>
  <table>
   <tbody><tr><th><img src="https://ftp.bit.nl/icons/blank.gif" alt="[ICO]"></th><th><a href="https://ftp.bit.nl/pub/debian/?C=N;O=D">Name</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=M;O=A">Last modified</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=S;O=A">Size</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=D;O=A">Description</a></th></tr>
   <tr><th colspan="5"><hr></th></tr>
<tr><td><img src="https://ftp.bit.nl/icons/back.gif" alt="[PARENTDIR]"></td><td><a href="https://ftp.bit.nl/pub/">Parent Directory</a></td><td>&nbsp;</td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/hand.right.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/README">README</a></td><td>2025-03-15 09:29  </td><td>1.2K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/unknown.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.CD-manufacture">README.CD-manufacture</a></td><td>2010-06-26 11:52  </td><td>1.3K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.html">README.html</a></td><td>2025-03-15 09:29  </td><td>2.9K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.mirrors.html">README.mirrors.html</a></td><td>2017-03-04 21:08  </td><td>291 </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.mirrors.txt">README.mirrors.txt</a></td><td>2017-03-04 21:08  </td><td> 86 </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/dists/">dists/</a></td><td>2025-03-15 09:29  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/doc/">doc/</a></td><td>2025-04-29 03:52  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/unknown.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/extrafiles">extrafiles</a></td><td>2025-04-29 04:27  </td><td>201K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/indices/">indices/</a></td><td>2025-04-29 04:26  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/compressed.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/ls-lR.gz">ls-lR.gz</a></td><td>2025-04-29 04:18  </td><td> 15M</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/pool/">pool/</a></td><td>2022-10-05 19:09  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/project/">project/</a></td><td>2008-11-18 00:05  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/tools/">tools/</a></td><td>2012-10-10 18:29  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/zzz-dists/">zzz-dists/</a></td><td>2023-10-07 13:07  </td><td>  - </td><td>&nbsp;</td></tr>
   <tr><th colspan="5"><hr></th></tr>
</tbody></table>



  <title>Debian Archive</title>
  <meta name="Modified" content="2025-03-15">



<h2>Debian Archive</h2>

<p>See <a href="https://www.debian.org/">https://www.debian.org/</a>
for information about Debian GNU/Linux.</p>

<h2>Current Releases</h2>

<p>Four Debian releases are available on the main site:</p>

<blockquote>
<dl>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/buster/">Debian 10.13, or buster</a></dt>
<dd>Debian 10.13 was released Saturday, 10th September 2022.
<a href="https://www.debian.org/releases/buster/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/buster/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/bullseye/">Debian 11.11, or bullseye</a></dt>
<dd>Debian 11.11 was released Saturday, 31st August 2024.
<a href="https://www.debian.org/releases/bullseye/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/bullseye/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/bookworm/">Debian 12.10, or bookworm</a></dt>
<dd>Debian 12.10 was released Saturday, 15th March 2025.
<a href="https://www.debian.org/releases/bookworm/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/bookworm/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/testing/">Testing, or trixie</a></dt>
<dd>The current tested development snapshot is named trixie.<br>
Packages which have been tested in unstable and passed automated
tests propagate to this release.<br>
<a href="https://www.debian.org/releases/testing/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/unstable/">Unstable, or sid</a></dt>
<dd>The current development snapshot is named sid.<br>
Untested candidate packages for future releases.<br>
<a href="https://www.debian.org/releases/unstable/">More information</a>
</dd>
</dl>
</blockquote>

<h2>Old Releases</h2>

<p>Older releases of Debian are at
<a href="http://archive.debian.org/debian-archive/">http://archive.debian.org/debian-archive</a>
<br>
<a href="https://www.debian.org/distrib/archive">More information</a>
</p>

<h2>CDs</h2>

<p>For more information about Debian CDs, please see
<a href="https://ftp.bit.nl/pub/debian/README.CD-manufacture">README.CD-manufacture</a>.
<br>
<a href="https://www.debian.org/CD/">Further information</a>
</p>

<h2>Mirrors</h2>

<p>For more information about Debian mirrors, please see
<a href="https://ftp.bit.nl/pub/debian/README.mirrors.html">README.mirrors.html</a>.
<br>
<a href="https://www.debian.org/mirror/">Further information</a>
</p>

<h2>Other directories</h2>

<table summary="Other directories">
<tbody><tr><td><a href="https://ftp.bit.nl/pub/debian/doc/">doc</a></td>          <td>Debian documentation.</td></tr>
<tr><td><a href="https://ftp.bit.nl/pub/debian/indices/">indices</a></td>  <td>Various indices of the site.</td></tr>
<tr><td><a href="https://ftp.bit.nl/pub/debian/project/">project</a></td>  <td>Experimental packages and other miscellaneous files.</td></tr>
</tbody></table>




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[LibreLingo – FOSS Alternative to Duolingo (616 pts)]]></title>
            <link>https://librelingo.app</link>
            <guid>43829035</guid>
            <pubDate>Tue, 29 Apr 2025 05:45:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://librelingo.app">https://librelingo.app</a>, See on <a href="https://news.ycombinator.com/item?id=43829035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="LibreLingo Mascot" src="https://librelingo.app/images/mascot-jetpack-noshadow.svg" data-test="mascot-jetpack"> </p> <p><h2><span data-tkey="index.subtitle">an experiment to create a community-driven language-learning platform</span></h2> </p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A single line of code cost $8000 (240 pts)]]></title>
            <link>https://pietrasiak.com/one-line-of-code-that-did-cost-dollar8000</link>
            <guid>43829006</guid>
            <pubDate>Tue, 29 Apr 2025 05:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pietrasiak.com/one-line-of-code-that-did-cost-dollar8000">https://pietrasiak.com/one-line-of-code-that-did-cost-dollar8000</a>, See on <a href="https://news.ycombinator.com/item?id=43829006">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2 id="6229ad4c359d4dbf908c56b33a98c24c">TLDR</h2></p><div><p>Due to a bug, our screen recorder app - <a target="_blank" href="http://screen.studio/">screen.studio</a> app kept downloading the auto-update file repeatedly, every 5 minutes for every single user. The update file is approximately 250MB. This resulted in <b>9 million file downloads and more than 2 petabytes (2,000,000 gigabytes)</b> of traffic on Google Cloud.</p></div><figure><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" decoding="async" alt="Image without caption" src="https://image-forwarder.notaku.so/aHR0cHM6Ly93d3cubm90aW9uLnNvL2ltYWdlL2h0dHBzJTNBJTJGJTJGczMtdXMtd2VzdC0yLmFtYXpvbmF3cy5jb20lMkZzZWN1cmUubm90aW9uLXN0YXRpYy5jb20lMkZlMjZiZThjNS05ZDcxLTRmYTgtODBlYS1iNDdiNmNmNTg3Y2ElMkZVbnRpdGxlZC5wbmc_dGFibGU9YmxvY2smc3BhY2VJZD1iYTMzYWM4NC1mN2VjLTQwZDMtODU0MS0wNDMyODI5YzBkNDgmaWQ9NzNkMzk3N2YtNjJhNy00MmQ1LTk2ZTQtNThiYmQwMDc5NzMyJmNhY2hlPXYyJndpZHRoPTE2NzQuOTY4NzU="></p></div></figure><div><p>This screenshot might not look so scary at first, but take a look at the scale of it. For over a month, we generated at least 100Mib/s (a second!) and, at times, almost 1GiB/s of traffic (every single second!)</p></div><hr><div><p>That bug was painfully simple and stupid.</p></div><div><p>Screen Studio is a screen recorder for macOS. It is desktop app. It means we need some auto-updater to allow users to install the latest app version easily.</p></div><div><p>The app checks for the update every 5 minutes or when the user activates the app.</p></div><div><p>Normally, when the app detected the update - it downloaded it and stopped the 5 minutes interval until the user installed it and restarted it.</p></div><p><h2 id="86e96dd5a9a64f7eb2a3408771948538">Tragic refactor</h2></p><div><p>The problem with the auto-updater we had was that it would prompt the user to update the app as soon as it became available. This resulted in a popup appearing while users were recording the screen, which obviously provided a bad experience as it interrupted the recording the user was making.</p></div><div><p>While refactoring it, <b>I forgot to add the code to stop the 5-minute interval after the new version file was available and downloaded.</b></p></div><div><p>It meant <b>the app was downloading the same 250MB file, over and over again, every 5 minutes.</b></p></div><p><h2 id="8bc5a4ebee2d4a2387cb4af3e66bda00">Tragic context - app running in the background for weeks</h2></p><div><p>It turns out thousands of our users had the app running in the background, even though they were not using it or checking it <b>for weeks</b> (!). It meant thousands of users had auto-updater constantly running and downloading the new version file (250MB) over and over again every 5 minutes</p></div><p><h2 id="0c7c00acd73e4f7ba7a05ffd8ee2f5bf">The math</h2></p><div><p>Let’s do some quick math here.</p></div><ul><li>Doing something every 5 minutes means doing it approximately <b>288 times a day</b>.</li></ul><ul><li>The update file is about 250 MB, meaning <b>72 GB of downloads per user daily</b>.</li></ul><ul><li>We had this situation happening for <b>over a month before we noticed it</b>.</li></ul><ul><li>We had at least a <b>thousand such app instances running</b> in the background at any moment.</li></ul><ul><li><b>250 MB * 288 downloads per day * 30 days * 1000 users:</b></li></ul><figure><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" decoding="async" alt="Image without caption" src="https://image-forwarder.notaku.so/aHR0cHM6Ly93d3cubm90aW9uLnNvL2ltYWdlL2h0dHBzJTNBJTJGJTJGczMtdXMtd2VzdC0yLmFtYXpvbmF3cy5jb20lMkZzZWN1cmUubm90aW9uLXN0YXRpYy5jb20lMkY1Y2FlZmVkYS0wMDFjLTQ2NDktODM2Zi1lMTI0MzM0NDRjMGUlMkZVbnRpdGxlZC5wbmc_dGFibGU9YmxvY2smc3BhY2VJZD1iYTMzYWM4NC1mN2VjLTQwZDMtODU0MS0wNDMyODI5YzBkNDgmaWQ9M2FjZmFjNzAtOTc2ZC00ZmI4LTkwYTgtMzU5NmQzMTFkZGFlJmNhY2hlPXYyJndpZHRoPTE2NzQuOTg0Mzc1"></p></div></figure><ul><li><b>2 000 000 gigabytes, </b></li></ul><ul><li><b>or 2 000 terabytes </b></li></ul><ul><li><b>or 2 petabytes of traffic.</b></li></ul><p><h2 id="b0ab8050379441e89f1489895de53ae8">Series of bad mistakes</h2></p><div><p><b>We did not have cost alerts on Google Cloud</b>. Before this situation occurred, we were paying at most $300 a month.</p></div><div><p>We were also not regularly checking the situation as it just worked.</p></div><div><p>We noticed it because my credit card started to block the transaction due to limits I had set on it (lucky me!).</p></div><figure><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" decoding="async" alt="Image without caption" src="https://image-forwarder.notaku.so/aHR0cHM6Ly93d3cubm90aW9uLnNvL2ltYWdlL2h0dHBzJTNBJTJGJTJGczMtdXMtd2VzdC0yLmFtYXpvbmF3cy5jb20lMkZzZWN1cmUubm90aW9uLXN0YXRpYy5jb20lMkYxMWRhNDI1YS05YWU0LTRiZTgtOGIzNi1hMzc3MmQwOTdjMTElMkZVbnRpdGxlZC5wbmc_dGFibGU9YmxvY2smc3BhY2VJZD1iYTMzYWM4NC1mN2VjLTQwZDMtODU0MS0wNDMyODI5YzBkNDgmaWQ9NmE3M2JhYzgtZWMwYi00ZmJiLWJjOTctNzEyYjk4NDM3YmM4JmNhY2hlPXYyJndpZHRoPTI0MDA="></p></div></figure><p><h2 id="1bf63d5f09bb48f9a25dc3820bb49e03">Consequences for the users</h2></p><div><p>It was not only bad for us but even worse for some of the users.</p></div><div><p>As mentioned, the app was generating so much traffic. <b>It means it was their machine generating network traffic on their home router and their internet provider.</b></p></div><div><p>One of our users, who lived in a house, had their internet provider cancel their contract due to enormous traffic generated during a month. It was extremely problematic as there was no other internet provider available around.</p></div><div><p>We decided to take responsibility and offer to cover all the costs related to this situation.</p></div><div><p>Luckily, it was not needed as the person could figure out the situation with the provider without bigger problems.</p></div><div><p>That was, however, quite a terrible experience for that person and me. As a designer, I value the experience product I create provides to the users. And this was not even a bad experience; it was actually harmful.</p></div><p><h2 id="cba99c00ddec4d4890f45cf68e515c40">Summarising</h2></p><ul><li><b>Set alerts</b> on your cloud at all times.</li></ul><ul><li>Write your auto-updater code very carefully.</li></ul><ul><li>Actually, <b>write any code that has the potential to generate costs carefully</b>.</li></ul><ul><li>Add special signals you can change on your server, which the app will understand, such as a <b>forced update</b> that will install without asking the user.</li></ul><ul><li>Regularly check your cloud.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oracle engineers caused five days software outage at U.S. hospitals (163 pts)]]></title>
            <link>https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html</link>
            <guid>43828915</guid>
            <pubDate>Tue, 29 Apr 2025 05:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html">https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html</a>, See on <a href="https://news.ycombinator.com/item?id=43828915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108079907" data-test="InlineImage"><p>Larry Ellison, co-founder and executive chairman of Oracle Corp., speaks during the Oracle OpenWorld 2018 conference in San Francisco, California, U.S., on Monday, Oct. 22, 2018.</p><p>David Paul Morris | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/ORCL/">Oracle</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> engineers mistakenly triggered a five-day software outage at a number of Community Health Systems hospitals, causing the facilities to temporarily return to paper-based patient records.</p><p>CHS told CNBC that the outage involving Oracle Health, the company's electronic health record (EHR) system, affected "several" hospitals, leading them to activate "downtime procedures." Trade publication Becker's Hospital Review reported that 45 hospitals were hit.</p><p>The outage began on April 23, after engineers conducting maintenance work mistakenly deleted critical storage connected to a key database, a CHS spokesperson said in a statement. The outage was resolved on Monday, and was not related to a cyberattack or other security incident.</p><p>CHS is based in Tennessee and includes 72 hospitals in 14 states, according to the medical system's website.</p><p>"Despite this being a major outage, our hospitals were able to maintain services with no&nbsp;material impact," the spokesperson said. "We are proud of our clinical and support teams who worked through the multi-day outage with professionalism and a commitment to delivering high-quality, safe care for&nbsp;patients."&nbsp;</p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Oracle stock this year</p></div><div><p>Oracle didn't immediately respond to CNBC's request for comment.</p><p>An EHR is a digital version of a patient's medical history that's updated by doctors and nurses. It's crucial software within the U.S. health-care system, and outages can cause serious disruptions to patient care. Oracle acquired EHR vendor Cerner in 2022 for $28.3 billion, becoming the second-biggest player in the market, behind Epic Systems.</p><p>Now that Oracle's systems are back online, CHS said that the impacted hospitals&nbsp;are working to "re-establish full functionality and return to normal operations and procedures."</p><p>Oracle's CHS error comes weeks after the company's federal electronic health record experienced a <a href="https://www.cnbc.com/2025/03/06/oracles-federal-electronic-health-record-suffered-nation-wide-outage-.html">nationwide outage</a>. Oracle has struggled with a thorny, years-long EHR rollout with the Department of Veterans Affairs, marred by patient safety concerns. The agency launched a&nbsp;<a href="https://news.va.gov/press-room/va-announces-strategic-review-of-electronic-health-record-modernization-program/" target="_blank">strategic review</a>&nbsp;of Cerner in 2021, before Oracle's acquisition, and it temporarily&nbsp;<a href="https://digital.va.gov/ehr-modernization/ehr-deployment-schedule/" target="_blank">paused deployment</a>&nbsp;of the software in 2023.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/02/20/oracle-ceo-safra-catz-being-number-one-is-very-important.html">Interview with Oracle CEO Safra Catz</a></p></div><div id="Placeholder-ArticleBody-Video-108105025" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000367188" aria-labelledby="Placeholder-ArticleBody-Video-108105025"><p><img src="https://image.cnbcfm.com/api/v1/image/108105026-17400675801740067576-38548750502-1080pnbcnews.jpg?v=1740067578&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Oracle CEO Safra Catz: Being number one is very important"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Knowledge-based society, my ass (328 pts)]]></title>
            <link>https://mihaiolteanu.me/knowledge-based-society-my-ass</link>
            <guid>43828713</guid>
            <pubDate>Tue, 29 Apr 2025 04:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mihaiolteanu.me/knowledge-based-society-my-ass">https://mihaiolteanu.me/knowledge-based-society-my-ass</a>, See on <a href="https://news.ycombinator.com/item?id=43828713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contents">

            

            <p>
            Right after I get admitted, I inform Professor that I also have a full-time
            job. He insists that we must start working right away. I quit as a result and
            instantly breathe a refined air. I am now a scientist! A week later I approach
            Professor and let him know I'm ready for work.
            </p>

            <p>
            "Ready for what?" he greets me as though our previous conversation didn't
            happen. I remind him that he's my PhD supervisor and that, at his proposal, we
            are studying the effects of electromagnetic fields on patients with carotid
            stent implants. "There is nothing for you to do at the University, you can
            stay home for now," he tells me. Is he really serious? Does he want me to do
            research from bed? I insist on reading materials related to our field of
            research. I want to start right away. But he can't recommend any.
            </p>

            <p>
            My first day of research is over. It's autumn 2009. I spend the following days
            of my academic life oversleeping and strolling the city parks. I actually
            enjoy this newfound freedom from the alarm clock. I think, not without a
            certain longing, of my former factory colleagues. How we used to laugh at the
            stupidest of things, how it all felt like a big family. But I have a new life
            now. An intellectual one.
            </p>

            <p>
            A few weeks go by. No word from Professor. It's exhausting to conduct research
            like this. I need some color. I approach Professor again and ask for basic
            research equipment, "I need an office, Professor," I begin, and, after a short
            while, I raise my stakes, "And a computer, too!"  I've gone too
            far. "Everybody is happy around here, except you!!!" he snaps at me. I get a
            feeling that I'm going nowhere with Professor.
            </p>

            <p>
            I approach the Head of Department. The Head listens carefully for my
            complaints and kindly informs me he doesn't mingle in Professor's business. It
            is up to my advisor to decide where the resources are allocated within his
            team. A nice way of deflecting responsibility.
            </p>

            <p>
            The Faculty's Dean doesn't give a damn either but I think he wants to avoid
            even more troubles seeing that I'm so stubborn. I soon receive an email from
            Professor as a result. He decides to offer me an office and a computer. Two
            months wasted. Even so, I celebrate my first academic victory.
            </p>

            <p>
            "Grab a computer and follow me," Professor instructs me a few days later. I
            can barely hide my enthusiasm. We take the stairs to the first floor. Then
            ground floor. Then basement. "Almost there," I hear Professor in the
            darkness. After two more turns he opens a big door and hands me over the
            keys. My office is a rather spacious but austere room in the University's
            basement. My initial enthusiasm is fading. There's a simple desk with a basic
            chair at one end and a small, too high to reach window at the other. The walls
            are immaculately white. A hospital-style metal locker where Professor keeps his
            valuables under key completes the picture. 
            </p>

            <p>
            "Doctoral Studies in Engineering Sciences for Developing the Knowledge-Based
            Society" is the name of our project. It pays me, and approximately one hundred
            other colleagues of mine, all PhD candidates, our €500/month scholarships, or
            about the average wage. It's one project from among the four thousand projects
            sponsored by the European Union's "Operational Program for Human Resources
            Development." This grandiose program, with an available budget of €5 billion,
            aims to "develop the human capital and increase competitiveness by bringing
            education and life-long learning in sync with a modern, flexible and inclusive
            labor market and increase future opportunities for 1.650.000 people." Yes,
            those are millions, 15.000 of which are to become PhD students! The Government
            says so, I it as fact.
            </p>

            <p>
            For my part, I have to publish at least three scientific papers, present my
            research at one international conference and successfully defend my thesis in
            public. I have exactly three years at my disposal. If I fail, I have to return
            my scholarship in full. It's also up to me to rejoin the labor market and take
            care of my future, possibly as a teaching assistant here, at the
            University. So I take things seriously and go to work each morning. I learn
            and labor as hard as I can.
            </p>

            <p>
            I begin with the documentation phase and I read, among others, a very detailed
            series of experiments: human subjects placed in anechoic chambers with all
            kinds of electromagnetic fields directed at them. They measure changes in
            sweat rate, breathing rate, exhaled humidity, body temperatures, blood
            pressure and everything one can imagine. They try to figure out how the body
            responds and adapts to such an external stimulus.
            </p>

            <p>
            I, for my side, have to see what happens inside the human neck artery when an
            implanted stent heats up under the influence of electromagnetic fields. How
            does the body react and compensate for such a temperature increase, if there
            is one? I have things to discover. But I also have zero lab equipment. Not
            even a digital thermometer, let alone medical equipment of the kind I would
            need. The whole medical engineering's lab, the one I took my computer from, is
            a room twice the size of my basement with ten desktop computers in it, a
            blackboard, a small window blocked by another building and an extra door for a
            special room: Professor's own office.
            </p>

            <p>
            Critically, I do not even get to see or touch a real stent. We don't have
            any. There are no interactions with patients, no collaboration with doctors
            and no conversations with other engineers from our University. I'm alone in my
            office. I'm not sure what people do around here. When the whole Department
            gathers around, I hear professors complaining about "kids these days" but zero
            technical discussions and hardly any interest in scientific topics. We are
            one-man teams, each working in their only little basements, so to speak.
            </p>

            <p>
            Professor reassures me that computer simulations are enough for our study. So
            I try to find software licenses plus realistic computer models for my stents
            and human heads. They all cost money and are hard to find. To develop them
            from scratch is outside my specialty. Ideally, I should understand a bit of
            human biology, too, but that's again outside my specialty. I wonder at this
            point if I actually have a specialty. What makes me qualified to approach
            these issues? Why would my "discoveries", born out of such meager
            possibilities, have any relevance for science?
            </p>

            <p>
            I don't lack motivation, though. I try to get a license for the €20k per year
            software we're using. It proves to be another catacombic adventure. The
            company offers two free licenses per public institution. I ask Professor for a
            license, but "There aren't any left," he informs me. "Don't we have two?" I
            insist. "Well, yes, but one license is on my laptop, which I always carry with
            me, and the other is on my office computer," he replies. I ask permission to
            his office to run some simulations from time to time, but "No, my office is
            closed and only I have the key." I conclude Professor has a terrible fondness
            for locks and keys. I drop it. I'm not sure what he does with two
            licenses. Maybe he sells them on the black market? Maybe Professor is a
            gangster? Who knows.
            </p>

            <p>
            One of my colleagues who is pursuing his PhD in the same Department under a
            different professor and a similar area of research, with whom I only cross
            paths when our blood pressure runs too high, happens to also work for a public
            institution. He applies for the two free licenses and is generous enough to
            offer me one.
            </p>

            <p>
            Another victory. But I'm fed up with these victories. It's exhausting to fight
            all these absurd battles. My time is running out. I have to write some papers
            soon. I accept my fate. I accept I'm not gonna be a scientist the way I've
            imagined more than a year ago. I don't see any future for me here at the
            University. As a result, I simplify things tremendously. I draw a big sphere
            and pretend it's a human head, I place a long metallic cylinder inside it and
            pretend it's a real stent and I place a simple antenna close by. Anything more
            complicated than this crashes my toy computer. I soon realize that I play
            scientist like kids play cop with water pistols.
            </p>

            <p>
            I get to publish my first paper in this way. I'm actually quite proud of it,
            given the circumstances. I actually start to enjoy writing. I put down my
            colleague as a co-author as a thank you for lending me the license. We've
            learned this trick from the professors who do it all the time with their
            books, papers and conferences. They are required, just as we are, to publish
            and look active in the community per their contract with the University.
            </p>
            
            <p>
            Out of curiosity, I start reading our school's newspaper, as my colleague
            calls our University's scientific journal. I soon spot inconsistencies. The
            wording is in plain, boring language with long introductions repeating the
            same generalities and facts known to all. But the style changes unexpectedly
            sometimes. I search these peculiar phrases online and my intuition is
            confirmed. Unacknowledged commandeering of intellectual labor via
            indiscriminate copy and paste practices. Plagiarism, in short. I find dozens
            of such instances. I see the name of our Head in there, too. I try to raise
            awareness for a month or two. Nobody gives a damn.
            </p><p>

            I stop reading the school's newspaper and concentrate on publishing my other
            papers instead. They are nothing more than variations on the first paper with
            different titles and different pictures. I let my computer run overnight and
            invent slightly different simulation scenarios and I underline different
            aspects of my results in each paper. After this, I take a more relaxed
            approach regarding my scientific pursuits, enjoy the show around me
            instead and stop giving a damn about Professor from now on.
            </p>
            
            <p>
            I notice the Head is emphasizing the "academic dress code" all the time. He
            even publishes an official Department guideline on this topic pressing us all
            to read it. I notice professors are always addressing each other formally even
            in informal settings, though they've been acquainted for years. This title
            caries great importance here. I myself make a blunder in this respect when I
            visit Professor's office one day for some official papers. I ask if he's
            around but I refer to him by his family name only. I get admonished for
            skipping the "professor" part. I apologies, add the missing title and address
            the question again. "No, Professor is not here!" comes the reply abruptly.
            </p>

            <p>
            Our Head both informs and threatens us, "Per the Department guidelines, every
            PhD candidate is required to teach for one semester. Find yourselves a seminar
            or a lab or I'll pick one for you." It so happens that I get friendly with an
            electronics department's professor. He asks me to be his teaching assistant. I
            inform the Head with great pleasure about this development and he, in turn,
            informs me with great satisfaction that "I do not give this position to PhD
            candidates." I insist, but in vain. I get used to insisting in vain. I get
            used to failing to figure out how this whole clusterfuck works. One colleague
            is appointed to teach C++ by the Head. "You know C++?" I ask enthusiastically,
            as I am looking to become a software engineer myself at this point. "I don't,"
            she informs me, "but there's enough time until Monday to learn it." It's
            Friday, the last day of my teaching career.
            </p>

            <p>
            Professor becomes my hero for a short time during a Department meeting. He
            insists that the design of high-voltage power lines is not actually a subject
            for his medical students. He wants more biology and medical related courses,
            instead. I truly believe in his vision. My mouth is wide open. But the Head
            again masterfully defends his position insisting on the necessity of assigning
            the minimum required number of hours per semester to each member of our
            Department, per the University guidelines. Nobody backs up my hero, not even
            he himself. The next topic on the agenda is the training of all our staff in
            the arts of digital blackboards "to help improve the teaching experience."
            </p>

            <p>
            I'm sinfully enjoying myself. What else do they do around here? Mrs. S. is our
            Department's team assistant. She's near retirement age and lives up in the
            attic. We visit her monthly to physically sign our presence in the attendance
            register. Sometimes she scolds us for signing in the wrong place, "That was a
            public holiday! You didn't work then, did you?!" There is no "Sir" nor
            "professor" with her. We are inhabiting a prestigious institution of higher
            learning, otherwise she would certainly call us morons. I'm wondering at the
            inefficiency on relying on handwritten notebooks for timekeeping. This
            Technical University has a Computer Science department, after all. But things
            are as they should be around here. There are many advantages to the analog
            methods. For instance, we avoid software bugs so this method is more precise,
            it fosters social interactions so it is more humane, we avoid proprietary
            software so users have complete control to modify the source code. We turn up
            once a month, sign and then we're free to do whatever. The Professor has given
            me the correct advice on that first day.
            </p>

            <p>
            The only constant human presence in the whole building during the warm summer
            days is the cleaning lady. I befriend her and we talk each morning. She
            provides me with paper towels and liquid soap for "When you might need it."
            Summer is vacation for both students and teachers, after all. From time to
            time I meet a stray professor in the hallways and they tell me I do a good
            job, always working, always studying, always present. Then, they excuses
            themselves with "I have to change my car's windshield" and other such
            important matters and then disappear for days or weeks on end.
            </p>

            <p>
            There's a big park with a lake nearby and a small river passes just behind the
            building. I often take small brakes from my academic life and stroll
            aimlessly. I sometimes watch the little fishes from the nearby bridge
            gathering in the shade of the willow trees. A kid approaches me one day, "Did
            you see the big one?" We chat a little. That's how I spend my
            days.
            </p>

            <p>
            With three months left, I send Professor my thesis. Days later, he warmly
            congratulates me, "You are an embarrassment to our city!" I stand
            alarmed. "Yes, you are ruining the prestige of our University!" I move closer.
            He points out a paragraph in my thesis where "almost impossible" is heavily
            underlined in red. "Something is either possible or impossible," he mocks me
            with a noticeable grin on his face. I update the offending sentence. I also
            fix a few typos in the following week and rephrase some paragraphs which were
            not to his liking. He eventually approves it. I present it in front of the
            whole Department, the last step before facing the official commission. It gets
            approved.
            </p>

            <p>
            My celebration is cut short a few days later. For some reason, it is of the
            utmost importance to have an actual, real-life experiment to confirm our
            theoretical results. "We can't present a theoretical thesis, we're a Technical
            University," Professor accuses me. I actually agree with him, though I have no
            soul left in this endeavor. How did he come up with this idea? I don't
            know. He probably got admonished by some higher-up. It was fine without it,
            the Department approved it, the Head approved it, it was ready for
            defending. Now it isn't. I shrug and accept it as another fact I can't
            understand nor influence.
            </p>

            <p>
            Professor finds a public institution to lend us their watermelon-sized
            anechoic chamber for two hours. We visit the supermarket one morning to buy
            pork chops for the human head. I want to bring to Professor's attention that
            we're studying a dynamic system and not dead meat. But it's autumn 2012
            already and the parks are in full color. It's way too late for any dialogue. I
            pull out a small plastic bag with a few miniature temperature sensors I bought
            the other day. Professor glues them to a metallic cylinder and inserts it in
            "the head." I see Professor was inspired by my way of handling the lack of
            real stents. I think it is a nail or wire of some sort but I'm not
            sure. Professor handles all the "sensitive equipment" himself. I take pictures
            and write down the results in a notebook. For the next two hours we gather
            temperature readings. I publish a paper with our findings shortly after,
            attach his name to it, update my thesis and everything is good again. I start
            to develop a faint feeling that I sleep better at night when I play along and
            nod approvingly to things I don't actually agree with instead of being
            pigheaded.
            </p>

            <p>
            The final day is approaching. Mrs. B., from the Department of Doctoral
            Studies, informs me that I personally have to prepare and bring in food,
            drinks and coffee for the commission when I defend my thesis. I refuse. She
            insists. I point out that the University charges €1000 per student for the
            final show, that each member of the commission is actually getting paid for
            their trouble and that all these expenses plus transport and accommodation are
            already sponsored by our project. She shows signs of slowly winning back her
            memory. Mrs. B. also informs me that I won't be able to hold back my tears
            upon successfully defending my thesis in front of family, friends and
            colleagues. I successfully defend my thesis a few days later and I refuse her
            that pleasure, too.
            </p>

            <p>
            We celebrate at a local restaurant with the whole Department and the
            commission of five professors that evening. I join out from politeness. There
            is not much science to celebrate. After dinner I shake hands with Professor
            and the Head. "He did make a lot of noise around here but he did a great job
            and has very nice results," the Professor praises me in front of the Head. I
            smile without saying anything. I leave the place and begin to think about the
            years in front of me. But Professor catches up with me. He is a changed man,
            "Let's keep working together!" He is brimming with enthusiasm. I refuse him
            politely but he keeps talking as though my previous answer carries no weight
            with him, as he always does. "Yes, let's keep cooperating on new projects
            together," he goes on and on. I don't know what's gotten into him. Maybe he
            likes his name on new papers too much? He begins to get on my nerves. I answer
            respectfully with simple no's to all of his questions and proposals. I
            eventually say my goodbyes to him and turn my back. I leave Professor in the
            dark alley and my basement behind for good.
            </p>

            <p>
                © Mihai Olteanu, 2025
            </p>
        </div></div>]]></description>
        </item>
    </channel>
</rss>