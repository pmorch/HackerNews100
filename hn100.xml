<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Dec 2023 01:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What if an SQL statement returned a database? (113 pts)]]></title>
            <link>https://arxiv.org/abs/2312.00638</link>
            <guid>38606146</guid>
            <pubDate>Mon, 11 Dec 2023 22:05:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2312.00638">https://arxiv.org/abs/2312.00638</a>, See on <a href="https://news.ycombinator.com/item?id=38606146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2312.00638.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Every SQL statement is limited to return a single, possibly denormalized, table. This design decision has far reaching consequences. (1.) for databases users in terms of slow query performance, long query result transfer times, usability-issues of SQL in web applications and object-relational mappers. In addition, (2.) for database architects it has consequences when designing query optimizers leading to logical (algebraic) join enumeration effort, memory consumption for intermediate result materialization, and physical operator selection effort. So basically, the entire query optimization stack is shaped by that design decision. In this paper, we argue that the single-table limitation should be dropped. We extend the SELECT-clause of SQL by a keyword 'RESULTDB' to support returning a result database. Our approach has clear semantics, i.e. our extended SQL returns subsets of all tables with only those tuples that would be part of the traditional (single-table) query result set, however without performing any denormalization through joins. Our SQL-extension is downward compatible. Moreover, we discuss the surprisingly long list of benefits of our approach. First, for database users: far simpler and more readable application code, better query performance, smaller query results, better query result transfer times. Second, for database architects, we present how to leverage existing closed source systems as well as change open source database systems to support our feature. We propose a couple of algorithms to integrate our feature into both closed-source as well as open source database systems. We present an initial experimental study with promising results.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Joris Nix [<a href="https://arxiv.org/show-email/69170ca3/2312.00638">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 1 Dec 2023 14:59:29 UTC (84 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FISA "reform" bill would greatly expand the entities forced to surveil users (153 pts)]]></title>
            <link>https://www.zwillgen.com/law-enforcement/fisa-reform-bill-702-surveillance/</link>
            <guid>38605525</guid>
            <pubDate>Mon, 11 Dec 2023 21:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zwillgen.com/law-enforcement/fisa-reform-bill-702-surveillance/">https://www.zwillgen.com/law-enforcement/fisa-reform-bill-702-surveillance/</a>, See on <a href="https://news.ycombinator.com/item?id=38605525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-aos="fadeInLeft">
                        
                        <p><a href="https://www.zwillgen.com/blog?cat=law-enforcement">Law Enforcement</a></p><!-- .category -->
						
                         <!-- .h2 -->

                        <p><b>Published: </b>Dec. 08, 2023</p> <!-- .date-block -->

															 <!-- .author-block -->
							
													<p><img width="960" height="540" src="https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-960x540.png" alt="Photo of the U.S. Capitol building dome" itemprop="image" decoding="async" fetchpriority="high" srcset="https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-960x540.png 960w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-768x432.png 768w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-355x200.png 355w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-720x406.png 720w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-992x558.png 992w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-24x14.png 24w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-36x20.png 36w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding-48x27.png 48w, https://www.zwillgen.com/wp-content/uploads/2023/12/uscapitolbuilding.png 1500w" sizes="(max-width: 960px) 100vw, 960px">							</p> <!-- .featured-image-block -->
						
						<div itemprop="articleBody">
							
<p>Yesterday the House Permanent Select Committee on Intelligence unanimously approved the <a href="https://www.congress.gov/118/meeting/house/116666/documents/BILLS-118pih-ReformandreauthorizeauthoritiesoftheForeignIntelligenceSurveillanceActof1978.pdf" target="_blank" rel="noreferrer noopener">FISA Reform and Reauthorization Act of 2023</a> (FRRA), which would reauthorize Section 702 of the Foreign Intelligence Surveillance Act (FISA 702).&nbsp;The full House is expected to vote on this bill next Tuesday.&nbsp;Like other competing reauthorization bills that have been introduced (see&nbsp;<a href="https://www.zwillgen.com/law-enforcement/congress-government-surveillance-ecpa/" target="_blank" rel="noreferrer noopener">here</a>&nbsp;and&nbsp;<a href="https://www.congress.gov/118/meeting/house/116659/documents/BILLS-118HRih-U1.pdf" target="_blank" rel="noreferrer noopener">here</a>), the FRRA would impose new restrictions on the government’s access to and use of information about U.S. persons that has been incidentally acquired pursuant to FISA 702, a provision that permits the government to conduct warrantless surveillance of non-U.S. persons located outside the United States.&nbsp;&nbsp;</p>



<p>Although the FRRA is ostensibly a reform bill, it contains one notable provision that would&nbsp;<strong>significantly expand</strong>&nbsp;the government’s authority under FISA 702 by broadening the definition of “electronic communication service providers” (ECSPs) whom the government may compel to assist in FISA 702 surveillance.&nbsp;The statutory definition of ECSP currently covers:</p>



<p>(1) a telecommunications carrier;&nbsp;</p>



<p>(2) a provider of an electronic communication service or a remote computing service, as defined in the Electronic Communications Privacy Act;&nbsp;</p>



<p>(3) “<strong>any other communication service provider who has access to wire or electronic communications either as such communications are transmitted or as such communications are stored</strong>”; and&nbsp;</p>



<p>(4) an officer, employee, or agent of any such entity.&nbsp;&nbsp;</p>



<p>Section 504 of the FRRA would broaden the “catch all” definition in (3) above to cover:&nbsp;</p>



<p>“<strong>any service provider&nbsp;</strong>who has access to wire or electronic communications either as such communications are transmitted or as such communications are stored&nbsp;<strong>or equipment that is being or may be used to transmit or store such communications</strong>.”</p>



<p>Section 504 would also expand the definition in (4) above to include not only an officer, employee, or agent of an ECSP, but also any “<strong>custodian</strong>” of such an entity.</p>



<p>These changes would vastly widen the scope of businesses, entities, and their affiliates who are eligible to be compelled to assist 702 surveillance.&nbsp;By including any “service provider”—rather than any “other&nbsp;<strong>communication</strong><em>&nbsp;</em>service provider”—that has access not just to communications, but also to the “equipment that is being or may be used to transmit or store . . . communications,” the expanded definition would appear to cover data centers, colocation providers, business landlords, shared workspaces, or even hotels where guests connect to the Internet. And the addition of the term “custodian” in (4) above could be understood to sweep in any third party involved in providing equipment, storage, or even cleaning services to such entities.</p>



<p>The FRRA’s new definition of ECSP would effectively overrule recent decisions of the Foreign Intelligence Surveillance Court (FISC) and the Foreign Intelligence Surveillance Court of Review (FISCR) that&nbsp;<a href="https://www.zwillgen.com/privacy/fisc-and-fisc-r-decisions-section-702-orders/" target="_blank" rel="noreferrer noopener">interpreted the current “catch all” definition much more narrowly</a>, seemingly to exclude those communication service providers who merely have access to equipment on which communications are transmitted or stored.&nbsp;The expanded definition would also effectively restore the broad assistance provision of Section 702’s predecessor, the Protect America Act, which Congress specifically rejected when it originally enacted Section 702 as part of the FISA Amendments Act.&nbsp;The new definition, when combined with NSA’s ability to conduct “upstream collection,” could give the government warrantless access to any communication system in America through which any one-side-foreign communication could be found.</p>



<p>If the FRRA is enacted in its current form, the broadened definition of ECSP could put U.S. companies that offer co-location computer storage and commercial landlords at a competitive disadvantage to foreign service providers and property owners given the possibility that customer or tenant communications could be obtained from them.</p>
						</div> <!-- .content-block -->

						 <!-- .bottom-block -->

					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built an OSS alternative to Azure OpenAI services (107 pts)]]></title>
            <link>https://github.com/bricks-cloud/BricksLLM</link>
            <guid>38603853</guid>
            <pubDate>Mon, 11 Dec 2023 18:56:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bricks-cloud/BricksLLM">https://github.com/bricks-cloud/BricksLLM</a>, See on <a href="https://news.ycombinator.com/item?id=38603853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/bricks-cloud/BricksLLM/blob/main/assets/bricks-logo.png"><img src="https://github.com/bricks-cloud/BricksLLM/raw/main/assets/bricks-logo.png" width="150"></a>
</p>
<h2 tabindex="-1" dir="auto"><strong>BricksLLM: AI Gateway For Putting LLM In Production</strong></h2>
<p dir="auto">
   <a href="https://www.ycombinator.com/" rel="nofollow"><img alt="YCombinator S22" src="https://camo.githubusercontent.com/5aa9a31e0319e4ff13b2e1ed7518bf8463f9858f25ead74eac306d938b86fe15/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d323032322d6f72616e6765" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-2022-orange"></a>
   <a href="http://makeapullrequest.com/" rel="nofollow"><img alt="PRs Welcome" src="https://camo.githubusercontent.com/f4f4ab0968b86d8028118d8c475896001543bf2adea2dd29954f7f98dd897006/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d3433414631312e7376673f7374796c653d736869656c6473" data-canonical-src="https://img.shields.io/badge/PRs-welcome-43AF11.svg?style=shields"></a>
   <a href="https://discord.gg/dFvdt4wqWh" rel="nofollow"><img src="https://camo.githubusercontent.com/18357fa7693e3cf2079af815f818a90192a9c8ddea510598bbbfcc85580e8800/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d427269636b734c4c4d2d626c75653f6c6f676f3d646973636f7264266c6162656c436f6c6f723d324542363744" alt="Join BricksLLM on Discord" data-canonical-src="https://img.shields.io/badge/discord-BricksLLM-blue?logo=discord&amp;labelColor=2EB67D"></a>
   <a href="https://github.com/bricks-cloud/bricks/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/e6df402f35f541941c32696ed191577947023efbf47105c6ed75bd9908ca6d95/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d726564" alt="License" data-canonical-src="https://img.shields.io/badge/license-MIT-red"></a>
</p>
<p dir="auto"><strong>BricksLLM</strong> is a cloud native AI gateway written in Go. Currently, it serves as a proxy to OpenAI. We let you create API keys that have rate limits, cost limits and TTLs. The API keys can be used in both development and production to achieve fine-grained access control that is not provided by OpenAI at the moment. The proxy is compatible with OpenAI API and its SDKs.</p>
<p dir="auto">The vision of BricksLLM is to support many more large language models such as LLama2, Claude, PaLM2 etc, and streamline LLM operations.</p>
<h2 tabindex="-1" dir="auto">Roadmap</h2>
<ul>
<li> Access control via API key with rate limit, cost limit and ttl</li>
<li> Logging integration</li>
<li> Statsd integration</li>
<li> Custom Provider Integration</li>
<li> PII detection and masking 🚧</li>
</ul>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<p dir="auto">The easiest way to get started with BricksLLM is through <a href="https://github.com/bricks-cloud/BricksLLM-Docker">BricksLLM-Docker</a>.</p>
<h3 tabindex="-1" dir="auto">Step 1 - Clone BricksLLM-Docker repository</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/bricks-cloud/BricksLLM-Docker"><pre>git clone https://github.com/bricks-cloud/BricksLLM-Docker</pre></div>
<h3 tabindex="-1" dir="auto">Step 2 - Change to BricksLLM-Docker directory</h3>

<h3 tabindex="-1" dir="auto">Step 3 - Deploy BricksLLM locally with Postgresql and Redis</h3>

<p dir="auto">You can run this in detach mode use the -d flag: <code>docker-compose up -d</code></p>
<h3 tabindex="-1" dir="auto">Step 4 - Create a provider setting</h3>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X PUT http://localhost:8001/api/provider-settings \
   -H &quot;Content-Type: application/json&quot; \
   -d '{
          &quot;provider&quot;:&quot;openai&quot;,
          &quot;setting&quot;: {
             &quot;apikey&quot;: &quot;YOUR_OPENAI_KEY&quot;
          }
      }'   "><pre>curl -X PUT http://localhost:8001/api/provider-settings \
   -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
   -d <span><span>'</span>{</span>
<span>          "provider":"openai",</span>
<span>          "setting": {</span>
<span>             "apikey": "YOUR_OPENAI_KEY"</span>
<span>          }</span>
<span>      }<span>'</span></span>   </pre></div>
<p dir="auto">Copy the <code>id</code> from the response.</p>
<h3 tabindex="-1" dir="auto">Step 5 - Create a Bricks API key</h3>
<p dir="auto">Use <code>id</code> from the previous step as <code>settingId</code> to create a key with a rate limit of 2 req/min and a spend limit of 25 cents.</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X PUT http://localhost:8001/api/key-management/keys \
   -H &quot;Content-Type: application/json&quot; \
   -d '{
	      &quot;name&quot;: &quot;My Secret Key&quot;,
	      &quot;key&quot;: &quot;my-secret-key&quot;,
	      &quot;tags&quot;: [&quot;mykey&quot;],
        &quot;settingId&quot;: &quot;ID_FROM_STEP_FOUR&quot;,
        &quot;rateLimitOverTime&quot;: 2,
        &quot;rateLimitUnit&quot;: &quot;m&quot;,
        &quot;costLimitInUsd&quot;: 0.25
      }'   "><pre>curl -X PUT http://localhost:8001/api/key-management/keys \
   -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
   -d <span><span>'</span>{</span>
<span>	      "name": "My Secret Key",</span>
<span>	      "key": "my-secret-key",</span>
<span>	      "tags": ["mykey"],</span>
<span>        "settingId": "ID_FROM_STEP_FOUR",</span>
<span>        "rateLimitOverTime": 2,</span>
<span>        "rateLimitUnit": "m",</span>
<span>        "costLimitInUsd": 0.25</span>
<span>      }<span>'</span></span>   </pre></div>
<h3 tabindex="-1" dir="auto">Congratulations you are done!!!</h3>
<p dir="auto">Then, just redirect your requests to us and use OpenAI as you would normally. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X POST http://localhost:8002/api/providers/openai/v1/chat/completions \
   -H &quot;Authorization: Bearer my-secret-key&quot; \
   -H &quot;Content-Type: application/json&quot; \
   -d '{
          &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
          &quot;messages&quot;: [
              {
                  &quot;role&quot;: &quot;system&quot;,
                  &quot;content&quot;: &quot;hi&quot;
              }
          ]
      }'"><pre>curl -X POST http://localhost:8002/api/providers/openai/v1/chat/completions \
   -H <span><span>"</span>Authorization: Bearer my-secret-key<span>"</span></span> \
   -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
   -d <span><span>'</span>{</span>
<span>          "model": "gpt-3.5-turbo",</span>
<span>          "messages": [</span>
<span>              {</span>
<span>                  "role": "system",</span>
<span>                  "content": "hi"</span>
<span>              }</span>
<span>          ]</span>
<span>      }<span>'</span></span></pre></div>
<p dir="auto">Or if you're using an SDK, you could change its <code>baseURL</code> to point to us. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// OpenAI Node SDK v4
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: &quot;some-secret-key&quot;, // key created earlier
  baseURL: &quot;http://localhost:8002/api/providers/openai/v1&quot;, // redirect to us
});"><pre><span>// OpenAI Node SDK v4</span>
<span>import</span> <span>OpenAI</span> <span>from</span> <span>'openai'</span><span>;</span>

<span>const</span> <span>openai</span> <span>=</span> <span>new</span> <span>OpenAI</span><span>(</span><span>{</span>
  <span>apiKey</span>: <span>"some-secret-key"</span><span>,</span> <span>// key created earlier</span>
  <span>baseURL</span>: <span>"http://localhost:8002/api/providers/openai/v1"</span><span>,</span> <span>// redirect to us</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<h2 tabindex="-1" dir="auto">How to Update?</h2>
<p dir="auto">For updating to the latest version</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull luyuanxin1995/bricksllm:latest"><pre>docker pull luyuanxin1995/bricksllm:latest</pre></div>
<p dir="auto">For updating to a particular version</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull luyuanxin1995/bricksllm:1.4.0"><pre>docker pull luyuanxin1995/bricksllm:1.4.0</pre></div>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<h2 tabindex="-1" dir="auto">Environment variables</h2>
<blockquote>
<table>
<thead>
<tr>
<th>Name</th>
<th>type</th>
<th>description</th>
<th>default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>POSTGRESQL_HOSTS</code></td>
<td>required</td>
<td>Hosts for Postgresql DB. Seperated by ,</td>
<td><code>localhost</code></td>
</tr>
<tr>
<td><code>POSTGRESQL_DB_NAME</code></td>
<td>optional</td>
<td>Name for Postgresql DB.</td>
<td></td>
</tr>
<tr>
<td><code>POSTGRESQL_USERNAME</code></td>
<td>required</td>
<td>Postgresql DB username</td>
<td></td>
</tr>
<tr>
<td><code>POSTGRESQL_PASSWORD</code></td>
<td>required</td>
<td>Postgresql DB password</td>
<td></td>
</tr>
<tr>
<td><code>POSTGRESQL_SSL_MODE</code></td>
<td>optional</td>
<td>Postgresql SSL mode</td>
<td><code>disable</code></td>
</tr>
<tr>
<td><code>POSTGRESQL_PORT</code></td>
<td>optional</td>
<td>The port that Postgresql DB runs on</td>
<td><code>5432</code></td>
</tr>
<tr>
<td><code>POSTGRESQL_READ_TIME_OUT</code></td>
<td>optional</td>
<td>Timeout for Postgresql read operations</td>
<td><code>2s</code></td>
</tr>
<tr>
<td><code>POSTGRESQL_WRITE_TIME_OUT</code></td>
<td>optional</td>
<td>Timeout for Postgresql write operations</td>
<td><code>1s</code></td>
</tr>
<tr>
<td><code>REDIS_HOSTS</code></td>
<td>required</td>
<td>Host for Redis. Seperated by ,</td>
<td><code>localhost</code></td>
</tr>
<tr>
<td><code>REDIS_PASSWORD</code></td>
<td>optional</td>
<td>Redis Password</td>
<td></td>
</tr>
<tr>
<td><code>REDIS_PORT</code></td>
<td>optional</td>
<td>The port that Redis DB runs on</td>
<td><code>6379</code></td>
</tr>
<tr>
<td><code>REDIS_READ_TIME_OUT</code></td>
<td>optional</td>
<td>Timeout for Redis read operations</td>
<td><code>1s</code></td>
</tr>
<tr>
<td><code>REDIS_WRITE_TIME_OUT</code></td>
<td>optional</td>
<td>Timeout for Redis write operations</td>
<td><code>500ms</code></td>
</tr>
<tr>
<td><code>IN_MEMORY_DB_UPDATE_INTERVAL</code></td>
<td>optional</td>
<td>The interval BricksLLM API gateway polls Postgresql DB for latest key configurations</td>
<td><code>1s</code></td>
</tr>
<tr>
<td><code>STATS_PROVIDER</code></td>
<td>optional</td>
<td>This value can only be datadog. Required for integration with Datadog.</td>
<td></td>
</tr>
<tr>
<td><code>PROXY_TIMEOUT</code></td>
<td>optional</td>
<td>This value can only be datadog. Required for integration with Datadog.</td>
<td></td>
</tr>
</tbody>
</table>
</blockquote>
<h2 tabindex="-1" dir="auto">Configuration Endpoints</h2>
<p dir="auto">The configuration server runs on Port <code>8001</code>.</p>
<details>
  <summary>Get keys: <code>GET</code> <code><b>/api/key-management/keys</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving key configurations using a query param called tag.</p>
<h5 tabindex="-1" dir="auto">Query Parameters</h5>
<blockquote>
<table>
<thead>
<tr>
<th>name</th>
<th>type</th>
<th>data type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tag</code></td>
<td>optional</td>
<td>string</td>
<td>Identifier attached to a key configuration</td>
</tr>
<tr>
<td><code>tags</code></td>
<td>optional</td>
<td>array of string</td>
<td>Identifiers attached to a key configuration</td>
</tr>
<tr>
<td><code>provider</code></td>
<td>optional</td>
<td>string</td>
<td>Provider attached to a key provider configuration. Its value can only be <code>openai</code>.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>400</code>, <code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td>400</td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/key-management/keys</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Response Body</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>[]KeyConfiguration</code></td>
</tr>
</tbody>
</table>
</blockquote>
<p dir="auto">Fields of KeyConfiguration</p>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>spike's developer key</td>
<td>Name of the API key.</td>
</tr>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td>1257894000</td>
<td>Key configuration creation time in unix.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td>1257894000</td>
<td>Key configuration update time in unix.</td>
</tr>
<tr>
<td>revoked</td>
<td><code>boolean</code></td>
<td>true</td>
<td>Indicator for whether the key is revoked.</td>
</tr>
<tr>
<td>revokedReason</td>
<td><code>string</code></td>
<td>The key has expired</td>
<td>Reason for why the key is revoked.</td>
</tr>
<tr>
<td>tags</td>
<td><code>[]string</code></td>
<td>["org-tag-12345"]</td>
<td>Identifiers associated with the key.</td>
</tr>
<tr>
<td>keyId</td>
<td><code>string</code></td>
<td>550e8400-e29b-41d4-a716-446655440000</td>
<td>Unique identifier for the key.</td>
</tr>
<tr>
<td>costLimitInUsd</td>
<td><code>float64</code></td>
<td><code>5.5</code></td>
<td>Total spend limit of the API key.</td>
</tr>
<tr>
<td>costLimitInUsdOverTime</td>
<td><code>float64</code></td>
<td><code>2</code></td>
<td>Total spend within period of time. This field is required if costLimitInUsdUnit is specified.</td>
</tr>
<tr>
<td>costLimitInUsdUnit</td>
<td><code>enum</code></td>
<td>d</td>
<td>Time unit for costLimitInUsdOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>].</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitUnit</td>
<td><code>string</code></td>
<td>m</td>
<td>Time unit for rateLimitOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>]</td>
</tr>
<tr>
<td>ttl</td>
<td><code>string</code></td>
<td>2d</td>
<td>time to live. Available units are [<code>s</code>, <code>m</code>, <code>h</code>]</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Create key: <code>PUT</code> <code><b>/api/key-management/keys</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving key configurations using a query param called tag.</p>
<h5 tabindex="-1" dir="auto">Request</h5>

<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>required</td>
<td><code>string</code></td>
<td>/api/providers/openai/v1/chat/completion</td>
<td>Allowed path</td>
</tr>
<tr>
<td>method</td>
<td>required</td>
<td><code>string</code></td>
<td>POST</td>
<td>HTTP Method</td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>required</td>
<td><code>string</code></td>
<td>spike's developer key</td>
<td>Name of the API key.</td>
</tr>
<tr>
<td>tags</td>
<td>optional</td>
<td><code>[]string</code></td>
<td><code>["org-tag-12345"] </code></td>
<td>Identifiers associated with the key.</td>
</tr>
<tr>
<td>key</td>
<td>required</td>
<td><code>string</code></td>
<td>abcdef12345</td>
<td>API key</td>
</tr>
<tr>
<td>settingId</td>
<td>required</td>
<td><code>string</code></td>
<td>98daa3ae-961d-4253-bf6a-322a32fdca3d</td>
<td>API key</td>
</tr>
<tr>
<td>costLimitInUsd</td>
<td>optional</td>
<td><code>float64</code></td>
<td><code>5.5</code></td>
<td>Total spend limit of the API key.</td>
</tr>
<tr>
<td>costLimitInUsdOverTime</td>
<td>optional</td>
<td><code>float64</code></td>
<td><code>2</code></td>
<td>Total spend within period of time. This field is required if costLimitInUsdUnit is specified.</td>
</tr>
<tr>
<td>costLimitInUsdUnit</td>
<td>optional</td>
<td><code>enum</code></td>
<td>d</td>
<td>Time unit for costLimitInUsdOverTime. Possible values are [<code>h</code>, <code>d</code>].</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td>optional</td>
<td><code>int</code></td>
<td>2</td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitUnit</td>
<td>optional</td>
<td><code>enum</code></td>
<td>m</td>
<td>Time unit for rateLimitOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>]</td>
</tr>
<tr>
<td>ttl</td>
<td>optional</td>
<td><code>string</code></td>
<td>2d</td>
<td>time to live. Available units are [<code>s</code>, <code>m</code>, <code>h</code>]</td>
</tr>
<tr>
<td>allowedPaths</td>
<td>optional</td>
<td><code>[]PathConfig</code></td>
<td>2d</td>
<td>Pathes allowed for access</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>400</code>, <code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/key-management/keys</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Responses</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>spike's developer key</td>
<td>Name of the API key.</td>
</tr>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td><code>1257894000</code></td>
<td>Key configuration creation time in unix.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td><code>1257894000</code></td>
<td>Key configuration update time in unix.</td>
</tr>
<tr>
<td>revoked</td>
<td><code>boolean</code></td>
<td><code>true</code></td>
<td>Indicator for whether the key is revoked.</td>
</tr>
<tr>
<td>revokedReason</td>
<td><code>string</code></td>
<td>The key has expired</td>
<td>Reason for why the key is revoked.</td>
</tr>
<tr>
<td>tags</td>
<td><code>[]string</code></td>
<td>["org-tag-12345"]</td>
<td>Identifiers associated with the key.</td>
</tr>
<tr>
<td>keyId</td>
<td><code>string</code></td>
<td>550e8400-e29b-41d4-a716-446655440000</td>
<td>Unique identifier for the key.</td>
</tr>
<tr>
<td>costLimitInUsd</td>
<td><code>float64</code></td>
<td><code>5.5</code></td>
<td>Total spend limit of the API key.</td>
</tr>
<tr>
<td>costLimitInUsdOverTime</td>
<td><code>float64</code></td>
<td>2</td>
<td>Total spend within period of time. This field is required if costLimitInUsdUnit is specified.</td>
</tr>
<tr>
<td>costLimitInUsdUnit</td>
<td><code>enum</code></td>
<td>d</td>
<td>Time unit for costLimitInUsdOverTime. Possible values are [<code>h</code>, <code>d</code>].</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitUnit</td>
<td><code>string</code></td>
<td>m</td>
<td>Time unit for rateLimitOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>].</td>
</tr>
<tr>
<td>ttl</td>
<td><code>string</code></td>
<td>2d</td>
<td>time to live. Available units are [<code>s</code>, <code>m</code>, <code>h</code>]</td>
</tr>
<tr>
<td>allowedPaths</td>
<td><code>[]PathConfig</code></td>
<td><code>[{ "path": "/api/providers/openai/v1/chat/completion", method: "POST"}]</code></td>
<td>Allowed paths that can be accessed using the key.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Update key: <code>PATCH</code> <code><b>/api/key-management/keys/{keyId}</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for updating key configurations using key id.</p>
<h5 tabindex="-1" dir="auto">Parameters</h5>
<blockquote>
<table>
<thead>
<tr>
<th>name</th>
<th>type</th>
<th>data type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>keyId</code></td>
<td>required</td>
<td>string</td>
<td>Unique key configuration identifier.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Request</h5>

<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>required</td>
<td><code>string</code></td>
<td>/api/providers/openai/v1/chat/completion</td>
<td>Allowed path</td>
</tr>
<tr>
<td>method</td>
<td>required</td>
<td><code>string</code></td>
<td>POST</td>
<td>HTTP Method</td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>optional</td>
<td><code>string</code></td>
<td>spike's developer key</td>
<td>Name of the API key.</td>
</tr>
<tr>
<td>tags</td>
<td>optional</td>
<td><code>[]string</code></td>
<td><code>["org-tag-12345"]</code></td>
<td>Identifiers associated with the key.</td>
</tr>
<tr>
<td>revoked</td>
<td>optional</td>
<td><code>boolean</code></td>
<td><code>true</code></td>
<td>Indicator for whether the key is revoked.</td>
</tr>
<tr>
<td>revokedReason</td>
<td>optional</td>
<td><code>string</code></td>
<td>The key has expired</td>
<td>Reason for why the key is revoked.</td>
</tr>
<tr>
<td>costLimitInUsdOverTime</td>
<td>optional</td>
<td><code>float64</code></td>
<td><code>2</code></td>
<td>Total spend within period of time. This field is required if costLimitInUsdUnit is specified.</td>
</tr>
<tr>
<td>costLimitInUsdUnit</td>
<td>optional</td>
<td><code>enum</code></td>
<td>d</td>
<td>Time unit for costLimitInUsdOverTime. Possible values are [<code>h</code>, <code>d</code>].</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td>optional</td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitUnit</td>
<td>optional</td>
<td><code>enum</code></td>
<td>m</td>
<td>Time unit for rateLimitOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>]</td>
</tr>
<tr>
<td>allowedPaths</td>
<td>optional</td>
<td><code>[]PathConfig</code></td>
<td>2d</td>
<td>Pathes allowed for access.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>400</code>, <code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td>400</td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/key-management/keys</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>spike's developer key</td>
<td>Name of the API key.</td>
</tr>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td><code>1257894000</code></td>
<td>Key configuration creation time in unix.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td><code>1257894000</code></td>
<td>Key configuration update time in unix.</td>
</tr>
<tr>
<td>revoked</td>
<td><code>boolean</code></td>
<td><code>true</code></td>
<td>Indicator for whether the key is revoked.</td>
</tr>
<tr>
<td>revokedReason</td>
<td><code>string</code></td>
<td>The key has expired</td>
<td>Reason for why the key is revoked.</td>
</tr>
<tr>
<td>tags</td>
<td><code>[]string</code></td>
<td><code>["org-tag-12345"]</code></td>
<td>Identifiers associated with the key.</td>
</tr>
<tr>
<td>keyId</td>
<td><code>string</code></td>
<td>550e8400-e29b-41d4-a716-446655440000</td>
<td>Unique identifier for the key.</td>
</tr>
<tr>
<td>costLimitInUsd</td>
<td><code>float64</code></td>
<td><code>5.5</code></td>
<td>Total spend limit of the API key.</td>
</tr>
<tr>
<td>costLimitInUsdOverTime</td>
<td><code>float64</code></td>
<td><code>2</code></td>
<td>Total spend within period of time. This field is required if costLimitInUsdUnit is specified.</td>
</tr>
<tr>
<td>costLimitInUsdUnit</td>
<td><code>enum</code></td>
<td>d</td>
<td>Time unit for costLimitInUsdOverTime. Possible values are [<code>h</code>, <code>d</code>].</td>
</tr>
<tr>
<td>rateLimitOverTime</td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>rate limit over period of time. This field is required if rateLimitUnit is specified.</td>
</tr>
<tr>
<td>rateLimitUnit</td>
<td><code>string</code></td>
<td>m</td>
<td>Time unit for rateLimitOverTime. Possible values are [<code>h</code>, <code>m</code>, <code>s</code>, <code>d</code>]</td>
</tr>
<tr>
<td>ttl</td>
<td><code>string</code></td>
<td>2d</td>
<td>time to live. Available units are [<code>s</code>, <code>m</code>, <code>h</code>]</td>
</tr>
<tr>
<td>allowedPaths</td>
<td><code>[]PathConfig</code></td>
<td><code>[{ "path": "/api/providers/openai/v1/chat/completion", method: "POST"}]</code></td>
<td>Allowed paths that can be accessed using the key.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Create a provider setting: <code>POST</code> <code><b>/api/provider-settings</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is creating a provider setting.</p>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>provider</td>
<td>required</td>
<td><code>enum</code></td>
<td>openai</td>
<td>This value can only be <code>openai</code> as for now.</td>
</tr>
<tr>
<td>setting</td>
<td>required</td>
<td><code>object</code></td>
<td><code>{ "apikey": "YOUR_OPENAI_KEY" }</code></td>
<td>A map of values used for authenticating with the selected provider.</td>
</tr>
<tr>
<td>setting.apikey</td>
<td>required</td>
<td><code>string</code></td>
<td>xx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</td>
<td>This field is required if <code>provider</code> is <code>openai</code>.</td>
</tr>
<tr>
<td>name</td>
<td>optional</td>
<td><code>string</code></td>
<td>YOUR_PROVIDER_SETTING_NAME</td>
<td>This field is used for giving a name to provider setting</td>
</tr>
<tr>
<td>allowedModels</td>
<td><code>[]string</code></td>
<td><code>["text-embedding-ada-002"]</code></td>
<td>Allowed models for this provider setting.</td>
<td></td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>400</code>, <code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/provider-settings</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>enum</code></td>
<td><code>openai</code></td>
<td>This value can only be <code>openai</code> as for now.</td>
</tr>
<tr>
<td>id</td>
<td><code>string</code></td>
<td>98daa3ae-961d-4253-bf6a-322a32fdca3d</td>
<td>This value is a unique identifier.</td>
</tr>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>YOUR_PROVIDER_SETTING_NAME</td>
<td>Provider setting name.</td>
</tr>
<tr>
<td>allowedModels</td>
<td><code>[]string</code></td>
<td><code>["text-embedding-ada-002"]</code></td>
<td>Allowed models for this provider setting.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Get all provider settings: <code>GET</code> <code><b>/api/provider-settings</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is getting all provider settings.</p>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/provider-settings</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>

<p dir="auto">ProviderSetting</p>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>enum</code></td>
<td><code>openai</code></td>
<td>This value can only be <code>openai</code> as for now.</td>
</tr>
<tr>
<td>id</td>
<td><code>string</code></td>
<td>98daa3ae-961d-4253-bf6a-322a32fdca3d</td>
<td>This value is a unique identifier.</td>
</tr>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>YOUR_PROVIDER_SETTING_NAME</td>
<td>Provider setting name.</td>
</tr>
<tr>
<td>allowedModels</td>
<td><code>[]string</code></td>
<td><code>["text-embedding-ada-002"]</code></td>
<td>Allowed models for this provider setting.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Update a provider setting: <code>PATCH</code> <code><b>/api/provider-settings/:id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is updating a provider setting .</p>
<h5 tabindex="-1" dir="auto">Parameters</h5>
<blockquote>
<table>
<thead>
<tr>
<th>name</th>
<th>type</th>
<th>data type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>id</code></td>
<td>required</td>
<td><code>string</code></td>
<td>Unique identifier for the provider setting that you want to update.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>setting</td>
<td>required</td>
<td><code>object</code></td>
<td><code>{ "apikey": "YOUR_OPENAI_KEY" }</code></td>
<td>A map of values used for authenticating with the selected provider.</td>
</tr>
<tr>
<td>setting.apikey</td>
<td>required</td>
<td><code>string</code></td>
<td>xx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</td>
<td>This field is required if <code>provider</code> is <code>openai</code>.</td>
</tr>
<tr>
<td>name</td>
<td>optional</td>
<td><code>string</code></td>
<td>YOUR_PROVIDER_SETTING_NAME</td>
<td>This field is used for giving a name to provider setting</td>
</tr>
<tr>
<td>allowedModels</td>
<td><code>[]string</code></td>
<td><code>["text-embedding-ada-002"]</code></td>
<td>Allowed models for this provider setting.</td>
<td></td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>400</code>, <code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/provider-settings</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>createdAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updatedAt</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>enum</code></td>
<td><code>openai</code></td>
<td>This value can only be <code>openai</code> as for now.</td>
</tr>
<tr>
<td>id</td>
<td><code>string</code></td>
<td><code>98daa3ae-961d-4253-bf6a-322a32fdca3d</code></td>
<td>This value is a unique identifier</td>
</tr>
<tr>
<td>name</td>
<td><code>string</code></td>
<td>YOUR_PROVIDER_SETTING_NAME</td>
<td>Provider setting name.</td>
</tr>
<tr>
<td>allowedModels</td>
<td><code>[]string</code></td>
<td><code>["text-embedding-ada-002"]</code></td>
<td>Allowed models for this provider setting.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Retrieve Metrics: <code>POST</code> <code><b>/api/reporting/events</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is retrieving aggregated metrics given an array of key ids and tags.</p>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>keyIds</td>
<td>required</td>
<td><code>[]string</code></td>
<td><code>["key-1", "key-2", "key-3" ]</code></td>
<td>Array of ids that specicify the keys that you want to aggregate stats from.</td>
</tr>
<tr>
<td>tags</td>
<td>required</td>
<td><code>[]string</code></td>
<td><code>["tag-1", "tag-2"]</code></td>
<td>Array of tags that specicify the keys that you want to aggregate stats from.</td>
</tr>
<tr>
<td>start</td>
<td>required</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Start timestamp for the requested timeseries data.</td>
</tr>
<tr>
<td>end</td>
<td>required</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>End timestamp for the requested timeseries data.</td>
</tr>
<tr>
<td>increment</td>
<td>required</td>
<td><code>int</code></td>
<td><code>60</code></td>
<td>This field is the increment in seconds for the requested timeseries data.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/provider-settings</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>dataPoints</td>
<td><code>[]dataPoint</code></td>
<td><code>[{ "timeStamp": 1699933571, "numberOfRequests": 1, "costInUsd": 0.8, "latencyInMs": 600, "promptTokenCount": 0, "completionTokenCount": 0, "successCount": 1 }]</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>latencyInMsMedian</td>
<td><code>float64</code></td>
<td><code>656.7</code></td>
<td>Median latency for the given time period.</td>
</tr>
<tr>
<td>latencyInMs99th</td>
<td><code>float64</code></td>
<td><code>555.7</code></td>
<td>99th percentile latency for the given time period.</td>
</tr>
<tr>
<td>dataPoints.[].timeStamp</td>
<td><code>int64</code></td>
<td><code>555.7</code></td>
<td>Timestamp of the data point</td>
</tr>
<tr>
<td>dataPoints.[].numberOfRequests</td>
<td><code>int64</code></td>
<td><code>555.7</code></td>
<td>Aggregated number of http requests over the given time increment.</td>
</tr>
<tr>
<td>dataPoints.[].costInUsd</td>
<td><code>int64</code></td>
<td><code>555.7</code></td>
<td>Aggregated cost of http requests over the given time increment.</td>
</tr>
<tr>
<td>dataPoints.[].latencyInMs</td>
<td><code>float64</code></td>
<td><code>555.7</code></td>
<td>Aggregated latency of http requests over the given time increment.</td>
</tr>
<tr>
<td>dataPoints.[].promptTokenCount</td>
<td><code>int</code></td>
<td><code>555.7</code></td>
<td>Aggregated prompt token counts over the given time increment.</td>
</tr>
<tr>
<td>dataPoints.[].completionTokenCount</td>
<td><code>int</code></td>
<td><code>555.7</code></td>
<td>Aggregated completion token counts over the given time increment.</td>
</tr>
<tr>
<td>dataPoints.[].successCount</td>
<td><code>int</code></td>
<td><code>555.7</code></td>
<td>Aggregated number of successful http requests over the given time increment.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Get events: <code>GET</code> <code><b>/api/events</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is for getting events.</p>
<h5 tabindex="-1" dir="auto">Query Parameters</h5>
<blockquote>
<table>
<thead>
<tr>
<th>name</th>
<th>type</th>
<th>data type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>customId</code></td>
<td>optional</td>
<td>string</td>
<td>Custom identifier attached to an event</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/provider-settings</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>

<p dir="auto">Event</p>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unique identifier associated with the event.</td>
</tr>
<tr>
<td>created_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>tags</td>
<td><code>int64</code></td>
<td><code>["YOUR_TAG"]</code></td>
<td>Tags of the key.</td>
</tr>
<tr>
<td>key_id</td>
<td><code>string</code></td>
<td>YOUR_KEY_ID</td>
<td>Key Id associated with the proxy request.</td>
</tr>
<tr>
<td>cost_in_usd</td>
<td><code>float64</code></td>
<td>0.0004</td>
<td>Cost incured by the proxy request.</td>
</tr>
<tr>
<td>model</td>
<td><code>string</code></td>
<td>gpt-4-1105-preview</td>
<td>Model used in the proxy request.</td>
</tr>
<tr>
<td>provider</td>
<td><code>string</code></td>
<td><code>openai</code></td>
<td>Provider for the proxy request.</td>
</tr>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>200</code></td>
<td>Http status.</td>
</tr>
<tr>
<td>prompt_token_count</td>
<td><code>int</code></td>
<td><code>8</code></td>
<td>Prompt token count of the proxy request.</td>
</tr>
<tr>
<td>completion_token_count</td>
<td><code>int</code></td>
<td><code>16</code></td>
<td>Completion token counts of the proxy request.</td>
</tr>
<tr>
<td>latency_in_ms</td>
<td><code>int</code></td>
<td><code>160</code></td>
<td>Provider setting name.</td>
</tr>
<tr>
<td>path</td>
<td><code>string</code></td>
<td>/api/v1/chat/completion</td>
<td>Provider setting name.</td>
</tr>
<tr>
<td>method</td>
<td><code>string</code></td>
<td>POST</td>
<td>Http method for the assoicated proxu request.</td>
</tr>
<tr>
<td>custom_id</td>
<td><code>string</code></td>
<td>YOUR_CUSTOM_ID</td>
<td>Custom Id passed by the user in the headers of proxy requests.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Create custom provider: <code>POST</code> <code><b>/api/custom/providers</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is creating custom providers.</p>
<h5 tabindex="-1" dir="auto">Route Config</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>required</td>
<td><code>string</code></td>
<td><code>/chat/completion</code></td>
<td>Path associated with the custom provider route. It must be unique within the custom provider.</td>
</tr>
<tr>
<td>target_url</td>
<td>required</td>
<td><code>string</code></td>
<td><code>https://api.openai.com/v1/chat/completions</code></td>
<td>Proxy destination URL for the custom provider route.</td>
</tr>
<tr>
<td>model_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>model</code></td>
<td>JSON field for the model in the HTTP request.</td>
</tr>
<tr>
<td>request_prompt_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>messages.#.content</code></td>
<td>JSON field for the prompt request in the HTTP request.</td>
</tr>
<tr>
<td>response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.message.content</code></td>
<td>JSON field for the completion content in the HTTP response.</td>
</tr>
<tr>
<td>stream_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>stream</code></td>
<td>JSON field for the stream boolean in the HTTP request.</td>
</tr>
<tr>
<td>stream_end_word</td>
<td>required</td>
<td><code>string</code></td>
<td><code>[DONE]</code></td>
<td>End word for the stream.</td>
</tr>
<tr>
<td>stream_response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.delta.content</code></td>
<td>JSON field for the completion content in the streaming response.</td>
</tr>
<tr>
<td>stream_max_empty_messages</td>
<td>required</td>
<td><code>int</code></td>
<td><code>10</code></td>
<td>Number of max empty messages in stream.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>provider</td>
<td>required</td>
<td><code>string</code></td>
<td><code>bricks</code></td>
<td>Unique identifier associated with the route config.</td>
</tr>
<tr>
<td>route_configs</td>
<td>required</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Route configurations for the custom provider.</td>
</tr>
<tr>
<td>authentication_param</td>
<td>optional</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code>, <code>400</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/custom/providers</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unique identifier associated with the event.</td>
</tr>
<tr>
<td>created_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updated_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>string</code></td>
<td><code>bricks</code></td>
<td>Unique identifier associated with the route config.</td>
</tr>
<tr>
<td>route_configs</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Start timestamp for the requested timeseries data.</td>
</tr>
<tr>
<td>authentication_param</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Update custom provider: <code>PATCH</code> <code><b>/api/custom/providers/:id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is updating a custom provider.</p>
<h5 tabindex="-1" dir="auto">Route Config</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>required</td>
<td><code>string</code></td>
<td><code>/chat/completion</code></td>
<td>Path associated with the custom provider route. It must be unique within the custom provider.</td>
</tr>
<tr>
<td>target_url</td>
<td>required</td>
<td><code>string</code></td>
<td><code>https://api.openai.com/v1/chat/completions</code></td>
<td>Proxy destination URL for the custom provider route.</td>
</tr>
<tr>
<td>model_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>model</code></td>
<td>JSON field for the model in the HTTP request.</td>
</tr>
<tr>
<td>request_prompt_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>messages.#.content</code></td>
<td>JSON field for the prompt request in the HTTP request.</td>
</tr>
<tr>
<td>response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.message.content</code></td>
<td>JSON field for the completion content in the HTTP response.</td>
</tr>
<tr>
<td>stream_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>stream</code></td>
<td>JSON field for the stream boolean in the HTTP request.</td>
</tr>
<tr>
<td>stream_end_word</td>
<td>required</td>
<td><code>string</code></td>
<td><code>[DONE]</code></td>
<td>End word for the stream.</td>
</tr>
<tr>
<td>stream_response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.delta.content</code></td>
<td>JSON field for the completion content in the streaming response.</td>
</tr>
<tr>
<td>stream_max_empty_messages</td>
<td>required</td>
<td><code>int</code></td>
<td><code>10</code></td>
<td>Number of max empty messages in stream.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>route_configs</td>
<td>optional</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Route configurations for the custom provider.</td>
</tr>
<tr>
<td>authentication_param</td>
<td>optional</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code>, <code>404</code>, <code>400</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/custom/providers</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unique identifier associated with the event.</td>
</tr>
<tr>
<td>created_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updated_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>string</code></td>
<td><code>bricks</code></td>
<td>Unique identifier associated with the route config.</td>
</tr>
<tr>
<td>route_configs</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Start timestamp for the requested timeseries data.</td>
</tr>
<tr>
<td>authentication_param</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<details>
  <summary>Get custom providers: <code>GET</code> <code><b>/api/custom/providers</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is for getting custom providers.</p>
<h5 tabindex="-1" dir="auto">Route Config</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>required</td>
<td><code>string</code></td>
<td><code>/chat/completion</code></td>
<td>Path associated with the custom provider route. It must be unique within the custom provider.</td>
</tr>
<tr>
<td>target_url</td>
<td>required</td>
<td><code>string</code></td>
<td><code>https://api.openai.com/v1/chat/completions</code></td>
<td>Proxy destination URL for the custom provider route.</td>
</tr>
<tr>
<td>model_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>model</code></td>
<td>JSON field for the model in the HTTP request.</td>
</tr>
<tr>
<td>request_prompt_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>messages.#.content</code></td>
<td>JSON field for the prompt request in the HTTP request.</td>
</tr>
<tr>
<td>response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.message.content</code></td>
<td>JSON field for the completion content in the HTTP response.</td>
</tr>
<tr>
<td>stream_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>stream</code></td>
<td>JSON field for the stream boolean in the HTTP request.</td>
</tr>
<tr>
<td>stream_end_word</td>
<td>required</td>
<td><code>string</code></td>
<td><code>[DONE]</code></td>
<td>End word for the stream.</td>
</tr>
<tr>
<td>stream_response_completion_location</td>
<td>required</td>
<td><code>string</code></td>
<td><code>choices.#.delta.content</code></td>
<td>JSON field for the completion content in the streaming response.</td>
</tr>
<tr>
<td>stream_max_empty_messages</td>
<td>required</td>
<td><code>int</code></td>
<td><code>10</code></td>
<td>Number of max empty messages in stream.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Request</h5>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>required</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>route_configs</td>
<td>optional</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Route configurations for the custom provider.</td>
</tr>
<tr>
<td>authentication_param</td>
<td>optional</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Error Response</h5>
<blockquote>
<table>
<thead>
<tr>
<th>http code</th>
<th>content-type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>500</code></td>
<td><code>application/json</code></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>status</td>
<td><code>int</code></td>
<td><code>400</code></td>
</tr>
<tr>
<td>title</td>
<td><code>string</code></td>
<td>request body reader error</td>
</tr>
<tr>
<td>type</td>
<td><code>string</code></td>
<td>/errors/request-body-read</td>
</tr>
<tr>
<td>detail</td>
<td><code>string</code></td>
<td>something is wrong</td>
</tr>
<tr>
<td>instance</td>
<td><code>string</code></td>
<td>/api/custom/providers</td>
</tr>
</tbody>
</table>
</blockquote>
<h5 tabindex="-1" dir="auto">Response</h5>

<p dir="auto">Provider</p>
<blockquote>
<table>
<thead>
<tr>
<th>Field</th>
<th>type</th>
<th>example</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unique identifier associated with the event.</td>
</tr>
<tr>
<td>created_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for creation time.</td>
</tr>
<tr>
<td>updated_at</td>
<td><code>int64</code></td>
<td><code>1699933571</code></td>
<td>Unix timestamp for update time.</td>
</tr>
<tr>
<td>provider</td>
<td><code>string</code></td>
<td><code>bricks</code></td>
<td>Unique identifier associated with the route config.</td>
</tr>
<tr>
<td>route_configs</td>
<td><code>[]RouteConfig</code></td>
<td><code>{{ "path": "/chat/completions", "target_url": "https://api.openai.com/v1/chat/completions" }}</code></td>
<td>Start timestamp for the requested timeseries data.</td>
</tr>
<tr>
<td>authentication_param</td>
<td><code>string</code></td>
<td><code>apikey</code></td>
<td>The authentication parameter required for.</td>
</tr>
</tbody>
</table>
</blockquote>
</details>
<h2 tabindex="-1" dir="auto">OpenAI Proxy</h2>
<p dir="auto">The OpenAI proxy runs on Port <code>8002</code>.</p>
<h5 tabindex="-1" dir="auto">Headers</h5>
<blockquote>
<table>
<thead>
<tr>
<th>name</th>
<th>type</th>
<th>data type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x-custom-event-id</code></td>
<td>optional</td>
<td><code>string</code></td>
<td>Custom Id that can be used to retrieve an event associated with each proxy request.</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 tabindex="-1" dir="auto">Chat Completion</h3>
<details>
  <summary>Call OpenAI chat completions: <code>POST</code> <code><b>/api/providers/openai/v1/chat/completions</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for proxying OpenAI chat completion requests. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/chat" rel="nofollow">here</a>.</p>
</details>
<h3 tabindex="-1" dir="auto">Embeddings</h3>
<details>
  <summary>Call OpenAI embeddings: <code>POST</code> <code><b>/api/providers/openai/v1/embeddings</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for proxying OpenAI embedding requests. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/embeddings/create" rel="nofollow">here</a>.</p>
</details>
<h3 tabindex="-1" dir="auto">Moderations</h3>
<details>
  <summary>Call OpenAI moderations: <code>POST</code> <code><b>/api/providers/openai/v1/moderations</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for proxying OpenAI moderation requests. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/moderations/create" rel="nofollow">here</a>.</p>
</details>
<h3 tabindex="-1" dir="auto">Models</h3>
<details>
  <summary>Get OpenAI models: <code>GET</code> <code><b>/api/providers/openai/v1/models</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving OpenAI models. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/models/list" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve an OpenAI model: <code>GET</code> <code><b>/api/providers/openai/v1/models/:model</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI model. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/models/retrieve" rel="nofollow">here</a>.</p>
</details>
<h3 tabindex="-1" dir="auto">Files</h3>
<details>
  <summary>List files: <code>GET</code> <code><b>/api/providers/openai/v1/files</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for list OpenAI files. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/files/list" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Upload a file: <code>POST</code> <code><b>/api/providers/openai/v1/files</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/files/create" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Delete a file: <code>POST</code> <code><b>/api/providers/openai/v1/files/:file_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/files/delete" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve a file: <code>GET</code> <code><b>/api/providers/openai/v1/files/:file_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/files/retrieve" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve file content: <code>GET</code> <code><b>/api/providers/openai/v1/files/:file_id/content</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI file content. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/files/retrieve-contents" rel="nofollow">here</a>.</p>
</details>
<h3 tabindex="-1" dir="auto">Assistants</h3>
<details>
  <summary>Create assistant: <code>POST</code> <code><b>/api/providers/openai/v1/assistants</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI assistant. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/createAssistant" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve assistant: <code>GET</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI assistant. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/getAssistant" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Modify assistant: <code>POST</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for modifying an OpenAI assistant. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/modifyAssistant" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Delete assistant: <code>DELETE</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for deleting an OpenAI assistant. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/deleteAssistant" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List assistants: <code>GET</code> <code><b>/api/providers/openai/v1/assistants</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for listing OpenAI assistants. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/listAssistants" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Create assistant file: <code>POST</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id/files</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI assistant file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/createAssistantFile" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve assistant file: <code>GET</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id/files/:file_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI assistant file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/getAssistantFile" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Delete assistant file: <code>DELETE</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id/files/:file_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for deleting an OpenAI assistant file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/deleteAssistantFile" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List assistant files: <code>GET</code> <code><b>/api/providers/openai/v1/assistants/:assistant_id/files</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving OpenAI assistant files. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/assistants/listAssistantFiles" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Create thread: <code>POST</code> <code><b>/api/providers/openai/v1/threads</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI thread. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/threads/createThread" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve thread: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI thread. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/threads/getThread" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Modify thread: <code>POST</code> <code><b>/api/providers/openai/v1/threads/:thread_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for modifying an OpenAI thread. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/threads/modifyThread" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Delete thread: <code>DELETE</code> <code><b>/api/providers/openai/v1/threads/:thread_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for deleting an OpenAI thread. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/threads/deleteThread" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Create message: <code>POST</code> <code><b>/api/providers/openai/v1/threads/:thread_id/messages</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI message. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/createMessage" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve message: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/messages/:message_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI message. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/getMessage" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Modify message: <code>POST</code> <code><b>/api/providers/openai/v1/files/:file_id/content</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for modifying an OpenAI message. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/modifyMessage" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List messages: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/messages</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for listing OpenAI messages. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/listMessages" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve message file: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/messages/:message_id/files/:file_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI message file. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/getMessageFile" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List message files: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/messages/:message_id/files</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving OpenAI message files. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/messages/listMessageFiles" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Create run: <code>POST</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/createRun" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve run: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs/:run_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/getRun" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Modify run: <code>POST</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs/:run_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for modifying an OpenAI run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/modifyRun" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List runs: <code>GET</code> <code><b>/api/providers/openai/v1/threads/runs</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving OpenAI runs. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/listRuns" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Submit tool outputs to run: <code>POST</code> <code><b>/api/providers/openai/v1/threads/runs</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for submitting tool outputs to an OpenAI run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/submitToolOutputs" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Cancel a run: <code>POST</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs/:run_id/cancel</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for cancellling an OpenAI run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/cancelRun" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Create thread and run: <code>POST</code> <code><b>/api/providers/openai/v1/threads/runs</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for creating an OpenAI thread and run. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/createThreadAndRun" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>Retrieve run step: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs/:run_id/steps/:step_id</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for retrieving an OpenAI run step. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/getRunStep" rel="nofollow">here</a>.</p>
</details>
<details>
  <summary>List run steps: <code>GET</code> <code><b>/api/providers/openai/v1/threads/:thread_id/runs/:run_id/steps</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for listing OpenAI run steps. Documentation for this endpoint can be found <a href="https://platform.openai.com/docs/api-reference/runs/listRunSteps" rel="nofollow">here</a>.</p>
</details>
<h2 tabindex="-1" dir="auto">Anthropic Proxy</h2>
<p dir="auto">The custom provider proxy runs on Port <code>8002</code>.</p>
<details>
  <summary>Create Anthropic completion: <code>POST</code> <code><b>/api/providers/anthropic/v1/complete</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">This endpoint is set up for proxying Anthropic completion requests. Documentation for this endpoint can be found <a href="https://docs.anthropic.com/claude/reference/complete_post" rel="nofollow">here</a>.</p>
</details>
<h2 tabindex="-1" dir="auto">Custom Provider Proxy</h2>
<p dir="auto">The custom provider proxy runs on Port <code>8002</code>.</p>
<details>
  <summary>Call custom providers: <code>POST</code> <code><b>/api/custom/providers/:provider/*</b></code></summary>
<h5 tabindex="-1" dir="auto">Description</h5>
<p dir="auto">First you need to use create custom providers endpoint to create custom providers. Then create corresponding provider setting for the newly created custom provider. Afterward, you can start creating keys associated with the custom provider, and use the keys to access this endpoint by placing the created key in <code>Authorization: Bearer YOUR_BRICKSLLM_KEY</code> as part of your HTTP request headers.</p>
</details>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GigaGPT: GPT-3 sized models in 565 lines of code (204 pts)]]></title>
            <link>https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code</link>
            <guid>38603207</guid>
            <pubDate>Mon, 11 Dec 2023 18:13:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code">https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code</a>, See on <a href="https://news.ycombinator.com/item?id=38603207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>GigaGPT is Cerebras’ implementation of Andrei Karpathy’s nanoGPT – the simplest and most compact code base to train and fine-tune GPT models. Whereas nanoGPT can train models in the 100M parameter range, gigaGPT trains models well over 100B parameters. We do this without introducing additional code or relying on third party frameworks – the entire <a href="https://github.com/Cerebras/gigaGPT" target="_blank" rel="noopener">repo</a> is just 565 lines of code. Instead gigaGPT utilizes the large memory and compute capacity of Cerebras hardware to enable large scale training on vanilla torch.nn code. With no modifications, gigaGPT supports long context lengths and works with a variety of optimizers.</p>
<h3>Why gigaGPT</h3>
<p>While the transformer architecture is simple, training a large transformer on a large number of GPUs is hard. Beyond a few billion parameters, vanilla GPT models run out of memory on even the latest GPUs. Training larger models requires breaking up models into smaller pieces, distributing them to multiple GPUs, coordinating the workload among the workers, and assembling the results. This is typically done via LLM scaling frameworks such as Megatron, DeepSpeed, NeoX, Fairscale, and Mosaic Foundry. Though powerful, these frameworks introduce significant complexity.</p>
</div><div><p>A small model such as nanoGPT requires just 639 lines of code to implement. To implement a 20B parameter model using Nvidia Megatron model requires 20,507 lines of code – a 32x increase in complexity. Even though this code doesn’t need to be written from scratch, implementing, debugging, and maintaining such a project is a major undertaking. Many ML teams struggle to get these frameworks to work and few manage to converge models with decent utilization. gigaGPT shows that on Cerebras hardware you can have the best of both worlds – a compact, hackable codebase and the ability to train GPT-3 sized models with long context.</p>
<h3>The Models</h3>
</div><div><p>gigaGPT implements the basic GPT-2 architecture in a way that matches nanoGPT. In particular, we use learned position embeddings, standard attention, and biases throughout the model. These choices were made primarily to stick closely to nanoGPT and can easily be changed. We validate gigaGPT by training four models with 111M, 13B, 70B, and 175B parameters. All the models tested use the OpenWebText dataset using the GPT-2 tokenizer with preprocessing code taken from nanoGPT. As the goal of this project was to create a clean, performant, and usable code base for others to use rather than to train state of the art models ourselves, our validation was geared towards functional correctness rather than convergence, downstream performance, or other similar metrics. To the best of our knowledge, this is the only GPT model that scales from millions to hundreds of billions of parameters without specialized parallelization techniques.</p>
<h3>gigaGPT-111M</h3>
</div><p>The 111M config was inspired by Cerebras-GPT. It uses the same model dimensions, learning rate, batch size, and training schedule and differ from Cerebras-GPT primarily in their dataset. The loss trend looks good and roughly matches what we observed for the corresponding Cerebras-GPT configuration despite the different choice of dataset. While we were writing gigaGPT, we performed thorough side-by-side numerical checks against a trusted reference code base, so seeing this loss trend isn’t surprising, but it’s always nice to get the confirmation of a converged model.</p><p>As with the 111M configuration, the 13B configuration also closely matches the model of that size from Cerebras-GPT in model dimensions, learning rate, batch size, and training schedule. Over the first hundred steps we see a few minor loss spikes, but nothing the model can’t recover from. A 13B model training run takes a substantial amount of compute so we stop after about 100 training steps. At this point we’re well past the scale that nanoGPT can accommodate, but gigaGPT handles it without a problem.</p><div><p>The 70B configuration is loosely inspired by Llama-2 70B. It takes its model dimensions and from that work and trains for the same 2T parameters as Llama-2 70B with a similar batch size. The learning rate used is slightly more conservative, as previous groups have noticed more instabilities when using learned position embeddings compared to RoPE . Since our goal wasn’t to converge the model, we didn’t perform rigorous hyperparameter selection. Even so, loss appears to be decreasingly steadily and training is fairly stable. At 70B gigaGPT continues to show great performance and scaling. Even though we grew the model size by two orders of magnitude, and made no effort to optimize for throughput, utilization remains equal or better than previous runs; we just wrote it, ran it, and immediately saw fast results. This model code is also trivial to scale out despite having been written as a single monolithic model – gigaGPT-70B works from 1 to 64 systems such as the Condor Galaxy 1 by just editing a single flag in the configuration file.</p>
<h3>gigaGPT-175B</h3>
<p>After validating the 70B model, we were curious to further probe the limits of model scale that gigaGPT could accommodate. We changed the model dimensions of the 70B config to match what was reported in the original GPT-3 paper, scaled the learning rate and initialization using common heuristics, and launched a run. The few steps we trained weren’t of much interest from a convergence perspective, but the model ran without any issues at the same utilization as the 70B model. What’s most notable here for ML practitioners is that going large does not cause Cerebras hardware to run out of memory. Based on the results we believe gigaGPT can scale to models in excess of 1 trillion parameters.</p>
<h3>How gigaGPT Works</h3>
<p>The gigaGPT model does not use any sharding or pipelining techniques because it fits entirely into the system memory of Cerebras hardware. To briefly recap: Cerebras Wafer Scale Clusters are comprised of 1 to 192 Cerebras CS-2 systems supported by accompanying CPU server nodes that store parameters (MemoryX), data and an interconnect (SwarmX). Unlike GPU based clusters, compute and memory are de-coupled. The entire model – upward of trillions of parameters – is stored in a dedicated MemoryX appliance. The model weights are streamed to the wafer one layer at a time during training. By storing models in large, unified memory systems, we obviate the need to break models apart using complex frameworks. All model training from 1 to 192 systems is done using standard mini batches aka data parallelism.</p>
<p>gigaGPT is comprised mainly of model.py and train.py. Looking more closely at the model code, we see that it looks quite similar to concise GPT implementations written for GPUs. It is built from `torch.nn` components without use of any fancy external libraries like xFormers or DeepSpeed. In fact, the model code is quite boring. Compared to nanoGPT, we rewrote the attention layer to use primitive torch ops instead of fused attention algorithms and to expose the attention mask as an argument to the attention layer to increase flexibility. Other than that, the differences compared to nanoGPT are mainly cosmetic.</p>
<p>Likewise, the main training loop is also very simple. It utilizes `cerebras_pytorch` (a custom PyTorch wrapper specialized for CS System execution) as a drop-in replacement for calls to standard torch APIs. There are only a couple of parts of this file that will look new to someone who is already familiar with PyTorch, in particular the use of a few decorators to section off different sections of functionality as well as the use of a `cerebras_pytorch.backend` scope for model creation. Overall the code is easy to understand, familiar looking to PyTorch users, and easy to modify and customize.</p>
<p>Training a large model across a huge cluster requires careful orchestration of multiple independent jobs running across heterogeneous hardware, on its face a very daunting challenge. The `cerebras_pytorch` package is the solution to this problem and is the crux of what allows the gigaGPT code to be so simple. `cerebras_pytorch` wraps some PyTorch functionality that users will already be familiar with and adds a small number of new APIs that help simplify the distributed computing needs of the problem. In this section we will walk through the code of gigaGPT’s `train.py` to better illuminate these APIs.</p>
<p>We’ll start with the end of the `main` function which contains the high-level logic for the training loop and work backwards through the definitions of each of the components it uses.</p>
<pre>for step, batch in enumerate(executor, start=global_step + 1):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if step &gt; config.num_steps:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loss = training_step(batch)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log_loss(loss, step)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; save_checkpoint(step)</pre>
<p>This is a simple starting point, so we don’t need to spend too much time here. Let’s first take a closer look at the `executor` used above.</p>
<pre>    dataloader = cstorch.utils.data.DataLoader(
        get_dataloader,
        data_path,
        config.sequence_length,
        config.batch_size,
        config.seed,
    )
    executor = cstorch.utils.data.DataExecutor(
        dataloader,
        num_steps=config.num_steps - global_step,
        checkpoint_steps=config.checkpoint_steps,
        cs_config=cs_config,
        writer=writer,
    )</pre>
<p>For Cerebras system runs, there are dedicated CPU nodes that handle loading data and feeding it into the model. `cstorch.utils.data.DataLoader` handles the job of defining the dataloader instance that will run on each of these worker nodes. It takes a function that returns a dataloader instance for ease of setting up independent properly sharded dataloaders on each worker node. This `cstorch.utils.data.DataLoader` object is then fed into a `DataExecutor`, which is responsible for the top-level coordination of all the different independent tasks required for the run.</p>
<p>Next, let’s look more closely at the components involved in defining a single training step. Internally, `cerebras_pytorch` uses PyTorch LTC to trace the compute graph associated with the training job and maps this compute graph down to operations that can be run on the Cerebras Wafer Scale Engine (WSE). Accordingly, the first step towards defining the training logic is to create the model instance in a way that enables it to be traced later. This is accomplished by the following code at the top of `train.py::main`:</p>
<pre>    backend = cstorch.backend(config.backend, use_cs_grad_accum=True)
    …

    with backend.device:
        model = GPTModel(model_config)

    compiled_model = cstorch.compile(model, backend)
</pre>
<p>With this model definition along with an optimizer and learning rate scheduler that are created with APIs that directly mirror PyTorch APIs, we are now ready to define the logic of a basic training step.</p>
<pre>    @cstorch.trace
    def training_step(batch):
        input_ids, labels = batch
        loss = compiled_model(input_ids, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(list(all_params), config.max_gradient_norm)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        return loss
</pre>
<p>The body of this function is fairly standard training code. The only interesting part is the `@cstorch.trace` decorator. This signals to the framework that the code in the function is intended to be traced and run on CS system. No tensors can be eagerly executed within this scope, which means the code in here can’t include any logging functionality or python conditionals. For that, we need a different decorator:</p>
<pre>    @cstorch.step_closure
    def log_loss(loss, step):
        rate = executor.profiler.rate()
        global_rate = executor.profiler.global_rate()

        logger.info(
            f"| Step={step}, "
            f"Loss={loss.item():.5f}, "
            f"Rate={rate:.2f} samples/sec, "
            f"GlobalRate={global_rate:.2f} samples/sec"
        )
        writer.add_scalar("loss", loss.item(), step)
        writer.add_scalar("samples_per_second", global_rate, step)
</pre>
<p>This logging code requires eagerly executing tensor values and doesn’t need to run on the WSE, so we wrap it in a `@cstorch.trace decorator`. Checkpointing code works similarly, except that we want to make sure that it only runs every `checkpoint_steps` steps for whatever value of `checkpoint_steps` we pass into the `DataExecutor` above. For this we have the `@cstorch.checkpoint_closure` decorator. Functions wrapped in this decorator can be called at any time but will only execute if the current step is a checkpoint step.</p>
<pre>@cstorch.checkpoint_closure
    def save_checkpoint(step):
        checkpoint_path = out_dir.joinpath(f"checkpoint_{step}.mdl")
        state_dict = {
            "model": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "lr_scheduler": lr_scheduler.state_dict(),
            "global_step": step,
            "model_config": asdict(model_config),
        }
        cstorch.save(state_dict, checkpoint_path)
        logger.info(f"Saved checkpoint to {checkpoint_path}")
</pre>
<p>With that, we’re done defining the functions we used in the main training loop we started with. After a couple of lines of code to fill in the gaps of checkpoint loading, config handling, etc, we end up with a `train.py` which in just 156 lines of code is able to seamlessly coordinate training jobs across huge distributed clusters.</p>
<h3>Conclusion</h3>
</div><div><p>We’d like to thank Andrei Karpathy for creating nanoGPT and inspiring this work. We believe simple, hackable, and performant code is essential for advancing machine learning research. By combining the benefits of a compact code base and the ability to train GPT-3 scale models, gigaGPT on Cerebras hardware represents a significant leap towards more accessible, scalable, and efficient AI model training. If you’re working with the Cerebras platform, we encourage you to experiment with gigaGPT and share your feedback.</p>
<p>Find at: <a href="https://github.com/Cerebras/gigaGPT" target="_blank" rel="noopener">https://github.com/Cerebras/gigaGPT</a><br>
Author: William Marshall<br>
Contributors: James Wang, Gavia Gray</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photorealistic Video Generation with Diffusion Models (135 pts)]]></title>
            <link>https://walt-video-diffusion.github.io/</link>
            <guid>38603014</guid>
            <pubDate>Mon, 11 Dec 2023 18:00:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://walt-video-diffusion.github.io/">https://walt-video-diffusion.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38603014">Hacker News</a></p>
<div id="readability-page-1" class="page">


  <div>
                    
                    <div>
                      <p><span>
                          <a target="_blank" href="http://web.stanford.edu/~agrim/">Agrim&nbsp;Gupta</a><sup>1,2,*</sup>,
                          <a target="_blank" href="https://me.lj-y.com/">Lijun&nbsp;Yu</a><sup>2</sup>,
                          <a target="_blank" href="https://sites.google.com/site/kihyuksml/">Kihyuk&nbsp;Sohn</a><sup>2</sup>,
                          <a target="_blank" href="https://laoreja.github.io/">Xiuye&nbsp;Gu</a><sup>2</sup>,
                          <a target="_blank" href="https://meerahahn.github.io/">Meera&nbsp;Hahn</a><sup>2</sup>,
                          <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li&nbsp;Fei-Fei</a><sup>1</sup>,
                          <br>
                          <a target="_blank" href="http://www.irfanessa.com/">Irfan&nbsp;Essa</a><sup>2, 3</sup>,
                          <a target="_blank" href="http://www.lujiang.info/">Lu&nbsp;Jiang</a><sup>2</sup>,
                          <a target="_blank" href="https://jlezama.github.io/">José&nbsp;Lezama</a><sup>2</sup>
                      </span>
                    </p></div>
  
                    <p>
                      <span><sup>2</sup>Google Research;&nbsp;</span>
                      <span><sup>3</sup>Georgia Institute of Teechnology </span>
                  </p>
  
                  <p><span><sup>*</sup>Work partially done during an internship at Google.</span>
                  </p>
  
  
                  
                  
  
                </div>
  


  <div>
            <p>
              <h2>Text-to-Video Examples</h2>
            </p>
          </div>
  
  




<div>
        <div>
          <p>
            <h2>Image-to-Video Examples</h2>
          </p>
        </div>
      <div>
            <p>
                Images in this section is not generated by our model.</p>
        </div>

  </div>




<div>
          <p>
            <h2>Consistent 3D Camera Motion</h2>
          </p>
        </div>




<div>
              <h2>Abstract</h2>
              <p>
                    We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 x 896 resolution at 8 frames per second.
                  </p>
          </div>
<div>
              <p><img src="https://walt-video-diffusion.github.io/assets/images/system_fig.png" alt="">
                  <br>
                  <span>
                      <b>W.A.L.T:</b> 
                      We encode images and videos into a shared latent space. The transformer backbone processes these latents
                      with blocks having two layers of window-restricted attention: spatial layers capture spatial relations in both images and video, while
                      spatiotemporal layers model temporal dynamics in videos and passthrough images via identity attention mask. Text conditioning is done via
                      spatial cross-attention.</span>
              </p>
          </div>

<div>
        <h2>Acknowledgement</h2>
          <div>
              <p><span>
                    We thank Bryan Seybold, Dan Kondratyuk, David Ross, Hartwig Adam, Huisheng Wang, Jason Baldridge, Mauricio Delbracio and Orly Liba for helpful discussions and feedback.
              </span></p>
          </div>
      </div>







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[US lawyer who put Big Tobacco on trial takes aim at sports betting (154 pts)]]></title>
            <link>https://www.ft.com/content/497fdb45-fe81-4b13-97d0-70abe667b94f</link>
            <guid>38602786</guid>
            <pubDate>Mon, 11 Dec 2023 17:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/497fdb45-fe81-4b13-97d0-70abe667b94f">https://www.ft.com/content/497fdb45-fe81-4b13-97d0-70abe667b94f</a>, See on <a href="https://news.ycombinator.com/item?id=38602786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-o-grid-colspan="12 L6" data-component="articleBanner" data-component-unique-name="default"><p>Keep abreast of significant corporate, financial and political developments around the world. Stay informed and spot emerging risks and opportunities with independent global reporting, expert commentary and analysis you can trust.</p></div>
<div data-component="unlockBanner" data-component-unique-name="default"><p><img src="https://www.ft.com/__assets/creatives/optimizely/MAR090/key_icon.svg" alt=""><span id="text-unlockBanner-default">Subscribe to unlock this article</span></p></div>
<div data-theme="" data-component="heroOffer" data-component-unique-name="CHE-Print"><div data-o-grid-colspan="12"><p><h3>Try unlimited access</h3>
<h3><strong>Only CHF 1 for 4 weeks</strong></h3></p></div><div><div data-o-grid-colspan="12 M6"><ul>
<li>Then CHF 85 per month</li>
<li>New customers only</li>
<li>Cancel anytime during your trial</li>
</ul></div><div data-o-grid-colspan="12 M6"><p><a id="charge-button-CHE-Print" data-trackable="41218b9e-c8ae-c934-43ad-71b13fcb4465" href="https://www.ft.com/buy/offer/41218b9e-c8ae-c934-43ad-71b13fcb4465/"><span><p>Keep reading for CHF 1</p></span></a></p></div></div></div>
<p data-component="subscriptionOptionsHeader" data-component-unique-name="default"><h4 id="text-subscriptionOptionsHeader-default">Explore our subscriptions</h4></p>
<div data-component="subscriptionOptions" data-component-unique-name="CHE-Print"><div><h5 id="title-CHE-Print">Individual</h5><p>Find the plan that suits you best.</p><ul><li data-text="Digital"><a id="button1-CHE-Print" data-trackable="digital" href="https://subs.ft.com/digital_edit?ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4">Digital</a></li><li data-text="Print"><a id="button2-CHE-Print" data-trackable="print" href="https://subs.ft.com/spa3_uk3m?segmentId=461cfe95-f454-6e0b-9f7b-0800950bef25&amp;utm_us=JJIBAX&amp;utm_eu=WWIBEAX&amp;utm_ca=JJIBAZ&amp;utm_as=FIBAZ&amp;ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4">Print</a></li><li data-text="Print + Digital"><a id="button3-CHE-Print" data-trackable="digital-print" href="https://subs.ft.com/bundleoptions?segmentId=de88addc-8125-43ec-21f1-152c9886e67f&amp;utm_us=JJIBAX&amp;utm_eu=WWIBEAX&amp;utm_ca=JJIBAZ&amp;utm_as=FIBAZ">Print + Digital</a></li></ul></div></div>
<div data-component="subscriptionOptions" data-component-unique-name="default"><div><h5 id="title-default">Professional</h5><p>Premium access for businesses and educational institutions.</p><ul><li data-text="Get Started"><a id="button1-default" data-trackable="professional" href="https://professional.ft.com/en-gb/services/professional-subscriptions/?barrierName=anon_barrier&amp;ft-content-uuid=7da5d7e5-2c4d-4619-9c81-294d9a634ac4&amp;segmentId=9fbe4fe1-9315-3d67-cc6d-2bc7650c4aea">Get Started</a></li><li data-text=""><a id="button2-default" data-trackable="" href=""></a></li><li data-text=""><a id="button3-default" data-trackable="" href=""></a></li></ul><p>Check if your <a href="https://www.ft.com/licence-finder?segmentId=a0e9a794-4c6d-bb35-e4dc-8bd409e0f54f" data-trackable="edu-finder">university</a> or <a href="https://enterprise.ft.com/licence-finder?segmentId=9fb23d7d-afe4-12f3-3eaa-ff7a41e9d073" data-trackable="license-finder">organisation</a> offers FT membership to read for free.</p>
</div></div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The best WebAssembly runtime may be no runtime (109 pts)]]></title>
            <link>https://00f.net/2023/12/11/webassembly-compilation-to-c/</link>
            <guid>38602750</guid>
            <pubDate>Mon, 11 Dec 2023 17:44:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://00f.net/2023/12/11/webassembly-compilation-to-c/">https://00f.net/2023/12/11/webassembly-compilation-to-c/</a>, See on <a href="https://news.ycombinator.com/item?id=38602750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrap">
<header>

  <h2><a href="https://00f.net/">Frank DENIS random thoughts.</a></h2>

</header>



<section>

<article>
 <section>
    <p>When we think “a fast AOT WebAssembly compiler and runtime”, we typically think about V8, Wasmer, WasmEdge or Wasmtime.</p>

<p>All of these have in common that they are large, complicated pieces of software, that come with a lot of overhead, and only work on a limited set of platforms.</p>

<p>But how about transpiling WebAssembly code to C source code, and leveraging the state-of-the-art optimization passes of C compilers?</p>

<p>This is the approach taken by the <code>wasm2c</code> tool from the <a href="https://github.com/WebAssembly/wabt">WABT</a> package, as well as the <a href="https://github.com/ziglang/zig/blob/master/stage1/wasm2c.c">single-file WebAssembly transpiler</a> used to <a href="https://ziglang.org/news/goodbye-cpp/">bootstrap the Zig compiler</a>.</p>

<p>The output of these tools is really a line-by-line conversion of the WebAssembly code to dumb, unoptimized C code.</p>

<p>There are instant benefits to this. First, the ouptut is kinda human readable, which is useful for debugging. A WebAssembly function shows up as a regular C function, that can be directly called from C or any language with a C FFI.</p>

<p>Take existing C code, compile it to WebAssembly, transpile it back to C, and you get the same code, but sandboxed. The transformation acts as a sanitizer that improves safety by restricting the range of virtual memory accessible to each instance.</p>

<p>Of course, that works with any WebAssembly module, no matter what original languages it was written in.</p>

<p>With this approach, assembling different WebAssembly modules also becomes very easy.</p>

<p>Startup time is negligible. There’s no overhead. No runtime either. Just WebAssembly functions directly transpiled to C functions, that are trivial to embed in any project.</p>

<p>The reponsibility to compile that source code to native code is left to a regular C compiler. If the generated C source code is portable enough, this is also a great way to compile and run WebAssembly on embedded targets and new operating systems, that require custom compilers.</p>

<p>This approach can also greatly improve security and reliability. Because they don’t perform any optimization, WebAssembly to C transpilers are extremely small and simple. And the resulting code can even be compiled with formally-verified C compilers such as <a href="https://compcert.org/">CompCert</a> for high assurance code generation.</p>

<p>But how about features, usability and performance?</p>

<h2 id="w2c2">w2c2</h2>

<p><a href="https://github.com/turbolent/w2c2">w2c2</a> is probably the most advanced of these transpilers.</p>

<p>Among other things, it supports many WebAssembly extensions, including WASI-core and threads.</p>

<p>This is a fantastic piece of software, but it unfortunately requires a bit of work to setup and use.</p>

<h2 id="installing-w2c2">Installing w2c2</h2>

<p>Clone the <code>w2c2</code> repository:</p>

<div><pre><code>git clone &lt;https://github.com/turbolent/w2c2&gt;
<span>cd </span>w2c2
</code></pre></div>

<p>Compile it:</p>

<div><pre><code><span>mkdir </span>build
<span>cd </span>build
cmake <span>-DCMAKE_BUILD_TYPE</span><span>=</span>Release ..
make
</code></pre></div>

<p>Define where the files will be installed:</p>

<div><pre><code><span>export </span><span>W2C2_DIR</span><span>=</span>/tmp/w2c2
</code></pre></div>

<p>Install them:</p>

<div><pre><code><span>install</span> <span>-d</span> <span>"</span><span>$W2C2_DIR</span><span>"</span>/<span>{</span>bin,lib,include/<span>{</span>w2c2,wasi<span>}}</span>
<span>install</span> <span>-s</span> w2c2/w2c2 <span>"</span><span>$W2C2_DIR</span><span>"</span>/bin/
<span>install </span>wasi/libw2c2wasi.a <span>"</span><span>$W2C2_DIR</span><span>"</span>/lib/
<span>install</span> <span>-m</span> 0644 ../w2c2/w2c2_base.h <span>"</span><span>$W2C2_DIR</span><span>"</span>/include/w2c2/
<span>install</span> <span>-m</span> 0644 ../wasi/wasi.h <span>"</span><span>$W2C2_DIR</span><span>"</span>/include/wasi/
</code></pre></div>

<p>Alright, let’s get back to the root directory, and add <code>w2c2</code> to the <code>PATH</code> environment variable:</p>

<div><pre><code><span>cd</span> ../..
<span>export </span><span>PATH</span><span>=</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/bin:<span>$PATH</span>
rehash <span># only needed on some shells such as zsh</span>
</code></pre></div>

<p>On macOS, the <code>w2c2</code> command weights about 150KB only. Yes, that’s all we need to compile WebAssembly modules.
For reference, the <code>wasmer</code> executable alone is 42MB large.</p>

<h2 id="creating-a-webassembly-file">Creating a WebAssembly file</h2>

<p>Next, let’s clone the Zig project boilerplate in order to get an example WebAssembly module:</p>

<div><pre><code><span>mkdir test</span> <span>&amp;&amp;</span> <span>cd test
</span>zig init
zig build <span>-Dtarget</span><span>=</span>wasm32-wasi <span>-Doptimize</span><span>=</span>ReleaseSmall <span>\</span>
  <span>-Dcpu</span><span>=</span>baseline+bulk_memory+sign_ext+nontrapping_fptoint
</code></pre></div>

<p>The resulting WebAssembly module can be found in <code>zig-out/bin/test.wasm</code>.</p>

<p>By defaut, WebAssembly modules created by Zig aim at maximizing portability, so they target the <code>baseline</code> virtual CPU, that doesn’t enable any WebAssembly extension.</p>

<p>However, bulk memory, sign extension and non-trapping floating point are not a problem for <code>w2c2</code>, so we enable them.</p>

<p>The above example doesn’t use threads, but since Zig supports WASI threads, we could add <code>+atomic</code> to the list of features in order to run multithreaded WebAssembly code.</p>

<h2 id="compiling-webassembly-to-c">Compiling WebAssembly to C</h2>

<p>Now that we have a <code>test.wasm</code> WebAssembly module, the time has come to convert it to C.</p>

<div><pre><code><span>mkdir </span>transpiled <span>&amp;&amp;</span> <span>cd </span>transpiled

w2c2 <span>-p</span> ../zig-out/bin/test.wasm test.c
</code></pre></div>

<p>The above command creates the <code>test.c</code> and <code>test.h</code> files, containing a C version of our WebAssembly module.</p>

<p>Since the module was named <code>test</code>, automatically generated functions are given <code>test</code> prefix. This allows multiple modules to be used together in the same application without name collisions.</p>

<p>Let’s try compiling this to native code, using a C compiler.</p>

<div><pre><code>zig cc test.c <span>-I</span><span>.</span> <span>-I</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/include <span>-L</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/lib <span>-lw2c2wasi</span>

error: undefined reference to symbol_main
error: undefined reference to symbol _wasiMemory
    note: referenced <span>in</span> /private/tmp/w2c2/lib/libw2c2wasi.a<span>(</span>wasi.c.o<span>)</span>
error: undefined reference to symbol_trap
    note: referenced <span>in</span> /Users/j/.cache/zig/o/5868ac77d407c9e2eae52a37fd79e706/test.o
</code></pre></div>

<p>The code contains our translated functions, but we still need to instantiate the module and call functions in order to get an application we can actually run.</p>

<p>Let’s create a <code>main.c</code> file to do so:</p>

<div><pre><code><span>#include</span> <span>&lt;stdio.h&gt;</span><span>
#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;w2c2_base.h&gt;</span><span>
#include</span> <span>&lt;wasi.h&gt;</span><span>
#include</span> <span>"test.h"</span><span>
</span>
<span>void</span> <span>trap</span><span>(</span><span>Trap</span> <span>trap</span><span>)</span>
<span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"TRAP: %s</span><span>\n</span><span>"</span><span>,</span> <span>trapDescription</span><span>(</span><span>trap</span><span>));</span>
    <span>abort</span><span>();</span>
<span>}</span>

<span>wasmMemory</span> <span>*</span><span>wasiMemory</span><span>(</span><span>void</span> <span>*</span><span>instance</span><span>)</span>
<span>{</span>
    <span>return</span> <span>test_memory</span><span>((</span><span>testInstance</span> <span>*</span><span>)</span> <span>instance</span><span>);</span>
<span>}</span>

<span>extern</span> <span>char</span> <span>**</span><span>environ</span><span>;</span>

<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>*</span><span>argv</span><span>[])</span>
<span>{</span>
    <span>testInstance</span> <span>i</span><span>;</span>

    <span>testInstantiate</span><span>(</span><span>&amp;</span><span>i</span><span>,</span> <span>NULL</span><span>);</span>
    <span>if</span> <span>(</span><span>!</span><span>wasiInit</span><span>(</span><span>argc</span><span>,</span> <span>argv</span><span>,</span> <span>environ</span><span>))</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"failed to initialize WASI</span><span>\n</span><span>"</span><span>);</span>
        <span>return</span> <span>1</span><span>;</span>
    <span>}</span>
    <span>test__start</span><span>(</span><span>&amp;</span><span>i</span><span>);</span>
    <span>testFreeInstance</span><span>(</span><span>&amp;</span><span>i</span><span>);</span>

    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>In WebAssembly commands, the entry point is named <code>_start()</code>. Functions transpiled to C are named <code>&lt;module name&gt;_&lt;function name&gt;</code>, so in order to call the <code>_start()</code> function, we simply call <code>test__start()</code> with the instance as an argument.</p>

<p>Now that we have a main function, let’s compile all this, and link the <code>w2c2</code> WASI-core implementation by the way since our test example uses it.</p>

<div><pre><code>zig cc <span>-o</span> <span>test</span> <span>-O3</span> <span>-s</span> main.c test.c <span>\</span>
  <span>-I</span><span>.</span> <span>-I</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/include/w2c2 <span>-I</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/include/wasi <span>\</span>
  <span>-L</span><span>"</span><span>$W2C2_DIR</span><span>"</span>/lib <span>-lw2c2wasi</span>
</code></pre></div>

<p>Done!</p>

<p>We can now run our application:</p>

<div><pre><code>./test

All your codebase are belong to us.
</code></pre></div>

<p>Yep, that’s it! Our WebAssembly module got converted into a small, self-contained, native executable. Executable size and memory usage are ridiculously small compared to traditional runtimes.</p>

<h2 id="how-about-speed">How about speed?</h2>

<p>Certainly, code blindly transpiled to C cannot be as efficient as a dedicated WebAssembly compiler, right?</p>

<p>Let’s compile the famous <a href="https://libsodium.org/">libsodium</a> test/benchmark suite to WebAssembly:</p>

<div><pre><code>zig build <span>-Denable_benchmarks</span> <span>-Dtarget</span><span>=</span>wasm32-wasi <span>-Doptimize</span><span>=</span>ReleaseFast <span>\</span>
  <span>-Dcpu</span><span>=</span>baseline+bulk_memory+sign_ext+nontrapping_fptoint
</code></pre></div>

<p>The resulting tests are placed into the <code>zig-out/bin</code> directory.</p>

<p>Now, let’s try a pretty heavy one, such as the Ed25519 signature test: <code>zig-out/bin/sign.wasm</code>.</p>

<p>The tests print the time they take to complete, excluding initialization and finalization.</p>

<p>First, with <code>wasmtime</code>, enabling all the supported extensions:</p>

<div><pre><code>❯ <span>time </span>wasmtime run <span>--wasm-features</span><span>=</span>all test.wasm
625928560000

wasmtime run <span>--wasm-features</span><span>=</span>all /tmp/test.wasm  124.76s user 0.02s system 99% cpu 2:05.21 total
</code></pre></div>

<p>Then, after transpiling the module to C using <code>w2c2</code> and compiling it with <code>zig cc</code>:</p>

<div><pre><code>❯ <span>time</span> ./test
361604070000

./test  73.38s user 0.01s system 99% cpu 1:13.65 total
</code></pre></div>

<p>You got that right. A naive transpilation to C almost always beats dedicated WebAssembly compilers, sometimes by a large margin. That may change over time, though.</p>

<h2 id="downsides">Downsides</h2>

<p>Of course, there are downsides. An obvious one being that this is incompatible with just-in-time compilation or singlepass compilation. Compilation is not fast, since an initial transpilation pass is required.</p>

<p>The wasm-to-C approach would thus not be a good fit for running arbitrary code in a Web browser.</p>

<p>But no matter what the runtime is, with the <code>wasm32</code> target, memory accesses are limited to 32-bit offsets.
Taking advantage of this and virtual memory, runtimes usually add guard pages around each instance’s memory in order to catch accesses outside the reserved areas.</p>

<p>But the code currently generated by <code>w2c2</code> doesn’t automatically setup these guard pages. This is still left to the application.</p>

<p>As an alternative to guard pages, for <code>wasm64</code> or on platforms without virtual memory, some traditional WebAssembly compilers can decorate memory accesses with bound-checking code. <code>w2c2</code> currently can’t, but it totally could.</p>

<p>In spite of having excellent support for WebAssembly extensions, WASI-core and WASI-threads, <code>w2c2</code> intentionally doesn’t have bells and whistles, some of them being critical for some applications. For example, traditional runtimes often support gas metering and preemption, which are mandatory for many applications.</p>

<h2 id="is-it-for-you">Is it for you?</h2>

<p>If the justification to use WebAssembly is the ability to improve code safety, then <code>w2c2</code> (and the generic wasm-to-C approach) is for you. The result will be small, efficient and easy to use in any language with a C FFI.</p>

<p>On platforms not supported by big runtimes, <code>w2c2</code> may also work for you, and the resulting native code will be faster than interpreters such as <code>wasm3</code>.</p>

<p>For all other cases, traditional runtimes still offer way more features and flexibility.</p>

 </section>
</article>

<nav>
  <a rel="home" href="https://00f.net/">Jump back to the home page</a>
</nav>



</section>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beeper Mini Is Back (807 pts)]]></title>
            <link>https://blog.beeper.com/p/beeper-mini-is-back</link>
            <guid>38602575</guid>
            <pubDate>Mon, 11 Dec 2023 17:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.beeper.com/p/beeper-mini-is-back">https://blog.beeper.com/p/beeper-mini-is-back</a>, See on <a href="https://news.ycombinator.com/item?id=38602575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png" width="1456" height="604" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:604,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0111f204-871b-40a1-bd77-30e1d6087415_2000x830.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>We've created an updated version of Beeper Mini that fixes an issue that caused messages not to be sent or received.</p><p><span>You can get the update directly from </span><a href="http://beeper.com/update" rel="">beeper.com/update</a><span> on your phone. We are still doing some final testing before submitting the update to the Google Play Store for distribution to all users. If you encounter any issues, you may need to uninstall and reinstall the app.</span></p><p>Update: Thank you Google! The update is now live on Google Play: https://play.google.com/store/apps/details?id=com.beeper.ima</p><p>We even added in a few new feature improvements: chats now open at the last unread message, and we polished the video player a bit!</p><p><strong>Four other things to note:</strong></p><ul><li><p><span>The security and privacy of Beeper Mini is unchanged. It is still local, end-to-end encrypted on your device, as we described in </span><a href="https://blog.beeper.com/p/how-beeper-mini-works" rel="">our post</a><span>.</span></p></li><li><p><span>Phone number registration is not working yet. All users must now sign in with an AppleID. Messages will be sent and received via your email address rather than phone number. </span><strong>We’re currently working on a fix for this.</strong></p></li><li><p><strong>We’ve made Beeper free to use.</strong><span> Things have been a bit chaotic, and we’re not comfortable subjecting paying users to this. As soon as things stabilize (we hope they will), we’ll look at turning on subscriptions again. If you want to keep supporting us, feel free to leave the subscription on 🙂.</span></p></li><li><p><span>Our </span><a href="https://play.google.com/store/apps/details?id=com.beeper.ima" rel="">Play Store ranking</a><span> dropped precipitously on Friday. Leaving us a nice review there would help tremendously.</span></p></li></ul><p>It’s been an extremely busy, tiring, exciting, and eventful week. Props to the entire Beeper team for working basically 24/7 to get Beeper Mini working again. Huge thanks to you, the Beeper and broader community, for supporting us.</p><p>Beeper Mini launched on Tuesday and rocketed to top 20 of Play Store charts. It was an instant hit. From what we can tell, Beeper Mini was the fastest growing paid Android application launch in history. In the first 48 hours, it was downloaded by more than 100,000 people.</p><p><span>The reason for its success is clear: </span><strong>Android and iPhone customers desperately want to be able to chat together with high quality images/video, encryption, emojis, typing status, read receipts, and all modern chat features.</strong><span> We all want a fun, easy and secure way to chat. For a glorious 3 days last week, Beeper Mini made this possible.</span></p><p>On Friday, we started getting reports that Beeper Cloud and Beeper Mini users could not send or receive messages. We investigated the issue and started working on a fix.</p><p><span>Within 24 hours, we fixed the issue for Beeper Cloud and published an </span><a href="https://twitter.com/onbeeper/status/1733537832520855925" rel="">update</a><span>. Beeper Cloud users can now send and receive messages. It’s working exactly as it did before Friday. </span></p><blockquote><p>Note: Beeper Cloud’s new Oct 2023 iMessage bridge never used Mac relay servers and still does not today. It uses a similar method to Beeper Mini, but runs on a cloud server.</p></blockquote><p><span>At the same time, we took steps to deregister all phone numbers associated with Beeper Mini, and we sent push notifications to all users updating them on the situation. In hindsight, our timing was a mistake: </span><strong>we should have communicated to our users sooner.</strong><span> We’re extremely sorry for the inconvenience caused by the outage.</span></p><p><span>Today, less than 3 days later, </span><strong>we are publishing an update to fix Beeper Mini.</strong><span> Users can now sign in, send and receive messages. Beeper Mini is back.</span></p><p>Despite reaching out, we still have not heard anything directly from Apple.</p><p><span>I got a call from David Pierce over at </span><em>The Verge</em><span> on Saturday. Apple had sent them a </span><a href="https://www.theverge.com/2023/12/9/23995150/beeper-imessage-android-apple-statement" rel="">statement</a><span>:</span></p><blockquote><p>At Apple, we build our products and services with industry-leading privacy and security technologies designed to give users control of their data and keep personal information safe. We took steps to protect our users by blocking techniques that exploit fake credentials in order to gain access to iMessage. These techniques posed significant risks to user security and privacy, including the potential for metadata exposure and enabling unwanted messages, spam, and phishing attacks. We will continue to make updates in the future to protect our users.</p></blockquote><p><span>We—of course—expected a response. What we didn’t expect was 1984-esque doublespeak. The statement is complete </span><a href="https://en.wikipedia.org/wiki/Fear,_uncertainty,_and_doubt" rel="">FUD</a><span>. </span><strong>Beeper Mini made communication between Android and iPhone users more secure. That is a fact.</strong></p><p>Make no mistake, the changes Apple made on Friday were designed to protect the lock-in effect of iMessage. The end result is that iPhone customers have less security and privacy than before.</p><p>Before Beeper Mini, Messages app (the default chat app on iPhone) forced all iPhone customers to send unencrypted, unsecure green bubble SMS messages to Android friends, family and colleagues. Even worse, when iPhone customers added an Android phone number to an existing iMessage secure encrypted group chat, the Messages app would by default switch the entire group chat to using unencrypted, unsecure SMS. This immediately made communication between iPhone customers in the group chat less secure.</p><p>Beeper Mini fixed this problem (and many others), and made it possible for Android and iPhone customers to enjoy a secure, easy and high quality chat experience. We are working to make chat more secure, and enable consumer choice. </p><p><span>Many people have asked, ‘why don’t people just use Signal or WhatsApp?’. The answer is that Messages App is the default chat app for all iPhone customers. Not only is it the </span><a href="https://en.wikipedia.org/wiki/Default_effect" rel="">default</a><span>, iOS makes it impossible to change the default chat app. In the US, where the majority of people have iPhones, this means that the easiest way to chat is by tapping on your friend’s name in your contact list and hitting the ‘message’ button.</span></p><p><span>We deeply object to the allegation that Beeper Mini ‘poses significant risks to user security and privacy’. This is completely untrue. As we explained above, the opposite is actually true. Beeper Mini increases the security and privacy of both Android and iPhone customers. To prove this, we published a detailed </span><a href="https://blog.beeper.com/p/how-beeper-mini-works" rel="">blog post</a><span> about how the app keeps data secure and private. Beeper Mini is end-to-end encrypted. The underlying connection method is </span><a href="https://github.com/JJTech0130/pypush#dataplist-and-mac-serial-numbers" rel="">open source</a><span>, for anyone to review.</span></p><p>Today, we’re taking that dedication to security and privacy even further.</p><ol><li><p>If Apple doubts the security and privacy of our app, we’re willing to share the entire Beeper Mini codebase with a mutually agreed upon 3rd party security research firm.</p></li><li><p>If Apple insists, we would consider adding a pager emoji 📟️ to metadata on all messages sent via Beeper Mini. This would make it easy for Messages App to filter out any messages from Beeper Mini users.</p></li></ol><p>At the end of the day, we are committed to building the best chat app on earth. We will continue working on that.</p><p><strong><span>Eric Migicovsky and Brad Murray</span><br></strong><span>Beeper cofounders</span><br></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who will lead the charge to a four-day work week? (116 pts)]]></title>
            <link>https://www.politico.com/newsletters/weekly-shift/2023/12/11/who-will-lead-the-charge-to-a-four-day-work-week-00131035</link>
            <guid>38602102</guid>
            <pubDate>Mon, 11 Dec 2023 16:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/newsletters/weekly-shift/2023/12/11/who-will-lead-the-charge-to-a-four-day-work-week-00131035">https://www.politico.com/newsletters/weekly-shift/2023/12/11/who-will-lead-the-charge-to-a-four-day-work-week-00131035</a>, See on <a href="https://news.ycombinator.com/item?id=38602102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p><span><b>FORAYS INTO FOUR DAYS:</b></span> When the United Auto Workers started at the bargaining table with Detroit auto companies months ago, they had some eye catching demands: Historic pay raises. Job protections. And, for a moment, a shorter work week.</p><p><b>They won </b><a href="https://www.politico.com/news/2023/10/30/uaw-big-3-labor-biden-economy-00124368#:~:text=What%20the%20pending%20UAW%2DBig,off%20President%20Joe%20Biden's%20plate." target="_blank"><b>some of those things</b></a><b>, in unprecedented fashion</b>. A 32-hour work week wasn’t one of them.</p><p>The union’s negotiations thrust the idea to the top of the media’s consciousness. But if there’s a wider movement toward shorter hours, unions won’t necessarily be the only force at the forefront of it, advocates told Shift.</p><p><b>“We should be rationally talking about policies </b>that will take us, in an orderly fashion, to a new norm,” Rep. <a href="https://directory.politicopro.com/congress/member/198829" target="_blank">Mark Takano</a> (D-Calif.) said in an interview.</p><p><b>Between the rise of generative artificial intelligence</b> this year and the reckoning of work-life balance that came with the Covid-19 pandemic, efforts to shorten the workweek have accelerated.</p><p><b>Much of the experimentation so far, though, has come from small </b>and medium businesses, looking at <a href="https://www.washingtonpost.com/wellness/2023/02/21/four-day-work-week-results-uk/" target="_blank">pilot</a> <a href="https://www.hr-brew.com/stories/2023/12/08/world-of-hr-south-africa-tries-a-four-day-workweek" target="_blank">cases</a> that indicate reworking the work day improved employee well-being while maintaining productivity. While still rare, four-day-a-week job postings are up on Indeed, <a href="https://www.hiringlab.org/2023/10/30/october-2023-us-labor-market-update/?utm_source=newsletter&amp;utm_medium=email&amp;utm_campaign=newsletter_axioscloser&amp;stream=top" target="_blank">according to the platform</a>. In Europe, Lamborghini just last week agreed with its unions to reduce production workers’ hours without reducing pay, <a href="https://www.reuters.com/business/autos-transportation/lamborghini-introduces-four-day-week-production-workers-2023-12-05/" target="_blank">Reuters reported</a>.</p><p><b>Jon Leland, chief strategy officer at Kickstarter</b> and co-founder of the Four Day Workweek Campaign, is one executive who took the plunge. And he sees AI as a potentially transformative force in the equation.</p><p><b>“Does everyone just keep churning out stuff</b> all the time and have that value accrue to shareholders?” Leland said. He would rather distribute the productive value of AI, he said, not just in the form of money, but also time.</p><p><b>The upheaval has also inspired some state and federal legislators. </b>Takano this year <a href="https://legislation.politicopro.com/bill/US_118_HR_1332" target="_blank">reintroduced</a> a bill that would amend the Fair Labor Standards Act to standardize a 32-hour workweek, after which overtime pay would kick in. Labor organizations including the AFL-CIO and SEIU backed it, along with a handful of Democrats.</p><p>A Senate version could come next year, said Jon Steinman, another co-founder of the Four Day Workweek Campaign. Sen. <a href="https://directory.politicopro.com/congress/member/51605" target="_blank">Bernie Sanders</a> (I-Vt.) has already come out publicly <a href="https://x.com/SenSanders/status/1646555149643440129?s=20" target="_blank">in favor</a> of the idea.</p><p><b>Some folks are skeptical at the idea</b> of a societal overhaul in favor of working less. Liberty Vittert, a statistician who has advocated against the four-day week, said she worries about a slippery slope that could hurt the U.S. in its competition with China. Some arrangements could also open the door for more part-time jobs as companies look to fill working hours, which don’t typically provide the same benefits as full-time work, she said.</p><p><b>Federal legislation is highly unlikely to become law </b>anytime soon, given that this split Congress has struggled with labor issues far less controversial than changing the entire structure of workers’ calendars. But true believers are inspired by the pace of the conversation between employers, workers, lawmakers and tech.</p><p><b>“My bottom line is technology for the sake of human happiness,” </b>Takano said, adding: “To make liberal democracy sustainable … we have to be sure capitalism and technology serves humanity, not the other way around.”</p><p><b>GOOD MORNING.</b> It’s Monday, Dec. 11. Welcome back to Morning Shift, your go-to tipsheet on labor and employment-related immigration. It’s been 272 days since the Senate received Julie Su’s nomination for Labor secretary. “<a href="https://www.wsj.com/lifestyle/careers/crazy-career-moves-delulu-is-the-solulu-7020b615?mod=hp_featst_pos4" target="_blank">Everyone should be delulu.</a>” Send feedback, tips and exclusives to <a href="https://www.politico.com/cdn-cgi/l/email-protection#214f4f4844455b56484045444a61514e4d485548424e0f424e4c" target="_blank"><span data-cfemail="2a4444434f4e505d434b4e4f416a5a4546435e43494504494547">[email&nbsp;protected]</span></a> and <a href="https://www.politico.com/cdn-cgi/l/email-protection#127d7d7e737c76776052627d7e7b667b717d3c717d7f" target="_blank"><span data-cfemail="a1cececdc0cfc5c4d3e1d1cecdc8d5c8c2ce8fc2cecc">[email&nbsp;protected]</span></a>. Follow us on X, formerly known as Twitter, at <a href="http://twitter.com/nickniedz" target="_blank">@NickNiedz</a> and <a href="http://twitter.com/oliviaolanderr" target="_blank">@oliviaolanderr</a>.<br></p><p><i>Want to receive this newsletter every weekday? </i><a href="https://www.politicopro.com/act-on-the-news?cid=promkt_20q1_corenews_act_energy" target="_blank"><i>Subscribe to POLITICO Pro</i></a><i>. You’ll also receive daily policy news and other intelligence you need to act on the day’s biggest stories.</i><br></p>

<p><span><b>MARKUPS SCHEDULED: </b></span>The two major bipartisan workforce development bills introduced last week will get a committee vote on Tuesday, the House Education and the Workforce Committee announced.</p><p>The bill reauthorizing the Workforce Innovation and Opportunity Act, technically called “<a href="https://legislation.politicopro.com/bill/US_118_HR_6655" target="_blank">A Stronger Workforce for America Act</a>,” and a bill that would <a href="https://subscriber.politicopro.com/article/2023/12/foxx-sets-vote-on-bipartisan-deal-to-expand-pell-grants-to-short-term-programs-00130843" target="_blank">allow Pell Grants</a> for short-term job training programs, are both scheduled for markups.</p><p>Committee members throughout the year have suggested bipartisan consensus on those two issues would be possible, though odds are more uncertain in the full House.</p><p><b>Details: </b>10:15 a.m. on Tuesday, 2172 Rayburn.<br></p>

<p><span><b>NLRB’S DATE WITH GRINDR:</b> </span>The National Labor Relations Board this week will count votes of employees at dating app Grindr, deciding whether they’ll be associated with Communications Workers of America.</p><p><b>Refresher:</b> The union has alleged a strict return-to-office policy announced in August — causing many employees to resign rather than relocate — was retaliation for their organization efforts. The company denies those claims, The New York Times <a href="https://www.nytimes.com/2023/08/12/business/grindr-rto-union.html" target="_blank">reported at the time</a>.</p><p><b>Details:</b> The mail-in election ends Wednesday, with votes set to be counted Thursday at 5 p.m.<br></p>

<p><span><b>ICYMI:</b> </span>November’s jobs report — with a relatively tame 199,000 jobs added — is good news for Federal Reserve Chair Jerome Powell as he tries to curb inflation, <a href="https://www.politico.com/news/2023/12/08/jerome-powell-labor-market-slow-down-00130797" target="_blank" data-debug-source-uuid="0000018c-49c6-d99d-a9cf-7bff5b350000" data-debug-source-site="POLITICO Pro">our Sam Sutton reports</a>.</p><p><b>Also of note: </b>The report was bolstered by a one-time shot of some 40,000 formerly striking workers returning to work, as the auto workers and actors strikes both ended last month, Sam notes. That means employers are probably adding jobs at “a relatively soft pace,” said Omair Sharif, of Inflation Insights.<br></p>

<p><span><b>HEALTH CARE BUST: </b></span>An abundance of mining jobs can counterintuitively damage access to health care, <a href="https://www.politico.com/news/2023/12/10/mining-boom-local-health-care-00128143" target="_blank">our Megan Messerly reports</a> from Carlin, Nevada.</p><p><b>In some mining communities, “health systems</b> have been disrupted and distorted by the industry’s booms and busts, creating ripple effects that make it harder for people to access care, deepening inequities and worsening health outcomes in populations often predisposed to poor physical and mental health,” Megan reports.</p><p><span><b>AMID UAW UNION DRIVE:</b></span> “<a href="https://www.bloomberg.com/news/articles/2023-12-08/tesla-fined-for-california-worker-pinned-in-car-on-conveyor-belt" target="_blank">Tesla Fined After Conveyor Belt Mishap Seriously Injures Worker</a>,” from Bloomberg.</p><p><b>More in the workplace:</b> “<a href="https://www.washingtonpost.com/business/2023/12/09/conservatives-sue-law-firms-dei/" target="_blank">Conservatives are suing law firms over diversity efforts. It’s working</a>,” from The Washington Post.</p><p><b>Even more:</b> “<a href="https://www.bloomberg.com/news/articles/2023-12-07/ai-fast-food-drive-thrus-need-human-workers-70-of-time?srnd=premium" target="_blank">AI-Powered Drive-Thru Is Actually Run Almost Entirely By Humans</a>,” from Bloomberg.<br></p>

<p><span><b>JERSEY JOBLESSNESS: </b></span>New Jersey unemployment is on the rise — more so than any other state — amid a mismatch in industries with job losses and job openings, <a href="https://www.wsj.com/economy/jobs/the-state-where-unemployment-is-rising-the-fastest-is-having-trouble-filling-jobs-f645cbf7?mod=Searchresults_pos1&amp;page=1" target="_blank">The Wall Street Journal reports</a>. And the issue could reflect what’s to come for the entire U.S.</p><p><b>“Some of the state’s white-collar employers, </b>such as in business and professional services, cut jobs over the past year through October. Others struggle to fill openings for certain types of skilled workers, particularly in healthcare,” WSJ reports.</p><p>One Ridgewood, N.J., woman who had previously worked in corporate communications told WSJ she had applied for more than a thousand roles since losing her job in fall 2022.<br></p>

<p><span><b>NO WAY OUT: </b></span>Older undocumented farm workers who have spent decades in the industry are struggling to leave it, as they aren’t eligible for Social Security or Medicare, <a href="https://www.nytimes.com/2023/12/05/us/aging-undocumented-farmworkers.html" target="_blank">The New York Times reports</a>.</p><p>“For decades, retirement was not an issue,” as workers participated in a sort of circular migration that only brought them to the U.S. for harvesting, the Times reports.</p><p>But more recently, that system “became increasingly risky and expensive, as successive U.S. presidents, beginning in the 1990s, erected barriers and deployed technology and agents along the border to curb illegal entries,” the outlet reports.</p><p><b>More immigration news:</b> “<a href="https://www.politico.com/news/2023/12/08/republicans-issue-border-policy-counter-proposal-00130853" target="_blank">Republicans make new push for Trump-era border restrictions</a>,” from our Myah Ward, Burgess Everett and Jennifer Haberkorn.<br></p>

<p>— “<a href="https://www.wsj.com/business/factory-manufacturing-jobs-tough-to-fill-workers-cd4b48da?mod=business_lead_pos3" target="_blank">The Megafactories Are Coming. Now the Hustle Is On to Find Workers</a>,” from The Wall Street Journal.</p><p>— “<a href="https://www.wsj.com/business/logistics/fedex-sends-safety-warning-to-contractors-amid-reports-of-crimes-against-drivers-9852fb5a?mod=business_lead_pos5" target="_blank">FedEx Sends Safety Warning to Contractors Amid Reports of Crimes Against Drivers</a>,” from The Wall Street Journal.</p><p>— “<a href="https://www.nytimes.com/2023/12/07/us/chipotle-sentence-fast-food-work.html" target="_blank">Woman Who Threw Food at Chipotle Employee Sentenced to Work Fast-Food Job</a>,” from The New York Times.</p><p>— “<a href="https://www.washingtonpost.com/business/2023/12/06/front-line-employees-dont-envy-remote-workers-gallup-data-shows/" target="_blank">Front-line employees don’t envy remote workers, Gallup data shows</a>,” from The Washington Post.</p><p><span><b>THAT’S YOUR SHIFT!</b></span><br></p><ul>
<h2>Follow us on Twitter</h2>
<div>
<p><img src="https://static.politico.com/hosted/icon-twitter-circle-blue%402x.png"></p><ul>
<li>Nick Niedzwiadek <a href="https://twitter.com/nickniedz" target="_blank" rel="noopener noreferrer">@nickniedz</a></li>
<li>Olivia Olander <a href="https://twitter.com/oliviaolanderr" target="_blank" rel="noopener noreferrer">@oliviaolanderr</a></li>
</ul>
</div>
</ul>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bad science and bad statistics in the courtroom convict innocent people (185 pts)]]></title>
            <link>https://www.scientificamerican.com/article/bad-science-and-bad-statistics-in-the-courtroom-convict-innocent-people/</link>
            <guid>38601960</guid>
            <pubDate>Mon, 11 Dec 2023 16:26:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/bad-science-and-bad-statistics-in-the-courtroom-convict-innocent-people/">https://www.scientificamerican.com/article/bad-science-and-bad-statistics-in-the-courtroom-convict-innocent-people/</a>, See on <a href="https://news.ycombinator.com/item?id=38601960">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block="sciam/paragraph">The city of New York recently witnessed a <a href="https://www.nytimes.com/2023/11/16/nyregion/queens-murders-exonerated-settlement.html">record payout</a> to George Bell, falsely convicted of murder in 1999, after it emerged <a href="https://www.law.umich.edu/special/exoneration/Pages/casedetail.aspx?caseid=5976">prosecutors had deliberately hidden evidence</a> casting doubt on his guilt, giving false statements in court. Bell is the latest in a long line of people, especially Black Americans, unfoundedly convicted. More recently, <a href="https://www.nytimes.com/2023/11/27/nyregion/wayne-gardine-jabar-walker-exonerations.html">Jabar Walker and Wayne Gardine</a> were cleared after decades in prison. Conviction integrity units across <a href="https://www.nytimes.com/2023/05/16/opinion/randall-dale-adams-judicial-system.html">North</a> <a href="https://www.cbc.ca/news/canada/saskatchewan/wrongful-conviction-review-process-indigenous-black-systemic-issues-1.6752736">America</a> have found serious flaws with many long-standing convictions.</p><p data-block="sciam/paragraph">Alarmingly for scientists, misleading forensic and expert evidence is too often a deciding factor in <a href="https://www.scientificamerican.com/article/let-rsquo-s-keep-the-science-in-forensic-science/">such miscarriages of justice</a>; of the <a href="https://www.law.umich.edu/special/exoneration/Documents/NRE%20Annual%20Report%202022.pdf">233 exonerations in 2022 alone</a> recorded by the National Registry of Exonerations, deceptive forensic evidence and expert testimony was a factor in 44 of them. In an era of high-tech forensics, the persistence of such brazen miscarriages of justice is more than unsettling. The National Institute of Justice, part of the U.S. Department of Justice, has just published <a href="https://www.ojp.gov/news/news-release/nij-assesses-impact-forensic-science-wrongful-convictions">a report</a> that found certain techniques, including footprint analysis and fire debris, in forensic science were <a href="https://nij.ojp.gov/topics/articles/impact-false-or-misleading-forensic-evidence-wrongful-convictions">disproportionately associated with wrongful conviction</a>. The same report found expert testimony that “reported forensic science results in an erroneous manner” or “mischaracterized statistical weight or probability” was often the driving force in false convictions. The disconcerting reality is that illusions of scientific legitimacy and flawed expert testimony are often the catalyst for deeply unsound convictions.</p><p data-block="sciam/paragraph">This paradox arises because scientific evidence is highly valued by juries, which often lack the expertise to correctly interpret or question it. Juries with a lower understanding of the potential limitations of such evidence are more likely to convict <a href="https://www.news-medical.net/news/20100329/DNA-evidence-often-overwhelms-jurors-to-convict-wrongly-says-research.aspx">without questioning the evidence</a> or its context. This is exacerbated by undue trust in expert witnesses, who may overstate evidence or underplay uncertainty. As a 2016 <a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf">presidential advisors report</a> warned, “expert witnesses have often overstated the probative value of their evidence, going far beyond what the relevant science can justify.”</p><p data-block="sciam/paragraph">The debacle of British pediatrician Roy Meadow serves as a powerful exemplar of precisely this. Famed for his influential “<a href="https://en.wikipedia.org/wiki/Meadow%27s_law">Meadow’s law</a>,” which asserted that one sudden infant death is a tragedy, two is suspicious, and three is murder until proved otherwise, Meadow was a frequent expert witness in trials in the United Kingdom. His penchant for seeing sinister patterns, however, stemmed not from real insight, but from terrible statistical ineptitude. In the late 1990s, Sally Clark suffered a <a href="https://www.theguardian.com/society/2007/mar/17/childrensservices.uknews">double tragedy</a>, losing two infant sons to sudden infant death syndrome. Despite scant evidence of anything beyond misfortune, Clark was tried for murder, with Meadows testifying to her guilt.</p><p data-block="sciam/paragraph">In court, Meadow testified that families like the Clarks had a one-in-8,543 chance of a sudden infant death syndrome (SIDS) case. Thus, he asserted, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1179752/">the probability of two cases in one family was this squared, roughly one-in-73 million</a> of two deaths arising by chance alone. In a rhetorical flourish, he likened it to successfully backing an 80-to-1 outsider to win the Grand National horse race over four successive years. This seemingly unimpeachable, damning statistic figure convinced both jury and public of her guilt. Clark was demonized in the press and imprisoned for murder.</p><p data-block="sciam/paragraph">Yet this verdict horrified statisticians, for several reasons. To arrive at his figure, Meadow simply multiplied probabilities together. This is perfectly correct for truly independent events like roulette wheels or coin-flips, but fails horribly when this assumption is not met. By the late 1990s, there was <a href="https://www.atsjournals.org/doi/full/10.1164/ajrccm.164.3.9910045">overwhelming</a> <a href="https://publications.aap.org/pediatrics/article-abstract/104/4/e43/62359/Epidemiology-of-SIDS-and-Explained-Sudden-Infant">epidemiological</a> evidence that SIDS ran in families, rendering assumptions of independence untenable. More subtle but as damaging was a trick of perception. To many, this appeared equivalent to a one-in-73-million chance Clark was innocent. While this implication was &nbsp;intended by the prosecution, such an inference was a statistical error so ubiquitous in courtrooms it has a fitting moniker: <a href="https://academic.oup.com/aje/article/179/9/1125/103523">the prosecutor’s fallacy.</a></p><p data-block="sciam/paragraph">This variant of the <a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">base-rate fallacy</a> arises because while multiple cases of SIDS are rare, so too are multiple maternal infanticides. To determine which situation is more likely, the relative likelihood of these two competing explanations must be compared. In Clark’s case, this analysis would have shown that the probability of two SIDS deaths vastly exceeded the infant murder hypothesis. The Royal Statistical Society issued a <a href="https://web.archive.org/web/20080407224224/http:/www.rss.org.uk/PDF/RSS%20Statement%20regarding%20statistical%20issues%20in%20the%20Sally%20Clark%20case,%20October%2023rd%202001.pdf">damning indictment</a> of Meadow’s testimony, echoed by a paper in the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1117305/"><em>British Medical Journal</em></a>. But such rebukes did not save Clark from years in jail.</p><p data-block="sciam/paragraph">After a long campaign, Clark’s verdict was overturned in 2003, and <a href="https://www.theguardian.com/society/2005/jul/15/NHS.uknews1">several other women</a> convicted by Meadow’s testimony were subsequently exonerated. The General Medical Council found Meadow guilty of <a href="http://news.bbc.co.uk/2/hi/health/4685511.stm">professional misconduct</a> &nbsp;and barred him from practicing medicine. But Clark’s vindication was no consolation for the heartbreak she had suffered, and she died an <a href="http://news.bbc.co.uk/1/hi/england/essex/7082411.stm">alcohol-related death in 2007</a>. The prosecutor’s fallacy emerges constantly in problems of conditional probability, leading us sirenlike towards precisely the wrong conclusions—and undetected, sends innocent people to jail.&nbsp;</p><p data-block="sciam/paragraph">Earlier this year, Australia pardoned <a href="https://www.theguardian.com/australia-news/2023/jun/05/kathleen-folbigg-pardoned-after-20-years-in-jail-over-deaths-of-her-four-children">Kathleen Folbigg</a> after 20 years in jail after a conviction for murdering her four children in 2003 <a href="https://www.theguardian.com/australia-news/2023/may/07/kathleen-folbigg-science-sheds-new-light-on-case-of-mother-convicted-of-murdering-her-children">based on Meadow’s</a> discredited law. Dutch nurse <a href="https://en.wikipedia.org/wiki/Lucia_de_Berk_case">Lucia de Berk</a> was convicted of seven murders of patients in 2004, based on ostensible statistical evidence. While convincing to a jury, it also <a href="https://www.tandfonline.com/doi/abs/10.1080/09332480.2018.1549809">appalled statistical experts</a>, who lobbied for a reopening of the case. Again, the case against de Berk pivoted entirely on the prosecutor’s fallacy, and her conviction was overturned in 2010.</p><p data-block="sciam/paragraph">This isn’t just historical occurrence. The veneer of science and expert opinion has such an aura of authority that when invoked in open court, it is rarely challenged. Even effective techniques like blood splatter and DNA analysis can be misused in unsound convictions, underpinned by variants of the prosecutor’s fallacy. A suspect’s rare blood type (5 percent) matching traces at a scene, for example, does not imply that guilt is 95 percent certain. A hypothetical town of 2,000 potential suspects has 100 people matching that criterion, which renders the probability that the suspect is guilty in the absence of other evidence at just 1 percent.</p><p data-block="sciam/paragraph">Worse is when the science cited is so dubious as to be useless. One recent analysis found <a href="https://www.psychologicalscience.org/publications/psychological-assessment-in-legal-contexts-are-courts-keeping-junk-science-out-of-the-courtroom.html">only about 40 percent of psychological measures</a> cited in courts have strong evidentiary background, and yet they are rarely challenged. Entire techniques like <a href="https://www.nist.gov/news-events/news/2022/10/forensic-bitemark-analysis-not-supported-sufficient-data-nist-draft-review">bite-mark analysis</a> have been shown to be effectively useless despite convictions still turning on them. <a href="https://www.apa.org/topics/cognitive-neuroscience/polygraph">Polygraph tests</a> are so utterly inaccurate as to be deemed inadmissible by courts, and yet remain perversely popular with swathes of American law enforcement.</p><p data-block="sciam/paragraph">This can and does ruin lives. <a href="https://www.smithsonianmag.com/smart-news/FBI-Admits-Pseudoscientific-Hair-Analysis-Used-in-Hundreds-of-Cases-180955070/">Hair analysis</a>, dismissed by forensics experts worldwide as pseudoscientific, was embraced by the FBI for its ability to get convictions. But this hollow theater of science condemned innocent people, disproportionately affecting people of color like <a href="https://www.theguardian.com/us-news/2015/jun/23/fbi-evidence-single-hair-kirk-odom">Kirk Odom</a>, who languished in prison for 22 years for a rape he did not commit. Odom was but one victim of this illusory science; a 2015 report found hundreds of cases in which <a href="https://www.fbi.gov/news/press-releases/fbi-testimony-on-microscopic-hair-analysis-contained-errors-in-at-least-90-percent-of-cases-in-ongoing-review">hair examiners made erroneous statements</a> in inculpating defendants, including 33 cases that sent defendants to death row, nine of whom were already executed by the time the report saw daylight. <a href="https://www.propublica.org/article/lung-float-test-stillbirths-boston-university-northeastern-law?taid=656841097bdf63000121093e">As noted by ProPublica</a>, the use of “<a href="https://en.wikipedia.org/wiki/Lung_float_test">lung float” tests</a> to supposedly differentiate between stillbirth and murder is being challenged by experts. Despite the fact the test is highly fallible, it has already been used to justify imprisoning women who lost children for murder, raising alarm over yet another potential manifestation of the prosecutor’s fallacy.</p><p data-block="sciam/paragraph">While science and statistics are crucial in the pursuit of justice, their uncertainties and weaknesses must be as clearly communicated as strengths. Evidence and statistics demand context, lest they mislead rather than enlighten. Juries and Judges need to be educated on standards of scientific and statistical evidence, and to understand what to demand of expert testimony, before courts send people to prison. Without improved scientific and statistical integrity in courtrooms, the risk of convicting innocent people can neither be circumvented nor ignored.</p><p data-block="sciam/paragraph"><em>This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of&nbsp;</em>Scientific American.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Text Editor Data Structures: Rethinking Undo (129 pts)]]></title>
            <link>https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/rethinking-undo/</link>
            <guid>38601435</guid>
            <pubDate>Mon, 11 Dec 2023 15:27:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/rethinking-undo/">https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/rethinking-undo/</a>, See on <a href="https://news.ycombinator.com/item?id=38601435">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text"> <p>Undo and redo have been a staple operation of text editors probably since the first typo was ever made, yet there has not been a lot of innovation around refining the idea of what undo and redo <em>could</em> be. Let’s explore what I mean…</p>
<h2 id="on-the-subject-of-undo">On The Subject of Undo</h2>
<h3 id="undo">Undo</h3>
<p>In order to understand how to change undo we first need to understand what the fundamental operation of ‘undo’ is supposed to do. I have always learned best through example so let’s assume you start with the string <code>"Hello"</code> and you start typing the sequence <code>" World!"</code>:</p>
<p><span>Hello<span id="undo-basic"></span></span></p>
<p>In the scenario above, when the undo operation is invoked (typically CTRL+z) what is the typical expectation? There are a myriad of possibilities:</p>
<ol>
<li>Undo would delete the entire string <code>" World!"</code> since it was all part of the same insert operation.</li>
<li>Undo would delete only the last character added, <code>'!'</code>
</li>
<li>Undo would delete the sequence of characters within some time threshold of each keystroke.</li>
</ol>
<p>There are many approaches to what the undo operation should do and each one has its own merits. The approach I use is close to #1 where the undo operation will undo blocks of the same style of edit. I find this approach to be useful for my workflow. If you are interested in reading a bit more about how popular text editors more concretely implement undo/redo I encourage you to check out <a href="https://www.mattduck.com/undo-redo-text-editors">Undo/redo implementations in text editors</a> by Matt Duck who very carefully breaks down the approaches of several editors.</p>
<p>There are a few properties of undo that are very nice to have:</p>
<ol>
<li>The undo operation should create a corresponding redo operation, more on redo later.</li>
<li>If undo is applied to the beginning of edit history, the system should report that an undo is not possible and do nothing as a result.</li>
<li>If you undo (or redo) to the point of the last save of the document the system should recognize that there is nothing to save.</li>
</ol>
<p>Having #3 is very important to me because it helps me understand where my last ‘committed’ work was during an edit session. It seems like a small “nice to have feature” but the system recognizing various points in history is essential for the feature we’re about to implement.</p>
<h3 id="redo">Redo</h3>
<p>Redo is really quite similar to undo in many ways. Let’s take our example of <code>"Hello World!"</code>, and apply an undo operation (using the preferred implementation above):</p>
<p><span>Hello<span id="redo-basic"></span></span></p>
<p>Redo is the complement to undo and it should perform the same operation that the undo operation just removed. Since there’s a dependency on undo in order to redo it implies that the redo operation cannot be applied on a document which has had no undo operations performed. Furthermore, a redo operation should inherit all the same properties that undo has, e.g. the system should be aware of redo operations as a moment in time rather than a regular operation.</p>
<h3 id="perils-of-undo-and-redo">Perils of Undo and Redo</h3>
<p>Undo and redo are fantastic operations to have in our editor toolbox, in fact in the way I edit documents today I would argue that undo and redo are a fundamental operation. There are some drawbacks in the way that undo and redo tend to work in most text editors though. To help illustrate what I mean, let’s go back to our <code>"Hello World!"</code> example one more time:</p>
<p><span>Hello<span id="undo-history-break"></span></span></p>
<p>If we apply undo to remove <code>" World!"</code> and start typing again, most editors will remove the redo state. There are some interesting approaches to help combat this problem, one that comes to mind is the <a href="https://www.gnu.org/software/emacs/manual/html_node/emacs/Undo.html">Emacs undo system</a> which keeps undo operations in a separate buffer after you edit again so you can undo that undo and get back to the point where you can redo after an initial undo.</p>
<p>To get a better idea of exactly what happens in most editors, let’s see a small graph of the situation above:</p>
<p> <img src="https://cdacamar.github.io/images/undo-redo-graph-initial.png"></p>
<p>In our initial state, all we’ve done is inserted the text <code>" World!"</code>. When we apply the undo operation we get:</p>
<p> <img src="https://cdacamar.github.io/images/undo-redo-graph-undo.png"></p>
<p>However, after we insert the new string <code>" Cameron!"</code> we end up orphaning the original <code>" World!"</code> node because the history in most editors is intended to be linear:</p>
<p> <img src="https://cdacamar.github.io/images/undo-redo-graph-insert.png"></p>
<p>Perhaps with <a href="https://github.com/cdacamar/fredbuf">fredbuf</a> we could do better…</p>
<h2 id="rethinking-undo">Rethinking Undo</h2>
<h3 id="basic-idea">Basic Idea</h3>
<p>Since fredbuf is a purely functional data structure, we have the ability to very cheaply store any reference to an editor buffer state. So what if we created a graph out of it? What if instead of having a linear list of undos and redos that fredbuf currently has as a builtin, perhaps we could manage a graph of editor states ourself so we could then use some kind of system of UI to navigate it and ‘snap’ back to any of these states.</p>
<p>Imagine we now store the undo/redo data in the following type of structure:</p>
<div><pre><code><span>struct</span> <span>UndoRedoNode</span><span>;</span>
<span>using</span> <span>UndoRedoNodePtr</span> <span>=</span> <span>std</span><span>::</span><span>unique_ptr</span><span>&lt;</span><span>UndoRedoNode</span><span>&gt;</span><span>;</span>
<span>using</span> <span>UndoRedoChildren</span> <span>=</span> <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>UndoRedoNodePtr</span><span>&gt;</span><span>;</span>

<span>struct</span> <span>UndoRedoNode</span> <span>{</span>
    <span>UndoRedoChildren</span> <span>children</span><span>;</span>
    <span>PieceTree</span><span>::</span><span>RedBlackTree</span> <span>point</span><span>;</span>
<span>};</span>
</code></pre></div>
<p>Using the definition above, we can create a graph of undo/redo nodes. There might be a question of, why do we need <code>children</code> to be an array? This comes back to our last example where we showed the string <code>" Cameron!"</code> inserted. If the undo operation were applied yet again and a new string entered, we would create yet another branch at the root node, so we need the capability of the structure above to refer to all possible states.</p>
<h3 id="visualization">Visualization</h3>
<p>Now that we know how we need to model the data, how do we view it? Graph visualization is a massive topic and there are many intuitive ways to view various types of graphs. The graph we are building with our undo data is closer to a tree (but we can talk about how to turn this into something like a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> in the future). While editing I have always found it useful to associate timestamps with historical data, as they help me get a better idea for roughly what that change might have been, so visualizing the time the change was made is sensible to me. Additionally, since the tree is a sequence made over time it made sense to me to lay out the tree horizontally with branches moving towards the right of the screen, somewhat like a <a href="https://git-school.github.io/visualizing-git/">git branch structure visualization</a>.</p>
<p>When looking at an undo graph in particular it is generally not sufficient for me to simply see the nodes, I also need to see <em>what</em> those nodes contain so I can make an informed decision about whether or not I actually want to apply that undo/redo operation. Enter, the diffing method. The most natural way for me to see deltas in edits is good ol’ <code>diff</code>. Let’s look at a diff of adding <code>" World!"</code> to <code>"Hello"</code>:</p>

<p>Since I work with git extensively, this is exactly the kind of thing I want to see in the UI as I navigate from the current edit to some other edit in the past. It allows me to reason about the changes the jump will cause. This means there is only one last problem to solve: implementing the diff algorithm in the editor for various types of snap points. Luckily for us, there is a fantastic series of posts by James Coglan (<a href="https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/">“The Myers diff algorithm”</a>) which goes over all the gory details of how to implement the popular Myers diff algorithm—I can’t recommend this series enough for anyone interested in implementing their own diff algorithm.</p>
<h3 id="putting-it-all-together">Putting It All Together</h3>
<p>Now that we have the data structure and the visualization strategy down, let’s see how it looks all together. Let’s go back to that example where we branched the UI states but had no way to recover the redo of <code>"Hello"</code>:</p>
<p><span>Hello<span id="undo-history-break2"></span></span></p>
<p>And here’s that same series of edits made in fred:</p>
<p> <img src="https://cdacamar.github.io/images/fred-diff-demo.gif"></p>
<p>When you enter the undo/redo graph mode the editor allows you to navigate to any edit made at any point in time. Additionally, the UI will display the diff from the current edit to the selected node in the upper left-hand corner. It sort of turns your editor into a small source control system!</p>
<h2 id="conclusion">Conclusion</h2>
<p>What did I learn from this?</p>
<ol>
<li>Immutable data structures remain a fascinating point of interest for me. They enable so many interesting avenues to solve problems.</li>
<li>The Myers diff algorithm is incredibly easy to implement, but takes a lot of time to understand what is happening so that you can render it properly.</li>
<li>It is going to be hard for me to go back to linear undo/redo of most editors <img title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">.</li>
</ol>
<p>If you’re also interested in text editor design, the C++ language, or compilers have a chat with me over on Twitter. I’m always interested in learning and sharing any knowledge that could prove to be useful to others.</p>
<p>Until next time! <i></i></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marmot: Multi-writer distributed SQLite based on NATS (105 pts)]]></title>
            <link>https://github.com/maxpert/marmot</link>
            <guid>38600743</guid>
            <pubDate>Mon, 11 Dec 2023 14:09:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/maxpert/marmot">https://github.com/maxpert/marmot</a>, See on <a href="https://news.ycombinator.com/item?id=38600743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Marmot</h2>
<p dir="auto"><a href="https://goreportcard.com/report/github.com/maxpert/marmot" rel="nofollow"><img src="https://camo.githubusercontent.com/87831ce5c8f2926e577db195d8486ab14c92427246978f910ec010a65cbc26ae/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6178706572742f6d61726d6f74" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/maxpert/marmot"></a>
<a href="https://discord.gg/AWUwY66XsE" rel="nofollow"><img src="https://camo.githubusercontent.com/e2b4960290c2bfa50e37e1d5ebe4ac0f4ba50ec59e6146707b88f185eae57cd1/68747470733a2f2f62616467656e2e6e65742f62616467652f69636f6e2f646973636f72643f69636f6e3d646973636f7264266c6162656c3d4d61726d6f74" alt="Discord" data-canonical-src="https://badgen.net/badge/icon/discord?icon=discord&amp;label=Marmot"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d833bc56dd0b73030efcf262296010168436e1d2b3542293a48b1c6250506d33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6178706572742f6d61726d6f74"><img src="https://camo.githubusercontent.com/d833bc56dd0b73030efcf262296010168436e1d2b3542293a48b1c6250506d33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6178706572742f6d61726d6f74" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/maxpert/marmot"></a></p>
<h2 tabindex="-1" dir="auto">What &amp; Why?</h2>
<p dir="auto">Marmot is a distributed SQLite replicator with leaderless, and eventual consistency. It allows you to build a robust replication
between your nodes by building on top of fault-tolerant <a href="https://nats.io/" rel="nofollow">NATS JetStream</a>.</p>
<p dir="auto">So if you are running a read heavy website based on SQLite, you should be easily able to scale it out by adding more SQLite replicated nodes.
SQLite is probably the most ubiquitous DB that exists almost everywhere, Marmot aims to make it even more ubiquitous for server
side applications by building a replication layer on top.</p>
<h2 tabindex="-1" dir="auto">Quick Start</h2>
<p dir="auto">Download <a href="https://github.com/maxpert/marmot/releases/latest">latest</a> Marmot and extract package using:</p>
<div data-snippet-clipboard-copy-content="tar vxzf marmot-v*.tar.gz"><pre><code>tar vxzf marmot-v*.tar.gz
</code></pre></div>
<p dir="auto">From extracted directory run <code>examples/run-cluster.sh</code>. Make a change in <code>/tmp/marmot-1.db</code> using:</p>
<div data-snippet-clipboard-copy-content="bash > sqlite3 /tmp/marmot-1.db
sqlite3 > INSERT INTO Books (title, author, publication_year) VALUES ('Pride and Prejudice', 'Jane Austen', 1813);"><pre><code>bash &gt; sqlite3 /tmp/marmot-1.db
sqlite3 &gt; INSERT INTO Books (title, author, publication_year) VALUES ('Pride and Prejudice', 'Jane Austen', 1813);
</code></pre></div>
<p dir="auto">Now observe changes getting propagated to other database <code>/tmp/marmot-2.db</code>:</p>
<div data-snippet-clipboard-copy-content="bash > sqlite3 /tmp/marmot-2.db
sqlite3 > SELECT * FROM Books;"><pre><code>bash &gt; sqlite3 /tmp/marmot-2.db
sqlite3 &gt; SELECT * FROM Books;
</code></pre></div>
<p dir="auto">You should be able to make changes interchangeably and see the changes getting propagated.</p>
<h2 tabindex="-1" dir="auto">Out in wild</h2>
<p dir="auto">Here are some official, and community demos/usages showing Marmot out in wild:</p>
<ul dir="auto">
<li><a href="https://www.youtube.com/watch?v=HycGtLjlikI" rel="nofollow">2-node HA for edge Kubernetes - Using Marmot</a></li>
<li><a href="https://maxpert.github.io/marmot/demo" rel="nofollow">Scaling Isso with Marmot on Fly.io</a></li>
<li><a href="https://github.com/maxpert/marmot-pocketbase-flyio">Scaling PocketBase with Marmot on Fly.io</a></li>
<li><a href="https://www.youtube.com/watch?v=QqZl61bJ9BA" rel="nofollow">Scaling PocketBase with Marmot 0.4.x</a></li>
<li><a href="https://youtu.be/GQ5x8pc9vuI" rel="nofollow">Scaling Keystone 6 with Marmot 0.4.x</a></li>
</ul>
<h2 tabindex="-1" dir="auto">What is the difference from others?</h2>
<p dir="auto">Marmot is essentially a CDC (Change Data Capture) and replication pipeline running top of NATS. It can automatically configure appropriate
JetStreams making sure those streams evenly distribute load over those shards, so scaling simply boils down to adding more nodes, and
re-balancing those JetStreams (auto rebalancing not implemented yet).</p>
<p dir="auto">There are a few solutions like <a href="https://github.com/rqlite/rqlite">rqlite</a>, <a href="https://dqlite.io/" rel="nofollow">dqlite</a>, and
<a href="https://github.com/superfly/litefs">LiteFS</a> etc. All of them either are layers on top of SQLite (e.g.
rqlite, dqlite) that requires them to sit in the middle with network layer in order to provide
replication; or intercept physical page level writes to stream them off to replicas. In both
cases they require a single primary node where all the writes have to go, and then these
changes are applied to multiple readonly replicas.</p>
<p dir="auto">Marmot on the other hand is born different. It's born to act as a side-car to your existing processes:</p>
<ul dir="auto">
<li>Instead of requiring single primary, there is <strong>no primary</strong>! Which means <strong>any node can make changes to its local DB</strong>.
Marmot will use triggers to capture your changes, and then stream them off to NATS.</li>
<li>Instead of being strongly consistent, Marmot is <strong>eventually consistent</strong>. Which means no locking, or blocking of nodes.</li>
<li>It does not require any changes to your existing SQLite application logic for reading/writing.</li>
</ul>
<p dir="auto">Making these choices has multiple benefits:</p>
<ul dir="auto">
<li>You can read, and write to your SQLite database like you normally do. No extension, or VFS changes.</li>
<li>You can write on any node! You don't have to go to single primary for writing your data.</li>
<li>As long as you start with same copy of database, all the mutations will eventually converge
(hence eventually consistent).</li>
</ul>
<h2 tabindex="-1" dir="auto">What happens when there is a race condition?</h2>
<p dir="auto">In Marmot every row is uniquely mapped to a JetStream. This guarantees that for any node to publish changes for a row it has to go through
same JetStream as everyone else. If two nodes perform a change to same row in parallel, both of the nodes will compete to publish their
change to JetStream cluster. Due to <a href="https://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering#raft" rel="nofollow">RAFT quorum</a>
constraint only one of the writer will be able to get its changes published first. Now as these changes are applied (even the publisher applies
its own changes to database) the <strong>last writer</strong> will always win. This means there is NO serializability guarantee of a transaction
spanning multiple tables. This is a design choice, in order to avoid any sort of global locking, and performance.</p>
<h2 tabindex="-1" dir="auto">Limitations</h2>
<p dir="auto">Right now there are a few limitations on current solution:</p>
<ul dir="auto">
<li>Marmot does not support schema changes propagation, so any tables you create or columns you change won't be reflected.
This feature is being <a href="https://github.com/maxpert/marmot/discussions/59" data-hovercard-type="discussion" data-hovercard-url="/maxpert/marmot/discussions/59/hovercard">debated</a> and will be available in future
versions of Marmot.</li>
<li>You can't watch tables selectively on a DB. This is due to various limitations around snapshot and restore mechanism.</li>
<li>WAL mode required - since your DB is going to be processed by multiple processes the only way to have multi-process
changes reliably is via WAL.</li>
<li>Marmot is eventually consistent - This simply means rows can get synced out of order, and <code>SERIALIZABLE</code> assumptions
on transactions might not hold true anymore. However your application can choose to redirect writes to single node
so that your changes are always replayed in order.</li>
</ul>
<h2 tabindex="-1" dir="auto">Features</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a935962de0edf3144e936c824324624f4864636703b523c8e80103bc3f3fb83a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4576656e7475616c6c79253230436f6e73697374656e742d2545322539432539342545462542382538462d677265656e"><img src="https://camo.githubusercontent.com/a935962de0edf3144e936c824324624f4864636703b523c8e80103bc3f3fb83a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4576656e7475616c6c79253230436f6e73697374656e742d2545322539432539342545462542382538462d677265656e" alt="Eventually Consistent" data-canonical-src="https://img.shields.io/badge/Eventually%20Consistent-%E2%9C%94%EF%B8%8F-green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8af699b361c3fc9fd6e9fce1b73bfc07cf72937fc2300bfa5018f23bdeeb206c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c65616465726c6573732532305265706c69636174696f6e2d2545322539432539342545462542382538462d677265656e"><img src="https://camo.githubusercontent.com/8af699b361c3fc9fd6e9fce1b73bfc07cf72937fc2300bfa5018f23bdeeb206c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c65616465726c6573732532305265706c69636174696f6e2d2545322539432539342545462542382538462d677265656e" alt="Leaderless Replication" data-canonical-src="https://img.shields.io/badge/Leaderless%20Replication-%E2%9C%94%EF%B8%8F-green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6fd11563aff681a5aec2104c198d7b60201f8417935b49d8e614e16e60231933/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4661756c74253230546f6c6572616e742d2545322539432539342545462542382538462d677265656e"><img src="https://camo.githubusercontent.com/6fd11563aff681a5aec2104c198d7b60201f8417935b49d8e614e16e60231933/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4661756c74253230546f6c6572616e742d2545322539432539342545462542382538462d677265656e" alt="Fault Tolerant" data-canonical-src="https://img.shields.io/badge/Fault%20Tolerant-%E2%9C%94%EF%B8%8F-green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/25779ee511c7ba9877c484fd6238ac4a0cc55630297e933f49e3bec28dff9aa8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c742532306f6e2532304e4154532d2545322539432539342545462542382538462d677265656e"><img src="https://camo.githubusercontent.com/25779ee511c7ba9877c484fd6238ac4a0cc55630297e933f49e3bec28dff9aa8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c742532306f6e2532304e4154532d2545322539432539342545462542382538462d677265656e" alt="Built on NATS" data-canonical-src="https://img.shields.io/badge/Built%20on%20NATS-%E2%9C%94%EF%B8%8F-green"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Leaderless replication never requiring a single node to handle all write load.</p>
</li>
<li>
<p dir="auto">Ability to snapshot and fully recover from those snapshots. Multiple storage options for snapshot:</p>

</li>
<li>
<p dir="auto">Built with NATS, abstracting stream distribution and replication.</p>
</li>
<li>
<p dir="auto">Support for log entry compression, handling content heavy CMS needs.</p>
</li>
<li>
<p dir="auto">Sleep timeout support for serverless scenarios.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Dependencies</h2>
<p dir="auto">Starting 0.8+ Marmot comes with embedded <a href="https://nats.io/download/" rel="nofollow">nats-server</a> with JetStream support. This not only reduces
the dependencies/processes that one might have to spin up, but also provides with out-of-box tooling like
<a href="https://github.com/nats-io/natscli">nat-cli</a>. You can also use existing libraries to build additional
tooling and scripts due to standard library support. Here is one example using Deno:</p>
<div data-snippet-clipboard-copy-content="deno run --allow-net https://gist.githubusercontent.com/maxpert/d50a49dfb2f307b30b7cae841c9607e1/raw/6d30803c140b0ba602545c1c0878d3394be548c3/watch-marmot-change-logs.ts -u <nats_username> -p <nats_password> -s <comma_seperated_server_list>"><pre><code>deno run --allow-net https://gist.githubusercontent.com/maxpert/d50a49dfb2f307b30b7cae841c9607e1/raw/6d30803c140b0ba602545c1c0878d3394be548c3/watch-marmot-change-logs.ts -u &lt;nats_username&gt; -p &lt;nats_password&gt; -s &lt;comma_seperated_server_list&gt;
</code></pre></div>
<p dir="auto">The output will look something like this:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/22441/196061378-21f885b3-7958-4a7e-994b-09d4e86df721.png"><img src="https://user-images.githubusercontent.com/22441/196061378-21f885b3-7958-4a7e-994b-09d4e86df721.png" alt="image"></a></p>
<h2 tabindex="-1" dir="auto">Production status</h2>
<ul dir="auto">
<li><code>v0.8.x</code> introduced support for embedded NATS. This is recommended version for production.</li>
<li><code>v0.7.x</code> moves to file based configuration rather than CLI flags, and S3 compatible snapshot storage.</li>
<li><code>v0.6.x</code> introduces snapshot save/restore. It's in pre-production state.</li>
<li><code>v0.5.x</code> introduces change log compression with zstd.</li>
<li><code>v0.4.x</code> introduces NATS based change log streaming, and continuous multi-directional sync.</li>
<li><code>v0.3.x</code> is deprecated, and unstable. DO NOT USE IT IN PRODUCTION.</li>
</ul>
<h2 tabindex="-1" dir="auto">CLI Documentation</h2>
<p dir="auto">Marmot picks simplicity, and lesser knobs to configure by choice. Here are command line options you can use to
configure marmot:</p>
<ul dir="auto">
<li><code>config</code> - Path to a TOML configuration file. Check out <code>config.toml</code> comments for detailed documentation
on various configurable options.</li>
<li><code>cleanup</code> (default: <code>false</code>) - Just cleanup and exit marmot. Useful for scenarios where you are
performing a cleanup of hooks and change logs.</li>
<li><code>save-snapshot</code> (default: <code>false</code> <code>Since 0.6.x</code>) - Just snapshot the local database, and upload snapshot
to NATS/S3 server</li>
<li><code>cluster-addr</code> (default: none <code>Since 0.8.x</code>) - Sets the binding address for cluster, when specifying
this flag at-least two nodes will be required (or <code>replication_log.replicas</code>). It's a simple
<code>&lt;bind_address&gt;:&lt;port&gt;</code> pair that can be used to bind cluster listening server.
<ul dir="auto">
<li>Since <code>v0.8.4</code> Marmot will automatically expose a leaf server on <code>&lt;bind_address&gt;:&lt;port + 1&gt;</code>. This is
intended to reduce the number for flags. So if you expose cluster on port <code>4222</code> the port <code>4223</code> will
be automatically a leaf server listener.</li>
</ul>
</li>
<li><code>cluster-peers</code> (default: none <code>Since 0.8.x</code>) - Comma separated list of <code>nats://&lt;host&gt;:&lt;port&gt;/</code> peers of
NATS cluster. You can also use (Since version <code>v0.8.4</code> ) <code>dns://&lt;dns&gt;:&lt;port&gt;/</code> to A/AAAA record lookups.
Marmot will automatically resolve the DNS IPs at boot time to expand the routes with value of
<code>nats://&lt;ip&gt;:&lt;port&gt;/</code> value, where <code>&lt;ip&gt;</code> is replaced with all the DNS entries queried. There
are two additional query parameters you can use:
<ul dir="auto">
<li><code>min</code> - forcing Marmot to wait for minimum number of entries (e.g. <code>dns://foo:4222/?min=3</code> will require
3 DNS entries to be present before embedded NATs server is started)</li>
<li><code>interval_ms</code> - delay between DNS queries, which will prevent Marmot from flooding DNS server.</li>
</ul>
</li>
<li><code>leaf-server</code> (default: none <code>Since v0.8.4</code> )- Comma separated list of <code>nats://&lt;host&gt;:&lt;port&gt;/</code>
or <code>dns://&lt;dns&gt;:&lt;port&gt;/</code> just like <code>cluster-peers</code> can be used to connect to a cluster
as a leaf node.</li>
</ul>
<p dir="auto">For more details and internal workings of marmot <a href="https://maxpert.github.io/marmot/" rel="nofollow">go to these docs</a>.</p>
<h2 tabindex="-1" dir="auto">FAQs &amp; Community</h2>
<ul dir="auto">
<li>For FAQs visit <a href="https://maxpert.github.io/marmot/intro#faq" rel="nofollow">this page</a></li>
<li>For community visit our <a href="https://discord.gg/AWUwY66XsE" rel="nofollow">discord</a> or discussions on GitHub</li>
</ul>
<h2 tabindex="-1" dir="auto">Our sponsor</h2>
<p dir="auto">Last but not least we would like to thank our sponsors who have been supporting development of this project.</p>
<p dir="auto"><a href="https://www.jetbrains.com/?utm_medium=opensource&amp;utm_source=marmot" rel="nofollow"><img src="https://camo.githubusercontent.com/c4c69e1b3d0c498dfc3319aeb429c38aa811edaa6960eab7f970cc09d654d121/68747470733a2f2f7265736f75726365732e6a6574627261696e732e636f6d2f73746f726167652f70726f64756374732f636f6d70616e792f6272616e642f6c6f676f732f476f4c616e645f69636f6e2e706e67" alt="GoLand logo." height="64" data-canonical-src="https://resources.jetbrains.com/storage/products/company/brand/logos/GoLand_icon.png">
<img src="https://camo.githubusercontent.com/316c7802db512367d7e93c9d0028e891e3fa065116f6449636537c74f34cc2c3/68747470733a2f2f7265736f75726365732e6a6574627261696e732e636f6d2f73746f726167652f70726f64756374732f636f6d70616e792f6272616e642f6c6f676f732f6a625f6265616d2e706e67" alt="JetBrains Logo (Main) logo." height="64" data-canonical-src="https://resources.jetbrains.com/storage/products/company/brand/logos/jb_beam.png"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transparent wood could soon find uses in smartphone screens, insulated windows (143 pts)]]></title>
            <link>https://arstechnica.com/science/2023/12/why-scientists-are-making-transparent-wood/</link>
            <guid>38600031</guid>
            <pubDate>Mon, 11 Dec 2023 12:31:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2023/12/why-scientists-are-making-transparent-wood/">https://arstechnica.com/science/2023/12/why-scientists-are-making-transparent-wood/</a>, See on <a href="https://news.ycombinator.com/item?id=38600031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      a potential sustainable material    —
</h4>
            
            <h2 itemprop="description">The material is being exploited for smartphone screens, insulated windows, and more. </h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/media_transparent-wood-1600x600-1-800x300.jpg" alt="a transparent piece of wood on top of a green leaf">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/media_transparent-wood-1600x600-1.jpg" data-height="600" data-width="1600">Enlarge</a> <span>/</span> See-through wood has a number of interesting properties that researchers hope to exploit.</p></figcaption>  </figure>

  




<!-- cache hit 639:single/related:43485a65e2dc3fb4194930e11ebecf1a --><!-- empty -->
<p>Thirty years ago, a botanist in Germany had a simple wish: to see the inner workings of woody plants without dissecting them. By bleaching away the pigments in plant cells, Siegfried Fink managed to <a href="https://www.degruyter.com/document/doi/10.1515/hfsg.1992.46.5.403/html" target="_blank" rel="noopener">create transparent wood</a>, and he published his technique in a niche wood technology journal. The 1992 paper remained the last word on see-through wood for more than a decade, until a researcher named Lars Berglund stumbled across it.</p>
<p>Berglund was inspired by Fink’s discovery, but not for botanical reasons. The materials scientist, who works at KTH Royal Institute of Technology in Sweden, specializes in polymer composites and was interested in creating a more robust alternative to transparent plastic. And he wasn’t the only one interested in wood’s virtues. Across the ocean, researchers at the University of Maryland were busy on a related goal: harnessing the strength of wood for nontraditional purposes.</p>
<p>Now, after years of experiments, the research of these groups is starting to bear fruit. Transparent wood could soon find uses in super-strong screens for smartphones; in soft, glowing light fixtures; and even as structural features, such as color-changing windows.</p>
<p>“I truly believe this material has a promising future,” says Qiliang Fu, a wood nanotechnologist at Nanjing Forestry University in China who worked in Berglund’s lab as a graduate student.</p>
<p>Wood is made up of countless little vertical channels, like a tight bundle of straws bound together with glue. These tube-shaped cells transport water and nutrients throughout <a href="https://knowablemagazine.org/article/living-world/2018/what-makes-tree-tree" target="_blank" rel="noopener">a tree</a>, and when the tree is harvested and the moisture evaporates, pockets of air are left behind. To create see-through wood, scientists first need to modify or get rid of the glue, called lignin, that holds the cell bundles together and provides trunks and branches with most of their earthy brown hues. After bleaching lignin’s color away or otherwise removing it, a milky-white skeleton of hollow cells remains.</p>
<p>This skeleton is still opaque, because the cell walls bend light to a different degree than the air in the cell pockets does—a value called a refractive index. Filling the air pockets with a substance like epoxy resin that bends light to a similar degree to the cell walls renders the wood transparent.</p>                                            
                                                        
<p>The material the scientists worked with is thin—typically less than a millimeter to around a centimeter thick. But the cells create a sturdy honeycomb structure, and the tiny wood fibers are stronger than the best carbon fibers, says materials scientist Liangbing Hu, who leads the research group working on transparent wood at the University of Maryland in College Park. And with the resin added, transparent wood outperforms plastic and glass: In tests measuring how easily materials fracture or break under pressure, transparent wood came out around three times stronger than transparent plastics like Plexiglass and about 10 times tougher than glass.</p>
<p>“The results are amazing, that a piece of wood can be as strong as glass,” says Hu, who highlighted the <a href="https://www.annualreviews.org/doi/10.1146/annurev-matsci-010622-105440" target="_blank" rel="noopener">features of transparent wood</a> in the 2023 Annual Review of Materials Research.</p>
<p>The process also works with thicker wood but the view through that substance is hazier because it scatters more light. In their original studies from 2016, Hu and Berglund both found that millimeter-thin sheets of the resin-filled wood skeletons let through 80 to 90 percent of light. As the thickness gets closer to a centimeter, light transmittance drops: Berglund’s group reported that 3.7-millimeter-thick wood—roughly two pennies thick—transmitted only 40 percent of light.</p>
<p>The slim profile and strength of the material means it could be a great alternative to products made from thin, easily shattered cuts of plastic or glass, such as display screens. The French company Woodoo, for example, uses a similar lignin-removing process in its wood screens, but leaves a bit of lignin to create a different color aesthetic. The company is tailoring its recyclable, touch-sensitive digital displays for products, including car dashboards and advertising billboards.</p>
<p>But most research has centered on transparent wood as an architectural feature, with windows a particularly promising use, says Prodyut Dhar, a biochemical engineer at the Indian Institute of Technology Varanasi. Transparent wood is a far better insulator than glass, so it could help buildings retain heat or keep it out. Hu and colleagues have also used polyvinyl alcohol, or PVA—a polymer used in glue and food packaging—to infiltrate the wood skeletons, making transparent wood that conducts heat at a rate <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adfm.201907511" target="_blank" rel="noopener">five times lower than that of glass,</a> the team reported in 2019 in Advanced Functional Materials.</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/science/2023/12/why-scientists-are-making-transparent-wood/2/">2</a> <a href="https://arstechnica.com/science/2023/12/why-scientists-are-making-transparent-wood/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tohands – Smart calculator for small businesses (232 pts)]]></title>
            <link>https://smart.tohands.in/</link>
            <guid>38599270</guid>
            <pubDate>Mon, 11 Dec 2023 10:27:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smart.tohands.in/">https://smart.tohands.in/</a>, See on <a href="https://news.ycombinator.com/item?id=38599270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="inner-wrap">
		<main id="main" role="main">
						<div>
				<article id="post-939" class="page">
	<div data-elementor-type="wp-page" data-elementor-id="939">
									<div data-id="6ccf21a" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
					<div data-id="798ce1f" data-element_type="column">
								<div data-id="1c9a784" data-element_type="section">
								<div data-id="0c6d278" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Keep Record Of 
Sales &amp; Expense, Easily.</h2>		</p>
				</div>
				<div data-id="bb5af4a" data-element_type="widget" data-widget_type="star-rating.default">
							<p>50+ Reviews (April 2023)</p>
						<p><i>★</i><i>★</i><i>★</i><i>★</i><i>★</i> <span itemprop="ratingValue">4.6/5</span></p>		</div>
					</div>
				<div data-id="63dc50f" data-element_type="section">
								<div data-id="de06157" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Tohands Smart Calculator is a powerful, easy to use, smart calculator that helps shopkeepers keep track of all income and expense, provides them with daily, weekly, monthly reports and analytics.</p>
				</div>
				<div data-id="8724e5f" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Due to the very high demand for the Smart calculator, we are currently unable to take orders. Please join our waitlist, and when we restock, you will be able to place your order. We sincerely request your patience and support as a small startup (Made In India<img role="img" draggable="false" src="https://s.w.org/images/core/emoji/14.0.0/svg/2764.svg" alt="❤️" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">. Thank you for your understanding. <br>(<b>Payment is not required for the waitlist)</b></p>
				</div>
					</div>
				<div data-id="240b5c2" data-element_type="section" data-widget_type="heading.default">
				<p>
			<h2><span>₹3500</span> ₹2999</h2>		</p>
				</div>
				
					</div>
				<div data-id="681994f" data-element_type="column" data-widget_type="image.default">
				<p><img width="1024" height="683" src="https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1024x683.jpg" alt="Shopkeeper with Tohands Smart calculator" srcset="https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1024x683.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-300x200.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-768x512.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1536x1024.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-2048x1365.jpg 2048w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-600x400.jpg 600w" sizes="(max-width: 1024px) 100vw, 1024px" data-srcset="https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1024x683.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-300x200.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-768x512.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1536x1024.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-2048x1365.jpg 2048w, https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-600x400.jpg 600w" data-src="https://smart.tohands.in/wp-content/uploads/2022/09/Best-photo-of-shopkeeper-with-calci-1024x683.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">															</p>
				</div>
							</div>
				<div data-id="d0d5319" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
								<div data-id="2413cdc" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>FEATURED ON</h4>		</p>
				</div>
				
					</div>
				
				<div data-id="2c0c045" data-element_type="section">
								<div data-id="5361ab0" data-element_type="section">
					<div data-id="d7da445" data-element_type="column">
								<div data-id="9cd8ca4" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Smart Calculator makes your life easy</h2>		</p>
				</div>
				<div data-id="fe047da" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Get total transparency on your income and expense, real time.&nbsp;</p>
				</div>
					</div>
				<div data-id="23bb654" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
								<div data-id="ad883ae" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Made for Shopkeepers</h3>		</p>
				</div>
				
				<div data-id="52ced66" data-element_type="widget" data-widget_type="icon-list.default">
					<ul>
							<li>
											<span>
													</span>
										<span>Bigger, richer display.</span>
									</li>
								<li>
											<span>
													</span>
										<span>Easy to use.</span>
									</li>
								<li>
											<span>
													</span>
										<span>Made In India.</span>
									</li>
						</ul>
				</div>
					</div>
							</div>
				<div data-elementor-open-lightbox="yes" data-elementor-lightbox="{&quot;type&quot;:&quot;video&quot;,&quot;videoType&quot;:&quot;youtube&quot;,&quot;url&quot;:&quot;https:\/\/www.youtube.com\/embed\/sMbQO27BK5o?feature=oembed&amp;start&amp;end&amp;wmode=opaque&amp;loop=0&amp;controls=1&amp;mute=0&amp;rel=0&amp;modestbranding=0&quot;,&quot;modalOptions&quot;:{&quot;id&quot;:&quot;elementor-lightbox-6a38ae3&quot;,&quot;entranceAnimation&quot;:&quot;&quot;,&quot;entranceAnimation_tablet&quot;:&quot;&quot;,&quot;entranceAnimation_mobile&quot;:&quot;&quot;,&quot;videoAspectRatio&quot;:&quot;169&quot;}}" e-action-hash="#elementor-action%3Aaction%3Dlightbox%26settings%3DeyJ0eXBlIjoidmlkZW8iLCJ2aWRlb1R5cGUiOiJ5b3V0dWJlIiwidXJsIjoiaHR0cHM6XC9cL3d3dy55b3V0dWJlLmNvbVwvZW1iZWRcL3NNYlFPMjdCSzVvP2ZlYXR1cmU9b2VtYmVkJnN0YXJ0JmVuZCZ3bW9kZT1vcGFxdWUmbG9vcD0wJmNvbnRyb2xzPTEmbXV0ZT0wJnJlbD0wJm1vZGVzdGJyYW5kaW5nPTAiLCJtb2RhbE9wdGlvbnMiOnsiaWQiOiJlbGVtZW50b3ItbGlnaHRib3gtNmEzOGFlMyIsImVudHJhbmNlQW5pbWF0aW9uIjoiIiwiZW50cmFuY2VBbmltYXRpb25fdGFibGV0IjoiIiwiZW50cmFuY2VBbmltYXRpb25fbW9iaWxlIjoiIiwidmlkZW9Bc3BlY3RSYXRpbyI6IjE2OSJ9fQ%3D%3D" data-id="6a38ae3" data-element_type="widget" data-settings="{&quot;youtube_url&quot;:&quot;https:\/\/youtu.be\/sMbQO27BK5o&quot;,&quot;show_image_overlay&quot;:&quot;yes&quot;,&quot;image_overlay&quot;:{&quot;url&quot;:&quot;https:\/\/smart.tohands.in\/wp-content\/uploads\/2022\/09\/untitled.367-1.jpg&quot;,&quot;id&quot;:1085,&quot;alt&quot;:&quot;&quot;,&quot;source&quot;:&quot;library&quot;},&quot;lightbox&quot;:&quot;yes&quot;,&quot;video_type&quot;:&quot;youtube&quot;,&quot;controls&quot;:&quot;yes&quot;,&quot;aspect_ratio&quot;:&quot;169&quot;}" data-widget_type="video.default">
											<p><img width="1920" height="1440" src="https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1.jpg" alt="" srcset="https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1.jpg 1920w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-300x225.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-1024x768.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-768x576.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-1536x1152.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-600x450.jpg 600w" sizes="(max-width: 1920px) 100vw, 1920px" data-srcset="https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1.jpg 1920w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-300x225.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-1024x768.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-768x576.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-1536x1152.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1-600x450.jpg 600w" data-src="https://smart.tohands.in/wp-content/uploads/2022/09/untitled.367-1.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p><p>							<span>Play Video</span>
						</p>
									</div>
					</div>
				<div data-id="1fe4052" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
								<div data-id="f2f2908" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Calculate. Record . Sync.</h2>		</p>
				</div>
				<div data-id="71b6f25" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Tohands Smart Calculator and App enables you to keep a tab on all transactions with a single click.</p>
				</div>
					</div>
				<div data-id="46f9449" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
					<div data-id="a544947" data-element_type="column">
								<div data-id="b578067" data-element_type="section">
								<div data-id="458ca2f" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>Works Offline </h4>		</p>
				</div>
				<div data-id="9868ef1" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Smart calculator has an Internal memory to record up to 40 lakh transactions inside the device. Even if the device is not connected to WiFi you can save transaction</p>
				</div>
				
					</div>
				<div data-id="ac41ebc" data-element_type="section">
								<div data-id="c6e0a38" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>Affordable </h4>		</p>
				</div>
				<div data-id="6a11140" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Normal billing machines cost ₹40,000 and they are complex to use, at just ₹2,999 Price tag smart calculator is the most affordable tool for shopkeepers.</p>
				</div>
					</div>
					</div>
				<div data-id="0775c02" data-element_type="column">
								<div data-id="b13794a" data-element_type="widget" data-widget_type="image.default">
				<p><img width="1300" height="1300" src="https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics.png" alt="" srcset="https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics.png 1300w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-300x300.png 300w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-1024x1024.png 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-150x150.png 150w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-768x768.png 768w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-600x600.png 600w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-100x100.png 100w" sizes="(max-width: 1300px) 100vw, 1300px" data-srcset="https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics.png 1300w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-300x300.png 300w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-1024x1024.png 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-150x150.png 150w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-768x768.png 768w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-600x600.png 600w, https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics-100x100.png 100w" data-src="https://smart.tohands.in/wp-content/uploads/2022/09/Middle-graphics.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">															</p>
				</div>
				<div data-id="1e62ae6" data-element_type="section" data-widget_type="heading.default" data-settings="{&quot;background_background&quot;:&quot;gradient&quot;}">
				<p>
			<h5>Easy to use. Simple to Record.</h5>		</p>
				</div>
					</div>
				<div data-id="9adcef1" data-element_type="column">
								<div data-id="9bb7fe7" data-element_type="section">
								<div data-id="316bad4" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>Save 30 Min per day </h4>		</p>
				</div>
				<div data-id="6646fe8" data-element_type="widget" data-widget_type="text-editor.default">
				<p>End of the day tally, of accounts, become smooth with the Smart calculator, you can just open the Tohands app to view all the income, expense, and total balance on the app.</p>
				</div>
				
					</div>
				<div data-id="cf3d022" data-element_type="section">
								<div data-id="3658fb2" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>1 yr Warranty</h4>		</p>
				</div>
				<div data-id="a316ddf" data-element_type="widget" data-widget_type="text-editor.default">
				<p>Unlike other calculators that break every 3 months, Tohands Smart calculator comes with a 1-year warranty, Just in case something goes wrong we will fix it as soon as possible.&nbsp;</p>
				</div>
					</div>
					</div>
							</div>
				<div data-id="438c2d77" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
								<div data-id="49c719c6" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h4>FEATURED IN</h4>		</p>
				</div>
				<div data-id="7b6c1461" data-element_type="section">
					<div data-id="4a3c806" data-element_type="column" data-widget_type="testimonial.default">
							<p>A smart calculator that spares shopkeepers the hassle of keeping track of in-store transactions, including income and expenses during a specific period, and which presents an updated statement at the click of a button has been launched by Tohands, 

a startup supported by Startup India Seed fund and T-Hub.</p>
			
						<div>
											<p><a href="https://www.thehindu.com/news/cities/Hyderabad/startup-devices-smart-calculator-for-shops/article65938992.ece"><img width="306" height="165" src="https://smart.tohands.in/wp-content/uploads/2022/10/download.png" alt="" data-src="https://smart.tohands.in/wp-content/uploads/2022/10/download.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></a>						</p>
					
										
									</div>
					</div>
				<div data-id="2ab4bfd1" data-element_type="column" data-widget_type="testimonial.default">
							<p>Over the years every electronic gadget has transformed to become smart in an attempt to reduce hassles of its users. However, calculator – one of the most used device – remained more or less in the same form for several decades.

The calculator, nevertheless, could finally get a smart makeover. Courtesy, a Hyderabad-based startup Tohands.</p>
			
						<div>
											<p><a href="https://www.deccanchronicle.com/nation/in-other-news/270922/hyderabad-based-startup-finally-turns-the-calculator-smart.html"><img width="225" height="225" src="https://smart.tohands.in/wp-content/uploads/2022/10/download-1.png" alt="" data-src="https://smart.tohands.in/wp-content/uploads/2022/10/download-1.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></a>						</p>
					
										
									</div>
					</div>
				<div data-id="2db5552" data-element_type="column" data-widget_type="testimonial.default">
							<p>This device spares the shopkeeper the hassle of jotting down all the income and expenses made during the day and presents an updated statement at the click of a button.


The calculator comes with an alpha numeric keyboard, with battery back up of three days. “It has an internal memory that can record up to five lakh transactions and saves the details,”



</p>
			
						<div>
											<p><a href="https://www.thehindubusinessline.com/info-tech/a-smart-calculator-for-shopkeepers-accounts/article65937990.ece"><img width="400" height="400" src="https://smart.tohands.in/wp-content/uploads/2022/10/thehindubusinessline-logo.png" alt="" data-src="https://smart.tohands.in/wp-content/uploads/2022/10/thehindubusinessline-logo.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></a>						</p>
					
										
									</div>
					</div>
							</div>
					</div>
				<div data-id="3902dec" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
								<div data-id="8519903" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Made In India
Smart calculator </h2>		</p>
				</div>
				<div data-id="93ebcba" data-element_type="section" data-widget_type="text-editor.default">
				<p>Tohands smart calculator is a completely Made In India device with the most advance features.</p>
				</div>
				
				<div data-id="2b02f91" data-element_type="section" data-widget_type="icon-box.default">
				<h5>
					<span>
						A/a/1/@					</span>
				</h5>
									<p>
						Alpha-numeric Keyboard					</p>
							</div>
					</div>
				<div data-id="e475d41" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
					<div data-id="223a671" data-element_type="column">
								<div data-id="34f70fe" data-element_type="section" data-widget_type="heading.default">
				<p>
			<h2>Smart Calculator for Smart Shopkeeper's</h2>		</p>
				</div>
				<div role="tablist" data-id="e00f87b" data-element_type="widget" data-widget_type="toggle.default">
							<div>
					<h5 id="elementor-tab-title-2341" data-tab="1" role="tab" aria-controls="elementor-tab-content-2341" aria-expanded="false">
												
												<a href="">What is the price of Tohands Smart Calculator?</a>
					</h5>

					<div id="elementor-tab-content-2341" data-tab="1" role="tabpanel" aria-labelledby="elementor-tab-title-2341"><p>The Tohands Smart Calculator is priced at ₹2540 plus 18% GST, which brings the total cost to ₹2999 inclusive of all taxes.&nbsp; </p><p>We strive to provide our customers with affordable and high-quality products, and we believe that the Tohands Smart Calculator is an excellent value for the price.</p></div>
				</div>
							<div>
					<h5 id="elementor-tab-title-2342" data-tab="2" role="tab" aria-controls="elementor-tab-content-2342" aria-expanded="false">
												
												<a href="">How long does it take to receive my order?</a>
					</h5>

					<p><strong>We aim to deliver your order within 14-20 days across India. However, please note that due to high demand and limited supply, there may be some delays in shipping your order. We are trying our best to ship your product as soon as possible and appreciate your patience and understanding. If you have any questions or concerns about the status of your order, please don’t hesitate to contact us.</strong></p>
				</div>
							<div>
					<h5 id="elementor-tab-title-2343" data-tab="3" role="tab" aria-controls="elementor-tab-content-2343" aria-expanded="false">
												
												<a href="">How can I contact Tohands Private Limited?</a>
					</h5>

					<div id="elementor-tab-content-2343" data-tab="3" role="tabpanel" aria-labelledby="elementor-tab-title-2343"><p>We are always available to assist you. You can reach out to us through any of the following channels:</p><ul><li>WhatsApp: You can message us on WhatsApp by clicking on the following link: <a href="https://wa.me/message/VOSKZDLEQBTEO1" target="_new">https://wa.me/message/VOSKZDLEQBTEO1</a>. This is the best option to get in touch with us, as we are available to chat with you 24×7.</li></ul><ul><li>Phone: You can call us at +91 95739 27197. Our customer support team is available 24×7 to assist you.</li><li>Email: You can email us at <a href="mailto:hello@tohands.in" target="_new">hello@tohands.in</a>. Our founder personally monitors the inbox and will reply to your email as soon as possible.</li></ul><p>We appreciate your business and are always here to help you with any questions or concerns you may have.</p></div>
				</div>
							<div>
					<h5 id="elementor-tab-title-2344" data-tab="4" role="tab" aria-controls="elementor-tab-content-2344" aria-expanded="false">
												
												<a href="">What is the warranty period for the product?</a>
					</h5>

					<p>We offer a 1-year warranty on our products.&nbsp;<br>If the device has any issues during the warranty period, please contact us and we will initiate the return process. We will take the device back, fix it, and reship it to you. If the issue is not resolvable, we will provide you with a new unit. We always strive to provide our customers with the best service possible, and we hope to resolve any issues promptly and efficiently.</p>
				</div>
							<div>
					<h5 id="elementor-tab-title-2345" data-tab="5" role="tab" aria-controls="elementor-tab-content-2345" aria-expanded="false">
												
												<a href="">How to return product if unsatisfied?</a>
					</h5>

					<p>We hope that you are fully satisfied with your purchase. However, if for any reason you are not satisfied, please message us on WhatsApp to initiate the return process. Once we receive the returned product and verify its condition, we will process the refund. The refund amount will be ₹2920 (₹70 will be deducted by the payment gateway). Please note that the return process generally takes 5-10 days. We always strive to do the right thing for our customers, and we hope to provide you with a hassle-free return process.</p>
				</div>
							<div>
					<h5 id="elementor-tab-title-2346" data-tab="6" role="tab" aria-controls="elementor-tab-content-2346" aria-expanded="false">
												
												<a href="">Why Waitlist</a>
					</h5>

					<p>Due to the very high demand for the Smart calculator, we are currently unable to take orders. Please join our waitlist, and when we restock, you will be able to place your order. We sincerely request your patience and support as a small startup. Thank you for your understanding.</p>
				</div>
								</div>
					</div>
				<div data-id="2e810ab" data-element_type="column" data-widget_type="image.default">
				<p><img width="2048" height="1365" src="https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-2048x1365.jpg" alt="" srcset="https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-2048x1365.jpg 2048w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-300x200.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-1024x683.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-768x512.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-1536x1024.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-600x400.jpg 600w" sizes="(max-width: 2048px) 100vw, 2048px" data-srcset="https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-2048x1365.jpg 2048w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-300x200.jpg 300w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-1024x683.jpg 1024w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-768x512.jpg 768w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-1536x1024.jpg 1536w, https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-600x400.jpg 600w" data-src="https://smart.tohands.in/wp-content/uploads/2022/09/MG_5664-2048x1365.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">															</p>
				</div>
							</div>
				
				<div data-id="878829c" data-element_type="section" data-widget_type="heading.default" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
				<p>
			<h4>Order Tohands Smart Calculator now.</h4>		</p>
				</div>
				
							</div>
</article><!-- #post-939 -->

			</div>
					</main><!-- #main -->
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magical Fibonacci Formulae (135 pts)]]></title>
            <link>https://orlp.net/blog/magical-fibonacci-formulae/</link>
            <guid>38599168</guid>
            <pubDate>Mon, 11 Dec 2023 10:05:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://orlp.net/blog/magical-fibonacci-formulae/">https://orlp.net/blog/magical-fibonacci-formulae/</a>, See on <a href="https://news.ycombinator.com/item?id=38599168">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<time datetime="2023-02-06">2023-02-06</time>
<p>The following Python function computes the <a href="https://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci
sequence</a>, without loops,
recursion or floating point arithmetic:</p>
<pre data-lang="python"><code data-lang="python"><span>f=</span><span>lambda </span><span>n:(b:=</span><span>2</span><span>&lt;&lt;n)**n*b//(b*b-b-</span><span>1</span><span>)%b
</span></code></pre>
<p>It really does:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; [f(n) </span><span>for </span><span>n </span><span>in </span><span>range(</span><span>10</span><span>)]
</span><span>[</span><span>0</span><span>, </span><span>1</span><span>, </span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>, </span><span>5</span><span>, </span><span>8</span><span>, </span><span>13</span><span>, </span><span>21</span><span>, </span><span>34</span><span>]
</span></code></pre>
<p>How does it work? As a teaser, look at the decimal expansions of $100 / 9899$
and $1000 / 998999$ and see if you notice anything:</p>
<p>$$\frac{100}{9899} = 0.0101020305081321345590463…$$
$$\frac{1000}{998999} = 0.001001002003005008013021034055089144233377610988…$$</p>
<h2 id="generating-functions"><a href="#generating-functions" aria-label="Anchor link for: generating-functions">Generating functions</a></h2>
<p>We define the Fibonacci sequence as 
\begin{align*}
f(0) &amp;= 0,\\
f(1) &amp;= 1,\\
f(n) &amp;= f(n - 1) + f(n - 2).
\end{align*}
However, we will also define a rather strange object known as a <a href="https://en.wikipedia.org/wiki/Generating_function">generating
function</a>. It is an
‘infinite polynomial’ (also known as a <a href="https://en.wikipedia.org/wiki/Power_series">power series</a>)
in variable $x$ whose <em>coefficients</em> correspond to our series of interest. This
gives us
$$F(x) = 0 + 1x + 1x^2 + 2x^3 + 3x^4 + 5x^5 + 8x^6 + \cdots,$$
$$F(x) = f(0)x^0 + f(1)x^1 + f(2)x^2 + \cdots = \sum_{n=0}^\infty f(n)x^n.$$</p>
<p>Does this function converge? Is this really allowed? All interesting questions
we are going to ignore here. Instead, let’s just start doing interesting things
with our new object, like taking out the first two terms from the sum:</p>

<p>$$F(x) = f(0)x^0 + f(1)x^1 + \sum_{n=2}^\infty f(n)x^n = x + \sum_{n=2}^\infty f(n)x^n.$$</p>
<p>We can also substitute $f(n) = f(n - 1) + f(n - 2)$ now, because our iteration
variable starts at $2$:</p>
<p>\begin{align*}
F(x) &amp;= x + \sum_{n=2}^\infty \Big(f(n-1) + f(n-2) \Big)x^n\\
&amp;= x + \sum_{n=2}^\infty f(n-1)x^n + \sum_{n=2}^\infty f(n-2)x^n.
\end{align*}
Now we can substitute the loop variables, re-insert the $f(0)$ term (which is just
zero) into the first sum, and factor out the extra $x$ terms:
\begin{align*}
F(x) &amp;= x + \sum_{n=1}^\infty f(n)x^{n+1} + \sum_{n=0}^\infty f(n)x^{n+2}\\
&amp;= x - f(0)x^{1} + \sum_{n=0}^\infty f(n)x^{n+1} + \sum_{n=0}^\infty f(n)x^{n+2}\\
&amp;= x + x\sum_{n=0}^\infty f(n)x^{n} + x^2\sum_{n=0}^\infty f(n)x^{n}.
\end{align*}</p>
<p>And now using the crucial observation that $F(x) = \sum_{n=0}^\infty f(n)x^{n}$:
\begin{align*}
F(x) &amp;= x + xF(x) + x^2F(x),\\
(1 - x - x^2)F(x) &amp;= x,\\
F(x) &amp;= \frac{x}{1 - x - x^2}.
\end{align*}
Wow. Somehow the simple expression $x / (1 - x - x^2)$ ‘contains’ the entire
Fibonacci sequence. If you substitute $x = 10^{-3}$ in $F(x)$ you will retrieve
our earlier value
$$\frac{1000}{998999} = 0.001001002003005008013021034055089144233377610988…$$</p>
<h2 id="an-interlude-by-binet"><a href="#an-interlude-by-binet" aria-label="Anchor link for: an-interlude-by-binet">An interlude by Binet</a></h2>
<p>Solving $1 - x - x^2 = 0$ with the <a href="https://en.wikipedia.org/wiki/Quadratic_formula">quadratic formula</a> gives us
roots $-(\sqrt{5} \pm 1)/2$, which one might recognize as
the (negative) <a href="https://en.wikipedia.org/wiki/Golden_ratio">golden ratio</a> $\phi$ and its inverse
$\phi^{-1}$:
$$\phi = \frac{\sqrt{5} + 1}{2}, \quad \phi^{-1} = \frac{2}{\sqrt{5} + 1} = \frac{2\left(\sqrt{5} - 1\right)}{\left(\sqrt{5} + 1\right)\left(\sqrt{5} - 1\right)} = \frac{\sqrt{5} - 1}{2}.$$</p>
<p>This allows us to factor and
use  <a href="https://en.wikipedia.org/wiki/Partial_fraction_decomposition">partial fraction decomposition</a>
on $F(x)$:
$$\frac{x}{1 - x - x^2} = \frac{x}{(1 - \phi x)(1 + \phi^{-1} x)} = \frac{1}{\phi + \phi^{-1}}\left(\frac{1}{1 - \phi x} - \frac{1}{1 + \phi^{-1} x}\right).$$
This is a rather arduous (but strictly elementary) algebraic process so it is much easier
to verify by expanding that the identity holds than following along.
To verify use the fact that $\phi \cdot \phi^{-1} = \phi - \phi^{-1} = 1$.</p>
<p>If we recall the formula
for the <a href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a>,</p>
<p>$$\frac{1}{1 - r} = \sum_{n=0}^\infty r^n,$$</p>
<p>and apply it to our above expression (once
again completely ignoring convergence) while
noticing that $\phi + \phi^{-1} = \sqrt{5}$ we find</p>
<p>$$F(x) = \frac{1}{\sqrt{5}} \left( \sum_{n=0}^\infty \phi^n x^n - \sum_{n=0}^\infty {(-\phi})^{-n} x^n \right),$$
$$F(x) = \sum_{n=0}^\infty \frac{1}{\sqrt{5}} \left(  \phi^n - {(-\phi})^{-n} \right) x^n.$$</p>
<p>And now for the true magic, recall that we defined $F(x) = \sum_{n=0}^\infty f(n)x^n$, and thus we conclude
$$f(n) = \frac{1}{\sqrt{5}}\left(\phi^n - {(-\phi})^{-n} \right).$$</p>

<p>We have recovered <a href="https://en.wikipedia.org/wiki/Fibonacci_number#Binet's_formula">Binet’s formula</a>
for the Fibonacci numbers, a closed form. Unfortunately evaluating it in
Python would eventually fail due to the use of floating-point numbers,
which is why this is only an interlude. But it is nevertheless cool:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; phi = (</span><span>1 </span><span>+ </span><span>5</span><span>**</span><span>0.5</span><span>) / </span><span>2
</span><span>&gt;&gt;&gt; [(phi**n - (-phi)**-n) / </span><span>5</span><span>**</span><span>0.5 </span><span>for </span><span>n </span><span>in </span><span>range(</span><span>10</span><span>)]
</span><span>[</span><span>0.0</span><span>, </span><span>1.0</span><span>, </span><span>1.0</span><span>, </span><span>2.0</span><span>, </span><span>3.0000000000000004</span><span>, </span><span>5.000000000000001</span><span>, </span><span>8.000000000000002</span><span>, </span><span>13.000000000000002</span><span>, </span><span>21.000000000000004</span><span>, </span><span>34.00000000000001</span><span>]
</span></code></pre>
<h2 id="evaluating-generating-functions"><a href="#evaluating-generating-functions" aria-label="Anchor link for: evaluating-generating-functions">Evaluating generating functions</a></h2>
<p>Take another look at $F(10^{-3})$:</p>
<p>$$\frac{1000}{998999} = 0.001001002003005008013021034055089144233377610988…$$</p>
<p>Each next integer in the series starts three places shifted back from the previous
one. This makes sense, because $F(10^{-3})$ is the sum of $f(n)10^{-3n}$
for all $n$.</p>
<p>This also means that eventually the method fails, when numbers outgrow
three digits and overflow into the previous one. If we ignore that for now,
we can study $10^{3n} F(10^{-3})$:</p>
<pre><code><span>n = 0                   0.001001002003005008013021034055...
</span><span>n = 1                   1.001002003005008013021034055089...
</span><span>n = 2                1001.002003005008013021034055089144...
</span><span>n = 3             1001002.003005008013021034055089144233...
</span><span>n = 4          1001002003.005008013021034055089144233377...
</span><span>n = 5       1001002003005.008013021034055089144233377610...
</span><span>n = 6    1001002003005008.013021034055089144233377610988...
</span><span>                      ^^^
</span></code></pre>
<p>If we could extract just the highlighted column we have our Fibonacci numbers.
Luckily, we can. By flooring we can remove the entire fractional part, and with
modulo $10^3$ we can ignore everything after the third digit.</p>
<p>We still have the overflowing issue however. But this is easily fixed by
choosing a number $k$ instead of $3$, large enough such that the next Fibonacci
number doesn’t overflow into our number of interest. For example the choice $k
= n$ works, as the $n$th Fibonacci number certainly won’t overflow $n$ decimal
digits, giving formula</p>
<p>$$f(n) = \lfloor 10^{n^2} \cdot F(10^{-n}) \rfloor \bmod 10^{n}.$$</p>
<p>We can generalize much more however. Our choice of $10^3$ and $10^n$ as bases
was rather arbitrary. You can use any base $b$, as long as $b$ is large enough. This gives:</p>

<p>$$f(n) = \left\lfloor b^n \cdot F(b^{-1}) \right\rfloor \bmod b,$$
$$f(n) = \left\lfloor b^n \cdot \frac{b^{-1}}{1 - b^{-1} - b^{-2}} \right\rfloor \bmod b,$$
$$f(n) = \left\lfloor \frac{b^{n+1}}{b^2 - b - 1} \right\rfloor \bmod b.$$</p>
<p>This is actually a closed form we can evaluate without the use of floating
point arithmetic, as it simply consists of the division of two integers.
I have experimentally found that choosing $b = 3^n$ suffices to not overflow
for computing $f(n)$, giving our magical formula:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; f = </span><span>lambda </span><span>n: </span><span>3</span><span>**(n*(n+</span><span>1</span><span>)) // (</span><span>3</span><span>**(</span><span>2</span><span>*n) - </span><span>3</span><span>**n - </span><span>1</span><span>) % </span><span>3</span><span>**n
</span><span>&gt;&gt;&gt; [f(n) </span><span>for </span><span>n </span><span>in </span><span>range(</span><span>10</span><span>)]
</span><span>[</span><span>0</span><span>, </span><span>1</span><span>, </span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>, </span><span>5</span><span>, </span><span>8</span><span>, </span><span>13</span><span>, </span><span>21</span><span>, </span><span>34</span><span>]
</span></code></pre>

<p>We can <a href="https://en.wikipedia.org/wiki/Code_golf">golf</a> this quite a bit by using Python’s <a href="https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions">“walrus operator”</a>
(also called <em>assignment expression</em> by boring people) introduced in Python 3.8. If you write <code>(foo := bar)</code>
in an expression the parentheses take on value <code>bar</code> as well as storing <code>bar</code>
in a new variable <code>foo</code> available in the rest of the expression. Finally as a
bit of flair and efficiency, ${b = 2^{n+1}}$ also works, which can be computed as
<code>2 &lt;&lt; n</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; f=</span><span>lambda </span><span>n:(b:=</span><span>2</span><span>&lt;&lt;n)**n*b//(b*b-b-</span><span>1</span><span>)%b
</span><span>&gt;&gt;&gt; [f(n) </span><span>for </span><span>n </span><span>in </span><span>range(</span><span>10</span><span>)]
</span><span>[</span><span>0</span><span>, </span><span>1</span><span>, </span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>, </span><span>5</span><span>, </span><span>8</span><span>, </span><span>13</span><span>, </span><span>21</span><span>, </span><span>34</span><span>]
</span></code></pre>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a bare-metal bootable game for Raspberry Pi in C# (178 pts)]]></title>
            <link>https://migeel.sk/blog/2023/12/08/building-bare-metal-bootable-game-for-raspberry-pi-in-csharp/</link>
            <guid>38599027</guid>
            <pubDate>Mon, 11 Dec 2023 09:35:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://migeel.sk/blog/2023/12/08/building-bare-metal-bootable-game-for-raspberry-pi-in-csharp/">https://migeel.sk/blog/2023/12/08/building-bare-metal-bootable-game-for-raspberry-pi-in-csharp/</a>, See on <a href="https://news.ycombinator.com/item?id=38599027">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>With the addition of <a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot/" target="_blank" rel="noopener">native AOT</a> in .NET 7, it became possible to compile C# into native code that doesn’t need a virtual machine to run. Instead of a “runtime” to manage the execution of the program, all that’s needed is a “runtime library”, like in C++.</p><p>Ability to compile to fully native code allows doing fun experiments, like replacing the runtime library that comes with the .NET SDK with something that is stripped to the bare minimum to run a very small subset of C#. You can build tiny programs where you can oversee every little aspect of its execution. I’ve been experimenting with what’s possible with the shipping .NET SDK in the <a href="https://github.com/MichalStrehovsky/zerosharp" target="_blank" rel="noopener">zerosharp repo</a>. One of the demos in the zerosharp repo is a bare metal <a href="https://github.com/MichalStrehovsky/zerosharp/tree/master/efi-no-runtime" target="_blank" rel="noopener">boot application to display Hello World</a>.</p><p>While it is possible to hack the official .NET SDK into building a boot application like it’s done in zerosharp, it’s not particularly pleasant (see the number of hacks in the project file, and reimplementation of things like <code>System.Object</code> in the C# source file). I created the <a href="https://flattened.net/" target="_blank" rel="noopener">bflat</a> project to make these kinds of experiments simpler.</p><p>Bflat is an ahead of time compiler for C# built from parts of the official .NET SDK. At its core is a fork of the <a href="https://github.com/dotnet/runtime" target="_blank" rel="noopener">dotnet/runtime</a> repository with a couple opinionated changes. This is built into the bflat CLI that exposes a C# compiler that can target both IL and native code.</p><p>The <a href="https://flattened.net/" target="_blank" rel="noopener">bflat compiler</a> allows you to choose one of three options when it comes to the runtime libraries – you can either use the full runtime library that comes with .NET, or bflat’s own minimal implementation called zerolib, or no standard library at all.</p><div><table><thead><tr><th></th><th>DotNet</th><th>Zero</th><th>None</th></tr></thead><tbody><tr><td>Primitive types</td><td>✓</td><td>✓</td><td>✗</td></tr><tr><td>Rich standard library</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Marshalling-less p/invoke</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Full p/invoke</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Garbage collector</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Exception handling</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Building apps and libraries</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Target Linux, Windows, Android</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Target UEFI</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>Starting app size</td><td>600 kB</td><td>4 kB</td><td>~4 kB</td></tr></tbody></table></div><p>Everything that I’m going to describe below is also possible to achieve with the official .NET SDK. It’s just a lot more work (and hacks) to get there, because as expected, the .NET SDK was not designed for this.</p><p>We’re going to build a small bootable maze “game” in C# that kind of looks like Wolfenstein 3D.</p><h2 id="the-boot-process-of-modern-computers">The boot process of modern computers</h2><p>In modern times, the firmware of a computer can provide a lot more services than it used to in the 1980s BIOS-based systems. The boot process of most of today’s computers follows the UEFI standard. UEFI is an extensible standard that defines an interface for interaction of hardware with the operating system and its boot loader.</p><p>The UEFI firmware knows how to access storage devices, read files from their file system, or load executables from it. In a way, the firmware looks like a very simple operating system. The bflat compiler can target it by specifying the <code>--os:uefi</code> argument to <code>bflat build</code>. A simple hello world looks exactly the way you’d expect it.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>System</span><span>.</span><span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>"Hello World"</span><span>);</span>
</span></span></code></pre></div><p>Build with:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>$ bflat build --stdlib:zero --os:uefi --arch:x64 -o:bootx64.efi hello.cs
</span></span></code></pre></div><p>The generated bootx64.efi file can now be placed on a FAT32-formatted USB flash drive (must be placed under <code>efi/boot</code> directory) and a UEFI-compatible computer should be able to boot from it. If the computer is configured to boot from the USB device, the UEFI boot environment will look for a file named <code>efi/boot/bootx64.efi</code> (or <code>efi/boot/bootaa64.efi</code> for ARM64) and execute it.</p><p>Bflat’s zerolib contains an implementation of <code>System.Console</code> that uses UEFI APIs to talk to the firmware. We’ll talk about those later. If you look at the samples that come with bflat, you’ll find a small <a href="https://github.com/bflattened/bflat/tree/master/samples/Snake" target="_blank" rel="noopener">snake game</a> that uses Console APIs for I/O. The game is written in a platform agnostic way and can be built for any of the OSes supported by bflat with either the standard runtime library, or zerolib. The runtime library papers over any platform differences.</p><h2 id="doing-graphics-in-uefi">Doing graphics in UEFI</h2><p>Doing graphics is harder. The bflat project doesn’t want to be in the business of adding extensions to APIs in .NET that are not built into it in the first place. Any bflat project should also be a valid .NET project (although the set of supported platforms, or characteristics like size might be different). I often wish .NET had a built-in crossplat API to push pixels to the screen and samples to the speakers like libSDL. Unfortunately, there isn’t one, so it follows that bflat doesn’t have one either.</p><p>But since C# makes it easy to interoperate with unmanaged code through p/invoke (platform invoke), we can easily make one. On most platforms, interoperating with external APIs is done though dynamic loading. The executable file specifies the name of the API (optionally also name of the dynamic library implementing the API) and the operating system loader fixes things up at startup so that the API is callable when the program starts executing. The p/invoke mechanism used is called <code>DllImport</code>.</p><p>On UEFI, most communication is done through interfaces. At program start, the program entrypoint receives a pointer to the “EFI system table” from the UEFI firmware. This table contains a set of function pointers to access various services. This matches what .NET exposes under a different p/invoke mechanism - this one is called unmanaged function pointers.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>[StructLayout(LayoutKind.Sequential)]</span>
</span></span><span><span><span>unsafe</span> <span>readonly</span> <span>struct</span> <span>EFI_SYSTEM_TABLE</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_TABLE_HEADER</span> <span>Hdr</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>char</span><span>*</span> <span>FirmwareVendor</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>uint</span> <span>FirmwareRevision</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_HANDLE</span> <span>ConsoleInHandle</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_SIMPLE_TEXT_INPUT_PROTOCOL</span><span>*</span> <span>ConIn</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_HANDLE</span> <span>ConsoleOutHandle</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_SIMPLE_TEXT_OUTPUT_PROTOCOL</span><span>*</span> <span>ConOut</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_HANDLE</span> <span>StandardErrorHandle</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_SIMPLE_TEXT_OUTPUT_PROTOCOL</span><span>*</span> <span>StdErr</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_RUNTIME_SERVICES</span><span>*</span> <span>RuntimeServices</span><span>;</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>EFI_BOOT_SERVICES</span><span>*</span> <span>BootServices</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span><span><span><span>
</span></span></span><span><span><span>[StructLayout(LayoutKind.Sequential)]</span>
</span></span><span><span><span>unsafe</span> <span>readonly</span> <span>struct</span> <span>EFI_SIMPLE_TEXT_OUTPUT_PROTOCOL</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>private</span> <span>readonly</span> <span>IntPtr</span> <span>_pad0</span><span>;</span> <span>// Exact method/signature omitted for brevity</span>
</span></span><span><span>
</span></span><span><span>    <span>// Function pointer to a function that takes a void* and char* and returns void*</span>
</span></span><span><span>    <span>public</span> <span>readonly</span> <span>delegate</span><span>*</span> <span>unmanaged</span><span>&lt;</span><span>void</span><span>*,</span> <span>char</span><span>*,</span> <span>void</span><span>*&gt;</span> <span>OutputString</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>// Rest of the structure omitted for brevity</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>For example, to output a text string given a system table in variable <code>mySystemTable</code>, we’d do <code>mySystemTable-&gt;ConOut-&gt;OutputString(EfiSystemTable-&gt;ConOut, stringOfChars);</code>.
The zerolib in bflat abstracts all of this behind the standard APIs it implements, like <code>Console.WriteLine</code>. Unfortunately this also means that we don’t have access to this from user code – the C# <code>Main</code> starts with a method that takes an array of <code>string</code> (command line arguments), not a pointer to the system table.</p><p>Peeking into zerolib implementation, we can see that zerolib internally stores this in the <code>object. s_efiSystemTable</code> field. With .NET 8, it’s pretty easy to get access to this using <a href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafeaccessorattribute?view=net-8.0" target="_blank" rel="noopener">unsafe accessors</a>.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>var</span> <span>mySystemTable</span> <span>=</span> <span>(</span><span>EFI_SYSTEM_TABLE</span><span>*)</span><span>GetEfiSystemTable</span><span>(</span><span>null</span><span>);</span>
</span></span><span><span><span>
</span></span></span><span><span><span>[UnsafeAccessor(UnsafeAccessorKind.StaticField, Name = "s_efiSystemTable")]</span>
</span></span><span><span><span>static</span> <span>extern</span> <span>ref</span> <span>void</span><span>*</span> <span>GetEfiSystemTable</span><span>(</span><span>object</span> <span>e</span><span>);</span>
</span></span></code></pre></div><p>Now that we have a system table, let’s have a look at what UEFI provides. The spec at the <a href="https://uefi.org/specs/UEFI/2.10/04_EFI_System_Table.html" target="_blank" rel="noopener">official website</a> is quite readable and translating the data structures to C# is not very complicated.</p><p>One of the members of the system table is a pointer to <code>BootServices</code> - another small structure with a bunch of function pointers. The interesting function on it is <code>LocateProtocol</code> which lets us look up more data structures with a bunch of other function pointers. One of the protocols is a protocol for graphics output.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>EFI_GRAPHICS_OUTPUT_PROTOCOL</span><span>*</span> <span>gop</span><span>;</span>
</span></span><span><span><span>var</span> <span>status</span> <span>=</span> <span>efiSys</span><span>-&gt;</span><span>BootServices</span><span>-&gt;</span><span>LocateProtocol</span><span>(</span><span>EFI_GRAPHICS_OUTPUT_PROTOCOL_GUID</span><span>,</span> <span>null</span><span>,</span> <span>(</span><span>void</span><span>**)&amp;</span><span>gop</span><span>);</span>
</span></span><span><span><span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>    <span>Fail</span><span>(</span><span>"LocateProtocol"</span><span>,</span> <span>status</span><span>);</span>
</span></span></code></pre></div><p>This protocol contains function to enumerate supported display modes. We find one that is just big enough for what we need:</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>EFI_GRAPHICS_OUTPUT_MODE_INFORMATION</span> <span>*</span><span>info</span><span>;</span>
</span></span><span><span><span>nuint</span> <span>SizeOfInfo</span><span>,</span> <span>nativeMode</span><span>;</span>
</span></span><span><span><span>status</span> <span>=</span> <span>gop</span><span>-&gt;</span><span>QueryMode</span><span>(</span><span>gop</span><span>,</span> <span>gop</span><span>-&gt;</span><span>Mode</span> <span>==</span> <span>null</span> <span>?</span> <span>0</span> <span>:</span> <span>gop</span><span>-&gt;</span><span>Mode</span><span>-&gt;</span><span>Mode</span><span>,</span> <span>&amp;</span><span>SizeOfInfo</span><span>,</span> <span>&amp;</span><span>info</span><span>);</span>
</span></span><span><span><span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>    <span>status</span> <span>=</span> <span>gop</span><span>-&gt;</span><span>SetMode</span><span>(</span><span>gop</span><span>,</span> <span>0</span><span>);</span>
</span></span><span><span><span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>    <span>Fail</span><span>(</span><span>"Query(Set)Mode"</span><span>,</span> <span>status</span><span>);</span>
</span></span><span><span>
</span></span><span><span><span>uint</span> <span>bestMode</span> <span>=</span> <span>0xFFFFFFFF</span><span>;</span>
</span></span><span><span><span>uint</span> <span>bestHRes</span> <span>=</span> <span>0</span><span>,</span> <span>bestVRes</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span><span>for</span> <span>(</span><span>uint</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>gop</span><span>-&gt;</span><span>Mode</span><span>-&gt;</span><span>MaxMode</span><span>;</span> <span>i</span><span>++)</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>status</span> <span>=</span> <span>gop</span><span>-&gt;</span><span>QueryMode</span><span>(</span><span>gop</span><span>,</span> <span>i</span><span>,</span> <span>&amp;</span><span>SizeOfInfo</span><span>,</span> <span>&amp;</span><span>info</span><span>);</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>        <span>Fail</span><span>(</span><span>"QueryMode"</span><span>,</span> <span>status</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>if</span> <span>(</span><span>info</span><span>-&gt;</span><span>HorizontalResolution</span> <span>&gt;=</span> <span>640</span> <span>&amp;&amp;</span> <span>info</span><span>-&gt;</span><span>VerticalResolution</span> <span>&gt;=</span> <span>480</span><span>)</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>        <span>if</span> <span>(</span><span>bestMode</span> <span>==</span> <span>0xFFFFFFFF</span>
</span></span><span><span>            <span>||</span> <span>(</span><span>info</span><span>-&gt;</span><span>HorizontalResolution</span> <span>&lt;</span> <span>bestHRes</span> <span>&amp;&amp;</span> <span>info</span><span>-&gt;</span><span>VerticalResolution</span> <span>&lt;</span> <span>bestVRes</span><span>))</span>
</span></span><span><span>        <span>{</span>
</span></span><span><span>            <span>bestMode</span> <span>=</span> <span>i</span><span>;</span>
</span></span><span><span>            <span>bestHRes</span> <span>=</span> <span>info</span><span>-&gt;</span><span>HorizontalResolution</span><span>;</span>
</span></span><span><span>            <span>bestVRes</span> <span>=</span> <span>info</span><span>-&gt;</span><span>VerticalResolution</span><span>;</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span><span>if</span> <span>(</span><span>bestMode</span> <span>==</span> <span>0xFFFFFFFF</span><span>)</span>
</span></span><span><span>    <span>Fail</span><span>(</span><span>"No usable display mode found"</span><span>,</span> <span>0</span><span>);</span>
</span></span></code></pre></div><p>We then create a frame buffer (a byte array for 640 x 480 pixels, each pixel having RGB components and one byte of padding) and enter the main game loop where we check for input and render the game screen into the buffer. We then blit the buffer to screen with the Blt function provided by UEFI.</p><p>Zerolib fortunately provides APIs for checking for input. They’re not ideal (no key down/key up event) but that’s actually an UEFI limitation as well so we can’t do better even if we wanted.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>byte</span><span>[]</span> <span>fb</span> <span>=</span> <span>new</span> <span>byte</span><span>[</span><span>640</span> <span>*</span> <span>480</span> <span>*</span> <span>4</span><span>];</span>
</span></span><span><span>
</span></span><span><span><span>System</span><span>.</span><span>Threading</span><span>.</span><span>Thread</span><span>.</span><span>Sleep</span><span>(</span><span>2000</span><span>);</span>
</span></span><span><span>
</span></span><span><span><span>fixed</span> <span>(</span><span>byte</span><span>*</span> <span>pBuffer</span> <span>=</span> <span>fb</span><span>)</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>status</span> <span>=</span> <span>gop</span><span>-&gt;</span><span>SetMode</span><span>(</span><span>gop</span><span>,</span> <span>bestMode</span><span>);</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>        <span>Fail</span><span>(</span><span>"Set"</span><span>,</span> <span>status</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>uint</span> <span>gameTime</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>    <span>while</span> <span>(</span><span>true</span><span>)</span>
</span></span><span><span>    <span>{</span>
</span></span><span><span>        <span>keyState</span> <span>=</span> <span>default</span><span>;</span>
</span></span><span><span>        <span>if</span> <span>(</span><span>Console</span><span>.</span><span>KeyAvailable</span><span>)</span>
</span></span><span><span>            <span>keyState</span> <span>=</span> <span>Console</span><span>.</span><span>ReadKey</span><span>(</span><span>false</span><span>).</span><span>Key</span> <span>switch</span>
</span></span><span><span>            <span>{</span>
</span></span><span><span>                <span>ConsoleKey</span><span>.</span><span>UpArrow</span> <span>=&gt;</span> <span>KeyState</span><span>.</span><span>Up</span><span>,</span>
</span></span><span><span>                <span>ConsoleKey</span><span>.</span><span>DownArrow</span> <span>=&gt;</span> <span>KeyState</span><span>.</span><span>Down</span><span>,</span>
</span></span><span><span>                <span>ConsoleKey</span><span>.</span><span>LeftArrow</span> <span>=&gt;</span> <span>KeyState</span><span>.</span><span>Left</span><span>,</span>
</span></span><span><span>                <span>ConsoleKey</span><span>.</span><span>RightArrow</span> <span>=&gt;</span> <span>KeyState</span><span>.</span><span>Right</span><span>,</span>
</span></span><span><span>                <span>_</span> <span>=&gt;</span> <span>default</span><span>,</span>
</span></span><span><span>            <span>};</span>
</span></span><span><span>
</span></span><span><span>        <span>RenderEffect</span><span>(</span><span>gameTime</span><span>,</span> <span>pBuffer</span><span>);</span>
</span></span><span><span>        <span>status</span> <span>=</span> <span>gop</span><span>-&gt;</span><span>Blt</span><span>(</span><span>gop</span><span>,</span>
</span></span><span><span>            <span>(</span><span>EFI_GRAPHICS_OUTPUT_BLT_PIXEL</span><span>*)</span><span>pBuffer</span><span>,</span>
</span></span><span><span>            <span>EFI_GRAPHICS_OUTPUT_BLT_OPERATION</span><span>.</span><span>EfiBltBufferToVideo</span><span>,</span>
</span></span><span><span>            <span>0</span><span>,</span>
</span></span><span><span>            <span>0</span><span>,</span>
</span></span><span><span>            <span>(</span><span>bestHRes</span> <span>-</span> <span>640</span><span>)</span> <span>/</span> <span>2</span><span>,</span>
</span></span><span><span>            <span>(</span><span>bestVRes</span> <span>-</span> <span>480</span><span>)</span> <span>/</span> <span>2</span><span>,</span>
</span></span><span><span>            <span>640</span><span>,</span>
</span></span><span><span>            <span>480</span><span>,</span>
</span></span><span><span>            <span>0</span><span>);</span>
</span></span><span><span>
</span></span><span><span>        <span>System</span><span>.</span><span>Threading</span><span>.</span><span>Thread</span><span>.</span><span>Sleep</span><span>(</span><span>10</span><span>);</span>
</span></span><span><span>        <span>gameTime</span> <span>+=</span> <span>10</span><span>;</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> <span>(</span><span>status</span> <span>!=</span> <span>0</span><span>)</span>
</span></span><span><span>            <span>Fail</span><span>(</span><span>"Blt"</span><span>,</span> <span>status</span><span>);</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>If you’re wondering about the maze drawing logic itself, I refer you to either:</p><ul><li><a href="https://lodev.org/cgtutor/raycasting.html#Textured_Raycaster" target="_blank" rel="noopener">Lode’s computer graphics tutorial</a>, or</li><li><a href="https://fabiensanglard.net/gebb/" target="_blank" rel="noopener">The game engine black book – Wolfenstein 3D</a></li></ul><p>I just took Lode’s C++ code and translated it into C#. There’s not much for me to talk about.
I hit a tiny snag that the <code>System.Math</code> class is not implemented in zerolib so I had dig into what I still remembered from university. This is my Sin/Cos (<a href="https://en.wikipedia.org/wiki/Taylor_series" target="_blank" rel="noopener">Taylor’s version</a>). It’s good enough for what I need it for.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>class</span> <span>Math</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>public</span> <span>static</span> <span>double</span> <span>Sin</span><span>(</span><span>double</span> <span>x</span><span>)</span>
</span></span><span><span>        <span>=&gt;</span> <span>x</span> <span>-</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>6</span><span>))</span> <span>+</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>120</span><span>))</span> <span>-</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>5040</span><span>));</span>
</span></span><span><span>
</span></span><span><span>    <span>public</span> <span>static</span> <span>double</span> <span>Cos</span><span>(</span><span>double</span> <span>x</span><span>)</span>
</span></span><span><span>        <span>=&gt;</span> <span>1</span> <span>-</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>2</span><span>))</span> <span>+</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>24</span><span>))</span> <span>-</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>720</span><span>))</span> <span>+</span> <span>((</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>*</span><span>x</span><span>)/(</span><span>40320</span><span>));</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>I’ve placed the whole thing on <a href="https://github.com/MichalStrehovsky/uefimaze" target="_blank" rel="noopener">GitHub</a>.</p><p>Here’s one more version of the game running on an old PC for a change:</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google have removed RSS support from their developer blogs (299 pts)]]></title>
            <link>https://developer.chrome.com/feeds</link>
            <guid>38598983</guid>
            <pubDate>Mon, 11 Dec 2023 09:26:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/feeds">https://developer.chrome.com/feeds</a>, See on <a href="https://news.ycombinator.com/item?id=38598983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<p>If you've ended up here, chances are you're looking to subscribe to our blog
in your RSS feed.</p>

<p>Unfortunately, we don't have official RSS feed support for now, but we're
actively working on a solution.</p>

<p>Thank you for your patience.</p>

<div>
  <p><a href="https://developer.chrome.com/blog">Visit the Blog</a>
</p></div>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Earliest Carpenters: 476k-year-old log structure discovered in Zambia (188 pts)]]></title>
            <link>https://www.archaeology.org/issues/537-features/top10/11937-zambia-kalambo-river-earliest-woodworking</link>
            <guid>38598742</guid>
            <pubDate>Mon, 11 Dec 2023 08:40:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.archaeology.org/issues/537-features/top10/11937-zambia-kalambo-river-earliest-woodworking">https://www.archaeology.org/issues/537-features/top10/11937-zambia-kalambo-river-earliest-woodworking</a>, See on <a href="https://news.ycombinator.com/item?id=38598742">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>	
		<p><img src="https://www.archaeology.org/images/JF2024/Top_Ten/Zambia-Wooden-Structure-Notch-Combo.jpg" alt="Zambia Wooden Structure Notch Combo" width="710" height="355" title="Oldest wooden structure (left), Kalambo River, Zambia; Intentionally cut notch (right)" longdesc="(Courtesy Larry Barham)"><img src="https://www.archaeology.org/images/JF2024/Top_Ten/Zambia-Kalambo-River-Wooden-Tools.jpg" alt="Zambia Kalambo River Wooden Tools" width="355" height="350" title="Left to right: Wedge-shaped object, digging stick, and flattened log" longdesc="(Courtesy Larry Barham )"><strong>Rarely has a single find changed</strong> scholars’ views of the capabilities of people of the past as radically as the discovery of the world’s earliest known wooden architecture, which dates to nearly half a million years ago. The pair of interlocking logs joined by an intentionally cut notch was unearthed beneath a bank of Zambia’s Kalambo River by a team led by University of Liverpool archaeologist Larry Barham. Researchers believe the logs may have formed part of a walkway or the foundation of a platform built over wetlands. Prior to this discovery, the oldest known surviving wooden structures were built by people living in northern England around 11,000 years ago.</p>

<p>The 476,000-year-old log structure predates the appearance of the first modern humans by some 150,000 years and was likely the handiwork of the archaic human species <em>Homo heidelbergensis</em>. Paleoanthropologists believe <em>H. heidelbergensis</em> was highly mobile. Thus, it is surprising that the hominins would have invested labor in building a semipermanent structure. “We haven’t seen archaic humans manipulating their environment on such a large scale before,” says Barham. “It suggests an attachment to a single point on the landscape.”</p>

<p>At the same site, the team unearthed stone axes as well as four wooden tools dating to between 390,000 and 324,000 years ago. These included a digging stick, a wedge-shaped object, a notched branch, and a flattened log. Marks on the log, notes Barham, resemble nothing so much as tool nicks on a work bench, inviting speculation as to what other structures an imaginative <em>H. heidelbergensis</em> woodworker might have fashioned.</p>	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral: Our first AI endpoints are available in early access (454 pts)]]></title>
            <link>https://mistral.ai/news/la-plateforme/</link>
            <guid>38598568</guid>
            <pubDate>Mon, 11 Dec 2023 08:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/la-plateforme/">https://mistral.ai/news/la-plateforme/</a>, See on <a href="https://news.ycombinator.com/item?id=38598568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Mistral AI brings the strongest open generative models to the developers, along with efficient ways to deploy and customise them for production.</p><p>We’re opening a beta access to our <a href="https://console.mistral.ai/">first platform services</a> today. We start simple: la plateforme serves three chat endpoints for generating text following textual instructions and an embedding endpoint. Each endpoint has a different performance/price tradeoff.</p><h4 id="generative-endpoints">Generative endpoints</h4><p>The two first endpoints, mistral-tiny and mistral-small, currently use our two released open models; the third, mistral-medium, uses a prototype model with higher performances that we are testing in a deployed setting.</p><p>We serve instructed versions of our models. We have worked on consolidating the most effective alignment techniques (efficient fine-tuning, direct preference optimisation) to create easy-to-control and pleasant-to-use models. We pre-train models on data extracted from the open Web and perform instruction fine-tuning from annotations.</p><p><strong>Mistral-tiny</strong>. Our most cost-effective endpoint currently serves Mistral 7B Instruct v0.2, a new minor release of Mistral 7B Instruct.
Mistral-tiny only works in English. It obtains 7.6 on MT-Bench.
The instructed model can be downloaded <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">here</a>.</p><p><strong>Mistral-small</strong>. This endpoint currently serves our newest model, Mixtral 8x7B, described in more detail in our <a href="https://mistral.ai/news/mixtral-of-experts">blog post</a>.
It masters English/French/Italian/German/Spanish and code and obtains 8.3 on MT-Bench.</p><p><strong>Mistral-medium</strong>. Our highest-quality endpoint currently serves a prototype model,
that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench. The following table compare the performance of the base models of Mistral-medium, Mistral-small and the endpoint of a competitor.</p><p><img src="https://mistral.ai/images/news/la-plateforme/mistral-medium.png" alt="mistral-medium" width="50%"></p><h4 id="embedding-endpoint">Embedding endpoint</h4><p>Mistral-embed, our embedding endpoint, serves an embedding model with a 1024 embedding dimension. Our embedding model has been designed with retrieval capabilities in mind. It achieves a retrieval score of <strong>55.26</strong> on MTEB.</p><h4 id="api-specifications">API specifications</h4><p>Our API follows the specifications of the popular chat interface initially proposed by our dearest competitor. We provide a Python and Javascript client library to query our endpoints. Our endpoints allow users to provide a system prompt to set a higher level of moderation on model outputs for applications where this is an important requirement.</p><h4 id="ramping-up-from-beta-access-to-general-availability">Ramping up from beta access to general availability</h4><p>Anyone can <a href="https://console.mistral.ai/">register</a> to use our API as of today as we progressively ramp up our capacity. Our business team can help qualify your needs and accelerate access. Expect rough edges as we stabilise our platform towards fully self-served availability.</p><h4 id="acknowledgement">Acknowledgement</h4><p>We are grateful to Nvidia for supporting us in integrating TRT-LLM and Triton and working alongside us to make a sparse mixture of experts compatible with TRT-LLM.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral: Mixtral of Experts (608 pts)]]></title>
            <link>https://mistral.ai/news/mixtral-of-experts/</link>
            <guid>38598559</guid>
            <pubDate>Mon, 11 Dec 2023 08:01:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</a>, See on <a href="https://news.ycombinator.com/item?id=38598559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Mistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.</p><p>Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.</p><p>Mixtral has the following capabilities.</p><ul><li>It gracefully handles a context of 32k tokens.</li><li>It handles English, French, Italian, German and Spanish.</li><li>It shows strong performance in code generation.</li><li>It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.</li></ul><h4 id="pushing-the-frontier-of-open-models-with-sparse-architectures">Pushing the frontier of open models with sparse architectures</h4><p>Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively.</p><p>This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.
Concretely, Mixtral has 45B total parameters but only uses 12B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12B model.</p><p>Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously.</p><h4 id="performance">Performance</h4><p>We compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks.</p><p><img src="https://mistral.ai/images/news/mixtral-of-experts/overview.png" alt="Performance overview" width="66%"></p><p>On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models.</p><p><img src="https://mistral.ai/images/news/mixtral-of-experts/scaling.png" alt="Scaling of performances" width="100%"></p><p>The following table give detailed results on the figure above.</p><p><img src="https://mistral.ai/images/news/mixtral-of-experts/open_models.png" alt="Detailed benchmarks" width="100%"></p><p><strong>Hallucination and biases.</strong> To identify possible flaws to be corrected by fine-tuning / preference modelling,
we measure the <em>base</em> model performance on TruthfulQA/BBQ/BOLD.</p><p><img src="https://mistral.ai/images/news/mixtral-of-experts/bbq_bold.png" alt="BBQ BOLD benchmarks" width="40%"></p><p>Compared to Llama 2, Mixtral is more truthful (73.9% vs 50.2% on the TruthfulQA benchmark) and presents less bias on the BBQ benchmark.
Overall, Mixtral displays more positive sentiments than Llama 2 on BOLD, with similar variances within each dimension.</p><p><strong>Language.</strong> Mixtral 8x7B masters French, German, Spanish, Italian, and English.</p><p><img src="https://mistral.ai/images/news/mixtral-of-experts/multilingual.png" alt="Multilingual benchmarks" width="100%"></p><h4 id="instructed-models">Instructed models</h4><p>We release Mixtral 8x7B Instruct alongside Mixtral 8x7B. This model has been optimised through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5.</p><p>Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified <a href="https://docs.mistral.ai/platform/guardrailing">here</a>. A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given.</p><h4 id="deploy-mixtral-with-an-open-source-deployment-stack">Deploy Mixtral with an open-source deployment stack</h4><p>To enable the community to run Mixtral with a fully open-source stack, we have submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference.</p><p>Skypilot allows the deployment of vLLM endpoints on any instance in the cloud.</p><h4 id="use-mixtral-on-our-platform">Use Mixtral on our platform.</h4><p>We’re currently using Mixtral 8x7B behind our endpoint <em>mistral-small</em>, which is <a href="https://mistral.ai/news/la-plateforme">available in beta</a>. <a href="https://console.mistral.ai/">Register</a> to get early access to all generative and embedding endpoints.</p><h4 id="acknowledgement">Acknowledgement</h4><p>We thank CoreWeave and Scaleway teams for technical support as we trained our models.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fixing Classical Cats: How I Got Tricked by 28-Year-Old Defensive Programming (151 pts)]]></title>
            <link>https://www.mistys-internet.website/blog/blog/2023/12/10/fixing-classical-cats-or/</link>
            <guid>38598325</guid>
            <pubDate>Mon, 11 Dec 2023 07:09:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mistys-internet.website/blog/blog/2023/12/10/fixing-classical-cats-or/">https://www.mistys-internet.website/blog/blog/2023/12/10/fixing-classical-cats-or/</a>, See on <a href="https://news.ycombinator.com/item?id=38598325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Every now and then, when working on ScummVM’s Director engine, I run across a disc that charms me so much I just have to get it working right away. That happened when I ran into Classical Cats, a digital art gallery focused on the work of Japanese artist and classical musician Mitsuhiro Amada. I <a href="https://cdrom.ca/games/2023/12/06/classical-cats.html">wrote about the disc’s contents</a> in more detail at my <a href="https://cdrom.ca/">CD-ROM blog</a>, but needless to say I was charmed—I wanted to share this with more people.</p>

<p><img src="https://www.mistys-internet.website/blog/images/classicalcats/scummvm-classicalcats-mac-ja-00004.png" alt="Screenshot of a cat playing piano next to a cat playing a violin and a cat playing cello"></p>

<p>I first found out about Classical Cats when fellow ScummVM developer einstein95 pointed me at it because its music wasn’t working. Like a lot of early Director discs, Classical Cats <em>mostly</em> just worked on the first try. At this point in ScummVM’s development, I’m often more surprised if a disc made in Director 3 or 4 fails to boot right away. The one thing that didn’t work was the music.</p>

<p>Classical Cats uses CD audio for its music, and I’d already written code to support this in early releases of <a href="https://www.mobygames.com/game/2059/alice-an-interactive-museum/">Alice: An Interactive Museum</a> for Mac. I’d optimistically hoped that Classical Cats might be as easy, but it turned out to present some extra technical complexity. Regardless, for a disc called “Classical” Cats, I knew that getting music working would be important. I could tell that I wasn’t having the full experience.</p>

<p>While many CD-ROMs streamed their music from files on the disc, some discs used CD audio tracks for music instead. (If you’re already familiar with CD audio and mixed-mode CDs, you can skip to the next paragraph.) CD audio is the same format used in audio CDs; these tracks aren’t files in a directory and don’t have names, but are simply numbered tracks like you’d see in a CD player. Data on a CD is actually contained within a track on the disc, just like audio; data tracks are just skipped over by CD players. A <em>mixed mode</em> CD is one that contains a mixture of one or more data tracks and one or more audio tracks on the same disc. This was often used by games and multimedia discs as a simple and convenient way to store their audio.</p>

<p>Director software is written in its own programming language called Lingo; I’ve written about it <a href="https://www.mistys-internet.website/blog/blog/2022/01/06/do-you-speak-the-lingo/">a few</a> <a href="https://www.mistys-internet.website/blog/blog/2023/05/29/untangling-another-lingo-parser-edge-case/">times</a> before. In addition to writing logic in Lingo, developers are able to write modules called XObjects; these can be implemented in another language like C, but expose an interface to Lingo code. It works very similarly to C extensions in languages like Ruby or Python.</p>

<p>While ScummVM is able to run Lingo code directly, it doesn’t emulate the original XObjects. Instead, it contains new clean-room reimplementations embedded into ScummVM that expose the same interfaces as the originals. If a disc tries to call an unimplemented XObject, ScummVM just logs a warning and is able to continue. I’d already implemented one of Director’s builtin audio CD XObjects earlier, which was how I fixed Alice’s music earlier.</p>

<p>ScummVM has builtin support for playing emulated audio CDs by replacing the audio tracks with MP3 or FLAC files. For Alice, I <a href="https://github.com/scummvm/scummvm/pull/4231">wrote</a> an implementation of Director’s builtin Apple Audio CD XObject. That version was straightforward and easy to implement; it has a minimal API that allows an app to request playback of a CD via track number, which maps perfectly onto ScummVM’s virtual CD backend.</p>

<p>I already knew Classical Cats uses a different XObject, and so I’d have to write a new implementation for it, it turns out the API was very different from Alice’s. Alice, along with many other Director games I’ve looked at, uses a fairly high-level, track-oriented API that was simple to implement. ScummVM’s builtin CD audio infrastructure is great at handling requests like “play track 5”, or “play the first 30 seconds of track 7”. What it’s not at all prepared for is requests like “play from position 12:00:42 on the disc”.</p>

<p>You can probably guess what Classical Cats does! Instead of working with tracks, it starts and stops playback based on absolute positions on a disc. This may sound strange, but it’s how the disc itself is set up. On a real CD, tracks themselves are just indices into where tracks start and stop on a disc, and a regular CD player looks up those indices to decide where to seek to when you ask it to play a particular track. In theory, it’s pretty similar to dropping a record player needle on a specific spot on the disc.</p>

<p>This might not sound too complex to manage, but there’s actually something that makes it a lot harder: translating requests to play an absolute timecode to an audio file on disc. ScummVM isn’t (usually) playing games from a real CD, but emulating a drive using the game data and FLAC or MP3 files replacing the CD audio tracks. ScummVM generally plays games using the data extracted from the CD into a folder on the hard drive, which causes a problem: the data track on a mixed mode CD is usually the first track, which means that the timing of every other track on the disc is offset by the length of the data track. We can’t guess where anything else is stored without knowing exactly how long the data track is. If we’ve extracted the data from the CD, we no longer know how big that track is, and we can’t guess at the layout of the rest of the disc.</p>

<p>“Knowing the disc layout” is a common problem with CD ripping and authoring, and a number of standards exist already. Single-disc data CDs can easily be represented as an ISO file, but anything more complex requires an actual table of contents. When thinking about how to solve this problem for ScummVM, I immediately thought of <a href="https://en.wikipedia.org/wiki/Cue_sheet_(computing">cuesheets</a>)—one of the most popular table of contents formats for CD ripping, and one that’s probably familiar to gamers who have used BIN/CUE rips of 32-bit era video games. Among all the formats available for documenting a disc’s table of contents, cuesheets were attractive for a few reasons: I’ve worked with it before, so I’m already familiar with it; it’s human-readable, so it’s easy to validate that it’s being used properly; and it provides a simple, high-level interface that abstracts away irrelevant details that I wouldn’t need to implement this feature. A sample cuesheet for a mixed mode CD looks something like this:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>9</span>
<span>10</span>
</pre></td><td><pre><code><span>FILE "CLSSCATS.BIN" BINARY
</span><span>  TRACK 01 MODE1/2352
</span><span>    INDEX 01 00:00:00
</span><span>  TRACK 02 AUDIO
</span><span>    PREGAP 00:02:00
</span><span>    INDEX 01 17:41:36
</span><span>  TRACK 03 AUDIO
</span><span>    INDEX 01 19:20:46
</span><span>  TRACK 04 AUDIO
</span><span>    INDEX 01 22:09:17</span></code></pre></td></tr></tbody></table></div></figure>


<p>Once you understand the format, it’s straightforward to read and makes it clear exactly where every track is located on the disc.</p>

<p>The main blocker here was simply that ScummVM didn’t have a cuesheet parser yet, and I wasn’t eager to write one myself. Just when I was on the verge of switching to another solution, however, ScummVM project lead Eugene Sandulenko offered to write a new one integrated into ScummVM itself. As soon as that was ready, I was able to get to work.</p>

<p>The XObject Classical Cats uses has a fairly complicated interface that’s meant to support not just CDs, but also media like video cassettes. To keep things simple, I decided to limit myself to implementing just the API that this disc uses and ignore methods it never calls. It’s hard to make sure my implementation’s compatible if I don’t actually see parts of it in use, after all. By watching to see which method stubs are called, I could see that I mainly had to deal with a limit set of methods. Aside from being able to see which methods are called and the arguments passed to them, I was able to consult the official documentation in the Director 4.0 manual.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Two of the most fundamental methods I began with were <code>mSetInPoint</code> and <code>mSetOutPoint</code>, whose names were pretty self-explanatory. Rather than have a single method to begin playback with start/stop positions, this library uses a cue system. Callers first call <code>mSetInPoint</code> to define the start playback position and <code>mSetOutPoint</code> to set a stop position. These positions are tracked in <em>frames</em>, a unit representing 1/75th of a second.</p>

<p>On a real drive, they can then call <code>mPlayCue</code> to seek to the start of the position so that the drive is ready. Given the slow seek times of early CD-ROM drives, this separation forced developers to consider that the device might not actually be able to start playback as soon as they request it and take that into account with their app’s interactive features. After starting the seek operation, the developer was meant to repeatedly call <code>mService</code> to retrieve a status code and find out whether the drive was still seeking, had finished seeking, or encountered an error. Since ScummVM is usually acting on an emulated drive without actual seek times, I simplified this. <code>mSetInPoint</code> and <code>mSetOutPoint</code> simply assign instance variables with the appropriate values, and <code>mService</code> always immediately returns the “drive ready” code.</p>

<p>At this point, I did what I should have done in the first place and checked the source code. As I mentioned in a <a href="https://www.mistys-internet.website/blog/blog/2022/01/06/do-you-speak-the-lingo/">previous post</a>, early Director software includes the source code as a part of the binary, and luckily that’s true for Classical Cats. As I checked its CD-ROM helper library, I stumbled on the method that made me realize exactly where I’d gone wrong:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
</pre></td><td><pre><code><span>on mGetFirstFrame me, aTrack
</span><span>  put the pXObj of me into myXObj
</span><span>  if myXObj(mRespondsTo, "mGetFirstFrame") = 0 then
</span><span>    return 0
</span><span>  else
</span><span>    return  myXObj(mGetFirstFrame, aTrack)
</span><span>  end if
</span><span>end</span></code></pre></td></tr></tbody></table></div></figure>


<p>This code might be familiar to Rubyists, since Ruby has a very similar construct. This class wraps the AppleCD SC XObject, instantiated in the instance variable <code>myXObj</code>, and calls methods on it. But it’s written defensively: before calling a number of methods, it calls <code>mRespondsTo</code> first to see if <code>myXObj</code> has the requested method. If it doesn’t, it just stubs it out instead of erroring. Since ScummVM implements <code>mRespondsTo</code> correctly, it means this code was doing what the original authors intended: seeing that my implementation of AppleCD SC didn’t have an <code>mGetFirstFrame</code> method, and just returning a stub value. Unfortunately for me, I was being lazy and had chosen which methods to implement based on seeing the disc try to use them—so I let myself be tricked into thinking those methods were never used.</p>

<p>As it turns out, they were actually key to getting the right timing data. Classical Cats was trying to ask the CD drive about timing information for tracks, and storing that to use to actually play the songs. With these methods missing, it was stuck without knowing where the songs were and how to play them.</p>

<p>And here I realized the great irony of what I was doing. Internally, Classical Cats thinks about its audio in terms of tracks, and asks the XObject for absolute timing data for each track. It then passes that data back into the XObject to play the songs, where ScummVM intercepts it and translates it back into track-oriented timing so its CD drive emulation knows how to play them. It’s a lot of engineering work just to take it all full circle.</p>

<p>At the end of the day, though, what’s important is it <em>does</em> work. Before I finished writing this, it was difficult to play Classical Cats on any modern computer; now, anyone with the latest version of ScummVM can give it a try. Now that it’s more accessible, I hope other people are able to discover it too.</p>

<p>Note: CD audio support for this disc is available in nightly builds of ScummVM, and will be available in a future stable release.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: RFC 9330 (344 pts)]]></title>
            <link>https://datatracker.ietf.org/doc/rfc9330/</link>
            <guid>38597744</guid>
            <pubDate>Mon, 11 Dec 2023 04:50:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://datatracker.ietf.org/doc/rfc9330/">https://datatracker.ietf.org/doc/rfc9330/</a>, See on <a href="https://news.ycombinator.com/item?id=38597744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <pre>﻿

Internet Engineering Task Force (IETF)                   B. Briscoe, Ed.
Request for Comments: 9330                                   Independent
Category: Informational                                   K. De Schepper
ISSN: 2070-1721                                          Nokia Bell Labs
                                                              M. Bagnulo
                                        Universidad Carlos III de Madrid
                                                                G. White
                                                               CableLabs
                                                            January 2023

 Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service:
                              Architecture

<span>Abstract</span>

   This document describes the L4S architecture, which enables Internet
   applications to achieve low queuing latency, low congestion loss, and
   scalable throughput control.  L4S is based on the insight that the
   root cause of queuing delay is in the capacity-seeking congestion
   controllers of senders, not in the queue itself.  With the L4S
   architecture, all Internet applications could (but do not have to)
   transition away from congestion control algorithms that cause
   substantial queuing delay and instead adopt a new class of congestion
   controls that can seek capacity with very little queuing.  These are
   aided by a modified form of Explicit Congestion Notification (ECN)
   from the network.  With this new architecture, applications can have
   both low latency and high throughput.

   The architecture primarily concerns incremental deployment.  It
   defines mechanisms that allow the new class of L4S congestion
   controls to coexist with 'Classic' congestion controls in a shared
   network.  The aim is for L4S latency and throughput to be usually
   much better (and rarely worse) while typically not impacting Classic
   performance.

<span>Status of This Memo</span>

   This document is not an Internet Standards Track specification; it is
   published for informational purposes.

   This document is a product of the Internet Engineering Task Force
   (IETF).  It represents the consensus of the IETF community.  It has
   received public review and has been approved for publication by the
   Internet Engineering Steering Group (IESG).  Not all documents
   approved by the IESG are candidates for any level of Internet
   Standard; see Section 2 of RFC 7841.

   Information about the current status of this document, any errata,
   and how to provide feedback on it may be obtained at
   https://www.rfc-editor.org/info/rfc9330.

<span>Copyright Notice</span>

   Copyright (c) 2023 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (https://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Revised BSD License text as described in Section 4.e of the
   Trust Legal Provisions and are provided without warranty as described
   in the Revised BSD License.

<span>Table of Contents</span>

   1.  Introduction
     1.1.  Document Roadmap
   2.  L4S Architecture Overview
   3.  Terminology
   4.  L4S Architecture Components
     4.1.  Protocol Mechanisms
     4.2.  Network Components
     4.3.  Host Mechanisms
   5.  Rationale
     5.1.  Why These Primary Components?
     5.2.  What L4S Adds to Existing Approaches
   6.  Applicability
     6.1.  Applications
     6.2.  Use Cases
     6.3.  Applicability with Specific Link Technologies
     6.4.  Deployment Considerations
       6.4.1.  Deployment Topology
       6.4.2.  Deployment Sequences
       6.4.3.  L4S Flow but Non-ECN Bottleneck
       6.4.4.  L4S Flow but Classic ECN Bottleneck
       6.4.5.  L4S AQM Deployment within Tunnels
   7.  IANA Considerations
   8.  Security Considerations
     8.1.  Traffic Rate (Non-)Policing
       8.1.1.  (Non-)Policing Rate per Flow
       8.1.2.  (Non-)Policing L4S Service Rate
     8.2.  'Latency Friendliness'
     8.3.  Interaction between Rate Policing and L4S
     8.4.  ECN Integrity
     8.5.  Privacy Considerations
   9.  Informative References
   Acknowledgements
   Authors' Addresses

1.  Introduction

   At any one time, it is increasingly common for all of the traffic in
   a bottleneck link (e.g., a household's Internet access or Wi-Fi) to
   come from applications that prefer low delay: interactive web, web
   services, voice, conversational video, interactive video, interactive
   remote presence, instant messaging, online and cloud-rendered gaming,
   remote desktop, cloud-based applications, cloud-rendered virtual
   reality or augmented reality, and video-assisted remote control of
   machinery and industrial processes.  In the last decade or so, much
   has been done to reduce propagation delay by placing caches or
   servers closer to users.  However, queuing remains a major, albeit
   intermittent, component of latency.  For instance, spikes of hundreds
   of milliseconds are not uncommon, even with state-of-the-art Active
   Queue Management (AQM) [COBALT] [DOCSIS3AQM].  A Classic AQM in an
   access network bottleneck is typically configured to buffer the
   sawteeth of lone flows, which can cause peak overall network delay to
   roughly double during a long-running flow, relative to expected base
   (unloaded) path delay [BufferSize].  Low loss is also important
   because, for interactive applications, losses translate into even
   longer retransmission delays.

   It has been demonstrated that, once access network bit rates reach
   levels now common in the developed world, increasing link capacity
   offers diminishing returns if latency (delay) is not addressed
   [Dukkipati06] [Rajiullah15].  Therefore, the goal is an Internet
   service with very low queuing latency, very low loss, and scalable
   throughput.  Very low queuing latency means less than 1 millisecond
   (ms) on average and less than about 2 ms at the 99th percentile.
   End-to-end delay above 50 ms [Raaen14], or even above 20 ms [NASA04],
   starts to feel unnatural for more demanding interactive applications.
   Therefore, removing unnecessary delay variability increases the reach
   of these applications (the distance over which they are comfortable
   to use) and/or provides additional latency budget that can be used
   for enhanced processing.  This document describes the L4S
   architecture for achieving these goals.

   Differentiated services (Diffserv) offers Expedited Forwarding (EF)
   [RFC3246] for some packets at the expense of others, but this makes
   no difference when all (or most) of the traffic at a bottleneck at
   any one time requires low latency.  In contrast, L4S still works well
   when all traffic is L4S -- a service that gives without taking needs
   none of the configuration or management baggage (traffic policing or
   traffic contracts) associated with favouring some traffic flows over
   others.

   Queuing delay degrades performance intermittently [Hohlfeld14].  It
   occurs i) when a large enough capacity-seeking (e.g., TCP) flow is
   running alongside the user's traffic in the bottleneck link, which is
   typically in the access network, or ii) when the low latency
   application is itself a large capacity-seeking or adaptive rate flow
   (e.g., interactive video).  At these times, the performance
   improvement from L4S must be sufficient for network operators to be
   motivated to deploy it.

   Active Queue Management (AQM) is part of the solution to queuing
   under load.  AQM improves performance for all traffic, but there is a
   limit to how much queuing delay can be reduced by solely changing the
   network without addressing the root of the problem.

   The root of the problem is the presence of standard congestion
   control (Reno [RFC5681]) or compatible variants (e.g., CUBIC
   [RFC8312]) that are used in TCP and in other transports, such as QUIC
   [RFC9000].  We shall use the term 'Classic' for these Reno-friendly
   congestion controls.  Classic congestion controls induce relatively
   large sawtooth-shaped excursions of queue occupancy.  So if a network
   operator naively attempts to reduce queuing delay by configuring an
   AQM to operate at a shallower queue, a Classic congestion control
   will significantly underutilize the link at the bottom of every
   sawtooth.  These sawteeth have also been growing in duration as flow
   rate scales (see Section 5.1 and [RFC3649]).

   It has been demonstrated that, if the sending host replaces a Classic
   congestion control with a 'Scalable' alternative, the performance
   under load of all the above interactive applications can be
   significantly improved once a suitable AQM is deployed in the
   network.  Taking the example solution cited below that uses Data
   Center TCP (DCTCP) [RFC8257] and a Dual-Queue Coupled AQM [RFC9332]
   on a DSL or Ethernet link, queuing delay under heavy load is roughly
   1-2 ms at the 99th percentile without losing link utilization
   [L4Seval22] [DualPI2Linux] (for other link types, see Section 6.3).
   This compares with 5-20 ms on _average_ with a Classic congestion
   control and current state-of-the-art AQMs, such as Flow Queue CoDel
   [RFC8290], Proportional Integral controller Enhanced (PIE) [RFC8033],
   or DOCSIS PIE [RFC8034] and about 20-30 ms at the 99th percentile
   [DualPI2Linux].

   L4S is designed for incremental deployment.  It is possible to deploy
   the L4S service at a bottleneck link alongside the existing best
   efforts service [DualPI2Linux] so that unmodified applications can
   start using it as soon as the sender's stack is updated.  Access
   networks are typically designed with one link as the bottleneck for
   each site (which might be a home, small enterprise, or mobile
   device), so deployment at either or both ends of this link should
   give nearly all the benefit in the respective direction.  With some
   transport protocols, namely TCP [ACCECN], the sender has to check
   that the receiver has been suitably updated to give more accurate
   feedback, whereas with more recent transport protocols, such as QUIC
   [RFC9000] and Datagram Congestion Control Protocol (DCCP) [RFC4340],
   all receivers have always been suitable.

   This document presents the L4S architecture.  It consists of three
   components: network support to isolate L4S traffic from Classic
   traffic; protocol features that allow network elements to identify
   L4S traffic; and host support for L4S congestion controls.  The
   protocol is defined separately in [RFC9331] as an experimental change
   to Explicit Congestion Notification (ECN).  This document describes
   and justifies the component parts and how they interact to provide
   the low latency, low loss, and scalable Internet service.  It also
   details the approach to incremental deployment, as briefly summarized
   above.

1.1.  Document Roadmap

   This document describes the L4S architecture in three passes.  First,
   the brief overview in Section 2 gives the very high-level idea and
   states the main components with minimal rationale.  This is only
   intended to give some context for the terminology definitions that
   follow in Section 3 and to explain the structure of the rest of the
   document.  Then, Section 4 goes into more detail on each component
   with some rationale but still mostly stating what the architecture
   is, rather than why.  Finally, Section 5 justifies why each element
   of the solution was chosen (Section 5.1) and why these choices were
   different from other solutions (Section 5.2).

   After the architecture has been described, Section 6 clarifies its
   applicability by describing the applications and use cases that
   motivated the design, the challenges applying the architecture to
   various link technologies, and various incremental deployment models
   (including the two main deployment topologies, different sequences
   for incremental deployment, and various interactions with preexisting
   approaches).  The document ends with the usual tailpieces, including
   extensive discussion of traffic policing and other security
   considerations in Section 8.

2.  L4S Architecture Overview

   Below, we outline the three main components to the L4S architecture:
   1) the Scalable congestion control on the sending host; 2) the AQM at
   the network bottleneck; and 3) the protocol between them.

   But first, the main point to grasp is that low latency is not
   provided by the network; low latency results from the careful
   behaviour of the Scalable congestion controllers used by L4S senders.
   The network does have a role, primarily to isolate the low latency of
   the carefully behaving L4S traffic from the higher queuing delay
   needed by traffic with preexisting Classic behaviour.  The network
   also alters the way it signals queue growth to the transport.  It
   uses the Explicit Congestion Notification (ECN) protocol, but it
   signals the very start of queue growth immediately, without the
   smoothing delay typical of Classic AQMs.  Because ECN support is
   essential for L4S, senders use the ECN field as the protocol that
   allows the network to identify which packets are L4S and which are
   Classic.

   1)  Host:

       Scalable congestion controls already exist.  They solve the
       scaling problem with Classic congestion controls, such as Reno or
       CUBIC.  Because flow rate has scaled since TCP congestion control
       was first designed in 1988, assuming the flow lasts long enough,
       it now takes hundreds of round trips (and growing) to recover
       after a congestion signal (whether a loss or an ECN mark), as
       shown in the examples in Section 5.1 and [RFC3649].  Therefore,
       control of queuing and utilization becomes very slack, and the
       slightest disturbances (e.g., from new flows starting) prevent a
       high rate from being attained.

       With a Scalable congestion control, the average time from one
       congestion signal to the next (the recovery time) remains
       invariant as flow rate scales, all other factors being equal.
       This maintains the same degree of control over queuing and
       utilization, whatever the flow rate, as well as ensuring that
       high throughput is more robust to disturbances.  The Scalable
       control used most widely (in controlled environments) is DCTCP
       [RFC8257], which has been implemented and deployed in Windows
       Server Editions (since 2012), in Linux, and in FreeBSD.  Although
       DCTCP as-is functions well over wide-area round-trip times
       (RTTs), most implementations lack certain safety features that
       would be necessary for use outside controlled environments, like
       data centres (see Section 6.4.3).  Therefore, Scalable congestion
       control needs to be implemented in TCP and other transport
       protocols (QUIC, Stream Control Transmission Protocol (SCTP),
       RTP/RTCP, RTP Media Congestion Avoidance Techniques (RMCAT),
       etc.).  Indeed, between the present document being drafted and
       published, the following Scalable congestion controls were
       implemented: Prague over TCP and QUIC [PRAGUE-CC] [PragueLinux],
       an L4S variant of the RMCAT SCReAM controller [SCReAM-L4S], and
       the L4S ECN part of Bottleneck Bandwidth and Round-trip
       propagation time (BBRv2) [BBRv2] intended for TCP and QUIC
       transports.

   2)  Network:

       L4S traffic needs to be isolated from the queuing latency of
       Classic traffic.  One queue per application flow (FQ) is one way
       to achieve this, e.g., FQ-CoDel [RFC8290].  However, using just
       two queues is sufficient and does not require inspection of
       transport layer headers in the network, which is not always
       possible (see Section 5.2).  With just two queues, it might seem
       impossible to know how much capacity to schedule for each queue
       without inspecting how many flows at any one time are using each.
       And it would be undesirable to arbitrarily divide access network
       capacity into two partitions.  The Dual-Queue Coupled AQM was
       developed as a minimal complexity solution to this problem.  It
       acts like a 'semi-permeable' membrane that partitions latency but
       not bandwidth.  As such, the two queues are for transitioning
       from Classic to L4S behaviour, not bandwidth prioritization.

       Section 4 gives a high-level explanation of how the per-flow
       queue (FQ) and DualQ variants of L4S work, and [RFC9332] gives a
       full explanation of the DualQ Coupled AQM framework.  A specific
       marking algorithm is not mandated for L4S AQMs.  Appendices of
       [RFC9332] give non-normative examples that have been implemented
       and evaluated and give recommended default parameter settings.
       It is expected that L4S experiments will improve knowledge of
       parameter settings and whether the set of marking algorithms
       needs to be limited.

   3)  Protocol:

       A sending host needs to distinguish L4S and Classic packets with
       an identifier so that the network can classify them into their
       separate treatments.  The L4S identifier spec [RFC9331] concludes
       that all alternatives involve compromises, but the ECT(1) and
       Congestion Experienced (CE) codepoints of the ECN field represent
       a workable solution.  As already explained, the network also uses
       ECN to immediately signal the very start of queue growth to the
       transport.

3.  Terminology

   Classic Congestion Control:  A congestion control behaviour that can
      coexist with standard Reno [RFC5681] without causing significantly
      negative impact on its flow rate [RFC5033].  The scaling problem
      with Classic congestion control is explained, with examples, in
      Section 5.1 and in [RFC3649].

   Scalable Congestion Control:  A congestion control where the average
      time from one congestion signal to the next (the recovery time)
      remains invariant as flow rate scales, all other factors being
      equal.  For instance, DCTCP averages 2 congestion signals per
      round trip, whatever the flow rate, as do other recently developed
      Scalable congestion controls, e.g., Relentless TCP [RELENTLESS],
      Prague for TCP and QUIC [PRAGUE-CC] [PragueLinux], BBRv2 [BBRv2]
      [BBR-CC], and the L4S variant of SCReAM for real-time media
      [SCReAM-L4S] [RFC8298].  See Section 4.3 of [RFC9331] for more
      explanation.

   Classic Service:  The Classic service is intended for all the
      congestion control behaviours that coexist with Reno [RFC5681]
      (e.g., Reno itself, CUBIC [RFC8312], Compound [CTCP], and TFRC
      [RFC5348]).  The term 'Classic queue' means a queue providing the
      Classic service.

   Low Latency, Low Loss, and Scalable throughput (L4S) service:  The
      'L4S' service is intended for traffic from Scalable congestion
      control algorithms, such as the Prague congestion control
      [PRAGUE-CC], which was derived from DCTCP [RFC8257].  The L4S
      service is for more general traffic than just Prague -- it allows
      the set of congestion controls with similar scaling properties to
      Prague to evolve, such as the examples listed above (Relentless,
      SCReAM, etc.).  The term 'L4S queue' means a queue providing the
      L4S service.

      The terms Classic or L4S can also qualify other nouns, such as
      'queue', 'codepoint', 'identifier', 'classification', 'packet',
      and 'flow'.  For example, an L4S packet means a packet with an L4S
      identifier sent from an L4S congestion control.

      Both Classic and L4S services can cope with a proportion of
      unresponsive or less-responsive traffic as well but, in the L4S
      case, its rate has to be smooth enough or low enough to not build
      a queue (e.g., DNS, Voice over IP (VoIP), game sync datagrams,
      etc.).

   Reno-friendly:  The subset of Classic traffic that is friendly to the
      standard Reno congestion control defined for TCP in [RFC5681].
      The TFRC spec [RFC5348] indirectly implies that 'friendly' is
      defined as "generally within a factor of two of the sending rate
      of a TCP flow under the same conditions".  Reno-friendly is used
      here in place of 'TCP-friendly', given the latter has become
      imprecise, because the TCP protocol is now used with so many
      different congestion control behaviours, and Reno is used in non-
      TCP transports, such as QUIC [RFC9000].

   Classic ECN:  The original Explicit Congestion Notification (ECN)
      protocol [RFC3168] that requires ECN signals to be treated as
      equivalent to drops, both when generated in the network and when
      responded to by the sender.

      For L4S, the names used for the four codepoints of the 2-bit IP-
      ECN field are unchanged from those defined in the ECN spec
      [RFC3168], i.e., Not-ECT, ECT(0), ECT(1), and CE, where ECT stands
      for ECN-Capable Transport and CE stands for Congestion
      Experienced.  A packet marked with the CE codepoint is termed
      'ECN-marked' or sometimes just 'marked' where the context makes
      ECN obvious.

   Site:  A home, mobile device, small enterprise, or campus where the
      network bottleneck is typically the access link to the site.  Not
      all network arrangements fit this model, but it is a useful,
      widely applicable generalization.

   Traffic Policing:  Limiting traffic by dropping packets or shifting
      them to a lower service class (as opposed to introducing delay,
      which is termed 'traffic shaping').  Policing can involve limiting
      the average rate and/or burst size.  Policing focused on limiting
      queuing but not the average flow rate is termed 'congestion
      policing', 'latency policing', 'burst policing', or 'queue
      protection' in this document.  Otherwise, the term rate policing
      is used.

4.  L4S Architecture Components

   The L4S architecture is composed of the elements in the following
   three subsections.

4.1.  Protocol Mechanisms

   The L4S architecture involves: a) unassignment of the previous use of
   the identifier; b) reassignment of the same identifier; and c)
   optional further identifiers:

   a.  An essential aspect of a Scalable congestion control is the use
       of explicit congestion signals.  Classic ECN [RFC3168] requires
       an ECN signal to be treated as equivalent to drop, both when it
       is generated in the network and when it is responded to by hosts.
       L4S needs networks and hosts to support a more fine-grained
       meaning for each ECN signal that is less severe than a drop, so
       that the L4S signals:

       *  can be much more frequent and

       *  can be signalled immediately, without the significant delay
          required to smooth out fluctuations in the queue.

       To enable L4S, the Standards Track Classic ECN spec [RFC3168] has
       had to be updated to allow L4S packets to depart from the
       'equivalent-to-drop' constraint.  [RFC8311] is a Standards Track
       update to relax specific requirements in [RFC3168] (and certain
       other Standards Track RFCs), which clears the way for the
       experimental changes proposed for L4S.  Also, the ECT(1)
       codepoint was previously assigned as the experimental ECN nonce
       [RFC3540], which [RFC8311] recategorizes as historic to make the
       codepoint available again.

   b.  [RFC9331] specifies that ECT(1) is used as the identifier to
       classify L4S packets into a separate treatment from Classic
       packets.  This satisfies the requirement for identifying an
       alternative ECN treatment in [RFC4774].

       The CE codepoint is used to indicate Congestion Experienced by
       both L4S and Classic treatments.  This raises the concern that a
       Classic AQM earlier on the path might have marked some ECT(0)
       packets as CE.  Then, these packets will be erroneously
       classified into the L4S queue.  Appendix B of [RFC9331] explains
       why five unlikely eventualities all have to coincide for this to
       have any detrimental effect, which even then would only involve a
       vanishingly small likelihood of a spurious retransmission.

   c.  A network operator might wish to include certain unresponsive,
       non-L4S traffic in the L4S queue if it is deemed to be paced
       smoothly enough and at a low enough rate not to build a queue,
       for instance, VoIP, low rate datagrams to sync online games,
       relatively low rate application-limited traffic, DNS, Lightweight
       Directory Access Protocol (LDAP), etc.  This traffic would need
       to be tagged with specific identifiers, e.g., a low-latency
       Diffserv codepoint such as Expedited Forwarding (EF) [RFC3246],
       Non-Queue-Building (NQB) [NQB-PHB], or operator-specific
       identifiers.

4.2.  Network Components

   The L4S architecture aims to provide low latency without the _need_
   for per-flow operations in network components.  Nonetheless, the
   architecture does not preclude per-flow solutions.  The following
   bullets describe the known arrangements: a) the DualQ Coupled AQM
   with an L4S AQM in one queue coupled from a Classic AQM in the other;
   b) per-flow queues with an instance of a Classic and an L4S AQM in
   each queue; and c) Dual queues with per-flow AQMs but no per-flow
   queues:

   a.  The Dual-Queue Coupled AQM (illustrated in Figure 1) achieves the
       'semi-permeable' membrane property mentioned earlier as follows:

       *  Latency isolation: Two separate queues are used to isolate L4S
          queuing delay from the larger queue that Classic traffic needs
          to maintain full utilization.

       *  Bandwidth pooling: The two queues act as if they are a single
          pool of bandwidth in which flows of either type get roughly
          equal throughput without the scheduler needing to identify any
          flows.  This is achieved by having an AQM in each queue, but
          the Classic AQM provides a congestion signal to both queues in
          a manner that ensures a consistent response from the two
          classes of congestion control.  Specifically, the Classic AQM
          generates a drop/mark probability based on congestion in its
          own queue, which it uses both to drop/mark packets in its own
          queue and to affect the marking probability in the L4S queue.
          The strength of the coupling of the congestion signalling
          between the two queues is enough to make the L4S flows slow
          down to leave the right amount of capacity for the Classic
          flows (as they would if they were the same type of traffic
          sharing the same queue).

       Then, the scheduler can serve the L4S queue with priority
       (denoted by the '1' on the higher priority input), because the
       L4S traffic isn't offering up enough traffic to use all the
       priority that it is given.  Therefore:

       *  for latency isolation on short timescales (sub-round-trip),
          the prioritization of the L4S queue protects its low latency
          by allowing bursts to dissipate quickly;

       *  but for bandwidth pooling on longer timescales (round-trip and
          longer), the Classic queue creates an equal and opposite
          pressure against the L4S traffic to ensure that neither has
          priority when it comes to bandwidth -- the tension between
          prioritizing L4S and coupling the marking from the Classic AQM
          results in approximate per-flow fairness.

       To protect against the prioritization of persistent L4S traffic
       deadlocking the Classic queue for a while in some
       implementations, it is advisable for the priority to be
       conditional, not strict (see Appendix A of the DualQ spec
       [RFC9332]).

       When there is no Classic traffic, the L4S queue's own AQM comes
       into play.  It starts congestion marking with a very shallow
       queue, so L4S traffic maintains very low queuing delay.

       If either queue becomes persistently overloaded, drop of some
       ECN-capable packets is introduced, as recommended in Section 7 of
       the ECN spec [RFC3168] and Section 4.2.1 of the AQM
       recommendations [RFC7567].  The trade-offs with different
       approaches are discussed in Section 4.2.3 of the DualQ spec
       [RFC9332] (not shown in the figure here).

       The Dual-Queue Coupled AQM has been specified as generically as
       possible [RFC9332] without specifying the particular AQMs to use
       in the two queues so that designers are free to implement diverse
       ideas.  Informational appendices in that document give pseudocode
       examples of two different specific AQM approaches: one called
       DualPI2 (pronounced Dual PI Squared) [DualPI2Linux] that uses the
       PI2 variant of PIE and a zero-config variant of Random Early
       Detection (RED) called Curvy RED.  A DualQ Coupled AQM based on
       PIE has also been specified and implemented for Low Latency
       DOCSIS [DOCSIS3.1].

                     (3)                  (2)
                     .-------^------..------------^------------------.
        ,-(1)-----.                               _____
       ; ________  :            L4S  -------.    |     |
       :|Scalable| :               _\      ||__\_|mark |
       :| sender | :  __________  / /      ||  / |_____|\   _________
       :|________|\; |          |/   -------'       ^    \1|condit'nl|
        `---------'\_|  IP-ECN  |          Coupling :     \|priority |_\
         ________  / |Classifier|                   :     /|scheduler| /
        |Classic |/  |__________|\   -------.     __:__  / |_________|
        | sender |                \_\ || | ||__\_|mark/|/
        |________|                  / || | ||  / |drop |
                             Classic -------'    |_____|

       (1) Scalable sending host
       (2) Isolation in separate network queues
       (3) Packet identification protocol

           Figure 1: Components of an L4S DualQ Coupled AQM Solution

   b.  Per-Flow Queues and AQMs: A scheduler with per-flow queues, such
       as FQ-CoDel or FQ-PIE, can be used for L4S.  For instance, within
       each queue of an FQ-CoDel system, as well as a CoDel AQM, there
       is typically also the option of ECN marking at an immediate
       (unsmoothed) shallow threshold to support use in data centres
       (see Section 5.2.7 of the FQ-CoDel spec [RFC8290]).  In Linux,
       this has been modified so that the shallow threshold can be
       solely applied to ECT(1) packets [FQ_CoDel_Thresh].  Then, if
       there is a flow of Not-ECT or ECT(0) packets in the per-flow
       queue, the Classic AQM (e.g., CoDel) is applied; whereas, if
       there is a flow of ECT(1) packets in the queue, the shallower
       (typically sub-millisecond) threshold is applied.  In addition,
       ECT(0) and Not-ECT packets could potentially be classified into a
       separate flow queue from ECT(1) and CE packets to avoid them
       mixing if they share a common flow identifier (e.g., in a VPN).

   c.  Dual queues but per-flow AQMs: It should also be possible to use
       dual queues for isolation but with per-flow marking to control
       flow rates (instead of the coupled per-queue marking of the Dual-
       Queue Coupled AQM).  One of the two queues would be for isolating
       L4S packets, which would be classified by the ECN codepoint.
       Flow rates could be controlled by flow-specific marking.  The
       policy goal of the marking could be to differentiate flow rates
       (e.g., [Nadas20], which requires additional signalling of a per-
       flow 'value') or to equalize flow rates (perhaps in a similar way
       to Approx Fair CoDel [AFCD] [CODEL-APPROX-FAIR] but with two
       queues not one).

       Note that, whenever the term 'DualQ' is used loosely without
       saying whether marking is per queue or per flow, it means a dual-
       queue AQM with per-queue marking.

4.3.  Host Mechanisms

   The L4S architecture includes two main mechanisms in the end host
   that we enumerate next:

   a.  Scalable congestion control at the sender: Section 2 defines a
       Scalable congestion control as one where the average time from
       one congestion signal to the next (the recovery time) remains
       invariant as flow rate scales, all other factors being equal.
       DCTCP is the most widely used example.  It has been documented as
       an informational record of the protocol currently in use in
       controlled environments [RFC8257].  A list of safety and
       performance improvements for a Scalable congestion control to be
       usable on the public Internet has been drawn up (see the so-
       called 'Prague L4S requirements' in Appendix A of [RFC9331]).
       The subset that involve risk of harm to others have been captured
       as normative requirements in Section 4 of [RFC9331].  TCP Prague
       [PRAGUE-CC] has been implemented in Linux as a reference
       implementation to address these requirements [PragueLinux].

       Transport protocols other than TCP use various congestion
       controls that are designed to be friendly with Reno.  Before they
       can use the L4S service, they will need to be updated to
       implement a Scalable congestion response, which they will have to
       indicate by using the ECT(1) codepoint.  Scalable variants are
       under consideration for more recent transport protocols (e.g.,
       QUIC), and the L4S ECN part of BBRv2 [BBRv2] [BBR-CC] is a
       Scalable congestion control intended for the TCP and QUIC
       transports, amongst others.  Also, an L4S variant of the RMCAT
       SCReAM controller [RFC8298] has been implemented [SCReAM-L4S] for
       media transported over RTP.

       Section 4.3 of the L4S ECN spec [RFC9331] defines Scalable
       congestion control in more detail and specifies the requirements
       that an L4S Scalable congestion control has to comply with.

   b.  The ECN feedback in some transport protocols is already
       sufficiently fine-grained for L4S (specifically DCCP [RFC4340]
       and QUIC [RFC9000]).  But others either require updates or are in
       the process of being updated:

       *  For the case of TCP, the feedback protocol for ECN embeds the
          assumption from Classic ECN [RFC3168] that an ECN mark is
          equivalent to a drop, making it unusable for a Scalable TCP.
          Therefore, the implementation of TCP receivers will have to be
          upgraded [RFC7560].  Work to standardize and implement more
          accurate ECN feedback for TCP (AccECN) is in progress [ACCECN]
          [PragueLinux].

       *  ECN feedback was only roughly sketched in the appendix of the
          now obsoleted second specification of SCTP [RFC4960], while a
          fuller specification was proposed in a long-expired document
          [ECN-SCTP].  A new design would need to be implemented and
          deployed before SCTP could support L4S.

       *  For RTP, sufficient ECN feedback was defined in [RFC6679], but
          [RFC8888] defines the latest Standards Track improvements.

5.  Rationale

5.1.  Why These Primary Components?

   Explicit congestion signalling (protocol):  Explicit congestion
      signalling is a key part of the L4S approach.  In contrast, use of
      drop as a congestion signal creates tension because drop is both
      an impairment (less would be better) and a useful signal (more
      would be better):

      *  Explicit congestion signals can be used many times per round
         trip to keep tight control without any impairment.  Under heavy
         load, even more explicit signals can be applied so that the
         queue can be kept short whatever the load.  In contrast,
         Classic AQMs have to introduce very high packet drop at high
         load to keep the queue short.  By using ECN, an L4S congestion
         control's sawtooth reduction can be smaller and therefore
         return to the operating point more often, without worrying that
         more sawteeth will cause more signals.  The consequent smaller
         amplitude sawteeth fit between an empty queue and a very
         shallow marking threshold (~1 ms in the public Internet), so
         queue delay variation can be very low, without risk of
         underutilization.

      *  Explicit congestion signals can be emitted immediately to track
         fluctuations of the queue.  L4S shifts smoothing from the
         network to the host.  The network doesn't know the round-trip
         times (RTTs) of any of the flows.  So if the network is
         responsible for smoothing (as in the Classic approach), it has
         to assume a worst case RTT, otherwise long RTT flows would
         become unstable.  This delays Classic congestion signals by
         100-200 ms.  In contrast, each host knows its own RTT.  So, in
         the L4S approach, the host can smooth each flow over its own
         RTT, introducing no more smoothing delay than strictly
         necessary (usually only a few milliseconds).  A host can also
         choose not to introduce any smoothing delay if appropriate,
         e.g., during flow start-up.

      Neither of the above are feasible if explicit congestion
      signalling has to be considered 'equivalent to drop' (as was
      required with Classic ECN [RFC3168]), because drop is an
      impairment as well as a signal.  So drop cannot be excessively
      frequent, and drop cannot be immediate; otherwise, too many drops
      would turn out to have been due to only a transient fluctuation in
      the queue that would not have warranted dropping a packet in
      hindsight.  Therefore, in an L4S AQM, the L4S queue uses a new L4S
      variant of ECN that is not equivalent to drop (see Section 5.2 of
      the L4S ECN spec [RFC9331]), while the Classic queue uses either
      Classic ECN [RFC3168] or drop, which are still equivalent to each
      other.

      Before Classic ECN was standardized, there were various proposals
      to give an ECN mark a different meaning from drop.  However, there
      was no particular reason to agree on any one of the alternative
      meanings, so 'equivalent to drop' was the only compromise that
      could be reached.  [RFC3168] contains a statement that:

         An environment where all end nodes were ECN-Capable could
          allow new criteria to be developed for setting the CE
          codepoint, and new congestion control mechanisms for end-node
          reaction to CE packets.  However, this is a research issue,
          and as such is not addressed in this document.

   Latency isolation (network):  L4S congestion controls keep queue
      delay low, whereas Classic congestion controls need a queue of the
      order of the RTT to avoid underutilization.  One queue cannot have
      two lengths; therefore, L4S traffic needs to be isolated in a
      separate queue (e.g., DualQ) or queues (e.g., FQ).

   Coupled congestion notification:  Coupling the congestion
      notification between two queues as in the DualQ Coupled AQM is not
      necessarily essential, but it is a simple way to allow senders to
      determine their rate packet by packet, rather than be overridden
      by a network scheduler.  An alternative is for a network scheduler
      to control the rate of each application flow (see the discussion
      in Section 5.2).

   L4S packet identifier (protocol):  Once there are at least two
      treatments in the network, hosts need an identifier at the IP
      layer to distinguish which treatment they intend to use.

   Scalable congestion notification:  A Scalable congestion control in
      the host keeps the signalling frequency from the network high,
      whatever the flow rate, so that queue delay variations can be
      small when conditions are stable, and rate can track variations in
      available capacity as rapidly as possible otherwise.

   Low loss:  Latency is not the only concern of L4S.  The 'Low Loss'
      part of the name denotes that L4S generally achieves zero
      congestion loss due to its use of ECN.  Otherwise, loss would
      itself cause delay, particularly for short flows, due to
      retransmission delay [RFC2884].

   Scalable throughput:  The 'Scalable throughput' part of the name
      denotes that the per-flow throughput of Scalable congestion
      controls should scale indefinitely, avoiding the imminent scaling
      problems with Reno-friendly congestion control algorithms
      [RFC3649].  It was known when TCP congestion avoidance was first
      developed in 1988 that it would not scale to high bandwidth-delay
      products (see footnote 6 in [TCP-CA]).  Today, regular broadband
      flow rates over WAN distances are already beyond the scaling range
      of Classic Reno congestion control.  So 'less unscalable' CUBIC
      [RFC8312] and Compound [CTCP] variants of TCP have been
      successfully deployed.  However, these are now approaching their
      scaling limits.

      For instance, we will consider a scenario with a maximum RTT of 30
      ms at the peak of each sawtooth.  As Reno packet rate scales 8
      times from 1,250 to 10,000 packet/s (from 15 to 120 Mb/s with 1500
      B packets), the time to recover from a congestion event rises
      proportionately by 8 times as well, from 422 ms to 3.38 s.  It is
      clearly problematic for a congestion control to take multiple
      seconds to recover from each congestion event.  CUBIC [RFC8312]
      was developed to be less unscalable, but it is approaching its
      scaling limit; with the same max RTT of 30 ms, at 120 Mb/s, CUBIC
      is still fully in its Reno-friendly mode, so it takes about 4.3 s
      to recover.  However, once flow rate scales by 8 times again to
      960 Mb/s it enters true CUBIC mode, with a recovery time of 12.2
      s.  From then on, each further scaling by 8 times doubles CUBIC's
      recovery time (because the cube root of 8 is 2), e.g., at 7.68 Gb/
      s, the recovery time is 24.3 s.  In contrast, a Scalable
      congestion control like DCTCP or Prague induces 2 congestion
      signals per round trip on average, which remains invariant for any
      flow rate, keeping dynamic control very tight.

      For a feel of where the global average lone-flow download sits on
      this scale at the time of writing (2021), according to [BDPdata],
      the global average fixed access capacity was 103 Mb/s in 2020 and
      the average base RTT to a CDN was 25 to 34 ms in 2019.  Averaging
      of per-country data was weighted by Internet user population (data
      collected globally is necessarily of variable quality, but the
      paper does double-check that the outcome compares well against a
      second source).  So a lone CUBIC flow would at best take about 200
      round trips (5 s) to recover from each of its sawtooth reductions,
      if the flow even lasted that long.  This is described as 'at best'
      because it assumes everyone uses an AQM, whereas in reality, most
      users still have a (probably bloated) tail-drop buffer.  In the
      tail-drop case, the likely average recovery time would be at least
      4 times 5 s, if not more, because RTT under load would be at least
      double that of an AQM, and the recovery time of Reno-friendly
      flows depends on the square of RTT.

      Although work on scaling congestion controls tends to start with
      TCP as the transport, the above is not intended to exclude other
      transports (e.g., SCTP and QUIC) or less elastic algorithms (e.g.,
      RMCAT), which all tend to adopt the same or similar developments.

5.2.  What L4S Adds to Existing Approaches

   All the following approaches address some part of the same problem
   space as L4S.  In each case, it is shown that L4S complements them or
   improves on them, rather than being a mutually exclusive alternative:

   Diffserv:  Diffserv addresses the problem of bandwidth apportionment
      for important traffic as well as queuing latency for delay-
      sensitive traffic.  Of these, L4S solely addresses the problem of
      queuing latency.  Diffserv will still be necessary where important
      traffic requires priority (e.g., for commercial reasons or for
      protection of critical infrastructure traffic) -- see
      [L4S-DIFFSERV].  Nonetheless, the L4S approach can provide low
      latency for all traffic within each Diffserv class (including the
      case where there is only the one default Diffserv class).

      Also, Diffserv can only provide a latency benefit if a small
      subset of the traffic on a bottleneck link requests low latency.
      As already explained, it has no effect when all the applications
      in use at one time at a single site (e.g., a home, small business,
      or mobile device) require low latency.  In contrast, because L4S
      works for all traffic, it needs none of the management baggage
      (traffic policing or traffic contracts) associated with favouring
      some packets over others.  This lack of management baggage ought
      to give L4S a better chance of end-to-end deployment.

      In particular, if networks do not trust end systems to identify
      which packets should be favoured, they assign packets to Diffserv
      classes themselves.  However, the techniques available to such
      networks, like inspection of flow identifiers or deeper inspection
      of application signatures, do not always sit well with encryption
      of the layers above IP [RFC8404].  In these cases, users can have
      either privacy or Quality of Service (QoS), but not both.

      As with Diffserv, the L4S identifier is in the IP header.  But, in
      contrast to Diffserv, the L4S identifier does not convey a want or
      a need for a certain level of quality.  Rather, it promises a
      certain behaviour (Scalable congestion response), which networks
      can objectively verify if they need to.  This is because low delay
      depends on collective host behaviour, whereas bandwidth priority
      depends on network behaviour.

   State-of-the-art AQMs:  AQMs for Classic traffic, such as PIE and FQ-
      CoDel, give a significant reduction in queuing delay relative to
      no AQM at all.  L4S is intended to complement these AQMs and
      should not distract from the need to deploy them as widely as
      possible.  Nonetheless, AQMs alone cannot reduce queuing delay too
      far without significantly reducing link utilization, because the
      root cause of the problem is on the host -- where Classic
      congestion controls use large sawtoothing rate variations.  The
      L4S approach resolves this tension between delay and utilization
      by enabling hosts to minimize the amplitude of their sawteeth.  A
      single-queue Classic AQM is not sufficient to allow hosts to use
      small sawteeth for two reasons: i) smaller sawteeth would not get
      lower delay in an AQM designed for larger amplitude Classic
      sawteeth, because a queue can only have one length at a time and
      ii) much smaller sawteeth implies much more frequent sawteeth, so
      L4S flows would drive a Classic AQM into a high level of ECN-
      marking, which would appear as heavy congestion to Classic flows,
      which in turn would greatly reduce their rate as a result (see
      Section 6.4.4).

   Per-flow queuing or marking:  Similarly, per-flow approaches, such as
      FQ-CoDel or Approx Fair CoDel [AFCD], are not incompatible with
      the L4S approach.  However, per-flow queuing alone is not enough
      -- it only isolates the queuing of one flow from others, not from
      itself.  Per-flow implementations need to have support for
      Scalable congestion control added, which has already been done for
      FQ-CoDel in Linux (see Section 5.2.7 of [RFC8290] and
      [FQ_CoDel_Thresh]).  Without this simple modification, per-flow
      AQMs, like FQ-CoDel, would still not be able to support
      applications that need both very low delay and high bandwidth,
      e.g., video-based control of remote procedures or interactive
      cloud-based video (see Note 1 below).

      Although per-flow techniques are not incompatible with L4S, it is
      important to have the DualQ alternative.  This is because handling
      end-to-end (layer 4) flows in the network (layer 3 or 2) precludes
      some important end-to-end functions.  For instance:

      a.  Per-flow forms of L4S, like FQ-CoDel, are incompatible with
          full end-to-end encryption of transport layer identifiers for
          privacy and confidentiality (e.g., IPsec or encrypted VPN
          tunnels, as opposed to DTLS over UDP), because they require
          packet inspection to access the end-to-end transport flow
          identifiers.

          In contrast, the DualQ form of L4S requires no deeper
          inspection than the IP layer.  So as long as operators take
          the DualQ approach, their users can have both very low queuing
          delay and full end-to-end encryption [RFC8404].

      b.  With per-flow forms of L4S, the network takes over control of
          the relative rates of each application flow.  Some see it as
          an advantage that the network will prevent some flows running
          faster than others.  Others consider it an inherent part of
          the Internet's appeal that applications can control their rate
          while taking account of the needs of others via congestion
          signals.  They maintain that this has allowed applications
          with interesting rate behaviours to evolve, for instance: i) a
          variable bit-rate video that varies around an equal share,
          rather than being forced to remain equal at every instant or
          ii) end-to-end scavenger behaviours [RFC6817] that use less
          than an equal share of capacity [LEDBAT_AQM].

          The L4S architecture does not require the IETF to commit to
          one approach over the other, because it supports both so that
          the 'market' can decide.  Nonetheless, in the spirit of 'Do
          one thing and do it well' [McIlroy78], the DualQ option
          provides low delay without prejudging the issue of flow-rate
          control.  Then, flow rate policing can be added separately if
          desired.  In contrast to scheduling, a policer would allow
          application control up to a point, but the network would still
          be able to set the point at which it intervened to prevent one
          flow completely starving another.

      Note:

      1.  It might seem that self-inflicted queuing delay within a per-
          flow queue should not be counted, because if the delay wasn't
          in the network, it would just shift to the sender.  However,
          modern adaptive applications, e.g., HTTP/2 [RFC9113] or some
          interactive media applications (see Section 6.1), can keep low
          latency objects at the front of their local send queue by
          shuffling priorities of other objects dependent on the
          progress of other transfers (for example, see [lowat]).  They
          cannot shuffle objects once they have released them into the
          network.

   Alternative Back-off ECN (ABE):  Here again, L4S is not an
      alternative to ABE but a complement that introduces much lower
      queuing delay.  ABE [RFC8511] alters the host behaviour in
      response to ECN marking to utilize a link better and give ECN
      flows faster throughput.  It uses ECT(0) and assumes the network
      still treats ECN and drop the same.  Therefore, ABE exploits any
      lower queuing delay that AQMs can provide.  But, as explained
      above, AQMs still cannot reduce queuing delay too much without
      losing link utilization (to allow for other, non-ABE, flows).

   BBR:  Bottleneck Bandwidth and Round-trip propagation time (BBR)
      [BBR-CC] controls queuing delay end-to-end without needing any
      special logic in the network, such as an AQM.  So it works pretty
      much on any path.  BBR keeps queuing delay reasonably low, but
      perhaps not quite as low as with state-of-the-art AQMs, such as
      PIE or FQ-CoDel, and certainly nowhere near as low as with L4S.
      Queuing delay is also not consistently low, due to BBR's regular
      bandwidth probing spikes and its aggressive flow start-up phase.

      L4S complements BBR.  Indeed, BBRv2 can use L4S ECN where
      available and a Scalable L4S congestion control behaviour in
      response to any ECN signalling from the path [BBRv2].  The L4S ECN
      signal complements the delay-based congestion control aspects of
      BBR with an explicit indication that hosts can use, both to
      converge on a fair rate and to keep below a shallow queue target
      set by the network.  Without L4S ECN, both these aspects need to
      be assumed or estimated.

6.  Applicability

6.1.  Applications

   A transport layer that solves the current latency issues will provide
   new service, product, and application opportunities.

   With the L4S approach, the following existing applications also
   experience significantly better quality of experience under load:

   *  gaming, including cloud-based gaming;

   *  VoIP;

   *  video conferencing;

   *  web browsing;

   *  (adaptive) video streaming; and

   *  instant messaging.

   The significantly lower queuing latency also enables some interactive
   application functions to be offloaded to the cloud that would hardly
   even be usable today, including:

   *  cloud-based interactive video and

   *  cloud-based virtual and augmented reality.

   The above two applications have been successfully demonstrated with
   L4S, both running together over a 40 Mb/s broadband access link
   loaded up with the numerous other latency-sensitive applications in
   the previous list, as well as numerous downloads, with all sharing
   the same bottleneck queue simultaneously [L4Sdemo16]
   [L4Sdemo16-Video].  For the former, a panoramic video of a football
   stadium could be swiped and pinched so that, on the fly, a proxy in
   the cloud could generate a sub-window of the match video under the
   finger-gesture control of each user.  For the latter, a virtual
   reality headset displayed a viewport taken from a 360-degree camera
   in a racing car.  The user's head movements controlled the viewport
   extracted by a cloud-based proxy.  In both cases, with a 7 ms end-to-
   end base delay, the additional queuing delay of roughly 1 ms was so
   low that it seemed the video was generated locally.

   Using a swiping finger gesture or head movement to pan a video are
   extremely latency-demanding actions -- far more demanding than VoIP
   -- because human vision can detect extremely low delays of the order
   of single milliseconds when delay is translated into a visual lag
   between a video and a reference point (the finger or the orientation
   of the head sensed by the balance system in the inner ear, i.e., the
   vestibular system).  With an alternative AQM, the video noticeably
   lagged behind the finger gestures and head movements.

   Without the low queuing delay of L4S, cloud-based applications like
   these would not be credible without significantly more access-network
   bandwidth (to deliver all possible areas of the video that might be
   viewed) and more local processing, which would increase the weight
   and power consumption of head-mounted displays.  When all interactive
   processing can be done in the cloud, only the data to be rendered for
   the end user needs to be sent.

   Other low latency high bandwidth applications, such as:

   *  interactive remote presence and

   *  video-assisted remote control of machinery or industrial processes

   are not credible at all without very low queuing delay.  No amount of
   extra access bandwidth or local processing can make up for lost time.

6.2.  Use Cases

   The following use cases for L4S are being considered by various
   interested parties:

   *  where the bottleneck is one of various types of access network,
      e.g., DSL, Passive Optical Networks (PONs), DOCSIS cable, mobile,
      satellite; or where it's a Wi-Fi link (see Section 6.3 for some
      technology-specific details)

   *  private networks of heterogeneous data centres, where there is no
      single administrator that can arrange for all the simultaneous
      changes to senders, receivers, and networks needed to deploy
      DCTCP:

      -  a set of private data centres interconnected over a wide area
         with separate administrations but within the same company

      -  a set of data centres operated by separate companies
         interconnected by a community of interest network (e.g., for
         the finance sector)

      -  multi-tenant (cloud) data centres where tenants choose their
         operating system stack (Infrastructure as a Service (IaaS))

   *  different types of transport (or application) congestion control:

      -  elastic (TCP/SCTP);

      -  real-time (RTP, RMCAT); and

      -  query-response (DNS/LDAP).

   *  where low delay QoS is required but without inspecting or
      intervening above the IP layer [RFC8404]:

      -  Mobile and other networks have tended to inspect higher layers
         in order to guess application QoS requirements.  However, with
         growing demand for support of privacy and encryption, L4S
         offers an alternative.  There is no need to select which
         traffic to favour for queuing when L4S can give favourable
         queuing to all traffic.

   *  If queuing delay is minimized, applications with a fixed delay
      budget can communicate over longer distances or via more
      circuitous paths, e.g., longer chains of service functions
      [RFC7665] or of onion routers.

   *  If delay jitter is minimized, it is possible to reduce the
      dejitter buffers on the receiving end of video streaming, which
      should improve the interactive experience.

6.3.  Applicability with Specific Link Technologies

   Certain link technologies aggregate data from multiple packets into
   bursts and buffer incoming packets while building each burst.  Wi-Fi,
   PON, and cable all involve such packet aggregation, whereas fixed
   Ethernet and DSL do not.  No sender, whether L4S or not, can do
   anything to reduce the buffering needed for packet aggregation.  So
   an AQM should not count this buffering as part of the queue that it
   controls, given no amount of congestion signals will reduce it.

   Certain link technologies also add buffering for other reasons,
   specifically:

   *  Radio links (cellular, Wi-Fi, or satellite) that are distant from
      the source are particularly challenging.  The radio link capacity
      can vary rapidly by orders of magnitude, so it is considered
      desirable to hold a standing queue that can utilize sudden
      increases of capacity.

   *  Cellular networks are further complicated by a perceived need to
      buffer in order to make hand-overs imperceptible.

   L4S cannot remove the need for all these different forms of
   buffering.  However, by removing 'the longest pole in the tent'
   (buffering for the large sawteeth of Classic congestion controls),
   L4S exposes all these 'shorter poles' to greater scrutiny.

   Until now, the buffering needed for these additional reasons tended
   to be over-specified -- with the excuse that none were 'the longest
   pole in the tent'.  But having removed the 'longest pole', it becomes
   worthwhile to minimize them, for instance, reducing packet
   aggregation burst sizes and MAC scheduling intervals.

   Also, certain link types, particularly radio-based links, are far
   more prone to transmission losses.  Section 6.4.3 explains how an L4S
   response to loss has to be as drastic as a Classic response.
   Nonetheless, research referred to in the same section has
   demonstrated potential for considerably more effective loss repair at
   the link layer, due to the relaxed ordering constraints of L4S
   packets.

6.4.  Deployment Considerations

   L4S AQMs, whether DualQ [RFC9332] or FQ [RFC8290], are in themselves
   an incremental deployment mechanism for L4S -- so that L4S traffic
   can coexist with existing Classic (Reno-friendly) traffic.
   Section 6.4.1 explains why only deploying an L4S AQM in one node at
   each end of the access link will realize nearly all the benefit of
   L4S.

   L4S involves both the network and end systems, so Section 6.4.2
   suggests some typical sequences to deploy each part and why there
   will be an immediate and significant benefit after deploying just one
   part.

   Sections 6.4.3 and 6.4.4 describe the converse incremental deployment
   case where there is no L4S AQM at the network bottleneck, so any L4S
   flow traversing this bottleneck has to take care in case it is
   competing with Classic traffic.

6.4.1.  Deployment Topology

   L4S AQMs will not have to be deployed throughout the Internet before
   L4S can benefit anyone.  Operators of public Internet access networks
   typically design their networks so that the bottleneck will nearly
   always occur at one known (logical) link.  This confines the cost of
   queue management technology to one place.

   The case of mesh networks is different and will be discussed later in
   this section.  However, the known-bottleneck case is generally true
   for Internet access to all sorts of different 'sites', where the word
   'site' includes home networks, small- to medium-sized campus or
   enterprise networks and even cellular devices (Figure 2).  Also, this
   known-bottleneck case tends to be applicable whatever the access link
   technology, whether xDSL, cable, PON, cellular, line of sight
   wireless, or satellite.

   Therefore, the full benefit of the L4S service should be available in
   the downstream direction when an L4S AQM is deployed at the ingress
   to this bottleneck link.  And similarly, the full upstream service
   will typically be available once an L4S AQM is deployed at the
   ingress into the upstream link.  (Of course, multihomed sites would
   only see the full benefit once all their access links were covered.)

                                            ______
                                           (      )
                         __          __  (          )
                        |DQ\________/DQ|( enterprise )
                    ___ |__/        \__| ( /campus  )
                   (   )                   (______)
                 (      )                           ___||_
   +----+      (          )  __                 __ /      \
   | DC |-----(    Core    )|DQ\_______________/DQ|| home |
   +----+      (          ) |__/               \__||______|
                  (_____) __
                         |DQ\__/\        __ ,===.
                         |__/    \  ____/DQ||| ||mobile
                                  \/    \__|||_||device
                                            | o |
                                            `---'

       Figure 2: Likely Location of DualQ (DQ) Deployments in Common
                             Access Topologies

   Deployment in mesh topologies depends on how overbooked the core is.
   If the core is non-blocking, or at least generously provisioned so
   that the edges are nearly always the bottlenecks, it would only be
   necessary to deploy an L4S AQM at the edge bottlenecks.  For example,
   some data-centre networks are designed with the bottleneck in the
   hypervisor or host Network Interface Controllers (NICs), while others
   bottleneck at the top-of-rack switch (both the output ports facing
   hosts and those facing the core).

   An L4S AQM would often next be needed where the Wi-Fi links in a home
   sometimes become the bottleneck.  Also an L4S AQM would eventually
   need to be deployed at any other persistent bottlenecks, such as
   network interconnections, e.g., some public Internet exchange points
   and the ingress and egress to WAN links interconnecting data centres.

6.4.2.  Deployment Sequences

   For any one L4S flow to provide benefit, it requires three (or
   sometimes two) parts to have been deployed: i) the congestion control
   at the sender; ii) the AQM at the bottleneck; and iii) older
   transports (namely TCP) need upgraded receiver feedback too.  This
   was the same deployment problem that ECN faced [RFC8170], so we have
   learned from that experience.

   Firstly, L4S deployment exploits the fact that DCTCP already exists
   on many Internet hosts (e.g., Windows, FreeBSD, and Linux), both
   servers and clients.  Therefore, an L4S AQM can be deployed at a
   network bottleneck to immediately give a working deployment of all
   the L4S parts for testing, as long as the ECT(0) codepoint is
   switched to ECT(1).  DCTCP needs some safety concerns to be fixed for
   general use over the public Internet (see Section 4.3 of the L4S ECN
   spec [RFC9331]), but DCTCP is not on by default, so these issues can
   be managed within controlled deployments or controlled trials.

   Secondly, the performance improvement with L4S is so significant that
   it enables new interactive services and products that were not
   previously possible.  It is much easier for companies to initiate new
   work on deployment if there is budget for a new product trial.  In
   contrast, if there were only an incremental performance improvement
   (as with Classic ECN), spending on deployment tends to be much harder
   to justify.

   Thirdly, the L4S identifier is defined so that network operators can
   initially enable L4S exclusively for certain customers or certain
   applications.  However, this is carefully defined so that it does not
   compromise future evolution towards L4S as an Internet-wide service.
   This is because the L4S identifier is defined not only as the end-to-
   end ECN field, but it can also optionally be combined with any other
   packet header or some status of a customer or their access link (see
   Section 5.4 of [RFC9331]).  Operators could do this anyway, even if
   it were not blessed by the IETF.  However, it is best for the IETF to
   specify that, if they use their own local identifier, it must be in
   combination with the IETF's identifier, ECT(1).  Then, if an operator
   has opted for an exclusive local-use approach, they only have to
   remove this extra rule later to make the service work across the
   Internet -- it will already traverse middleboxes, peerings, etc.

   +-+--------------------+----------------------+---------------------+
   | | Servers or proxies |      Access link     |             Clients |
   +-+--------------------+----------------------+---------------------+
   |0| DCTCP (existing)   |                      |    DCTCP (existing) |
   +-+--------------------+----------------------+---------------------+
   |1|                    |Add L4S AQM downstream|                     |
   | |       WORKS DOWNSTREAM FOR CONTROLLED DEPLOYMENTS/TRIALS        |
   +-+--------------------+----------------------+---------------------+
   |2| Upgrade DCTCP to   |                      |Replace DCTCP feedb'k|
   | | TCP Prague         |                      |         with AccECN |
   | |                 FULLY     WORKS     DOWNSTREAM                  |
   +-+--------------------+----------------------+---------------------+
   | |                    |                      |    Upgrade DCTCP to |
   |3|                    | Add L4S AQM upstream |          TCP Prague |
   | |                    |                      |                     |
   | |              FULLY WORKS UPSTREAM AND DOWNSTREAM                |
   +-+--------------------+----------------------+---------------------+

                 Figure 3: Example L4S Deployment Sequence

   Figure 3 illustrates some example sequences in which the parts of L4S
   might be deployed.  It consists of the following stages, preceded by
   a presumption that DCTCP is already installed at both ends:

   1.  DCTCP is not applicable for use over the public Internet, so it
       is emphasized here that any DCTCP flow has to be completely
       contained within a controlled trial environment.

       Within this trial environment, once an L4S AQM has been deployed,
       the trial DCTCP flow will experience immediate benefit, without
       any other deployment being needed.  In this example, downstream
       deployment is first, but in other scenarios, the upstream might
       be deployed first.  If no AQM at all was previously deployed for
       the downstream access, an L4S AQM greatly improves the Classic
       service (as well as adding the L4S service).  If an AQM was
       already deployed, the Classic service will be unchanged (and L4S
       will add an improvement on top).

   2.  In this stage, the name 'TCP Prague' [PRAGUE-CC] is used to
       represent a variant of DCTCP that is designed to be used in a
       production Internet environment (that is, it has to comply with
       all the requirements in Section 4 of the L4S ECN spec [RFC9331],
       which then means it can be used over the public Internet).  If
       the application is primarily unidirectional, 'TCP Prague' at the
       sending end will provide all the benefit needed, as long as the
       receiving end supports Accurate ECN (AccECN) feedback [ACCECN].

       For TCP transports, AccECN feedback is needed at the other end,
       but it is a generic ECN feedback facility that is already planned
       to be deployed for other purposes, e.g., DCTCP and BBR.  The two
       ends can be deployed in either order because, in TCP, an L4S
       congestion control only enables itself if it has negotiated the
       use of AccECN feedback with the other end during the connection
       handshake.  Thus, deployment of TCP Prague on a server enables
       L4S trials to move to a production service in one direction,
       wherever AccECN is deployed at the other end.  This stage might
       be further motivated by the performance improvements of TCP
       Prague relative to DCTCP (see Appendix A.2 of the L4S ECN spec
       [RFC9331]).

       Unlike TCP, from the outset, QUIC ECN feedback [RFC9000] has
       supported L4S.  Therefore, if the transport is QUIC, one-ended
       deployment of a Prague congestion control at this stage is simple
       and sufficient.

       For QUIC, if a proxy sits in the path between multiple origin
       servers and the access bottlenecks to multiple clients, then
       upgrading the proxy with a Scalable congestion control would
       provide the benefits of L4S over all the clients' downstream
       bottlenecks in one go -- whether or not all the origin servers
       were upgraded.  Conversely, where a proxy has not been upgraded,
       the clients served by it will not benefit from L4S at all in the
       downstream, even when any origin server behind the proxy has been
       upgraded to support L4S.

       For TCP, a proxy upgraded to support 'TCP Prague' would provide
       the benefits of L4S downstream to all clients that support AccECN
       (whether or not they support L4S as well).  And in the upstream,
       the proxy would also support AccECN as a receiver, so that any
       client deploying its own L4S support would benefit in the
       upstream direction, irrespective of whether any origin server
       beyond the proxy supported AccECN.

   3.  This is a two-move stage to enable L4S upstream.  An L4S AQM or
       TCP Prague can be deployed in either order as already explained.
       To motivate the first of two independent moves, the deferred
       benefit of enabling new services after the second move has to be
       worth it to cover the first mover's investment risk.  As
       explained already, the potential for new interactive services
       provides this motivation.  An L4S AQM also improves the upstream
       Classic service significantly if no other AQM has already been
       deployed.

   Note that other deployment sequences might occur.  For instance, the
   upstream might be deployed first; a non-TCP protocol might be used
   end to end, e.g., QUIC and RTP; a body, such as the 3GPP, might
   require L4S to be implemented in 5G user equipment; or other random
   acts of kindness might arise.

6.4.3.  L4S Flow but Non-ECN Bottleneck

   If L4S is enabled between two hosts, the L4S sender is required to
   coexist safely with Reno in response to any drop (see Section 4.3 of
   the L4S ECN spec [RFC9331]).

   Unfortunately, as well as protecting Classic traffic, this rule
   degrades the L4S service whenever there is any loss, even if the
   cause is not persistent congestion at a bottleneck, for example:

   *  congestion loss at other transient bottlenecks, e.g., due to
      bursts in shallower queues;

   *  transmission errors, e.g., due to electrical interference; and

   *  rate policing.

   Three complementary approaches are in progress to address this issue,
   but they are all currently research:

   *  In Prague congestion control, ignore certain losses deemed
      unlikely to be due to congestion (using some ideas from BBR
      [BBR-CC] regarding isolated losses).  This could mask any of the
      above types of loss while still coexisting with drop-based
      congestion controls.

   *  A combination of Recent Acknowledgement (RACK) [RFC8985], L4S, and
      link retransmission without resequencing could repair transmission
      errors without the head of line blocking delay usually associated
      with link-layer retransmission [UnorderedLTE] [RFC9331].

   *  Hybrid ECN/drop rate policers (see Section 8.3).

   L4S deployment scenarios that minimize these issues (e.g., over
   wireline networks) can proceed in parallel to this research, in the
   expectation that research success could continually widen L4S
   applicability.

6.4.4.  L4S Flow but Classic ECN Bottleneck

   Classic ECN support is starting to materialize on the Internet as an
   increased level of CE marking.  It is hard to detect whether this is
   all due to the addition of support for ECN in implementations of FQ-
   CoDel and/or FQ-COBALT, which is not generally problematic, because
   flow queue (FQ) scheduling inherently prevents a flow from exceeding
   the 'fair' rate irrespective of its aggressiveness.  However, some of
   this Classic ECN marking might be due to single-queue ECN deployment.
   This case is discussed in Section 4.3 of the L4S ECN spec [RFC9331].

6.4.5.  L4S AQM Deployment within Tunnels

   An L4S AQM uses the ECN field to signal congestion.  So in common
   with Classic ECN, if the AQM is within a tunnel or at a lower layer,
   correct functioning of ECN signalling requires standards-compliant
   propagation of the ECN field up the layers [RFC6040] [ECN-SHIM]
   [ECN-ENCAP].

7.  IANA Considerations

   This document has no IANA actions.

8.  Security Considerations

8.1.  Traffic Rate (Non-)Policing

8.1.1.  (Non-)Policing Rate per Flow

   In the current Internet, ISPs usually enforce separation between the
   capacity of shared links assigned to different 'sites' (e.g.,
   households, businesses, or mobile users -- see terminology in
   Section 3) using some form of scheduler [RFC0970].  And they use
   various techniques, like redirection to traffic scrubbing facilities,
   to deal with flooding attacks.  However, there has never been a
   universal need to police the rate of individual application flows --
   the Internet has generally always relied on self-restraint of
   congestion controls at senders for sharing intra-'site' capacity.

   L4S has been designed not to upset this status quo.  If a DualQ is
   used to provide L4S service, Section 4.2 of [RFC9332] explains how it
   is designed to give no more rate advantage to unresponsive flows than
   a single-queue AQM would, whether or not there is traffic overload.

   Also, in case per-flow rate policing is ever required, it can be
   added because it is orthogonal to the distinction between L4S and
   Classic.  As explained in Section 5.2, the DualQ variant of L4S
   provides low delay without prejudging the issue of flow-rate control.
   So if flow-rate control is needed, per-flow queuing (FQ) with L4S
   support can be used instead, or flow rate policing can be added as a
   modular addition to a DualQ.  However, per-flow rate control is not
   usually deployed as a security mechanism, because an active attacker
   can just shard its traffic over more flow identifiers if the rate of
   each is restricted.

8.1.2.  (Non-)Policing L4S Service Rate

   Section 5.2 explains how Diffserv only makes a difference if some
   packets get less favourable treatment than others, which typically
   requires traffic rate policing for a low latency class.  In contrast,
   it should not be necessary to rate-police access to the L4S service
   to protect the Classic service, because L4S is designed to reduce
   delay without harming the delay or rate of any Classic traffic.

   During early deployment (and perhaps always), some networks will not
   offer the L4S service.  In general, these networks should not need to
   police L4S traffic.  They are required (by both the ECN spec
   [RFC3168] and the L4S ECN spec [RFC9331]) not to change the L4S
   identifier, which would interfere with end-to-end congestion control.
   If they already treat ECN traffic as Not-ECT, they can merely treat
   L4S traffic as Not-ECT too.  At a bottleneck, such networks will
   introduce some queuing and dropping.  When a Scalable congestion
   control detects a drop, it will have to respond safely with respect
   to Classic congestion controls (as required in Section 4.3 of
   [RFC9331]).  This will degrade the L4S service to be no better (but
   never worse) than Classic best efforts whenever a non-ECN bottleneck
   is encountered on a path (see Section 6.4.3).

   In cases that are expected to be rare, networks that solely support
   Classic ECN [RFC3168] in a single queue bottleneck might opt to
   police L4S traffic so as to protect competing Classic ECN traffic
   (for instance, see Section 6.1.3 of the L4S operational guidance
   [L4SOPS]).  However, Section 4.3 of the L4S ECN spec [RFC9331]
   recommends that the sender adapts its congestion response to properly
   coexist with Classic ECN flows, i.e., reverting to the self-restraint
   approach.

   Certain network operators might choose to restrict access to the L4S
   service, perhaps only to selected premium customers as a value-added
   service.  Their packet classifier (item 2 in Figure 1) could identify
   such customers against some other field (e.g., source address range),
   as well as classifying on the ECN field.  If only the ECN L4S
   identifier matched, but not (say) the source address, the classifier
   could direct these packets (from non-premium customers) into the
   Classic queue.  Explaining clearly how operators can use additional
   local classifiers (see Section 5.4 of [RFC9331]) is intended to
   remove any motivation to clear the L4S identifier.  Then at least the
   L4S ECN identifier will be more likely to survive end to end, even
   though the service may not be supported at every hop.  Such local
   arrangements would only require simple registered/not-registered
   packet classification, rather than the managed, application-specific
   traffic policing against customer-specific traffic contracts that
   Diffserv uses.

8.2.  'Latency Friendliness'

   Like the Classic service, the L4S service relies on self-restraint to
   limit the rate in response to congestion.  In addition, the L4S
   service requires self-restraint in terms of limiting latency
   (burstiness).  It is hoped that self-interest and guidance on dynamic
   behaviour (especially flow start-up, which might need to be
   standardized) will be sufficient to prevent transports from sending
   excessive bursts of L4S traffic, given the application's own latency
   will suffer most from such behaviour.

   Because the L4S service can reduce delay without discernibly
   increasing the delay of any Classic traffic, it should not be
   necessary to police L4S traffic to protect the delay of Classic
   traffic.  However, whether burst policing becomes necessary to
   protect other L4S traffic remains to be seen.  Without it, there will
   be potential for attacks on the low latency of the L4S service.

   If needed, various arrangements could be used to address this
   concern:

   Local bottleneck queue protection:  A per-flow (5-tuple) queue
      protection function [DOCSIS-Q-PROT] has been developed for the low
      latency queue in DOCSIS, which has adopted the DualQ L4S
      architecture.  It protects the low latency service from any queue-
      building flows that accidentally or maliciously classify
      themselves into the low latency queue.  It is designed to score
      flows based solely on their contribution to queuing (not flow rate
      in itself).  Then, if the shared low latency queue is at risk of
      exceeding a threshold, the function redirects enough packets of
      the highest scoring flow(s) into the Classic queue to preserve low
      latency.

   Distributed traffic scrubbing:  Rather than policing locally at each
      bottleneck, it may only be necessary to address problems
      reactively, e.g., punitively target any deployments of new bursty
      malware, in a similar way to how traffic from flooding attack
      sources is rerouted via scrubbing facilities.

   Local bottleneck per-flow scheduling:  Per-flow scheduling should
      inherently isolate non-bursty flows from bursty flows (see
      Section 5.2 for discussion of the merits of per-flow scheduling
      relative to per-flow policing).

   Distributed access subnet queue protection:  Per-flow queue
      protection could be arranged for a queue structure distributed
      across a subnet intercommunicating using lower layer control
      messages (see Section 2.1.4 of [QDyn]).  For instance, in a radio
      access network, user equipment already sends regular buffer status
      reports to a radio network controller, which could use this
      information to remotely police individual flows.

   Distributed Congestion Exposure to ingress policers:  The Congestion
      Exposure (ConEx) architecture [RFC7713] uses an egress audit to
      motivate senders to truthfully signal path congestion in-band,
      where it can be used by ingress policers.  An edge-to-edge variant
      of this architecture is also possible.

   Distributed domain-edge traffic conditioning:  An architecture
      similar to Diffserv [RFC2475] may be preferred, where traffic is
      proactively conditioned on entry to a domain, rather than
      reactively policed only if it leads to queuing once combined with
      other traffic at a bottleneck.

   Distributed core network queue protection:  The policing function
      could be divided between per-flow mechanisms at the network
      ingress that characterize the burstiness of each flow into a
      signal carried with the traffic and per-class mechanisms at
      bottlenecks that act on these signals if queuing actually occurs
      once the traffic converges.  This would be somewhat similar to
      [Nadas20], which is in turn similar to the idea behind core
      stateless fair queuing.

   No single one of these possible queue protection capabilities is
   considered an essential part of the L4S architecture, which works
   without any of them under non-attack conditions (much as the Internet
   normally works without per-flow rate policing).  Indeed, even where
   latency policers are deployed, under normal circumstances, they would
   not intervene, and if operators found they were not necessary, they
   could disable them.  Part of the L4S experiment will be to see
   whether such a function is necessary and which arrangements are most
   appropriate to the size of the problem.

8.3.  Interaction between Rate Policing and L4S

   As mentioned in Section 5.2, L4S should remove the need for low
   latency Diffserv classes.  However, those Diffserv classes that give
   certain applications or users priority over capacity would still be
   applicable in certain scenarios (e.g., corporate networks).  Then,
   within such Diffserv classes, L4S would often be applicable to give
   traffic low latency and low loss as well.  Within such a Diffserv
   class, the bandwidth available to a user or application is often
   limited by a rate policer.  Similarly, in the default Diffserv class,
   rate policers are sometimes used to partition shared capacity.

   A Classic rate policer drops any packets exceeding a set rate,
   usually also giving a burst allowance (variants exist where the
   policer re-marks noncompliant traffic to a discard-eligible Diffserv
   codepoint, so they can be dropped elsewhere during contention).
   Whenever L4S traffic encounters one of these rate policers, it will
   experience drops and the source will have to fall back to a Classic
   congestion control, thus losing the benefits of L4S (Section 6.4.3).
   So in networks that already use rate policers and plan to deploy L4S,
   it will be preferable to redesign these rate policers to be more
   friendly to the L4S service.

   L4S-friendly rate policing is currently a research area (note that
   this is not the same as latency policing).  It might be achieved by
   setting a threshold where ECN marking is introduced, such that it is
   just under the policed rate or just under the burst allowance where
   drop is introduced.  For instance, the two-rate, three-colour marker
   [RFC2698] or a PCN threshold and excess-rate marker [RFC5670] could
   mark ECN at the lower rate and drop at the higher.  Or an existing
   rate policer could have congestion-rate policing added, e.g., using
   the 'local' (non-ConEx) variant of the ConEx aggregate congestion
   policer [CONG-POLICING].  It might also be possible to design
   Scalable congestion controls to respond less catastrophically to loss
   that has not been preceded by a period of increasing delay.

   The design of L4S-friendly rate policers will require a separate,
   dedicated document.  For further discussion of the interaction
   between L4S and Diffserv, see [L4S-DIFFSERV].

8.4.  ECN Integrity

   Various ways have been developed to protect the integrity of the
   congestion feedback loop (whether signalled by loss, Classic ECN, or
   L4S ECN) against misbehaviour by the receiver, sender, or network (or
   all three).  Brief details of each, including applicability, pros,
   and cons, are given in Appendix C.1 of the L4S ECN spec [RFC9331].

8.5.  Privacy Considerations

   As discussed in Section 5.2, the L4S architecture does not preclude
   approaches that inspect end-to-end transport layer identifiers.  For
   instance, L4S support has been added to FQ-CoDel, which classifies by
   application flow identifier in the network.  However, the main
   innovation of L4S is the DualQ AQM framework that does not need to
   inspect any deeper than the outermost IP header, because the L4S
   identifier is in the IP-ECN field.

   Thus, the L4S architecture enables very low queuing delay without
   _requiring_ inspection of information above the IP layer.  This means
   that users who want to encrypt application flow identifiers, e.g., in
   IPsec or other encrypted VPN tunnels, don't have to sacrifice low
   delay [RFC8404].

   Because L4S can provide low delay for a broad set of applications
   that choose to use it, there is no need for individual applications
   or classes within that broad set to be distinguishable in any way
   while traversing networks.  This removes much of the ability to
   correlate between the delay requirements of traffic and other
   identifying features [RFC6973].  There may be some types of traffic
   that prefer not to use L4S, but the coarse binary categorization of
   traffic reveals very little that could be exploited to compromise
   privacy.

9.  Informative References

   [ACCECN]   Briscoe, B., Kühlewind, M., and R. Scheffenegger, "More
              Accurate ECN Feedback in TCP", Work in Progress, Internet-
              Draft, draft-ietf-tcpm-accurate-ecn-22, 9 November 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-ietf-tcpm-
              accurate-ecn-22&gt;.

   [AFCD]     Xue, L., Kumar, S., Cui, C., Kondikoppa, P., Chiu, C-H.,
              and S-J. Park, "Towards fair and low latency next
              generation high speed networks: AFCD queuing", Journal of
              Network and Computer Applications, Volume 70, pp. 183-193,
              DOI 10.1016/j.jnca.2016.03.021, July 2016,
              &lt;https://doi.org/10.1016/j.jnca.2016.03.021&gt;.

   [BBR-CC]   Cardwell, N., Cheng, Y., Hassas Yeganeh, S., Swett, I.,
              and V. Jacobson, "BBR Congestion Control", Work in
              Progress, Internet-Draft, draft-cardwell-iccrg-bbr-
              congestion-control-02, 7 March 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-cardwell-
              iccrg-bbr-congestion-control-02&gt;.

   [BBRv2]    "TCP BBR v2 Alpha/Preview Release", commit 17700ca, June
              2022, &lt;https://github.com/google/bbr&gt;.

   [BDPdata]  Briscoe, B., "PI2 Parameters", TR-BB-2021-001,
              arXiv:2107.01003 [cs.NI], DOI 10.48550/arXiv.2107.01003,
              October 2021, &lt;https://arxiv.org/abs/2107.01003&gt;.

   [BufferSize]
              Appenzeller, G., Keslassy, I., and N. McKeown, "Sizing
              Router Buffers", SIGCOMM '04: Proceedings of the 2004
              conference on Applications, technologies, architectures,
              and protocols for computer communications, pp. 281-292,
              DOI 10.1145/1015467.1015499, October 2004,
              &lt;https://doi.org/10.1145/1015467.1015499&gt;.

   [COBALT]   Palmei, J., Gupta, S., Imputato, P., Morton, J.,
              Tahiliani, M. P., Avallone, S., and D. Täht, "Design and
              Evaluation of COBALT Queue Discipline", IEEE International
              Symposium on Local and Metropolitan Area Networks
              (LANMAN), DOI 10.1109/LANMAN.2019.8847054, July 2019,
              &lt;https://ieeexplore.ieee.org/abstract/document/8847054&gt;.

   [CODEL-APPROX-FAIR]
              Morton, J. and P. Heist, "Controlled Delay Approximate
              Fairness AQM", Work in Progress, Internet-Draft, draft-
              morton-tsvwg-codel-approx-fair-01, 9 March 2020,
              &lt;https://datatracker.ietf.org/doc/html/draft-morton-tsvwg-
              codel-approx-fair-01&gt;.

   [CONG-POLICING]
              Briscoe, B., "Network Performance Isolation using
              Congestion Policing", Work in Progress, Internet-Draft,
              draft-briscoe-conex-policing-01, 14 February 2014,
              &lt;https://datatracker.ietf.org/doc/html/draft-briscoe-
              conex-policing-01&gt;.

   [CTCP]     Sridharan, M., Tan, K., Bansal, D., and D. Thaler,
              "Compound TCP: A New TCP Congestion Control for High-Speed
              and Long Distance Networks", Work in Progress, Internet-
              Draft, draft-sridharan-tcpm-ctcp-02, 11 November 2008,
              &lt;https://datatracker.ietf.org/doc/html/draft-sridharan-
              tcpm-ctcp-02&gt;.

   [DOCSIS-Q-PROT]
              Briscoe, B., Ed. and G. White, "The DOCSIS® Queue
              Protection Algorithm to Preserve Low Latency", Work in
              Progress, Internet-Draft, draft-briscoe-docsis-q-
              protection-06, 13 May 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-briscoe-
              docsis-q-protection-06&gt;.

   [DOCSIS3.1]
              CableLabs, "MAC and Upper Layer Protocols Interface
              (MULPI) Specification, CM-SP-MULPIv3.1", Data-Over-Cable
              Service Interface Specifications DOCSIS 3.1 Version i17 or
              later, 21 January 2019, &lt;https://specification-
              search.cablelabs.com/CM-SP-MULPIv3.1&gt;.

   [DOCSIS3AQM]
              White, G., "Active Queue Management Algorithms for DOCSIS
              3.0: A Simulation Study of CoDel, SFQ-CoDel and PIE in
              DOCSIS 3.0 Networks", CableLabs Technical Report, April
              2013, &lt;https://www.cablelabs.com/wp-
              content/uploads/2013/11/
              Active_Queue_Management_Algorithms_DOCSIS_3_0.pdf&gt;.

   [DualPI2Linux]
              Albisser, O., De Schepper, K., Briscoe, B., Tilmans, O.,
              and H. Steen, "DUALPI2 - Low Latency, Low Loss and
              Scalable (L4S) AQM", Proceedings of Linux Netdev 0x13,
              March 2019, &lt;https://www.netdevconf.org/0x13/
              session.html?talk-DUALPI2-AQM&gt;.

   [Dukkipati06]
              Dukkipati, N. and N. McKeown, "Why Flow-Completion Time is
              the Right Metric for Congestion Control", ACM SIGCOMM
              Computer Communication Review, Volume 36, Issue 1, pp.
              59-62, DOI 10.1145/1111322.1111336, January 2006,
              &lt;https://dl.acm.org/doi/10.1145/1111322.1111336&gt;.

   [ECN-ENCAP]
              Briscoe, B. and J. Kaippallimalil, "Guidelines for Adding
              Congestion Notification to Protocols that Encapsulate IP",
              Work in Progress, Internet-Draft, draft-ietf-tsvwg-ecn-
              encap-guidelines-17, 11 July 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-ietf-tsvwg-
              ecn-encap-guidelines-17&gt;.

   [ECN-SCTP] Stewart, R., Tuexen, M., and X. Dong, "ECN for Stream
              Control Transmission Protocol (SCTP)", Work in Progress,
              Internet-Draft, draft-stewart-tsvwg-sctpecn-05, 15 January
              2014, &lt;https://datatracker.ietf.org/doc/html/draft-
              stewart-tsvwg-sctpecn-05&gt;.

   [ECN-SHIM] Briscoe, B., "Propagating Explicit Congestion Notification
              Across IP Tunnel Headers Separated by a Shim", Work in
              Progress, Internet-Draft, draft-ietf-tsvwg-rfc6040update-
              shim-15, 11 July 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-ietf-tsvwg-
              rfc6040update-shim-15&gt;.

   [FQ_CoDel_Thresh]
              "fq_codel: generalise ce_threshold marking for subset of
              traffic", commit dfcb63ce1de6b10b, October 2021,
              &lt;https://git.kernel.org/pub/scm/linux/kernel/git/netdev/
              net-next.git/commit/?id=dfcb63ce1de6b10b&gt;.

   [Hohlfeld14]
              Hohlfeld, O., Pujol, E., Ciucu, F., Feldmann, A., and P.
              Barford, "A QoE Perspective on Sizing Network Buffers",
              IMC '14: Proceedings of the 2014 Conference on Internet
              Measurement, pp. 333-346, DOI 10.1145/2663716.2663730,
              November 2014,
              &lt;https://doi.acm.org/10.1145/2663716.2663730&gt;.

   [L4S-DIFFSERV]
              Briscoe, B., "Interactions between Low Latency, Low Loss,
              Scalable Throughput (L4S) and Differentiated Services",
              Work in Progress, Internet-Draft, draft-briscoe-tsvwg-l4s-
              diffserv-02, 4 November 2018,
              &lt;https://datatracker.ietf.org/doc/html/draft-briscoe-
              tsvwg-l4s-diffserv-02&gt;.

   [L4Sdemo16]
              Bondarenko, O., De Schepper, K., Tsang, I., Briscoe, B.,
              Petlund, A., and C. Griwodz, "Ultra-Low Delay for All:
              Live Experience, Live Analysis", Proceedings of the 7th
              International Conference on Multimedia Systems, Article
              No. 33, pp. 1-4, DOI 10.1145/2910017.2910633, May 2016,
              &lt;https://dl.acm.org/citation.cfm?doid=2910017.2910633&gt;.

   [L4Sdemo16-Video]
              "Videos used in IETF dispatch WG 'Ultra-Low Queuing Delay
              for All Apps' slot",
              &lt;https://riteproject.eu/dctth/#1511dispatchwg&gt;.

   [L4Seval22]
              De Schepper, K., Albisser, O., Tilmans, O., and B.
              Briscoe, "Dual Queue Coupled AQM: Deployable Very Low
              Queuing Delay for All", TR-BB-2022-001, arXiv:2209.01078
              [cs.NI], DOI 10.48550/arXiv.2209.01078, September 2022,
              &lt;https://arxiv.org/abs/2209.01078&gt;.

   [L4SOPS]   White, G., Ed., "Operational Guidance for Deployment of
              L4S in the Internet", Work in Progress, Internet-Draft,
              draft-ietf-tsvwg-l4sops-03, 28 April 2022,
              &lt;https://datatracker.ietf.org/doc/html/draft-ietf-tsvwg-
              l4sops-03&gt;.

   [LEDBAT_AQM]
              Al-Saadi, R., Armitage, G., and J. But, "Characterising
              LEDBAT Performance Through Bottlenecks Using PIE, FQ-CoDel
              and FQ-PIE Active Queue Management", IEEE 42nd Conference
              on Local Computer Networks (LCN), DOI 10.1109/LCN.2017.22,
              October 2017,
              &lt;https://ieeexplore.ieee.org/document/8109367&gt;.

   [lowat]    Meenan, P., "Optimizing HTTP/2 prioritization with BBR and
              tcp_notsent_lowat", Cloudflare Blog, October 2018,
              &lt;https://blog.cloudflare.com/http-2-prioritization-with-
              nginx/&gt;.

   [McIlroy78]
              McIlroy, M.D., Pinson, E. N., and B. A. Tague, "UNIX Time-
              Sharing System: Foreword", The Bell System Technical
              Journal 57: 6, pp. 1899-1904,
              DOI 10.1002/j.1538-7305.1978.tb02135.x, July 1978,
              &lt;https://archive.org/details/bstj57-6-1899&gt;.

   [Nadas20]  Nádas, S., Gombos, G., Fejes, F., and S. Laki, "A
              Congestion Control Independent L4S Scheduler", ANRW '20:
              Proceedings of the Applied Networking Research Workshop,
              pp. 45-51, DOI 10.1145/3404868.3406669, July 2020,
              &lt;https://doi.org/10.1145/3404868.3406669&gt;.

   [NASA04]   Bailey, R., Trey Arthur III, J., and S. Williams, "Latency
              Requirements for Head-Worn Display S/EVS Applications",
              Proceedings of SPIE 5424, DOI 10.1117/12.554462, April
              2004, &lt;https://ntrs.nasa.gov/api/citations/20120009198/
              downloads/20120009198.pdf?attachment=true&gt;.

   [NQB-PHB]  White, G. and T. Fossati, "A Non-Queue-Building Per-Hop
              Behavior (NQB PHB) for Differentiated Services", Work in
              Progress, Internet-Draft, draft-ietf-tsvwg-nqb-15, 11
              January 2023, &lt;https://datatracker.ietf.org/doc/html/
              draft-ietf-tsvwg-nqb-15&gt;.

   [PRAGUE-CC]
              De Schepper, K., Tilmans, O., and B. Briscoe, Ed., "Prague
              Congestion Control", Work in Progress, Internet-Draft,
              draft-briscoe-iccrg-prague-congestion-control-01, 11 July
              2022, &lt;https://datatracker.ietf.org/doc/html/draft-
              briscoe-iccrg-prague-congestion-control-01&gt;.

   [PragueLinux]
              Briscoe, B., De Schepper, K., Albisser, O., Misund, J.,
              Tilmans, O., Kühlewind, M., and A.S. Ahmed, "Implementing
              the 'TCP Prague' Requirements for Low Latency Low Loss
              Scalable Throughput (L4S)", Proceedings Linux Netdev 0x13,
              March 2019, &lt;https://www.netdevconf.org/0x13/
              session.html?talk-tcp-prague-l4s&gt;.

   [QDyn]     Briscoe, B., "Rapid Signalling of Queue Dynamics", TR-BB-
              2017-001, arXiv:1904.07044 [cs.NI],
              DOI 10.48550/arXiv.1904.07044, April 2019,
              &lt;https://arxiv.org/abs/1904.07044&gt;.

   [Raaen14]  Raaen, K. and T-M. Grønli, "Latency Thresholds for
              Usability in Games: A Survey", Norsk IKT-konferanse for
              forskning og utdanning (Norwegian ICT conference for
              research and education), 2014,
              &lt;http://ojs.bibsys.no/index.php/NIK/article/view/9/6&gt;.

   [Rajiullah15]
              Rajiullah, M., "Towards a Low Latency Internet:
              Understanding and Solutions", Dissertation, Karlstad
              University, 2015, &lt;https://www.diva-
              portal.org/smash/get/diva2:846109/FULLTEXT01.pdf&gt;.

   [RELENTLESS]
              Mathis, M., "Relentless Congestion Control", Work in
              Progress, Internet-Draft, draft-mathis-iccrg-relentless-
              tcp-00, 4 March 2009,
              &lt;https://datatracker.ietf.org/doc/html/draft-mathis-iccrg-
              relentless-tcp-00&gt;.

   [RFC0970]  Nagle, J., "On Packet Switches With Infinite Storage",
              RFC 970, DOI 10.17487/RFC0970, December 1985,
              &lt;https://www.rfc-editor.org/info/rfc970&gt;.

   [RFC2475]  Blake, S., Black, D., Carlson, M., Davies, E., Wang, Z.,
              and W. Weiss, "An Architecture for Differentiated
              Services", RFC 2475, DOI 10.17487/RFC2475, December 1998,
              &lt;https://www.rfc-editor.org/info/rfc2475&gt;.

   [RFC2698]  Heinanen, J. and R. Guerin, "A Two Rate Three Color
              Marker", RFC 2698, DOI 10.17487/RFC2698, September 1999,
              &lt;https://www.rfc-editor.org/info/rfc2698&gt;.

   [RFC2884]  Hadi Salim, J. and U. Ahmed, "Performance Evaluation of
              Explicit Congestion Notification (ECN) in IP Networks",
              RFC 2884, DOI 10.17487/RFC2884, July 2000,
              &lt;https://www.rfc-editor.org/info/rfc2884&gt;.

   [RFC3168]  Ramakrishnan, K., Floyd, S., and D. Black, "The Addition
              of Explicit Congestion Notification (ECN) to IP",
              RFC 3168, DOI 10.17487/RFC3168, September 2001,
              &lt;https://www.rfc-editor.org/info/rfc3168&gt;.

   [RFC3246]  Davie, B., Charny, A., Bennet, J.C.R., Benson, K., Le
              Boudec, J.Y., Courtney, W., Davari, S., Firoiu, V., and D.
              Stiliadis, "An Expedited Forwarding PHB (Per-Hop
              Behavior)", RFC 3246, DOI 10.17487/RFC3246, March 2002,
              &lt;https://www.rfc-editor.org/info/rfc3246&gt;.

   [RFC3540]  Spring, N., Wetherall, D., and D. Ely, "Robust Explicit
              Congestion Notification (ECN) Signaling with Nonces",
              RFC 3540, DOI 10.17487/RFC3540, June 2003,
              &lt;https://www.rfc-editor.org/info/rfc3540&gt;.

   [RFC3649]  Floyd, S., "HighSpeed TCP for Large Congestion Windows",
              RFC 3649, DOI 10.17487/RFC3649, December 2003,
              &lt;https://www.rfc-editor.org/info/rfc3649&gt;.

   [RFC4340]  Kohler, E., Handley, M., and S. Floyd, "Datagram
              Congestion Control Protocol (DCCP)", RFC 4340,
              DOI 10.17487/RFC4340, March 2006,
              &lt;https://www.rfc-editor.org/info/rfc4340&gt;.

   [RFC4774]  Floyd, S., "Specifying Alternate Semantics for the
              Explicit Congestion Notification (ECN) Field", BCP 124,
              RFC 4774, DOI 10.17487/RFC4774, November 2006,
              &lt;https://www.rfc-editor.org/info/rfc4774&gt;.

   [RFC4960]  Stewart, R., Ed., "Stream Control Transmission Protocol",
              RFC 4960, DOI 10.17487/RFC4960, September 2007,
              &lt;https://www.rfc-editor.org/info/rfc4960&gt;.

   [RFC5033]  Floyd, S. and M. Allman, "Specifying New Congestion
              Control Algorithms", BCP 133, RFC 5033,
              DOI 10.17487/RFC5033, August 2007,
              &lt;https://www.rfc-editor.org/info/rfc5033&gt;.

   [RFC5348]  Floyd, S., Handley, M., Padhye, J., and J. Widmer, "TCP
              Friendly Rate Control (TFRC): Protocol Specification",
              RFC 5348, DOI 10.17487/RFC5348, September 2008,
              &lt;https://www.rfc-editor.org/info/rfc5348&gt;.

   [RFC5670]  Eardley, P., Ed., "Metering and Marking Behaviour of PCN-
              Nodes", RFC 5670, DOI 10.17487/RFC5670, November 2009,
              &lt;https://www.rfc-editor.org/info/rfc5670&gt;.

   [RFC5681]  Allman, M., Paxson, V., and E. Blanton, "TCP Congestion
              Control", RFC 5681, DOI 10.17487/RFC5681, September 2009,
              &lt;https://www.rfc-editor.org/info/rfc5681&gt;.

   [RFC6040]  Briscoe, B., "Tunnelling of Explicit Congestion
              Notification", RFC 6040, DOI 10.17487/RFC6040, November
              2010, &lt;https://www.rfc-editor.org/info/rfc6040&gt;.

   [RFC6679]  Westerlund, M., Johansson, I., Perkins, C., O'Hanlon, P.,
              and K. Carlberg, "Explicit Congestion Notification (ECN)
              for RTP over UDP", RFC 6679, DOI 10.17487/RFC6679, August
              2012, &lt;https://www.rfc-editor.org/info/rfc6679&gt;.

   [RFC6817]  Shalunov, S., Hazel, G., Iyengar, J., and M. Kuehlewind,
              "Low Extra Delay Background Transport (LEDBAT)", RFC 6817,
              DOI 10.17487/RFC6817, December 2012,
              &lt;https://www.rfc-editor.org/info/rfc6817&gt;.

   [RFC6973]  Cooper, A., Tschofenig, H., Aboba, B., Peterson, J.,
              Morris, J., Hansen, M., and R. Smith, "Privacy
              Considerations for Internet Protocols", RFC 6973,
              DOI 10.17487/RFC6973, July 2013,
              &lt;https://www.rfc-editor.org/info/rfc6973&gt;.

   [RFC7560]  Kuehlewind, M., Ed., Scheffenegger, R., and B. Briscoe,
              "Problem Statement and Requirements for Increased Accuracy
              in Explicit Congestion Notification (ECN) Feedback",
              RFC 7560, DOI 10.17487/RFC7560, August 2015,
              &lt;https://www.rfc-editor.org/info/rfc7560&gt;.

   [RFC7567]  Baker, F., Ed. and G. Fairhurst, Ed., "IETF
              Recommendations Regarding Active Queue Management",
              BCP 197, RFC 7567, DOI 10.17487/RFC7567, July 2015,
              &lt;https://www.rfc-editor.org/info/rfc7567&gt;.

   [RFC7665]  Halpern, J., Ed. and C. Pignataro, Ed., "Service Function
              Chaining (SFC) Architecture", RFC 7665,
              DOI 10.17487/RFC7665, October 2015,
              &lt;https://www.rfc-editor.org/info/rfc7665&gt;.

   [RFC7713]  Mathis, M. and B. Briscoe, "Congestion Exposure (ConEx)
              Concepts, Abstract Mechanism, and Requirements", RFC 7713,
              DOI 10.17487/RFC7713, December 2015,
              &lt;https://www.rfc-editor.org/info/rfc7713&gt;.

   [RFC8033]  Pan, R., Natarajan, P., Baker, F., and G. White,
              "Proportional Integral Controller Enhanced (PIE): A
              Lightweight Control Scheme to Address the Bufferbloat
              Problem", RFC 8033, DOI 10.17487/RFC8033, February 2017,
              &lt;https://www.rfc-editor.org/info/rfc8033&gt;.

   [RFC8034]  White, G. and R. Pan, "Active Queue Management (AQM) Based
              on Proportional Integral Controller Enhanced (PIE) for
              Data-Over-Cable Service Interface Specifications (DOCSIS)
              Cable Modems", RFC 8034, DOI 10.17487/RFC8034, February
              2017, &lt;https://www.rfc-editor.org/info/rfc8034&gt;.

   [RFC8170]  Thaler, D., Ed., "Planning for Protocol Adoption and
              Subsequent Transitions", RFC 8170, DOI 10.17487/RFC8170,
              May 2017, &lt;https://www.rfc-editor.org/info/rfc8170&gt;.

   [RFC8257]  Bensley, S., Thaler, D., Balasubramanian, P., Eggert, L.,
              and G. Judd, "Data Center TCP (DCTCP): TCP Congestion
              Control for Data Centers", RFC 8257, DOI 10.17487/RFC8257,
              October 2017, &lt;https://www.rfc-editor.org/info/rfc8257&gt;.

   [RFC8290]  Hoeiland-Joergensen, T., McKenney, P., Taht, D., Gettys,
              J., and E. Dumazet, "The Flow Queue CoDel Packet Scheduler
              and Active Queue Management Algorithm", RFC 8290,
              DOI 10.17487/RFC8290, January 2018,
              &lt;https://www.rfc-editor.org/info/rfc8290&gt;.

   [RFC8298]  Johansson, I. and Z. Sarker, "Self-Clocked Rate Adaptation
              for Multimedia", RFC 8298, DOI 10.17487/RFC8298, December
              2017, &lt;https://www.rfc-editor.org/info/rfc8298&gt;.

   [RFC8311]  Black, D., "Relaxing Restrictions on Explicit Congestion
              Notification (ECN) Experimentation", RFC 8311,
              DOI 10.17487/RFC8311, January 2018,
              &lt;https://www.rfc-editor.org/info/rfc8311&gt;.

   [RFC8312]  Rhee, I., Xu, L., Ha, S., Zimmermann, A., Eggert, L., and
              R. Scheffenegger, "CUBIC for Fast Long-Distance Networks",
              RFC 8312, DOI 10.17487/RFC8312, February 2018,
              &lt;https://www.rfc-editor.org/info/rfc8312&gt;.

   [RFC8404]  Moriarty, K., Ed. and A. Morton, Ed., "Effects of
              Pervasive Encryption on Operators", RFC 8404,
              DOI 10.17487/RFC8404, July 2018,
              &lt;https://www.rfc-editor.org/info/rfc8404&gt;.

   [RFC8511]  Khademi, N., Welzl, M., Armitage, G., and G. Fairhurst,
              "TCP Alternative Backoff with ECN (ABE)", RFC 8511,
              DOI 10.17487/RFC8511, December 2018,
              &lt;https://www.rfc-editor.org/info/rfc8511&gt;.

   [RFC8888]  Sarker, Z., Perkins, C., Singh, V., and M. Ramalho, "RTP
              Control Protocol (RTCP) Feedback for Congestion Control",
              RFC 8888, DOI 10.17487/RFC8888, January 2021,
              &lt;https://www.rfc-editor.org/info/rfc8888&gt;.

   [RFC8985]  Cheng, Y., Cardwell, N., Dukkipati, N., and P. Jha, "The
              RACK-TLP Loss Detection Algorithm for TCP", RFC 8985,
              DOI 10.17487/RFC8985, February 2021,
              &lt;https://www.rfc-editor.org/info/rfc8985&gt;.

   [RFC9000]  Iyengar, J., Ed. and M. Thomson, Ed., "QUIC: A UDP-Based
              Multiplexed and Secure Transport", RFC 9000,
              DOI 10.17487/RFC9000, May 2021,
              &lt;https://www.rfc-editor.org/info/rfc9000&gt;.

   [RFC9113]  Thomson, M., Ed. and C. Benfield, Ed., "HTTP/2", RFC 9113,
              DOI 10.17487/RFC9113, June 2022,
              &lt;https://www.rfc-editor.org/info/rfc9113&gt;.

   [RFC9331]  De Schepper, K. and B. Briscoe, Ed., "The Explicit
              Congestion Notification (ECN) Protocol for Low Latency,
              Low Loss, and Scalable Throughput (L4S)", RFC 9331,
              DOI 10.17487/RFC9331, January 2023,
              &lt;https://www.rfc-editor.org/info/rfc9331&gt;.

   [RFC9332]  De Schepper, K., Briscoe, B., Ed., and G. White, "Dual-
              Queue Coupled Active Queue Management (AQM) for Low
              Latency, Low Loss, and Scalable Throughput (L4S)",
              RFC 9332, DOI 10.17487/RFC9332, January 2023,
              &lt;https://www.rfc-editor.org/info/rfc9332&gt;.

   [SCReAM-L4S]
              "SCReAM", commit fda6c53, June 2022,
              &lt;https://github.com/EricssonResearch/scream&gt;.

   [TCP-CA]   Jacobson, V. and M. Karels, "Congestion Avoidance and
              Control", Laurence Berkeley Labs Technical Report ,
              November 1988, &lt;https://ee.lbl.gov/papers/congavoid.pdf&gt;.

   [UnorderedLTE]
              Austrheim, M., "Implementing immediate forwarding for 4G
              in a network simulator", Master's Thesis, University of
              Oslo, 2018.

<span>Acknowledgements</span>

   Thanks to Richard Scheffenegger, Wes Eddy, Karen Nielsen, David
   Black, Jake Holland, Vidhi Goel, Ermin Sakic, Praveen
   Balasubramanian, Gorry Fairhurst, Mirja Kuehlewind, Philip Eardley,
   Neal Cardwell, Pete Heist, and Martin Duke for their useful review
   comments.  Thanks also to the area reviewers: Marco Tiloca, Lars
   Eggert, Roman Danyliw, and Éric Vyncke.

   Bob Briscoe and Koen De Schepper were partly funded by the European
   Community under its Seventh Framework Programme through the Reducing
   Internet Transport Latency (RITE) project (ICT-317700).  The
   contribution of Koen De Schepper was also partly funded by the
   5Growth and DAEMON EU H2020 projects.  Bob Briscoe was also partly
   funded by the Research Council of Norway through the TimeIn project,
   partly by CableLabs, and partly by the Comcast Innovation Fund.  The
   views expressed here are solely those of the authors.

<span>Authors' Addresses</span>

   Bob Briscoe (editor)
   Independent
   United Kingdom
   Email: ietf@bobbriscoe.net
   URI:   https://bobbriscoe.net/

   Koen De Schepper
   Nokia Bell Labs
   Antwerp
   Belgium
   Email: koen.de_schepper@nokia.com
   URI:   https://www.bell-labs.com/about/researcher-profiles/
   koende_schepper/

   Marcelo Bagnulo
   Universidad Carlos III de Madrid
   Av. Universidad 30
   28911 Madrid
   Spain
   Phone: 34 91 6249500
   Email: marcelo@it.uc3m.es
   URI:   https://www.it.uc3m.es

   Greg White
   CableLabs
   United States of America
   Email: G.White@CableLabs.com
</pre>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Templ: A language for writing HTML user interfaces in Go (213 pts)]]></title>
            <link>https://github.com/a-h/templ</link>
            <guid>38597599</guid>
            <pubDate>Mon, 11 Dec 2023 04:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/a-h/templ">https://github.com/a-h/templ</a>, See on <a href="https://news.ycombinator.com/item?id=38597599">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/a-h/templ/raw/main/templ.png"><img src="https://github.com/a-h/templ/raw/main/templ.png" alt="templ"></a></p>
<h2 tabindex="-1" dir="auto">A HTML templating language for Go that has great developer tooling.</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/a-h/templ/blob/main/ide-demo.gif"><img src="https://github.com/a-h/templ/raw/main/ide-demo.gif" alt="templ" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">See user documentation at <a href="https://templ.guide/" rel="nofollow">https://templ.guide</a></p>
<p dir="auto">
<a href="https://pkg.go.dev/github.com/a-h/templ" rel="nofollow"><img src="https://camo.githubusercontent.com/b46b0d21e17e4b311cbbbfa0a6e2a5a82bf30d1c79f24baeb0d8d52ce318d83a/68747470733a2f2f706b672e676f2e6465762f62616467652f6769746875622e636f6d2f612d682f74656d706c2e737667" alt="Go Reference" data-canonical-src="https://pkg.go.dev/badge/github.com/a-h/templ.svg"></a>
<a href="https://xcfile.dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/8bbda4c55cd011f9a8066b82d711043cff39fed043be6035d4a8397d28fefab1/68747470733a2f2f786366696c652e6465762f62616467652e737667" alt="xc compatible" data-canonical-src="https://xcfile.dev/badge.svg"></a>
<a href="https://raw.githack.com/wiki/a-h/templ/coverage.html" rel="nofollow"><img src="https://github.com/a-h/templ/wiki/coverage.svg" alt="Go Coverage"></a>
<a href="https://goreportcard.com/report/github.com/a-h/templ" rel="nofollow"><img src="https://camo.githubusercontent.com/8accb9caecd0f6cdbadfb817b60c9f4b7390edd8968e11c6d90ab39945c9d6b0/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f612d682f74656d706c" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/a-h/templ">
</a></p><h2 tabindex="-1" dir="auto">Tasks</h2>
<h3 tabindex="-1" dir="auto">build</h3>
<p dir="auto">Build a local version.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go run ./get-version > .version
cd cmd/templ
go build"><pre>go run ./get-version <span>&gt;</span> .version
<span>cd</span> cmd/templ
go build</pre></div>
<h3 tabindex="-1" dir="auto">install-snapshot</h3>
<p dir="auto">Build and install to ~/bin</p>
<div dir="auto" data-snippet-clipboard-copy-content="rm cmd/templ/lspcmd/*.txt || true
go run ./get-version > .version
cd cmd/templ &amp;&amp; go build -o ~/bin/templ"><pre>rm cmd/templ/lspcmd/<span>*</span>.txt <span>||</span> <span>true</span>
go run ./get-version <span>&gt;</span> .version
<span>cd</span> cmd/templ <span>&amp;&amp;</span> go build -o <span>~</span>/bin/templ</pre></div>
<h3 tabindex="-1" dir="auto">build-snapshot</h3>
<p dir="auto">Use goreleaser to build the command line binary using goreleaser.</p>
<div dir="auto" data-snippet-clipboard-copy-content="goreleaser build --snapshot --clean"><pre>goreleaser build --snapshot --clean</pre></div>
<h3 tabindex="-1" dir="auto">generate</h3>
<p dir="auto">Run templ generate using local version.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go run ./cmd/templ generate -include-version=false"><pre>go run ./cmd/templ generate -include-version=false</pre></div>
<h3 tabindex="-1" dir="auto">test</h3>
<p dir="auto">Run Go tests.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go run ./get-version > .version
go run ./cmd/templ generate -include-version=false
go test ./..."><pre>go run ./get-version <span>&gt;</span> .version
go run ./cmd/templ generate -include-version=false
go <span>test</span> ./...</pre></div>
<h3 tabindex="-1" dir="auto">test-cover</h3>
<p dir="auto">Run Go tests.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create test profile directories.
mkdir -p coverage/fmt
mkdir -p coverage/generate
mkdir -p coverage/unit
# Build the test binary.
go build -cover -o ./coverage/templ-cover ./cmd/templ
# Run the covered generate command.
GOCOVERDIR=coverage/fmt ./coverage/templ-cover fmt .
GOCOVERDIR=coverage/generate ./coverage/templ-cover generate -include-version=false
# Run the unit tests.
go test -cover ./... -args -test.gocoverdir=&quot;$PWD/coverage/unit&quot;
# Display the combined percentage.
go tool covdata percent -i=./coverage/fmt,./coverage/generate,./coverage/unit
# Generate a text coverage profile for tooling to use.
go tool covdata textfmt -i=./coverage/fmt,./coverage/generate,./coverage/unit -o coverage.out
# Print total
go tool cover -func coverage.out | grep total"><pre><span><span>#</span> Create test profile directories.</span>
mkdir -p coverage/fmt
mkdir -p coverage/generate
mkdir -p coverage/unit
<span><span>#</span> Build the test binary.</span>
go build -cover -o ./coverage/templ-cover ./cmd/templ
<span><span>#</span> Run the covered generate command.</span>
GOCOVERDIR=coverage/fmt ./coverage/templ-cover fmt <span>.</span>
GOCOVERDIR=coverage/generate ./coverage/templ-cover generate -include-version=false
<span><span>#</span> Run the unit tests.</span>
go <span>test</span> -cover ./... -args -test.gocoverdir=<span><span>"</span><span>$PWD</span>/coverage/unit<span>"</span></span>
<span><span>#</span> Display the combined percentage.</span>
go tool covdata percent -i=./coverage/fmt,./coverage/generate,./coverage/unit
<span><span>#</span> Generate a text coverage profile for tooling to use.</span>
go tool covdata textfmt -i=./coverage/fmt,./coverage/generate,./coverage/unit -o coverage.out
<span><span>#</span> Print total</span>
go tool cover -func coverage.out <span>|</span> grep total</pre></div>
<h3 tabindex="-1" dir="auto">benchmark</h3>
<p dir="auto">Run benchmarks.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go run ./cmd/templ generate -include-version=false &amp;&amp; go test ./... -bench=. -benchmem"><pre>go run ./cmd/templ generate -include-version=false <span>&amp;&amp;</span> go <span>test</span> ./... -bench=. -benchmem</pre></div>
<h3 tabindex="-1" dir="auto">fmt</h3>
<p dir="auto">Format all Go and templ code.</p>
<div dir="auto" data-snippet-clipboard-copy-content="gofmt -s -w .
go run ./cmd/templ fmt ."><pre>gofmt -s -w <span>.</span>
go run ./cmd/templ fmt <span>.</span></pre></div>
<h3 tabindex="-1" dir="auto">lint</h3>
<div dir="auto" data-snippet-clipboard-copy-content="golangci-lint run --verbose"><pre>golangci-lint run --verbose</pre></div>
<h3 tabindex="-1" dir="auto">release</h3>
<p dir="auto">Create production build with goreleaser.</p>
<div dir="auto" data-snippet-clipboard-copy-content="if [ &quot;${GITHUB_TOKEN}&quot; == &quot;&quot; ]; then echo &quot;No github token, run:&quot;; echo &quot;export GITHUB_TOKEN=`pass github.com/goreleaser_access_token`&quot;; exit 1; fi
./push-tag.sh
goreleaser --clean"><pre><span>if</span> [ <span><span>"</span><span>${GITHUB_TOKEN}</span><span>"</span></span> <span>==</span> <span><span>"</span><span>"</span></span> ]<span>;</span> <span>then</span> <span>echo</span> <span><span>"</span>No github token, run:<span>"</span></span><span>;</span> <span>echo</span> <span><span>"</span>export GITHUB_TOKEN=<span><span>`</span>pass github.com/goreleaser_access_token<span>`</span></span><span>"</span></span><span>;</span> <span>exit</span> 1<span>;</span> <span>fi</span>
./push-tag.sh
goreleaser --clean</pre></div>
<h3 tabindex="-1" dir="auto">docs-run</h3>
<p dir="auto">Run the development server.</p>
<p dir="auto">Directory: docs</p>

<h3 tabindex="-1" dir="auto">docs-build</h3>
<p dir="auto">Build production docs site.</p>
<p dir="auto">Directory: docs</p>

</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Remade the Fake Google Gemini Demo, Except Using GPT-4 and It's Real (402 pts)]]></title>
            <link>https://sagittarius.greg.technology/</link>
            <guid>38596953</guid>
            <pubDate>Mon, 11 Dec 2023 02:17:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sagittarius.greg.technology/">https://sagittarius.greg.technology/</a>, See on <a href="https://news.ycombinator.com/item?id=38596953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      

      <p>Please see below for a (real) demo. All the code is in <a href="https://github.com/gregsadetsky/sagittarius">this repo</a>! Cheers.</p>



<iframe width="100%" height="515" src="https://www.youtube.com/embed/__nL7Vc0OCg?si=tReedrTnNuSBFHXs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<hr>

<p>Made by <a href="https://greg.technology/">Greg Technology</a>.</p>


      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[John Carmack and John Romero reunited to talk DOOM on its 30th Anniversary (323 pts)]]></title>
            <link>https://www.pcgamer.com/for-dooms-30th-anniversary-the-johns-romero-and-carmack-reunited-to-celebrate-the-fps-that-changed-everything-i-want-to-thank-everybody-in-the-doom-community-for-keeping-this-game-alive/</link>
            <guid>38596634</guid>
            <pubDate>Mon, 11 Dec 2023 01:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/for-dooms-30th-anniversary-the-johns-romero-and-carmack-reunited-to-celebrate-the-fps-that-changed-everything-i-want-to-thank-everybody-in-the-doom-community-for-keeping-this-game-alive/">https://www.pcgamer.com/for-dooms-30th-anniversary-the-johns-romero-and-carmack-reunited-to-celebrate-the-fps-that-changed-everything-i-want-to-thank-everybody-in-the-doom-community-for-keeping-this-game-alive/</a>, See on <a href="https://news.ycombinator.com/item?id=38596634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>To celebrate the 30th anniversary of the launch of Doom, id Software co-founders John Carmack and John Romero <a data-analytics-id="inline-link" href="https://www.twitch.tv/videos/2000693432" target="_blank" data-url="https://www.twitch.tv/videos/2000693432">reunited to talk about the legendary FPS</a>. The discussion was moderated by <a data-analytics-id="inline-link" href="https://linktr.ee/davidlcraddock" target="_blank" data-url="https://linktr.ee/davidlcraddock">David Craddock</a> (The FPS Documentary, Long Live Mortal Kombat), with interview questions from Craddock and the Twitch chat.</p><p>The conversation was understandably warm and celebratory, but I was also surprised at how critical the two were of their own work. Carmack alluded to "flashier" (and potentially technically riskier) graphical effects he wishes he had built into Doom's engine, and he noted that he thinks the more grounded, military sci-fi aesthetic of Episode One has aged better than the abstract hellscapes later in the game.</p><p>Romero, meanwhile, contrasted Doom with the id games before and after, arguing it represented a technical "sweet spot" before Quake and full 3D acceleration started to seriously complicate development and limit how many enemies they could fit on screen. The developer praised Doom's engine for allowing more complex maps than Wolfenstein though, ruefully remarking that "Making levels for Wolfenstein had to be the most boring level design job ever."</p><p>The two also fondly reminisced about the technical limitations of the time. Carmack remarked that, although he thought id could "just sell [Doom] in a brown paper bag" off its quality alone, he was glad they went the extra mile with its iconic box art and marketing. Both devs expressed an appreciation for '90s PC big box packaging and accompanying "feelies" like cloth maps, and I'm 100% with them on that.</p><p>As far as development stories, I was struck by Romero's recollection of getting multiplayer working for the first time shortly before Doom's release: "I went into my office—I was making E1M7 at the time—I'm looking out the window and I'm seeing two characters fighting, rockets are flying up at a high window and someone is plasma gunning the other guy.</p><p>And I'm like, this is going to be the coolest fucking game the planet has ever seen, I can't wait to play that."</p><iframe width="620" height="378" scrolling="no" frameborder="0" data-lazy-priority="low" data-lazy-src="https://player.twitch.tv/?video=2000693432&amp;parent=www.pcgamer.com"></iframe><p>"I've said before that I'm not a very sentimental person, that I don't spend a lot of time reminiscing about the good old days," Carmack confided as a means of farewell, "But they were really quite good. I'm very proud of the things that we built back then and that they have this legacy that's lasted to this day."</p><p>Romero echoed the sentiment, thanking Carmack for the years they spent working together, and also extending his appreciation to the players who keep coming back to Doom: "I want to thank everybody in the Doom community for keeping this game alive. And really, just thank you for playing our games everybody."</p><p>You can check out the conversation in its entirety on <a data-analytics-id="inline-link" href="https://www.twitch.tv/videos/2000693432" target="_blank" data-url="https://www.twitch.tv/videos/2000693432">John Romero's Twitch channel</a>, and it's also a perfect time to dive into <a data-analytics-id="inline-link" href="https://romero.com/sigil" target="_blank" data-url="https://romero.com/sigil">Sigil 2</a>, the sequel to Romero's 2019 Doom megawad and subject of <a data-analytics-id="inline-link" href="https://www.pcgamer.com/pc-gamer-magazines-latest-issue-on-sale-now-sigil-ii-and-dooms-30th-anniversary/" target="_blank" data-before-rewrite-localise="https://www.pcgamer.com/pc-gamer-magazines-latest-issue-on-sale-now-sigil-ii-and-dooms-30th-anniversary//">PC Gamer's latest print cover story</a>. While you can pay for a full-on classico big box with all those feelies we love, both Sigil megawads are free to download.</p><p>If that's not enough WAD action for you, the megawad Eviternity <a data-analytics-id="inline-link" href="https://www.pcgamer.com/five-years-later-doom-megawad-eviternity-just-got-a-surprise-sequel/" target="_blank" data-before-rewrite-localise="https://www.pcgamer.com/five-years-later-doom-megawad-eviternity-just-got-a-surprise-sequel//">also just got a sequel campaign</a> to celebrate Doom's 30th birthday, and you can peruse the list of this year's <a data-analytics-id="inline-link" href="https://www.doomworld.com/cacowards/2023/index/" target="_blank" data-url="https://www.doomworld.com/cacowards/2023/index/">Cacowards</a> for more quality creations.</p>
</div><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-znKUehQFaNW6HSuLsNzvAR"><section><p>Sign up to get the best content of the week, and great gaming deals, as picked by the editors.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Could we stop Yellowstone from erupting with a geothermal power plant? (135 pts)]]></title>
            <link>https://www.construction-physics.com/p/could-we-stop-yellowstone-from-erupting</link>
            <guid>38596622</guid>
            <pubDate>Mon, 11 Dec 2023 01:12:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.construction-physics.com/p/could-we-stop-yellowstone-from-erupting">https://www.construction-physics.com/p/could-we-stop-yellowstone-from-erupting</a>, See on <a href="https://news.ycombinator.com/item?id=38596622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png" width="634" height="422.3763736263736" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:970,&quot;width&quot;:1456,&quot;resizeWidth&quot;:634,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540b2cb0-6cc0-4a5f-a9f0-93fb176e8086_2048x1365.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>It’s become fairly common knowledge that Yellowstone National Park, in addition to being incredibly beautiful, is sitting on top of an enormous </span><a href="https://en.wikipedia.org/wiki/Supervolcano" rel="">supervolcano</a><span> that catastrophically erupts every few hundred thousand years. Unlike normal volcanoes, which tend to produce large cone-shaped mountains made of ash and lava, supervolcano eruptions (defined being at least an 8 on the </span><a href="https://en.wikipedia.org/wiki/Volcanic_explosivity_index" rel="">Volcanic Explosivity Index</a><span>, meaning at least 1000 cubic kilometers of rock are released) are so enormous that the ground beneath the eruption collapses, forming a huge depression known as a caldera. Yellowstone Park largely exists within the </span><a href="https://en.wikipedia.org/wiki/Yellowstone_Caldera" rel="">Yellowstone Caldera</a><span>, formed during Yellowstone’s last large eruption 640,000 years ago.</span></p><p>(Not every Yellowstone eruption is a large, catastrophic one. Its most recent eruption, 70,000 years ago, was much smaller).</p><p><span>The supervolcano is the result of the </span><a href="https://en.wikipedia.org/wiki/Yellowstone_hotspot" rel="">Yellowstone hotspot</a><span>, an area of high temperature within the earth’s crust suspected to be caused by large mantle plume, an upwelling of the earth’s hot, molten mantle. The Yellowstone hotspot has gradually moved east over time as the earth’s tectonic plates have shifted, leaving behind a series of calderas that mark each large-scale eruption.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png" width="387" height="226" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:226,&quot;width&quot;:387,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f4e3766-7188-400d-a451-cd6fe61c129a_387x226.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Earth hasn’t seen a supervolcano eruption in recorded history, but if one were to occur the effects would be catastrophic. From “The Precipice”</p><blockquote><p><em>Everything within 100 kilometers of the blast is buried in falling rock, incandescent with heat. Thick ash rains down over the entire continent. When the Indonesian volcano, Toba, erupted 74,000 years ago, it covered India in a blanket of ash a meter thick and traces were found as far away as Africa…The dark volcanic dust and reflective sulfate aerosols unleashed by the Toba eruption caused a “volcanic winter,” which is thought to have lowered global temperatures by several degrees for several years. Even the much smaller eruption of Indonesia’s Mount Tambora in 1815 (less than a hundredth the size) caused a global cooling of 1°C, with places as far away as the United States suffering crop failure and June snows in what became known as the “year without a summer</em></p></blockquote><p><span>The volume of ash ejected into the atmosphere would be similar in magnitude to the </span><a href="https://en.wikipedia.org/wiki/Chicxulub_crater" rel="">asteroid impact that killed the dinosaurs</a><span> (along with 75% of all plant and animal species), and have similar effects:</span></p><blockquote><p><em>The effects may be roughly comparable to those of the one-to ten-kilometer asteroids, with major global crop failures lasting for years on end. Since the world only has about six months of food reserves, there is a possibility that billions of people could starve and that civilization could suffer a global collapse&nbsp;</em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg" width="674" height="532.015228426396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:622,&quot;width&quot;:788,&quot;resizeWidth&quot;:674,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;circles representing the volume of magma erupted from different volcanoes &quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="circles representing the volume of magma erupted from different volcanoes " title="circles representing the volume of magma erupted from different volcanoes " srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f311aa4-9895-4c3f-ad01-79554caad6d8_788x622.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Size of various volcano eruptions, via </span><a href="https://www.usgs.gov/faqs/what-a-supervolcano-what-a-supereruption" rel="">USGS</a></figcaption></figure></div><p><span>Unlike most catastrophic risks, however, addressing the risk of a supervolcano eruption also presents an opportunity. A volcano is fed by a magma chamber, a large mass of liquid and semi-liquid rock deep below the surface. In Yellowstone’s case, there are two magma chambers: a smaller, more liquid one 4 to 14 kilometers below the surface which is about 10% molten rock, and a larger one 20 to 45 kilometers below the surface which is about 2% molten rock (though more recent analyses suggest a </span><a href="https://www.science.org/doi/10.1126/science.ade0347" rel="">higher melt fraction</a><span>). Combined, these magma chambers are about 56,000 cubic kilometers in volume, and the lower chamber is thought to grow at a rate of about 0.3 cubic kilometers per year.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png" width="590" height="449.37161430119176" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/067953f4-4681-4029-b0ec-323922e98e55_923x703.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:703,&quot;width&quot;:923,&quot;resizeWidth&quot;:590,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F067953f4-4681-4029-b0ec-323922e98e55_923x703.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Yellowstone magma chambers, via </span><a href="https://www.discovermagazine.com/environment/theres-more-magma-under-yellowstone-than-we-thought" rel="">Discover</a></figcaption></figure></div><p>If heat could be bled off of the magma chambers, cooling and solidifying them, not only would that (theoretically) stop the volcano from erupting, but the heat could be used to generate electric power with a geothermal energy plant. Depending on the size of the plant and how quickly it bled off the heat, this could be a very large amount of electricity - enough to power the entire US.</p><p><span>The earth contains an enormous amount of heat - the crust alone has </span><a href="https://www.elidourado.com/p/geothermal" rel="">41 times more thermal energy</a><span> than all known petroleum and nuclear fuel reserves </span><em>combined</em><span>. Geothermal energy plants tap this heat to generate electric power. Though there are different technologies available, they all use the same basic principle - a plant draws up heated water (or some other heat-conducting fluid) from within the earth, and uses it to create steam to drive a generator. The fluid then gets pumped back into the earth to absorb more heat.</span></p><p><span>Historically, geothermal energy plants have been limited to places where heat from the interior was unusually close to the surface, and there was a ready supply of heated water flowing through permeable rock that could be tapped for power. The largest geothermal plant in the world, </span><a href="https://en.wikipedia.org/wiki/The_Geysers" rel="">The Geysers in California</a><span>, is built on the site of large naturally occurring hot springs. But geothermal energy is available anywhere if you can drill deep enough and find a way to pump fluid in and out. Increasingly, people are investigating </span><a href="https://en.wikipedia.org/wiki/Enhanced_geothermal_system" rel="">enhanced geothermal systems</a><span> (EGS), which can create geothermal power nearly anywhere by drilling very deeply and creating permeable rock with fracking techniques.</span></p><p>As you might expect, the magma chamber beneath Yellowstone releases an enormous amount of heat. Yellowstone is hotter 7 kilometers down than almost anywhere else in the US. A large enough geothermal plant could tap this heat, generating electric power while at the same time reducing or eliminating the risk of a catastrophic eruption.</p><p><span>I’m aware of two proposals for doing this - one produced by </span><a href="https://scienceandtechnology.jpl.nasa.gov/sites/default/files/documents/DefendingCivilizationFromSupervolcanos20151015.pdf" rel="">NASA’s Jet Propulsion Laboratory</a><span> in 2017, and a more recent one by </span><a href="https://www.sciencedirect.com/science/article/pii/S0960148122012290" rel="">Thomas Arciuolo and Miad Faezipour</a><span>.</span></p><p>The JPL proposal is primarily aimed at reducing the risk of Yellowstone erupting, with electric power generation as a side benefit. The authors note that given the size of past Yellowstone eruptions, and the span of time between them, the rate that energy builds below the volcano is only around 1.5 gigawatts - less heat than a typical power plant sheds. Yellowstone currently bleeds heat at a rate of about 4.5 to 6 gigawatts, mostly through heated water moving below the surface. You'd thus (theoretically) only need to increase the heat bleed by around 35% to stop energy accumulating and stop future eruptions. [0]</p><p><span>Drilling into, or above, the magma chamber below Yellowstone would be fraught with problems. For one, it could potentially cause the volcano to erupt, the very thing we’re trying to avoid. For another, the environment above the magma body isn’t solid rock but is a “mush” of rock, acid, and brine that would be incredibly difficult to drill through. And even if you could successfully drill through it, the environment is so corrosive that </span><a href="http://www.geothermal-energy.org/pdf/IGAstandard/Japan/1997/Lichti.pdf" rel="">even corrosion-resistant materials</a><span> would start to corrode within days of exposure.</span></p><p>Instead, the authors suggest constructing a series of 160 geothermal plants around the perimeter of the magma body, spaced every 1.5 kilometers. Each plant would drill a pair of wells (a production well to draw water from, and an injection well to reinject it) up to 10 kilometers deep. At the projected flow rate of an enhanced geothermal system (80 kilograms a second), 160 plants together would extract about 20 gigawatts of heat energy, and generate around 3.5 gigawatts of electric power. At a projected cost of $1 per watt, such a system would cost on the order of $3.5 billion, and generate electric power for less than ten cents a kilowatt-hour, though I have been told that $1 per watt is an extremely optimistic projection for EGS cost.</p><p>This system would create a “ring” of cooled rock around the magma chamber around 50 meters wide. Every 50 years, a new series of plants would be constructed in a slightly smaller perimeter (or new wells would be directionally drilled from the existing plants), slowly growing the ring of cooled rock around the magma chamber. At this rate, the energy of the next eruption would be drained after about 16,000 years, and in less than 50,000 years the magma chamber would be cooled completely. While this is a long time, it’s short in the context of Yellowstone, which has only had three large eruptions in the past 2.1 million years.</p><p><span>3.5 gigawatts is less than the capacity of a </span><a href="https://en.wikipedia.org/wiki/Palo_Verde_Nuclear_Generating_Station" rel="">single nuclear power plant.</a><span> But the authors of the JPL paper note that “It is…straightforward to imagine that the entire capacity could be made several times the nominal 20 GW considered here, reducing proportionately the time to drain the heat from the magma chamber”. This is roughly what Arciuolo and Faezipour (which I’ll shorten to A&amp;F) propose.</span></p><p><span>Conceptually, A&amp;F’s proposal isn’t all that different from the JPL proposal, but it’s much larger in scale. In the A&amp;F proposal, 100 shafts eight meters in diameter would be drilled eight kilometers down into the Yellowstone magma chamber. (By comparison, a typical geothermal well is around one meter in diameter at the top, and narrows as it goes down). A typical oil and gas well is eight to twelve inches in diameter). A large central pipe would extend to the bottom of the shaft, where it would split into ten smaller pipes and come back up, where it would feed into a large steam turbine. Water (the authors suggest using </span><a href="https://en.wikipedia.org/wiki/Shoshone_Lake" rel="">Shoshone Lake</a><span> as a source) would be pumped down the central pipe, and it would come back up as superheated steam to drive the turbines.</span></p><p>A&amp;F suggest gold-plated copper for the piping - gold-plating for corrosion resistance, and copper for heat conduction. Whereas the JPL proposal only extracted 20 gigawatts of heat from the ground, the A&amp;F system is projected to extract more than 1200 gigawatts, over 60 times as much. Rather than taking 50,000 years to drain heat from the magma chamber, the A&amp;F proposal would do it in around 830 years.</p><p>Other than the scale, the A&amp;F proposal mostly differs from the JPL proposal in the details of the proposed energy extraction system. Unfortunately, as far as I can tell the A&amp;F system is not especially well thought out, and most of the details provided don’t make much sense.</p><p><span>For one, the choice of gold-plated copper as a construction material is bizarre, and not appropriate for the harsh conditions of a geothermal well. Copper is comparatively soft and weak, and it loses its already low strength rapidly at elevated temperatures - not ideal for a geothermal well where temperatures are likely to exceed 1000 degrees Fahrenheit. Existing geothermal wells typically use steel or (for highly corrosive environments) </span><a href="https://www1.eere.energy.gov/geothermal/pdfs/drillinghandbook.pdf" rel="">titanium</a><span> for the pipes and casing. The </span><a href="https://nea.is/geothermal/the-iceland-deep-drilling-project/" rel="">Iceland Deep Drilling Project</a><span>, an experimental project which drilled a well into magma and generated power from the resulting steam, used </span><a href="https://www.sciencedirect.com/science/article/pii/S0375650513000606" rel="">K55 steel</a><span> for the well casing.</span></p><p><span>Gold-plating the piping likewise doesn’t seem to make much sense. While gold-plating is often used for electrical connectors to provide corrosion resistance, the thin layer of gold wears away over time. Gold-plated jewelry wears through after a few years, and even </span><a href="https://www.radiall.com/media/Guide%20PLATING%20D1C004XEe.pdf" rel="">mil-spec gold-plated connectors</a><span> have a lifespan of a few thousand mating cycles (times being connected/reconnected). The environment in a geothermal well, where the piping would be exposed to high temperature water and steam flowing at thousands of liters a minute, and acid and brine-soaked rocks, is going to wear through a thin layer of gold plating much faster. It goes without saying that current geothermal wells do not use gold plated piping.</span></p><p><span>A&amp;F calculated that their system would generate about 1260 gigawatts of electricity, more than the entire electric generation capacity of the US (</span><a href="https://www.eia.gov/energyexplained/electricity/electricity-in-the-us-generation-capacity-and-sales.php" rel="">1143 gigawatts as of 2021</a><span>). But this calculation is incorrect. The authors assume that the conversion of heat to electricity would be roughly 90% efficient based on the efficiency of the turbine (minus a very small amount of energy needed to pump the water). But this 90% is the efficiency of the </span><em>turbine</em><span>, based on comparing it to an </span><a href="https://en.wikipedia.org/wiki/Isentropic_process" rel="">ideal, zero entropy turbine</a><span> - it’s not the efficiency of the overall process. Any heat engine is fundamentally limited in how efficiently it can turn heat into useful work, and that limit is much lower than 90%. Existing geothermal plants operate at </span><a href="https://www.geothermal-energy.org/pdf/IGAstandard/NZGW/2012/46654final00097.pdf" rel="">roughly 10-15% thermal efficiency</a><span>. Coal plants which use supercritical steam </span><a href="https://www.ge.com/steam-power/coal-power-plant/usc-ausc" rel="">can approach 50% thermal efficiency</a><span>. The JPL proposal, which uses a similar temperature steam as the A&amp;F proposal, calculated a 17% thermal efficiency. So A&amp;F have overestimated how much electricity their system will generate by about a factor of five.</span></p><p>There’s also various other strange things about this proposal. A&amp;F don’t seem to be using insulated pipes (in their analysis steam temperature falls from more than 1400 degrees F at the bottom of the shaft to around 600 degrees F as it travels back up from the magma chamber), even though with insulation losses should be on the order of a few percent (another reason why it doesn’t make sense to use copper). Their arrangement of the pipes (the downflow on the inside, and the upflow on the outside) is backwards - the water on its way down should be on the outside, to absorb heat from the surrounding rock. An eight-meter shaft (which would require a much larger diameter near the surface) would, as the authors note, require something akin to a tunnel-boring machine to drill, and would be vastly more expensive than using a larger number of smaller shafts.</p><p>Regardless of the specifics of the details, in the abstract, such a scheme is in some sense, possible. How should we consider it?</p><p><span>On the one hand, building a huge geothermal power station at Yellowstone would generate a large amount of (potentially cheap) electric power while simultaneously reducing a catastrophic risk. Moreover, developing the technology to build it would have benefits that would extend beyond Yellowstone. Enhanced geothermal technology is still in the very early stages, and the advances required to build an enormous plant at Yellowstone could profitably be applied to other plants. Likewise, the knowledge gained figuring out how to stop one supervolcano from erupting could be applied to preventing </span><em>other</em><span> supervolcanoes from erupting. And more broadly, stopping supervolcanoes from erupting is&nbsp; something we as a civilization are likely underinvesting in. In “The Precipice'', Toby Ord estimates that the risk to civilization from supervolcano eruption is 100 times greater than the risk from asteroid and comet impact, which NASA spends </span><a href="https://www.planetary.org/articles/nasas-planetary-defense-budget-growth" rel="">over $100 million on each year</a><span>. Based on that, we should be spending at least on the order of $10 billion every year on preventing supervolcano eruptions.</span></p><p><span>On the other hand, in many ways Yellowstone is a particularly bad place to try to build such a plant. The harsh, corrosive conditions in and around the magma chamber would make drilling the wells especially difficult, and its location in the middle of nowhere would require the construction of enormous transmission lines (something the US is </span><a href="https://www.cnbc.com/2023/02/21/why-its-so-hard-to-build-new-electrical-transmission-lines-in-the-us.html" rel="">not so great at</a><span>). One of the main benefits of enhanced geothermal is that you can construct it nearly anywhere, enabling things like using geothermal directly for industrial process heat (eliminating wasteful conversions of heat to electricity to heat again), or repowering existing coal plants (reusing their steam turbines and other electrical infrastructure). A massive Yellowstone geothermal plant gives up a lot of these benefits.</span></p><p><span>And while Yellowstone is at risk of a catastrophic super-eruption, it’s extensively monitored by the </span><a href="https://www.usgs.gov/volcanoes/yellowstone" rel="">Yellowstone Volcano Observatory</a><span>. We’d (hopefully!) have advanced warning long before Yellowstone actually erupted. A much bigger risk is likely large eruptions from volcanoes that we aren’t tracking and have no data for. Data from ice cores suggests there have been 97 large-magnitude volcano eruptions in the last 60,000 years, but only a handful of those can be attributed to specific volcanoes. Of all the volcano eruptions that have occurred since 1950, </span><a href="https://www.nature.com/articles/d41586-022-02177-x#ref-CR2" rel="">only 27%</a><span> were monitored by any sort of instrument such as a seismometer prior to eruption. There are no dedicated satellites for monitoring potential volcano eruptions.</span></p><p><span>In any case, the debate is likely to remain academic for the foreseeable future. Using Yellowstone for geothermal power was made illegal by the Geothermal Steam Act of 1970, which “</span><a href="https://www.usgs.gov/observatories/yvo/news/why-cant-we-drill-yellowstone-stop-eruptions-and-make-power" rel="">requires the Department of the Interior to preserve and monitor hydrothermal features like Old Faithful</a><span>”. And while there’s increasing realization that </span><a href="https://constructionphysics.substack.com/p/how-nepa-works" rel="">extensive environmental review</a><span> can often </span><a href="https://progress.institute/environmental-review/" rel="">harm the environment</a><span>, and that some kind of permitting reform will be necessary to build out </span><a href="https://www.motherjones.com/environment/2023/04/electrify-everything-scope-data/" rel="">low-carbon energy infrastructure</a><span>, we’re still in the early days. Trying to build an enormous geothermal power plant and associated transmission lines(!) in one of the most beloved National Parks(!!), which there’s specifically a law against (!!!), and which could potentially trigger a civilization-destroying volcanic eruption (!!!!) is like the </span><em>final boss</em><span> of the permitting reform movement. [1]</span></p><p><em>(Thanks to Austin Vernon for reading a draft of this. All errors are my own!)</em></p><p><em>[0] - If instead you look at the rate that magma is accumulating in the lower magma chamber, you get a larger, but still manageable number of around 22 gigawatts.</em></p><p><em>[1] - The only thing it's missing is rich NIMBYs who would be inconvenienced by the construction.</em></p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>