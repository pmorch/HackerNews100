<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 19 Jun 2025 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Andrej Karpathy: Software in the era of AI [video] (182 pts)]]></title>
            <link>https://www.youtube.com/watch?v=LCEmiRjPEtQ</link>
            <guid>44314423</guid>
            <pubDate>Thu, 19 Jun 2025 00:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ">https://www.youtube.com/watch?v=LCEmiRjPEtQ</a>, See on <a href="https://news.ycombinator.com/item?id=44314423">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Unregistry – "docker push" directly to servers without a registry (238 pts)]]></title>
            <link>https://github.com/psviderski/unregistry</link>
            <guid>44314085</guid>
            <pubDate>Wed, 18 Jun 2025 23:17:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/psviderski/unregistry">https://github.com/psviderski/unregistry</a>, See on <a href="https://news.ycombinator.com/item?id=44314085">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/psviderski/unregistry/blob/main/.github/images/logo-light.svg#gh-light-mode-only"><img src="https://github.com/psviderski/unregistry/raw/main/.github/images/logo-light.svg#gh-light-mode-only" alt="Unregistry logo"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/psviderski/unregistry/blob/main/.github/images/logo-dark.svg#gh-dark-mode-only"><img src="https://github.com/psviderski/unregistry/raw/main/.github/images/logo-dark.svg#gh-dark-mode-only" alt="Unregistry logo"></a></p><p dir="auto"><strong>▸ Push docker images directly to remote servers without an external registry ◂</strong></p>
  <p dir="auto">
    <a href="https://discord.gg/eR35KQJhPu" rel="nofollow"><img src="https://camo.githubusercontent.com/bce982f6fd064725216dd2c5bbfc5fa12afeaee92ae5d356ea3f5abf78d0ad88/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d3538363546322e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Join Discord" data-canonical-src="https://img.shields.io/badge/discord-5865F2.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white"></a>
    <a href="https://x.com/psviderski" rel="nofollow"><img src="https://camo.githubusercontent.com/7ef3b84c3487de0b84ead3bc3ac609c4cbaa26f231976bbdf4e56b855dfaf42d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f666f6c6c6f772d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d58266c6f676f436f6c6f723d7768696c65" alt="Follow on X" data-canonical-src="https://img.shields.io/badge/follow-black?style=for-the-badge&amp;logo=X&amp;logoColor=while"></a>
  </p>
</div>
<p dir="auto">Unregistry is a lightweight container image registry that stores and serves images directly from your Docker daemon's
storage.</p>
<p dir="auto">The included <code>docker pussh</code> command (extra 's' for SSH) lets you push images straight to remote Docker servers over SSH.
It transfers only the missing layers, making it fast and efficient.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description docker-pussh-demo.mp4">docker-pussh-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/783910/456396282-9d704b87-8e0d-4c8a-9544-17d4c63bd050.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAyOTY5MDEsIm5iZiI6MTc1MDI5NjYwMSwicGF0aCI6Ii83ODM5MTAvNDU2Mzk2MjgyLTlkNzA0Yjg3LThlMGQtNGM4YS05NTQ0LTE3ZDRjNjNiZDA1MC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwMTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NWQzNjViYWFlNTI3MjA0MTgxOTUyZWZiMDc5NTkxMThmMjM2MjQwNDI3MWU3ZDc1YTg0NGRmMjcyNTFmYWNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.kp-WEQJdsYOZa8e8DyU8_JbguC9jt8GXLWSwGsF4o4k" data-canonical-src="https://private-user-images.githubusercontent.com/783910/456396282-9d704b87-8e0d-4c8a-9544-17d4c63bd050.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAyOTY5MDEsIm5iZiI6MTc1MDI5NjYwMSwicGF0aCI6Ii83ODM5MTAvNDU2Mzk2MjgyLTlkNzA0Yjg3LThlMGQtNGM4YS05NTQ0LTE3ZDRjNjNiZDA1MC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwMTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NWQzNjViYWFlNTI3MjA0MTgxOTUyZWZiMDc5NTkxMThmMjM2MjQwNDI3MWU3ZDc1YTg0NGRmMjcyNTFmYWNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.kp-WEQJdsYOZa8e8DyU8_JbguC9jt8GXLWSwGsF4o4k" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">The problem</h2><a id="user-content-the-problem" aria-label="Permalink: The problem" href="#the-problem"></a></p>
<p dir="auto">You've built a Docker image locally. Now you need it on your server. Your options suck:</p>
<ul dir="auto">
<li><strong>Docker Hub / GitHub Container Registry</strong> - Your code is now public, or you're paying for private repos</li>
<li><strong>Self-hosted registry</strong> - Another service to maintain, secure, and pay for storage</li>
<li><strong>Save/Load</strong> - <code>docker save | ssh | docker load</code> transfers the entire image, even if 90% already exists on the server</li>
<li><strong>Rebuild remotely</strong> - Wastes time and server resources. Plus now you're debugging why the build fails in production</li>
</ul>
<p dir="auto">You just want to move an image from A to B. Why is this so hard?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The solution</h2><a id="user-content-the-solution" aria-label="Permalink: The solution" href="#the-solution"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server"><pre>docker pussh myapp:latest user@server</pre></div>
<p dir="auto">That's it. Your image is on the remote server. No registry setup, no subscription, no intermediate storage, no
exposed ports. Just a <strong>direct transfer</strong> of the <strong>missing layers</strong> over SSH.</p>
<p dir="auto">Here's what happens under the hood:</p>
<ol dir="auto">
<li>Establishes SSH tunnel to the remote server</li>
<li>Starts a temporary unregistry container</li>
<li>Forwards a random localhost port to the unregistry port over the tunnel</li>
<li><code>docker push</code> to unregistry through the forwarded port, transferring only the layers that don't already exist
remotely. The transferred image is instantly available on the remote Docker daemon</li>
<li>Stops the unregistry container and closes the SSH tunnel</li>
</ol>
<p dir="auto">It's like <code>rsync</code> for Docker images — simple and efficient.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Unregistry was created for <a href="https://github.com/psviderski/uncloud">Uncloud</a>, a lightweight tool for deploying
containers across multiple Docker hosts. We needed something simpler than a full registry but more efficient than
save/load.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">macOS/Linux via Homebrew</h3><a id="user-content-macoslinux-via-homebrew" aria-label="Permalink: macOS/Linux via Homebrew" href="#macoslinux-via-homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install psviderski/tap/docker-pussh"><pre>brew install psviderski/tap/docker-pussh</pre></div>
<p dir="auto">After installation, to use <code>docker-pussh</code> as a Docker CLI plugin (<code>docker pussh</code> command) you need to create a symlink:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir -p ~/.docker/cli-plugins
ln -sf $(brew --prefix)/bin/docker-pussh ~/.docker/cli-plugins/docker-pussh"><pre>mkdir -p <span>~</span>/.docker/cli-plugins
ln -sf <span><span>$(</span>brew --prefix<span>)</span></span>/bin/docker-pussh <span>~</span>/.docker/cli-plugins/docker-pussh</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">macOS/Linux via direct download</h3><a id="user-content-macoslinux-via-direct-download" aria-label="Permalink: macOS/Linux via direct download" href="#macoslinux-via-direct-download"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download the latest version
curl -sSL https://raw.githubusercontent.com/psviderski/unregistry/main/docker-pussh \
  -o ~/.docker/cli-plugins/docker-pussh

# Make it executable
chmod +x ~/.docker/cli-plugins/docker-pussh"><pre><span><span>#</span> Download the latest version</span>
curl -sSL https://raw.githubusercontent.com/psviderski/unregistry/main/docker-pussh \
  -o <span>~</span>/.docker/cli-plugins/docker-pussh

<span><span>#</span> Make it executable</span>
chmod +x <span>~</span>/.docker/cli-plugins/docker-pussh</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Windows is not currently supported, but you can try using <a href="https://docs.docker.com/desktop/features/wsl/" rel="nofollow">WSL 2</a>
with the above Linux instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Verify installation</h3><a id="user-content-verify-installation" aria-label="Permalink: Verify installation" href="#verify-installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Push an image to a remote server. Please make sure the SSH user has permissions to run <code>docker</code> commands (user is
<code>root</code> or non-root user is in <code>docker</code> group). If <code>sudo</code> is required, ensure the user can run <code>sudo docker</code> without
a password prompt.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server.example.com"><pre>docker pussh myapp:latest user@server.example.com</pre></div>
<p dir="auto">With SSH key authentication if the private key is not added to your SSH agent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest ubuntu@192.168.1.100 -i ~/.ssh/id_rsa"><pre>docker pussh myapp:latest ubuntu@192.168.1.100 -i <span>~</span>/.ssh/id_rsa</pre></div>
<p dir="auto">Using a custom SSH port:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server:2222"><pre>docker pussh myapp:latest user@server:2222</pre></div>
<p dir="auto">Push a specific platform for a multi-platform image. The local Docker has to use
<a href="https://docs.docker.com/desktop/features/containerd/" rel="nofollow">containerd image store</a> to support multi-platform images.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server --platform linux/amd64"><pre>docker pussh myapp:latest user@server --platform linux/amd64</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy to production servers</h3><a id="user-content-deploy-to-production-servers" aria-label="Permalink: Deploy to production servers" href="#deploy-to-production-servers"></a></p>
<p dir="auto">Build locally and push directly to your production servers. No middleman.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build --platform linux/amd64 -t myapp:1.2.3 .
docker pussh myapp:1.2.3 deploy@prod-server
ssh deploy@prod-server docker run -d myapp:1.2.3"><pre>docker build --platform linux/amd64 -t myapp:1.2.3 <span>.</span>
docker pussh myapp:1.2.3 deploy@prod-server
ssh deploy@prod-server docker run -d myapp:1.2.3</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CI/CD pipelines</h3><a id="user-content-cicd-pipelines" aria-label="Permalink: CI/CD pipelines" href="#cicd-pipelines"></a></p>
<p dir="auto">Skip the registry complexity in your pipelines. Build and push directly to deployment targets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: Build and deploy
  run: |
    docker build -t myapp:${{ github.sha }} .
    docker pussh myapp:${{ github.sha }} deploy@staging-server"><pre>- <span>name</span>: <span>Build and deploy</span>
  <span>run</span>: <span>|</span>
<span>    docker build -t myapp:${{ github.sha }} .</span>
<span>    docker pussh myapp:${{ github.sha }} deploy@staging-server</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homelab and air-gapped environments</h3><a id="user-content-homelab-and-air-gapped-environments" aria-label="Permalink: Homelab and air-gapped environments" href="#homelab-and-air-gapped-environments"></a></p>
<p dir="auto">Distribute images in isolated networks without exposing them to the internet.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh image:latest user@192.168.1.100"><pre>docker pussh image:latest user@192.168.1.100</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On local machine</h3><a id="user-content-on-local-machine" aria-label="Permalink: On local machine" href="#on-local-machine"></a></p>
<ul dir="auto">
<li>Docker CLI with plugin support (Docker 19.03+)</li>
<li>OpenSSH client</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">On remote server</h3><a id="user-content-on-remote-server" aria-label="Permalink: On remote server" href="#on-remote-server"></a></p>
<ul dir="auto">
<li>Docker is installed and running</li>
<li>SSH user has permissions to run <code>docker</code> commands (user is <code>root</code> or non-root user is in <code>docker</code> group)</li>
<li>If <code>sudo</code> is required, ensure the user can run <code>sudo docker</code> without a password prompt</li>
</ul>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">The remote Docker daemon works best with <a href="https://docs.docker.com/engine/storage/containerd/" rel="nofollow">containerd image store</a>
enabled. This allows unregistry to access images more efficiently.</p>
<p dir="auto">Add the following configuration to <code>/etc/docker/daemon.json</code> on the remote server and restart the <code>docker</code> service:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;features&quot;: {
    &quot;containerd-snapshotter&quot;: true
  }
}"><pre>{
  <span>"features"</span>: {
    <span>"containerd-snapshotter"</span>: <span>true</span>
  }
}</pre></div>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Advanced usage</h2><a id="user-content-advanced-usage" aria-label="Permalink: Advanced usage" href="#advanced-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running unregistry standalone</h3><a id="user-content-running-unregistry-standalone" aria-label="Permalink: Running unregistry standalone" href="#running-unregistry-standalone"></a></p>
<p dir="auto">Sometimes you want a local registry without the overhead. Unregistry works great for this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run unregistry locally and expose it on port 5000
docker run -d -p 5000:5000 --name unregistry \
  -v /run/containerd/containerd.sock:/run/containerd/containerd.sock \
  ghcr.io/psviderski/unregistry

# Use it like any registry
docker tag myapp:latest localhost:5000/myapp:latest
docker push localhost:5000/myapp:latest"><pre><span><span>#</span> Run unregistry locally and expose it on port 5000</span>
docker run -d -p 5000:5000 --name unregistry \
  -v /run/containerd/containerd.sock:/run/containerd/containerd.sock \
  ghcr.io/psviderski/unregistry

<span><span>#</span> Use it like any registry</span>
docker tag myapp:latest localhost:5000/myapp:latest
docker push localhost:5000/myapp:latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom SSH options</h3><a id="user-content-custom-ssh-options" aria-label="Permalink: Custom SSH options" href="#custom-ssh-options"></a></p>
<p dir="auto">Need custom SSH settings? Use the standard SSH config file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ~/.ssh/config
Host prod-server
    HostName server.example.com
    User deploy
    Port 2222
    IdentityFile ~/.ssh/deploy_key

# Now just use
docker pussh myapp:latest prod-server"><pre><span><span>#</span> ~/.ssh/config</span>
Host prod-server
    HostName server.example.com
    User deploy
    Port 2222
    IdentityFile <span>~</span>/.ssh/deploy_key

<span><span>#</span> Now just use</span>
docker pussh myapp:latest prod-server</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Found a bug or have a feature idea? We'd love your help!</p>
<ul dir="auto">
<li>🐛 Found a bug? <a href="https://github.com/psviderski/unregistry/issues">Open an issue</a></li>
<li>💡 Have ideas or need help? <a href="https://discord.gg/eR35KQJhPu" rel="nofollow">Join Uncloud Discord community</a> where we discuss features,
roadmap, implementation details, and help each other out.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration &amp; acknowledgements</h2><a id="user-content-inspiration--acknowledgements" aria-label="Permalink: Inspiration &amp; acknowledgements" href="#inspiration--acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://github.com/spegel-org/spegel">Spegel</a> - P2P container image registry that inspired me to implement a
registry that uses containerd image store as a backend.</li>
<li><a href="https://github.com/distribution/distribution">Docker Distribution</a> - the bulletproof Docker registry implementation
that unregistry uses as a base.</li>
</ul>

<p>
  Built with ❤️ by <a href="https://github.com/psviderski">Pasha Sviderski</a> who just wanted to deploy his images
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New US visa rules will force foreign students to unlock social media profiles (179 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening</link>
            <guid>44314054</guid>
            <pubDate>Wed, 18 Jun 2025 23:11:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening">https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening</a>, See on <a href="https://news.ycombinator.com/item?id=44314054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Foreign students will be required to unlock their social media profiles to allow US diplomats to review their online activity before receiving educational and exchange visas, the state department has announced. Those who fail to do so will be suspected of hiding that activity from US officials.</p><p>The new guidance, unveiled by the state department on Wednesday, directs US diplomats to conduct an online presence review to look for “any indications of hostility toward the citizens, culture, government, institutions, or founding principles of the United States”.</p><p>A cable separately obtained by Politico also instructs diplomats to flag any “advocacy for, aid or support for foreign terrorists and other threats to US national security” and “support for unlawful antisemitic harassment or violence”.</p><p>The screening for “antisemitic” activity matches similar guidance given at US Citizenship and Immigration Services under the Department of Homeland Security and has been criticised as an effort to crack down on opposition to the conduct of Israel’s war in Gaza.</p><p>The new state department checks are directed at students and other applicants for visas in the F, M and J categories, which refer to academic and vocational education, as well as cultural exchanges.</p><p>“It is an expectation from American citizens that their government will make every effort to make our country safer, and that is exactly what the <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link" data-component="auto-linked-tag">Trump administration</a> is doing every single day,” said a senior state department official, adding that Marco Rubio was “helping to make America and its universities safer while bringing the state Department into the 21st century”.</p><figure id="2c0c0441-fde7-4b33-9f05-c7264f202232" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:6,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;US issues broad order to consulates to vet student visas over ‘terrorist activity’&quot;,&quot;elementId&quot;:&quot;2c0c0441-fde7-4b33-9f05-c7264f202232&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/mar/28/student-visa-applications-denials&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>The Trump administration paused the issuance of new education visas late last month as it mulled new social media vetting strategies. The US had also targeted Chinese students for special scrutiny amid a tense negotiation over tariffs and the supply of rare-earth metals and minerals to the United States.</p><p>The state department directive allowed diplomatic posts to resume the scheduling of interviews for educational and exchange visas, but added that consular officers would conduct a “comprehensive and thorough vetting” of all applicants applying for F, M and J visas.</p><p>“To facilitate this vetting, all applicants for F, M and J non-immigrant visas will be asked to adjust the privacy settings on all their social media profiles to ‘public’”, the official said. “The enhanced social media vetting will ensure we are properly screening every single person attempting to visit our country.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Websites Are Tracking You via Browser Fingerprinting (166 pts)]]></title>
            <link>https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html</link>
            <guid>44313206</guid>
            <pubDate>Wed, 18 Jun 2025 20:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html">https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html</a>, See on <a href="https://news.ycombinator.com/item?id=44313206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent" name="Main Page Content" role="main" aria-labelledby="maincontent">
						
					
																																		                	            	     			<div aria-label="Websites Are Tracking You Via Browser Fingerprinting content 0" role="region"><p>Clearing your cookies is not enough to protect your privacy online.&nbsp;</p>
<p><a href="https://dl.acm.org/doi/10.1145/3696410.3714548" rel="noopener" target="_blank">New research</a> led by Texas A&amp;M University found that websites are covertly using browser fingerprinting — a method to uniquely identify a web browser — to track people across browser sessions and sites.</p>
<p>“Fingerprinting has always been a concern in the privacy community, but until now, we had no hard proof that it was actually being used to track users,” said Dr. Nitesh Saxena, cybersecurity researcher, professor of computer science and engineering and associate director of the Global Cyber Research Institute&nbsp;at Texas A&amp;M. “Our work helps close that gap.”</p>
<p>When you visit a website, your browser shares a surprising amount of information, like your screen resolution, time zone, device model and more. When combined, these details create a “fingerprint” that’s often unique to your browser. Unlike cookies — which users can delete or block — fingerprinting is much harder to detect or prevent. Most users have no idea it’s happening, and even privacy-focused browsers struggle to fully block it.</p>
<p>“Think of it as a digital signature you didn’t know you were leaving behind,” explained co-author Zengrui Liu, a former doctoral student in Saxena’s lab. “You may look anonymous, but your device or browser gives you away.”</p>
<p>This research marks a turning point in how computer scientists understand the real-world use of browser fingerprinting by connecting it with the use of ads.</p>
<p>“While prior works have studied browser fingerprinting and its usage on different&nbsp;websites, ours is the first to correlate browser fingerprints and ad behaviors, essentially establishing the relationship between web tracking and fingerprinting,” said co-author Dr. Yinzhi Cao, associate professor of computer science and technical director of the Information Security Institute at Johns Hopkins University.</p></div>
				
			
																																					
	
	
		
	
				
																		
																								
								
						
						
										
						
						
								
								
						
						
						
						
														
		
									        	        		<div>
        			<p>Think of it as a digital signature you didn’t know you were leaving behind. You may look anonymous, but your device or browser gives you away.</p>
        			<p><cite>Zengrui Liu</cite>
        		</p></div>
        	        				
			
																																		                	            	     			<div aria-label="Websites Are Tracking You Via Browser Fingerprinting content 1" role="region"><p>To investigate whether websites are using fingerprinting data to track people, the researchers had to go beyond simply scanning websites for the presence of fingerprinting code. They developed a measurement framework called FPTrace, which assesses fingerprinting-based user tracking by analyzing how ad systems respond to changes in browser fingerprints. This approach is based on the insight that if browser fingerprinting influences tracking, altering fingerprints should affect advertiser bidding — where ad space is sold in real time based on the profile of the person viewing the website — and HTTP records — records of communication between a server and a browser.&nbsp;</p>
<p>“This kind of analysis lets us go beyond the surface,” said co-author Jimmy Dani, Saxena’s doctoral student. “We were able to detect not just the presence of fingerprinting, but whether it was being used to <i>identify</i> and <i>target</i> users — which is much harder to prove.”</p>
<p>The researchers found that tracking occurred even when users cleared or deleted cookies. The results showed notable differences in bid values and a decrease in HTTP records and syncing events when fingerprints were changed, suggesting an impact on targeting and tracking.</p>
<p>Additionally, some of these sites linked fingerprinting behavior to backend bidding processes — meaning fingerprint-based profiles were being used in real time, likely to tailor responses to users or pass along identifiers to third parties.&nbsp;</p>
<p>Perhaps more concerning, the researchers found that even users who explicitly opt out of tracking under privacy laws like Europe’s General Data Protection Regulation (GDPR) and California’s California Consumer Privacy Act (CCPA) may still be silently tracked across the web through browser fingerprinting.</p>
<p>Based on the results of this study, the researchers argue that current privacy tools and policies are not doing enough. They call for stronger defenses in browsers and new regulatory attention on fingerprinting practices. They hope that their FPTrace framework can help regulators audit websites and providers who participate in such activities, especially without user consent.&nbsp;</p>
<p>This research was conducted in collaboration with Johns Hopkins University and presented at the ACM Web Conference (WWW) 2025.</p>
<p>Funding for this research is administered by the Texas A&amp;M Engineering Experiment Station (TEES), the official research agency for Texas A&amp;M Engineering.</p></div>
				
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game Hacking – Valve Anti-Cheat (VAC) (109 pts)]]></title>
            <link>https://codeneverdies.github.io/posts/gh-2/</link>
            <guid>44311682</guid>
            <pubDate>Wed, 18 Jun 2025 17:19:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeneverdies.github.io/posts/gh-2/">https://codeneverdies.github.io/posts/gh-2/</a>, See on <a href="https://news.ycombinator.com/item?id=44311682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><hr><blockquote><h2 id="intro">Intro</h2></blockquote><p>In 2002 Valve created an Anti-Cheat solution called “Valve Anti-Cheat” aka <strong>VAC</strong>.
The first game they implemented VAC into was Counter-Strike. When <strong>VAC</strong> was introduced it only operated in
User Mode (Still does) meaning it runs entirely in user space <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> and has no kernel component.</p><p>Below is a list of games that use <strong>VAC</strong>.. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><pre><code>Call of Duty: Modern Warfare 2
Call of Duty: Modern Warfare 3
Counter-Strike (video game)
Counter-Strike: Condition Zero
Counter-Strike: Source
Counter-Strike 2
Day of Defeat
Day of Defeat: Source
Deathmatch Classic
Half-Life 2: Deathmatch
Half-Life Deathmatch: Source
Ricochet
Team Fortress
Team Fortress Classic
</code></pre><p>A longer list can be found here <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p><hr><blockquote><h2 id="vac-cident">VAC-cident?</h2></blockquote><p>So.. if you don’t know <strong>VAC</strong> has been around for quite a while, at the time of writing it’ll be 23 years.
Over the time they’ve made some mistakes but who doesn’t? (Taken from wikipedia) <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p><pre><code>- In July 2010, [snip] Approximately 12,000 owners of Call of Duty: Modern Warfare 2 were 
banned when Steam updated a DLL file on disk after it had been loaded into memory by the 
game, causing a false positive detection. These bans were revoked and those affected received 
a free copy of Left 4 Dead 2 or an extra copy to send as a gift. 

- In October 2023, certain users of AMD graphics cards were banned from Counter-Strike 2 
after AMD added support for their "Anti-Lag+" feature via a driver update, which the game 
flagged as a cheat due to it detouring certain DLL functions. AMD subsequently withdrew the 
driver update and Valve pledged to unban any affected users.
</code></pre><p>This post isn’t created to bash Valve they clean up after their mistakes and listen to their community,
gotta love devs when they do that. I also commend them because getting <strong>VAC</strong> banned isn’t such a slap
on the wrist. Getting <strong>VAC</strong> banned has some stipulations such as:</p><ul><li>Having the <strong>VAC</strong> ban show on your Steam profile</li><li>Being banned from all <strong>“GoldSrc”</strong> games</li><li>Being banned from all <strong>“Source engine”</strong> games (The Counter-Strike serise)</li><li>Not being able to <strong>refund</strong> the game you’re <strong>VAC</strong> banned on</li></ul><p>Knowing what’ll happen if you get <strong>VAC</strong> banned is important to know because regardless if
you’re cheating or not false bans are no good. People in the community took it upon themselves
to reverse engineer the Anti-Cheat and understand what it does (some did it just to cheat).</p><hr><blockquote><h2 id="vac-what">VAC what?</h2></blockquote><p>The previous section brings us to a term you may have heard before, the infamous <strong>“VAC Bypass”</strong>.
Searching online for information about bypassing <strong>VAC</strong> brings many blogs and repos that all seem to do/talk about something
similar and that’s “Dumping the <strong>VAC</strong> modules”. Let me explain, <strong>VAC</strong> is <strong>NOT</strong> just one executable on the system it streams it’s
Anti-Cheat modules (DLLs) from a server, once a module is recieved by some routine in <strong>steamservice.dll</strong> inside <strong>steamservice.exe</strong>
(or <strong>steam.exe</strong> if <strong>ran as admin</strong>) it will be loaded using one of the two methods below. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> <sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> <sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> <sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> <sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> <sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup> <sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p><ul><li><strong>Reflectively load</strong> the DLL into memory</li><li>Use the WinAPI function <strong>LoadLibrary</strong></li></ul><p>By default the Anti-Cheat modules are reflectively loaded into memory. The goal is to force <strong>LoadLibrary</strong> into being used,
then someone can hook that function and dump the modules (DLLs) to disk, allowing someone to analyse the dumped DLLs and understand
what they’re doing to detect cheating.</p><hr><blockquote><h2 id="dumping-vac-modules-in-the-big-25">Dumping VAC Modules in the big ‘25</h2></blockquote><p>To kick start the journey on dumping the <strong>VAC</strong> modules load <strong>steamservice.dll</strong> into <strong>Binary Ninja</strong>. Once the binary is fully analyzed
go to “Triage Summary”</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-1.png" data-src="/VAC-1.png" data-srcset="/VAC-1.png, /VAC-1.png 1.5x, /VAC-1.png 2x" data-sizes="auto" alt="/VAC-1.png" title="VAC-1" srcset="https://codeneverdies.github.io/VAC-1.png, https://codeneverdies.github.io/VAC-1.png 1.5x, https://codeneverdies.github.io/VAC-1.png 2x"></p></blockquote><p>It is <strong>VERY</strong> important to take note that this is a 32-bit process, so all pointers will be 32-bits in size you’ll see why this is important later.</p><p>Next we’ll search for calls to <strong>LoadLibrary*</strong> I’ll save the reader some time and tell you that we should be looking
for calls to <strong>LoadLibraryW</strong> it will be called in a very important function that we can use to back track.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-2.png" data-src="/VAC-2.png" data-srcset="/VAC-2.png, /VAC-2.png 1.5x, /VAC-2.png 2x" data-sizes="auto" alt="/VAC-2.png" title="VAC-2" srcset="https://codeneverdies.github.io/VAC-2.png, https://codeneverdies.github.io/VAC-2.png 1.5x, https://codeneverdies.github.io/VAC-2.png 2x"></p></blockquote><p>Following the reference takes us to an interesting function <strong>sub_10086f80</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-3.png" data-src="/VAC-3.png" data-srcset="/VAC-3.png, /VAC-3.png 1.5x, /VAC-3.png 2x" data-sizes="auto" alt="/VAC-3.png" title="VAC-3" srcset="https://codeneverdies.github.io/VAC-3.png, https://codeneverdies.github.io/VAC-3.png 1.5x, https://codeneverdies.github.io/VAC-3.png 2x"></p></blockquote><p>Judging by the return value <strong>HMODULE</strong> and the calls to LoadLibrary* it’s safe to say this function’s job is to load
some kind of module and return a handle to it. Following references to where this function is called leads us to another interesting
function <strong>sub_10086c40</strong>.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-4.png" data-src="/VAC-4.png" data-srcset="/VAC-4.png, /VAC-4.png 1.5x, /VAC-4.png 2x" data-sizes="auto" alt="/VAC-4.png" title="VAC-4" srcset="https://codeneverdies.github.io/VAC-4.png, https://codeneverdies.github.io/VAC-4.png 1.5x, https://codeneverdies.github.io/VAC-4.png 2x"></p></blockquote><p>The beginning of the <strong>sub_10086c40</strong> function didn’t look too important (at the time) but we should remember that this function also returns a handle to a module.
I looked at the references and it shows that this function is called once in the function <strong>sub_10059040</strong>.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-17.png" data-src="/VAC-17.png" data-srcset="/VAC-17.png, /VAC-17.png 1.5x, /VAC-17.png 2x" data-sizes="auto" alt="/VAC-17.png" title="VAC-17" srcset="https://codeneverdies.github.io/VAC-17.png, https://codeneverdies.github.io/VAC-17.png 1.5x, https://codeneverdies.github.io/VAC-17.png 2x"></p></blockquote><p>We can see <strong>sub_10086c40</strong> being called if we trace back the <strong>first</strong> argument passed to that function,
we’ll see that it was used by another function <strong>sub_100859d0</strong>. If that function call is successful
execution carries on, so it’s safe to say it’s important. Let’s take a look at this function.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-18.png" data-src="/VAC-18.png" data-srcset="/VAC-18.png, /VAC-18.png 1.5x, /VAC-18.png 2x" data-sizes="auto" alt="/VAC-18.png" title="VAC-5" srcset="https://codeneverdies.github.io/VAC-18.png, https://codeneverdies.github.io/VAC-18.png 1.5x, https://codeneverdies.github.io/VAC-18.png 2x"></p></blockquote><p>This function makes two WinAPI calls</p><ul><li><p><strong>GetTempPathW</strong>
: Retrieves the path of the directory designated for temporary files.</p></li><li><p><strong>GetTempFileNameW</strong>
: Creates a name for a temporary file. If a unique file name is generated, an empty file is created and the handle to it is released; otherwise, only a file name is generated.</p></li></ul><p>The combination of these calls tells us that we need to be looking for any <code>.TMP</code> files being accessed, the names are usually in this format <code>&lt;uuuu&gt;.TMP</code>.</p><p>Now there’s a path to a DLL floating in memory how does it get used? Look no more.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-5.png" data-src="/VAC-5.png" data-srcset="/VAC-5.png, /VAC-5.png 1.5x, /VAC-5.png 2x" data-sizes="auto" alt="/VAC-5.png" title="VAC-5" srcset="https://codeneverdies.github.io/VAC-5.png, https://codeneverdies.github.io/VAC-5.png 1.5x, https://codeneverdies.github.io/VAC-5.png 2x"></p></blockquote><pre tabindex="0"><code>100591f7   HMODULE eax_13 = sub_10086c40(edi_1, 0)
100591ff   *(esi + 4) = eax_13
100591ff   
10059204   if (eax_13 != 0)
10059215       int32_t eax_14 = sub_10086c20(eax_13, "_runfunc@20")
1005921d       *(esi + 0xc) = eax_14
</code></pre><p>The path <code>edi_1</code> is used by <strong>sub_10086c40</strong>, this function is used to get a handle to a module <code>eax_13</code> then it’s passing that handle to <strong>sub_10086c20</strong>.
<strong>sub_10086c20</strong> takes two arguments we know the first is a handle to a module the second is from what we can see here a string <code>_runfunc@20</code>, the return value
<code>int32_t</code> looks a little weird but this is a 32-bit process remember ;) so this could be a pointer to something dont ya think? Here’s the function prototype</p><pre tabindex="0"><code>int32_t sub_10086c20(HMODULE arg1, PSTR arg2)
</code></pre><p>Place your bets on it being a GetProcAddress wrapper.. Drum roll please… It is…</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-6.png" data-src="/VAC-6.png" data-srcset="/VAC-6.png, /VAC-6.png 1.5x, /VAC-6.png 2x" data-sizes="auto" alt="/VAC-6.png" title="VAC-6" srcset="https://codeneverdies.github.io/VAC-6.png, https://codeneverdies.github.io/VAC-6.png 1.5x, https://codeneverdies.github.io/VAC-6.png 2x"></p></blockquote><p>So with this bit of information we know <strong>steamservice.dll</strong> recieves the <strong>VAC</strong> modules, it’s using a function <strong>sub_10086c40</strong> which calls
<strong>sub_10086f80</strong> to load the Anti-Cheat module and return a handle, then that handle is passed to <strong>sub_10086c20</strong> to get the address
of a function named <code>_runfunc@20</code>. By default as said earlier the modules are reflectively loaded so this isn’t the regular control flow of
<strong>steamservice.dll</strong>, this can be confirmed if you scroll up a bit in <strong>sub_10059040</strong> you’ll see a flag being checked.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-7.png" data-src="/VAC-7.png" data-srcset="/VAC-7.png, /VAC-7.png 1.5x, /VAC-7.png 2x" data-sizes="auto" alt="/VAC-7.png" title="VAC-7" srcset="https://codeneverdies.github.io/VAC-7.png, https://codeneverdies.github.io/VAC-7.png 1.5x, https://codeneverdies.github.io/VAC-7.png 2x"></p></blockquote><p><strong>steamservice.dll</strong> will most likely take this path unless we can do something about it</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-8.png" data-src="/VAC-8.png" data-srcset="/VAC-8.png, /VAC-8.png 1.5x, /VAC-8.png 2x" data-sizes="auto" alt="/VAC-8.png" title="VAC-8" srcset="https://codeneverdies.github.io/VAC-8.png, https://codeneverdies.github.io/VAC-8.png 1.5x, https://codeneverdies.github.io/VAC-8.png 2x"></p></blockquote><p>Let’s look at it in assembly</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-9.png" data-src="/VAC-9.png" data-srcset="/VAC-9.png, /VAC-9.png 1.5x, /VAC-9.png 2x" data-sizes="auto" alt="/VAC-9.png" title="VAC-9" srcset="https://codeneverdies.github.io/VAC-9.png, https://codeneverdies.github.io/VAC-9.png 1.5x, https://codeneverdies.github.io/VAC-9.png 2x"></p></blockquote><p>Take a look at <code>je 0x10059127</code> ( <code>0x74 0x47</code> )</p><p><code>0x74</code> is the jump if equal instruction and <code>0x47</code> is how many bytes forward to jump (71) in hex</p><p>What we wan’t to do is change the first instruction at <strong>steamservice.dll</strong> + 0x590DE (0x100590de)</p><ul><li>to <code>jne 0x10059127</code> ( <code>0x75 0x47</code> )</li></ul><p>we’re changing the first byte of this instruction to be <code>0x75</code> which is jump if <strong>NOT</strong>
zero/equal. (Inverting)</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-10.png" data-src="/VAC-10.png" data-srcset="/VAC-10.png, /VAC-10.png 1.5x, /VAC-10.png 2x" data-sizes="auto" alt="/VAC-10.png" title="VAC-10" srcset="https://codeneverdies.github.io/VAC-10.png, https://codeneverdies.github.io/VAC-10.png 1.5x, https://codeneverdies.github.io/VAC-10.png 2x"></p></blockquote><p>Now that we have a potential way of dumping the <strong>VAC</strong> modules let’s test it! first we start steam and launch <strong>x32dbg</strong> as
admin we should remember the offset to our instructions <strong>steamservice.dll</strong> + 0x590DE.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-11.png" data-src="/VAC-11.png" data-srcset="/VAC-11.png, /VAC-11.png 1.5x, /VAC-11.png 2x" data-sizes="auto" alt="/VAC-11.png" title="VAC-11" srcset="https://codeneverdies.github.io/VAC-11.png, https://codeneverdies.github.io/VAC-11.png 1.5x, https://codeneverdies.github.io/VAC-11.png 2x"></p></blockquote><p>Once <strong>x32dbg</strong> is loaded attach to <strong>steamservice.dll</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-12.png" data-src="/VAC-12.png" data-srcset="/VAC-12.png, /VAC-12.png 1.5x, /VAC-12.png 2x" data-sizes="auto" alt="/VAC-12.png" title="VAC-12" srcset="https://codeneverdies.github.io/VAC-12.png, https://codeneverdies.github.io/VAC-12.png 1.5x, https://codeneverdies.github.io/VAC-12.png 2x"></p></blockquote><p>Press <code>CTRL+G</code> and enter <code>steamservice.dll + 0x590DE</code></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-13.png" data-src="/VAC-13.png" data-srcset="/VAC-13.png, /VAC-13.png 1.5x, /VAC-13.png 2x" data-sizes="auto" alt="/VAC-13.png" title="VAC-13" srcset="https://codeneverdies.github.io/VAC-13.png, https://codeneverdies.github.io/VAC-13.png 1.5x, https://codeneverdies.github.io/VAC-13.png 2x"></p></blockquote><p>Now we’re where we need to patch</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-14.png" data-src="/VAC-14.png" data-srcset="/VAC-14.png, /VAC-14.png 1.5x, /VAC-14.png 2x" data-sizes="auto" alt="/VAC-14.png" title="VAC-14" srcset="https://codeneverdies.github.io/VAC-14.png, https://codeneverdies.github.io/VAC-14.png 1.5x, https://codeneverdies.github.io/VAC-14.png 2x"></p></blockquote><p>Right click on that instruction and click “Assemble” then enter <code>jnz 0x10059127</code> and hit ok</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-15.png" data-src="/VAC-15.png" data-srcset="/VAC-15.png, /VAC-15.png 1.5x, /VAC-15.png 2x" data-sizes="auto" alt="/VAC-15.png" title="VAC-15" srcset="https://codeneverdies.github.io/VAC-15.png, https://codeneverdies.github.io/VAC-15.png 1.5x, https://codeneverdies.github.io/VAC-15.png 2x"></p></blockquote><p>It should be changed</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-16.png" data-src="/VAC-16.png" data-srcset="/VAC-16.png, /VAC-16.png 1.5x, /VAC-16.png 2x" data-sizes="auto" alt="/VAC-16.png" title="VAC-16" srcset="https://codeneverdies.github.io/VAC-16.png, https://codeneverdies.github.io/VAC-16.png 1.5x, https://codeneverdies.github.io/VAC-16.png 2x"></p></blockquote><p>The next step is to open <strong>Procmon</strong> play a game that uses <strong>VAC</strong> (I chose CSGO) and wait for
<strong>steamservice.exe</strong> to access some <code>.TMP</code> files.</p><p>Here are the <strong>Procmon</strong> filters</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-19.png" data-src="/VAC-19.png" data-srcset="/VAC-19.png, /VAC-19.png 1.5x, /VAC-19.png 2x" data-sizes="auto" alt="/VAC-19.png" title="VAC-19" srcset="https://codeneverdies.github.io/VAC-19.png, https://codeneverdies.github.io/VAC-19.png 1.5x, https://codeneverdies.github.io/VAC-19.png 2x"></p></blockquote><p>While loading the game we see our first TMP file <code>C:\Windows\Temp\D54A.tmp</code></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-20.png" data-src="/VAC-20.png" data-srcset="/VAC-20.png, /VAC-20.png 1.5x, /VAC-20.png 2x" data-sizes="auto" alt="/VAC-20.png" title="VAC-20" srcset="https://codeneverdies.github.io/VAC-20.png, https://codeneverdies.github.io/VAC-20.png 1.5x, https://codeneverdies.github.io/VAC-20.png 2x"></p></blockquote><p>Let’s join a public match and see if there are others</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-21.png" data-src="/VAC-21.png" data-srcset="/VAC-21.png, /VAC-21.png 1.5x, /VAC-21.png 2x" data-sizes="auto" alt="/VAC-21.png" title="VAC-21" srcset="https://codeneverdies.github.io/VAC-21.png, https://codeneverdies.github.io/VAC-21.png 1.5x, https://codeneverdies.github.io/VAC-21.png 2x"></p></blockquote><p>And some more..</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-22.png" data-src="/VAC-22.png" data-srcset="/VAC-22.png, /VAC-22.png 1.5x, /VAC-22.png 2x" data-sizes="auto" alt="/VAC-22.png" title="VAC-22" srcset="https://codeneverdies.github.io/VAC-22.png, https://codeneverdies.github.io/VAC-22.png 1.5x, https://codeneverdies.github.io/VAC-22.png 2x"></p></blockquote><p>We can also look at these files in the temp directory..</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-23.png" data-src="/VAC-23.png" data-srcset="/VAC-23.png, /VAC-23.png 1.5x, /VAC-23.png 2x" data-sizes="auto" alt="/VAC-23.png" title="VAC-23" srcset="https://codeneverdies.github.io/VAC-23.png, https://codeneverdies.github.io/VAC-23.png 1.5x, https://codeneverdies.github.io/VAC-23.png 2x"></p></blockquote><p>I copied all of these files to a new directory and loaded <code>D54A.tmp</code> into <strong>PE-bear</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-24.png" data-src="/VAC-24.png" data-srcset="/VAC-24.png, /VAC-24.png 1.5x, /VAC-24.png 2x" data-sizes="auto" alt="/VAC-24.png" title="VAC-24" srcset="https://codeneverdies.github.io/VAC-24.png, https://codeneverdies.github.io/VAC-24.png 1.5x, https://codeneverdies.github.io/VAC-24.png 2x"></p></blockquote><p>We see something familiar <code>_runfunc@20</code> this is the function that was found using <strong>sub_10086c20</strong>.</p><hr><blockquote><h2 id="to-be-continued">To be continued</h2></blockquote><p>In the next post we will be doing analysis on these Anti-Cheat Modules to get a better understanding of what’s going on.
I hope you enjoyed this post and most importantly learned a thing or two. Stay tuned!</p><hr><blockquote><h2 id="references">References</h2></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Andrej Karpathy's YC AI SUS talk on the future of the industry (226 pts)]]></title>
            <link>https://www.donnamagi.com/articles/karpathy-yc-talk</link>
            <guid>44311509</guid>
            <pubDate>Wed, 18 Jun 2025 16:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.donnamagi.com/articles/karpathy-yc-talk">https://www.donnamagi.com/articles/karpathy-yc-talk</a>, See on <a href="https://news.ycombinator.com/item?id=44311509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://www.donnamagi.com/">back</a></p><p>This is a transcript of Andrej Karpathy's talk on Software 3.0 on June 17th. I was in the audience, and it was recorded in a noisy environment - set your expectations accordingly.</p><p>YC has said the official video will take a few weeks to release, by which Karpathy himself agrees the talk will be deprecated.<!-- --> <a href="https://x.com/karpathy/status/1935077692258558443" target="_blank" referrerpolicy="origin">https://x.com/karpathy/status/1935077692258558443</a></p><p>and ok wow, this is going viral. let's<!-- --> <a href="https://x.com/DonnaMagi" target="_blank" referrerpolicy="origin">keep in touch</a>?</p><p>I think it's actually an extremely unique and very interesting time to enter the industry right now. And I think fundamentally the reason for that is that software is changing. Again. And I say again because I actually gave this talk already. But the problem is that software keeps changing, so I actually have a lot of material to create new talks. And I think it's changing quite fundamentally. I think broadly speaking, software has not changed much at such a fundamental level for 70 years. And then it's changed, I think, about twice quite rapidly in the last few years. And so there's just a huge amount of work to do, a huge amount of software to write and rewrite.</p><p>So let's take a look at maybe the realm of software. So if we're going to think of this as like a map of software, this is a really cool tool called Map. This is kind of like all the software that's written. These aren't instructions. Computer for clearing out tasks in digital space. So if you zoom in here, these are all different kinds of repositories. And this is all the code that has been written. And a few years ago, I kind of observed that software was kind of changing and there was kind of like a new type of software around. And I called this Software 2.0 at the time. And the idea here was that Software 1.0 is the code you write on the computer. Software 2.0 are basically neural networks. In particular, the weights of the neural network. And you're not writing this code correctly. You're more like tuning the data sets and then you're running an optimizer to create the parameters. And I think at the time, neural networks were kind of seen as just a different kind of classifier, like a decision tree or something like that. And so I think this training was a lot more appropriate.</p><p>And now actually what we have is kind of like an equivalent of GitHub in the realm of Software 2.0. And I think the hugging face is basically an equivalent of GitHub in Software 2.0. And there's also a model atlas and you can visualize all the code written there. In case you're curious, by the way, the giant circle, the point in the middle, these are the parameters of Flux, the image generator. And so any time someone tunes a lower on top of a Flux model, you basically create a GitHub event in this space and create a different kind of image generator. So basically what we have is Software 1.0 is the computer code that programs the computer. Software 2.0 are the weights which program neural networks. And here's an example of AlexNet image recognizer neural network. Now so far, all of the neural networks that we've been familiar with until recently were kind of like fixed-functional computers. Image categories or something like that. And I think what's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0. And basically your products are now programs that program people all over. And remarkably, these products are written in English. So it's kind of a very interesting programming language.</p><p>So maybe to summarize the business, if you're doing central classification, for example, you can imagine writing some Python to basically do central classification. Or you can train neural networks. Or you can drop a large amount of code. So here I'm just using SoftPrompt, and you can imagine changing it and programming the computer's life. So basically we have Software 1.0, Software 2.0. And I think we're seeing, I mean, you've seen a lot of GitHub code is not just like code anymore. There's a bunch of English interspersed with code. And so I think there's a growing category of that kind of code. So not only is it a new programming paradigm, it's also remarkable to me that it's in our native language of English. And so this blew my mind a few years ago now. I tweeted this, and I think it captured the attention of a lot of people. And one of the things that I currently pinpoint to them is that arguably we're not programming computers in English.</p><p>Now when I was at Tesla, we were working on the Autopilot, and we were trying to get the car to drive. And I sort of showed this slide at the time where you can imagine that the inputs for the car are on the bottom, and they're going through the software stack to produce the steering and acceleration. And I made the observation at the time that there was a ton of C++ code around in the Autopilot, which was the software model development. And that there were some neural nets in there doing the interactions. And I kind of observed that over time as we made the Autopilot better, basically the neural network grew in capability and size. And in addition to that, all this C++ code was being completed. And a lot of the capabilities and functionality that was originally in 1.0 was migrated to 2.0. So as an example, a lot of the stitching up of information across images from the different cameras across time was done by neural network, and we were able to delete a lot of code. And so the software development stack was quite literally made through the software stack of the Autopilot. So I thought this was a brilliant model at the time. And I think we're seeing the same thing again, where basically we have a new kind of software, and it's being through the stack, made through a completely different programming paradigm.</p><p>And I think if you're entering the industry, it's a very good idea to be fluent in all of them. Because they all have slight pros and cons, and you may want to program some functionality in 1.0 or 2.0, or 3.0, or you're going to train in LLM, or you're going to just run from LLM, that shouldn't be any software that's explicit, etc. So we don't have to make these decisions to actually potentially fluidly transition to LLMs.</p><p>So what I want to get into now is, first I want to, in the first part, talk about LLMs, and what I think of this new paradigm in the ecosystem, and what that looks like. What is this new computer? What does it look like? And what does the ecosystem look like? I was struck by this quote from Andrew, actually many years ago now, I think. And I think Andrew is going to speak right after me. But he said that the term AI is probably not his thing. And I do think that it captures something very interesting, in that LLMs certainly feel like they have properties of utilities, right? So, LLM labs, like OpenAI, Gemini, Fungi, etc, they spend time to train the LLMs, and this is kind of equivalent to a built-in AI algorithm, and then there's op-ecs to serve them intelligence over APIs to all of us. And this is done through internet access, where we pay per million tokens or something like that, and we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, etc.</p><p>In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid, solar, or battery, or generator. In LLMs, we have maybe open router, and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't need more physical space, so it's okay to have, basically, like six electricity providers that you can switch between, right? Because they don't need this directly. And I think what's also really fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of stuck and unable to work.</p><p>And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable on the grid, and the planet just gets smaller. The more reliance we have on these models, which already is, like, really dramatic, and I think will continue to grow. But LLMs don't only have case of utilities. I think it's also fair to say that they have some power tools called CAPs. And the reason for this is that the CAPs required for building LLMs is actually quite large. It's not just like building some power station or something like that, right? You're investing a huge amount of money, and I think the technology is growing quite rapidly. So we're in a world where we have, sort of, deep tech trees, research and development, secrets, that are centralizing our shiny LLM labs. But I think the analogy varies a little bit also, because, as I mentioned, this is software. And software is a bit less expensive, but because it is so valuable. And so I think that's just an interesting kind of thing to think about.</p><p>There's many analogies you can make, like a formatted data process, and maybe, for instance, something like a cluster with certain max-ops. And you can think about, when you're using it, you actually use it only through the software, not through the hardware. That's kind of what the CAPs are. But if you're actually also building it on hardware, and you're sharing it using Google, that's kind of like an integral on your platform. So I think there's analogies here that make sense. But actually, I think the analogy that makes the most sense, perhaps, is that, in my mind, LLM's have very strong analogies to operating systems, in that this is not just electricity or power. It's not something that comes automatically to happen. It's a commodity. These are now increasingly complex software ecosystems. So they're not just simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way, where you have a few closed-source providers, like Windows and macOS, and then you have an open-source alternative, like Linux. And I think for LLM's as well, we have a few competing closed-source providers. And then maybe the LLM ecosystem is currently maybe a close approximation to something that may grow into something like Linux. Again, I think it's still very early, because these are just simple LLMs. And I'm starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, but it's about the tool use, the full-time analogies, how all that works. And so when I sort of have this realization about that, I try to sketch it out. And it kind of seems to me like LLMs are kind of like a new operating system, right? So the LLM is a new kind of computer. It's kind of like a CPU equivalent. The context windows are kind of like the memory. And then the LLM is orchestrating memory and compute for problem solving using all these capabilities. And so definitely, if you look at it, it looks very much like software. from that perspective.</p><p>A few more analogies, for example, if you want to download an app, say I go to VS Code and I go to Download, you can download VS Code and you can run it on Windows, Linux, or Mac, in the same way as you can take an LLM app, like a cursor, and you can run it on GBT, or Blob, or JPEG streams, right? It's just a drop-in. So it's kind of like similar in that way as well. The more analogies that I can describe to you is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer, and that forces the LLMs to be centralized in the cloud, and we're all just sort of fake clients that interact with it over the network, and none of us have full utilization of these computers. And therefore, it makes sense to use timesharing, where we're all just, you know, at the mission of the batch, when they're running the computer in the cloud. And this is very much what the computers used to look like. During this time, the operating systems were in the cloud, everything was streamed around them, and they were batching. And so the personal computing revolution hasn't happened yet, because it's just not that common, and it doesn't make sense, but I think some people are trying. And it turns out that Mac minis, for example, are a very good fit for some of the LLMs, because it's all purely batch-run inference, this is all super-memory-bound, so this actually works. And I think these are some early indications that you have personal computing, but this hasn't really happened yet, and it's not clear what this looks like. Maybe some of you guys are interested in talking about what this is, or how it works, or what it should be.</p><p>Maybe one more analogy that I'll mention is, whenever I talk to ChartsQt or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like, it's text, it's direct access to the operating system, and I think a GUI hasn't yet really been invented in a general way, like ChartsQt had a GUI, but different than just a text box. Certainly some of the apps that we're going to go into in a bit have GUI, but there's no GUI across all the tasks and devices. There are some ways in which LLMs are different from operating systems in some kind of unique way, and from early computing. And I wrote about this one particular property that strikes me as very different. It's that LLMs, they flip the direction of technology that is usually present in technology. So for example, with electricity, the economy is getting quite expensive, and lots of new transformative technologies have not been around, particularly in government-owned corporations that are the first officers, because it's new and expensive, etc. And only later did they fix it to consumers. But I feel like LLMs are kind of what flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about up the oil bank or something like that. This is certainly like a lot of minds.</p><p>And so it's really fascinating to me that we have a new magical computer, and it's like helping the oil bank. It's not helping the government to do something really crazy like some military ballistics or some special technology. Indeed, corporations are now getting the lagging, apparently not, of all of us, of all these technologies. So it's just backwards. And I think it informs me in some of the uses of how we want to use this technology, or like where I saw the first apps in the store.</p><p>So in summary so far, LLMs, app LLMs, I think it's accurate language to use. LLMs are complicated operating systems. They're circa 1960s computing, or we do computing already. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software. And Chachapitibos leans down to our computers that collect billions of people like this and play it overnight. And this is insane. And it's kind of insane to me that this is the case. And now it is our time to enter the industry and program these computers. It's crazy.</p><p>So I think this is a better part. Before we program LLMs, we have to kind of like spend some time to think about what these things are. And I especially like to talk about their psychology. So the way I like to think about LLMs is that they're kind of like little spirits. They are stochastic simulations of, and the simulator in this case happens to be an autoregressive transformer. So a transformer is a neural network. And it just kind of like goes from level of focus, it goes chunk, chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. And this simulator, of course, is just, it's basically there's some weights involved and we fit it to all the text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained in humans, it's got this emergent psychology that is human-like.</p><p>So the first thing you'll notice is, of course, LLMs have this type of deep knowledge and memory. And they can remember lots of things, a lot more than any single individual can because they've read so many things. It actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie. I love this movie. Dustin Hoffman here is an autistic sponge who has almost perfect memory. So he can read like a notebook and remember all of the names and phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember shock hashes and lots of different kinds of things, very, very easily. So they certainly have superpowers and stuff in some respect, but they also have a bunch of, I would say, cognitive deficits. So they hallucinate quite a bit and they kind of make up stuff and don't have a very good sort of internal model of self-knowledge, but not sufficient at least. And this has gotten better, but not perfect. They display jagged intelligence. So they're going to be superhuman in some problem-solving ways, and then they're going to make mistakes that basically no human will make, like, you know, it will insist that 9.11 is greater than 9.9, or that there are two bars of strawberry. These are some famous examples. But basically there are rough edges that you can trip on. So that's kind of, I think, also kind of cool. They also kind of suffer from internal brain amnesia. So, and I think alluding to the fact that if you have a coworker, which is your organization, this coworker will, over time, learn your organization and they will understand and gain, like, a huge amount of context on the organization and they go home and they sleep and they consolidate knowledge and they build expertise over time. LLMs don't natively do this. This is not something that has really been solved in R&amp;D with LLMs by them. And so context windows are really kind of like a working memory that you have to sort of program the working memory quite directly because they don't just kind of, like, get borrowed by people. And I think a lot of people get tripped up by analogies in this way. In popular culture, I recommend people watch these two movies, Memento and First Dates. In both of these movies, the protagonists, their ways are mixed, and their context windows get wiped every single morning. And it's really problematic to go to work or have relationships when this happens. And this happens to a lot of us all the time.</p><p>I guess one more thing I would point to is security-related limitations on the use of LLMs. So, for example, LLMs are quite vulnerable. They are susceptible to conflict-injection risks. They might leak your data, etc. And there's many other considerations security-related. So, basically, long story short, you have to simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we enhance them? They are extremely likely usable. And so how do we program them? And how do we work around their deficits and toy with their superhuman powers?</p><p>So what I'm going to switch to now is talk about the operators. How do we use these models? And what are some of the biggest operators? This is not a comprehensive list of some of the things that I thought were interesting in this stuff. The first thing I'm kind of excited about is what I would call partial autonomy apps. So, for example, let's work with the example of coding. You can certainly go to chat.gt directly, and you can start copy-pasting code around, and copy-pasting awkward words and stuff around, and then code, and copy-pasting everything around. Why would you do that? Why would you go directly to the operators? It makes a lot more sense to have an app dedicated for this. And so I think many of you use Cursor. I do as well. And Cursor is kind of like the thing you want instead. You don't want to just directly go to the chat.gt. And I think Cursor is a very good example of an early LLM app that has a bunch of properties that I think are useful across all the LLMs. So in particular, you will notice that we have a traditional interface that allows a human to go in and do all the work manually, just as before. But in addition to that, we now have this LLM integration that allows us to go in bigger chunks. And so some of the properties of LLM apps that I think are shared are useful. Number one, the LLMs basically do a ton of good context management. Number two, they orchestrate multiple calls to LLMs. So in the case of Cursor, there's under-the-hood eventing models for all your files, the actual chat models, models that apply ifs to the code, and this whole orchestra is for you. A really big one that I think also may be not fully appreciated in all this is application-specific and the importance of it. Because you don't just want to talk to the operating system directly in text. Text is very hard to read, interpret, understand, and also you don't want to take some of these actions natively in text. So it's much better to just see the bit as like red and green change, and you can see what's the matter of subtracting. It's much easier to just do command Y to accept or command N to reject. You don't have to type it in text. So it really allows the human to audit the work of these fabulous systems and to grow faster. We're going to come back to this in a little bit later as well. And the last kind of feature I want to point out is that there's what I call the autonomous slider. So for example, in Cursor, you can just do calculation. You're mostly in charge. You can select a chunk of code and command K to change a static chunk of code. You can do Command L to change this entire file, or you can do Command I, which just, you know, for better or worse, it packages up a lot of things, makes sure that it orchestrates people all at once. It's got a GUI that allows you to audit some of its work. So, for example, it will cite sources that you can imagine inspecting them, and it's got an autonomy slider. You can either just do a quick search, or you can do research, or you can do deep research and come back to all this later. So this is all just very well-structured, automated, and optimized.</p><p>So I guess my question is, I feel like a lot of software will become partially autonomous. And I'm trying to think through, like, what does that look like? In fact, many of you maintain products and services. How are you going to make your products and services partially autonomous? Can an LLM see everything that a human can see? Can an LLM act in all the ways that a human can act? And can humans supervise and stay in the loop of this activity? Because, again, these are allowable systems that aren't yet perfect. And what does the diff look like on Photoshop? There's a lot of things we don't know. And also, a lot of the traditional software right now has all these switches and all this kind of stuff. It's all designed for people. All this has to change and become accessible to LLMs.</p><p>So one thing I wanted to stress with a lot of these LLM apps that I'm not sure gets as much attention as it should, is LLM. We're now kind of, like, cooperating with the apps. And usually, they are doing a generation, and we assume this sort of verification. It is in our interest to make this move as fast as possible, so we're getting a lot of work. There are two major ways that I think this can be done. Number one, you can speed up verification a lot. And I think GUIs, for example, are extremely important for this, because a GUI utilizes your computer vision GPU in all of our head. Reading text is effortful, and it's not looking at stuff. It's on a headset. It's just a, like, a highway to your brain. So I think GUIs are very useful for auditing systems and visual representations in general. And number two, I would say is, we have to keep the AI in a leash. I think a lot of people are getting way overexcited with AI engines, and it's not useful to me to get the diff of 1,000 lines of code into my repo. Like, I have to, I'm still the bottom line, right? Even though the 1,000 lines come out instantly, I have to make sure that this thing is not introducing bugs. It's just like, and that it's doing the correct thing, right, and that there's no security issues and so on. So I think that, yeah, basically, you have to sort of, like, it's in our interest to make the flow of these two go very, very fast, and we have to somehow keep the AI in a leash because it gets way too overactive. It's kind of like this. This is how I feel when I do AI-assisted coding. If I'm just live coding, everything is nice and great, but if I'm actually trying to get work done, it's not so great to have an overactive engine doing all this kind of stuff. So this slide is not very good, I'm sorry, but I guess I'm trying to develop, like many of you, some ways of utilizing these engines in my career to look at AI-assisted coding, and if I don't work, I'm always scared to get way too big bits. I always go with small incremental chunks. I want to make sure that everything is good. I want to spin this thing very, very fast, and I sort of work on small chunks of single-token thing, and so I think many of you probably have a little bit similar ways to work with algorithms.</p><p>I also saw a number of blog posts that try to develop these best practices for working with algorithms, and here's one that I recently came across that's quite good, and it kind of discusses some techniques, and some of them have to do with how you keep the AI on a leash. And so as an example, if you're on prompting, if your prompt is vague, then the AI might not do exactly what you want it, and in that case, the verification will fail. You're gonna ask for something else. If the verification fails, then you're gonna start spinning. So it makes a lot more sense to spend a bit more time to be more complete in your prompts, which increases the probability of a successful verification, and you can move forward. And so I think a lot of us are gonna end up finding techniques like this.</p><p>I think in my own work as well, I'm very interested in what education looks like, and together with, now that we have an AI, and a lens for what does education look like, and I think a large amount of thought for me goes into how we keep AI on a leash. I don't think it just works to go through trashy ATM, you know, like the aging business. I don't think this works, because the AI is like, it gets lost in the woods. And so for me, this is actually two separate apps, for example, there's an app for a teacher that creates courses, and then there's an app that takes courses and serves them to students. And in both cases, we now have this intermediate artifact of a course that is auditable, we can make sure it's good, we can make sure it's consistent, and the AI is kept on a leash with respect to a certain syllabus, a certain progression of projects, and so on. And so this is one way of keeping AI on a leash that I think has a much higher likelihood of working. And the AI is not getting lost in the woods.</p><p>One more kind of methodology I wanted to sort of allude to is I'm not a most major to partial autonomy, I've kind of worked on this, I think, for five years for Tesla, and this is also a partial autonomy product that shares a lot of features, like for example, right there, the instrument panel has the weight of the product, so it's showing me what the neuron sees, and so on. And then the autonomy slide, where over the course of my tenure there, we did more and more autonomous tasks for Google News. And maybe the story that I wanted to tell very briefly is, actually the first time I drove a self-driving vehicle was in 2013, and I had a friend who works at Rainbow, and he offered to give me a drive around Palo Alto. I took this picture using a moving bus at the time, and many of you are so young that you might not even know what that is, but yeah, this was like all the rage at the time. And we got into this car, and we went for about a 40-minute drive around Palo Alto, and the highways, the streets, and so on, and this drive was perfect. There was zero traffic issues. And this was in 2013, which is now 12 years ago. And it kind of struck me, because at the time when I had this perfect drive, it was a perfect gift, I felt like, wow, self-driving was imminent, because this just worked, this is incredible. But here we are 12 years later, and we are still working on the tunnel. We are still working on the driving engines. And even now, we haven't actually like fully solved the problem. Like, you may see way-bos going around, and they look driverless, but there's still a lot of tool operation, and a lot of human input of what was driving. So we still haven't even like declared success, but I think it's definitely like going to succeed at this, but it just took a long time. And so I think, to me, what I found there is that there's a very large, what I call network-to-product gap that people don't intuitively always understand very well. And look, if you imagine works as like this binary array of a different situation, of like what works and doesn't work, then that is worse than many in products, in like works at all. In the sense that, if anything works, you can make demos. But in many cases, lots of things must work, but very few people are new to the product. And this is especially the case in high-reliability areas. Not all the areas are like this, but for sure in high-reliability cases, it means. And I would say autonomy, of course, because like this thing is going to crash, which would be bad. But I would also say software is like this. If you make a single mistake in software, we know that there could be a code path that's going to break, or it's going to introduce a security issue, or a zero-day, or something like that. Like, look, software is really tricky, I think, in the same way that driving is tricky. And so when I see things like, oh, 2035 is the year of agents, I get very concerned, and I kind of feel like, you know, this is the decade of agents, and this is going to be, by some time, you do this carefully, and this is software. Let's be serious here, okay?</p><p>One more kind of analogy that I always think through is the Iron Man suit. I think this is, I always love Iron Man, I think it's like so correct in a bunch of ways, with respect to technology and how it will play out. And what I love about the Iron Man suit is that it's both an augmentation, and twin-star mechanic driver. And it's also an agent, and in some of the movies, the Iron Man suit is quite autonomous, and you can fly around, climb trees, and all this kind of stuff. And so this is the autonomy sluggish, it can be, we can build augmentations, or we can build agents, and we kind of want to do a bit of both, but at this stage, I would say, working with Palo Alto LMS itself, I would say, you know, it's less Iron Man robots, and more Iron Man suits that you want to build. It's less, like, building flashy demos of autonomous agents, and more, building partial autonomy products. And these products affect who you speak, and we're trying to, and this is done so that the generation verification of the human is very, very fast. But we are not losing the sight of the fact that it is, in principle, possible to automate this work, and there should be an autonomy slider in your product, and you should be thinking about how you can slide that autonomy slider, and make your product sort of more autonomous over time. But this is kind of how, I think there's lots of opportunities in these kinds of products.</p><p>I want to now switch gears a little bit, and talk about one other dimension that I think is very important. Not only is there a new type of programmer language that allows for autonomy in software, but also, as I mentioned, it's programmed in English, which is this natural interface. And suddenly, everyone is a programmer, because everyone speaks natural language, like English. So this is extremely bullish, and very interesting to me, and also completely unprecedented, I would say. It used to be the case that you need to spend five to 10 years studying something to be able to do something that software can do. This is not the case anymore.</p><p>So I think that, by chance, anyone has heard of IBM? Okay. This is the tweet that introduced this, but I'm told that this is now a major meme. A story about this is that, I've been on Twitter for 15 years, or something like that, at this point, and I still have no clue which tweet will become viral, and which tweet this will send over the years. And I thought that this tweet was going to be, I'm going to just have a shower of thoughts, but this became a total meme, and I really just can't tell, but I guess it's working. Or even name something that everyone is calling, but can apply it to the same words. Now they're speaking Yiddish and everything. This is life. Yeah, this is like a major contribution now or something like that, so.</p><p>So Tom Wolfe from Hugging Feet shared this beautiful video that I really love. These are kids vibe coding. And I find that this is such a wholesome video, like, I love this video. Like, how can you look at this video and feel bad about the future? The future is great. I think this will end up being like a gateway drug to software development. I'm not a doer about the future of the generation, and I think, yeah, I love this video. So, I tried vibe coding a little bit as well, because it's so fun. So, vibe coding is so great when you want to build something super duper custom that doesn't appear to exist, and you just want to wing it because it's a Saturday or something like that. So, I built this iOS app, and I built into the, I can't actually program it in Swift, but I was really shocked that I was able to build a super basic app, and I'm not going to explain it, that's really dumb. But, I kind of like, this was just like a day of work, and this was running on my phone like later that day, and I was like, wow, this is amazing. I didn't have to like leave from Swift for like five days or something like that to like get started. I also vibe coded this app with a menu gem, and this is a lot, you can try a menu gem on that. And, I basically have this problem where I show up at a restaurant, I reach for the menu, and I have no idea what any of the things are, and I need pictures. So, this doesn't exist, so I was like, hey, I'm going to vibe code it. So, this is what it looks like. You go to menu gem, that app, and take a picture of the menu, and then menu gem generates the images. And, everyone gets five dollars in credits for free when you sign up, and therefore, this is a major cost center in my life. So, this is a negative revenue app for me right now. I lost a huge amount of money on menu gem. Okay.</p><p>But, the fascinating thing about menu gem for me is that the code, well, the vibe coding part, the code was actually an easy part of vibe coding menu gem. And, most of it actually was when I tried to make it real so that you can actually have authentication, payments, domain name, and personal deployment. This was really hard, and all of this was not code. All of this DevOps stuff was me and the browser clicking stuff. And, this was an extreme spot into another room. So, it was really fascinating that I had the menu gem basically demo working on my laptop in a few hours, and then it took me a week because I was trying to make it do it. And, the reason for this is this was just really annoying. So, for example, if you try to add Google log into your webpage, I know this is very small, but just a huge amount of instructions of this important library telling you how to integrate this, and this is crazy, like it's telling me, go to this URL, click on this dropdown, choose this, go to this, and click on that, and it's like telling me what to do. Like, the computer is telling me the actions I should be taking, like, you do it. What do I do? What the hell? I had to follow all these instructions. This was crazy.</p><p>So, I think the last part of my talk, therefore, focuses on can we just build for agents? I don't want to do this work anymore. Thank you. Okay.</p><p>So, roughly speaking, I think there's a new category of consumer and manipulator of digital information. It used to be just humans through GUIs, or computers, or APIs. And now it's a completely new thing. And agents are computers, but they are human-like. Kind of, right? They're people spirits. There's people spirits on the internet, and they need to interact with our software infrastructure. Can we build for them? It's a new thing. So, as an example, you can have robots.txt in your domain, and you can instruct, or advise, I suppose, web crawlers on how to behave with your website. In the same way, you can have maybe LLMs.txt file, which is just a simple markdown that's telling LLMs what this domain is about. And this is very readable to an LLM. If it had to be said, get the HTML of your webpage and try to parse it. This is very error-prone and difficult, and it will screw it up, and it's not going to work. So, we can just directly speak to the LLM, and it's worth it. 5.1. I see some of the services now are transitioning a lot of their docs to be specifically for LLMs. So, for Cell and Stripe, as an example, are early users here. But there are a few more that I've seen before. And they offer their documentation in Markdown. Markdown is crazy for LLMs to understand. This is great. Maybe one simple example from my experience as well. Maybe someone you know from Google Brown who makes beautiful animation videos. Yeah, I love this library that he wrote, Mavic. And I wanted to make my own. And there's extensive documentation on how to use M. So, I didn't want to actually read through it. So, I copy-pasted the whole thing to an LLM. And I just grabbed what I wanted, and it just worked out of the box. But LLM just byte-coded the animation exactly what I wanted. And I was like, wow, this is amazing. So, if we can make docs legible to LLMs, it's going to unlock a huge amount of objectives. And I think this is one of the core issues that should happen.</p><p>The other thing I wanted to point out is that we do unfortunately have to. It's not just about taking your docs and making them appear in Markdown. That's the easy part. We actually have to change the docs. Because any time your docs say, like, this is bad. And LLM will not be able to agent-maintain this action right now. So, Purcell, for example, is replacing every occurrence of play with an equivalent program that your LLM agent could take on your behalf. And so, I think this is very interesting. And then, of course, there's a model context protocol from them. And this is also another way. It's a protocol that's given directly to agents. It's a new consumer and particular commercial application. So, I'm very bullish on this.</p><p>The other thing I really like is the number of little tools here and there that are helping ingest data in very LLM-friendly formats. So, for example, when I go to a GitHub repo, I can't feed this to an LLM and ask questions about it. Because this is a human interface from GitHub. So, when you just change the URL from GitHub to GitHub Ingest, then this will actually concatenate all the files into a single giant text. And it will create a directory structure. It's ready to copy-paste it into a favorite LLM. And you can use it. Maybe even more of a dramatic example of this is Eclipse, where it's not just the raw content of its files. This is from Devon. But also, they have Devon basically do analysis of the GitHub repo. Devon basically builds up a whole box of pages just for your repo. And you can imagine that this is even more helpful to copy-paste into your LLM. So, I love all the little tools that basically just change the URL and make something accessible to an LLM. So, this is all well and great. And I think there's more to come.</p><p>One little note I wanted to make is that it is absolutely possible that in the future, LLMs will be able to, it's not even future, this is today, they'll be able to go around and they'll be able to put stuff at home. But I still think it's very worth basically eating LLM's pathway and making it easier for them to access all this information. Because this is still fairly expensive, I would say, to do this. And a lot more difficult. And so, I do think that lots of software that are being long-tailed, where it won't let them gap. Because these are not live layers or repositories or traditional infrastructure. And we will need these tools. But I think for everyone else, I think it's very worth it. So, I'm bullish on both.</p><p>So, in summary, what an amazing time to give to the industry. We need to rewrite a ton of code. A ton of code will be written by professionals and by players. These LLM's are kind of like utilities, kind of like labs, but they're kind of, especially, like operating systems. But it's so early. It's like 1960s operating systems. And I think a lot of the analogies cross over. And these LLM's are kind of like these fallible people spirits that we have to learn to work with. And in order to do that properly, we need to adjust our infrastructure. So, when you're building these LLM's, it's practical ways of working effectively with these LLM's and some of the tools that make that possible. And how you can spin this very, very quickly and basically create partial time for products. And then, yeah, a lot of code has to be written for the agents. But, in any case, going back to the Iron Man suit analogy, I think what we'll see over the next decade, roughly, is we're going to take the slider from left to right. Very interesting. It's going to be very interesting to see what that looks like. So, with all of you. Thank you.</p><p>that's all. let's keep in touch?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unreasonable effectiveness of fuzzing for porting programs (190 pts)]]></title>
            <link>https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</link>
            <guid>44311241</guid>
            <pubDate>Wed, 18 Jun 2025 16:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing">https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</a>, See on <a href="https://news.ycombinator.com/item?id=44311241">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

<section>
<p><em>A simple strategy of having LLMs write fuzz tests and build up a port in topological order seems effective at automating porting from C to Rust.</em></p>
<h2>Agents are starting to produce more and more code</h2>
<p>A week or 2 back, I was reflecting on some code Claude had generated for me and
I had a sort of moment of clarity. "Clarity" might be overstating it; more
like the type of thought you have in the shower or after a few beers. Anyway.</p>
<p>The thought was: LLMs produce more and more code, and they'll eventually be
producing more code than people. We can imagine at some point we're going to
fork from mostly people writing code to mostly computers. What does this mean
for how we're going to treat our libraries and code maintenance in the future?</p>
<p>LLMs make it easier to deal with API issues or inconsistencies, they're dealing
with it, not us. Will this continue, leaving us with a <a href="https://en.wikipedia.org/wiki/The_Princess_and_the_Pea">The
Princess and the Pea</a> situation where we deal with piles of leaking
abstractions? Or will we use LLMs to radically change our APIs when needed - will
things get cleaner and better and faster as we go?</p>
<p>LLMs open up the door to performing radical updates that we'd
never really consider in the past. We can port our libraries from one language
to another. We can change our APIs to fix issues, and give downstream users an
LLM prompt to migrate over to the new version automatically, instead of
rewriting their code themselves. We can make massive internal refactorings.
These are types of tasks that in the past, <em>rightly</em>, a senior engineer would
reject in a project until its the last possibly option. Breaking customers
almost never pays off, and its hard to justify refactoring on a "maintenance
mode" project.</p>
<p>But if its more about finding the right prompt and letting an
LLM do the work, maybe that changes our decision process.</p>
<h2>Maintaining big important libraries is no fun</h2>
<p>I used to work on
<a href="https://www.tensorflow.org/">TensorFlow</a>. (TensorFlow is a system to develop
gradient based models, like PyTorch. It's not used so much anymore outside of
Google.) Now TensorFlow had some design flaws in the core language, and a
botched version 1 -&gt; version 2 migration didn't help matters too much. But as a
maintainer, the biggest issue was the enormous technical debt. It suffered from
a sort of "career advancement syndrome" <label for="sn-sad-9e298f0f"></label><span>I sadly missed this period and only was around for the aftermath where we had to deal with the cruft. </span>: TensorFlow was popular and you got credit for
contributing to it, so there was a huge incentive to add some feature as quickly
as you could and then get away.</p>
<p>As a result of a few years of this style of development, a huge surface of
Python code had been cobbled together on top of the C++ core. The complexity
only spiraled over time: engineers came into the project and needed to get
something done. Increasingly the easiest thing to do was to add some Python global or
context manager, shove something in it, then grab it later. Do this over and
over and eventually it becomes impossibly to figure out what's happening. It
also ended up being incredibly slow. It would take 10s of minutes to build some
graphs (TensorFlow's equivalent of a program).</p>
<p>As a result of this over time TensorFlow became harder and harder to maintain,
much less evolve <label for="sn-drag-2cb313ba"></label><span>It was also hard to keep good engineers working on the project. If
you're a good engineer, other teams want you and you could choose to work on
less crufty, more exciting projects like <a href="https://docs.jax.dev/">Jax</a> or
<a href="https://rjp.io/blog/openxla.org">XLA</a>, or you know, Ads. </span>. One idea we discussed internally to try to fix this was
porting the Python code, bit by bit, into a more comprehensible C++ layer, and
then re-exporting it via PyBind. At first you'd still call everything through
the Python interface, but as you ported code over, you could start to use the
internal C++ interfaces more and more, getting faster and more stable as you
went. This would incrementally improve the performance and consistency of the
system, or that was the hope.</p>
<p>But it would have been an enormous effort to do this all. Each time you tried to
port a module you'd reveal subtle errors or assumptions that you'd need to debug
and revisit, update users etc. Given our staffing, we couldn't really justify
the time versus the real work of making sure Ads didn't break.</p>
<p>Even though TensorFlow was moving to maintenance mode, if we could have made
this type of change it would have improved the maintainer and users' experience
for the years we need to keep the lights on. But because the cleanup was thorny
and slow, we just couldn't justify the cost.</p>
<p>This is by no means limited to TensorFlow. If you're working on something like
<a href="https://github.com/libexpat/libexpat">libexpat</a> its hard to justify
refactoring your code or rewriting it in Rust, no matter how potentially useful
it would be, when you've got this at the top of your page:</p>
<p><img alt="libexpat staffing warning" src="https://rjp.io/blog/libexpat-warning.png"></p>
<p>But what if we could make this type of refactoring much, much cheaper?</p>
<h2>Infinite patience versus really hard problems</h2>
<p>My experience with coding agents like <a href="https://github.com/anthropics/claude-code">Claude
Code</a> and <a href="https://aider.chat/">Aider</a>
makes me think this could be different today. I've found that agents are really
good when you set them up against a concrete obstacle: "I changed this API, now
fix everything until the tests pass". They can show real ingenuity in finding
solutions to subtle problems. I've had the LLM diagnose subtle bugs in a
program, write test cases, write minimizers, and probe for solutions, where my
contribution to this conversation largely consisted of "keep going until the
test passes". Not always, of course. Good luck getting an agent to produce
anything other than React &amp; Tailwind on your new project, for instance.</p>
<p>My hunch is that the stronger the test case, the more concrete the objective the
LLM has to fulfill, the less likely it is to try to sabotage the problem. e.g. "I made
an empty test and it passed", or to misinterpret the request "you asked for a
Ruby on Rails project, so here's NextJS, like you asked for".</p>
<p>You're putting the agent between a rock and a hard place, as it were, and this
reduces the easy solutions it could use, and thus it works harder to find a real
solution.</p>
<p>In general its really hard to define test cases, a priori, that are specific
enough to constrain the LLM to do exactly what you want. But what about when
we're changing an existing project? What if we could constrain the output before
and after the changes to be the same? This could be as simple as running all of
our tests for our project, if we're confident they're very thorough. For most
complex changes, it might be hard to keep both versions aligned.</p>
<p>What happens if we take a very specific and important type of refactoring:
moving from an unsafe language like C to a safe language like Rust? "I did something with memory I shouldn't have" are
still in the <a href="https://www.cvedetails.com/vulnerabilities-by-types.php">top 3 for CVEs</a><label for="sn-cve-f8b1dde5"></label><span>No longer #1, not because the number of memory CVEs is going down, but because XSS vulnerabilities have grown so quickly 🤦. </span>. This is exactly the
type of thing we might <em>like</em> to do, but hard to justify.  Can we make porting more of a prompt engineering problem instead of a coding
one? Let's find out...</p>
<h2>Going from C to Rust</h2>
<p>Porting C to Rust with LLMs isn't a brand new idea:
<a href="https://github.com/immunant/c2rust">c2rust</a> can produce a mechanical
translation of C code to Rust, though the result is intentionally "C in Rust
syntax", with the idea that a person uses it as a starting point to then Rustify
the project.</p>
<p>More recently researchers have started throwing LLMs at the problem.
<a href="https://arxiv.org/pdf/2412.14234">Syzygy</a> is the only paper I found which produced a complete working
implementation of a library, the other papers were more like "we threw this at
the wall and it compiled". Syzygy leverages a bunch of machinery to mechanize
parts of the porting process:</p>
<p><img alt="syzygy" src="https://rjp.io/blog/syzygy.png"></p>
<p>What's cool is their approach <em>works</em>: they get a compression program out of it, and its <a href="https://github.com/syzygy-project/Syzygy_Zopfli/blob/main/rust_code/src/main.rs">all safe Rust</a>. That's a non-trivial achievement! That said, it runs 3x slower than the C version, and its not
<em>identical</em>: how much this detail matters depends on the users of your API and how much
you care about <a href="https://en.wikipedia.org/wiki/Hyrum%27s_Law">Hyrum's Law</a>.</p>
<p>Syzygy puts a lot of work into developing test cases for symbols as they port,
but because the tests are against inferred properties of the C program, instead
of directly comparing against the C behavior, you can end up with subtle changes
in behavior. This is hard to avoid with their approach: if the interfaces
differ, it becomes hard to perform a direct comparison between your C and Rust
programs.</p>
<p>I wanted to test if we could do something radically simpler and more direct: <em>what if we just
randomly compared the C and Rust output as we ported</em>? Would that be sufficient
to port an entire library? Or would our tests be ineffectual and we'd get stuck
halfway through. In effect we'd be performing a specific type of fuzz or
<a href="https://en.wikipedia.org/wiki/Property_testing">property testing</a> to our
program as we ported it.</p>
<h2>Property testing</h2>
<p>Property testing has an interesting supposition: we can avoid the drudge work of
writing tests by having the computer build out the test cases for us, and just
check if a property holds for our system.  Trivia item: it is a fundamental law
of all property testing frameworks that they start with one of 2 examples:</p>
<ul>
<li>reverse is its own inverse, so <code>x = reverse(reverse(x))</code>.</li>
<li>the elements of a sorted list are in order</li>
</ul>
<p>Don't believe me? Check it out:</p>
<ul>
<li><a href="https://github.com/BurntSushi/quickcheck">https://github.com/BurntSushi/quickcheck</a></li>
<li><a href="https://github.com/HypothesisWorks/hypothesis">https://github.com/HypothesisWorks/hypothesis</a></li>
<li><a href="https://hackage-content.haskell.org/package/QuickCheck-2.16.0.0/docs/Test-QuickCheck.html">Haskell Quickcheck</a></li>
<li><a href="https://github.com/emil-e/rapidcheck">https://github.com/emil-e/rapidcheck</a></li>
</ul>
<p>These examples are common for a reason. They're the perfect fit for property
testing: you just feed random lists in and check your condition holds. A common
critique of property testing is that it doesn't really extend beyond these
examples. Either its too hard to generate meaningful inputs, or
properties to test, and so we're better off writing individual test
cases. My experience is that there's some narrow cases where property testing is
great, but I still end up writing plenty of individual tests.</p>
<p>Sometimes determining the property that holds is hard: I can
test individual cases and validate them, but I don't know how to generalize.  Or
generating interesting inputs might be hard. If I want to test a C parser,
sampling random bit strings isn't going to help me very much.  Entire projects
like <a href="https://github.com/csmith-project/csmith">Csmith</a> are dedicated to fuzzing C compilers.</p>
<p>But in our case we've solved at least the property test is solved for us: we
have the <em>perfect</em> output property to test: for a given input X, does our Rust
library produce the exact same output as our C library? If we can test this over
an interesting part of our input space, then we can be confident we've preserved
our behavior.</p>
<p>Let's see how this works out.</p>
<h2>Porting C to Rust, attempt 0</h2>
<p>The logs of my first attempt are fortunately lost to the howling void. I spent
an hour or 2 with Claude Code, following this policy:</p>
<ul>
<li>Port symbol X</li>
<li>Write a test for symbol X to make sure it works</li>
</ul>
<p>I wasn't comparing directly against the C version, instead relying on the unit tests
to validate the behavior was preserved. This attempt did not go well. As modules
were ported over, and we started to use symbols from previous ports, we exposed
more and more latent bugs. Eventually, you're in a state where the top-level
<code>ZopfliCompress</code> doesn't work, and you're stuck debugging the whole program to
figure out what happened.</p>
<p>I realized this wasn't going to scale. Even if I got something working, I was
looking at a lot of work and I couldn't explain how to replicate what I did with
another project.</p>
<h2>Porting C to Rust, attempt 1</h2>
<p>My <del>first</del>second attempt was <del>truly</del> slightly less crude:</p>
<ul>
<li>For each C module, ask Claude Code to write a Rust version.</li>
<li>Then write a fuzz test for that module</li>
</ul>
<p>The results of this experiment are on
<a href="https://github.com/rjpower/zopfli/tree/master/zopfli-rs/">Github</a>.</p>
<p>This time, it worked: other than writing the <a href="https://github.com/rjpower/zopfli/blob/master/port/RUST_PORTING.md">porting guidelines</a> and cajoling the model, I didn't do much work.  I didn't do any debugging by
hand or have to intercede to understand what was happening. In the case a test found a discrepancy, I would have the model write a <a href="https://github.com/rjpower/zopfli/blob/master/port/bugs/20241219_deflate_tree_encoding_discrepancy.md">bug report</a>
and then iterate until it solved the problem.</p>
<p>But again this wasn't automated and it was hardly reproducible. I had to babysit
the process quite a bit<label for="sn-babysit-06e936e9"></label><span>Mostly telling it to keep going. I probably could have written <code>while true; echo 'keep going until the test passes' | claude</code> and gotten most of the way there. </span>, and while the library seemed to work, the
results were <em>subtly different</em> than the original Zopfli C implementation. And
because of the ad-hoc approach, I didn't have a good idea of what had changed.</p>
<p>Still this was promising. So I was incentivized to try again, with a bit more rigor and automation.</p>
<h2>Porting C to Rust, "for real"</h2>
<p>Our final process would be more rigorous. We want to port a small portion of our
program at a time, and automate as much of the porting as possibly. Instead of
calling into an agent and babysitting it, we'd need to hand-roll our own
machinery to call into an LLM API and perform the necessary edits in a fully
automated fashion. So our overall process would be:</p>
<ul>
<li>
<p>Sort symbols in topological order based on the static call graph. If function F calls function G, then first port function G, then function F, etc. (Cycles need to be ported together, but we didn't have any for this library). This ensures that when we are porting a new symbol into Rust, we can call into our child symbols immediately; we don't have to implement anything new or call into C. "Symbol" here means a struct, enum or function.</p>
</li>
<li>
<p>For a given public C declaration (in a header file), create an FFI entry for it and a Rust implementation with the exact same signature. We'll do this by giving the LLM the C header &amp; source and asking for a translation.</p>
</li>
</ul>
<pre><code>// ffi.rs
pub fn AddDynamicTree(
    ll_lengths: *const ::std::os::raw::c_uint,
    d_lengths: *const ::std::os::raw::c_uint,
    bp: *mut ::std::os::raw::c_uchar,
    out: *mut *mut ::std::os::raw::c_uchar,
    outsize: *mut usize,
);

// deflate.rs
pub unsafe fn AddDynamicTree(
    ll_lengths: *const c_uint,
    d_lengths: *const c_uint,
    bp: *mut c_uchar,
    out: *mut *mut c_uchar,
    outsize: *mut usize,
) {
    ...
}
</code></pre>
<ul>
<li>Have the LLM write a fuzz test which samples over possibly inputs and compares the C and Rust implementations, asserting if there is a discrepancy <a href="https://github.com/rjpower/portkit/blob/1d5a2c21adbfc71cb0906fd16a0cf7252be5dcd5/zopfli-port/rust/fuzz/fuzz_targets/fuzz_AddDynamicTree.rs">example</a>. No attempt was made to guide the fuzz test inputs other than instructing the LLM to avoid generating invalid structures (e.g. null pointers or numbers out of an expected range).</li>
</ul>
<pre><code>#[derive(Debug, arbitrary::Arbitrary)]
struct FuzzInput {
    ll_lengths: Vec&lt;u32&gt;,
    d_lengths: Vec&lt;u32&gt;,
}

fuzz_target!(|input: FuzzInput| {
    ...
    // setup inputs
    let c_result = ffi::AddDynamicTree(...)
    let rust_result = rust::AddDynamicTree(...)

    assert_eq!(c_result, rust_result, "C and Rust diverged.");
</code></pre>
<ul>
<li>Run the fuzz test and fix until everything compiles and the fuzz test passes.</li>
</ul>
<p>We repeat this for each symbol in the program until we hit the top-level main().</p>
<p>Prior to porting I modified the C source slightly to make a few static functions
extern so that we could create an FFI to them. This would allow the LLM to port
smaller chunks at a time (otherwise there were some modules where the LLM would
have to port 1000 lines of code at once because only the main symbol was
visible). I also copied some <code>#define</code> constants by hand because my C traversal
hadn't detected them.</p>
<p>Originally I tried to break up the task of porting a symbol. We'd separately
define the FFI to C, a stub implementation of the Rust function, define a fuzz
test, then build the implementation. The idea was that this would let us verify
each of these via a separate call and we'd be more robust. Ultimately this
proved more confusing to the agents than helpful.</p>
<p>In the end, I simply prompted the LLM to do all the steps for porting a symbol
at once, reprompting it if the fuzz test didn't exist or pass.</p>
<p>And in the end... it worked? The result is a <a href="https://github.com/rjpower/portkit/tree/main/zopfli-port">Rust implementation of
Zopfli</a> that gives
<em>identical</em> results on every input I've tried to the C version. This is
different from the Syzygy results, where they ended up with a program that
compresses correctly, but not identically <label for="sn-better-46a42f45"></label><span>This isn't a claim my approach is <em>better</em> than Syzygy, just different. </span> to the C version.  Because
we locked the Rust and C versions to use the same API at each step, the
resulting program isn't very "rusty", but its a complete translation.</p>
<p>About 90% of symbols were auto-ported using gemini-2.5-pro-06-05 and a crappy
"agent" library I wrote up for this task. The remaining 10% I switched over to
running in a Claude Code, as Gemini seemed to struggle with patch formats and
imports. The only work I did manually was to adjust a few places where the LLM
was calling the FFI code from Rust and clean up some warning messages.</p>
<h2>But why does this work at all?</h2>
<p>To clarify, the surprising result is not that fuzzing would detect discrepancies
at the top-level of our library. Target specific fuzzers work great for this task:
<a href="https://github.com/csmith-project/csmith">CSmith</a> and
<a href="https://jepsen.io/">Jepsen</a> find all sorts of weird interesting bugs.
What's surprising is that with only one exception<label for="sn-claude-5296e104"></label><span>Gemini introduces a weird typo into a program, which was detected a few symbols downstream by Claude and repaired. You can see <a href="https://rjp.io/blog/claude-rust-port-conversation">the session log here</a>. </span>, the LLM translation + naive
fuzz test correctly validated the symbol behavior <em>for every symbol</em>. This meant
that we didn't run into any issues where after porting A, B ... Q, suddenly we
detect a subtle bug in R.</p>
<p>This meant we didn't have to "backtrack", or inspect the rest of the code base
as we ported symbols: each symbol ported in isolation, we tested it (implicitly
testing the dependent symbols as well), and we moved on. If this didn't work,
we'd have to inspect the whole program and debug it, over and over, as we built
it up.</p>
<p>If we think about it, this shouldn't have worked as well as it did. Fuzzing
shouldn't hold for all functions. Imagine I'm converting something like a manual
floating-point multiplier:</p>
<pre><code>mul(a_bits: &amp;[byte], b_bits: &amp;[byte], c_bits: mut &amp;[byte]):
</code></pre>
<p>If I fuzz this by providing random bytes, am I likely to detect a subtle issue
with underflow, or detect the <a href="https://en.wikipedia.org/wiki/Pentium_F00F_bug">Pentium FOOF bug</a>?  Or imagine a C parser:
fuzzing random bytes wouldn't trigger much of the parser. Without a lot of
probes, it seems hard to test these function spaces!</p>
<p>My hunch is that this works due to a combination of factors:</p>
<ul>
<li>I chose a simple library to work with. Zopfli isn't trivial, but you aren't juggling a lot of state, for example.</li>
<li>Fuzzers are good at probing the input space. If you <a href="https://llvm.org/docs/LibFuzzer.html#tracing-cmp-instructions">compile with the right arguments</a> the fuzzer will try to choose inputs which trigger different branches. You can even give good examples (a corpus) to start the fuzzer off. I didn't do this for my experiments, but its easy to imagine an LLM generating a decent starting corpus.</li>
<li>Most code doesn't express subtle logic paths. If I test if a million inputs are correctly sorted, I've probably implemented the sorter correctly.</li>
<li>Complex logic, when it exists, is broken up across functions. Our individual tests make it easier to ensure the combined calls work.</li>
<li>The LLM produces correct code most of the time! The fuzz test is just there to validate we did the right thing.</li>
</ul>
<h2>Caveats, or don't try this at home</h2>
<p>While the overall system seems to work, its far from ready to drop in to a new project.</p>
<p><em>The resulting Rust code is very "C-like"</em></p>
<p>By construction, we use the same unsafe C interface for each symbol we port.
This is different from Syzygy, which generates a safe Rust version of the
program in one shot.  This was convenient but not strictly required: I could
have tweaked the prompts and clearly indicate public/private interfaces, letting
the LLM use more idiomatic Rust for internal functions. Retaining the same
interfaces simplified writing the fuzz tests and let the LLM call into the
original C code when debugging.</p>
<p>That said, because our end result has end-to-end fuzz tests and tests for every
symbol, its now much easier to "rustify" the code with confidence. This is a
great task for future work.</p>
<p><em>The automation isn't complete</em></p>
<p>I tweaked the agent framework and prompts as I went to get better results out of Gemini. I needed to have Claude come in at the end to finish the last few symbols, and I fixed a few typos here and there. With better prompting and a better agent framework, this intervention would be reduced, but likely not go away entirely.</p>
<p><em>Zopfli is easy</em></p>
<p>Zopfli is a simple library with many functions which are input/output oriented.
It's not clear how this would work if you introduced more stateful interfaces.</p>
<h2>Conclusions and future work</h2>
<p>Whew. This ended up taking a few days and being more than the initial trivial
experiment I intended. That said, I think there's some interesting insights:</p>
<ul>
<li><em>Porting via LLMs is surprisingly cost effective and only getting cheaper</em>. Even with my multiple rounds of experimentation and tweaking agents etc, the total cost for this experiment was ~$50, or about $0.01 a line. (For comparison, Syzygy cost ~$1500 using the O3 model). I suspect this could be brought down another 10x by walking up a "complexity tree": first try porting symbols mechanically (constants, structs, enums), then trying out a cheap model before falling back to more expensive models. For instance, Gemini Flash 2.5 is capable of producing correct Rust code for some non-trivial problems.</li>
<li><em>Full automation is hard</em>. The chat based interface of LLMs/agents can lead us to believe that they are more capable than they really are. We don't often realize how much we're guiding the direction of the LLMs until we try to go completely hands off.</li>
<li><em>Full automation isn't necessary</em>. Imagine a system where you have agents port as many symbols as they possibly can, in parallel. If a symbol fails to port, you mark it "tainted" and move on. You then continue until you can't port anything else. In such a system, a human could be brought in to fix issues as they emerge, but you could still rely on the agents to handle &gt;90% of the work of porting in an asynchronous manner.</li>
</ul>
<p>If you're interested in the (truly terrible) code I used for the porting, you
can find it on <a href="https://github.com/rjpower/portkit/">Github</a>. I'm not sure if I'll
continue down this route further myself, but let me know if you're interested in
this space or trying to port a project and I'm happy to chat more!</p>


</section>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing documentation for AI: best practices (156 pts)]]></title>
            <link>https://docs.kapa.ai/improving/writing-best-practices</link>
            <guid>44311217</guid>
            <pubDate>Wed, 18 Jun 2025 16:23:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.kapa.ai/improving/writing-best-practices">https://docs.kapa.ai/improving/writing-best-practices</a>, See on <a href="https://news.ycombinator.com/item?id=44311217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Retrieval-Augmented Generation (RAG) systems like Kapa rely on your
documentation to provide accurate, helpful information. When documentation
serves both humans and machines well, it creates a self-reinforcing loop of
content quality: clear documentation improves AI answers, and those answers
help surface gaps that further improve the docs.</p>
<p>This guide provides best practices for creating documentation that works
effectively for both human readers and AI/LLM consumption in RAG systems. Many
best practices benefit both simultaneously, often in complementary ways.</p>
<h2 id="why-documentation-quality-matters">Why documentation quality matters<a href="#why-documentation-quality-matters" aria-label="Direct link to Why documentation quality matters" title="Direct link to Why documentation quality matters">​</a></h2>
<p>Documentation quality has always been important for helping users understand
and use your product effectively. And it becomes even more important when AI
systems use that same content to answer user questions. Poor documentation
doesn't just frustrate human readers, it directly degrades the quality of AI
responses, creating a compounding problem where bad content leads to bad
answers.</p>
<p>Understanding how AI systems process and use your documentation reveals why
content quality is non-negotiable for good AI performance.</p>
<h3 id="how-ai-systems-process-your-documentation">How AI systems process your documentation<a href="#how-ai-systems-process-your-documentation" aria-label="Direct link to How AI systems process your documentation" title="Direct link to How AI systems process your documentation">​</a></h3>
<p>Kapa works by finding relevant pieces of your content and using them to
construct answers. The process involves three main components:</p>
<ul>
<li><strong>Retriever</strong>: Searches through your knowledge sources to find content that
matches the user's question</li>
<li><strong>Vector database</strong>: Stores your content in a searchable format that enables
fast and accurate retrieval</li>
<li><strong>Generator</strong>: A Large Language Model (LLM) that uses the retrieved content
to create helpful responses</li>
</ul>
<p>Information flows through a specific process once you connect knowledge sources
to Kapa:</p>
<ol>
<li><strong>Ingestion</strong>: Content is divided into chunks (smaller, focused sections)
and stored in the vector database</li>
<li><strong>Query processing</strong>: When users ask questions, the system converts their
question into a searchable format</li>
<li><strong>Retrieval</strong>: The system finds the most relevant chunks from your
documentation</li>
<li><strong>Answer generation</strong>: The LLM uses these chunks as context to generate a
response</li>
</ol>
<p>In the steps that an AI takes to consume your content, there are some writing
and structural patterns worth highlighting that can negatively impact how well
your content is understood:</p>
<ul>
<li><strong>AI systems work with chunks</strong>: They process documentation as discrete,
independent pieces rather than reading it as a continuous narrative</li>
<li><strong>They rely on content matching</strong>: They find information by comparing user
questions with your content, not by following logical document structure</li>
<li><strong>They lose implicit connections</strong>: Relationships between sections may not be
preserved unless explicitly stated</li>
<li><strong>They cannot infer unstated information</strong>: Unlike humans who can make
reasonable assumptions, AI systems can only work with explicitly documented
information</li>
</ul>
<p>Documentation optimized for AI systems should ideally be explicit,
self-contained, and contextually complete. The more a chunk can stand alone
while maintaining clear relationships to related content, the better it can be
understood by the AI. The more explicit and less ambiguous the information is,
the better the retrieval accuracy is and the better equipped the AI becomes at
answering questions confidently.</p>
<p>While AI does work remarkably well with unstructured content, it's also true
that information written and structured for with retrieval in mind can greatly
improve the quality of an "Ask AI" interface to your knowledge sources.</p>
<h3 id="why-chunking-is-necessary">Why chunking is necessary<a href="#why-chunking-is-necessary" aria-label="Direct link to Why chunking is necessary" title="Direct link to Why chunking is necessary">​</a></h3>
<p>Ideally, chunking would not be necessary, and the AI could continuously keep
your entire knowledge base in context, all the time. Unfortunately, this is
impractical. Not only due to token limits but also because LLMs perform
significantly better when provided with optimized, focused contexts. A large or
overly broad context increases the likelihood that the model overlooks or
misinterprets critical information, resulting in reduced accuracy and less
coherent outputs.</p>
<p>Dividing documents into smaller, semantically coherent chunks enables retrieval
systems to present the most relevant content to the LLM. This targeted approach
significantly improves model comprehension, retrieval precision, and overall
response quality.</p>
<h2 id="quick-tips-to-optimize-your-content">Quick tips to optimize your content<a href="#quick-tips-to-optimize-your-content" aria-label="Direct link to Quick tips to optimize your content" title="Direct link to Quick tips to optimize your content">​</a></h2>
<p>Optimizing content for AI is similar in principle to optimizing content for
accessibility and screen readers: the clearer, more structured, and more
machine-readable your content is, the better it performs. Just as clear
semantic structure helps accessibility tools parse content effectively, a clear
structure significantly improves AI accuracy. This section outlines some
actionable, practical improvements you can apply today to make your docs more
machine-readable.</p>
<p>Prioritizing these adjustments sets a strong foundation for addressing more
nuanced content challenges, as discussed in the section <a href="#content-design-challenges-for-ai">Content design
challenges for AI</a>.</p>
<h3 id="1-use-standardized-semantic-html">1. Use standardized semantic HTML<a href="#1-use-standardized-semantic-html" aria-label="Direct link to 1. Use standardized semantic HTML" title="Direct link to 1. Use standardized semantic HTML">​</a></h3>
<p>For website sources, ensure correct and semantic use of HTML elements like
headings (<code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>), lists (<code>&lt;ul&gt;</code>, <code>&lt;ol&gt;</code>), and tables (<code>&lt;table&gt;</code>).
Semantic HTML ensures clear document structure, improving how accurately
content is chunked and retrieved.</p>
<div><p>Example</p><div><pre tabindex="0"><code><span><span>&lt;</span><span>h2</span><span>&gt;</span><span>How to enable webhooks</span><span>&lt;/</span><span>h2</span><span>&gt;</span><span></span><br></span><span><span></span><span>&lt;</span><span>ol</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Log in to your CloudSync dashboard.</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Navigate to Settings </span><span>&amp;gt;</span><span> Webhooks.</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Toggle webhooks to "Enabled".</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span></span><span>&lt;/</span><span>ol</span><span>&gt;</span><br></span></code></pre></div></div>
<p>More importantly, <strong>avoid incorrect use</strong> of elements. An incorrectly placed
<code>&lt;h2&gt;</code> element, for example, can have dire consequences for how a machine
parses your content.</p>
<h3 id="2-avoid-pdfs-prefer-html-or-markdown">2. Avoid PDFs, prefer HTML or Markdown<a href="#2-avoid-pdfs-prefer-html-or-markdown" aria-label="Direct link to 2. Avoid PDFs, prefer HTML or Markdown" title="Direct link to 2. Avoid PDFs, prefer HTML or Markdown">​</a></h3>
<p>PDF documents often have complex visual layouts that make machine parsing
difficult. Migrating content from PDFs to HTML or Markdown drastically improves
text extraction and retrieval quality.</p>
<h3 id="3-create-crawler-friendly-content">3. Create crawler-friendly content<a href="#3-create-crawler-friendly-content" aria-label="Direct link to 3. Create crawler-friendly content" title="Direct link to 3. Create crawler-friendly content">​</a></h3>
<p>Simplify page structures by reducing or eliminating custom UI elements,
JavaScript-driven dynamic content, and complex animations. Clear, predictable
HTML structure facilitates easier indexing and parsing.</p>
<p>Replace complex JavaScript widgets with plain-text alternatives or simple
interactive elements.</p>
<h3 id="4-ensure-semantic-clarity">4. Ensure semantic clarity<a href="#4-ensure-semantic-clarity" aria-label="Direct link to 4. Ensure semantic clarity" title="Direct link to 4. Ensure semantic clarity">​</a></h3>
<p>Use descriptive headings and meaningful URLs reflecting the content hierarchy.
Semantic clarity helps the AI correctly infer content relationships, greatly
enhancing retrieval accuracy.</p>
<div><p>Example of a meaningful URL</p><div><pre tabindex="0"><code><span><span>✅ Good: /docs/cloudsync/setup-webhooks</span><br></span><span><span>❌ Poor: /docs/page12345</span><br></span></code></pre></div></div>
<h3 id="5-provide-text-equivalents-for-visuals">5. Provide text equivalents for visuals<a href="#5-provide-text-equivalents-for-visuals" aria-label="Direct link to 5. Provide text equivalents for visuals" title="Direct link to 5. Provide text equivalents for visuals">​</a></h3>
<p>Always include clear text descriptions for critical visual information such as
diagrams, charts, and screenshots. This ensures crucial details remain
accessible to machines and screen readers alike.</p>
<div><p>Example</p><div><pre tabindex="0"><code><span><span>!</span><span>[</span><span>System architecture diagram</span><span>](</span><span>architecture.png</span><span>)</span><span></span><br></span><span><span></span><br></span><span><span></span><span>**</span><span>Figure 1:</span><span>**</span><span> Diagram illustrating the CloudSync integration workflow,</span><br></span><span><span>detailing authentication, data upload, and confirmation steps.</span><br></span></code></pre></div></div>
<h3 id="6-keep-layouts-simple">6. Keep layouts simple<a href="#6-keep-layouts-simple" aria-label="Direct link to 6. Keep layouts simple" title="Direct link to 6. Keep layouts simple">​</a></h3>
<p>Avoid layouts where meaning is derived heavily from visual positioning or
formatting. Layout is lost during conversion, and any meaning it was designed
to convey with it. Content structured simply with clear headings, lists, and
paragraphs translates effectively into plain text.</p>
<h2 id="content-design-challenges-for-ai">Content design challenges for AI<a href="#content-design-challenges-for-ai" aria-label="Direct link to Content design challenges for AI" title="Direct link to Content design challenges for AI">​</a></h2>
<p>This section takes a closer look at common content design anti-patterns that
can create challenges for AI systems. These challenges often arise from how
information is organized, contextualized, or assumed rather than how it's
formatted. Each example highlights a specific problem pattern, why it causes
issues for AI, and how to rewrite or restructure your content to avoid it.</p>
<h3 id="contextual-dependencies">Contextual dependencies<a href="#contextual-dependencies" aria-label="Direct link to Contextual dependencies" title="Direct link to Contextual dependencies">​</a></h3>
<p><strong>The problem:</strong> Documentation that scatters key details and definitions across
multiple sections or paragraphs creates problems when content is divided into
chunks. When critical information is separated from its context, individual
chunks can become ambiguous or incomplete.</p>
<p>Understanding how chunking works in practice reveals why proximity matters.
Kapa attempts to preserve document structure by keeping sections intact when
possible, but practical constraints often force splits:</p>
<ul>
<li>Sections that are too long get divided at paragraph or sentence boundaries</li>
<li>Sections that are too short get combined with neighboring content</li>
<li>Chunk sizes must be balanced for optimal retrieval performance</li>
</ul>
<p>Since chunk boundaries can't be perfectly predicted, the closer related
information appears in your source content, the more likely it stays together
after chunking. This proximity principle becomes critical for maintaining
meaning.</p>
<p>Consider this (simplified) problematic example:</p>
<div><pre tabindex="0"><code><span><span>Authentication tokens expire after 24 hours by default.</span><br></span><span><span></span><br></span><span><span>The system provides several configuration options for different environments.</span><br></span><span><span></span><br></span><span><span>When implementing the login flow, ensure you handle this appropriately.</span><br></span></code></pre></div>
<p>When this content gets chunked, the middle sentence about configuration options
might cause the chunking algorithm to separate the token expiration detail from
the implementation guidance. The resulting chunk containing "When implementing
the login flow, ensure you handle this appropriately" loses crucial context
about what "this" refers to and the specific 24-hour timeframe.</p>
<p><strong>The remedy:</strong> Keep related information together within close proximity. When
introducing a concept that has important constraints or context, include those
details in the same paragraph or immediately adjacent paragraphs.</p>
<div><pre tabindex="0"><code><span><span>Authentication tokens expire after 24 hours by default. When implementing the</span><br></span><span><span>login flow, ensure you handle token expiration by refreshing tokens before the</span><br></span><span><span>24-hour limit or implementing proper error handling for expired token</span><br></span><span><span>responses.</span><br></span><span><span></span><br></span><span><span>The system provides several configuration options for different environments,</span><br></span><span><span>including custom token expiration periods.</span><br></span></code></pre></div>
<p>By keeping the constraint (24-hour expiration) close to its implementation
guidance, they're much more likely to remain in the same chunk, regardless of
where the boundaries fall.</p>
<p>Look for sections that become unclear when read in isolation, especially where
section headings are generic and multi-step processes that reference context
from earlier paragraphs.</p>
<h3 id="semantic-discoverability-gaps">Semantic discoverability gaps<a href="#semantic-discoverability-gaps" aria-label="Direct link to Semantic discoverability gaps" title="Direct link to Semantic discoverability gaps">​</a></h3>
<p><strong>The problem:</strong> Kapa finds information based on semantic similarity between
queries and content. If important terms or concepts aren't present in a chunk,
that chunk won't be retrieved for relevant queries, even if it contains exactly
the information needed.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> Configure timeouts</span><span></span><br></span><span><span></span><br></span><span><span>Configure custom timeout settings and retry logic for improved reliability in</span><br></span><span><span>production environments. Access these options through the admin panel.</span><br></span></code></pre></div>
<p>If a user asks "How do I configure <strong>CloudSync</strong> timeouts?", this chunk might
not be retrieved because "CloudSync" doesn't appear in the text.</p>
<p><strong>The remedy:</strong> Establish consistent terminology for your product's unique
concepts and use them systematically. Include specific product or feature names
when documenting functionality.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> Configure CloudSync timeouts</span><span></span><br></span><span><span></span><br></span><span><span>Configure custom CloudSync timeout settings and retry logic for improved</span><br></span><span><span>reliability in production environments. Access these options through the</span><br></span><span><span>CloudSync admin panel.</span><br></span></code></pre></div>
<p>Your product's unique terminology won't be well-represented in the model's
training data. Explicit, consistent usage helps establish what content relates
to which product features.</p>
<p><strong>A note of balance:</strong> This doesn't mean you should repeat the product name in
every sentence or heading. Kapa also uses document structure, URLs, and parent
headings to infer context. The important thing is that for any given chunk,
there’s a clear and consistent signal that connects it to your product or
feature. See <a href="#hierarchical-information-architecture">Hierarchical information architecture</a>
for how structural metadata supports this.</p>
<h3 id="implicit-knowledge-assumptions">Implicit knowledge assumptions<a href="#implicit-knowledge-assumptions" aria-label="Direct link to Implicit knowledge assumptions" title="Direct link to Implicit knowledge assumptions">​</a></h3>
<p><strong>The problem:</strong> Kapa operates on a simple principle: if information isn't
explicitly documented, it doesn't exist in the system's knowledge base. Unlike
human readers who can draw on external knowledge or make reasonable inferences,
Kapa only works with the information provided.</p>
<p>When documentation assumes user knowledge, these become dangerous gaps.
Well-designed RAG systems should choose uncertainty over inaccuracy, but this
only works when documentation explicitly addresses the topics users ask about.</p>
<p><strong>The remedy:</strong> Include prerequisite steps within procedural content rather
than assuming prior setup. When referencing external tools or concepts, provide
brief context or links to detailed explanations.</p>
<div><p>Before</p><div><pre tabindex="0"><code><span><span>##</span><span> Setting up webhooks</span><span></span><br></span><span><span></span><br></span><span><span>Configure your endpoint URL in the dashboard and test the connection.</span><br></span></code></pre></div></div>
<div><p>After</p><div><pre tabindex="0"><code><span><span>##</span><span> Setting up CloudSync webhooks</span><span></span><br></span><span><span></span><br></span><span><span>Before configuring webhooks, ensure you have:</span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> A publicly accessible HTTPS endpoint</span><br></span><span><span></span><span>-</span><span> Valid SSL certificate</span><br></span><span><span></span><span>-</span><span> CloudSync API credentials</span><br></span><span><span></span><br></span><span><span>Configure your endpoint URL in the CloudSync dashboard under Settings &gt;</span><br></span><span><span>Integrations, then use the "Test connection" button to verify setup.</span><br></span></code></pre></div></div>
<p>Look for instructions that assume familiarity with tools or interfaces, or
reference "standard" configurations without explanation.</p>
<h3 id="visual-information-dependencies">Visual information dependencies<a href="#visual-information-dependencies" aria-label="Direct link to Visual information dependencies" title="Direct link to Visual information dependencies">​</a></h3>
<p><strong>The problem:</strong> Critical information embedded in images, diagrams, and videos
create problems for the ingestion processes that parse your documentation. When
key information appears only in visual elements, users may receive incomplete
answers.</p>
<div><p>Example: Information that completely depends on a graphical element</p><div><pre tabindex="0"><code><span><span>See the diagram below for the complete API workflow:</span><br></span><span><span></span><span>!</span><span>[</span><span>Complex flowchart showing 8-step process</span><span>](</span><span>workflow.png</span><span>)</span><span></span><br></span><span><span></span><br></span><span><span>Follow these steps to implement the integration.</span><br></span></code></pre></div></div>
<p>Instructions that depend on visual elements become inaccessible to automated
systems, making the instruction meaningless.</p>
<p><strong>The remedy:</strong> Provide text-based alternatives that capture the essential
information. Represent workflow diagrams as numbered step lists while keeping
visual elements as supplements.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> CloudSync API workflow</span><span></span><br></span><span><span></span><br></span><span><span>The CloudSync integration follows this workflow:</span><br></span><span><span></span><br></span><span><span></span><span>1.</span><span> </span><span>**</span><span>Authentication</span><span>**</span><span>: Send API credentials to </span><span>`/auth/token`</span><span> endpoint</span><br></span><span><span></span><span>2.</span><span> </span><span>**</span><span>Validation</span><span>**</span><span>: System validates credentials and returns access token</span><br></span><span><span></span><span>3.</span><span> </span><span>**</span><span>Data preparation</span><span>**</span><span>: Format your data according to CloudSync schema</span><br></span><span><span></span><span>4.</span><span> </span><span>**</span><span>Upload request</span><span>**</span><span>: POST data to </span><span>`/sync/upload`</span><span> with access token</span><br></span><span><span></span><span>5.</span><span> </span><span>**</span><span>Processing</span><span>**</span><span>: CloudSync validates and processes the data</span><br></span><span><span></span><span>6.</span><span> </span><span>**</span><span>Status check</span><span>**</span><span>: Poll </span><span>`/sync/status/{job_id}`</span><span> for processing updates</span><br></span><span><span></span><span>7.</span><span> </span><span>**</span><span>Completion</span><span>**</span><span>: Receive confirmation when sync completes</span><br></span><span><span></span><span>8.</span><span> </span><span>**</span><span>Error handling</span><span>**</span><span>: Handle any validation or processing errors</span><br></span><span><span></span><br></span><span><span></span><span>!</span><span>[</span><span>API workflow diagram</span><span>](</span><span>workflow.png</span><span>)</span><span></span><br></span><span><span></span><span>_</span><span>Visual representation of the workflow steps above</span><span>_</span><br></span></code></pre></div>
<h3 id="layout-dependent-information">Layout-dependent information<a href="#layout-dependent-information" aria-label="Direct link to Layout-dependent information" title="Direct link to Layout-dependent information">​</a></h3>
<p><strong>The problem:</strong> Information that depends on visual layout, positioning, or
table structure often loses meaning when processed as text by machines. While
humans can interpret visual relationships and grouped content, AI systems
struggle to maintain these connections.</p>
<p>Complex or poorly structured comparison tables with merged headers and visual
groupings become ambiguous when converted to plain text:</p>
<table><thead><tr><th>Pricing</th><th></th><th></th></tr></thead><tbody><tr><td><strong>Basic Plan</strong></td><td><strong>Standard Plan</strong></td><td><strong>Enterprise Plan</strong></td></tr><tr><td>5 users</td><td>25 users</td><td>Unlimited users</td></tr><tr><td>1GB storage</td><td>10GB storage</td><td>Unlimited storage</td></tr><tr><td>Email support</td><td>Phone support</td><td>24/7 dedicated support</td></tr><tr><td><strong>API Limits</strong></td><td></td><td></td></tr><tr><td>100 requests/hour</td><td>1,000 requests/hour</td><td>No rate limit</td></tr><tr><td>Basic endpoints only</td><td>All endpoints</td><td>All endpoints + webhooks</td></tr></tbody></table>
<p><strong>The remedy:</strong> If a tabular representation is preferable, ensure that the
headers and rows are semantically correct. However, tabular representation is
not always appropriate or necessary. You may also consider alternatives that
preserve relationships in text form. Use structured lists or repeated context
that maintains the connections. For example:</p>
<div><pre tabindex="0"><code><span><span>##</span><span> CloudSync pricing plans</span><span></span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Basic Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> 5 users</span><br></span><span><span></span><span>-</span><span> 1GB storage</span><br></span><span><span></span><span>-</span><span> Email support</span><br></span><span><span></span><span>-</span><span> API limits: 100 requests/hour, basic endpoints only</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Standard Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> 25 users</span><br></span><span><span></span><span>-</span><span> 10GB storage</span><br></span><span><span></span><span>-</span><span> Phone support</span><br></span><span><span></span><span>-</span><span> API limits: 1,000 requests/hour, all endpoints</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Enterprise Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> Unlimited users</span><br></span><span><span></span><span>-</span><span> Unlimited storage</span><br></span><span><span></span><span>-</span><span> 24/7 dedicated support</span><br></span><span><span></span><span>-</span><span> API limits: No rate limit, all endpoints plus webhooks</span><br></span></code></pre></div>
<p>Keep simple reference tables where each row is self-contained, but supplement
or replace complex tables where relationships between cells convey important
meaning.</p>
<h2 id="content-organization">Content organization<a href="#content-organization" aria-label="Direct link to Content organization" title="Direct link to Content organization">​</a></h2>
<p>The following techniques help create content that can be effectively retrieved,
without sacrificing readability.</p>
<h3 id="hierarchical-information-architecture">Hierarchical information architecture<a href="#hierarchical-information-architecture" aria-label="Direct link to Hierarchical information architecture" title="Direct link to Hierarchical information architecture">​</a></h3>
<p>When your content gets ingested into Kapa, preprocessing steps extract metadata
that helps preserve context and boost retrieval accuracy. One of the most
valuable pieces of data extracted is the hierarchical position of each document
or section.</p>
<p>This hierarchy includes multiple layers of context: URL paths, document titles,
and headings. These elements work together to build contextual understanding
for content chunks after they're separated from their original location.</p>
<p>Design your content hierarchy so that each section carries sufficient context
to be understood independently, while maintaining clear relationships to parent
and sibling content.</p>
<p>When planning content structure, consider how users would find any given
section without search. Ensure each section includes enough context to be
understood independently:</p>
<ul>
<li><strong>Product family</strong>: Which product or service area</li>
<li><strong>Product name</strong>: Specific product or feature name</li>
<li><strong>Version information</strong>: When applicable</li>
<li><strong>Component specificity</strong>: Subfeatures or modules</li>
<li><strong>Functional context</strong>: What the user is trying to accomplish</li>
</ul>
<p>This hierarchical clarity helps AI systems understand relationships between
concepts and provides richer context when retrieving information for user
queries.</p>
<h3 id="self-contained-sections">Self-contained sections<a href="#self-contained-sections" aria-label="Direct link to Self-contained sections" title="Direct link to Self-contained sections">​</a></h3>
<p>Documentation sections that depend on readers following a linear path or
remembering details from previous sections become problematic when processed as
independent chunks. Sections are retrieved based on relevance and document
order is not preserved, so sections should ideally make sense when encountered
in isolation.</p>
<p>Compare these two approaches to the same information:</p>
<div><p>Context-dependent</p><div><pre tabindex="0"><code><span><span>##</span><span> Updating webhook URLs</span><span></span><br></span><span><span></span><br></span><span><span>Now change the endpoint to your new URL and save the configuration.</span><br></span></code></pre></div></div>
<div><p>Self-contained</p><div><pre tabindex="0"><code><span><span>##</span><span> Updating webhook URLs</span><span></span><br></span><span><span></span><br></span><span><span>To update webhook endpoints in CloudSync:</span><br></span><span><span></span><br></span><span><span></span><span>1.</span><span> Navigate to Settings &gt; Webhooks in your CloudSync dashboard</span><br></span><span><span></span><span>2.</span><span> Select the webhook you want to modify</span><br></span><span><span></span><span>3.</span><span> Change the endpoint URL to your new address, and click Save</span><br></span></code></pre></div></div>
<p>The self-contained version works when retrieved as an isolated chunk because it
includes the essential context: what system (CloudSync), where to find the
setting (Settings &gt; Webhooks), and complete steps. The context-dependent
version assumes the reader knows what "endpoint" refers to and where they are
in the interface.</p>
<p>Front-load essential context and include complete information within each
section boundary. This doesn't mean repeating everything everywhere, but
ensuring sections remain actionable when encountered independently.</p>
<p>Consider starting each section with brief context about its scope and
prerequisites, using descriptive headings that indicate what the section
accomplishes, and including essential background information without assuming
prior reading. Look for sections that reference "as mentioned above," "now that
you've," or "with everything configured" as signals that context needs to be
made explicit.</p>
<h3 id="error-context-with-solutions">Error context with solutions<a href="#error-context-with-solutions" aria-label="Direct link to Error context with solutions" title="Direct link to Error context with solutions">​</a></h3>
<p>Troubleshooting documentation deserves special attention because users often
search by copying exact error messages they encounter. When your documentation
includes the specific error text alongside solutions, it creates direct matches
between user queries and helpful content.</p>
<p>When documenting troubleshooting steps, quote exact error messages and describe
observable symptoms alongside solutions.</p>
<div><p>Generic troubleshooting</p><div><pre tabindex="0"><code><span><span>##</span><span> Connection problems</span><span></span><br></span><span><span></span><br></span><span><span>If the connection fails, check your network settings and firewall configuration.</span><br></span></code></pre></div></div>
<div><p>Specific troubleshooting</p><div><pre tabindex="0"><code><span><span>##</span><span> CloudSync connection problems</span><span></span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Error: "Connection timeout after 30 seconds"</span><span></span><br></span><span><span></span><br></span><span><span>This error occurs when CloudSync cannot reach the…</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Error: "Authentication failed (401)"</span><span></span><br></span><span><span></span><br></span><span><span>This indicates invalid or expired credentials…</span><br></span></code></pre></div></div>
<p>Including exact error text ensures users can find help when searching with the
specific messages they're seeing. To identify which error messages to
prioritize in your documentation, review the <a href="https://docs.kapa.ai/analytics/common-questions">Common Questions</a>
analytics in the Kapa platform. This shows you the actual error messages and
problems users are asking about most frequently.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Creating documentation that serves both human readers and AI effectively
centers on a fundamental principle: explicit, self-contained content that
maintains clear relationships between concepts. Eliminating contextual
dependencies, ensuring discoverability, filling knowledge gaps, and providing
text alternatives for visual content help mitigate inherent limitations in how
machines consume your docs.</p>
<p>Documentation that works for AI is, at its core, just great documentation:
clear, structured, explicit, and user-focused. The better your docs serve your
users, the better your AI serves them, too.</p>
<p>Review and analyze user conversations, particularly conversations with
uncertain or downvoted answers. Start with immediate fixes to frequently asked
questions, then gradually restructure scattered information into coherent,
complete sections. The goal is documentation where every section stands alone
while maintaining logical connections to related concepts.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My iPhone 8 Refuses to Die: Now It's a Solar-Powered Vision OCR Server (248 pts)]]></title>
            <link>https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</link>
            <guid>44310944</guid>
            <pubDate>Wed, 18 Jun 2025 15:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/">https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</a>, See on <a href="https://news.ycombinator.com/item?id=44310944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>After running for over a year, my solar-powered setup has processed <strong>83,418 OCR requests</strong> and <strong>48GB of images</strong> using nothing but Apple’s Vision framework and renewable energy. Most people toss their old iPhones in a drawer when they upgrade. Me? I turned mine into a server that saves me money while running completely off-grid.</p>
<p><em>Note: The OCR processing serves a separate side project unrelated to this blog.</em></p>
<p>Could I have just run this on my Mac like a normal person? Absolutely. But where’s the fun in that?</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-vision-ocr-server-solar.jpg" alt="iPhone 8 OCR Server Setup">
</p>
<h2 id="tldr---technical-summary">TL;DR - Technical Summary</h2>
<p><strong>The Setup:</strong></p>
<ul>
<li>iPhone 8 running SwiftUI app with Apple Vision OCR</li>
<li>EcoFlow River 2 Pro (768Wh) + 220W solar panel</li>
<li>Mini PC handling web services and API routing</li>
<li>Tailscale network connecting everything</li>
</ul>
<p><strong>Performance After 1+ Year:</strong></p>
<ul>
<li>83,418 total OCR requests processed</li>
<li>48GB of image data handled</li>
<li>1000+ requests on busy days</li>
<li>76% battery health after continuous operation</li>
<li>$84-120 CAD annual electricity savings</li>
</ul>
<p><strong>Key Learnings:</strong></p>
<ul>
<li>Apple Vision framework rivals cloud services for accuracy</li>
<li>Old hardware is surprisingly reliable for server workloads</li>
<li>Solar power works well with proper battery management</li>
<li>Local processing beats cloud services for privacy and cost at scale</li>
</ul>
<h2 id="why-would-you-even-do-this">Why Would You Even Do This?</h2>
<h3 id="the-logical-approach">The Logical Approach</h3>
<p>I have an image-heavy personal project that chews through hundreds of images daily, categorizing them automatically. Any reasonable person would run the OCR processing on their Mac - Apple’s Vision framework works great on macOS.</p>
<h3 id="the-me-approach">The “Me” Approach</h3>
<p>But I’m not reasonable. I see a perfectly good iPhone 8 and think: <em>“You know what this needs? A second career as a solar-powered image processing servant."</em> My EcoFlow River 2 Pro was sitting idle between camping trips, so switching my existing OCR server to solar felt like the natural evolution.</p>
<h3 id="the-unexpected-benefits">The Unexpected Benefits</h3>
<ul>
<li><strong>Real-time dashboard</strong> on my window sill while bird watching</li>
<li><strong>Grid independence</strong> for personal projects</li>
<li><strong>Actual cost savings</strong> that add up over time</li>
<li><strong>Amazing conversation starter</strong> when people visit</li>
</ul>
<p>The financial benefits are modest but real. Looking at my actual power consumption data: 37.4 kWh in May ($7.21) and 45.8 kWh in April ($8.82). Over a year, that’s meaningful savings.</p>
<p><em>“Is it practical? Debatable. Is it cool? Absolutely."</em></p>
<h2 id="what-exactly-is-this-thing">What Exactly Is This Thing?</h2>
<p>Here’s my delightfully over-engineered setup:</p>
<ol>
<li><strong>Mini PC</strong> running my web server, image processing service, Plex server, and various other services</li>
<li><strong>iPhone 8</strong> perched on my window sill, running a SwiftUI app that serves as both OCR processor and real-time dashboard</li>
<li><strong>EcoFlow power station</strong> keeping both devices running off-grid</li>
<li><strong>Tailscale network</strong> connecting everything seamlessly</li>
</ol>
<p>The workflow is beautifully simple: My image processing service sends images to the phone for OCR processing using Apple’s Vision framework. The phone processes the text, sends it back, and updates its dashboard with processing stats. All while I watch birds outside my window and feel smug about my setup.</p>
<h2 id="the-hardware-setup-solar-meets-computing">The Hardware Setup: Solar Meets Computing</h2>
<h3 id="power-station-choice-and-reality-check">Power Station Choice and Reality Check</h3>
<p>I didn’t buy the EcoFlow River 2 Pro specifically for this project. I bought it because I convinced myself I was going to become one of those outdoorsy people who camps and needs portable power. Well, turns out I’m still more of an “indoor cat with outdoor aspirations” kind of person. But my impulse purchase isn’t gathering dust!</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/ecoflow-river-pro-2.jpg" alt="EcoFlow River 2 Pro portable power station">
</p>
<p><em>Pro tip: When researching portable power stations, <a href="https://gearscouts.com/">GearScouts.com</a> is an excellent price comparison site that could save you some time.</em></p>
<h3 id="power-consumption-and-solar-performance">Power Consumption and Solar Performance</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Idle Power</th>
<th>Processing Load</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>iPhone 8 OCR Server</td>
<td>0.5-1W</td>
<td>2-5W</td>
<td>Surprisingly efficient</td>
</tr>
<tr>
<td>Mini PC (multiple services)</td>
<td>15W</td>
<td>25-30W</td>
<td>Includes Plex, Archive Warriors</td>
</tr>
<tr>
<td><strong>Total Daily Consumption</strong></td>
<td><strong>~1.2kWh</strong></td>
<td><strong>Variable</strong></td>
<td>Based on actual TP-Link data</td>
</tr>
</tbody>
</table>
<p><strong>Solar Performance by Season:</strong></p>
<ul>
<li><strong>Summer</strong>: 150-220W peak input, infinite runtime with battery charging</li>
<li><strong>Fall/Spring</strong>: 20-60W average, hybrid solar/battery operation</li>
<li><strong>Winter</strong>: 5-20W if lucky, mostly battery power (15-20 hours runtime)</li>
</ul>
<p>The River 2 Pro’s 768Wh capacity provides excellent buffer for Canada’s unpredictable weather. Its battery management system deserves credit - it’s not just dumping power into devices; it’s managing charging curves properly.</p>
<h2 id="building-the-ios-ocr-server-app">Building the iOS OCR Server App</h2>
<p>Creating a server on iOS sounds complicated, but Apple’s done most of the heavy lifting. The real challenge was making it run continuously without iOS deciding my app wasn’t important enough to keep running.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-ocr-server.jpeg" alt="iPhone 8 on window sill serving as OCR server">
</p>
<h3 id="apples-vision-framework-the-unsung-hero">Apple’s Vision Framework: The Unsung Hero</h3>
<p>Apple’s Vision framework is genuinely impressive and criminally underused. While everyone obsesses over ChatGPT and cloud-based OCR services, Apple quietly shipped a local OCR solution that’s fast, accurate, and runs entirely on-device.</p>
<p>Here’s the core processing code:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>import</span> <span>Vision</span>
<span>import</span> <span>UIKit</span>

<span>func</span> <span>processImage</span>(<span>_</span> image: UIImage, completion: @escaping (String?) -&gt; Void) {
    <span>guard</span> <span>let</span> cgImage = image.cgImage <span>else</span> {
        completion(<span>nil</span>)
        <span>return</span>
    }

    <span>let</span> request = VNRecognizeTextRequest { request, error <span>in</span>
        <span>guard</span> <span>let</span> observations = request.results <span>as</span>? [VNRecognizedTextObservation] <span>else</span> {
            completion(<span>nil</span>)
            <span>return</span>
        }

        <span>let</span> recognizedText = observations.compactMap { observation <span>in</span>
            observation.topCandidates(<span>1</span>).first?.string
        }.joined(separator: <span>"</span><span>\n</span><span>"</span>)

        completion(recognizedText)
    }

    request.recognitionLevel = .accurate
    request.usesLanguageCorrection = <span>true</span>

    <span>let</span> handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    <span>try</span>? handler.perform([request])
}
</code></pre></div><p>The accuracy rivals some cloud services I’ve tested, and it’s processing everything locally. No API calls, no usage limits, no privacy concerns.</p>
<h3 id="swiftui-dashboard-and-analytics">SwiftUI Dashboard and Analytics</h3>
<p>The dashboard was the fun part - something that would look cool on my window sill and provide real-time stats:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>struct</span> <span>DashboardView</span>: View {
    @StateObject <span>private</span> <span>var</span> server = OCRServer()
    @State <span>private</span> <span>var</span> stats = ProcessingStats()

    <span>var</span> body: some View {
        VStack(spacing: <span>20</span>) {
            Text(<span>"OCR Server Status"</span>)
                .font(.title)
                .fontWeight(.bold)

            HStack {
                StatCard(title: <span>"Requests Today"</span>, value: <span>"</span><span>\(</span>stats.requestsToday<span>)</span><span>"</span>)
                StatCard(title: <span>"Total Processed"</span>, value: <span>"</span><span>\(</span>stats.totalProcessed<span>)</span><span>"</span>)
            }

            HStack {
                StatCard(title: <span>"Avg Processing Time"</span>, value: <span>"</span><span>\(</span>stats.avgProcessingTime<span>)</span><span>ms"</span>)
                StatCard(title: <span>"Success Rate"</span>, value: <span>"</span><span>\(</span>stats.successRate<span>)</span><span>%"</span>)
            }

            BatteryView(percentage: UIDevice.current.batteryLevel)

            Text(<span>"Server running on port 8080"</span>)
                .font(.caption)
                .foregroundColor(.secondary)
        }
        .padding()
    }
}
</code></pre></div><p>I integrated Google Analytics 4 because I’m a data nerd. The dashboard shows 139,917 total users with 17,643 this month, 6:28 average session duration, and 11 currently active users. It’s like having a tiny data center dashboard on your window sill.</p>
<h2 id="the-solar-power-challenge">The Solar Power Challenge</h2>
<p>Running electronics on solar power sounds simple until you face Canada’s weather reality. We get everything from blazing summer days (all 3 of them) to months of overcast skies that make solar panels about as useful as a chocolate teapot.</p>
<h3 id="seasonal-strategy-and-battery-management">Seasonal Strategy and Battery Management</h3>
<p>I’ve developed a weather-dependent approach:</p>
<ul>
<li><strong>Summer</strong>: Solar handles everything plus charges other devices</li>
<li><strong>Fall/Spring</strong>: Hybrid solar/battery with careful monitoring</li>
<li><strong>Winter</strong>: Mostly battery power with occasional solar boosts</li>
</ul>
<p>The phone’s battery health held up reasonably well. After over a year of constant operation, it’s at 76% capacity. The power station’s battery management deserves credit here.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-battery-health-ocr-server.jpeg" alt="iPhone 8 Battery Health After OCR Server Usage">
</p>
<p>One unexpected discovery: the phone performs OCR faster when slightly warm (but not hot). Cold Canadian mornings mean slower processing times - something I never would have noticed with wall power.</p>
<h2 id="cost-analysis-solar-vs-grid-power">Cost Analysis: Solar vs Grid Power</h2>
<h3 id="investment-and-operating-costs">Investment and Operating Costs</h3>
<p><strong>Initial Investment:</strong></p>
<ul>
<li>EcoFlow River 2 Pro: $599 CAD (bought for camping anyway)</li>
<li>220W Solar Panel: $180 CAD</li>
<li>Cables, mounting hardware: ~$50 CAD</li>
<li><strong>Additional solar investment</strong>: ~$230 CAD</li>
</ul>
<p><strong>Monthly Savings:</strong>
Based on actual EcoFlow data showing 37.4-45.8 kWh monthly consumption, I’m saving approximately $84-120 CAD annually. The payback period is about 2-3 years.</p>
<h3 id="comparison-with-cloud-ocr-services">Comparison with Cloud OCR Services</h3>
<p>Cloud OCR services typically charge $1.00-1.50 per 1,000 requests. With over 83,000 requests processed, cloud services would have cost me $83-125 CAD, plus privacy concerns about sending images to external servers.</p>
<p>My solar setup? Zero per-request costs and complete privacy.</p>
<h2 id="what-i-learned-after-a-year">What I Learned After a Year</h2>
<h3 id="reliability-surprises">Reliability Surprises</h3>
<p><strong>The hardware is incredibly reliable.</strong> This phone has been running continuously for over a year, and it just keeps going. Performance hasn’t degraded noticeably despite the constant workload.</p>
<p><strong>iOS background processing works better than expected</strong> - once you figure out the right approach. The key is using background app refresh properly and keeping the HTTP server active with regular requests.</p>
<p><strong>Apple’s Vision framework improves over time.</strong> Text recognition that used to fail now works perfectly, especially with handwritten text and unusual fonts.</p>
<h3 id="common-problems-and-solutions">Common Problems and Solutions</h3>
<p><strong>Solar Power Intermittency:</strong> I configured the power station to prioritize the phone (lower power draw) and gracefully shut down the Mini PC when battery gets low. The phone can handle basic OCR requests solo for several hours if needed.</p>
<p><strong>Heat Management:</strong> Direct sunlight plus continuous processing equals thermal throttling. I added shade, improved airflow, and implemented smart processing that reduces requests when the phone reports high temperature.</p>
<p><strong>iOS Background Limitations:</strong> iOS really doesn’t want apps running forever. I use background app refresh, minimal location services, and keep the HTTP server responding to requests. It’s a delicate balance between staying alive and preserving battery.</p>
<h2 id="why-this-actually-matters">Why This Actually Matters</h2>
<p>Beyond the obvious coolness factor, this project demonstrates several important principles:</p>
<p><strong>Privacy First:</strong> Every image stays on my devices. No cloud uploads, no third-party access. In an era where everything gets sent to someone else’s computer, truly local processing feels revolutionary.</p>
<p><strong>Energy Independence:</strong> While the savings aren’t life-changing, the principle matters. This proves meaningful computing workloads can run entirely on renewable energy, even in challenging climates.</p>
<p><strong>E-Waste Reduction:</strong> That phone was destined for a drawer. Now it’s a productive member of my tech ecosystem. How many old devices could be repurposed instead of becoming electronic waste?</p>
<p><strong>Local-First Computing:</strong> Not everything needs to be in the cloud. Sometimes the best solution is sitting right in front of you, powered by the sun, processing your data locally and privately.</p>
<p>The setup has become my go-to demonstration for visitors interested in renewable energy or local computing. Plus, I genuinely love glancing at my window sill and seeing real-time processing stats while watching birds at my feeders.</p>
<h2 id="resources-and-next-steps">Resources and Next Steps</h2>
<p>If you’re interested in building something similar:</p>
<h3 id="hardware">Hardware</h3>
<ul>
<li><a href="https://us.ecoflow.com/products/river-2-pro-portable-power-station">EcoFlow River 2 Pro</a> - The power station I use</li>
<li><a href="https://www.renogy.com/100-watt-12-volt-monocrystalline-solar-panel/">Renogy 100W Solar Panel</a> - Similar to my setup</li>
<li>Any iPhone 8 or newer with iOS 13+ for Vision framework support</li>
</ul>
<h3 id="software-resources">Software Resources</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision">Apple Vision Framework Documentation</a> - Official OCR implementation docs</li>
<li><a href="https://developer.apple.com/documentation/backgroundtasks">Background App Refresh Guide</a> - Keeping iOS apps alive</li>
<li><a href="https://github.com/Building42/Telegraph">SwiftUI HTTP Server Examples</a> - HTTP server implementation</li>
</ul>
<h3 id="power-management-tools">Power Management Tools</h3>
<ul>
<li><a href="https://www.kasasmart.com/">TP-Link Kasa Smart Plugs</a> - For monitoring actual power consumption</li>
<li>EcoFlow app - Built-in monitoring for the River 2 Pro</li>
<li><a href="https://gearscouts.com/">GearScouts.com</a> - Price comparison for power stations and outdoor gear</li>
</ul>
<p><em>This article was last updated while watching my solar-powered setup process its 83,418th OCR request, powered entirely by Canadian sunshine.</em></p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Laptop 12 review (220 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</link>
            <guid>44310583</guid>
            <pubDate>Wed, 18 Jun 2025 15:09:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/">https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</a>, See on <a href="https://news.ycombinator.com/item?id=44310583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2101033">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    how much would you pay for personality?
  </span>
</p>
    </div>

    

    <p>
      A sturdy, thoughtful, cute design that just can't compete in its price range.
    </p>

    

    <div>
            <p><a data-pswp-width="2560" data-pswp-height="1440" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" target="_blank">
              <img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" sizes="(max-width: 2560px) 100vw, 2560px">
            </a></p><div id="caption-2101680">
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>"What's this purple laptop? It's cool."</p>
<p>Over a decade-plus of doing gadget reviews and review-adjacent things, my wife (and, lately, my 5-year-old) have mostly stopped commenting on the ever-shifting selection of laptops I have in my bag or lying around the house at any given time. Maybe she can't tell them apart, or maybe she just figures there isn't that much to say about whatever black or silver metal slab I'm carrying around. Either way, they practically never elicit any kind of response, unless there are just too many of them sitting out in too many places.</p>
<p>But she&nbsp;<em>did</em> ask about the Framework Laptop 12, the third and latest major design in Framework's slowly expanding lineup of modular, repairable, upgradeable laptops. With its five two-toned color options and sturdy plastic exterior, it's definitely more approachable and friendly-looking than the Laptop 13 or Laptop 16, both metal slabs with a somewhat less-finished and prototype-y look to them. But it retains the features that a certain kind of PC geek likes about Framework's other laptops—user-customizable and swappable ports, an easy-to-open design, first-class Linux support, and the promise of future upgrades that improve its performance and other specs.</p>
<h2>Look and feel</h2>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12 stacked atop the Laptop 13.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Plastic gets a bad rap, and there are indeed many subpar plastic gadgets out there. When done poorly, plastic can look and feel cheap, resulting in less durable devices that show more wear over time.</p>
<p>But well-done plastic can still feel solid and high-quality, in addition to being easier to make in different colors. Framework says the Laptop 12's chassis is a combination of ABS plastic and <a href="https://en.wikipedia.org/wiki/Thermoplastic_polyurethane">TPU plastic</a> (a more flexible, rubberized material), molded over a metal inner structure. The result is something that can probably actually take the shock of a drop or a fall better than many aluminum-and-glass laptops without feeling overly cheap or chintzy.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>The five two-tone color options—the boring, businesslike black and gray, plus purple-and-gray lavender, pink-and-baby-blue bubblegum, and the green sage options—are the most fun thing about it, and the lavender and bubblegum colors are particularly eye-catching.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Keyboard and trackpad. Only the lavender and gray laptops get a color-matched trackpad; the keyboard and deck are always different shades of gray.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Matching other components to the exterior of the system can be a bit of a crapshoot, though. The screwdriver and spudger that Framework provides for upgrading and repairing all of its systems <em>does</em> match the color of the laptop, and the two-tone styluses for the touchscreens will also match the laptops when they're made available for purchase in the coming months.</p>
<p>The lavender option is the only one that can also be configured with a color-matched lavender trackpad—the only other trackpad option is gray, and the keyboard deck and the keyboard itself are all gray no matter what color laptop you pick. This is presumably meant to limit the number of different trackpad options that Framework has to manufacture and stock, but it is too bad that the laptop's keyboard and palm rest aren't as colorful as the rest of it.</p>
<p>The Laptop 12 also uses Framework's still-unique Expansion Card system for customizing the built-in ports. These are all 10 Gbps USB 3.2 Gen 2 ports rather than the Thunderbolt ports on the Intel versions of the Laptop 13, but all four support the same speeds, all four support charging, and all four support display output, so you really can put whatever port you want wherever you want it.</p>
<p>A downside of the Laptop 12 is that, as of this writing, only the USB-C Expansion Modules are available in color-matched versions. If you want USB-A, HDMI, DisplayPort, or any other kind of port on your system, you'll get the silver modules that were designed to match the finish on the Framework Laptops 13 and 16, so you'll have to put up with at least one mismatched port on your otherwise adorable system.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Only the USB-C Expansion Cards are available in lavender, which can make for goofy-looking mismatches. But I do prefer the Framework 16-style retention switches to the Framework Laptop 13's retention buttons, which you need to hold down as you pull out the Expansion Card.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Once you get past the adorable design, the Expansion Modules, and the sturdy construction, the system's downsides start to become more apparent. The 12.2-inch, 1920×1200 touchscreen gets plenty bright and has a respectable contrast ratio (440 nits and 1,775:1 in our testing, respectively). But it's surrounded by thick black bezels on all sides, particularly on the bottom—it does seem that either a larger screen or a slightly smaller laptop design would be possible if so much space weren't wasted by these thick borders.</p>
<p>The display has good viewing angles but a distinctly mediocre color gamut, covering around 60 percent of the SRGB color space (compared to the high 90s for the Laptop 13 and most midrange to high-end IPS screens in other laptops). This is low enough that most colors appear slightly muted and washed out—reds most noticeably, though greens aren't much better. You definitely don't need a colorimeter to see the difference here.</p>
<p>Framework's color-matched stylus isn't ready yet, but you won't need to wait for one if you want to use a pen with this touchscreen. Both the Universal Stylus Initiative (USI) 2.0 and Microsoft Pen Protocol (MPP) 2.0 specs are supported, so the Surface Pen, a bunch of Lenovo styluses, and any number of inexpensive third-party Amazon styluses will all work just fine. That said, the screen can only support one of those stylus specs at a time—MPP is on by default, and you can swap between them in the BIOS settings.</p>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The webcam and mic have locks to disable them so that the OS can't see or use them.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The keyboard feels mostly fine, with good key spacing and a nice amount of travel. I noticed that I was occasionally missing letters the first couple of days I used the laptop—I was pressing the keys, but they intermittently didn't register. That got better as I adjusted to the system. The trackpad is also unremarkable in a good way. Finger tracking and multi-touch gestures all worked as intended.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But the keyboard lacks a backlight, and it doesn't have the fingerprint sensor you get with the Laptop 13. With no fingerprint sensor and no IR webcam, there are no biometric authentication options available for use with Windows Hello, so you'll either need a PIN or a password to unlock your laptop every time you want to use it. Either omission would be sort of annoying in a laptop in this price range (we complained about the lack of keyboard backlight in <a href="https://arstechnica.com/gadgets/2022/07/review-microsofts-surface-laptop-go-2-has-a-lot-of-problems-but-i-like-it-anyway/">the $700 Surface Laptop Go 2</a> a few years ago), but to be missing&nbsp;<em>both</em> is particularly frustrating in a modern system that costs this much.</p>
<h2>Repairs and upgrades</h2>



<p>We've been inside the Framework Laptop 13 enough times that we don't do deep dives into its insides anymore, but as a new (and, in some ways, more refined) design, the Laptop 12 warrants a closer look this time around.</p>
<p>Framework's pack-in Torx screwdriver is still the only tool you need to work on the Laptop 12. Undo the eight captive screws on the bottom of the laptop, and you'll be able to lift away the entire keyboard and trackpad area to expose all of the other internal components, including the RAM, SSD, battery, and the motherboard itself.</p>
<p>The motherboard is quite a bit smaller than the Framework Laptop 13 board, and the two are definitely&nbsp;<em>not</em> interchangeable. Framework has never said otherwise, but it's worth highlighting that these are two totally separate models that will have their own distinct components and upgrade paths—that goes for parts like the speakers and battery, too.</p>

<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Laptop 12 motherboard on top, Laptop 13 motherboard on bottom.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>As a result of that reduction in board space, the Laptop 12 can only fit a single DDR5 RAM slot, which reduces memory bandwidth and limits your RAM capacity to 48GB. It also uses shorter M.2 2230 SSDs, like the Surface lineup or the Steam Deck. Unlike a few years ago, these SSDs are now readily available at retail, and it's also easy to buy warranty-less ones on eBay or elsewhere that have been pulled from OEM systems. But they're still a bit more expensive than the more common M.2 2280 size, and you have fewer options overall.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has already published <a href="https://guides.frame.work/Guide/Framework+Laptop+12+(13th+Gen+Intel%C2%AE+Core%E2%84%A2)+DIY+Edition+Quick+Start+Guide/429?_gl=1*1dc9fwi*sg_ga4w_production_ga*NTEzNDU2MDg1LjE3NTAxODA0Mzg.*sg_ga4w_production_ga_PYG8X65YJJ*czE3NTAxODA0MzckbzEkZzEkdDE3NTAxODI3NDckajYwJGwwJGgw">a guide on setting up the DIY Edition of the laptop</a> and&nbsp;<a href="https://guides.frame.work/c/Framework_Laptop_12">a few repair guides</a> for common components. Guides for replacing bigger or more co parts, like the display or the webcam, are still listed as "coming soon."</p>
<h2>Performance and battery life</h2>
<p>I could politely describe the Laptop 12's 2.5-year-old 13th-gen Intel Core processor as "mature." This generation of Intel chips <em>has&nbsp;</em>stuck around for a lot longer than usual, to the point that Intel recently acknowledged that it has been dealing with shortages. They're appealing to PC companies because they still offer decent everyday performance for basic computing without the additional costs imposed by things like on-package memory or having some or all of the chip manufactured outside of Intel's own factories.</p>
<p>The upside of a slightly older processor is a more stable computing experience, in both Windows and Linux, since the companies and communities involved have had more time to add support and work out bugs; I had none of the sleep-and-wake issues or occasional video driver crashes I had while testing the Ryzen AI 300 version of the Framework Laptop 13.</p>


<p>The downside, of course, is that performance is pretty unexciting. These low-power U-series 12th- and 13th-gen Intel chips remain capable when it comes to day-to-day computing, but they fall far behind the likes of Intel and AMD's newer chips, Qualcomm's Snapdragon chips from the Microsoft Surface and other Copilot+ PCs, or the Apple M4 in the MacBook Air.</p>


<p>And while none of these chips are really intended for gaming laptops, the Laptop 12 isn't even a great fit for that kind of casual Steam Deck-y 3D gaming that most Framework Laptop 13 models can handle. Technically, this is the same basic Intel Iris Xe GPU that the first few generations of Framework Laptop 13 used, which is not exciting as integrated GPUs go but is at least still minimally capable. But because the Laptop 12 only has a single RAM slot instead of two, memory bandwidth is halved, which makes the GPU identify itself as "Intel UHD Graphics" to the device manager and drags down performance accordingly. (This is something these GPUs have always done, but they usually ship in systems that either have two RAM slots or soldered-down memory, so it usually doesn't come up.)</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has tuned these chips to consume the same amount of power in both the "Balanced" and "Best Performance" power modes in Windows, with a 15 W sustained power limit and a 40 W limit for shorter, bursty workloads. This keeps the laptop feeling nice and responsive for day-to-day use and helps keep a lid on power usage for battery life reasons, but it also limits its performance for extended CPU-intensive workloads like our Handbrake video encoding test.</p>

<p>The Laptop 12 takes a&nbsp;<em>lot</em> longer to accomplish these tasks than some other laptops we've tested with similar chips, either because of the lower memory bandwidth or because Best Performance mode doesn't let the chip consume a bunch of extra power. I'm not inclined to complain too much about this because it's not the kind of thing you really buy an ultraportable laptop to do, but as with light gaming, it's worth noting that the Laptop 12 doesn't hit that same "usable for these workloads in a pinch" balance that the Laptop 13 does.</p>
<figure>
    <p><img width="2048" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-640x480.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1024x768.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-768x576.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1536x1152.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-980x735.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1440x1080.png 1440w" sizes="auto, (max-width: 2048px) 100vw, 2048px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12's battery life is decent relative to most Laptop 13s.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Core i5 version of the Laptop 12 lasted around 10 hours in the PCMark Modern Office battery life test, which isn't stunning but is a step up from what the fully specced versions of the Framework Laptop 13 can offer. It will be just fine for a long flight or a full day of work or school. Our Framework reviews often complain about battery life, but I don't think it will be an issue here for most users.</p>
<h2>About that price</h2>
<p>In some ways, the Laptop 12 is trying to be a fundamentally&nbsp;<em>different</em> laptop from the Laptop 13. For all the Laptop 13's upgrades over the years, it has never had a touchscreen option, stylus support, or a convertible hinge.</p>
<p>But in most of the ways that count, the Laptop 12 is meant to be an "entry-level, lower-cost laptop," which is how Framework CEO Nirav Patel <a href="https://www.youtube.com/watch?v=Ejl-7X74tgc&amp;t=171s">has positioned it</a> in the company's announcement blog posts and videos. It features a slightly smaller, lower-resolution, less colorful screen with a lower refresh rate; a non-backlit keyboard; and considerably weaker processors. It also lacks both a fingerprint reader and a face-scanning webcam for Windows Hello.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The issue is that these cost-cutting compromises come at a price that's a bit outside of what you'd expect of a "budget" laptop.</p>
<p>The DIY Edition of the Laptop 12 we're evaluating here—a version that ships with the Windows license and all the components you need but which you assemble yourself—will run you at least $1,176, depending on the Expansion Modules you choose for your ports. That includes 16GB of GDDR5 RAM and a 1TB M.2 2230 SSD, plus the Core i5-1334U processor option (2 P-cores, 8 E-cores). If you stepped down to a 500GB SSD instead, that's still $1,116. A pre-built edition—only available in black, but with identical specifications—would run you $1,049.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 13 compared to the Laptop 12. The Laptop 12 is missing quite a few quality-of-life things and has worse performance, but it isn't all that much cheaper.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>This puts the Framework Laptop 12 in the same general price range as Apple's MacBook Air, Microsoft's 13-inch Surface Laptop, and even many editions of the Framework Laptop 13. And the Laptop 12 is charming, but its day-to-day user experience falls well short of any of those devices.</p>
<p>You can make it cheaper! Say you go for the Core i3-1315U version (two P-cores, four E-cores) instead, and you buy your own 16GB stick of DDR5 RAM (roughly $50 instead of $80) and 1TB SSD ($70 or $80 for <a href="https://www.newegg.com/silicon-power-1tb/p/0D9-0021-00171?Item=9SIBDGPK454247">a decent one</a>, instead of $159). Say you have plenty of USB-C chargers at home so you don't need to pay $55 for Framework's version, and say you run Linux or ChromeOS, or you already have a Windows 11 product key, or you've brought your own Windows 11 key from one of those gray-market key selling sites (as little as $10).</p>
<p>Now we're talking about a PC that's a little under $700, which is closer to "reasonable" for a brand-new touchscreen PC. But the laptop's old CPU and poky performance also mean it's competing with a wide swath of refurbished, used, and closeout-priced older PCs from other manufacturers.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>In December, for example, I bought an SSD-less <a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadl/thinkpad-l13-yoga-gen-3-13-inch-intel/len101t0032?orgRef=https%253A%252F%252Fwww.google.com%252F&amp;srsltid=AfmBOoqjpN9S8iIG7xfG4vGA-Dv7fPWAoBj6sd6Y9oDZXN_KBVmVbRiT">Lenovo ThinkPad L13 Yoga Gen 3</a> from eBay for around $300, with around a year left on its warranty. After I'd added an SSD and reinstalled Windows—no additional cost because it had a valid Windows license already—I ended up with a PC with the same screen resolution and similar specs but with a better-quality display with smaller bezels that made the screen larger without making the laptop larger; a faster GPU configuration; a backlit keyboard; and a fingerprint reader.</p>
<p>I know it's not possible for everyone to just go out and buy a laptop like this. The boring black outline of a midrange ThinkPad is also the polar opposite of the Framework Laptop 12, but it's an example of what the tech-savvy buyer can find in the secondhand market if you're trying to find a cost-effective alternative to what Framework is offering here.</p>

<h2>A good laptop, but not a good value</h2>
<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Framework Laptop 12.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>There are plenty of factors beyond Framework’s control that contribute to the Laptop 12’s price, starting with on-again-off-again global trade wars and the uncertainty that comes with them. There's also Framework’s status as a niche independent PC company rather than a high-volume behemoth. When you ship the number of computers that Apple does, it’s almost certainly easier to make a $999 laptop that is both premium and profitable.</p>
<p>But whatever the reason, I can’t escape the feeling that the Laptop 12 was meant to be cheaper than it has ended up being. The result is a computer with many of the compromises of an entry-level system, but without a matching entry-level price tag. It’s hard to put a price on some of the less-tangible benefits of a Framework laptop, like ease of repairs and the promise of future upgrades, but my gut feeling is that the Framework Laptop 13 falls on the “right” side of that line, and the Laptop 12 doesn't.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>I am charmed by the Laptop 12. It's cute and functional, and it stands out among high-end aluminum slabs. It adds some subtle refinement to elements of the original Framework Laptop 13 design, including some things I hope end up making it into some future iteration of its design—softer corners, more color options, and an easier-to-install keyboard and trackpad. And it's far from a <em>bad</em> performer for day-to-day desktop use; it's just that the old, poky processor limits its capabilities compared to other PCs that don't cost that much more than it does.</p>
<p>I probably wouldn't recommend this over the Laptop 13 for anyone interested in what Framework is doing, unless a touchscreen is a make-or-break feature, and even then, I'd encourage people to take a good, long look at Microsoft, Lenovo, Dell, or HP's convertible offerings first. But I hope that Framework does what it's done for the Laptop 13 over the last four or so years: introduce updated components, iterate on different elements of the design, and gradually bring the price down into a more reasonable range through refurbished and factory-second parts. As a $1,000-ish computer, this leaves a lot to be desired. But as the foundation for a new Framework platform, it has enough promise to be interesting.</p>
<h3>The good</h3>
<ul>
<li>Eye-catching, colorful, friendly design that stands out among metal slabs.</li>
<li>Simple to build, repair, and upgrade.</li>
<li>Dual-plastic design over a metal frame is good for durability.</li>
<li>First convertible touchscreen in the Framework laptop.</li>
<li>Customizable ports.</li>
<li>Decent performance for everyday computing.</li>
<li>Respectable battery life.</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Old, slow chip isn't really suitable for light gaming or heavy productivity work that the larger Framework Laptop 13 can do.</li>
<li>Pre-built laptop only comes in boring black.</li>
<li>Mediocre colors and large bezels spoil the screen.</li>
<li>Keyboard sometimes felt like it was missing keystrokes until I had adjusted to compensate.</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>It's just too expensive for what it is. It looks and feels like a lower-cost laptop, but without a dramatically lower price than the nicer, faster Framework 13.</li>
</ul>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/andrew_cunningham/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/a.cunningham-45-1.jpg" alt="Photo of Andrew Cunningham"></a></p>
  </div>

  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/#comments" title="30 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    30 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/tech-policy/2025/06/trump-org-launches-47-month-wireless-service-teases-odd-499-phone/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2211177567-768x432.jpg" alt="Listing image for first story in Most Read: Mocked Trump Mobile yanks coverage map that ignored Trump renaming Gulf of Mexico" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Homomorphically Encrypting CRDTs (205 pts)]]></title>
            <link>https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</link>
            <guid>44309520</guid>
            <pubDate>Wed, 18 Jun 2025 12:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakelazaroff.com/words/homomorphically-encrypted-crdts/">https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</a>, See on <a href="https://news.ycombinator.com/item?id=44309520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content="" data-astro-cid-onuac4el=""> 



<p>Here’s a problem with local-first software.</p>
<p>You want to work on a document together with a friend who lives far away from you.
That sounds like local-first’s bread and butter: store the document as a CRDT, then use some sort of sync server to merge updates and relay them between you and your friend.</p>
<p>But there’s a catch: the contents of that document are secret.
So secret, in fact, that <em>you don’t even want the app developer to know what they are</em>.</p>
<p>One way to solve this is end-to-end encryption.
You and your friend agree on a secret key, known only to each other.
You each use that key to encrypt your changes before sending them, decrypt them upon receipt, and no one in the middle is able to listen in.
Because the document is a CRDT, you can each still get the latest document without the sync server merging the updates.</p>
<p>That is indeed a solution, and modern browser APIs make it fairly simple to implement a basic version of it. <a href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f="">Excalidraw’s writeup of their implementation</a><a data-tooltip="" href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f=""> <img src="https://excalidraw.nyc3.cdn.digitaloceanspaces.com/lp-cms/media/Excalidraw%20blog%20-%20End-to-End%20Encryption%20in%20the%20Browser-1.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Excalidraw Blog | End-to-End Encryption in the Browser</span> <span data-astro-cid-bi7aps5f="">Excalidraw introduces browser-based end-to-end encryption using Web Cryptography APIs for secure, private drawing storage.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://plus.excalidraw.com/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">plus.excalidraw.com/blog/end-to-end-encryption</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> is only about 750 words — including code samples!<sup><a href="#user-content-fn-e2ee" id="user-content-fnref-e2ee" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">1</a></sup></p>
<p>Unfortunately, we’ve introduced a new problem.</p>
<p>You and your friend live far away from each other, so you tend to work while they’re sleeping and vice versa.
That was fine when the sync server could merge your changes and send you the latest document when you opened it.</p>
<p>Now, however, the server can no longer understand the changes you send.
If you want to see your friend’s latest changes, you’ll need to both be online at the same time.</p>
<p>Enter <strong>homomorphic encryption</strong>: a special form of encryption that allows a computer to <em>run programs on encrypted data without decrypting it</em>.
Using a homomorphically encrypted CRDT, a sync server could merge your friend’s and your changes into one document without ever knowing what the document contains.<sup><a href="#user-content-fn-otherways" id="user-content-fnref-otherways" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">2</a></sup></p>
<p>In this article, we’ll explore how homomorphic encryption works and build a homomorphically encrypted last write wins register CRDT.
We’ll also learn about some fundamental limitations of homomorphic encryption, and how they affect local-first software specifically.</p>
<p>I try to assume as little knowledge as possible about both encryption and CRDTs.
If you want to brush up before continuing on, my <a href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f="">Interactive Intro to CRDTs</a><a data-tooltip="" href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/og/an-interactive-intro-to-crdts.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">An Interactive Intro to CRDTs | jakelazaroff.com</span> <span data-astro-cid-bi7aps5f="">CRDTs don't have to be all academic papers and math jargon. Learn what CRDTs are and how they work through interactive visualizations and code samples.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">jakelazaroff.com/words/an-interactive-intro-to-crdts/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> and Jeremy Kun’s <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> are good places to start.</p>
<p>(Obligatory disclaimer: I am not a cryptographer!
While I’m reasonably confident that my code and advice here is generally sound, cryptography is a field in which subtle bugs and exploits can look fine to the untrained eye.
Before using anything here in an environment you’d describe with the word “production”, consult someone who works on this professionally.)</p>
<h2 id="homomorphic-hello-world">Homomorphic Hello World</h2>
<p>First, let’s look at a small code sample that uses homomorphic encryption.</p>
<p>Writing the encryption code itself from scratch would take much more code than can fit in this article.
Instead, we’ll use <a href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f="">THFE-rs</a><a data-tooltip="" href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f=""> <img src="https://opengraph.githubassets.com/0076171220c59d2546a04eccf3e34abc6618c1b595c0711a3ea1f2f2d96312ad/zama-ai/tfhe-rs" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">GitHub - zama-ai/tfhe-rs: TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.</span> <span data-astro-cid-bi7aps5f="">TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data. - zama-ai/tfhe-rs</span> <span data-astro-cid-bi7aps5f=""> <img src="https://github.githubassets.com/favicons/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">github.com/zama-ai/tfhe-rs</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, a homomorphic encryption library written in Rust.</p>
<p>The flow goes something like this:</p>
<ol>
<li>A client generates a key pair consisting of a client key and a server key.</li>
<li>The client encrypts their data using the client key and sends both the encrypted data and server key to the server.</li>
<li>The server uses the server key to perform some computation on the encrypted data and sends the result back to the client.</li>
<li>The client decrypts the result with the client key.</li>
</ol>
<p>Here’s what this looks like in code.
We’ll take two numbers — <code>clear_a</code> and <code>clear_b</code> — and add them together.
Rather than actually sending anything over a network, we’ll just use a function called <code>server_compute</code> to play the part of the server.</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span>generate_keys<span>,</span> set_server_key<span>,</span> <span>ConfigBuilder</span><span>,</span> <span>FheUint32</span><span>,</span> <span>ServerKey</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> config <span>=</span> <span>ConfigBuilder</span><span>::</span><span>default</span><span>(</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// generate client and server keys</span>
    <span>let</span> <span>(</span>client_key<span>,</span> server_key<span>)</span> <span>=</span> <span>generate_keys</span><span>(</span>config<span>)</span><span>;</span>

    <span>// generate plaintext</span>
    <span>let</span> clear_a<span>:</span> <span>u32</span> <span>=</span> <span>27</span><span>;</span>
    <span>let</span> clear_b<span>:</span> <span>u32</span> <span>=</span> <span>128</span><span>;</span>

    <span>// encrypt plaintext and "send to server"</span>
    <span>let</span> result <span>=</span> <span>server_compute</span><span>(</span>
        server_key<span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_a<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_b<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
    <span>)</span><span>;</span>

    <span>// decrypt the result</span>
    <span>let</span> decrypted_result<span>:</span> <span>u32</span> <span>=</span> result<span>.</span><span>decrypt</span><span>(</span><span>&amp;</span>client_key<span>)</span><span>;</span>

    <span>// assert that the result is what we expect</span>
    <span>assert_eq!</span><span>(</span>decrypted_result<span>,</span> clear_a <span>+</span> clear_b<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>server_compute</span><span>(</span>key<span>:</span> <span>ServerKey</span><span>,</span> cipher_a<span>:</span> <span>FheUint32</span><span>,</span> cipher_b<span>:</span> <span>FheUint32</span><span>)</span> <span>-&gt;</span> <span>FheUint32</span> <span>{</span>
    <span>set_server_key</span><span>(</span>key<span>)</span><span>;</span>
    <span>return</span> cipher_a <span>+</span> cipher_b<span>;</span>
<span>}</span>
</code></pre>
<p>Get the keys, encrypt two numbers, add their ciphertexts together, decrypt the result.
Not too bad, right?</p>
<p>The simplicity is deceptive!
Rust supports operator overloading, so when we run <code>cipher_a + cipher_b</code> and both of the operands are <code>FheUint32</code>, what’s <em>really</em> happening is that TFHE-rs runs a bunch of cryptography code.</p>
<p>Before we build our homomorphically encrypted CRDT, let’s peek at what TFHE-rs is doing under the hood.</p>
<h2 id="under-the-hood">Under the Hood</h2>
<p>To start, what does it even mean to “run programs on encrypted data”?</p>
<p>In short, it means you can use encrypted data in certain math operations, and when you decrypt the data you get the result you would have gotten if you had performed the same operations with the plaintext data.
That requires an encryption scheme in which at least one of the following is true (I’ll use the notation <code>E(a)</code> to indicate the encrypted version of the plaintext <code>a</code>):</p>
<ul>
<li><code>E(a) + E(b) = E(a + b)</code>: adding the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted sum of the plaintext sum <code>a + b</code>.</li>
<li><code>E(a) × E(b) = E(a × b)</code>: multiplying the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted product of the plaintext product <code>a × b</code>.</li>
</ul>
<p>What this means is that <strong>if you add or multiply two homomorphically encrypted values, then decrypt them, <em>you get the respective sum or product of the original plaintext values</em></strong>.</p>
<p>Here’s an extremely simple example that you should absolutely never use anywhere.
First, let’s pick a number as a key.
We “encrypt” numbers by multiplying them by the key, and “decrypt” numbers by dividing them.</p>
<p>Let’s say our key is 7 and our “plaintext” numbers are 5 and 6.
We can multiply each number by our key 6 to get “encrypted” numbers of 35 and 42.
Even if someone has access to our encrypted numbers, they can’t figure out what our original plaintext numbers were without the key.</p>
<p>What they <em>can</em> do is add the encrypted numbers together.
If they give us back the sum, 77, we can divide it by our key 7 to get 11 — <em>the same result we’d get by directly adding our original numbers</em>.
Try it out by changing the numbers in the playground below:</p>
<homomorphic-addition-demo></homomorphic-addition-demo>
<p>Because it satisfies the first criterion — <code>E(a) + E(b) = E(a + b)</code> — we can say that our toy encryption scheme is homomorphic over addition.
Encryption that supports only one operation is called <em>partially homomorphic encryption</em>.
All in all, there are four different levels:</p>
<ul>
<li><strong>Partially homomorphic encryption</strong> allows only one of the two operations: <em>either</em> addition <em>or</em> multiplication, but not both.</li>
<li><strong>Somewhat homomorphic encryption</strong> and <strong>leveled homomorphic encryption</strong> allow both operations, but limit the amount of times they can be used.</li>
<li><strong>Fully homomorphic encryption</strong> allows an unlimited amount of both operations.</li>
</ul>
<p>Partially homomorphic encryption is relatively easy to implement, but has limited uses.
The word “relatively” is doing some heavy lifting here — you or I probably couldn’t come up with a partially homomorphic encryption scheme — but it’s simple enough that there are algorithms such as RSA that are accidentally homomorphic over one operation.</p>
<p>Supporting <em>more than one</em> operation is significantly more useful, but each calculation adds “noise” to the result.
Too much noise makes it impossible to decrypt.
There are two broad strategies for reducing noise: limiting the number or “depth” of operations (<em>somewhat</em> and <em>leveled</em> homomorphic encryption), and “bootstrapping”, which reduces the level of noise mid-computation (<em>fully</em> homomorphic encryption).</p>
<p>Why does it matter whether we can perform <em>both</em> addition and multiplication?</p>
<p>When we talk about doing math on encrypted data, we’re really talking about the underlying bits: the 1s and 0s that make it up.
To add and multiply the bits, we use the logical operations “exclusive or” (XOR) and “binary and” (AND), respectively.</p>
<p>Click on the switches in the playground below to toggle between 1 and 0.
You can see that the AND output is the product of its two inputs, and the XOR output is roughly the sum of its two inputs.<sup><a href="#user-content-fn-addition" id="user-content-fnref-addition" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">3</a></sup></p>
<logic-gates height="180"><switch-gate id="one-and-left" x="40" y="10"></switch-gate><switch-gate id="one-and-right" x="85" y="10"></switch-gate><and-gate id="one-and" x="55" y="70" left="#one-and-left" right="#one-and-right"></and-gate><output-gate x="63" y="140" center="#one-and"></output-gate><switch-gate id="one-xor-left" x="190" y="10"></switch-gate><switch-gate id="one-xor-right" x="235" y="10"></switch-gate><xor-gate id="one-xor" x="205" y="70" left="#one-xor-left" right="#one-xor-right"></xor-gate><output-gate x="213" y="140" center="#one-xor"></output-gate></logic-gates>
<p>This is called a <em>Boolean circuit</em> — essentially, a function that takes 1s and 0s as input and returns 1s and 0s as output.
In this context, the logical operations are called <em>logic gates</em>.</p>
<p>We can create new logic gates by combining ones we have.
Here’s how to create “inclusive or” (OR) and inverter (NOT) operations using only XOR and AND.</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 310"><rect x="1" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="150" height="300"></foreignObject><rect x="156" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="150" y="0" width="150" height="300"></foreignObject></svg>
<p>Once we’ve built a gate, we can then use it to build <em>yet other</em> gates.
Here’s how to make an “exclusive nor” (XNOR) using XOR and our newly-constructed NOT gate:</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 260"><rect x="61" y="1" width="143" height="250" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="210" height="300"></foreignObject></svg>
<p>It turns out that combining just XOR and AND like this is enough to perform <em>any computation</em>!
All other logical operations can be created by combining only XOR and AND, which means that adding and multiplying the encrypted data is sufficient to simulate arbitrary Boolean logic.</p>
<p>Here’s a circuit that implements the “greater than” operator on two-bit numbers (between 0 and 3).
Using only AND, XOR and the other gates we’ve built with them, it returns 1 if the first number is greater than the second, and 0 otherwise.</p>
<p>Type in the square boxes at the top to enter the input numbers.
The two rounded boxes below each square input box are the <em>binary representation</em> of that number.</p>
<p>Don’t forget this circuit — it’ll come in handy later!</p>
<logic-gates height="365"><text-gate id="four-left" x="60"></text-gate><text-gate id="four-right" x="160"></text-gate><value-gate x="60" y="30" id="four-left-ms" center="#four-left" value-prop="leftValue"></value-gate><value-gate x="98" y="30" id="four-left-ls" center="#four-left" value-prop="rightValue"></value-gate><value-gate x="160" y="30" id="four-right-ms" center="#four-right" value-prop="leftValue"></value-gate><value-gate x="198" y="30" id="four-right-ls" center="#four-right" value-prop="rightValue"></value-gate><xnor-gate id="four-msbeq-1" x="25" y="110" left="#four-left-ms" right="#four-right-ms"></xnor-gate><not-gate id="four-msbgt-1" x="105" y="90" center="#four-right-ms"></not-gate><and-gate id="four-msbgt-2" x="105" y="150" left="#four-left-ms" right="#four-msbgt-1"></and-gate><not-gate id="four-lsbgt-1" x="200" y="90" center="#four-right-ls"></not-gate><and-gate id="four-lsbgt-2" x="200" y="150" left="#four-left-ls" right="#four-lsbgt-1"></and-gate><and-gate id="four-gt-1" x="140" y="220" left="#four-msbeq-1" right="#four-lsbgt-2"></and-gate><or-gate id="four-gt-2" x="112" y="280" left="#four-msbgt-2" right="#four-gt-1"></or-gate><output-gate x="120" y="330" center="#four-gt-2"></output-gate></logic-gates>
<p>In these examples, we’ve been looking at circuits that use plaintext 1s and 0s as their inputs and outputs.
With homomorphic encryption, the circuits operate on <em>encrypted data</em>.
Performing an AND on two encrypted bits returns another encrypted bit — and we can’t find out what it is unless we have the key.</p>
<p>So that’s how homomorphic encryption works in a nutshell.
You express your program as a Boolean circuit, and then simulate the circuit using the encrypted data as input.
The output of the circuit will be the encrypted result, which the client can then decrypt.</p>
<p>Crucially, <em>none of this reveals any sort of relationship between the plaintext values</em>.
For example, even if <code>E(a) + E(b)</code> were positive, <code>E(a + b)</code> might be negative.
Adding and multiplying ciphertext corresponds to the same operations on the underlying plaintext, but there’s no correlation between any of the ciphertext results and the underlying plaintext results — you need to decrypt the result to figure out what happened.</p>
<h2 id="a-fully-homomorphic-crdt">A Fully Homomorphic CRDT</h2>
<p>Now that we have a high level understanding of homomorphic encryption, let’s build a homomorphically-encrypted last write wins register.</p>
<p>A last write wins register holds a single value and two additional bits of metadata: a “clock” that gets incremented by one whenever the value is set, and an ID indicating the peer who last wrote to it.
Like all CRDTs, it also has a merge function that describes how it should be combined with another of the same type.</p>
<p>The last write wins register merge algorithm works like this:</p>
<ul>
<li>If the received clock is less than the local clock, the register doesn’t change its state.</li>
<li>If the received clock is greater than the local clock, the register overwrites its local value with the received value. It also stores the received clock and peer ID.</li>
<li>Ties are broken by comparing the local peer ID to the peer ID in the received state.</li>
</ul>
<p>Here’s a playground in which you can see how this algorithm works:</p>
<lwwregister-demo></lwwregister-demo>
<p>Try playing around with the latency and the network toggle.
See how updates are accepted only if the sending peer’s clock is higher than the receiving peer’s clock. If the clocks are tied, the update from the right peer will win out, since the peer ID <code>bob</code> is lexicographically greater than <code>alice</code>.</p>
<p>Okay, let’s look at some code.
First, here’s what an <em>unencrypted</em> last write wins register might look like in Rust:</p>
<pre data-language="rust"><code is:raw=""><span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>Register</span> <span>{</span>
    <span>pub</span> peer<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> clock<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Register</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>new</span><span>(</span>peer<span>:</span> <span>u64</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>,</span>
            clock<span>:</span> <span>0</span><span>,</span>
            value<span>:</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>)</span> <span>{</span>
        <span>self</span><span>.</span>peer <span>=</span> peer<span>;</span>
        <span>self</span><span>.</span>clock <span>+=</span> <span>1</span><span>;</span>
        <span>self</span><span>.</span>value <span>=</span> value<span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set_string</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>{</span>
        <span>let</span> bytes <span>=</span> value<span>.</span><span>as_bytes</span><span>(</span><span>)</span><span>;</span>
        <span>let</span> len <span>=</span> bytes<span>.</span><span>len</span><span>(</span><span>)</span><span>.</span><span>min</span><span>(</span><span>DATA_SIZE</span><span>)</span><span>;</span>

        <span>let</span> <span>mut</span> data <span>=</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>;</span>
        data<span>[</span><span>..</span>len<span>]</span><span>.</span><span>copy_from_slice</span><span>(</span><span>&amp;</span>bytes<span>[</span><span>..</span>len<span>]</span><span>)</span><span>;</span>

        <span>self</span><span>.</span><span>set</span><span>(</span>id<span>,</span> data<span>)</span><span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>Register</span><span>)</span> <span>{</span>
        <span>if</span> <span>self</span><span>.</span>clock <span>&gt;</span> other<span>.</span>clock <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span><span>;</span>

        <span>if</span> <span>self</span><span>.</span>clock <span>==</span> other<span>.</span>clock <span>&amp;&amp;</span> <span>self</span><span>.</span>peer <span>&gt;</span> other<span>.</span>peer <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span>

        <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
        <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
        <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>It has a peer ID, a clock and a value.
To merge with another register, it just takes the peer ID, clock and value from the register with the higher clock.
In case of a tie, it uses the peer ID as a tiebreaker.</p>
<p>Because Rust is a low-level language, we need separate functions to convert types such as strings into the raw bytes to store as the value.
We also store the value in an array with a statically-known size — although as we’ll see, that’s less of a Rust limitation than it is a fundamental constraint of homomorphic encryption.</p>
<p>Here’s the skeleton of an <code>EncryptedRegister</code> struct:</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>core<span>::</span></span>array<span>;</span>
<span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span><span>ClientKey</span><span>,</span> <span>FheUint64</span><span>,</span> <span>FheUint8</span><span>}</span><span>;</span>

<span>use</span> <span>crate</span><span>::</span><span>Register</span><span>;</span>

<span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>EncryptedRegister</span> <span>{</span>
    peer<span>:</span> <span>FheUint64</span><span>,</span>
    clock<span>:</span> <span>FheUint64</span><span>,</span>
    value<span>:</span> <span>[</span><span>FheUint8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>EncryptedRegister</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>encrypt</span><span>(</span>clear<span>:</span> <span>&amp;</span><span>Register</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>EncryptedRegister</span> <span>{</span>
        <span>EncryptedRegister</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
        <span>// ...</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>Pretty similar to the unencrypted <code>Register</code> struct!
<code>FheUint64</code> has replaced <code>u64</code>, and <code>value</code> is now an array of <code>FheUint8</code> rather than <code>u8</code>.
These are TFHE-rs types that encrypt the corresponding Rust types.
But other than that, the struct is the same.</p>
<p>The implementation has two new methods:</p>
<ul>
<li><code>encrypt</code>, which takes a normal <code>Register</code> and a client key, encrypts all the fields and returns an <code>EncryptedRegister</code>.</li>
<li><code>decrypt</code>, which takes a client key, decrypts all the fields and returns a normal <code>Register</code>.</li>
</ul>
<p>We’ve also omitted the <code>set</code> and <code>set_string</code> methods.
Since <code>EncryptedRegister</code> runs on the server, the value will never be set manually.
The only thing it needs to do is merge an incoming register with the register it has in memory.</p>
<p>Okay, so what does the <code>merge</code> method look like?</p>
<p>As we saw before, TFHE-rs overloads operators like <code>+</code> to make working with encrypted values more convenient.
For operators that don’t support overloading such as <code>&lt;</code>, TFHE-rs has methods like <code>gt</code>.</p>
<p>Given that, you might think we could write the <code>merge</code> method like this:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>id<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>id<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>self</span><span>.</span>id <span>=</span> other<span>.</span>id<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>This will <em>definitely not work</em>!</p>
<p>Remember that we can’t retrieve any information by operating on the encrypted data — <em>including information about the results of intermediate steps</em>.</p>
<p>To more clearly show the problem with this strategy, we can add some logging:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"local clock is greater than other clock!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"clocks are equal but local peer is greater than other peer!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>println!</span><span>(</span><span>"overwriting local data with remote data!"</span><span>)</span><span>;</span>
    <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>Although we still couldn’t decrypt the encrypted data, this (fake) implementation would reveal the result of the merge!
We’d know which branches our code took, and therefore learn which decrypted clock was higher and which encrypted data was written to the register.</p>
<p>Instead, our merge function must <em>eagerly</em> evaluate all branches in our code.
It also means that all loops must run for a statically-known number of iterations.
More generally, <strong>our code must always execute as though operating on the worst case input</strong>, because altering behavior based on the input would leak information about it.</p>
<p>Here’s the <em>real</em> code for our merge function:</p>
<pre data-language="rust"><code is:raw="">  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>

    <span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>

    <span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>

    <span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>
</code></pre>
<p>Superficially, it looks fairly similar, but there are a couple of important differences.
Let’s take it line by line.</p>
<p>First, we determine whether the local clock is higher than the other clock:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
</code></pre>
<p>If we think back to the logic gates, we can imagine what’s going on under the hood here, right?
We built this exact circuit!
Ours only operated on two-bit numbers, but the idea was the same: accept two numbers and return a 0 or 1 indicating whether the first number is higher than the second.</p>
<p>(In our circuit, the result was a <em>plaintext</em> 0 or 1 — but remember, homomorphic encryption operates with <em>encrypted</em> values!
The <code>gt</code> method actually returns an <code>FheBool</code>: an <em>encrypted bool</em> which indicates whether the local clock is higher than the other one.)</p>
<p>If we had the client key, we could decrypt that variable and find out its true value.
We can’t do that, but we can still <em>combine it with other encrypted values</em> to write our merge algorithm.</p>
<p>Here are the conditions to break a tie between the clocks:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
</code></pre>
<p>Two more <code>FheBool</code>s indicating whether the clocks are equal and if the local peer ID is higher.</p>
<p>Next, we combine them:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>
</code></pre>
<p>This combines all those <code>FheBool</code>s to determine whether to keep the local data or overwrite it with the merged data.<sup><a href="#user-content-fn-astute" id="user-content-fnref-astute" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">4</a></sup></p>
<p>Those <code>|</code> and <code>&amp;</code> operators are bitwise AND and bitwise OR, which work exactly like the AND and OR logic gates we made earlier.
They’re similar to the logical AND and OR we’re used to — <code>&amp;&amp;</code> and <code>||</code>, but with one big difference: bitwise operators are <em>eager</em>.
Whereas logical AND and OR might skip the second expression depending on the first, bitwise operators will <em>always</em> evaluate both sides.</p>
<p>Now that we’ve determined the register values to keep — even if we can’t tell which ones — we need to write the data to the register.
Here’s the secret sauce:</p>
<pre data-language="rust"><code is:raw=""><span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
<span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</code></pre>
<p>Rather than <code>if</code> or <code>match</code> expressions, we use <code>FheBool</code>’s <code>select</code> method.
It returns the first argument if the underlying <code>FheBool</code> value is <code>true</code>, or the second argument if the underlying value is <code>false</code>.</p>
<p>This is important: <em>the return value is different from both arguments</em>.
While decrypting the return value would reveal the same plaintext as one of the arguments, in ciphertext all three are distinct.
This means that we can’t tell which values we’ve set on the register by the end of the merge.</p>
<p>When the merge is done, every piece of ciphertext has changed — the peer ID, the clock and the register value.
The plaintext values might have updated (or might not have!) but there’s no way to tell by looking at the ciphertext.</p>
<p>Problem solved, right?
We can now have the server merge our CRDT without knowing what it contains?
Weeeellllllll…</p>
<h2 id="fundamental-limitations">Fundamental Limitations</h2>
<p>Homomorphic encryption has constraints that sharply limit its effectiveness with regard to local-first software.</p>
<p>For starters: encryption keys.
In both the simple adding example and the last-write wins register, we generated a key that would be passed to the server.
That only needs to happen once, but the difference between the size of our key and the size of our data can be surprising.</p>
<p>Our register took up only 32 bytes of data — 8 bytes each for the peer and clock, and 16 bytes for the value.
Meanwhile, TFHE-rs generated a <em>123 megabyte</em> server key.
We can compress the key down to about 27 megabytes, but still: that’s almost 850,000 times more key than data!</p>
<p>The payload here is particularly small, but a disparity of that size isn’t unheard of.
In his <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">overview of fully homomorphic encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, Jeremy Kun cites examples in which ciphertexts of dozens or hundreds of <em>kilobytes</em> require keys on the order of <em>gigabytes</em>.</p>
<p>Runtime performance is also — to put it lightly — lacking.
I benchmarked the unencrypted and encrypted versions of the last write wins register on an M4 MacBook Pro.
The unencrypted one averaged a merge time of 0.52 nanoseconds.</p>
<p>The encrypted one?
<em>1.06 seconds</em>.
That’s not a typo: the homomorphically encrypted merge is <em>two billion times slower</em>.<sup><a href="#user-content-fn-gpu" id="user-content-fnref-gpu" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">5</a></sup></p>
<p>Not great!</p>
<p>That’s not all.
We said before that our code must execute as though operating on the worst case input.
Even if the performance issues improve by many orders of magnitude, the “worst case” requirement will still impose constraints on the CRDT algorithm itself.</p>
<p>Consider a fully homomorphically encrypted last-write wins <em>map</em> CRDT.
Most maps store keys sparsely, so the map only grows in size as keys are added.</p>
<p>Here’s a playground that simulates encrypting a sparse map.<sup><a href="#user-content-fn-pretend" id="user-content-fnref-pretend" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">6</a></sup>
When you modify the plaintext map on the left, the encrypted map on the right updates.
Can you see a security issue?</p>
<encrypted-map-demo></encrypted-map-demo>
<p>Imagine you only had access to the map on the right.
You could still see data being added and removed!
Furthermore, this map lazily encrypts only the data that changes, which would allow you to see exactly which key changed (if any).</p>
<p>A homomorphically encrypted map CRDT couldn’t do that.
Since it must assume a worst-case input, it must store the keys <em>densely</em>: limiting the size to a fixed number of keys and reserving all the space up front.
Merging two identical maps would be exactly as computationally intensive as merging two maps in which <em>every</em> key was updated.<sup><a href="#user-content-fn-op" id="user-content-fnref-op" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">7</a></sup></p>
<p>The playground below simulates a homomorphically encrypted map.
While you can add and remove keys to the plaintext map on the left, the encrypted map on the right behaves as though every key is filled.
And no matter how you modify the plaintext map, <em>everything</em> in the encrypted map changes:</p>
<encrypted-map-demo dense="true" keys="4"></encrypted-map-demo>
<p>From the outside, there’s no way to tell what changed in the map: we see the exact same number of keys, and every value has changed.
To calculate the new map, the server must go through and merge <em>every single key</em>.
After that, it needs to transfer the full map to each peer — because remember, as far as it knows, the entire map is different.</p>
<p>These are fundamental limitations of homomorphic encryption!
The requirement that homomorphically encrypted code performs as though operating on the worst-case input dramatically increases both the space and time required to update.</p>
<h2 id="parting-thoughts">Parting Thoughts</h2>
<p>I started this article thinking that local-first software and homomorphic encryption would be natural bedfellows.</p>
<p>But honestly, I came away… a little less enamored.
The fundamental limitations of homomorphic encryption mean that it will always operate under a set of worst-case assumptions.
Homomorphically encrypted CRDTs aren’t intractable, but they are severely limited by these intrinsic constraints.</p>
<p>So the question remains: how can we secure local-first apps without severely degrading usability?</p>
<p>Luckily, I’m not the only one thinking about this problem!</p>

<p>CRDTs are a relatively young technology — the paper formalizing them was published in 2011 — so there’s still a lot of unexplored solution space.
We may not have solved this problem yet, but I’m confident that we’re closing in on it!</p>

<section data-footnotes="">
<ol>
<li id="user-content-fn-e2ee">
<p>More comprehensive solutions might try to implement things like <a href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f="">forward secrecy</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f=""> <img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/KDF_chain.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Forward secrecy - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Forward_secrecy</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, which is absolutely not easy.
But the basic version is still better than nothing. <a href="#user-content-fnref-e2ee" data-footnote-backref="" aria-label="Back to reference 1" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-otherways">
<p>Homomorphic encryption isn’t the only way to solve this problem.
You might instead just ignore it, and have the server store every version of your encrypted document — wasteful, but it’d work!
You could also use a CRDT implementation that only requires <em>changes</em> to be sent to the server, rather than the full document. <a href="#user-content-fnref-otherways" data-footnote-backref="" aria-label="Back to reference 2" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-addition">
<p>You might notice that when both XOR inputs are 1, the result is 0.
You might also remember from math class that the result of 1 + 1 is, uh, not 0.
So how can XOR represent addition?</p>
<p>Remember that we’re operating on binary numbers — all we have is 0 and 1!
Adding to 1 in binary is like adding to 9 in decimal: since we’re out of digits, we instead roll that place back to 0 and <em>carry the 1 to the next place</em>, giving us 10.
The XOR gate represents the <em>sum digit</em> of that Boolean addition.
To fully represent the result, <a href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">we’d need to use the AND gate as well to represent the <em>carry digit</em></a><a data-tooltip="" href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">XOR gate - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Addition" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/XOR_gate#Addition</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>. <a href="#user-content-fnref-addition" data-footnote-backref="" aria-label="Back to reference 3" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-astute">
<p><em>Truly</em> eagle-eyed readers will notice that this is <em>also</em> the same circuit we used to determine whether one two-bit number was greater than another.
At a high level, the logic there was “most significant bit is greater” OR (“most significant bits are equal” AND “least significant bit is greater”).
Here, the logic is “clock is greater” OR (“clocks are equal” AND “peer ID is greater”). <a href="#user-content-fnref-astute" data-footnote-backref="" aria-label="Back to reference 4" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-gpu">
<p>Granted, I was only able to run it on the CPU — TFHE-rs only supports GPU acceleration on Linux — but even if I could run it on the GPU, <a href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">TFHE-rs’s benchmarks</a><a data-tooltip="" href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">TFHE-rs</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://docs.zama.ai/~gitbook/image?url=https%3A%2F%2F572209210-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fcollections%252FprREL84Xd1lx94uAslRx%252Ficon%252F3R1LaM67E4BE3WhJZF5p%252FLogo%2520-%2520Square.png%3Falt%3Dmedia%26token%3Db39ed5d3-5537-4c62-9389-5b23f830072b&amp;width=48&amp;height=48&amp;sign=c9d8fc58&amp;sv=2" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">docs.zama.ai/tfhe-rs/get-started/summary</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> indicate that it would only speed things up by a factor of 3–5.
Even at the high end of that range, the encrypted merge would <em>still</em> be 400 million times slower than the unencrypted one. <a href="#user-content-fnref-gpu" data-footnote-backref="" aria-label="Back to reference 5" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-pretend">
<p>When I say “simulates encrypting”, I mean “displays a bunch of random hex digits”.
Please humor me! <a href="#user-content-fnref-pretend" data-footnote-backref="" aria-label="Back to reference 6" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-op">
<p>Note that this all assumes a <em>state-based</em> CRDT.
An <em>operation-based</em> CRDT — where the important operation is appending to a log of events rather than merging — might have a totally different set of tradeoffs. <a href="#user-content-fnref-op" data-footnote-backref="" aria-label="Back to reference 7" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-meri">
<p>Meri Leeworthy published a great <a href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f="">deep-dive explainer on KeyHive’s key encapsulation mechanism</a><a data-tooltip="" href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f=""> <img src="https://static.meri.garden/mesh-gradient.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">A deep-dive explainer on Ink and Switch's BeeKEM protocol</span> <span data-astro-cid-bi7aps5f="">I'm a programmer, designer, writer and artist. I try to make tools for community autonomy, creativity, and resistance.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://meri.garden/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">meri.garden/a-deep-dive-explainer-on-beekem-protocol/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> which is absolutely worth a read! <a href="#user-content-fnref-meri" data-footnote-backref="" aria-label="Back to reference 8" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workout.cool – Open-source fitness coaching platform (590 pts)]]></title>
            <link>https://github.com/Snouzy/workout-cool</link>
            <guid>44309320</guid>
            <pubDate>Wed, 18 Jun 2025 12:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snouzy/workout-cool">https://github.com/Snouzy/workout-cool</a>, See on <a href="https://news.ycombinator.com/item?id=44309320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#about">About</a></li>
<li><a href="#-project-origin--motivation">Project Origin &amp; Motivation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#exercise-database-import">Exercise Database Import</a></li>
<li><a href="#project-architecture">Project Architecture</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#license">License</a></li>
<li><a href="#-sponsor-this-project">Sponsor This Project</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<a href="https://github.com/Snouzy/workout-cool/graphs/contributors">
  <img src="https://camo.githubusercontent.com/7c851db85ae8783e22cf0dabfd6e6cce46c6c360fed141a0f287fa87d478c4f7/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d536e6f757a792f776f726b6f75742d636f6f6c" data-canonical-src="https://contrib.rocks/image?repo=Snouzy/workout-cool">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">A comprehensive fitness coaching platform that allows create workout plans for you, track progress, and access a vast exercise database with
detailed instructions and video demonstrations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Project Origin &amp; Motivation</h2><a id="user-content--project-origin--motivation" aria-label="Permalink: 🎯 Project Origin &amp; Motivation" href="#-project-origin--motivation"></a></p>
<p dir="auto">This project was born from a personal mission to revive and improve upon a previous fitness platform. As the <strong>primary contributor</strong> to the
original <a href="https://github.com/workout-lol/workout-lol">workout.lol</a> project, I witnessed its journey and abandonment. 🥹</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Story Behind <strong><em>workout.cool</em></strong></h3><a id="user-content-the-story-behind-workoutcool" aria-label="Permalink: The Story Behind workout.cool" href="#the-story-behind-workoutcool"></a></p>
<ul dir="auto">
<li>🏗️ <strong>Original Contributor</strong>: I was the main contributor to workout.lol</li>
<li>💼 <strong>Business Challenges</strong>: The original project faced major hurdles with exercise video partnerships (no reliable video provider) could
be established</li>
<li>💰 <strong>Project Sale</strong>: Due to these partnership issues, the project was sold to another party</li>
<li>📉 <strong>Abandonment</strong>: The new owner quickly realized that <strong>exercise video licensing costs were prohibitively expensive</strong>, began to be sick
and abandoned the entire project</li>
<li>🔄 <strong>Revival Attempts</strong>: For the past <strong>9 months</strong>, I've been trying to reconnect with the new stakeholder</li>
<li>📧 <strong>Radio Silence</strong>: Despite multiple (15) attempts, there has been no response</li>
<li>🚀 <strong>New Beginning</strong>: Rather than let this valuable work disappear, I decided to create a fresh, modern implementation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why <strong><em>workout.cool</em></strong> Exists</h3><a id="user-content-why-workoutcool-exists" aria-label="Permalink: Why workout.cool Exists" href="#why-workoutcool-exists"></a></p>
<p dir="auto"><strong>Someone had to step up.</strong></p>
<p dir="auto">The opensource fitness community deserves better than broken promises and abandoned platforms.</p>
<p dir="auto">I'm not building this for profit.</p>
<p dir="auto">This isn't just a revival : it's an evolution. <strong>workout.cool</strong> represents everything the original project could have been, with the
reliability, modern approach, and <strong>maintenance</strong> that the fitness open source community deserves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 From the Community, For the Community</h2><a id="user-content--from-the-community-for-the-community" aria-label="Permalink: 👥 From the Community, For the Community" href="#-from-the-community-for-the-community"></a></p>
<p dir="auto"><strong>I'm not just a developer : I'm a user who refused to let our community down.</strong></p>
<p dir="auto">I experienced firsthand the frustration of watching a beloved tool slowly disappear. Like many of you, I had workouts saved, progress
tracked, and a routine built around the platform.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">My Mission: Rescue &amp; Revive.</h3><a id="user-content-my-mission-rescue--revive" aria-label="Permalink: My Mission: Rescue &amp; Revive." href="#my-mission-rescue--revive"></a></p>
<p dir="auto"><em>If you were part of the original workout.lol community, welcome back! If you're new here, welcome to the future of fitness platform
management.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js 18+</li>
<li>Either:
<ul dir="auto">
<li>Docker</li>
<li>OR PostgreSQL external database</li>
</ul>
</li>
<li>pnpm (recommended) or npm</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Snouzy/workout-cool.git
cd workout-cool"><pre>git clone https://github.com/Snouzy/workout-cool.git
<span>cd</span> workout-cool</pre></div>
</li>
<li>
<p dir="auto"><strong>Install dependencies</strong></p>

</li>
<li>
<p dir="auto"><strong>Set up environment variables</strong></p>

<p dir="auto">Fill in your database URL and other required environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DATABASE_URL=&quot;postgresql://username:password@localhost:5432/workout_cool&quot;
BETTER_AUTH_SECRET=&quot;your-secret-key&quot;
# ... other variables"><pre><span>DATABASE_URL</span><span>=</span><span><span>"</span>postgresql://username:password@localhost:5432/workout_cool<span>"</span></span>
<span>BETTER_AUTH_SECRET</span><span>=</span><span><span>"</span>your-secret-key<span>"</span></span>
<span><span>#</span> ... other variables</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Set up the database</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Using Docker</h4><a id="user-content-option-1-using-docker" aria-label="Permalink: Option 1: Using Docker" href="#option-1-using-docker"></a></p>
<p dir="auto">The project provides a convenient <code>make</code> command that handles everything:</p>

<p dir="auto">This single command will:</p>
<ul dir="auto">
<li>Start the PostgreSQL database using Docker</li>
<li>Run database migrations</li>
<li>Start the development server</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 2: Manual PostgreSQL Setup</h4><a id="user-content-option-2-manual-postgresql-setup" aria-label="Permalink: Option 2: Manual PostgreSQL Setup" href="#option-2-manual-postgresql-setup"></a></p>
<p dir="auto">If you prefer to use your own PostgreSQL installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run migrations
npx prisma migrate deploy
npx prisma generate

# Start the development server
pnpm dev"><pre><span><span>#</span> Run migrations</span>
npx prisma migrate deploy
npx prisma generate

<span><span>#</span> Start the development server</span>
pnpm dev</pre></div>
</li>
<li>
<p dir="auto"><strong>Open your browser</strong> Navigate to <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Exercise Database Import</h2><a id="user-content-exercise-database-import" aria-label="Permalink: Exercise Database Import" href="#exercise-database-import"></a></p>
<p dir="auto">The project includes a comprehensive exercise database. To import a sample of exercises:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites for Import</h3><a id="user-content-prerequisites-for-import" aria-label="Permalink: Prerequisites for Import" href="#prerequisites-for-import"></a></p>
<ol dir="auto">
<li><strong>Prepare your CSV file</strong></li>
</ol>
<p dir="auto">Your CSV should have these columns:</p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value"><pre><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
</code></pre></div>
<p dir="auto">You can use the provided example.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Import Commands</h3><a id="user-content-import-commands" aria-label="Permalink: Import Commands" href="#import-commands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Import exercises from a CSV file
pnpm run import:exercises-full /path/to/your/exercises.csv

# Example with the provided sample data
pnpm run import:exercises-full ./data/sample-exercises.csv"><pre><span><span>#</span> Import exercises from a CSV file</span>
pnpm run import:exercises-full /path/to/your/exercises.csv

<span><span>#</span> Example with the provided sample data</span>
pnpm run import:exercises-full ./data/sample-exercises.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CSV Format Example</h3><a id="user-content-csv-format-example" aria-label="Permalink: CSV Format Example" href="#csv-format-example"></a></p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS"><pre lang="csv"><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Attribute Types</h3><a id="user-content-available-attribute-types" aria-label="Permalink: Available Attribute Types" href="#available-attribute-types"></a></p>
<ul dir="auto">
<li><strong>TYPE</strong>: <code>STRENGTH</code>, <code>CARDIO</code>, <code>PLYOMETRICS</code>, <code>STRETCHING</code>, etc.</li>
<li><strong>PRIMARY_MUSCLE</strong>: <code>QUADRICEPS</code>, <code>CHEST</code>, <code>BACK</code>, <code>SHOULDERS</code>, etc.</li>
<li><strong>SECONDARY_MUSCLE</strong>: Secondary muscle groups targeted</li>
<li><strong>EQUIPMENT</strong>: <code>BARBELL</code>, <code>DUMBBELL</code>, <code>BODYWEIGHT</code>, <code>MACHINE</code>, etc.</li>
<li><strong>MECHANICS_TYPE</strong>: <code>COMPOUND</code>, <code>ISOLATION</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Architecture</h2><a id="user-content-project-architecture" aria-label="Permalink: Project Architecture" href="#project-architecture"></a></p>
<p dir="auto">This project follows <strong>Feature-Sliced Design (FSD)</strong> principles with Next.js App Router:</p>
<div data-snippet-clipboard-copy-content="src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes"><pre><code>src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture Principles</h3><a id="user-content-architecture-principles" aria-label="Permalink: Architecture Principles" href="#architecture-principles"></a></p>
<ul dir="auto">
<li><strong>Feature-driven</strong>: Each feature is independent and reusable</li>
<li><strong>Clear domain isolation</strong>: <code>shared</code> → <code>entities</code> → <code>features</code> → <code>widgets</code> → <code>app</code></li>
<li><strong>Consistency</strong>: Between business logic, UI, and data layers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Feature Structure</h3><a id="user-content-example-feature-structure" aria-label="Permalink: Example Feature Structure" href="#example-feature-structure"></a></p>
<div data-snippet-clipboard-copy-content="features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls"><pre><code>features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Here are the next steps and goals for Workout.cool:</p>
<ul>
<li> 🏋️‍♂️ Add new exercises and videos</li>
<li> 📱 Mobile app (React Native)</li>
<li> 🏆 Badges and gamification system</li>
<li> 📊 Advanced progress statistics</li>
<li> 🤝 Integration with wearables (watches, trackers)</li>
<li> 🌍 Multilingual support</li>
<li> 🔒 OAuth authentication (Google, Apple, etc.)</li>
<li> 💬 Built-in community forum</li>
</ul>
<p dir="auto">Feel free to suggest your ideas via <a href="https://github.com/Snouzy/workout-cool/issues">issues</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions! Please see our <a href="https://github.com/Snouzy/workout-cool/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Workflow</h3><a id="user-content-development-workflow" aria-label="Permalink: Development Workflow" href="#development-workflow"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Make your changes</li>
<li>Commit your changes (<code>git commit -m 'feat: add amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<ul dir="auto">
<li>Follow TypeScript best practices</li>
<li>Use Feature-Sliced Design architecture</li>
<li>Write meaningful commit messages</li>
<li>Add tests for new features</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment</h2><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Docker (Not ready yet : todo)</h3><a id="user-content-using-docker-not-ready-yet--todo" aria-label="Permalink: Using Docker (Not ready yet : todo)" href="#using-docker-not-ready-yet--todo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image
docker build -t workout-cool .

# Run the container
docker run -p 3000:3000 workout-cool"><pre><span><span>#</span> Build the Docker image</span>
docker build -t workout-cool <span>.</span>

<span><span>#</span> Run the container</span>
docker run -p 3000:3000 workout-cool</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Deployment</h3><a id="user-content-manual-deployment" aria-label="Permalink: Manual Deployment" href="#manual-deployment"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the application
pnpm build

# Run database migrations
export DATABASE_URL=&quot;your-production-db-url&quot;
npx prisma migrate deploy

# Start the production server
pnpm start"><pre><span><span>#</span> Build the application</span>
pnpm build

<span><span>#</span> Run database migrations</span>
<span>export</span> DATABASE_URL=<span><span>"</span>your-production-db-url<span>"</span></span>
npx prisma migrate deploy

<span><span>#</span> Start the production server</span>
pnpm start</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://feature-sliced.design/" rel="nofollow">Feature-Sliced Design</a></li>
<li><a href="https://nextjs.org/docs" rel="nofollow">Next.js Documentation</a></li>
<li><a href="https://www.prisma.io/docs/" rel="nofollow">Prisma Documentation</a></li>
<li><a href="https://github.com/better-auth/better-auth">Better Auth</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/28f4d479bf0a9b033b3a3b95ab2adc343da448a025b01aefdc0fbc7f0e169eb8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667" alt="MIT License" data-canonical-src="https://img.shields.io/badge/License-MIT-green.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Join the Rescue Mission</h2><a id="user-content--join-the-rescue-mission" aria-label="Permalink: 🤝 Join the Rescue Mission" href="#-join-the-rescue-mission"></a></p>
<p dir="auto"><strong>This is about rebuilding what we lost, together.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How You Can Help</h3><a id="user-content-how-you-can-help" aria-label="Permalink: How You Can Help" href="#how-you-can-help"></a></p>
<ul dir="auto">
<li>🌟 <strong>Star this repo</strong> to show the world our community is alive and thriving</li>
<li>🐛 <strong>Report issues</strong> you find. I'm listening to every single one</li>
<li>💡 <strong>Share your feature requests</strong> finally, someone who will actually implement them !</li>
<li>🔄 <strong>Spread the word</strong> to fellow fitness enthusiasts who lost hope</li>
<li>🤝 <strong>Contribute code</strong> if you're a developer : let's build this together</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💖 Sponsor This Project</h2><a id="user-content--sponsor-this-project" aria-label="Permalink: 💖 Sponsor This Project" href="#-sponsor-this-project"></a></p>
<p dir="auto">Appear in the README and on the website as supporter by donating:</p>
<p><a href="https://ko-fi.com/workoutcool" rel="nofollow">
    <img src="https://camo.githubusercontent.com/70e2ef5e0263b261f9a2a314bb1d6919d1d43292eed117fe8fc766a68c7d96ea/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="Sponsor on Ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg">
  </a>
  &nbsp;&nbsp;&nbsp;
  
  
</p>
<p dir="auto">
  <em>If you believe in open-source fitness tools and want to help this project thrive,<br>
  consider buying me a coffee ☕ or sponsoring the continued development.</em>
</p>
<p dir="auto">
  Your support helps cover hosting costs, exercise database updates, and continuous improvement.<br>
  Thank you for keeping <strong>workout.cool</strong> alive and evolving 💪
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is There a Half-Life for the Success Rates of AI Agents? (218 pts)]]></title>
            <link>https://www.tobyord.com/writing/half-life</link>
            <guid>44308711</guid>
            <pubDate>Wed, 18 Jun 2025 10:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tobyord.com/writing/half-life">https://www.tobyord.com/writing/half-life</a>, See on <a href="https://news.ycombinator.com/item?id=44308711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
            <p><h2>Writing</h2></p>
            
          </div>
            <section data-content-field="main-content" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">  
  <article id="post-681b2734c79dc21300b23894" data-item-id="681b2734c79dc21300b23894">
    
    <!--POST TILE-->

    

    

    <!--MAIN CONTENT-->

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1746610169593" id="item-681b2734c79dc21300b23894"><div data-block-type="2" id="block-84469d78d64003385a03">
  <p>Building on the recent empirical work of Kwa et al. (2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model — a constant rate of failing during each minute a human would take to do the task. This implies an exponentially declining success rate with the length of the task and that each agent could be characterised by its own half-life. This empirical regularity allows us to estimate the success rate for an agent at different task lengths. And the fact that this model is a good fit for the data is suggestive of the underlying causes of failure on longer tasks — that they involve increasingly large sets of subtasks where failing any one fails the task. Whether this model applies more generally on other suites of tasks is unknown and an important subject for further work.</p><p><strong>METR’s results on the length of tasks agents can reliably complete</strong></p><p>A recent paper by <a href="https://doi.org/10.48550/arXiv.2503.14499"><span>Kwa et al. (2025)</span></a> from the research organisation <a href="https://metr.org/" target="_blank">METR</a> has found an exponential trend in the duration of the tasks that frontier AI agents can solve: every 7 months, the length of task they can solve doubles.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_8461">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image-dimensions="1926x1070" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" width="1926" height="1070" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_8778">

<p>These headline results are based on&nbsp; a test suite of 170 software engineering, cybersecurity, general reasoning, and ML tasks that they assembled to be indicative of the kinds of tasks that could help AI agents assist in AI research. These tasks are assembled from three different benchmarks that take different amounts of time for humans to achieve:</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_11913">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image-dimensions="1820x1028" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" width="1820" height="1028" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_12229">
  <p>In general, ability to perform a task drops off as its duration increases, so they use the AI agent’s performance on tasks of different lengths to estimate the task-length at which the model would have a 50% success rate. They then showed that this length has been doubling every 7 months as the capabilities of frontier agents improve. The task-lengths are measured by how long it took humans to solve the same tasks.</p><p>They used 50% success rate as their chief performance threshold because it is the easiest level to robustly estimate. They are well aware that for many tasks, the required success rate for useful work may be much higher — such as 80%, 99%, or 99.9999%. They do measure the 80% success rate and find their mean estimate to have a doubling time of 213 days, compared to 212 days of the 50% rate. These are close to identical within their margin of error (±40 days for the 50% rate), so they conclude that the particular threshold doesn’t seem to have much effect on their headline result about the rate of improvement.</p><p>But there is quite a gap between the 50% success rate time-horizon and the 80% success rate time horizon. For the best model (Claude 3.7 Sonnet) it could achieve a 50% success rate on tasks up to 59 minutes <em>vs</em> only 15 minutes if an 80% success rate was required. If those results generalise to the other models, then we could also see it like this: the task length for an 80% success rate is 1/4 the task length for a 50% success rate. Or in terms of improvement: what is doable with a 50% success rate now is doable with an 80% success rate in 14 months’ time (= 2 doubling times).</p><p>The idea of measuring improvement in AI capabilities over time via time horizons at a chosen success rate is novel and interesting. AI forecasting is often hamstrung by the lack of a good measure for the y-axis of performance over time. We can track progress within a particular benchmark, but these are often solved in a couple of years, and we lack a good measure of underlying capability that can span multiple benchmarks. METR’s measure allows comparisons between very different kinds of tasks in a common currency (time it takes a human) and shows a strikingly clear trend line — suggesting it is measuring something real.</p><p>But there are grounds for criticism too. In particular, there is room to wonder how much these results generalise outside of this kind of task suite. We know that there are some tasks humans can do very quickly that AIs can’t solve (e.g. some simple spatial reasoning or intuitive physics tasks) and others that would take humans an extremely long time, but AIs can do quickly (e.g. rote mathematics). So a simple measure of ‘time it would take a human’ cannot explain all AI capability improvements. Kwa et al. are&nbsp; aware of this and even list several other ways that this task-suite may be non-representative of real-world performance including:</p><ul data-rte-list="default"><li><p>All tasks were automatically scorable (a domain where RL works best)</p></li><li><p>No interaction with other agents</p></li><li><p>Lax resource constraints</p></li></ul><p>For the purposes of this essay, we will take the data for what it is (performance on a particular task suite that may or may not generalise further) and explore underlying mechanisms that could explain it.</p><h3><strong>Explaining these results via a constant hazard rate</strong></h3><p>These results call out for some explanation of what is going on. For example, exactly how does the time horizon shrink as the required success probability is increased? And what does it <em>mean</em> for an agent to be able to perform an 8-hour task, but not a 16-hour task. Isn’t a 16-hour task just one 8-hour task after another?</p><p><a href="https://en.wikipedia.org/wiki/Survival_analysis" target="_blank">Survival analysis</a> is the field of understanding how the probability of something failing increases as a function of time. It tracks the survival probability at a time <em>S</em>(<em>t</em>) — that is, the chance it still hasn’t failed by that point. The simplest model in survival analysis is a constant hazard rate. This means that the chance of something failing in the next step (conditional on making it that far) is constant. A constant hazard rate leads to an exponentially declining survival curve. This behaviour is well-known from phenomena like radioactive decay, where there is a constant chance of decay at any moment, leading to an exponentially declining chance of the isotope’s survival over time, which is often measured by a half-life.</p><p>If AI agent success-rates drop off with task length in this manner, then the 50% success rate time-horizon for each agent from Kwa et al. is precisely the <em>half-life</em> of that agent. As with the half-life of a radioisotope, this isn’t just the median lifespan, it is the median remaining lifespan starting at any time — something that is only possible for an exponential survival curve. Unlike for particles, this AI agent half-life would be measured not in clock time, but in how long it takes a human to complete the task.</p><p>One rationale for this constant hazard rate model for AI agents is that tasks require getting past a series of steps each of which could end your attempt, with the longer the duration of the task, the more such steps. More precisely, if tasks could be broken down into a long sequence of equal-length subtasks with a constant (and independent) chance of failure, such that to succeed in the whole task, the agent needs to succeed in <em>all</em> subtasks, then that would create an exponential survival curve. I.e. when Pr(<em>Task</em>) = Pr(<em>Subtask</em>$_1$ &amp; <em>Substask</em>$_2$ &amp; … &amp; <em>Subtask</em>$_N$).</p><p>But we don’t have to assume a perfect breakdown of the task into equal-length-equal-difficulty subtasks in order to get an exponential distribution (and corresponding constant hazard rate). We can also think of the constant hazard rate model as being agnostic as to how the task is broken down, just saying that the chance of succeeding in a subtask of duration <em>t</em> is always equal to the chance of succeeding in each of a set of smaller subtasks whose combined duration is also <em>t</em>. So on this model, it doesn’t matter what level of granularity you assess the task at — whether you see it as one 60-minute task, six 10-minute tasks, or a 20-minute task plus forty 1-minute tasks — the chance of succeeding is set by the total time it would take a human to complete it.</p><p>This constant hazard rate model would predict that the time horizon for an 80% success rate is about ⅓ of the time horizon for a 50% success rate. This is because the chance of surviving three periods with an 80% success rate = (0.8)$^3$ = 0.512 ≈ 50%. More precisely, the time horizon for a success probability of <em>p</em> would be ln(<em>p</em>)/ln(<em>q</em>) times as long as one with success probability <em>q</em>. So an 80% time-horizon would be ln(0.8)/ln(0.5) = 0.322 times as long as the 50% time-horizon. Kwa et al. estimate the 80% time-horizon for Claude 3.7 Sonnet to be 0.25 as long, which is close to this theoretical estimate and within the margin of error given the noisiness of the results. The following chart shows how these numbers relate to the exponential survival curve (where <em>T</em>$_{50}$ is the 50% time-horizon etc.).</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_24919">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image-dimensions="2427x1731" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" width="2427" height="1731" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_25236">
  <p>Here are some useful comparisons for how the predicted time horizons over which an agent could get very high success rates compare to the measured time horizon for a 50% success rate:</p><p><em>&nbsp; T</em>$_{80}$ ≈ 1/3&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{90}$ ≈ 1/7&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99}$ ≈ 1/70 <em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ ≈ 1/700 <em>T</em>$_{50}$<em><br>&nbsp; [and each additional ‘nine’ of reliability beyond this divides the time horizon by 10]</em></p><p>We can also use this model to calculate how long we’d expect it to take between the 50% success rate time horizon reaching a given length and a high success rate time horizon reaching that some length (on the assumption of the 7-month doubling time and the constant hazard rate model): </p><p><em>&nbsp; T</em>$_{80}$ reaches any particular length about 1 year after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{90}$ reaches any particular length about 2 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99}$ reaches any particular length about 4 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ reaches any particular length about 6 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>[each additional ‘nine’ of reliability requires 2 more years…]</em></p><p>Kwa et al. also attempt to fit a relationship between the success probability and time-horizon (see the figure below, which I’ve adapted from their paper). They plot the time horizon on a log scale and note that this reveals a sigmoid-shaped decay curve of success rate (the coloured bars). They show that this is reasonably well fit by a logistic function (the black curves). The paper doesn’t compare this to how well alternative functions would fit the data.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_26796">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image-dimensions="2503x1718" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" width="2503" height="1718" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_27113">
  <p>The data is also well-fit by an exponential function, which also looks like a sigmoid when plotted on this logarithmic x-axis. I’ve added this to the above figure (from Kwa et al.) in the form of the dotted blue curves.&nbsp; It fits the data better for some models (such as the top left) and worse on others. It fits the data roughly as well overall, while being substantially more likely <em>a priori</em> — exponential decay is <em>the </em>simplest survival curve and has only one free parameter instead of two. Moreover, while logistic functions are quite simple and natural, the black curve here is not really a logistic distribution, but the more complex log-logistic distribution which merely looks like a logistic distribution when plotted on a logarithmic x-axis.</p><p>The paper also plots the survival curve for <em>human</em> performance over increasing time periods:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_28357">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image-dimensions="2148x760" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" width="2148" height="760" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_28674">
  <p>Intriguingly, this human survival curve seems to be noticeably better than a constant hazard rate (i.e. the chance of succeeding over long timescales drops off more slowly for humans than the constant hazard rate predicts). For example, on this graph, humans had about a 50% success rate at the 1.5-hour mark, which suggests 25% at 3 hours, 12.5% at 6 hours and 6.25% at 12 hours if the hazard rate is constant. However, the humans were still above 20% success rate at that point.&nbsp;</p><p>This could indicate a different scaling behaviour of success rate with time horizon for humans compared to AI agents, which would be well worth investigating and may suggest important underlying mechanisms (e.g. that the humans were better at correcting earlier failed subtasks). If human performance scales differently with task length than AI agent performance, that would be an important result, suggesting that there is a notable inefficiency in the current AI paradigm. This warrants further research.&nbsp;</p><p>However, there are other potential explanations. For instance, it could also be an artefact of this graph being an aggregate of humans with different ability levels, since even if all individual humans have a constant hazard rate (and so each have an exponential survival curve) a mixture of different humans would be a weighted sum of exponentials with different time constants and that distribution decays slower than an exponential (see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details).</p><p>The AI agents don’t suffer from mixing different capability levels into the same statistics because there is a separate data series for each agent. However, there is a similar effect that could still be present in the measurement of AI agents’ performance over increasingly long tasks. It is plausible that some tasks are inherently easier than others per unit time, corresponding to different hazard rates. If so, then the survival curve over the whole task suite would be averaging different exponential decay curves together. This produces an aggregate decay curve with thicker tails than an exponential (corresponding to a declining effective hazard rate). Again, see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details. Dealing with this effect is important as it could still be the case that the mechanism of constant hazard rate (and what it implies about the agents’ behaviour) holds for every task, even if it isn’t visible in the aggregate of these tasks.</p><h3><strong>Upshots of the constant hazard rate model</strong></h3><p>If the constant hazard rate model is sufficient to explain the drop-off in success rates on a task suite, there are several interesting upshots:</p><ul data-rte-list="default"><li><p>It allows us to make predictions for the time-horizons at other success rates, such as 90% and 99%.</p></li></ul><ul data-rte-list="default"><li><p>It allows us to make predictions for how the success rate improves over time for a fixed task-length.</p></li></ul><ul data-rte-list="default"><li><p>It provides simple rules of thumb for predicting success probabilities (e.g. that if you double the task duration, you square the success probability).</p></li></ul><ul data-rte-list="default"><li><p>It suggests that AI agent performance (at least on this task suite) can be characterised by a half-life.</p></li></ul><ul data-rte-list="default"><li><p>It provides indirect evidence that what really is going on under the hood is that tasks are made up of many sequential subtasks and the chance of succeeding at the whole requires succeeding at every individual component. Moreover, this suggests that the current AI agents are not very good at recovering from earlier mistakes.</p></li></ul><ul data-rte-list="default"><li><p>Because the exponential distribution is the unique memoryless distribution, another way of seeing it is that the chance of failing at the next moment is independent of how far you’ve come — just like how the chance of a radioisotope decaying in the next minute is independent on how many minutes it has survived so far. This would be a surprising and interesting property for reasoning agents.</p></li></ul><ul data-rte-list="default"><li><p>Deviations from exponential decay for certain models may provide evidence that their hazard rate is increasing (or decreasing) with time, which might provide hints as to what they are doing wrong (or right).</p></li></ul><ul data-rte-list="default"><li><p>It can explain the same 7-month doubling time for the 50% time-horizon and 80% time-horizon: a 7-month halving-time for the underlying hazard rate would produce the 7-month doubling-time of all such time horizons.</p></li></ul><ul data-rte-list="default"><li><p>It also helps conceptually explain what the time horizons could even mean. For example, if it can complete a day’s work, why can’t it just do that twice to produce two days of work? On the constant hazard rate model, the issue is that if has a 50% chance of succeeding on Monday’s work, then it only has a 25% chance of succeeding in both Monday’s and Tuesday’s work, which is too low to count as reliably achieving the 2-day task (and similarly for any higher reliability threshold).</p></li></ul><p>Note that I am not claiming AI agents have a precisely constant rate of failure per minute of time it would take a human to complete the task. The claim is instead that something like this appears to be roughly true or stochastically true. All other models imply that there is some systematic change in the hazard rate over time, and my suggestion (pending more information about the precise fits of different models) is that the data doesn’t warrant such assumptions.</p><p>If systematic deviations from exponential decay are found, such as the hazard rate increasing (or decreasing) with time, this might provide useful hints as to what the agents are doing wrong (or right). i.e. the constant hazard rate model can also work as a theoretical baseline from which to measure empirical deviations.</p><p>Also, note that <em>whatever</em> the survival curve is, if you lower the hazard rate by a factor of <em>k</em> at all times, the time horizons for all success rate levels also increase by a factor of <em>k</em>  (because the entire survival curve is  being stretched horizontally by a factor of <em>k</em>). Moreover, this is bidirectional — the only way to increase the time horizons at all success-rate levels by the same factor is to lower hazard rate by that factor. So to the extent that the METR evidence suggests the time horizons at every success rate are doubling at the same rate, it is also suggesting that the agent improvements are taking the form of reducing the hazard rate across all times by the same factor (i.e.. halving it every 7 months). This is intriguing and would be true whether or not there is a constant hazard rate for each model..</p><h3><strong>Further work</strong></h3><p>So far these results and analysis are merely suggestive of a constant hazard rate. Ideally one would conduct a formal statistical analysis on how well an exponential decay curve fits METRs data compared to the log-logistic they use.&nbsp;It would also be good to run robust statistical comparisons of the human decay curves versus the AI agent decay curves to see if there are systematic differences (e.g. different half-lives or different shapes that aren’t just an artefact of the experimental setup). And of course it is also important to know how much any of this generalises to other suites of tasks.</p><h3><strong>References</strong></h3><p>Thomas Kwa et al., <a href="https://arxiv.org/abs/2503.14499"><span>Measuring AI Ability to Complete Long Tasks</span></a>, arXiv:2503.14499 [cs.AI], 2025.</p><p>Toby Ord, <a href="https://arxiv.org/abs/2308.09045"><span>The Lindy Effect</span></a>. arXiv:2308.09045 [physics.soc-ph], 2023.</p>
</div></div>

    <!--BLOG INJECTION-->

    <!-- MathJax: Reset equation counter after every blog entry -->
$\setCounter{0}$    

    <!--CATEGORIES-->

    

  </article>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terpstra Keyboard (218 pts)]]></title>
            <link>http://terpstrakeyboard.com/web-app/keys.htm</link>
            <guid>44308558</guid>
            <pubDate>Wed, 18 Jun 2025 10:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://terpstrakeyboard.com/web-app/keys.htm">http://terpstrakeyboard.com/web-app/keys.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44308558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="landing-page">
		<div>
		  
		  <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
          </p>

			<form id="settingsForm">
				<div>
					<div>
						<p><label>Tuning\Layout Quick Links</label>
                          

                          <label>Fundamental (Hz)</label>
                          
                       </p>

						<p><label>Right Facing Steps</label>
                          

                          <label>Up/Right Facing Steps</label>
                          
						</p>
                    </div>

					<div>
						<p><label>Hex Size (pixels)</label>
                          

                          <label>Rotation (degrees)</label>
                          
						</p>

						<p><label>Instrument</label>
                          

                          <label>
                              
                              <span>Enumerate Scale</span>
                          </label>

                          <label>
                              
                              <span>Use Spectrum Colors</span>
                          </label>

                          <label>
                              
                              <span>Blank Keys (No labels)</span>
                          </label>
						</p>
					</div>
				</div>

                <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
                </p>

				<div>
					<p><label>Scale (<a href="http://www.huygens-fokker.org/scala/scl_format.html" target="new">Scala format</a>)</label>
						
                  </p>

                  <div>
                    <p><label id="numberLabel">Steps To Equivalence Interval</label>
                      

                      
                      
					</p>

                    <p>
                        

                        <label id="note_colorsLabel">Color Layout</label>
                        
                    </p>
                  </div>
				</div>
				<br>
				
            </form>
        </div>

		<p>
          Designed by <a href="http://siementerpstra.com/" target="new">Siemen Terpstra</a> in the late ’80’s. WebApp developed by <a href="http://jamesfenn.com/" target="new">James Fenn</a> with additions and modifications by <a href="http://brandlew.com/" target="new">Brandon Lewis</a>, <a href="http://whatmusicreallyis.com/" title="What Music Really İs" target="new">Bo Constantinsen</a> and <a href="https://sites.google.com/site/wangchengu/" target="new">Chengu Wang</a>. Credits to Scott Thompson and <a href="http://ozanyarman.com/" target="new">Dr Ozan Yarman</a> for contributing samples. Current version 1.5.2 (Jan. 2015 — May 2019), released as Free/Libre and Open Source Software under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="new">GPL-3.0</a>. Download, fork, and get your name down here by fixing issues and implementing features via <a href="https://github.com/wcgbg/terpstrakeyboard/" target="new">GitHub</a>!
		</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I counted all of the yurts in Mongolia using machine learning (229 pts)]]></title>
            <link>https://monroeclinton.com/counting-all-yurts-in-mongolia/</link>
            <guid>44307629</guid>
            <pubDate>Wed, 18 Jun 2025 07:58:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monroeclinton.com/counting-all-yurts-in-mongolia/">https://monroeclinton.com/counting-all-yurts-in-mongolia/</a>, See on <a href="https://news.ycombinator.com/item?id=44307629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The <em>Fall of Civilizations</em> podcast put out a <a href="https://www.youtube.com/watch?v=YyqS9V7yHQA">6¾-hour episode</a>
on the history of the Mongol Empire, which I eagerly listened to. After finishing the episode I wondered
about contemporary Mongolian society, I wanted to learn what the lands that the Mongol Empire
exploded from are like in our current day. There are many ways to try to understand a society,
whether it be quantifying it or looking at the lived experiences within it. If you look at
data provided by the World Bank, you’ll see a country that has rapidly reduced poverty in the 21st
century, has a high economic growth rate, a healthy fertility rate, and is solidly an
upper-middle-income country. While Mongolia is a republic with a competitive party system,
<a href="https://www.worldbank.org/en/publication/worldwide-governance-indicators/interactive-data-access">Worldwide Governance Indicators</a>
from the World Bank show a government that has issues with corruption, regulatory quality, and effectiveness.</p>
<table>
<thead>
<tr>
<th>Indicator</th>
<th>Value</th>
<th>Years</th>
</tr>
</thead>
<tbody>
<tr>
<td>Population</td>
<td>3,481,145</td>
<td>2023</td>
</tr>
<tr>
<td>Fertility rate</td>
<td>2.7</td>
<td>2023</td>
</tr>
<tr>
<td>Intentional homicides (per 100,000 people)</td>
<td>6</td>
<td>2021</td>
</tr>
<tr>
<td>Individuals using the Internet (% of population)</td>
<td>1% → 83%</td>
<td>2000 → 2023</td>
</tr>
<tr>
<td>Poverty headcount ratio at $2.15 a day (2017 PPP) (% of population)</td>
<td>11.6% → 0.2%</td>
<td>2002 → 2022</td>
</tr>
<tr>
<td>Average GDP growth</td>
<td>6.62%</td>
<td>2003 → 2023</td>
</tr>
<tr>
<td>GDP per capita, PPP (current international $)</td>
<td>$4,399.4 → $18,004.9</td>
<td>2003 → 2023</td>
</tr>
</tbody>
</table>
<blockquote>
<p>(“Mongolia”)</p>
</blockquote>
<p>All of these indicators are interesting to look at, but they don’t really show what a society is
like. I feel you get much more understanding by going to a country, walking the streets, and
talking to people there. If you’re unable to do this, the next best thing is spending hours
exploring Google Maps, which I did. I opened a satellite view of Ulaanbaatar, the capital of Mongolia.
I saw new glass buildings, Soviet-designed apartment blocks (called ugsarmal), impressive government
buildings, factories, and industrial areas. But something stood out to me. Yurts, extending for kilometers
in all directions.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/yurts-1.jpg" alt="Satellite view of Ulaanbaatar containing yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>Naturally, I was impressed by the quantity of yurts I saw, and I was curious: just how many yurts (ger in Mongolian) are in
Mongolia and why? This set me on the path drawing bounding boxes on over 10,000 yurts to train a machine learning
model to count the rest of the yurts in the country. While I was training the model, I wondered what
the story behind these yurts are, I did a small investigation for later in this article. For now,
this is the story of counting them.</p>
<h2 id="counting-all-the-yurts-in-mongolia">Counting all the yurts in Mongolia</h2>
<p>I was unable to find a count of the yurts in Mongolia, this left me with
the task of doing it myself. Although I had never studied or worked with machine learning, I knew
through some osmosis that machine learning is well fit for this task. I created a simple plan in my
brain:</p>
<ol>
<li>Train a model to identify yurts</li>
<li>Reduce input space and parallelize searching of input space</li>
<li>Keep track of the yurts found</li>
</ol>
<h3 id="training-a-model-to-identify-yurts">Training a model to identify yurts</h3>
<p>The first thing I needed was training data, and lots of it. There’s many different options for satellite
imagery such as <a href="https://www.mapbox.com/imagery">Mapbox</a>, <a href="https://developers.google.com/maps/documentation/tile">Google Maps</a>,
and <a href="https://developers.arcgis.com/rest/basemap-styles/arcgis-imagery-webmap-get/">ArcGIS</a>. I
decided to use Google Maps since I’m already familiar with it.</p>
<p>For digital maps, many systems break the world up into a series of 256 x 256 tiles identified by X, Y, Z values. This is
referred to as tiled web maps and allows for progressively loading maps at different zoom levels and
positions. The zoom level values tend to be 0 through 20, where 0 has the least tiles and 20 the
most. The formula for calculating the number of tiles at a given zoom (z) level is: <span> $2^z * 2^z$ </span>
.
This means increasing <code>z</code> by one will increase the tile count by four times.</p>
<p>I wrote a Python script that generated tiles from a box around Ulaanbaatar and downloaded
them to a folder to use as training data. To list the tiles inside a bounding box made up of a
southwest and northeast coordinates, I used the <a href="https://mercantile.readthedocs.io/en/latest/">mercantile package</a>.</p>
<div><pre tabindex="0"><code data-lang="python"><span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>sw_lng</span><span>,</span> <span>sw_lat</span><span>,</span> <span>ne_lng</span><span>,</span> <span>ne_lat</span><span>,</span> <span>zooms</span><span>=</span><span>z</span><span>):</span>
    <span>download_tile</span><span>(</span><span>*</span><span>tile</span><span>)</span>
</code></pre></div><p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-1.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-2.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-3.jpeg" alt="Sample tile from Google Maps"></p>
<blockquote>
<p>Tiles from Google Maps, you can see yurts on the right tile. Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>I decided to start at zoom level <code>17</code> as it is the lowest zoom level that I can still identify yurts
at. Once I downloaded several hundred tiles at this zoom level, I needed a way to label the yurts on
these tiles. Labeling is the process of drawing boxes around objects in an image. The idea is to
draw these boxes manually, creating what is called annotated data, and then training a model to do
the labeling using the annotated data.
There’s an open source tool called <a href="https://labelstud.io/">Label Studio</a> that does just
this.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/label-studio.jpg" alt="Label Studio showing yurts labeled"></p>
<blockquote>
<p>Here I drew bounding boxes on the tile around the yurts.</p>
</blockquote>
<p>A couple dozen yurts later and I wanted to try and train a model based on my tiny amount of
annotated data. I had the choice between object detection (bounding boxes) and segmentation (outline
objects). Segmentation probably would be more accurate because yurts are not rectangular,
but it seemed like it would take longer to setup. I decided to go with object detection.</p>
<p>I looked at various ways to train an object detection model, my requirements were:</p>
<ul>
<li>Open source</li>
<li>As simple as possible to setup</li>
<li>Able to quickly iterate</li>
<li>Detection speed of the model is a priority due to the potentially large amount of data</li>
<li>Has good default settings around data augmentation, warmups, loss functions, etc</li>
<li>Monitor current and previous training runs to compare accuracy</li>
</ul>
<p>After doing a brief survey of the machine learning landscape, I landed on using <a href="https://docs.ultralytics.com/">YOLO11</a> by Ultralytics.
The YOLO series is a set of models that can complete computer vision tasks, and can be trained with
custom data.
In Label Studio you’re able to export to many different dataset types, YOLO being one of them. After
exporting my annotated data as a YOLO dataset, I split the dataset into training and validation data
and configured the dataset in <code>dataset.yaml</code> for YOLO to use.</p>





<div><pre tabindex="0"><code data-lang="yaml"><span>train</span><span>:</span><span> </span><span>images/train</span><span>
</span><span></span><span>val</span><span>:</span><span> </span><span>images/val</span><span>
</span><span>
</span><span></span><span>nc</span><span>:</span><span> </span><span>1</span><span>
</span><span></span><span>names</span><span>:</span><span>
</span><span>  </span>- <span>yurt</span></code></pre></div>

<p>From the ultralytics package, I used the YOLO class to use their pre-trained <code>yolo11n</code> object
detection model. Ultralytics allows easy tuning of the model with annotated data through the <code>train</code>
method of the <code>YOLO</code> class. The tuned model can be exported through <code>export</code> in various formats.</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> <span>ultralytics</span> <span>import</span> <span>YOLO</span>

<span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>
<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>device</span><span>=</span><span>"cpu"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
</code></pre></div><p>With some testing I found my Yurt model was less than adequate, which I expected due to the tiny
amount of annotated data. I then did a couple hours of labeling, but the model would always miss
around 10-15% of the yurts in a given tile. At this point I had two options, either increase the
zoom level or gather more training data. To base my decision I decided to calculate how many tiles I
would need to search at each zoom level.</p>
<h3 id="refining-the-search-area">Refining the search area</h3>
<p>Mongolia is 1,564,116 square kilometers, using this we can calculate how many tiles at each zoom
level there are in Mongolia. The world has <span> $2^z * 2^z$ </span>
 tiles, so
on a single axis there are <span> $2^z$ </span>
 tiles. The map projection is from a sphere
a tile will represent more or less area depending on the latitude.
To find the width of the projection at a latitude for Web Mercator, we can
use this formula where <span>$R = 6,378.137$</span>
 is the radius of the equator in kilometers and <span>$\phi = 47.923107575288114$</span>
 is the
latitude of Mongolia in degrees which is converted to radians:</p>
<p><span> $$2\pi * R * \cos(\phi * \dfrac{\pi}{180}) = 26,855.3636571$$</span></p><p>We then need to divide the number of tiles on the x-axis at this location to get the width of a
tile. For the area of a tile, just square the width and divide the area of Mongolia by the area of a
single tile to get the tile count.</p>
<table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>37,258,617</td>
</tr>
<tr>
<td>18</td>
<td>149,034,469</td>
</tr>
<tr>
<td>19</td>
<td>596,137,879</td>
</tr>
<tr>
<td>20</td>
<td>2,384,551,518</td>
</tr>
</tbody>
</table>
<p>Since Mongolia is such a large country, I began to wonder if there are more ways to reduce the
amount of tiles other than just zoom level. It’s a sparsely populated country, with much of the
country being uninhabited. Also, nearly all yurts are located in urban areas, with the City of
Ulaanbaatar estimating 60% of the population lives in ger (yurt) districts (City of Ulaanbaatar 17).</p>
<p>I used <a href="https://overpass-turbo.eu/">overpass turbo</a> to do a query for all the places human settlements might be in the
country and exported this data as GeoJSON. The query returned several thousand points of interest.</p>
<pre tabindex="0"><code>[out:json][timeout:25];
{{geocodeArea:Mongolia}}-&gt;.searchArea;
(
  node[place](area.searchArea);
  node[man_made](area.searchArea);
  node[historic](area.searchArea);
);
out body;
&gt;;
out skel qt;
</code></pre><p>I wanted to know how many unique tiles for searching a 2,000 meter area around each point there are,
so I wrote a script to do this using geopandas.</p>
<div><pre tabindex="0"><code data-lang="python"><span>gdf</span> <span>=</span> <span>gpd</span><span>.</span><span>read_file</span><span>(</span><span>"./mongolia.geojson"</span><span>)</span>
<span>gdf_merc</span> <span>=</span> <span>gdf</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:3857"</span><span>)</span>
<span>gdf_merc</span><span>[</span><span>"buffer"</span><span>]</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>geometry</span><span>.</span><span>buffer</span><span>(</span><span>2000</span><span>)</span>

<span>gdf_buffer</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>set_geometry</span><span>(</span><span>"buffer"</span><span>)</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:4326"</span><span>)</span>

<span>tiles</span> <span>=</span> <span>{}</span>
<span>for</span> <span>polygon</span> <span>in</span> <span>gdf_buffer</span><span>.</span><span>geometry</span><span>:</span>
    <span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span> <span>=</span> <span>polygon</span><span>.</span><span>bounds</span>

    <span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span><span>,</span> <span>zooms</span><span>=</span><span>Z</span><span>):</span>
        <span>tiles</span><span>[</span><span>"</span><span>{}</span><span>-</span><span>{}</span><span>-</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>str</span><span>(</span><span>tile</span><span>.</span><span>x</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>y</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>z</span><span>))]</span> <span>=</span> <span>True</span>
</code></pre></div><table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>270,559</td>
</tr>
<tr>
<td>18</td>
<td>1,016,617</td>
</tr>
<tr>
<td>19</td>
<td>3,938,174</td>
</tr>
<tr>
<td>20</td>
<td>15,506,872</td>
</tr>
</tbody>
</table>
<h3 id="building-a-model-backend-for-labeling">Building a model backend for labeling</h3>
<p>To speed up the labeling of yurts I wanted Label Studio to use my model to label yurts.
Label Studio has the ability to integrate with a model backend,
essentially an API wrapper around a model, to request predictions. When labeling a tile, Label
Studio makes a request to this API for predictions. The API returns the bounding boxes for the tile.
I fix any mistakes the model made, and submit the tile. Every so often I retrain the model, creating
a feedback loop that improves the model with more and more annotated data.</p>





<div><pre tabindex="0"><code data-lang="python"><span>class</span> <span>YurtModel</span><span>:</span>
    <span># Initialize trained model to reuse across requests</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"best.pt"</span><span>,</span> <span>task</span><span>=</span><span>"detect"</span><span>)</span>

    <span># Task a task sent by Label Studio, and return bounding boxes of yurts</span>
    <span>def</span> <span>predict</span><span>(</span><span>self</span><span>,</span> <span>tasks</span><span>):</span>
        <span>predictions</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>task</span> <span>in</span> <span>tasks</span><span>:</span>
            <span># Get the path to the file from label studio</span>
            <span>path</span> <span>=</span> <span>get_local_path</span><span>(</span>
                <span>task</span><span>[</span><span>"data"</span><span>][</span><span>"image"</span><span>],</span>
                <span>task_id</span><span>=</span><span>task</span><span>[</span><span>"id"</span><span>],</span>
            <span>)</span>

            <span>results</span> <span>=</span> <span>self</span><span>.</span><span>model</span><span>(</span><span>path</span><span>)</span>

            <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
                <span>regions</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
                    <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>
                    <span>regions</span><span>.</span><span>append</span><span>({</span>
                        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                        <span>"from_name"</span><span>:</span> <span>"label"</span><span>,</span>
                        <span>"to_name"</span><span>:</span> <span>"image"</span><span>,</span>
                        <span>"type"</span><span>:</span> <span>"rectanglelabels"</span><span>,</span>
                        <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
                        <span>"value"</span><span>:</span> <span>{</span>
                            <span>"x"</span><span>:</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"y"</span><span>:</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"width"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"height"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"rectanglelabels"</span><span>:</span> <span>[</span>
                                <span>"yurt"</span><span>,</span>
                            <span>],</span>
                        <span>},</span>
                    <span>})</span>

                <span>all_scores</span> <span>=</span> <span>[</span><span>region</span><span>[</span><span>"score"</span><span>]</span> <span>for</span> <span>region</span> <span>in</span> <span>regions</span> <span>if</span> <span>"score"</span> <span>in</span> <span>region</span><span>]</span>
                <span>avg_score</span> <span>=</span> <span>sum</span><span>(</span><span>all_scores</span><span>)</span> <span>/</span> <span>max</span><span>(</span><span>len</span><span>(</span><span>all_scores</span><span>),</span> <span>1</span><span>)</span>

                <span>predictions</span><span>.</span><span>append</span><span>({</span>
                    <span>"result"</span><span>:</span> <span>regions</span><span>,</span>
                    <span>"score"</span><span>:</span> <span>avg_score</span><span>,</span>
                    <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                <span>})</span>

        <span>return</span> <span>{</span>
            <span>"results"</span><span>:</span> <span>predictions</span><span>,</span>
        <span>}</span>

<span>model</span> <span>=</span> <span>YurtModel</span><span>()</span></code></pre></div>

<p>We then need to fill out the API routes that Label Studio expects, which is a <code>/predict</code> route for
label studio to send tiles and receive predictions, a <code>/setup</code> route to do any initialization
required, and a <code>/health</code> route to do health checks on. I used <a href="https://fastapi.tiangolo.com/">FastAPI</a> to build the API and use
the <code>YurtModel</code> from above.</p>





<div><pre tabindex="0"><code data-lang="python"><span>@app</span><span>.</span><span>post</span><span>(</span><span>"/predict"</span><span>)</span>
<span>async</span> <span>def</span> <span>predict</span><span>(</span><span>request</span><span>:</span> <span>Request</span><span>):</span>
    <span>res</span> <span>=</span> <span>await</span> <span>request</span><span>.</span><span>json</span><span>()</span>
    <span>return</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>res</span><span>[</span><span>"tasks"</span><span>])</span>

<span>@app</span><span>.</span><span>post</span><span>(</span><span>"/setup"</span><span>)</span>
<span>async</span> <span>def</span> <span>setup</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
    <span>}</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>"/health"</span><span>)</span>
<span>async</span> <span>def</span> <span>health</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"status"</span><span>:</span> <span>"UP"</span><span>,</span>
        <span>"model_class"</span><span>:</span> <span>str</span><span>(</span><span>YurtModel</span><span>.</span><span>__class__</span><span>),</span>
    <span>}</span></code></pre></div>

<p>By relying on the model to find most of the yurts when labeling, I was able to rapidly create more
annotated data. I quickly built a dataset of over 10,000 yurts.</p>
<h3 id="monitoring-accuracy-of-each-model">Monitoring accuracy of each model</h3>
<h3 id="scaling-training-of-models">Scaling training of models</h3>
<p>As the size of the annotated data grew, training the models on my laptop became too slow.
I decided to use <a href="https://vast.ai/">vast.ai</a> to rent GPUs to do my training runs. To train
the models on vast.ai, I needed everything to run in Docker. I wrote a Dockerfile for the training
script, and I pushed it to <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>.
In vast.ai I set up authentication with the private image registry so it could pull the image I pushed up.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/docker-auth.png" alt="vast.ai Docker authentication"></p>
<blockquote>
<p>Docker authentication in vast.ai</p>
</blockquote>
<p>Here is the Dockerfile that I used to run the training script on the dataset I created.</p>
<div><pre tabindex="0"><code data-lang="shell">FROM ghcr.io/astral-sh/uv:python3.10-bookworm-slim

WORKDIR /app

<span># Copy training script, annotated data, and requirements to image</span>
COPY scripts/train_model.py .
COPY datasets ./datasets
COPY dataset.yaml .
COPY pyproject.toml .
COPY uv.lock .

<span># Needed for ...</span>
RUN apt-get update -y <span>&amp;&amp;</span> apt-get install -y libgl1-mesa-dev libglib2.0-0

<span># Install package requirements</span>
RUN uv sync --no-dev

<span># Run the training script</span>
CMD <span>[</span><span>"uv"</span>, <span>"run"</span>, <span>"python"</span>, <span>"train_model.py"</span><span>]</span>
</code></pre></div><p>In order to build and push this image to GitHub I ran:</p>
<div><pre tabindex="0"><code data-lang="shell">docker build -t ghcr.io/monroeclinton/yurt -f Dockerfile .
docker push ghcr.io/monroeclinton/yurt:latest
</code></pre></div><p>Since the training happened in ephemeral containers, I needed a way to retrieve the finished model. I
decided to upload the model to S3 after it finished training. To monitor the accuracy of the
models, I also needed the metadata associated with the runs, so I uploaded everything in the
run folder to S3.</p>





<div><pre tabindex="0"><code data-lang="python"><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>

<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>epochs</span><span>=</span><span>1000</span><span>,</span>
    <span>patience</span><span>=</span><span>150</span><span>,</span>
    <span>imgsz</span><span>=</span><span>256</span><span>,</span>
    <span>device</span><span>=</span><span>"cuda"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
<span>train_dir</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>path</span><span>))</span>

<span>s3</span> <span>=</span> <span>boto3</span><span>.</span><span>client</span><span>(</span>
    <span>service_name</span> <span>=</span><span>"s3"</span><span>,</span>
    <span>endpoint_url</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ENDPOINT"</span><span>],</span>
    <span>aws_access_key_id</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ACCESS_KEY_ID"</span><span>],</span>
    <span>aws_secret_access_key</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_SECRET_ACCESS_KEY"</span><span>],</span>
    <span>region_name</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_REGION"</span><span>],</span>
<span>)</span>

<span>timestamp</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

<span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>train_dir</span><span>):</span>
    <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
        <span>local_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
        <span>s3_key</span> <span>=</span> <span>f</span><span>"models/</span><span>{</span><span>int</span><span>(</span><span>timestamp</span><span>)</span><span>}</span><span>/</span><span>{</span><span>os</span><span>.</span><span>path</span><span>.</span><span>relpath</span><span>(</span><span>local_path</span><span>,</span> <span>train_dir</span><span>)</span><span>}</span><span>"</span>
        <span>s3</span><span>.</span><span>upload_file</span><span>(</span><span>local_path</span><span>,</span> <span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_BUCKET"</span><span>],</span> <span>s3_key</span><span>)</span></code></pre></div>

<h3 id="deploying-models-and-searching-mongolia">Deploying models and searching Mongolia</h3>
<p>After dozens of training runs and greatly improving the accuracy of the model, I decided to finally
do my count of Mongolia. There were many options to run my deployment, however I made my choice based on three
criteria:</p>
<ul>
<li>Simplicity in setup and deployment</li>
<li>At least 100 instances of my model should be run</li>
<li>The bottleneck is I/O (downloading tiles), so should be deployed on many CPUs</li>
</ul>
<p>Based on these criteria, I used <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> to orchestrate the workload.
It’s already packaged in Docker, so there’s no need to install anything else. Docker Swarm also is
fairly simple to set up, scale, and deploy services with. I rented eight servers, each with 16 vCPUs
(128 vCPUs total), and connected them over a private network.</p>
<p>I picked one server to be the <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes">manager node</a>.
On this server, I ran this to initialize the swarm:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm init --advertise-addr 10.0.0.2
</code></pre></div><p>This command sets up the swarm and prints a command to run on the worker nodes to connect them to
the manager. Each worker node joined using the token and the manager’s address:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm join --token SWARM_TOKEN 10.0.0.2:2377
</code></pre></div><p>I deployed the container images, which I had pushed to GHCR, and pulled with
<code>--with-registry-auth</code> to allow access from the server to GHCR.
There were two images, the <code>api</code> image and the <code>worker</code> image. The API managed a list of
search areas (the areas around the points found from overpass turbo), giving
search areas to workers, and expanding the search radius by 500 meters when yurts were found.
The workers requested search areas from the API and sent back a list of yurts found within the
search areas.</p>
<h4 id="api">API</h4>
<p>I used FastAPI to build the API, in which there were two routes.</p>
<ul>
<li>GET /search-area - Workers sent a request to this route to get a search area to search.</li>
</ul>
<p>This route first checks if there are any stale areas, where a worker had requested a search area
but never finished it. The workers should send periodic health checks to the API, if this fails then
it will return the search area to a different worker after one minute.</p>
<div><pre tabindex="0"><code data-lang="python"><span>stale_area</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>SearchArea</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>searching</span> <span>==</span> <span>True</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>health_check</span> <span>&lt;</span> <span>one_minute_ago</span><span>)</span>
    <span>.</span><span>with_for_update</span><span>()</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>
</code></pre></div><p>If there are no stale search areas, then a new point will be selected at random, and the search area
will be increased if there have been previous searches.</p>
<div><pre tabindex="0"><code data-lang="python"><span>point</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Point</span><span>)</span>
    <span>.</span><span>options</span><span>(</span><span>joinedload</span><span>(</span><span>Point</span><span>.</span><span>search_areas</span><span>))</span>
    <span>.</span><span>filter</span><span>(</span><span>Point</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>~</span><span>Point</span><span>.</span><span>search_areas</span><span>.</span><span>any</span><span>(</span><span>SearchArea</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>))</span>
    <span>.</span><span>order_by</span><span>(</span><span>func</span><span>.</span><span>random</span><span>())</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>

<span>if</span> <span>not</span> <span>point</span><span>:</span>
    <span>raise</span> <span>HTTPException</span><span>(</span>
        <span>status_code</span><span>=</span><span>404</span><span>,</span> <span>detail</span><span>=</span><span>"No available point to search"</span><span>)</span>

<span>previous_areas</span> <span>=</span> <span>[</span><span>sa</span> <span>for</span> <span>sa</span> <span>in</span> <span>point</span><span>.</span><span>search_areas</span><span>]</span>
<span>if</span> <span>previous_areas</span><span>:</span>
    <span>max_meters</span> <span>=</span> <span>max</span><span>(</span><span>area</span><span>.</span><span>meters</span> <span>for</span> <span>area</span> <span>in</span> <span>previous_areas</span><span>)</span>
    <span>new_meters</span> <span>=</span> <span>max_meters</span> <span>+</span> <span>500</span>
<span>else</span><span>:</span>
    <span>new_meters</span> <span>=</span> <span>500</span>
</code></pre></div><p>A <code>SearchArea</code> has a list of tiles that are inside it. Each <code>Tile</code> has as status of <code>searched</code>.
I used geopandas, as shown earlier, to generate a bounding box over the search area and create a list
of tiles. For each of these tiles, I check the database to see if they have already been created + searched.
If they haven’t then they are upserted and assigned to the search area.</p>
<div><pre tabindex="0"><code data-lang="python"><span>created_tiles</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Tile</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span>
        <span>tuple_</span><span>(</span><span>Tile</span><span>.</span><span>x</span><span>,</span> <span>Tile</span><span>.</span><span>y</span><span>,</span> <span>Tile</span><span>.</span><span>z</span><span>)</span><span>.</span><span>in_</span><span>(</span>
            <span>[(</span><span>tile</span><span>[</span><span>"x"</span><span>],</span> <span>tile</span><span>[</span><span>"y"</span><span>],</span> <span>tile</span><span>[</span><span>"z"</span><span>])</span>
             <span>for</span> <span>tile</span> <span>in</span> <span>tiles_to_create</span><span>]</span>
        <span>)</span>
    <span>)</span>
    <span>.</span><span>all</span><span>()</span>
<span>)</span>

<span>new_area</span><span>.</span><span>tiles</span><span>.</span><span>extend</span><span>(</span><span>created_tiles</span><span>)</span>
</code></pre></div><p>The route returns the search area, containing a list of tiles to search.</p>
<ul>
<li>POST /search-area/:id - Workers sent a request containing the yurts to this route.</li>
</ul>
<p>This route inserts the yurts into the database, and marks the <code>Point</code>, <code>SearchArea</code>, and <code>Tile</code> as
searched as needed. The <code>Point</code> gets marked as searched if no yurts are found, and the <code>SearchArea</code>
and <code>Tile</code> are marked as searched.</p>
<div><pre tabindex="0"><code data-lang="python"><span>if</span> <span>len</span><span>(</span><span>yurts_to_create</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>stmt</span> <span>=</span> <span>insert</span><span>(</span><span>Yurt</span><span>)</span><span>.</span><span>values</span><span>(</span><span>yurts_to_create</span><span>)</span>
    <span>stmt</span> <span>=</span> <span>stmt</span><span>.</span><span>on_conflict_do_nothing</span><span>(</span>
        <span>index_elements</span><span>=</span><span>[</span><span>"longitude"</span><span>,</span> <span>"latitude"</span><span>])</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span><span>stmt</span><span>)</span>
<span>else</span><span>:</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span>
        <span>update</span><span>(</span><span>Point</span><span>)</span><span>.</span><span>where</span><span>(</span>
            <span>exists</span><span>()</span><span>.</span><span>where</span><span>(</span>
                <span>(</span><span>SearchArea</span><span>.</span><span>point_id</span> <span>==</span> <span>Point</span><span>.</span><span>id</span><span>)</span> <span>&amp;</span> <span>(</span><span>Point</span><span>.</span><span>id</span> <span>==</span> <span>id</span><span>)</span>
            <span>)</span>
        <span>)</span><span>.</span><span>values</span><span>(</span>
            <span>searched</span><span>=</span><span>True</span><span>,</span>
        <span>)</span>
    <span>)</span>
</code></pre></div><h4 id="worker">Worker</h4>
<p>The worker script ran in a loop until it encountered the <code>No available point to search</code> error.
This loop consisted of requesting the <code>/search-area</code> to get a list of tiles to search, downloading
each tile, then passing the tile image to the model to detect yurts. Finally, the worker sends a
list of yurts to the API.</p>
<div><pre tabindex="0"><code data-lang="python"><span>def</span> <span>find_yurts</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>):</span>
    <span>filepath</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>"tiles"</span><span>,</span> <span>"</span><span>{}</span><span>_</span><span>{}</span><span>_</span><span>{}</span><span>.jpeg"</span><span>.</span><span>format</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>))</span>

    <span>results</span> <span>=</span> <span>model</span><span>(</span><span>filepath</span><span>,</span> <span>imgsz</span><span>=</span><span>256</span><span>)</span>

    <span>yurts</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
        <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
            <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>

            <span># Find center of the bounding box</span>
            <span>pixel_x</span> <span>=</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>2</span>
            <span>pixel_y</span> <span>=</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>2</span>

            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>tile_xyz_to_lonlat</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>,</span> <span>pixel_x</span><span>,</span> <span>pixel_y</span><span>)</span>

            <span>yurts</span><span>.</span><span>append</span><span>({</span>
                <span>"latitude"</span><span>:</span> <span>lat</span><span>,</span>
                <span>"longitude"</span><span>:</span> <span>lon</span><span>,</span>
                <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
            <span>})</span>

    <span>return</span> <span>yurts</span>
</code></pre></div><p>I began scaling this service slowly and eventually ramped up to 120 workers running in parallel
using <code>docker service scale worker=120</code>. Each container processed its assigned tile, and if yurts
were found, posted their coordinates to the API.</p>
<h3 id="the-resulting-count">The resulting count</h3>
<p>After searching a couple million tiles I downloaded the yurt dataset, which I uploaded <a href="https://cdn.monroeclinton.com/yurts.json">here (12mb file)</a>.
In total I found 172,689 yurts with a prediction score of greater than 40%.</p>
<p>Perhaps there’s some lonesome yurts far in the Gobi Desert or the Altai Mountains I missed, so we
could add a hundred or so for those. I could have also done more like
providing image context and training on more data from smaller towns, but I only have so much time.</p>
<p>For fun I did some querying using <a href="https://postgis.net/">PostGIS</a> to find areas with high concentrations
of yurts. Generally I found places that are hotels or remote areas near mines.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/many-yurts.jpeg" alt="Many yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<h2 id="the-people-of-the-yurts">The people of the yurts</h2>
<p>Historically, yurts have been a home for the nomadic peoples of the steppe to live. As
Mongolia developed into the modern world, the usage of yurts changed with the country. For example,
I found a reference to yurts being used as makeshift schools in the early 1900s. This period was the
start of the transformation from a nomadic herder society to an urban industrial society.</p>
<blockquote>
In the rural areas, in addition to the existing 60 scribe schools, at least 49 state primary schools were established by 1917. They were largely housed in yurts and financed with state, municipal, and private funds. (Steiner-Khamsi and Stolpe 36)
</blockquote>

<p>This reflects the developmental history of Mongolia, and how people are adjusting to the modern
world. Mongolia has transitioned from a mostly nomadic herder society, to a mostly urbanized
industrial society. As people transition from one system of life to another, remnants of their old
system persist. Housing and infrastructure are expensive, so as Mongolia transformed, once nomadic
herders took their yurts to urban areas and continued living in them.</p>
<blockquote>
The 51 percent urban population reported in the 1979 census
reflected rapid migration to the cities in the 1970s. The influx of
rural people created housing problems, among them long waits for
assignment to an apartment, expansion of ger districts on the edges
of built-up areas, and pressure to invest in more housing, roads,
and other urban infrastructure. (Worden et al. 86)
</blockquote>

<p>Due to the large number of people moving to urban locations, it has been difficult for the government to
build the infrastructure needed for them. The informal settlements that grew from this difficulty
are now known as ger districts. There have been many efforts to formalize and develop these areas.
The Law on Allocation of Land to Mongolian Citizens for Ownership, passed in 2002, allowed for
existing ger district residents to formalize the land they settled, and allowed for others to
receive land from the government into the future.</p>
<p>Along with the privatization of land, the Mongolian government has been pushing for the development
of ger districts into areas with housing blocks connected to utilities. The plan for this was
published in 2014 as Ulaanbaatar 2020 Master Plan and Development Approaches for 2030. Although
progress has been slow (Choi and Enkhbat 7), they have been making progress in building housing blocks in ger
distrcts. Residents of ger districts sell or exchange their plots to developers who then build housing
blocks on them. Often this is in exchange for an apartment in the building, and often the value of the
apartment is less than the land they originally had (Choi and Enkhbat 15).</p>
<p>Based on what I’ve read about the ger districts, they have been around since at least the 1970s,
and progress on developing them has been slow. When ineffective policy results in a large chunk of
the populace generationally living in yurts on the outskirts of urban areas, it’s clear that there
is failure.
One of the most important functions of government is inspiring the citizenry to achieve greatness.
Most governments around the world fail in this, but we should all work towards it. I think a step
the Mongolian government could take for this is to analyze which policy failures have led to such
slow progress on the ger district issue.</p>
<p>The Mongolian government’s long-term vision is to provide utilities and good housing
for these areas. Although I can’t contribute anything to this vision, I wish for the best
success in this plan.
I’m glad to have learned about a country and people I used to know nothing about. Hopefully in the
future I’ll study more about Mongolia, but for now I’m off to my next project.</p>
<h3 id="further-questions">Further questions</h3>
<ul>
<li>What causes Mongolia and other countries to urbanize and industrialize?</li>
<li>Why do some Mongolians head to the cities and others stay?</li>
<li>What challenges does the Mongolian government face in developing ger districts?</li>
<li>What causes the difference in speed of development between countries?</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><p>Choi, Mack Joong, and Urandulguun Enkhbat. “Distributional Effects of Ger Area Redevelopment in Ulaanbaatar, Mongolia.” International Journal of Urban Sciences, vol. 24, no. 1, Jan. 2020, pp. 50–68. DOI.org (Crossref), <a href="https://doi.org/10.1080/12265934.2019.1571433">https://doi.org/10.1080/12265934.2019.1571433</a>.</p>

</li>
<li><p>City of Ulaanbaatar. <em>Ulaanbaatar 2020 Master Plan and Development Approach for 2030.</em> 2014.</p>

</li>
<li>

</li>
<li><p>Steiner-Khamsi, Gita, and Ines Stolpe. <em>Educational Import: Local Encounters with Global Forces in Mongolia.</em> 1st ed, Palgrave Macmillan, 2006.</p>

</li>
<li><p>Worden, Robert L, et al. <em>Mongolia: A Country Study.</em> Washington, D.C.: Federal Research Division, Library of Congress: For sale by the Supt. of Docs., U.S. G.P.O, 1991. Pdf. Retrieved from the Library of Congress, &lt;www.loc.gov/item/90006289/&gt;.</p>

</li>
<li><p>Yang, Jeasurk, et al. <em>Poverty Mapping in Mongolia with AI-Based Ger Detection Reveals Urban Slums Persist after the COVID-19 Pandemic.</em> arXiv:2410.09522, arXiv, 12 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.09522">https://doi.org/10.48550/arXiv.2410.09522</a>.</p>

</li>
</ul>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MiniMax-M1 open-weight, large-scale hybrid-attention reasoning model (326 pts)]]></title>
            <link>https://github.com/MiniMax-AI/MiniMax-M1</link>
            <guid>44307290</guid>
            <pubDate>Wed, 18 Jun 2025 06:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a>, See on <a href="https://news.ycombinator.com/item?id=44307290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source srcset="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Dark.png" media="(prefers-color-scheme: dark)">
      <img src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Light.png" width="60%" alt="MiniMax">
    
  </picture></themed-picture>
</div>
<hr>
<p><a href="https://www.minimax.io/" rel="nofollow">
    <img alt="Homepage" src="https://camo.githubusercontent.com/3faa1e14d767d4a75cf9ed5610309fbb6fe899cec1f03f659f57e5961de37e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f486f6d65706167652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://arxiv.org/abs/2506.13585" rel="nofollow">
    <img alt="Paper" src="https://camo.githubusercontent.com/2604333fbd9843f0be50433505993398230120aca88af3c4dc94e4e8e3034c43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93965f50617065722d4d696e694d61782d2d4d312d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/📖_Paper-MiniMax--M1-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://chat.minimax.io/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/cf1a97c2a522fe9d780765d5cc263bf289cde77cb8a51435a994aaf202e3e89f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f4d696e694d61785f436861742d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://www.minimax.io/platform" rel="nofollow">
    <img alt="API" src="https://camo.githubusercontent.com/033a96d7a4beb7f7872861878556c078064d874835da92f5d2ff5a0fe037c960/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa15f4150492d506c6174666f726d2d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚡_API-Platform-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-MCP">
    <img alt="MCP" src="https://camo.githubusercontent.com/a1dd6a9aad054731a18f4af8cc4f477aa43f9cf83920e1676324b97204238b5e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f4d43502d4d696e694d61785f4d43502d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🚀_MCP-MiniMax_MCP-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p><a href="https://huggingface.co/MiniMaxAI" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/94aa40386a1540394a4a71cdd73d62c70d4639e122e71df56f06b8a404754daa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4975f48756767696e675f466163652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤗_Hugging_Face-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1">
    <img alt="GitHub" src="https://camo.githubusercontent.com/f8147f98bcc14eb4bdbeba8461a7af5189604ad0ac027765507eaef31fb3456c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f90995f4769744875622d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🐙_GitHub-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://www.modelscope.cn/organization/MiniMax" rel="nofollow">
    <img alt="ModelScope" src="https://camo.githubusercontent.com/0715f6acf7fc905d6a032c08bd8f41b03f492fda6b3f70bf565e853643052b05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496efb88f5f4d6f64656c53636f70652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤖️_ModelScope-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/938892ddd7b2f59b079c0be16d5b388f03a3ff8868dbcf480033d0ae3690964f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a96efb88f5f4c6963656e73652d4170616368655f322e302d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚖️_License-Apache_2.0-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-01/blob/main/figures/wechat-qrcode.jpeg">
    <img alt="WeChat" src="https://camo.githubusercontent.com/e2a1862bfaa62514518f7eba6c5ad931f8b898d3ebf9867f69f29a83c0d4131d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5f5765436861742d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/💬_WeChat-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MiniMax-M1</h2><a id="user-content-minimax-m1" aria-label="Permalink: MiniMax-M1" href="#minimax-m1"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Model Overview</h2><a id="user-content-1-model-overview" aria-label="Permalink: 1. Model Overview" href="#1-model-overview"></a></p>
<p dir="auto">We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.
MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning
attention mechanism. The model is developed based on our previous <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01" rel="nofollow">MiniMax-Text-01 model</a>,
which contains a total of 456 billion parameters with 45.9 billion parameters activated
per token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism
in MiniMax-M1 enables efficient scaling of test-time compute – For example, compared to DeepSeek
R1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1
particularly suitable for complex tasks that require processing long inputs and thinking extensively.
MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from
traditional mathematical reasoning to sandbox-based, real-world software engineering environments.
We develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose
CISPO, a novel algorithm that clips importance sampling weights instead of token updates, which
outperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the
efficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We
train two versions of MiniMax-M1 models with <a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">40K</a> and
<a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">80K</a> thinking budgets respectively. Experiments
on standard benchmarks show that our models outperform other strong open-weight models such as
the original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,
and long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong
foundation for next-generation language model agents to reason and tackle real-world challenges.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/figures/TextBench.png"><img width="100%" src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/TextBench.png"></a>
  <br>
  <em>Benchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.</em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Evaluation</h2><a id="user-content-2-evaluation" aria-label="Permalink: 2. Evaluation" href="#2-evaluation"></a></p>
<p dir="auto"><strong>Performance of MiniMax-M1 on core benchmarks.</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Task</strong></th>
<th><strong>MiniMax-M1-80K</strong></th>
<th><strong>MiniMax-M1-40K</strong></th>
<th><strong>Qwen3-235B-A22B</strong></th>
<th><strong>DeepSeek-R1-0528</strong></th>
<th><strong>DeepSeek-R1</strong></th>
<th><strong>Seed-Thinking-v1.5</strong></th>
<th><strong>Claude 4 Opus</strong></th>
<th><strong>Gemini 2.5 Pro (06-05)</strong></th>
<th><strong>OpenAI-o3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><em>Extended Thinking</em></td>
<td><em>80K</em></td>
<td><em>40K</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>32k</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>64k</em></td>
<td><em>100k</em></td>
</tr>
<tr>
<td><em><strong>Mathematics</strong></em></td>
<td>AIME 2024</td>
<td>86.0</td>
<td>83.3</td>
<td>85.7</td>
<td>91.4</td>
<td>79.8</td>
<td>86.7</td>
<td>76.0</td>
<td>92.0</td>
<td>91.6</td>
</tr>
<tr>
<td></td>
<td>AIME 2025</td>
<td>76.9</td>
<td>74.6</td>
<td>81.5</td>
<td>87.5</td>
<td>70.0</td>
<td>74.0</td>
<td>75.5</td>
<td>88.0</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>MATH-500</td>
<td>96.8</td>
<td>96.0</td>
<td>96.2</td>
<td>98.0</td>
<td>97.3</td>
<td>96.7</td>
<td>98.2</td>
<td>98.8</td>
<td>98.1</td>
</tr>
<tr>
<td><em><strong>General Coding</strong></em></td>
<td>LiveCodeBench <em>(24/8~25/5)</em></td>
<td>65.0</td>
<td>62.3</td>
<td>65.9</td>
<td>73.1</td>
<td>55.9</td>
<td>67.5</td>
<td>56.6</td>
<td>77.1</td>
<td>75.8</td>
</tr>
<tr>
<td></td>
<td>FullStackBench</td>
<td>68.3</td>
<td>67.6</td>
<td>62.9</td>
<td>69.4</td>
<td>70.1</td>
<td>69.9</td>
<td>70.3</td>
<td>--</td>
<td>69.3</td>
</tr>
<tr>
<td><em><strong>Reasoning &amp; Knowledge</strong></em></td>
<td>GPQA Diamond</td>
<td>70.0</td>
<td>69.2</td>
<td>71.1</td>
<td>81.0</td>
<td>71.5</td>
<td>77.3</td>
<td>79.6</td>
<td>86.4</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>HLE <em>(no tools)</em></td>
<td>8.4*</td>
<td>7.2*</td>
<td>7.6*</td>
<td>17.7*</td>
<td>8.6*</td>
<td>8.2</td>
<td>10.7</td>
<td>21.6</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>ZebraLogic</td>
<td>86.8</td>
<td>80.1</td>
<td>80.3</td>
<td>95.1</td>
<td>78.7</td>
<td>84.4</td>
<td>95.1</td>
<td>91.6</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro</td>
<td>81.1</td>
<td>80.6</td>
<td>83.0</td>
<td>85.0</td>
<td>84.0</td>
<td>87.0</td>
<td>85.0</td>
<td>86.0</td>
<td>85.0</td>
</tr>
<tr>
<td><em><strong>Software Engineering</strong></em></td>
<td>SWE-bench Verified</td>
<td>56.0</td>
<td>55.6</td>
<td>34.4</td>
<td>57.6</td>
<td>49.2</td>
<td>47.0</td>
<td>72.5</td>
<td>67.2</td>
<td>69.1</td>
</tr>
<tr>
<td><em><strong>Long Context</strong></em></td>
<td>OpenAI-MRCR <em>(128k)</em></td>
<td>73.4</td>
<td>76.1</td>
<td>27.7</td>
<td>51.5</td>
<td>35.8</td>
<td>54.3</td>
<td>48.9</td>
<td>76.8</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>OpenAI-MRCR <em>(1M)</em></td>
<td>56.2</td>
<td>58.6</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>58.8</td>
<td>--</td>
</tr>
<tr>
<td></td>
<td>LongBench-v2</td>
<td>61.5</td>
<td>61.0</td>
<td>50.1</td>
<td>52.1</td>
<td>58.3</td>
<td>52.5</td>
<td>55.6</td>
<td>65.0</td>
<td>58.8</td>
</tr>
<tr>
<td><em><strong>Agentic Tool Use</strong></em></td>
<td>TAU-bench <em>(airline)</em></td>
<td>62.0</td>
<td>60.0</td>
<td>34.7</td>
<td>53.5</td>
<td>--</td>
<td>44.0</td>
<td>59.6</td>
<td>50.0</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>TAU-bench <em>(retail)</em></td>
<td>63.5</td>
<td>67.8</td>
<td>58.6</td>
<td>63.9</td>
<td>--</td>
<td>55.7</td>
<td>81.4</td>
<td>67.0</td>
<td>73.9</td>
</tr>
<tr>
<td><em><strong>Factuality</strong></em></td>
<td>SimpleQA</td>
<td>18.5</td>
<td>17.9</td>
<td>11.0</td>
<td>27.8</td>
<td>30.1</td>
<td>12.9</td>
<td>--</td>
<td>54.0</td>
<td>49.4</td>
</tr>
<tr>
<td><em><strong>General Assistant</strong></em></td>
<td>MultiChallenge</td>
<td>44.7</td>
<td>44.7</td>
<td>40.0</td>
<td>45.0</td>
<td>40.7</td>
<td>43.0</td>
<td>45.8</td>
<td>51.8</td>
<td>56.5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">* conducted on the text-only HLE subset.</p>
<p dir="auto">Our models are evaluated with <code>temperature=1.0</code>, <code>top_p=0.95</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SWE-bench methodology</h3><a id="user-content-swe-bench-methodology" aria-label="Permalink: SWE-bench methodology" href="#swe-bench-methodology"></a></p>
<p dir="auto">We report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:
<code>"astropy__astropy-7606"</code>,
<code>"astropy__astropy-8707"</code>,
<code>"astropy__astropy-8872"</code>,
<code>"django__django-10097"</code>,
<code>"matplotlib__matplotlib-20488"</code>,
<code>"psf__requests-2317"</code>,
<code>"psf__requests-2931"</code>,
<code>"psf__requests-5414"</code>,
<code>"pylint-dev__pylint-6528"</code>,
<code>"pylint-dev__pylint-7277"</code>,
<code>"sphinx-doc__sphinx-10435"</code>,
<code>"sphinx-doc__sphinx-7985"</code>,
<code>"sphinx-doc__sphinx-8269"</code>,
<code>"sphinx-doc__sphinx-8475"</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TAU-bench methodology</h3><a id="user-content-tau-bench-methodology" aria-label="Permalink: TAU-bench methodology" href="#tau-bench-methodology"></a></p>
<p dir="auto">We evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.
Our general system prompt is:</p>
<div data-snippet-clipboard-copy-content="- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies."><pre><code>- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Deployment Guide</h2><a id="user-content-3-deployment-guide" aria-label="Permalink: 3. Deployment Guide" href="#3-deployment-guide"></a></p>
<p dir="auto">Download the model from HuggingFace repository:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">MiniMax-M1-40k</a></li>
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">MiniMax-M1-80k</a></li>
</ul>
<p dir="auto">For production deployment, we recommend using <a href="https://docs.vllm.ai/en/latest/" rel="nofollow">vLLM</a> to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:</p>
<ul dir="auto">
<li>🔥 Outstanding service throughout performance</li>
<li>⚡ Efficient and intelligent memory management</li>
<li>📦 Powerful batch request processing capability</li>
<li>⚙️ Deeply optimized underlying performance</li>
</ul>
<p dir="auto">For detailed vLLM deployment instructions, please refer to our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/vllm_deployment_guide.md">vLLM Deployment Guide</a>.
Alternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/transformers_deployment_guide.md">MiniMax-M1 Transformers Deployment Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Function Calling</h2><a id="user-content-4-function-calling" aria-label="Permalink: 4. Function Calling" href="#4-function-calling"></a></p>
<p dir="auto">The MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/function_call_guide.md">MiniMax-M1 Function Call Guide</a> provides detailed instructions on how to use the function calling feature of MiniMax-M1.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chatbot &amp; API</h2><a id="user-content-5-chatbot--api" aria-label="Permalink: 5. Chatbot &amp; API" href="#5-chatbot--api"></a></p>
<p dir="auto">For general use and evaluation, we provide a <a href="https://chat.minimax.io/" rel="nofollow">Chatbot</a> with online search capabilities and the <a href="https://www.minimax.io/platform/" rel="nofollow">online API</a> for developers. For general use and evaluation, we provide the <a href="https://github.com/MiniMax-AI/MiniMax-MCP">MiniMax MCP Server</a> with video generation, image generation, speech synthesis, and voice cloning for developers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. Contact Us</h2><a id="user-content-6-contact-us" aria-label="Permalink: 6. Contact Us" href="#6-contact-us"></a></p>
<p dir="auto">Contact us at <a href="mailto:model@minimax.io">model@minimax.io</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is eating our brains. MIT study: Your brain on ChatGPT (124 pts)]]></title>
            <link>https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</link>
            <guid>44307257</guid>
            <pubDate>Wed, 18 Jun 2025 06:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/">https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</a>, See on <a href="https://news.ycombinator.com/item?id=44307257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <p>Check project's website:&nbsp;<a href="https://www.brainonllm.com/">https://www.brainonllm.com</a></p><p>With today's wide adoption of LLM products like ChatGPT from OpenAI, humans and businesses engage and use LLMs on a daily basis. Like any other tool, it carries its own set of advantages and limitations. This study focuses on finding out the <b>cognitive cost of using an LLM</b> in the educational context of writing an essay.</p><p>We assigned participants to three groups: <b>LLM group, Search Engine group, and Brain-only group, where each participant used a designated tool (or no tool in the latter) to write an essay</b>. We conducted 3 sessions with the same group assignment for each participant. In the 4th session we asked LLM group participants to use no tools (we refer to them as LLM-to-Brain), and the Brain-only group participants were asked to use LLM (Brain-to-LLM). We recruited a total of 54 participants for Sessions 1, 2, 3, and 18 participants among them completed session 4. </p><p>We used electroencephalography (EEG) to <b>record participants' brain activity </b>in order to assess their cognitive engagement and cognitive load, and to gain a deeper understanding of neural activations during the essay writing task. We performed <b>NLP analysis</b>, and we interviewed each participant after each session. We performed scoring with the help from the <b>human teachers and an AI judge</b> (a specially built AI agent).</p><p>We discovered a consistent homogeneity across the Named Entities Recognition (NERs), n-grams, ontology of topics within each group. EEG analysis presented robust evidence that LLM, Search Engine and Brain-only groups had <b>significantly different neural connectivity patterns</b>, reflecting divergent cognitive strategies. <b>Brain connectivity systematically scaled down with the amount of external support: the Brain‑only group exhibited the strongest, widest‑ranging networks, Search Engine group showed intermediate engagement, and LLM assistance elicited the weakest overall coupling.</b> In session 4, LLM-to-Brain participants showed weaker neural connectivity and under-engagement of alpha and beta networks; and the Brain-to-LLM participants demonstrated higher memory recall, and re‑engagement of widespread occipito-parietal and prefrontal nodes, likely supporting the visual processing, similar to the one frequently perceived in the Search Engine group. The reported<b> ownership </b>of LLM group's essays in the interviews <b>was low.</b> The Search Engine group had strong ownership, but lesser than the Brain-only group. The LLM group also <b>fell behind in their ability to quote</b> from the essays they wrote just minutes prior. </p><p>As the educational impact of LLM use only begins to settle with the general population, in this study we demonstrate the pressing matter of a likely <b>decrease in learning skills</b> based on the results of our study. The use of LLM had a measurable impact on participants, and while the benefits were initially apparent, as we demonstrated over the course of 4 months, the <b>LLM group's participants performed worse than their counterparts in the Brain-only group at all levels: neural, linguistic, scoring.</b></p><p>We hope this study serves as a preliminary guide to understanding the cognitive and practical impacts of AI on learning environments.</p><p>#cognitivedebt #brainonllm #yourbrainonchatgpt&nbsp;</p>
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make little apps for you and your friends (417 pts)]]></title>
            <link>https://pontus.granstrom.me/scrappy/</link>
            <guid>44306859</guid>
            <pubDate>Wed, 18 Jun 2025 05:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pontus.granstrom.me/scrappy/">https://pontus.granstrom.me/scrappy/</a>, See on <a href="https://news.ycombinator.com/item?id=44306859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Software is important to people. Most of us spend our workdays in front of computers. We use the computer in our pocket tens if not hundreds of times every day. The apps we use are almost exclusively mass-market, sold on an app-store, made for thousands if not millions of users. Or they are enterprise apps that are custom-built for hundreds of thousands of dollars.</p><p>But there isn’t really any equivalent of home-made software — apps made lovingly by you for your friends and family. Apps that aren’t polished or flashy, but are made to <em>your</em> <em>preference</em> and help you with <em>your particular needs.</em></p><p>We’re John and Pontus, and we’ve been exploring the potential of home-made software together.</p><p>We ended up creating a research prototype that we call <strong>Scrappy</strong> — a tool for making <strong>scrappy apps for just you and your friends.</strong> First and foremost, we aim to contribute a <em>vision</em> of what home-made software could be like. We want to make this vision as concrete as we can, by sharing a working tool and examples of apps made in it. Scrappy, in its current state, is a prototype, not a robust tool, but we hope it paints the picture we carry in our heads — of software as something that can be creative, personal, expressive. Made by anyone, for themselves and their loved ones.</p><h2 id="what-is-scrappy">What is Scrappy?</h2><p>It may not be clear what “a scrappy app for you and your friends” means. What kind of apps are these? Let us paint a picture with a few examples. (We call them “<strong>Scrapps</strong>”.)</p><div><div><p><strong>Arithmetic practice for a kid in elementary school.</strong> When outgrown, the Scrapp can be extended with harder problems.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/math_practice.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/math_practice.mp4" poster="https://pontus.granstrom.me/scrappy/examples/math_practice.jpg" controls=""></video></div><div><p><strong>Attendee counter for a local event.</strong> The counter’s state is shared, so the Scrapp can be used to let people in and out at multiple entrances.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/attendee_counter.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/attendee_counter.mp4" poster="https://pontus.granstrom.me/scrappy/examples/attendee_counter.jpg" controls=""></video></div></div><div><div><p><strong>Meeting cost clock,</strong> to help meetings stay on track. A Scrapp like this can be put together in 15 minutes and shared with coworkers right away.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/meeting_cost_clock.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.mp4" poster="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.jpg" controls=""></video></div><div><p><strong>Weekly chore tracker.</strong> Let roommates flexibly swap weeks, while making sure to track whose up next, to keep things fair.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/chores.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/chores.mp4" poster="https://pontus.granstrom.me/scrappy/examples/chores.jpg" controls=""></video></div></div><h2 id="what-is-it-like-to-make-an-app-in-scrappy">What is it like to make an app in Scrappy?</h2><p>Scrappy is an infinite canvas of interactive objects. The workflow is similar to an app such as Figma, Miro, or Google Slides — except you can attach behaviors to the objects.</p><p>You drag objects out on the canvas — a button, a textfield, a few labels. Select an object, and you can modify its attribute in an inspector panel. Certain objects, like buttons, has attributes like “when clicked” that contain javascript code. When the button is clicked, that code is run — maybe it records the contents of the textfield to a label that acts as a log. You build your app step by step: tweaking and rearranging the objects, and attaching a little bit of code to them.</p><p>There’s no better way to get a feeling for an authoring environment than to see someone use it in action. In the following videos, I’m making an attendee counter for an event.</p><p><strong>The basics.</strong> I start out by adding a number field to track the number of attendees, and two buttons for recording people entering and exiting the venue.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/basics.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/basics.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Reactive formulas.</strong> Next, I add a field for the venue’s capacity, and a warning when too many people have been let in.
I use a reactive formula to control the visibility of the warning and the border color of the field.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>A shared, persistent world.</strong> Without any extra work, Scrappy apps are multiplayer.
App state is persisted and synced, like users expect from online documents like Google Sheets or Figma.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>The app is always live.</strong> There’s no distinction between editing and running. I can edit the app while a friend is using it.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/liveness.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/liveness.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Selective sharing.</strong> I make a variant of the app that’s limited to only entering and exiting people. This is done by putting a part of the app in a frame, and sharing only that frame. The limited version is still linked to the main app.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.mp4" type="video/mp4">Your browser cannot play this video.</video><div><div><p><strong>Visible, tangible data.</strong> Here’s what the <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">Meeting Cost Clock app</a> shown above looks like when zoomed out, revealing a common pattern in Scrapps.</p><p>Outside the shared frame are a bunch of fields used to compute the cost of the meeting. This lets me see the data while I’m working on the Scrapp, just like in a spreadsheet, which is very helpful for debugging — and it makes future tweaking or remixing easier.</p></div><p><img src="https://pontus.granstrom.me/scrappy/walkthrough/meeting_cost_clock_internals.png" alt="Meeting cost clock internals"></p></div><h2 id="why-make-scrappy">Why make Scrappy?</h2><p>This project is driven by a desire to reimagine software creation and use. As part of a growing movement variously termed “<a href="https://hackernoon.com/big-and-small-computing-73dc49901b9a">small computing</a>,” “<a href="https://dubroy.com/blog/casual-programming/">casual programming</a>”, and “<a href="https://maggieappleton.com/home-cooked-software">home-cooked software</a>” we want to <a href="https://www.inkandswitch.com/end-user-programming/">emancipate end-users</a> — to “empower people to express themselves without requiring them to be heavy-duty programmers,” to “liberate the programming of computers from the priesthood to the layperson”, as Bill Atkinson worded it. We want to shift the world away from mass-market, industrially-produced software toward more <a href="https://x.com/davidhoang/status/1802140453292372272">personal, even disposable,</a> tools that are designed for and readily <a href="https://malleable.systems/">modified and adapted</a> to <a href="https://gwern.net/doc/technology/2004-03-30-shirky-situatedsoftware.html">specific social contexts</a>. Above all, we want to foster a sense of agency and to ultimately contribute to “<a href="https://x.com/cwervo/status/1808578326409457834">redistributing the means of software production</a>”.</p><p>We were inspired by the simplicity of tools like <a href="https://www.notion.so/">Notion</a>, <a href="https://www.tldraw.com/">tldraw</a>, and <a href="https://mmm.page/">mmm.page</a>, but wanted to empower people with richer interactivity and programming capabilities. However, knowing the strengths and limitations of the standard visual programming paradigms of blocks (e.g. <a href="https://scratch.mit.edu/">Scratch</a>, <a href="https://developers.google.com/blockly">Blockly</a>) and nodes-and-wires (e.g. <a href="https://cycling74.com/products/max">Max/MSP</a>, <a href="https://nodered.org/">Node-RED</a>, <a href="https://natto.dev/">natto</a>, <a href="https://www.holograph.so/">Holograph</a>), we deliberately wanted to go down a different path. Instead, we drew direct inspiration from “media with scripting” environments, both classic systems like <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a>, <a href="https://en.wikipedia.org/wiki/Visual_Basic_(classic)">Visual Basic</a>, and <a href="https://en.wikipedia.org/wiki/Adobe_Director">Macromedia Director</a>, as well as contemporary platforms like <a href="https://www.notion.so/Project-Concept-Definition-618cd9f0e26944f3b1ee4222c1db92c9?pvs=21">Dynamicland</a> and <a href="https://www.minecraft.net/">Minecraft</a>, where the “media with scripting” exist in a shared online world.</p><p>Overall, our target user experience was that of a productivity tool, specifically a canvas-based tool (e.g. <a href="https://www.figma.com/">Figma</a>, <a href="https://miro.com/">Miro</a>, and <a href="https://www.tldraw.com/">tldraw</a>)—rather than programming environments (e.g. <a href="https://squeak.org/">Squeak/Smalltalk</a>, modern IDEs) and website and app builders (e.g. <a href="https://www.squarespace.com/">Squarespace</a>, <a href="https://mmm.page/">mmm.page</a>, <a href="https://bubble.io/">Bubble</a>). And we also wanted that kind of modern “share link”-based real-time collaboration popularized by <a href="https://docs.google.com/">Google Docs</a> and <a href="https://www.figma.com/">Figma</a>.</p><p>Finally, while we acknowledge the capabilities of AI-centric systems that leverage LLMs for code generation (e.g., <a href="https://lovable.dev/">Lovable</a>, <a href="http://bolt.new/">bolt.new</a>, and <a href="https://computer.tldraw.com/">tldraw computer</a>), we deliberately chose to focus our design on direct manipulation and user control.</p><h2 id="who-is-scrappy-for">Who is Scrappy for?</h2><p>As we were prototyping, it wasn’t clear who the ideal user for Scrappy was. We left this open, to see what we’d learn from building the system. Eventually, a few potential personas revealed themselves.</p><ul><li><strong>The process optimizer.</strong> In business environments, there’s always some improvement that can be made using software. But the person who sees the process inefficiency likely can’t make software themselves, and involving a professional programmer is expensive. So what usually happens is they make what improvements they can using tools they are familiar with, such as Excel. Here Scrappy could be a more powerful and flexible Excel, while retaining familiarity and ease of use.</li><li><strong>Teachers and students</strong>. Teaching programming requires teaching a multitude of inessential technical details: how to use the command line, how file systems work, how to set up the environment, dependency management, version control, servers and clients, and on and on. With Scrappy, you can just create a button, write a line of code and click the button to run the code.</li><li><strong>Ourselves!</strong> We are professional programmers who don’t like programming. Why? Because of the all the aforementioned complexity that adds friction to what could be so much simpler. When making mass-market apps, we know we have to deal with that complexity, but when working on a fun hobby project?! Give us a break. Scrappy is that kind of break.</li><li><strong>The DIYer.</strong> People like to customize their house, grow their own vegetables, sow their own clothes, build their own furniture. Scrappy is where a DIY-inclined person makes their own little apps for themselves and their friends.</li></ul><p>As Scrappy solidified, we wanted to focus on one of these personas. There’s a pull toward business use cases, since businesses are the most willing to pay for a product, but we believe the incentives there would lead us too close to existing products like <a href="https://retool.com/">Retool</a> or <a href="https://livecode.com/">LiveCode</a>. The teaching use case is compelling, but we believe it needs a better coding experience (discoverability, better error messages, debugger) which was out of scope for us (for now). We are itching to make stuff for ourselves in Scrappy (and we are strong believers in dogfooding), but most of our projects required features that would balloon the scope.</p><p>The DIYer making home-made software is the least served by existing tools, and fits our vision of democratized computing the best. We decided this is where we could make the biggest contribution (the <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy">blue ocean strategy</a>), and decided to make the DIYer our target persona.</p><p>Ideally, Scrappy would let anyone with basic computer literacy make a simple app and learn from there. This is not quite the case yet — some JavaScript knowledge is required. So today, the person making Scrapps from scratch is a <strong>programmer DIYer.</strong> But when a Scrapp is shared with friends, those friends can use it and remix it without needing programming experience.</p><h2 id="what-should-i-make-in-scrappy">What should I make in Scrappy?</h2><p>Home-made, scrappy apps don’t really exist today, so most people (including us!) are not used to coming up with ideas for them. When faced with a problem that would make a great Scrapp, instead our minds go to “maybe there’s an app for that”, searching the web for one, giving up if we cannot find a good one. To start coming up with good uses for Scrappy requires a shift to a home-made mindset.</p><p>To help you build that mindset, here is an assortment of ideas for Scrapps (some of which are not feasible in the current prototype of Scrappy, but should be).</p><div><div><ul><li>Custom flashcards</li><li>Meeting agenda manager</li><li>Day clock for person with dementia</li><li>Online workshop facilitation</li><li>Consulting time tracker</li><li>Point-based voting for a board</li><li>Receipt generator</li><li>Simple word game</li><li>School grade calculator</li><li>Interactive visual recipe</li><li>Social quiz game</li></ul></div><div><ul><li>Typing tutor</li><li>Lyric writing aids (synonyms, rhymes)</li><li>Board game helper</li><li>Wedding RSVP + seating arrangement</li><li>Dynamic opening hours display</li><li>Family bulletin board</li><li>Group travel planner</li><li>Chore → allowance calculator</li><li>Chess clock productivity timer</li></ul></div></div><p>What makes a problem well-suited for Scrappy? Here are some things they have in common:</p><ul><li><strong>Shared with friends.</strong> While a Scrapp can be for just yourself, Scrappy really shines with multiples users, leveraging the shared, persistent world. Some problems that would need setting up a backend server can be built in minutes in Scrappy.</li><li><strong>Needs tweaking-as-you-go.</strong> Life changes, and so does requirements. In Scrappy, you can edit the app at any time — even while your friend is using it. No building, no deploying, no fuss.</li><li><strong>A sprinkle of computation.</strong> Scrappy shines when thought of as a shared document first, with a little bit of computation added on top. For complex systems with a lot of moving parts, we recommend reaching for traditional software engineering tools.</li><li><strong>Minimal friction.</strong> We all let out a groan inside whenever we are hit with “create an account to continue”. This “account friction” may not be much, but it multiplies when sharing with a group of people — there’s always going to be someone for whom the friction is too much. Scrapps don’t have this problem: just click the link.</li><li><strong>Small number of trusted users.</strong> Scrappy assumes you trust the people you share a Scrapp with, which removes a lot of friction, but if you need to control access and permissions, look elsewhere.</li><li><strong>Not mission-critical.</strong> If you need guaranteed correctness or perfect control over details, don’t reach for Scrappy. Those qualities are what you pay expert engineers for.</li></ul><h2 id="scrappy-vs-mass-market-apps">Scrappy vs mass-market apps</h2><p>When faced with a “scrappy” problem — something small that would benefit from a computer — most people will think “maybe there’s an app for that”, followed by searching an app store or the Internet to look for one.</p><p>If there is no app for that, or there’s no good one, you could make your own in Scrappy. We hope you do! But often there <em>is</em> an app for that. If there is, it will probably be more polished than anything you can make in Scrappy. In this case, there are still reasons to consider using making your own Scrapp:</p><ul><li><strong>Does exactly what you need.</strong> And only what you need. Nothing more, nothing less.</li><li><strong>Home-made with love.</strong> Scrapps are made by you for your friends. A home-knitted sweater will always mean more to you than a store-bought one.</li><li><strong>Fun and playful.</strong> In Scrappy, it’s easy to play around. Tweak the colors, make a cute layout, add little inside jokes.</li><li><strong>Remixable.</strong> Easy to share with others and modify to suit your needs.</li><li><strong>Collaborative by default.</strong> All Scrappy apps are multiplayer, like a Google Doc is. You can even edit them while they are being used by someone else!</li><li><strong>No accounts and signups.</strong> If you share a Scrappy app with someone, they can start using it right away — no tedious sign-up flows stopping your friends or family from joining in.</li><li><strong>You own your data.</strong> The data is stored locally and will only be used for nefarious purposes if its creator (you) wants to!</li></ul><h2 id="scrappy-vs-ai-written-apps">Scrappy vs AI-written apps</h2><p>What about asking an LLM to make a custom, home-made app?</p><p>LLMs are getting better and better, and while they are far from able to make a full-fledged app without a lot of help from a software engineer, they can make small apps pretty reliably.</p><p>So if I can ask ChatGPT or Claude to make an app, why would I use Scrappy?</p><ul><li><strong>Scrappy is understandable.</strong> Using an LLM means going from an English prompt to pages of React code, which is too big a leap for non-programmers. They end up having to rely on the LLM to make changes, and are left helpless if the LLM doesn’t do the right thing. In contrast, Scrappy’s objects-on-a-canvas model is easy to understand, more humane, and acts a shared substrate where user and AI can collaborate on equal footing. And because it is less overwhelming, it’s more likely the user will pick up some programming skills.</li><li><strong>Scrappy is collaborative.</strong> All Scrappy apps are little shared worlds, persistent and with live updating — all for free. LLMs are mainly useful for creating static front-end-only web apps. And in Scrappy, apps can be edited by multiple users in realtime, whereas AI workflows are mostly “type, then wait” with little room for collaboration between humans.</li><li><strong>Scrappy is more fun!</strong> While typing a few sentences of English and seeing a full app appear out of nowhere still feels like magic, it quickly grows old when you’re waiting for minutes only to see the LLM misunderstood you again. In Scrappy, there is joy in tweaking things or remixing something. A spark of “ooh I want it to do this” and it’s only a few clicks and keystrokes away. A sense of creative ownership. And you can edit it together with friends!</li></ul><h2 id="scrappy-vs-hypercard-and-its-successors">Scrappy vs HyperCard (and its successors)</h2><p><a href="https://hypercard.org/">HyperCard</a> was popular among Macintosh users in the early 90s, and is often held as an exemplar of enabling home-made software and end-user programming. Decades later, there have been a number of successors to HyperCard, both commercial (<a href="https://www.mackiev.com/hyperstudio/">HyperStudio</a>, <a href="https://en.wikipedia.org/wiki/SuperCard">SuperCard</a>, <a href="https://livecode.com/">LiveCode</a>) and non-commercial (<a href="https://beyondloom.com/decker/">Decker</a> and <a href="https://hypervariety.com/WildCard/">WildCard</a>, among a number of open-source remakes, most of which are abandonware). Most of these have been quite literal replicas of HyperCard, driven by nostalgia, down to the black-and-white graphics. None have been as successful as the original.</p><p>We wanted to create something in the spirit of HyperCard, rather than recreate HyperCard. Scrappy is different from HyperCard and its direct descendants in a few key ways:</p><ul><li><strong>Designed for the Internet.</strong> Scrappy apps are easily shareable online with a simple link, whereas using HyperCard and most of its descendants is like being trapped in a virtual machine.</li><li><strong>A shared world.</strong> HyperCard stacks could be shared as a file with other users. Scrappy takes this to the next level by letting users edit and use apps at the same time.</li><li><strong>Modern UI conventions.</strong> Scrappy apps live on a high-resolution infinite canvas, with selections, copying, panning and zooming, frames for grouping, etc.</li><li><strong>Uses JavaScript for scripting.</strong> HyperCard and a number of its descendants use programming languages that aren’t in common use. JavaScript is the most common programming language in the world, is native to the Web and works well for a dynamic environment such as Scrappy.</li><li><strong>A larger palette of interactive objects.</strong> Many HyperCard-likes only support a few elements like buttons, text fields, and images. Scrappy supports more UI elements like sliders and timers, but also data types beyond strings: numbers, dates, and compound JSON objects.</li><li><strong>Reactive formulas, like a spreadsheet.</strong> The idea of “this value changes when that value changes” is familiar to many, and can be a stepping stone toward event-based programming, where the user has to think about state.</li></ul><h2 id="future-directions">Future directions</h2><p>With our prototype, we think that we’ve been successful at proving the ideas and design principles that we started with. But there’s a lot more work to do. The number of Scrapps that can be built in a way that feel “Scrappy native” is still low. Much of the time, existing knowledge of JavaScript is required. To improve this, we need to continue work in both “lowering the floor” and “raising the ceiling”.</p><p>Lowering the floor means making things more friendly and approachable for people with little or no programming experience. For example:</p><ul><li><strong>Improve code discoverability.</strong> We’ve made coding easier by presenting the names of objects visually on the screen, and listing their methods in the properties panel. But there’s a ton more than we can do. You should be able to click on objects to discover their methods and insert them in the code. Available names should auto-complete so you don’t have remember syntax and do as much typing.</li><li><strong>Improve debugging.</strong> You should be able to visualize relationships between objects, perhaps as arrows showing which objects read or modify other objects. Error messages should be better worded and show more information about what went wrong. You should be able pause and rewind execution. All of this while live collaborating on the app with a friend.</li><li><strong>Leverage AI.</strong> <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">As we mentioned earlier</a>, we don’t believe in having an LLM make an entire app, but we are interested in having it act as an assistant, directed by the user. Maybe you’d click on the canvas and ask the AI to “make start and stop buttons here”, or go to a text label’s “when changed” handler and ask the AI to write code to “show an error message if using non-english characters”.</li><li><strong>Make it even easier to share and remix</strong>. It’s easier to learn by inspecting and tweaking other people’ work than it is to start from a blank canvas. We imagine a public gallery where users can publish their creations, and other users can adopt and customize them for their own needs and preferences.</li><li><strong>Make Scrapps work well on phones/tablets.</strong> A hand-sized touchscreen is too small for editing Scrapps comfortably, but using them on phones should work well — this is not currently the case. The infinite canvas paradigm means that objects have fixed positions, which is a way simpler mental model layout rules (like in CSS), but means designs aren’t responsive to screen size. However, drag-and-drop web page design tools like <a href="http://mmm.page/">mmm.page</a> and <a href="https://www.squarespace.com/">Squarespace</a> show a way to handle this: simply show safe areas for mobile to the user.</li></ul><p>Raising the ceiling means adding functionality and expressive power, letting users create more things with less effort. For example:</p><ul><li><strong>Add support for collections</strong>. Currently, you can edit and store strings, numbers, dates, and JSON data, but you cannot store lists of them or make an editable tables, like in a spreadsheet. We also don’t have any kind of layout containers, like lists, grids, or stacks. Adding this would let authors express more things visually, and they wouldn’t have to resort to JavaScript knowledge and hidden state.</li><li><strong>Instanced frames.</strong> Frames let you <a href="https://www.notion.so/Submission-writeup-1a227a2e3bb680c5be5ddc988af65ce2?pvs=21">selectively share</a> parts of your app, but that frame is fully synced in real-time across users. This is desirable in some cases and undesirable in others. For example, when sharing a form, each user should only see and edit their own copy. You should be able to share instances of the form, that still collects all the data in one place. Another example is a board game helper where there’s some hidden information and users should see some shared UI and some UI only visible to them.</li><li><strong>Tools for data processing.</strong> We’ve found ourselves wanting to use Scrappy to process tabular data. Things like: do the same operations to all rows, filter the table based on some criteria, etc. This can be done in JavaScript, but there should be a Scrappy-native way of doing this, where the data is shown.</li><li><strong>Better support for reuse.</strong> Currently, if you want to repeat an object or set of objects in your Scrapp, you have to manually edit them one by one, or write code to manage them. Instead, you should be able to define a reusable component and make instances that stay linked to the main component. Figma has this, PowerPoint has slide masters, HyperCard has card backgrounds, all to this effect. Further, these components could be shared across projects, or even with other users.</li><li><strong>Allow extending Scrappy.</strong> Some capabilities will be out of reach using Scrappy’s primitives. Currently, we are the only ones able to add new objects, but we’d want to open this up to more people. We expect this would require programming and web expertise, so it wouldn’t be something for a traditional engineer, not the typical Scrappy user.</li><li><strong>Clean up the conceptual model.</strong> Currently, some of the objects store data values, and some support event handlers like “when clicked” and “when changed” handlers. The current implementation is a bit arbitrary about this, which is not only confusing but also limiting. Common behaviors like editing, clicking, and storing data should be made more consistent and freely “mixable” — like the <a href="https://en.wikipedia.org/wiki/Entity_component_system">entity component systems</a> of game engines like <a href="https://unity.com/">Unity</a>.</li></ul><h2 id="conclusion">Conclusion</h2><p>We believe computers should work for people, and dream of a future where computing, like cooking or word processing, is available to everyone. Where you can solve your own small, unique problems with small, unique apps. Where you don’t just rely on mass-market apps made by expert programmers. Where you share home-made little apps with family and friends.</p><p>Scrappy is our contribution to this dream. Each Scrapp is a live, persistent world, easily shared and remixed, closer to familiar productivity apps than alien developer tools. Like any vision, ours is incomplete, but we’ve grounded our explorations in a working prototype with examples of apps.</p><a href="https://scrappy.jrcpl.us/">Try Scrappy! (desktop only)</a><p>We hope Scrappy will inspire you to further chase this particular windmill. If it does, please let us know!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Locally hosting an internet-connected server (160 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/72095.html</link>
            <guid>44306792</guid>
            <pubDate>Wed, 18 Jun 2025 04:58:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/72095.html">https://mjg59.dreamwidth.org/72095.html</a>, See on <a href="https://news.ycombinator.com/item?id=44306792">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I'm lucky enough to have a <a href="https://www.monkeybrains.net/">weird niche ISP</a> available to me, so I'm paying $35 a month for around 600MBit symmetric data. Unfortunately they don't offer static IP addresses to residential customers, and nor do they allow multiple IP addresses per connection, and I'm the sort of person who'd like to run a bunch of stuff myself, so I've been looking for ways to manage this.</p><p>What I've ended up doing is renting a cheap VPS from a vendor that lets me add multiple IP addresses for minimal extra cost. The precise nature of the VPS isn't relevant - you just want a machine (it doesn't need much CPU, RAM, or storage) that has multiple world routeable IPv4 addresses associated with it and has no port blocks on incoming traffic. Ideally it's geographically local and peers with your ISP in order to reduce additional latency, but that's a nice to have rather than a requirement.</p><p>By setting that up you now have multiple real-world IP addresses that people can get to. How do we get them to the machine in your house you want to be accessible? First we need a connection between that machine and your VPS, and the easiest approach here is <a href="https://wireguard.com/">Wireguard</a>. We only need a point-to-point link, nothing routable, and none of the IP addresses involved need to have anything to do with any of the rest of your network. So, on your local machine you want something like:</p><tt><p>[Interface]<br>PrivateKey = privkeyhere<br>ListenPort = 51820<br>Address = localaddr/32</p><p>[Peer]<br>Endpoint = VPS:51820 <br>PublicKey = pubkeyhere <br>AllowedIPs = VPS/0</p></tt><p>And on your VPS, something like:</p><tt><p>[Interface]<br>Address = vpswgaddr/32<br>SaveConfig = true<br>ListenPort = 51820<br>PrivateKey = privkeyhere</p><p>[Peer]<br>PublicKey = pubkeyhere<br>AllowedIPs = localaddr/32</p></tt><p>The addresses here are (other than the VPS address) arbitrary - but they do need to be consistent, otherwise Wireguard is going to be unhappy and your packets will not have a fun time. Bring that interface up with <a href="https://www.wireguard.com/quickstart/">wg-quick</a> and make sure the devices can ping each other. Hurrah! That's the easy bit.</p><p>Now you want packets from the outside world to get to your internal machine. Let's say the external IP address you're going to use for that machine is </p><tt>321.985.520.309</tt><p> and the wireguard address of your local system is </p><tt>867.420.696.005</tt><p>. On the VPS, you're going to want to do:</p><tt>iptables -t nat -A PREROUTING -p tcp -d 321.985.520.309 -j DNAT --to-destination 867.420.696.005</tt><p>Now, all incoming packets for </p><tt>321.985.520.309</tt><p> will be rewritten to head towards </p><tt>867.420.696.005</tt><p> instead (make sure you've set </p><tt>net.ipv4.ip_forward</tt><p> to 1 via </p><tt>sysctl</tt><p>!). Victory! Or is it? Well, no.</p><p>What we're doing here is rewriting the destination address of the packets so instead of heading to an address associated with the VPS, they're now going to head to your internal system over the Wireguard link. Which is then going to ignore them, because the </p><tt>AllowedIPs</tt><p> statement in the config only allows packets coming from your VPS, and these packets still have their original source IP. We could rewrite the source IP to match the VPS IP, but then you'd have no idea where any of these packets were coming from, and that sucks. Let's do something better. On the local machine, in the peer, let's update </p><tt>AllowedIps</tt><p> to </p><tt>0.0.0.0/0</tt><p> to permit packets form any source to appear over our Wireguard link. But if we bring the interface up now, it'll try to route <em>all</em> traffic over the Wireguard link, which isn't what we want. So we'll add </p><tt>table = off</tt><p> to the </p><tt>interface</tt><p> stanza of the config to disable that, and now we can bring the interface up without breaking everything but still allowing packets to reach us. However, we do still need to tell the kernel how to reach the remote VPN endpoint, which we can do with </p><tt>ip route add vpswgaddr dev wg0</tt><p>. Add this to the </p><tt>interface</tt><p> stanza as:</p><tt><p>PostUp = ip route add vpswgaddr dev wg0<br>PreDown = ip route del vpswgaddr dev wg0</p></tt><p>That's half the battle. The problem is that they're going to show up there with the source address still set to the original source IP, and your internal system is (because Linux) going to notice it has the ability to just send replies to the outside world via your ISP rather than via Wireguard and nothing is going to work. Thanks, Linux. Thinux.</p><p>But there's a way to solve this - policy routing. Linux allows you to have multiple separate routing tables, and define policy that controls which routing table will be used for a given packet. First, let's define a new table reference. On the local machine, edit </p><tt>/etc/iproute2/rt_tables</tt><p> and add a new entry that's something like:</p><tt><p>1 wireguard</p></tt><p>where "1" is just a standin for a number not otherwise used there. Now edit your wireguard config and replace </p><tt>table=off</tt><p> with </p><tt>table=wireguard</tt><p> - Wireguard will now update the </p><tt>wireguard</tt><p> routing table rather than the global one. Now all we need to do is to tell the kernel to push packets into the appropriate routing table - we can do that with </p><tt>ip rule add from localaddr lookup wireguard</tt><p>, which tells the kernel to take any packet coming from our Wireguard address and push it via the Wireguard routing table. Add that to your Wireguard interface config as:</p><tt><p>PostUp = ip rule add from localaddr lookup wireguard<br>PreDown = ip rule del from localaddr lookup wireguard</p></tt><p>and now your local system is effectively on the internet.</p><p>You can do this for multiple systems - just configure additional Wireguard interfaces on the VPS and make sure they're all listening on different ports. If your local IP changes then your local machines will end up reconnecting to the VPS, but to the outside world their accessible IP address will remain the same. It's like having a real IP without the pain of convincing your ISP to give it to you.</p></div></div>]]></description>
        </item>
    </channel>
</rss>