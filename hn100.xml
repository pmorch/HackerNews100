<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 16 Feb 2026 20:30:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Use protocols, not services (177 pts)]]></title>
            <link>https://notnotp.com/notes/use-protocols-not-services/</link>
            <guid>47038588</guid>
            <pubDate>Mon, 16 Feb 2026 18:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notnotp.com/notes/use-protocols-not-services/">https://notnotp.com/notes/use-protocols-not-services/</a>, See on <a href="https://news.ycombinator.com/item?id=47038588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article itemscope="" itemtype="https://schema.org/BlogPosting">
    
    <time itemprop="datePublished" datetime="2026-02-15T00:00:00+00:00">Published on 15 February 2026</time>

    <p>
      The Internet is almost anonymous and privacy-preserving by design. I mean,
      unless some administrator actively tries to track you, there is no
      built-in identity layer. What breaks both properties is the centralization
      of communication onto closed platforms, where identification becomes
      possible either by the hosting company itself, or by governments
      compelling them to cooperate.
    </p>
    <p>
      After recent events, it is time for us to start using protocols again
      instead of services.
    </p>

    <h2>Services are easy targets</h2>

    <p>
      A government that wants to identify users, censor content, or enforce
      compliance only needs to send one letter to one company. One subpoena, one
      court order, one regulatory demand: the service likely complies or faces
      fines, lawsuits, or bans.
    </p>
    <p>
      This is happening right now. Governments worldwide are passing laws that
      require platforms to verify the age of their users. Discord is voluntarily
      rolling out mandatory "teen-by-default" settings until proof of majority
      (by submitting a face scan or, God forbid, a government-issued ID), likely
      anticipating future regulatory obligations.
    </p>

    <p>
      None of this could happen with a protocol. You cannot require age
      verification on IRC, XMPP,
      <a href="https://activitypub.rocks/">ActivityPub</a>,
      <a href="https://nostr.com/">Nostr</a>, or
      <a href="https://matrix.org/">Matrix</a>, because there is no single
      entity to compel. Each server operator makes their own decisions. A
      government would need to individually pressure thousands of independent
      operators across dozens of jurisdictions, which is a legislative and
      enforcement impossibility. And even if one server complied, users would
      simply move to another.
    </p>

    <h2>Switching services solves nothing</h2>

    <p>
      After Discord's announcement, the instinct is to migrate to another
      service. This is pointless. The new service will either operate under the
      same jurisdiction and face the same rules, or it will be offshore and
      eventually blocked or pressured once it becomes large enough to matter.
      You are just moving from one regulable entity to another.
    </p>
    <p>
      The actual solution is to stop depending on a specific commercial service
      and start using a protocol. This is not a radical idea. We already do it
      with email. SMTP is a protocol. You can switch providers, self-host, or
      use any combination.
    </p>
    <p>
      Email may not seem to be the best example since it has become an oligopoly
      where Google, Microsoft, and maybe also Apple control the vast majority of
      the email infrastructure. But actually, this is a good example to show how
      protocols are resilient. Let's say Google bans your account, then you can
      move to another provider and still reach every Gmail user. In a more
      extreme scenario, let's even say Google and Microsoft discontinue their
      service (in your specific region, for example), even block any inbound
      message from you. Not ideal, but <em>SMTP implementations still exist and
        they still work</em> even in a very degraded mode. You'd need to migrate
      (as well as some of your connections), but there is absolutely no need to
      reimplement anything. That is the difference with a service like Discord.
    </p>
    <p>
      On a centralized service, if your account is deleted or banned, you are
      gone for good.
    </p>

    <h2>
      Use protocols
    </h2>
    <p>
      Every time we choose a service over a protocol, we opt into a system where
      a single company can be compelled to <strong>identify us, restrict us, or
        hand over our data</strong>, to their profit or out government's
      advantage.
    </p>
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I guess I kinda get why people hate AI (126 pts)]]></title>
            <link>https://anthony.noided.media/blog/ai/programming/2026/02/14/i-guess-i-kinda-get-why-people-hate-ai.html</link>
            <guid>47037628</guid>
            <pubDate>Mon, 16 Feb 2026 17:22:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anthony.noided.media/blog/ai/programming/2026/02/14/i-guess-i-kinda-get-why-people-hate-ai.html">https://anthony.noided.media/blog/ai/programming/2026/02/14/i-guess-i-kinda-get-why-people-hate-ai.html</a>, See on <a href="https://news.ycombinator.com/item?id=47037628">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      
      <span>
        Feb 14, 2026
      </span>
      <p>I’m sitting on a lānai in a hotel in Waikiki beach, writing this article, and wondering if the job I am starting nine days from now will be my last.</p>

<p>This is a unique situation for me in a few ways—I’ve never been to Hawaii before, I think the five minutes it’s taken me to come up with that opening sentence is the somehow the most time I’ve ever spent on a hotel balcony, and this is the first time I’ve actually followed through on the “I should delay my start date to take a vacation” idea I’ve had every time I’ve switched jobs.
There’s one difference, however, that looms larger in my mind.
It’s not the “wondering if the new job will be my last” thing.
I’ve worked exclusively in startups, and while the primary reason I’ve done so has been because I enjoy the agency and impact you can have at early-stage companies, I’d be lying if the idea of cashing in cheap ISOs into early retirement wasn’t a factor in each job offer I accepted.
The difference here is <em>why</em> I’m wondering that.
Previously, it was wondering if I would <em>need</em> a job after this one.</p>

<p>Now, it’s wondering if I’ll be able to <em>acquire</em> a job after this one, or if AI is going to completely take over my profession and ruin my career.</p>

<!--more-->

<h2 id="not-the-first-time">Not the first time</h2>

<p>I’m not the first human to have anxiety about technological development.
Change is scary, and technology changes a lot of stuff.
In my opinion, these changes are mostly for the better—but that’s not an opinion everybody shares.</p>

<p>The classical cultural example is the Luddites, a social movement that failed so utterly that its name because a common metaphor for stubborn morons who are terrified of technological innovation that helps everybody.
Deservedly so, to be clear—while it’s true that textile experts did suffer from the advent of mechanical weaving, their loss was far outweighed by the gains the rest of the human race received from being able to afford more than two shirts over the average lifespan.</p>

<p>The other example that comes to mind is the (possibly apocryphal) stories around the rollout of ATMs, where many supposedly predicted that the number of bankers in the US would collapse now that you could withdraw $20 in singles to leave tips without talking to a person.
The exact opposite happened, of course.
Being able to easily interact with banks, without waiting in a line that’s too long for the dum-dum you get at the end to be a real consolation, made people use banks <em>more</em>.
And suddenly tellers became loan managers, and account advisors, and the machine that was supposed to destroy banking employment wound up supercharging it.</p>

<p>I could go on, but <a href="https://qz.com/1681832/the-history-of-the-future-of-work">somebody else already has</a>, so there’s not much point in it.
Technology changes things, and sometimes it hurts people in the short-term, but every invention from fire to mRNA vaccines has wound up generally increasing human welfare.
I’ve long taken the view that this trend will continue.
I remember arguing with people who would link GCP Gray’s <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">“Humans need not apply”</a> video (which has apparently been retitled “humans are becoming horses”) about how wrong they were about AI.
In that era, nearly a decade before “Attention is all you need” would be published and usher in the LLM age, I was <em>so</em> confident that any developments in AI would be for the better.</p>

<p>I am now a little less confident than I was.</p>

<h2 id="not-a-hater">Not a hater</h2>

<p>I don’t hate AI.
Earlier today I was asking Gemini to find me a nearby bar that would optimize for price, tastiness of drinks, and “not making me feel lonely as a solo traveler on Valentine’s day.”
Gemini did include a speed-dating event at a local hotel as a response to that prompt, which I am choosing to interpret as it having supreme confidence in my charisma, rather than it making fun of me.
It’s also been helpful doing various tasks on <a href="https://www.github.com/AnthonySuper/noided-web">the weird little Haskell framework</a> I’m working on, and at my former place of employment, and I plan on using it at my newest place of employment.</p>

<p>If I’m to believe the boosters, like Sam Altman or obscure indie filmmaker <a href="https://youtu.be/m2IV0BajqAk?t=3213">Neil Breen</a>, AI could be humanity’s last invention, a machine we can hand the keys to and let it solve literally all of our problems.
And to some extent, that does kind of appeal.
It would be great if I could type “how can I be happy” in a prompt console somewhere and get back a step-by-step process to achieve enlightenment.
And we could also cure cancer, or whatever.
Sounds nice, right?</p>

<p>So why is this blog post titled “I guess I kinda get why people hate AI” as opposed to “AI haters are stupid and wrong?”</p>

<h2 id="the-people-in-charge-of-ai-keep-telling-me-to-hate-it">The people in charge of AI keep telling me to hate it</h2>

<p>Before I get into concerns I’ve found on my own, let me get the most blatantly obvious and infuriating reason that people might hate AI out of the way: <em>the people inventing it are telling me I should hate it</em>.
I’ve never seen this with any technological development, ever, in my life.
Henry Ford did not market the Model T as “a machine that will eventually cause environmental destruction, social isolation via car dependency, and health issues from pollution.”
The guy who invented penicillin didn’t say “one day this will lead to MRSA.”
People generally try to market new technologies by telling you their upsides, not their downsides.</p>

<p>But Microsoft’s AI CEO is saying <a href="https://www.businessinsider.com/microsoft-ai-ceo-mustafa-suleyman-white-collar-tasks-automation-prediction-2026-2">AI is going to take everybody’s job</a>.
And Sam Altman is saying that AI will <a href="https://www.yahoo.com/news/articles/sam-altman-says-openai-poised-130000116.html">wipe out <em>entire categories</em> of jobs</a>.
ANd Matt Shumer is saying that AI is currently like <a href="https://x.com/mattshumer_/status/2021256989876109403">Covid in January 2020</a>—as in, “kind of under the radar, but about to kill millions of people”.</p>

<p>I legitimately feel like I am going insane when I hear AI technologists talk about the technology.
They’re supposed to market it.
But they’re instead saying that it is going to leave me a poor, jobless wretch, a member of the “permanent underclass,” as the meme on Twitter goes.
Half the videos and blog posts I see about new models boil down to somebody running it through a benchmark, then saying “chat we’re cooked, might as well end it all now.”
This isn’t just a strange way of marketing a product, it is a completely psychotic one.</p>

<p>That’s not the only way people are talking about it, of course.
I liked Anthropic’s <a href="https://www.youtube.com/watch?v=FDNkDBNR7AM">superbowl ad for Claude</a>, and not just because it used “ALL CAPS” by MF DOOM as the backing track.
The idea of AI as an exterminator of human problems is much more appealing than AI as the exterminator of, you know, the career of me and everybody else on Earth.
But, somehow, “AI is good and will help you” is a less common marketing tactic than “AI will ruin you life” among <em>people in charge of AI companies</em>.</p>

<p>It’s completely fucking baffling to me.
I can’t understand it, unless all those AI marketing materials are really meant for the ultra-wealthy, and not for me.
“Fund this and you can become a permanent overclass and have millions of enslaved serfs bowing to your machine-god” is, I suppose, an appealing tactic to some kinds of people.
Or maybe the real message is “you should panic because if you don’t invest all your money in my company right now you’ll be a peasant like everybody else” is the real pitch.
I don’t know.
I’m not rich enough to be in that target demographic.</p>

<h2 id="what-if-they-believe-it">What if they believe it?</h2>

<p>The counter-argument to this is that people aren’t marketing, they’re just expressing their view of reality.
Microsoft’s AI CEO doesn’t <em>want</em> AI to ruin millions of lives, but it’s going to, so he has to be honest.</p>

<p>Okay, fine.
If that’s the case, I would encourage AI companies to lobby, right now, for world governments to pass legislation to deal with the problem they see on the horizon.
Now, obviously it would be a bit premature to enact a UBI right now, <em>before</em> AI takes over all work, but you don’t actually have to: you can pass a law with a trigger condition.
If unemployment rises above a certain percent while GDP is still growing, have additional taxes start kicking in, used to fund job training or UBI or whatever else you think is needed to prevent the “permanent underclass” from forming in the first place.
If AI doesn’t actually take everybody’s jobs, and it winds up being more similar to other technologies, great—the trigger conditions never kick in, and the law stays inactive.
If the AI jobpocalypse <em>does</em> happen, you don’t need to scramble to get anything done, the laws to deal with it (or at least help) are already on the books.</p>

<p>I’ve not seen a single AI CEO propose anything like this.
Various people have speculated that we might need laws, eventually, after AI takes over everybody’s jobs.
But if we’re actually two years out from <em>every office worker being automated</em>, which is <a href="https://www.dpeaflcio.org/factsheets/the-professional-and-technical-workforce-by-the-numbers">57.8% of the workforce</a>, we need to start legislating <em>right now</em> to have any hope of this not being a total disaster.
I can only think of three reasons why nobody is proposing this:</p>

<ol>
  <li>They are actually far, <em>far</em> less confident than they say that AI will actually get that good.</li>
  <li>They lack the imagination to think of the idea of passing a law with a trigger condition.
Such laws <a href="https://en.wikipedia.org/wiki/Automatic_stabilizer">do exist</a> but aren’t super well-publicized.
So this is actually a possibility.</li>
  <li>They don’t actually care about what their products may do to society—they just want to be sure they win the AI race, damn the consequences.</li>
</ol>

<p>To me, none of these are a good look.</p>

<h2 id="ai-right-now-has-a-few-huge-downsides">AI right now has a few huge downsides</h2>

<p>I have a friend who is a new TA at a university in California.
They’ve had to report <em>several students</em>, every semester, for basically pasting their assignments into ChatGPT.</p>

<p>They didn’t find this out via careful analysis, or use of any of the dubious AI detector tools.
The students didn’t even try to hide their use of ChatGPT—sometimes they literally left in the “would you like me to also do (related thing)?” that every AI puts at the end of their responses in the essays they submitted.
Total laziness, but laziness that these students presumably got away with in high school.
To my friend, their primary experience with AI is seeing students rob themselves of the opportunity to learn, so they can…
I dunno, hit the vape and watch Clavicular get framemogged, or whatever the hell Gen Z does.</p>

<p>In my own personal experience, my dad enthusiastically sent me a video about Elon’s new “smart house” initiative.
I realized, right away, that the video was AI generated, but I assumed it was a generated summary of some real press release.
Nope!
Every component of it was made up.
It was a top-to-bottom scam.
I researched this for like ten minutes, just to be sure, before gently telling my dad that it was fake.
He handled it well, apologized, and was clearly embarrassed.
But why should he be?
The video had graphics, good narration, music—things that used to be a sign of some degree of effort or sincerity.
Now, all of those signals are totally worthless.
AI is able to slop out fake content just as easily.</p>

<p>Then—and this is petty—I’ve also been subjected to “slop” myself, in the form of bizarre <a href="https://www.theguardian.com/culture/2025/aug/18/ai-has-created-a-new-breed-of-cat-video-addictive-disturbing-and-nauseatingly-quick-soap-operas">cat soap opera</a> videos that appeared in my TikTok feed.
Besides being unpleasant to look at, these shorts have weird racial undertones that are deeply, deeply strange and unsettling to me.
And even though I do the entire “long press and select ‘show less’” thing TikTok provides, they still sneak in, and they frankly irritate me to an irrational degree.</p>

<p>Then I hear about <code>cURL</code> having to <a href="https://news.ycombinator.com/item?id=46701733">stop their bug bounty program</a> because of so many AI submissions that hallucinate fake bugs.
Or I look at RAM prices, which have gone completely nuclear, largely because AI companies are buying so much of it.</p>

<p>I get that every technology has friction as its adopted.
I’m sure the first machine-made textiles were of vastly lower quality than anything you could get from even the worst hand-weaver.
AI, however, currently occupies a zone where it’s sometimes very helpful for doing high quality work, but <em>always</em> helpful for doing bullshit slop.
People could always write false press releases about smart houses, but it required them to actually <em>write</em> it.
People could always buy an Elsa costume and a Spiderman suit and make weirdly sexual slop videos, but at least they had to go to party city and buy a Sony camcorder and an SD card.
People could always hire a cheating service to write essays for them, but at least <em>somebody</em> would write the essay.
AI has lowered the barrier to entry for all of these things to the point where they’re effectively free.
Garbage, but free to produce.
And that does make some people’s primary interaction with the technology profoundly negative.</p>

<p>That’s not to say that there <em>aren’t</em> solutions.
Websites could use government IDs to verify that people are human, so spambots can stay out.
After selecting the “please stop showing me videos of anthropomorphic orange tabby cats being cuckolded by black-furred anthropomorphic cats” button on TikTok it eventually wised up and stopped showing me similar content.
Eventually somebody else is going to open up a RAM plant when you can get such stupidly high margins on sticks of DDR5.</p>

<p>But all of these solutions are <em>irritating</em>, difficult, and, frankly, a lot of work.
In some cases they’re even actively dangerous—considering how often companies <a href="https://discord.com/press-releases/update-on-security-incident-involving-third-party-customer-service">leak information</a>, giving your ID to a website to verify you’re not a slop-bot severely increases your data privacy risks.
Technology is supposed to save you from working.
For many, AI isn’t doing that.
It’s doing the opposite.</p>

<h2 id="so-i-guess-i-get-it">So I guess I get it.</h2>

<p>To be clear, I think AI will be ultimately extremely helpful.
I still am using it on my projects.
I am going to use it at my next job.
I, <em>personally</em>, don’t hate AI.</p>

<p>But I can’t deny that the vibes right now are <em>awful</em>.</p>

<p>Not just bad, <em>awful</em>.
It’s not just the “chat we’re cooked you’re the permanent underclass” stuff influencers say.
It’s not just the “everybody is fucked” hyperbole CEOs sprout.
It’s the actual, day-to-day experience with the technology.
I’m a programmer—AI actually helps me a <em>lot</em>.
But for normal people, their interactions are profoundly more negative, and none of the people behind this technology seem to care.</p>

<p>And I can’t help but wonder… What if the vibes get worse?</p>

<p>What if I actually lose my job?
What if I’m begging for change in six months, a new member of the permanent underclass?
What if AI actually automates all the fulfilling, interesting parts of life, and humans comparative advantage winds up being exclusively in scrubbing toilets and similar manual tasks?</p>

<p>Or, what if AI continues to lower the barrier to entry for annoying, low-quality things—and <em>never</em> gets to the point where it’s truly great?
What if dead internet theory becomes true, and we all drown in an avalanche of slop?</p>

<p>AI ushering in a cyberpunk dystopia would at least be interesting.
But right now I’m worried it’s just going to result in things becoming kind of generally worse, effectively rolling back a lot of innovations of the internet and social media and such by making such things totally unusable.</p>

<h2 id="i-dont-want-to-doom">I don’t want to doom</h2>

<p>To be clear: I like and use AI when it comes to coding, and even for other tasks.
I think it’s been very effective at increasing my productivity—not as effective as the influencers claim it should be, but effective nonetheless.
There’s a reason this blog post is not titled “I now hate AI,” because I don’t.</p>

<p>But, at times, it feels like the AI companies <em>want</em> me to.
Not only through their baffling marketing that I spent so long ranting about, but also through their seeming lack of interest in counter-acting any of the negative effects of their product.
Beyond the idea of lobbying for legislation in case of a job apocalypse, there’s a few simpler steps they could take:</p>

<ul>
  <li>They could form an alliance to make it easy for platforms to automatically disclose when a video has AI video or audio content (via watermarking everything they generate), and encourage every platform to do so.
Forming some kind of consortium or alliance on this would be ideal.</li>
  <li>YouTube could be substantially more aggressive about banning AI-generated misinformation.
I’m not suggesting the truth police here—I don’t want YouTube marking Michael Jordan compilations that call him the GOAT “misinformation” because LeBron exists.
But if somebody is uploading AI-narrated videos claiming that Michael Jordan and LeBron James have announced that they’re getting married, such videos should at least be gated
behind a “this is AI bullshit” screen, if not banned outright.</li>
  <li>Allow big open-source maintainers to request “no AI vulnerability finding” and have a layer in the models that enforces that.
I don’t know if this is technically feasible, but if you can add “No trying to find vulnerabilities in these projects:” to the system prompt and it works most of the time, that would at least be <em>something</em>.</li>
</ul>

<p>I know local models exist, and that it would be impossible to uphold all these ideas for their users.
But right now I would wager the vast majority of LLM usage is through cloud-based services, and on those, it <em>would</em> be possible to do, and it would at least be <em>something</em>—some acknowledgement that, for all its benefits, AI has actually made several important things worse.
So, while I personally <em>don’t</em> hate AI, I can see why people <em>do</em>.
And I can see why that hatred is seemingly becoming more common.
And I can even see a world where, within a few months, I write a follow-up post to this one entitled “Why I Now Hate AI,” even if the boosters are <em>wrong</em> and it doesn’t cause a job apocalypse.</p>

<p>And that is completely crazy to me, because AI is really useful to me!
AI has allowed me to eliminate the most annoying, manual, inelegant, and soul-crushing parts of my profession!
If <em>I</em> can somehow hate a machine that has <em>basically stopped me from having to write boring boilerplate code</em>, of course others are going to hate it!</p>

<p>Yet AI companies currently don’t seem to care.
Maybe they shouldn’t.
Maybe this is the final technological race, and some day soon Anthropic or Google or OpenAI is going to turn on a new model, birth a machine god, and instantly take over the world.
Maybe they think that they can care about the vibes after, once that’s finished.
Their Super-AGI will write the UBI law, and get it passed, when it has a few minutes between curing cancer and building a warp drive.
Or maybe they think the Super-AGI will be able to turn any of us peasants who oppose them into <a href="https://cepr.org/voxeu/columns/ai-and-paperclip-problem">paperclips</a> and they’ll get to rule as a dictator for all time.
In either of those cases, the current bad vibes don’t matter.</p>

<p>But I am not too sure about that.
Even if we’re five years away from a godlike, all-knowing super-intelligence—a timeline I think is probably off by at least an order of magnitude—that’s a lot of time for us to idle in a local minima where the average person’s experience of AI is profoundly negative.
Our society could fracture in unpredictable ways, and, eventually, suffer so badly people break out the torches and pitchforks and burn their local data center to the ground.</p>

<p>I would like to avoid that outcome.
Frankly, I don’t think doing so will even be too difficult.
I just wish the big AI labs thought it was worthwhile to even try.</p>


      
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iOS 27 'Rave' Update to Clean Up Code, Could Boost Battery Life (103 pts)]]></title>
            <link>https://www.macrumors.com/2026/02/16/apple-plans-snow-leopard-cleanup-ios-27/</link>
            <guid>47035718</guid>
            <pubDate>Mon, 16 Feb 2026 14:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2026/02/16/apple-plans-snow-leopard-cleanup-ios-27/">https://www.macrumors.com/2026/02/16/apple-plans-snow-leopard-cleanup-ios-27/</a>, See on <a href="https://news.ycombinator.com/item?id=47035718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2026/02/16/apple-plans-snow-leopard-cleanup-ios-27/"><p>Apple's iOS 27 update will prioritize cleaning up the operating system's internals, with engineers making changes that could result in better battery life, according to <a href="https://www.bloomberg.com/news/newsletters/2026-02-15/tesla-carplay-delays-related-to-ios-26-and-fsd-apple-s-new-siri-delays-ios-27"><em>Bloomberg</em>'s Mark Gurman</a>.</p>
<p><img src="https://images.macrumors.com/t/Jav40hKvhCs-fCAfcg0nmFK7O5s=/400x0/article-new/2025/11/iOS-27-Mock-Quick.jpg?lossy" srcset="https://images.macrumors.com/t/Jav40hKvhCs-fCAfcg0nmFK7O5s=/400x0/article-new/2025/11/iOS-27-Mock-Quick.jpg?lossy 400w,https://images.macrumors.com/t/PVOgLwWhGeS39JB8QE-rZ87zu7M=/800x0/article-new/2025/11/iOS-27-Mock-Quick.jpg?lossy 800w,https://images.macrumors.com/t/jRsQNWkQjvrremRHU2E3cnHUdro=/1600x0/article-new/2025/11/iOS-27-Mock-Quick.jpg 1600w,https://images.macrumors.com/t/Cmj55YNKcGgTstD0hG-wkgMM80g=/2500x0/filters:no_upscale()/article-new/2025/11/iOS-27-Mock-Quick.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 27 Mock Quick" width="2500" height="1406"><br>The effort is said to be similar to what Apple did with its Snow Leopard Mac update years ago, and will involve removing old code, rewriting existing features, and subtly upgrading apps to improve their performance.</p>
<p>The result should hopefully be a "snappier, more responsive" OS, says Gurman. Apple is also reportedly planning some interface tweaks, but nothing as dramatic as the Liquid Glass overhaul introduced with iOS 26, which will likely <a href="https://www.macrumors.com/2026/02/13/macos-tahoe-finder-bug-slipping-ui-polish/">comfort some users</a>.</p>
<p>Code-named "Rave" internally, iOS 27 will also include efficiency improvements that Apple hopes will translate into tangible battery gains for users, says Gurman. However, it's unclear whether Apple would market those improvements or simply let users discover them on their own.</p>
<p>Gurman says getting the software into good shape is especially important as Apple prepares to launch new device categories, including a touchscreen MacBook Pro and its first foldable iPhone, both of which are expected in the second half of 2026.</p>
<p>The cleanup effort comes alongside Apple's other major iOS 27 priority of <a href="https://www.macrumors.com/2026/02/12/siri-ios-26-launch-confirmed-apple/">improving its AI capabilities</a>. The revamped, chatbot-style Siri that Apple announced in June 2024 has been repeatedly delayed, and some of its features are now expected to arrive in iOS 27 rather than iOS 26, <a href="https://www.macrumors.com/2026/02/11/siri-features-delayed-ios-26-4/">reports Gurman</a>.</p>
<p><a href="https://www.bloomberg.com/subscriptions/checkout?id=D28F04"><img src="https://images.macrumors.com/t/6Ve8995RzEQOT6s7CMze_yE9iJY=/400x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy" srcset="https://images.macrumors.com/t/6Ve8995RzEQOT6s7CMze_yE9iJY=/400x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy 400w,https://images.macrumors.com/t/BPMTBuOQ0TeLGmafQqIVw8fpULk=/800x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy 800w,https://images.macrumors.com/t/HLHB34HPajJzIh9z3dlk6T6Mg-E=/1600x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg 1600w,https://images.macrumors.com/t/9B2wUG3w9m66F17epk3RrLnn96Q=/2500x0/filters:no_upscale()/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="MacRumors x Bloomberg Banner Warm" width="2500" height="559" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/6Ve8995RzEQOT6s7CMze_yE9iJY=/400x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy" data-srcset="https://images.macrumors.com/t/6Ve8995RzEQOT6s7CMze_yE9iJY=/400x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy 400w,https://images.macrumors.com/t/BPMTBuOQ0TeLGmafQqIVw8fpULk=/800x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg?lossy 800w,https://images.macrumors.com/t/HLHB34HPajJzIh9z3dlk6T6Mg-E=/1600x0/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg 1600w,https://images.macrumors.com/t/9B2wUG3w9m66F17epk3RrLnn96Q=/2500x0/filters:no_upscale()/article-new/2025/06/MacRumors-x-Bloomberg-Banner-Warm.jpg 2500w"></a></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2026/02/12/two-new-apple-products-coming-soon/">Apple's Next Two Products Are Coming Soon</a></h3><p>Thursday February 12, 2026 11:17 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple plans to release an iPhone 17e and an iPad Air with an M4 chip "in the coming weeks," according to the latest word from Bloomberg's Mark Gurman.
"Apple retail employees say that inventory of the iPhone 16e has basically dried out and the iPad Air is seeing shortages as well," said Gurman. "I've been expecting new versions of both (iPhone 17e and M4 iPad Air) in the coming weeks."...</p></div><div><h3><a href="https://www.macrumors.com/2026/02/13/apple-launching-sales-coach-app/">Apple Launching New 'Sales Coach' App</a></h3><p>Friday February 13, 2026 2:01 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple plans to launch a rebranded "Sales Coach" app on the iPhone and iPad later this month, according to a source familiar with the matter.
"Sales Coach" will arrive as an update to Apple's existing "SEED" app, and it will continue to provide sales tips and training resources to Apple Store and Apple Authorized Reseller employees around the world. For example, there are articles and videos...</p></div><div><h3><a href="https://www.macrumors.com/2026/02/13/five-iphone-18-pro-features-revealed-in-new-report/">Five iPhone 18 Pro Features Revealed in New Report</a></h3><p>Friday February 13, 2026 8:43 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>While the iPhone 18 Pro and iPhone 18 Pro Max are still seven months away, an analyst has revealed five new features the devices will allegedly have.
Rumored color options for the iPhone 18 Pro models 
In a research note with investment firm GF Securities on Thursday, analyst Jeff Pu outlined the following upgrades for the iPhone 18 Pro models:
  Smaller Dynamic Island: It has been rumored...</p></div><div><h3><a href="https://www.macrumors.com/2026/02/16/apple-announces-special-event-in-new-york/">Apple Announces Special Event in New York, London, and Shanghai on March 4</a></h3><p>Apple today announced a "special Apple Experience" in New York, London, and Shanghai, taking place on March 4, 2026 at 9:00am ET.
Apple invited select members of the media to the event in three major cities around the world. It is simply described as a "special Apple Experience," and there is no further information about what it may entail. The invitation features a 3D Apple logo design...</p></div><div><h3><a href="https://www.macrumors.com/2026/02/13/apple-home-2026-rumors/">Three New Apple Home Products Rumored for 2026</a></h3><p>Friday February 13, 2026 4:18 pm PST by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple has a long list of new products rumored for 2026, including a series of home products that will see the company establishing more of a presence in the smart home space. Robots are on the horizon for 2027, but the 2026 releases will be a little tamer.
HomePod mini
We're expecting a new HomePod mini 2 to launch at any time. Apple isn't going to update the device's design, but we could...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK Discord users were part of a Peter Thiel-linked data collection experiment (261 pts)]]></title>
            <link>https://www.rockpapershotgun.com/good-news-uk-discord-users-were-part-of-a-peter-thiel-linked-data-collection-experiment</link>
            <guid>47035679</guid>
            <pubDate>Mon, 16 Feb 2026 14:48:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rockpapershotgun.com/good-news-uk-discord-users-were-part-of-a-peter-thiel-linked-data-collection-experiment">https://www.rockpapershotgun.com/good-news-uk-discord-users-were-part-of-a-peter-thiel-linked-data-collection-experiment</a>, See on <a href="https://news.ycombinator.com/item?id=47035679">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article data-ads="true" data-article-type="news" data-article-group="news" data-paywalled="false" data-premium="false" data-sponsored="false" data-type="article">


<header data-component="article-header">
  

    <div id="main-content">
  

        <p>"The information you submit will be temporarily stored for up to 7 days..."</p>

    </div>


  <div>

  <figure>
        <picture>
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=380&amp;quality=85&amp;format=jpg&amp;auto=webp" media="(max-width: 549px) and (max-resolution: 1dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=380&amp;quality=85&amp;format=jpg&amp;dpr=1.5&amp;auto=webp" media="(max-width: 549px) and (max-resolution: 1.5dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=380&amp;quality=85&amp;format=jpg&amp;dpr=1.75&amp;auto=webp" media="(max-width: 549px) and (max-resolution: 1.75dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=380&amp;quality=85&amp;format=jpg&amp;dpr=2&amp;auto=webp" media="(max-width: 549px) and (max-resolution: 2dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=380&amp;quality=85&amp;format=jpg&amp;dpr=3&amp;auto=webp" media="(max-width: 549px) and (max-resolution: 3dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=690&amp;quality=85&amp;format=jpg&amp;auto=webp" media="(min-width: 550px) and (max-resolution: 1dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=690&amp;quality=85&amp;format=jpg&amp;dpr=1.5&amp;auto=webp" media="(min-width: 550px) and (max-resolution: 1.5dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=690&amp;quality=85&amp;format=jpg&amp;dpr=1.75&amp;auto=webp" media="(min-width: 550px) and (max-resolution: 1.75dppx)">
        <source srcset="https://assetsio.gnwcdn.com/Discordageverification2.png?width=690&amp;quality=85&amp;format=jpg&amp;dpr=2&amp;auto=webp" media="(min-width: 550px) and (max-resolution: 2dppx)">
        <img src="https://assetsio.gnwcdn.com/Discordageverification2.png?width=690&amp;quality=85&amp;format=jpg&amp;dpr=3&amp;auto=webp" alt="A screengrab of a prompt to verify your age on Discord." loading="eager" fetchpriority="high" data-uri="Discordageverification2.png" data-lightbox="" width="690" height="388">
      </picture>

        <figcaption>
          <span>Image credit: <cite>Discord / Rock Paper Shotgun</cite></span>
        </figcaption>

  </figure>
  </div>

    

</header>
  <div data-component="article-content">



            <p>
Discord have belatedly confirmed that they're working with Persona, an identity detection firm backed by a fund directed by Palantir chairman Peter Thiel, as part of Discord's <a href="https://www.rockpapershotgun.com/discord-roll-out-global-age-verification-system-including-an-age-inference-model-that-runs-in-the-background">new global age verification system rollout</a>. The collaboration is described as an "experiment" involving people in the UK specifically, whereby Persona will store user information on their servers for up to seven days. 
</p>
<!--more-->
<p>
Always good when your personal data forms part of an "experiment", isn't it? Never mind that Discord <a href="https://www.rockpapershotgun.com/discord-promise-they-wont-force-everybody-to-do-a-face-scan-following-outrage-at-new-age-verification-policy">assured us earlier</a> that "identity documents submitted to our vendors are deleted quickly--in most cases, immediately after age confirmation." Discord haven't yet said what the "experiment" is supposed to explore or prove. I'm sure it's fine, though. It's not like Thiel joints have a track record of working with any bloodthirsty snoops.
</p>
<p>
Following the announcement of Discord's new age verification policy - already in force in the UK and Australia, with a global rollout beginning in early March - social media users <a href="https://x.com/GiveMeBanHammer/status/2021851054519001133">shared screengrabs</a> of prompts to consent to Persona collecting their data over the weekend. Discord then sought to calm the flames by updating <a href="https://support.discord.com/hc/en-us/articles/30326565624343-How-to-Complete-Age-Assurance-on-Discord">their site FAQ</a> with the below disclaimer:
</p>
<blockquote>"If you're located in the UK, you may be part of an experiment where your information will be processed by an age-assurance vendor, Persona. The information you submit will be temporarily stored for up to 7 days, then deleted. For ID document verification, all details are blurred except your photo and date of birth, so only what's truly needed for age verification is used."</blockquote>
<p>
Many Discord users were already hopping mad about the new "age assurance" system, which involves face scan videos and a machine learning model, and the revelation of Persona's involvement only made them madder, with critical coverage appearing in <a href="https://kotaku.com/discord-palantir-peter-thiel-persona-age-verification-2000668951">Kotaku</a>, <a href="https://www.eurogamer.net/discord-advises-uk-users-that-they-may-be-part-of-an-experiment-where-instead-of-their-age-verification-data-never-leaving-their-phone-it-will-now-actually-leave-their-phone">Eurogamer</a> and cheery RPS fanzine <a href="https://www.pcgamer.com/software/platforms/oh-good-discords-age-verification-rollout-has-ties-to-palantir-co-founder-and-panopticon-architect-peter-thiel/">PCGamer</a>. The FAQ disclaimer has now vanished (here's an <a href="https://web.archive.org/web/20260214070331/https://support.discord.com/hc/en-us/articles/30326565624343-How-to-Complete-Age-Assurance-on-Discord">older version</a> preserved by the Wayback Machine).
</p><p>
As PCGamer note, Persona's lead investors during two recent rounds of venture capital funding were The Founders Fund, who <a href="https://www.bloomberg.com/news/articles/2021-09-15/founders-fund-values-identity-startup-persona-at-1-5-billion">valued them at $1.5 billion</a> in 2021. The Founders Fund were co-founded by Peter Thiel in 2020. Aside from being a <a href="https://www.theguardian.com/us-news/2025/oct/10/peter-thiel-lectures-antichrist">well-heeled Doomsday cultist</a> and <a href="https://aftermath.site/jeffrey-epstein-files-kotick-thiel-xbox-rockstar/">frequent Epstein correspondent</a>, Peter Thiel is one of the moneymen behind omni-payment platform Paypal and, more recently, Palantir, a godawful work of Saruman fan fiction that specialises in using AI for government and military surveillance. 
</p>
<p>
Palantir have, among other things, worked extensively with the USA's Immigration and Customs Enforcement, aka ICE, to track undocumented migrants, amid allegations of human rights breaches that include the recent <a href="https://www.bbc.co.uk/news/articles/cr571qg4m61o">killing of an ICU nurse</a>. The UK government have <a href="https://www.theguardian.com/society/2026/feb/12/nhs-deal-with-ai-firm-palantir-called-into-question-after-officials-concerns-revealed">commissioned</a> Palantir to make a patient database for the NHS, despite much opposition from doctors. I know children aren't responsible for the sins of their parents, but it doesn't seem wholly irrelevant here that Palantir's UK division is headed by Oswald Mosley's grandson.
</p>
<p>
All told, I would prefer not to participate in any identity verification "experiment" bearing Thiel's fingerprints, particularly not one that uses machine learning to check your identity in the background. And this is before we get into Discord's <a href="https://www.theguardian.com/games/2025/oct/07/discord-data-breach-proof-of-age-id-leaked">recent history of privacy breaches involving third parties</a>.
</p>

        </div>

      </article>




        
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Your Bluetooth Devices Reveal About You (188 pts)]]></title>
            <link>https://blog.dmcc.io/journal/2026-bluetooth-privacy-bluehood/</link>
            <guid>47035560</guid>
            <pubDate>Mon, 16 Feb 2026 14:39:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.dmcc.io/journal/2026-bluetooth-privacy-bluehood/">https://blog.dmcc.io/journal/2026-bluetooth-privacy-bluehood/</a>, See on <a href="https://news.ycombinator.com/item?id=47035560">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Building Bluehood, a Bluetooth scanner that reveals what information we leak just by having Bluetooth enabled on our devices.</p><section><p>If you’ve read much of this blog, you’ll know I have a <a href="https://blog.dmcc.io/privacy">thing for privacy</a>. Whether it’s <a href="https://blog.dmcc.io/journal/tor-relay-onion-location/">running my blog over Tor</a>, <a href="https://blog.dmcc.io/journal/tailscale-adguard-dns/">blocking ads network-wide with AdGuard</a>, or <a href="https://blog.dmcc.io/journal/proton-pass-cli-linux-secrets/">keeping secrets out of my dotfiles with Proton Pass</a>, I tend to think carefully about what data I’m exposing and to whom.</p><p>Last weekend I built <a href="https://github.com/dannymcc/bluehood">Bluehood</a>, a Bluetooth scanner that tracks nearby devices and analyses their presence patterns. The project was heavily assisted by AI, but the motivation was entirely human: I wanted to understand what information I was leaking just by having Bluetooth enabled.</p><p>The timing felt right. A few days ago, researchers at KU Leuven disclosed <a href="https://whisperpair.eu/">WhisperPair</a> (CVE-2025-36911), a critical vulnerability affecting hundreds of millions of Bluetooth audio devices. The flaw allows attackers to hijack headphones and earbuds remotely, eavesdrop on conversations, and track locations through Google’s Find Hub network. It’s a stark reminder that Bluetooth isn’t the invisible, harmless signal we treat it as.</p><h2 id="the-problem-nobody-talks-about">The Problem Nobody Talks About</h2><p>We’ve normalised the idea that Bluetooth is always on. Phones, laptops, smartwatches, headphones, cars, and even medical devices constantly broadcast their presence. The standard response to privacy concerns is usually “nothing to hide, nothing to fear.”</p><p>But here’s the thing: even if you have nothing to hide, you’re still giving away information you probably don’t intend to.</p><p>From my home office, running Bluehood in passive mode (just listening, never connecting), I could detect:</p><ul><li>When delivery vehicles arrived, and whether it was the same driver each time</li><li>The daily patterns of my neighbours based on their phones and wearables</li><li>Which devices consistently appeared together (someone’s phone and smartwatch, for instance)</li><li>The exact times certain people were home, at work, or elsewhere</li></ul><p>None of this required any special equipment. A Raspberry Pi with a Bluetooth adapter would do the job. So would most laptops.</p><h2 id="devices-you-cant-control">Devices You Can’t Control</h2><p>What concerns me most isn’t that people choose to have Bluetooth enabled. It’s that many devices don’t give users the option to disable it.</p><p>Hearing aids are a good example. Modern hearing aids often use Bluetooth Low Energy so audiologists can connect and adjust settings or run diagnostics. Pacemakers and other implanted medical devices sometimes broadcast BLE signals for the same reason. The user can’t simply turn this off.</p><p>Then there are vehicles. Delivery vans, police cars, ambulances, logistics fleets, and trains often have Bluetooth-enabled systems for fleet management, diagnostics, or driver assistance. These broadcast continuously, and the drivers have no control over it.</p><p>Even consumer devices aren’t always straightforward. Many smartwatches need Bluetooth to function at all. GPS collars for pets require it to communicate with the owner’s phone. Some fitness equipment won’t work without it.</p><p>What’s interesting is that some of the most privacy-focused projects actually require Bluetooth to be enabled.</p><p><a href="https://briarproject.org/">Briar</a> is a peer-to-peer messaging app designed for activists and journalists operating in hostile environments. It doesn’t rely on central servers, and when the internet goes down, it can sync messages via Bluetooth or Wi-Fi mesh networks. It’s a genuinely useful tool for maintaining communications during internet blackouts or in areas with heavy surveillance.</p><p><a href="https://bitchat.free/">BitChat</a> takes this even further. It’s a decentralised messaging app that operates entirely over Bluetooth mesh networks—no internet required, no servers, no phone numbers. Each device acts as both client and relay, automatically discovering peers and bouncing messages across multiple hops to extend the network’s reach. The project explicitly targets scenarios like protests, natural disasters, and regions with limited or censored connectivity.</p><p>Both are genuinely excellent projects solving real problems. But to use them, you need Bluetooth enabled. And every device with Bluetooth enabled is broadcasting its presence to anyone nearby who cares to listen.</p><p>This creates a strange tension. Tools designed to protect privacy often require a feature that compromises privacy in other ways.</p><p>People often underestimate what patterns reveal. A bad actor with a Bluetooth scanner doesn’t need to know your name. They just need to observe behaviour over time.</p><p>Consider what someone could learn by monitoring Bluetooth signals in a residential area for a few weeks:</p><ul><li>When is the house typically empty?</li><li>Does someone visit every Thursday afternoon?</li><li>Is there a regular pattern that suggests shift work?</li><li>When do the children come home from school?</li><li>Which homes have the same delivery driver, suggesting similar shopping habits?</li></ul><p>If there’s damage to your property, you could potentially go back through the logs and see which devices were in range at that time. A smartwatch on a dog-walker passing by. A phone in someone’s pocket. A vehicle with fleet tracking.</p><p>These might seem like edge cases, but they illustrate a broader point: we’re constantly leaving digital breadcrumbs we don’t even think about.</p><h2 id="what-bluehood-actually-does">What Bluehood Actually Does</h2><p>Bluehood is a Python application that runs on anything with a Bluetooth adapter. It continuously scans for nearby devices, identifies them by vendor and BLE service UUIDs, and tracks when they appear and disappear.</p><p>The key features:</p><ul><li><strong>Passive scanning</strong>: It only listens. It doesn’t try to connect or interact with any device.</li><li><strong>Device classification</strong>: Phones, audio devices, wearables, vehicles, IoT devices, and more, identified by BLE fingerprints.</li><li><strong>Pattern analysis</strong>: Hourly and daily heatmaps, dwell time tracking, and detection of correlated devices.</li><li><strong>Filtering</strong>: Randomised MAC addresses (used by modern phones for privacy) are detected and hidden from the main view.</li><li><strong>Web dashboard</strong>: A simple interface for monitoring and analysis.</li></ul><p>You can run it in Docker or install it directly. It stores data in SQLite and optionally sends push notifications via <a href="https://ntfy.sh/">ntfy.sh</a> when watched devices arrive or leave.</p><h2 id="running-it">Running It</h2><p>The simplest way to try Bluehood is with Docker:</p><div><pre><code data-lang="bash">git clone https://github.com/dannymcc/bluehood.git
<span>cd</span> bluehood
docker compose up -d
</code></pre></div><p>The dashboard is available at <code>http://localhost:8080</code>.</p><p>If you prefer a manual install:</p><div><pre><code data-lang="bash">sudo pacman -S bluez bluez-utils python-pip  <span># Arch</span>
sudo apt install bluez python3-pip           <span># Debian/Ubuntu</span>

pip install -e .
sudo bluehood
</code></pre></div><p>Bluetooth scanning needs elevated privileges. You can either run as root, grant capabilities to Python, or use the included systemd service for always-on monitoring.</p><h2 id="the-point-of-all-this">The Point of All This</h2><p>Bluehood isn’t a hacking tool. It’s an educational demonstration of what’s possible with commodity hardware and a bit of patience.</p><p>I built it because I wanted to see for myself what I was broadcasting. The results were sobering. Even with no malicious intent, anyone with basic technical knowledge could learn a lot about my household just by sitting in their car and running a script.</p><p>This isn’t about paranoia. It’s about understanding the trade-offs we make when we leave wireless radios enabled on our devices. For some use cases, Bluetooth is essential. For others, it’s just convenience. Being aware of what you’re exposing is the first step to making informed decisions about which category your devices fall into.</p><p>If you try Bluehood and it makes you think twice about your own Bluetooth habits, it’s done its job.</p><hr><p>The source code is available on <a href="https://github.com/dannymcc/bluehood">GitHub</a>. Feedback and contributions welcome.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Sideprocalypse (134 pts)]]></title>
            <link>https://johan.hal.se/wrote/2026/02/03/the-sideprocalypse/</link>
            <guid>47035371</guid>
            <pubDate>Mon, 16 Feb 2026 14:26:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johan.hal.se/wrote/2026/02/03/the-sideprocalypse/">https://johan.hal.se/wrote/2026/02/03/the-sideprocalypse/</a>, See on <a href="https://news.ycombinator.com/item?id=47035371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You can't open a feed today without having AI boosters fling word salad like "agentic engineering" or "openclaw" into your beautiful but disapproving face. I'm terrible at predicting the future—you should ask me about selling NVIDIA stock in early 2022 some time—but one thing seems abundantly clear at this point.</p>
<p>There's a wonderful Swedish proverb called "elda för kråkorna" (building a fire for the crows) that evokes the futility of someone lighting a nice warm fire indoors and then throwing the doors wide open, inviting the snow and sleet. Are you one of the thousands of developers with dreams of building a little SaaS on the side? Something you've been thinking about for a while, hacking away at on evenings and weekends, dreaming of the day you can have a couple of hundred paying customers giving you $19.99 a month?</p>

<p>Sorry to be the bringer of bad news, but that dream's dead. Doornail. Dodo. <em>Parrot</em>.</p>

<p>Every minute you put into that thing, be it your own time or your LLM agent's, is a minute for the crows. Imagine that beautiful but fragile little idea of yours curled up in a freshly-dug pit in your backyard, Claude and Gemini standing over it, chuckling and high-fiving each other.</p>

<p>Listen: every idea you've ever had, every single one, some cocaine-addled sales critter has had too. <em>And they're better than you at SEO</em>.</p>

<p>What's that you're saying? Yours actually works and is higher quality, because you know about things like TTFP and INP and "not putting your Supabase god-token in the client"? Oh, you sweet summer child: I take no pleasure in this but I need to tell you that these things don't matter anymore. Quality is not a metric anyone cares about in 2026. We've all been conditioned to accept <a href="https://johan.hal.se/wrote/2023/02/17/what-to-expect-from-your-framework/" target="_blank">tombstone spinners on first load</a> or purchase flows that straight-up <a href="https://johan.hal.se/wrote/2024/02/28/care/" target="_blank">don't work</a>, and there are walled gardens and tollbooths everywhere you look. React ate the web, Safari kneecapped it, Google stopped linking to it, and none of the Zaibatsu US corporations who hold the cards want you to succeed. The future, if there is a future in software, lies in high-touch enterprise sales. There are a select few companies allowed to make money on the Internet today, and if you have any sense of self-preservation you need to glom onto one of those and hold it like the Dickens. Give up those childish dreams of independence.</p>

<p>If you're a hopeful SaaS builder you may be first in line, but you can at least take grim satisfaction in the fact that the sloptimists, the hype-men, the breathless agents-are-working-while-I-sleep people, they're equally fucked when this all comes home to roost. The marginal value of code today is—well, possibly not zero, since the people selling spades for this frantic gold rush are doing okay for now—but it's dropping like a lead balloon. Josh Collinsworth <a href="https://joshcollinsworth.com/blog/sloptimism" target="_blank">has it right</a>, AI boosterism is a class privilege. But rest assured that this revolution, too, will end up eating its own. They fancy themselves the masters of mighty bot armies, an unstoppable force at their fingertips that will build them a software empire. But nobody will find it, nobody will pay for it, and all those tokens will have been burned for someone else's gain, a sad bonfire offering to the datacenter and GPU crows cawing overhead.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running My Own XMPP Server (177 pts)]]></title>
            <link>https://blog.dmcc.io/journal/xmpp-turn-stun-coturn-prosody/</link>
            <guid>47034801</guid>
            <pubDate>Mon, 16 Feb 2026 13:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.dmcc.io/journal/xmpp-turn-stun-coturn-prosody/">https://blog.dmcc.io/journal/xmpp-turn-stun-coturn-prosody/</a>, See on <a href="https://news.ycombinator.com/item?id=47034801">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Notes from setting up Prosody in Docker for federated messaging, with file sharing, voice calls, and end-to-end encryption.</p><section><p>About a year ago I <a href="https://blog.dmcc.io/journal/2025_my_privacy_reboot/">moved my personal messaging to Signal</a> as part of a broader push to take ownership of my digital life. That went well. Most of my contacts made the switch, and I’m now at roughly 95% Signal for day-to-day conversations. But Signal is still one company running one service. If they shut down tomorrow or change direction, I’m back to square one.</p><p>XMPP fixes that. It’s federated, meaning your server talks to other XMPP servers automatically and you’re never locked into a single provider. Your messages live on your hardware. The protocol has been around since 1999 and it’s not going anywhere. I’d tried XMPP years ago and bounced off it, but the clients have come a long way since then. <a href="https://monal-im.org/">Monal</a> and <a href="https://conversations.im/">Conversations</a> are genuinely nice to use now.</p><p>This post covers everything I did to get a fully working XMPP server running with <a href="https://prosody.im/">Prosody</a> in Docker, from DNS records through to voice calls.</p><h2 id="prerequisites">Prerequisites</h2><ul><li>A server with Docker and Docker Compose</li><li>A domain you control</li><li>TLS certificates (Let’s Encrypt works well)</li></ul><h2 id="dns-records">DNS records</h2><p>XMPP uses SRV records to let clients and other servers find yours. You’ll need these in your DNS:</p><div><pre><code data-lang="fallback">_xmpp-client._tcp.xmpp.example.com  SRV  0 5 5222 xmpp.example.com.
_xmpp-server._tcp.xmpp.example.com  SRV  0 5 5269 xmpp.example.com.
</code></pre></div><p>Port 5222 is for client connections, 5269 is for server-to-server federation. You’ll also want an A record pointing <code>xmpp.example.com</code> to your server’s IP.</p><p>If you want HTTP file uploads (I’d recommend it), add a CNAME or A record for <code>upload.xmpp.example.com</code> pointing to the same server. Same for <code>conference.xmpp.example.com</code> if you want group chats with a clean subdomain, though Prosody handles this internally either way.</p><h2 id="tls-certificates">TLS certificates</h2><p>Prosody won’t start without certificates. I use Let’s Encrypt with the Cloudflare DNS challenge so I don’t need to expose port 80:</p><div><pre><code data-lang="bash">docker run --rm <span>\
</span><span></span>  -v ~/docker/xmpp/certs:/etc/letsencrypt <span>\
</span><span></span>  -v ~/docker/xmpp/cloudflare.ini:/etc/cloudflare.ini:ro <span>\
</span><span></span>  certbot/dns-cloudflare certonly <span>\
</span><span></span>  --dns-cloudflare <span>\
</span><span></span>  --dns-cloudflare-credentials /etc/cloudflare.ini <span>\
</span><span></span>  -d xmpp.example.com
</code></pre></div><p>The <code>cloudflare.ini</code> file contains your API token:</p><div><pre><code data-lang="ini"><span>dns_cloudflare_api_token</span> <span>=</span> <span>your-cloudflare-api-token</span>
</code></pre></div><p>After certbot runs, fix the permissions so Prosody can read the certs:</p><div><pre><code data-lang="bash">chmod -R <span>755</span> ~/docker/xmpp/certs/live/ ~/docker/xmpp/certs/archive/
chmod <span>644</span> ~/docker/xmpp/certs/archive/xmpp.example.com/*.pem
</code></pre></div><p>Set up a cron to renew monthly:</p><div><pre><code data-lang="bash"><span>0</span> <span>3</span> <span>1</span> * * docker run --rm -v ~/docker/xmpp/certs:/etc/letsencrypt <span>\
</span><span></span>  -v ~/docker/xmpp/cloudflare.ini:/etc/cloudflare.ini:ro <span>\
</span><span></span>  certbot/dns-cloudflare renew <span>\
</span><span></span>  --dns-cloudflare-credentials /etc/cloudflare.ini <span>\
</span><span></span>  <span>&amp;&amp;</span> docker restart xmpp
</code></pre></div><h2 id="the-docker-setup">The Docker setup</h2><p>The <code>docker-compose.yml</code>:</p><div><pre><code data-lang="yaml"><span>services</span>:<span>
</span><span>  </span><span>prosody</span>:<span>
</span><span>    </span><span>image</span>:<span> </span>prosodyim/prosody:<span>13.0</span><span>
</span><span>    </span><span>container_name</span>:<span> </span>xmpp<span>
</span><span>    </span><span>restart</span>:<span> </span>unless-stopped<span>
</span><span>    </span><span>ports</span>:<span>
</span><span>      </span>- <span>"5222:5222"</span><span>
</span><span>      </span>- <span>"5269:5269"</span><span>
</span><span>    </span><span>volumes</span>:<span>
</span><span>      </span>- prosody-data:/var/lib/prosody<span>
</span><span>      </span>- ./prosody.cfg.lua:/etc/prosody/prosody.cfg.lua:ro<span>
</span><span>      </span>- ./certs/live/xmpp.example.com/fullchain.pem:/etc/prosody/certs/xmpp.example.com.crt:ro<span>
</span><span>      </span>- ./certs/live/xmpp.example.com/privkey.pem:/etc/prosody/certs/xmpp.example.com.key:ro<span>
</span><span>
</span><span></span><span>volumes</span>:<span>
</span><span>  </span><span>prosody-data</span>:<span>
</span></code></pre></div><p>Two ports exposed: 5222 for clients, 5269 for federation. The data volume holds user accounts and message archives. Config and certs are mounted read-only.</p><h2 id="prosody-configuration">Prosody configuration</h2><p>This is the core of it. I’ll walk through the key sections rather than dumping the whole file.</p><h3 id="modules">Modules</h3><p>Prosody is modular. My module list:</p><div><pre><code data-lang="lua">modules_enabled <span>=</span> {
    <span>-- Core</span>
    <span>"roster"</span>; <span>"saslauth"</span>; <span>"tls"</span>; <span>"dialback"</span>; <span>"disco"</span>;
    <span>"posix"</span>; <span>"ping"</span>; <span>"register"</span>; <span>"time"</span>; <span>"uptime"</span>; <span>"version"</span>;

    <span>-- Security</span>
    <span>"blocklist"</span>;

    <span>-- Multi-device &amp; mobile</span>
    <span>"carbons"</span>; <span>"csi_simple"</span>;
    <span>"smacks"</span>;         <span>-- Stream Management (reliable delivery)</span>
    <span>"cloud_notify"</span>;   <span>-- Push notifications for mobile</span>

    <span>-- Message archive</span>
    <span>"mam"</span>;

    <span>-- User profiles &amp; presence</span>
    <span>"vcard_legacy"</span>; <span>"pep"</span>; <span>"bookmarks"</span>;

    <span>-- Admin</span>
    <span>"admin_shell"</span>;
}
</code></pre></div><p>The ones I found matter most for a good mobile experience: <code>carbons</code> syncs messages across all your devices instead of delivering to whichever one happened to be online. <code>smacks</code> (Stream Management) handles flaky connections gracefully, so messages aren’t lost when your phone briefly drops signal. <code>cloud_notify</code> enables push notifications so mobile clients don’t need a persistent connection, which is essential for battery life. And <code>mam</code> (Message Archive Management) stores history server-side for search and cross-device sync.</p><h3 id="security-settings">Security settings</h3><div><pre><code data-lang="lua">c2s_require_encryption <span>=</span> <span>true</span>
s2s_require_encryption <span>=</span> <span>true</span>
s2s_secure_auth <span>=</span> <span>true</span>
authentication <span>=</span> <span>"internal_hashed"</span>
allow_registration <span>=</span> <span>false</span>
</code></pre></div><p>All connections are encrypted and registration is disabled since I create accounts manually with <code>prosodyctl</code>. I’ve enabled <code>s2s_secure_auth</code>, which means Prosody will reject connections from servers with self-signed or misconfigured certificates. You’ll lose federation with some poorly configured servers, but if you’re self-hosting for privacy reasons it doesn’t make much sense to relax authentication for other people’s mistakes.</p><h3 id="omemo-encryption">OMEMO encryption</h3><p>TLS encrypts connections in transit, but the server itself can still read your messages. If you’re self-hosting, that means you’re trusting yourself, which is fine. But if other people use your server, or if you just want the belt-and-braces approach, OMEMO adds end-to-end encryption so that not even the server operator can read message content.</p><p>OMEMO is built on the same encryption that Signal uses, so I’m comfortable trusting it. There’s nothing to configure on the server side either. OMEMO is handled entirely by the clients. Monal, Conversations, and Gajim all support it, and in most cases it’s enabled by default for new conversations. I’d recommend turning it on for everything and leaving it on.</p><h3 id="message-archive">Message archive</h3><div><pre><code data-lang="lua">archive_expires_after <span>=</span> <span>"1y"</span>
default_archive_policy <span>=</span> <span>true</span>
</code></pre></div><p>Messages are kept for a year and archiving is on by default. Clients can opt out per-conversation if they want.</p><h3 id="http-for-file-uploads">HTTP for file uploads</h3><div><pre><code data-lang="lua">http_interfaces <span>=</span> { <span>"*"</span> }
http_ports <span>=</span> { <span>5280</span> }
https_ports <span>=</span> { }
http_external_url <span>=</span> <span>"https://xmpp.example.com"</span>
</code></pre></div><p>Prosody serves HTTP on port 5280 internally. I leave HTTPS to my reverse proxy (Caddy), which handles TLS termination. The <code>http_external_url</code> tells Prosody what URL to hand clients when they upload files.</p><h3 id="virtual-host-and-components">Virtual host and components</h3><div><pre><code data-lang="lua">VirtualHost <span>"xmpp.example.com"</span>
    ssl <span>=</span> {
        key <span>=</span> <span>"/etc/prosody/certs/xmpp.example.com.key"</span>;
        certificate <span>=</span> <span>"/etc/prosody/certs/xmpp.example.com.crt"</span>;
    }

Component <span>"conference.xmpp.example.com"</span> <span>"muc"</span>
    modules_enabled <span>=</span> { <span>"muc_mam"</span> }
    restrict_room_creation <span>=</span> <span>"local"</span>

Component <span>"upload.xmpp.example.com"</span> <span>"http_file_share"</span>
    http_file_share_size_limit <span>=</span> <span>10485760</span>    <span>-- 10 MB</span>
    http_file_share_expires_after <span>=</span> <span>2592000</span>  <span>-- 30 days</span>
    http_external_url <span>=</span> <span>"https://xmpp.example.com"</span>
</code></pre></div><p>The MUC (Multi-User Chat) component gives you group chats with message history via <code>muc_mam</code>. I restrict room creation to local users so random federated accounts can’t spin up rooms on my server.</p><p>The file share component handles image and file uploads. A 10 MB limit and 30-day expiry keeps disk usage under control.</p><h2 id="reverse-proxy-for-file-uploads">Reverse proxy for file uploads</h2><p>Prosody’s HTTP port needs to be reachable from the internet for file uploads to work. I use Caddy:</p><div><pre><code data-lang="fallback">xmpp.example.com {
    reverse_proxy xmpp:5280
}
</code></pre></div><p>When a client sends an image, Prosody hands it a URL like <code>https://xmpp.example.com/upload/...</code> and the receiving client fetches it over HTTPS.</p><h2 id="creating-accounts">Creating accounts</h2><p>With registration disabled, accounts are created from the command line:</p><div><pre><code data-lang="bash">docker <span>exec</span> -it xmpp prosodyctl adduser danny@xmpp.example.com
</code></pre></div><p>It prompts for a password. Done. Log in from any XMPP client.</p><h2 id="firewall">Firewall</h2><p>Open the XMPP ports:</p><div><pre><code data-lang="bash">sudo ufw allow <span>5222</span> comment <span>'XMPP client'</span>
sudo ufw allow <span>5269</span> comment <span>'XMPP federation'</span>
</code></pre></div><p>Port 80/443 for the reverse proxy if you haven’t already. If your server is behind a router, forward 5222 and 5269.</p><h2 id="voice-and-video-calls">Voice and video calls</h2><p>Text and file sharing work at this point. Voice and video calls need one more piece: a TURN/STUN server. Without it, clients behind NAT can’t establish direct media connections.</p><p>I run <a href="https://github.com/coturn/coturn">coturn</a> alongside Prosody. The two share a secret, and Prosody generates temporary credentials for clients automatically.</p><p>Generate a shared secret:</p><p>The coturn <code>docker-compose.yml</code>:</p><div><pre><code data-lang="yaml"><span>services</span>:<span>
</span><span>  </span><span>coturn</span>:<span>
</span><span>    </span><span>image</span>:<span> </span>coturn/coturn:latest<span>
</span><span>    </span><span>container_name</span>:<span> </span>coturn<span>
</span><span>    </span><span>restart</span>:<span> </span>unless-stopped<span>
</span><span>    </span><span>network_mode</span>:<span> </span>host<span>
</span><span>    </span><span>volumes</span>:<span>
</span><span>      </span>- ./turnserver.conf:/etc/coturn/turnserver.conf:ro<span>
</span><span>    </span><span>tmpfs</span>:<span>
</span><span>      </span>- /var/lib/coturn<span>
</span></code></pre></div><p>It runs with <code>network_mode: host</code> because TURN needs real network interfaces to handle NAT traversal. Docker’s port mapping breaks this.</p><p>The <code>turnserver.conf</code>:</p><div><pre><code data-lang="fallback">listening-port=3478
tls-listening-port=5349
min-port=49152
max-port=49200
relay-threads=2
realm=xmpp.example.com
use-auth-secret
static-auth-secret=YOUR_SECRET_HERE
no-multicast-peers
no-cli
no-tlsv1
no-tlsv1_1
denied-peer-ip=10.0.0.0-10.255.255.255
denied-peer-ip=172.16.0.0-172.31.255.255
denied-peer-ip=192.168.0.0-192.168.255.255
log-file=stdout
</code></pre></div><p>If your server is behind NAT, add:</p><div><pre><code data-lang="fallback">external-ip=YOUR_PUBLIC_IP/YOUR_PRIVATE_IP
</code></pre></div><p>Then tell Prosody about it. Add <code>"turn_external"</code> to your modules, and inside the <code>VirtualHost</code> block:</p><div><pre><code data-lang="lua">    turn_external_host <span>=</span> <span>"xmpp.example.com"</span>
    turn_external_port <span>=</span> <span>3478</span>
    turn_external_secret <span>=</span> <span>"YOUR_SECRET_HERE"</span>
</code></pre></div><p>Open the firewall ports:</p><div><pre><code data-lang="bash">sudo ufw allow <span>3478</span> comment <span>'STUN/TURN'</span>
sudo ufw allow <span>5349</span> comment <span>'TURNS'</span>
sudo ufw allow 49152:49200/udp comment <span>'TURN relay'</span>
</code></pre></div><p>Verify with <code>docker exec xmpp prosodyctl check turn</code>.</p><h2 id="clients">Clients</h2><p>On iOS I went with <a href="https://monal-im.org/">Monal</a>, which is open source and supports all the modern XEPs. Push notifications work well. On Android, <a href="https://conversations.im/">Conversations</a> seems to be the go-to. On desktop, <a href="https://gajim.org/">Gajim</a> covers Linux and Windows, and Monal has a macOS build.</p><p>All of them support OMEMO encryption, file sharing, group chats, and voice/video calls.</p><h2 id="verifying-your-setup">Verifying your setup</h2><p>Prosody has solid built-in diagnostics:</p><div><pre><code data-lang="bash">docker <span>exec</span> xmpp prosodyctl check
</code></pre></div><p>This checks DNS records, TLS certificates, connectivity, and module configuration. Fix anything it flags. The error messages are genuinely helpful.</p><p>The <a href="https://compliance.conversations.im/">XMPP Compliance Tester</a> is worth running too. Mine scored above 90% after getting the config right.</p><h2 id="final-thoughts">Final thoughts</h2><p>The whole setup runs in two small Docker containers and a reverse proxy entry. Prosody, file uploads, message archive, push notifications, group chats, voice calls.</p><p>I still use Signal for most day-to-day conversations and I’m not planning to stop. But having my own XMPP server means I’m not entirely dependent on any single service. I can message anyone on any XMPP server, not just people who signed up to the same one. It’s a nice fallback to have.</p><p>If you’re already running Docker on a server somewhere, it’s a good weekend project.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ministry of Justice orders deletion of the UK's largest court reporting database (445 pts)]]></title>
            <link>https://www.legalcheek.com/2026/02/ministry-of-justice-orders-deletion-of-the-uks-largest-court-reporting-database/</link>
            <guid>47034713</guid>
            <pubDate>Mon, 16 Feb 2026 13:30:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.legalcheek.com/2026/02/ministry-of-justice-orders-deletion-of-the-uks-largest-court-reporting-database/">https://www.legalcheek.com/2026/02/ministry-of-justice-orders-deletion-of-the-uks-largest-court-reporting-database/</a>, See on <a href="https://news.ycombinator.com/item?id=47034713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                    <p><img alt="Avatar photo" src="https://www.legalcheek.com/wp-content/uploads/2023/08/cropped-legal-cheek-logo-up-and-down-96x96.jpeg" srcset="https://www.legalcheek.com/wp-content/uploads/2023/08/cropped-legal-cheek-logo-up-and-down-192x192.jpeg 2x" height="96" width="96" decoding="async">                    </p>

                    <p>By  on <time datetime="2026-02-11">Feb 11 2026 11:30am</time></p>
                </div><div>
            <p>Blow for open justice</p>
<p><img fetchpriority="high" decoding="async" src="https://www.legalcheek.com/wp-content/uploads/2024/10/AdobeStock_110501092-2.jpeg" alt="" width="800" height="423" srcset="https://www.legalcheek.com/wp-content/uploads/2024/10/AdobeStock_110501092-2.jpeg 800w, https://www.legalcheek.com/wp-content/uploads/2024/10/AdobeStock_110501092-2-300x159.jpeg 300w, https://www.legalcheek.com/wp-content/uploads/2024/10/AdobeStock_110501092-2-768x406.jpeg 768w, https://www.legalcheek.com/wp-content/uploads/2024/10/AdobeStock_110501092-2-360x190.jpeg 360w" sizes="(max-width: 800px) 100vw, 800px"><br>
<strong>A digital archive that helped journalists track criminal court cases is being shut down by the Ministry of Justice.</strong></p>
<p>Courtsdesk will reportedly be deleted within days after HM Courts &amp; Tribunals Service ordered every record wiped. The platform had been used by more than 1,500 reporters from 39 media outlets to search magistrates’ court lists and registers, but the move has triggered warnings that important cases could now go unreported.</p>
<p>Courtsdesk says it repeatedly found the media wasn’t being told about hearings, with two-thirds of courts regularly hearing cases without notifying journalists. </p>
<p>The platform was launched in 2020 following an agreement with HMCTS and approval by the Lord Chancellor and former Justice Minister Chris Philp, but HMCTS issued a cessation notice in November citing “unauthorised sharing” of court information.</p>

<p>Courtsdesk founder Enda Leahy said the company wrote to government agencies 16 times trying to save the service. It asked for the matter to be referred to the Information Commissioner’s Office but says that request went nowhere, and former Philp himself approached current courts minister Sarah Sackman asking for the archive not to be deleted. The government refused last week.</p>
<p>Leahy told <em>The Times</em> that HMCTS couldn’t do what Courtsdesk did. She pointed to figures showing the court service’s own records were accurate just 4.2% of the time and that 1.6 million criminal hearings went ahead without any advance notice to the press.</p>
<p>“We built the only system that could tell journalists what was actually happening in the criminal courts,” she said.</p>
<p>An HMCTS spokesperson said the press would continue to have full access to court information to support accurate reporting.</p>
<blockquote>
<p lang="en" dir="ltr">HMCTS acted to protect sensitive data after CourtsDesk sent information to a third-party AI company. </p>
<p>Journalists’ access to court information has not been affected: listings and records remain available. <a href="https://t.co/4KWlpCcaAq">pic.twitter.com/4KWlpCcaAq</a></p>
<p>— Ministry of Justice (@MoJGovUK) <a href="https://twitter.com/MoJGovUK/status/2021324797016219758?ref_src=twsrc%5Etfw">February 10, 2026</a></p></blockquote>

        </div><div>

            <p>
                <h2>Related Stories</h2>
            </p>

            <div>
                <div>
    <p><a href="https://www.legalcheek.com/2026/01/government-targets-law-firms-client-account-interest-to-help-fund-justice-system/">
        <img width="360" height="190" src="https://www.legalcheek.com/wp-content/uploads/2025/11/pro-bono-360x190.jpeg" alt="" decoding="async" loading="lazy">    </a></p>
</div><div>
    <p><a href="https://www.legalcheek.com/2025/02/inaccurate-media-lead-to-judicial-death-threats-says-lady-chief-justice/">
        <img width="360" height="190" src="https://www.legalcheek.com/wp-content/uploads/2025/02/Dame_Sue_Carr_2022-360x190.jpeg" alt="" decoding="async" loading="lazy">    </a></p>
</div>            </div>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thanks a lot, AI: Hard drives are sold out for the year, says WD (311 pts)]]></title>
            <link>https://mashable.com/article/ai-hard-drive-hdd-shortages-western-digital-sold-out</link>
            <guid>47034192</guid>
            <pubDate>Mon, 16 Feb 2026 12:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mashable.com/article/ai-hard-drive-hdd-shortages-western-digital-sold-out">https://mashable.com/article/ai-hard-drive-hdd-shortages-western-digital-sold-out</a>, See on <a href="https://news.ycombinator.com/item?id=47034192">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article" data-autopogo="">
                                                            <p>Looking to buy a new hard drive? Get ready to pay even more this year.</p><p>According to Western Digital, one of the world's biggest hard drive manufacturers, the company has already sold out of its storage capacity for 2026 with more than 10 months still left in the year.</p><p>"We're pretty much sold out for calendar 2026," <a href="https://www.tweaktown.com/news/110168/western-digital-runs-out-of-hdd-capacity-ceo-says-massive-ai-deals-secured-price-surges-ahead/index.html" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)"><u>said</u></a> Western Digital CEO Irving Tan on the company's recent quarterly <a href="https://investor.wdc.com/events/event-details/western-digital-second-quarter-fiscal-2026-earnings-call" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)"><u>earnings call</u></a>.&nbsp;</p>
<p>Tan shared that most of the storage space has been allocated to its "top seven customers." Three of these companies already have agreements with Western Digital for 2027 and even 2028.&nbsp;</p><section x-data="window.newsletter({ isDeal: false })" x-init="init()" aria-label="Newsletter Sign-Up">
        <p><span>Mashable Light Speed</span>
        </p>
        
    </section>

<p>Furthermore, the incentive for these hardware companies to prioritize the average consumer is also dwindling. According to Western Digital, thanks to a surge in demand from its enterprise customers, the consumer market now accounts for just 5 percent of the company's revenue.</p><p>AI companies have been eating up computer hardware as industry growth accelerates. Prices for products ranging from computer processors to video game consoles have skyrocketed due to these AI companies cannibalizing supply chains.</p><p>The tech industry has already been experiencing <a href="https://mashable.com/article/micron-memory-ram-shortage-not-ending-soon-ai" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body"><u>a shortage of memory</u></a>&nbsp;due to&nbsp;demand from AI companies. PC makers have been forced to <a href="https://mashable.com/article/framework-price-hike-ddr5-ram-memory-shortage" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body"><u>raise RAM prices</u></a>&nbsp;on a near-regular basis as shortages persist. Video game console makers, like Sony, have even <a href="https://mashable.com/article/playstation-6-delayed-report-memory-shortages" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body"><u>reportedly</u></a> considered pushing the next PlayStation launch beyond the planned 2027 release in hopes that AI-related hardware shortages would be resolved by then.</p><p>With this latest news from Western Digital, it appears the ever-increasing demands from AI companies for memory and storage will continue to grow, with no end in sight. Unless, of course, investors decide to <a href="https://mashable.com/article/ai-bubble-watch-tech-stocks-down" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body"><u>pull back</u></a> from AI over fears that AI's promises may not come to fruition. But, for now at least, the shortages – and price hikes for consumers – will continue.</p>

                                                                
                                    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Israeli spyware firm that accidentally just exposed itself (229 pts)]]></title>
            <link>https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally</link>
            <guid>47033976</guid>
            <pubDate>Mon, 16 Feb 2026 12:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally">https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally</a>, See on <a href="https://news.ycombinator.com/item?id=47033976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!IWq3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IWq3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 424w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 848w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!IWq3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg" width="1200" height="781" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:781,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:118333,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://ahmedeldin.substack.com/i/187749296?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!IWq3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 424w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 848w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!IWq3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb89bb-8aa4-420b-8831-a4ae10887d15_1200x781.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Israeli surveillance company Paragon Solutions briefly exposed its own spyware dashboard on LinkedIn, revealing the hidden architecture of a billion-dollar surveillance empire built on the backs of journalists, activists, and ordinary people.</p><p>The technical slip-up is a moment of rare transparency in an industry built on secrecy, exposing the operational interface used to compromise devices, intercept communications, and harvest data from targets worldwide.</p><p><a href="https://techcrunch.com/2024/02/paragon-solutions-acquisition-900-million/" rel="">The $900 million acquisition of Paragon by U.S.</a><span> private equity firm AE Industrial Partners tells you everything you need to know about who profits from your digital insecurity. </span><a href="https://www.calcalistech.com/tech/article/byh2r8r0t" rel="">Former Israeli Prime Minister Ehud Barak reportedly pocketed $10-15 million from the deal</a><span>, a tidy sum for a politician-turned-surveillance capitalist. </span></p><p>What the LinkedIn photos show is chilling: a Czech phone number labeled “Valentina,” interception logs marked “Completed,” and application-level data categories targeting encrypted services. This isn’t some theoretical cybersecurity threat, it is the real-time dashboard of the modern surveillance capitalism system we endure.</p><p data-attrs="{&quot;url&quot;:&quot;https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><a href="https://x.com/jsrailton?lang=en" rel="">John Scott-Railton</a><span>, a senior researcher at the University of Toronto’s Citizen Lab, described the disclosure as an “epic OPSEC fail,” highlighting how the operational-security discipline on which the commercial spyware industry depends had been violated.</span></p><a href="https://x.com/jsrailton/status/2021647308790911384" target="_blank" rel="noopener noreferrer" data-component-name="Twitter2ToDOM"><div data-attrs="{&quot;url&quot;:&quot;https://x.com/jsrailton/status/2021647308790911384&quot;,&quot;full_text&quot;:&quot;Epic OPSEC fail by Paragon  exposing Graphite spyware capabilities.\n\nAnnotated pic from what we know.\n\nPlease help me figure out the other apps in in this pic that the spyware can access:\n\n<span class=\&quot;tweet-fake-link\&quot;>#WhatsApp</span>\n<span class=\&quot;tweet-fake-link\&quot;>#Telegram</span>\n<span class=\&quot;tweet-fake-link\&quot;>#Signal</span>\n?\n<span class=\&quot;tweet-fake-link\&quot;>#Line</span>?\n ?\n<span class=\&quot;tweet-fake-link\&quot;>#Snapchat</span>?\n<span class=\&quot;tweet-fake-link\&quot;>#TikTok</span>?&quot;,&quot;username&quot;:&quot;jsrailton&quot;,&quot;name&quot;:&quot;John Scott-Railton&quot;,&quot;profile_image_url&quot;:&quot;https://pbs.substack.com/profile_images/1648379486688231453/Wfi5gqVC_normal.jpg&quot;,&quot;date&quot;:&quot;2026-02-11T18:07:33.000Z&quot;,&quot;photos&quot;:[{&quot;img_url&quot;:&quot;https://pbs.substack.com/media/HA5T-uhboAAD2xT.jpg&quot;,&quot;link_url&quot;:&quot;https://t.co/01KQQSGm3X&quot;}],&quot;quoted_tweet&quot;:{&quot;full_text&quot;:&quot;The general counsel of Paragon, uploaded a picture on Linkedin today showing the Paragon spyware control panel.\n\nThe panel shows a phone number in Czechia, Apps, Accounts, media on the phone, the interception status and numbers extracted from various apps.&quot;,&quot;username&quot;:&quot;DrWhax&quot;,&quot;name&quot;:&quot;Jurre van Bergen&quot;,&quot;profile_image_url&quot;:&quot;https://pbs.substack.com/profile_images/1772672414582751234/cntuFXJt_normal.jpg&quot;},&quot;reply_count&quot;:67,&quot;retweet_count&quot;:531,&quot;like_count&quot;:2423,&quot;impression_count&quot;:381982,&quot;expanded_url&quot;:null,&quot;video_url&quot;:null,&quot;belowTheFold&quot;:false}"><div><div title="User"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3RU0!,w_40,h_40,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 40w, https://substackcdn.com/image/fetch/$s_!3RU0!,w_80,h_80,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 80w, https://substackcdn.com/image/fetch/$s_!3RU0!,w_120,h_120,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 120w" sizes="40px"><img src="https://substackcdn.com/image/fetch/$s_!3RU0!,w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg" sizes="40px" alt="X avatar for @jsrailton" srcset="https://substackcdn.com/image/fetch/$s_!3RU0!,w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 40w, https://substackcdn.com/image/fetch/$s_!3RU0!,w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 80w, https://substackcdn.com/image/fetch/$s_!3RU0!,w_120,h_120,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1648379486688231453%2FWfi5gqVC.jpg 120w" width="40" height="40" draggable="false"></picture></div><p><span>John Scott-Railton</span><span>@jsrailton</span></p><svg role="img" style="height:20px;width:20px;" width="20" height="20" viewBox="0 0 20 20" fill="var(--color-fg-primary)" stroke-width="1.8" stroke="#000" xmlns="http://www.w3.org/2000/svg"><g><title></title><path stroke="none" fill-rule="evenodd" clip-rule="evenodd" d="M13.2879 19.1666L8.66337 12.575L2.87405 19.1666H0.424805L7.57674 11.0258L0.424805 0.833252H6.71309L11.0717 7.04577L16.5327 0.833252H18.982L12.1619 8.59699L19.5762 19.1666H13.2879ZM16.0154 17.3083H14.3665L3.93176 2.69159H5.58092L9.7601 8.54422L10.4828 9.55981L16.0154 17.3083Z"></path></g></svg></div><p>Epic OPSEC fail by Paragon  exposing Graphite spyware capabilities.

Annotated pic from what we know.

Please help me figure out the other apps in in this pic that the spyware can access:

<span>#WhatsApp</span>
<span>#Telegram</span>
<span>#Signal</span>
?
<span>#Line</span>?
 ?
<span>#Snapchat</span>?
<span>#TikTok</span>?</p><p><img src="https://pbs.substack.com/media/HA5T-uhboAAD2xT.jpg"></p><div><div><div title="User"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!R4Tm!,w_20,h_20,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 20w, https://substackcdn.com/image/fetch/$s_!R4Tm!,w_40,h_40,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 40w, https://substackcdn.com/image/fetch/$s_!R4Tm!,w_60,h_60,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 60w" sizes="20px"><img src="https://substackcdn.com/image/fetch/$s_!R4Tm!,w_20,h_20,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg" sizes="20px" alt="X avatar for @DrWhax" srcset="https://substackcdn.com/image/fetch/$s_!R4Tm!,w_20,h_20,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 20w, https://substackcdn.com/image/fetch/$s_!R4Tm!,w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 40w, https://substackcdn.com/image/fetch/$s_!R4Tm!,w_60,h_60,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fprofile_images%2F1772672414582751234%2FcntuFXJt.jpg 60w" width="20" height="20" draggable="false"></picture></div><p><span>Jurre van Bergen</span> <span>@DrWhax</span></p></div><p>The general counsel of Paragon, uploaded a picture on Linkedin today showing the Paragon spyware control panel.

The panel shows a phone number in Czechia, Apps, Accounts, media on the phone, the interception status and numbers extracted from various apps.</p></div><div><p><span>6:07 PM · Feb 11, 2026</span><span> · </span><span>382K Views</span></p><p><span>67 Replies</span><span> · </span><span>531 Reposts</span><span> · </span><span>2.42K Likes</span></p></div></div></a><p><span>Paragon’s flagship product, </span><a href="https://citizenlab.ca/research/a-first-look-at-paragons-proliferating-spyware-operations/" rel="">Graphite</a><span>, represents the cutting edge of what researchers call “mercenary spyware”— highly targeted intrusion systems sold exclusively to state agencies. Unlike conventional malware, these platforms are engineered for precision, using </span><a href="https://www.lrqa.com/en/insights/articles/an-introduction-to-zero-click-attacks/" rel="">zero-click exploit chains</a><span> that compromise devices without any action from the target.</span></p><p>Once installed, spyware operates at the operating-system level, granting operators visibility into:</p><ul><li><p>Stored data and communications</p></li><li><p>Microphone and camera activation</p></li><li><p>Enclosed applications and services</p></li><li><p>Messages accessed before encryption or after decryption</p></li></ul><p><span>The secure boundaries that millions rely on including encrypted chats, locked apps, protected accounts dissolve once the device itself is compromised.</span><a href="https://www.bitdefender.com/en-us/blog/hotforsecurity/nso-groups-spyware-installed-iphones-al-jazeera-employees-using-zero-day-exploit" rel=""> Even if Paragon executives call this “lawful access”,</a><span> we all know there is nothing lawful about accessing someone’s entire digital life without their knowledge or consent?</span></p><p data-attrs="{&quot;url&quot;:&quot;https://ahmedeldin.substack.com/subscribe&quot;,&quot;text&quot;:&quot;Become A Paid Subscriber&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://ahmedeldin.substack.com/subscribe" rel=""><span>Become A Paid Subscriber</span></a></p><p><span>After NSO Group’s </span><a href="https://www.aljazeera.com/news/2021/12/3/us-officials-phones-hacked-nso-group-spyware-reuters" rel="">Pegasus spyware generated global outrage</a><span> by targeting journalists and dissidents, Paragon tried to position itself as the “ethical” alternative—a “light-touch” approach operating “within apps” rather than compromising the entire device.</span></p><div id="youtube2-b6VxWBXuEGM" data-attrs="{&quot;videoId&quot;:&quot;b6VxWBXuEGM&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/b6VxWBXuEGM?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><a href="https://citizenlab.ca/research/first-forensic-confirmation-of-paragons-ios-mercenary-spyware-finds-journalists-targeted/" rel="">Independent researchers at Citizen Lab have repeatedly debunked this distinction. </a><span>Once spyware achieves device-level persistence, access pathways inevitably extend beyond the narrow confines vendors claim and describe. </span></p><p>The technical reality is clear: if you can compromise a device, you can access everything. </p><p>Laws and court precedents consider full device compromise as extraordinarily invasive, creating strict legal requirements. By describing their capabilities as 'selective' rather than 'systemic,' spyware vendors strategically frame their intrusion as less severe to navigate around these legal barriers and avoid meaningful oversight."</p><p><a href="https://www.theguardian.com/technology/2026/jan/31/us-authorities-reportedly-investigate-claims-that-meta-can-read-encrypted-whatsapp-messages" rel="">Applications like WhatsApp are widely understood as privacy-preserving technologies.</a><span> End-to-end encryption, Meta has long argued, ensures that only communicating users can read messages or access calls. For hundreds of millions of people, that assurance functions as a baseline assumption of digital safety.</span></p><p>Disclosures over the past year have repeatedly unsettled that belief.</p><p><span>In early 2025, </span><a href="https://about.fb.com/news/2025/01/whatsapp-security-update/" rel="">Meta notified roughly 90 WhatsApp users that their devices had been targeted with spyware linked to Paragon.</a><span> The victims reportedly included journalists and members of civil society—individuals whose communications are often politically sensitive and professionally vulnerable.</span></p><p>Researchers have emphasized that infections associated with mercenary spyware frequently occur without user interaction. There are no malicious links to click, no suspicious files to download. Devices are compromised through exploit chains operating below the threshold of ordinary detection.</p><p>The episode reinforced a reality digital security researchers have warned about for years: encryption protects communications in transit, but offers limited protection once the device itself has been compromised.</p><p><span>The $900 million valuation of Paragon Solutions reveals the brutal economics of surveillance capitalism. Remote device intrusion is immensely profitable. When government clients create the demand, and private equity investors drive the scale, the revolving door between state security and commercial surveillance becomes the engine that powers this industry. </span><a href="https://constantinecannon.com/whistleblower/whistleblower-insider-blog/paragon-systems-52-million-fraudulently-securing-govt-security-set-aside-contracts/" rel="">Former senior officials and intelligence veterans populate executive ranks, repackaging capabilities developed for national security into commercial products.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!VaZ2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VaZ2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 424w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 848w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VaZ2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg" width="1200" height="694" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:694,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:138550,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ahmedeldin.substack.com/i/187749296?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VaZ2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 424w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 848w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!VaZ2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20a9e5a8-afc2-41d2-810e-588ca613de96_1200x694.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Paragon founders with investor Ehud Barak (second from right)</figcaption></figure></div><div id="youtube2-5pPQydBGwQY" data-attrs="{&quot;videoId&quot;:&quot;5pPQydBGwQY&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/5pPQydBGwQY?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Israel has become the Silicon Valley of surveillance technology over the past two decades. A dense network of private firms operates alongside military and intelligence institutions, with personnel pipelines flowing between state security structures and commercial ventures.</p><p>Palestinians have long lived under one of the most extensively documented surveillance regimes in the world. The deployment of facial recognition systems, predictive analytics, and device monitoring technologies in the occupied Palestinian territories are widely documented by  human-rights organizations and digital researchers.</p><p data-attrs="{&quot;url&quot;:&quot;https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://ahmedeldin.substack.com/p/the-israeli-spyware-firm-that-accidentally?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>At the same time, </span><a href="https://www.americanimmigrationcouncil.org/blog/ice-ai-surveillance-tracking-americans/" rel="">U.S. immigration enforcement agencies are expanding their own technological arsenal</a><span>—biometric databases, algorithmic risk scoring, location tracking, and now, potentially, advanced spyware capabilities using the same institutional logics are identical: </span><em><a href="https://www.americanimmigrationcouncil.org/blog/ice-ai-surveillance-tracking-americans/" rel="">Identify. Track. Classify. Control.</a></em></p><p>What was developed to control Palestinians in occupied territories is now sold to control immigrants in America. The same surveillance infrastructure built under occupation becomes a commercial product sold to authoritarian regimes worldwide.</p><p>While the political contexts differ, the technologies increasingly circulate within overlapping global markets. The actors shaping this ecosystem include former heads of state, elite intelligence veterans, multinational investors, and government clients.</p><p><span>Public procurement records reveal tha</span><a href="https://www.wired.com/story/ice-paragon-solutions-contract/" rel="">t U.S. immigration enforcement agencies have engaged with Paragon’s Graphite technology</a><span>, including contracts with DHS and ICE. While the exact procurement process remains obscured in bureaucratic paperwork, the fact that ICE—an agency notorious for due process violations, detention conditions, and aggressive enforcement practices—secured advanced Israeli spyware capabilities speaks volumes about where the Trump administration’s priorities truly lay.</span></p><p>The Trump administration is expanding the surveillance state’s reach into the lives of immigrants and marginalized communities, using the most invasive tools available from the Israeli surveillance industry.</p><p><span>Spyware companies consistently invoke crime prevention and national security in their public messaging. </span><strong><a href="https://breached.company/the-cyber-arms-trade-how-commercial-spyware-is-reshaping-global-security/" rel="">But follow the money, and the real story emerges:</a></strong><span> extraordinary valuations are commanded by companies capable of defeating device security because governments continue purchasing those capabilities.</span></p><p>We live in an age of unprecedented surveillance, where the architecture of control is built not just with walls and checkpoints, but with algorithms and exploits. The same logic that governs the occupied territories now governs our digital lives, sold to us as progress and security.</p><p>Paragon’s moment of transparency showed us the face of modern surveillance capitalism. The irony is bitter: the same people who built the surveillance state now profit from selling its tools to the highest bidder. Their fortunes are built on the erosion of privacy, the compromise of security, the violation of digital sanctuary. We are told this is about national security, about fighting terrorism. But follow the money, and the truth emerges: this is about power. This is about control. This is about the commodification of human vulnerability.</p><p>The devices in our pockets are no longer just tools. They are also windows, and mirrors. They are the architecture of our own surveillance, sold to us as convenience, packaged as security, and monetized as data.</p><p>Paragon exposed itself not through negligence, but through arrogance and the belief that their work was so normalized, so institutionalized, that it could be displayed openly on a professional networking site.</p><p>And in that moment of exposure, we saw the truth: the surveillance industry operates not in shadows, but in plain sight. The only thing hiding its true nature is our own willingness to look away, our own complicity in the fiction that this is somehow about security rather than control.</p><p>The surveillance industry does not just compromise our devices. It compromises our humanity. And that is the ultimate cost of this billion-dollar empire.</p><p>The threat is global and immediate. The tools developed in Israel to surveil and control Palestinians are now in the hands of ICE to surveil and control immigrants in America. They are in the hands of authoritarian regimes worldwide to surveil and control their citizens. They are in the hands of corporations to surveil and control consumers.</p><p>This is the new colonialism, executed not through armies and occupation, but through algorithms and exploits. The same institutional logic that justified the occupation of Palestinian territories justifies the occupation of our digital lives. The same people who profited from one occupation now profit from the next.</p><p>When former intelligence chiefs and politicians sit on corporate boards and rake in millions from the surveillance industry, when governments purchase these tools without democratic debate, when the media fails to connect the dots between occupation and digital control—this is the conspiracy.</p><p>The dissent that was once silenced in Gaza is now being erased in our inboxes. The journalists who were targeted in the occupied territories are now being targeted in our own cities. The activists who were monitored in refugee camps are now being monitored in our communities.</p><p>This is a crisis of global proportion, a threat to human dignity that crosses borders and transcends politics. The question is no longer whether we should be concerned about surveillance. The question is whether we will allow this system to continue unchecked, whether we will demand accountability from those who profit from our vulnerability, whether we will reclaim our digital lives from those who would turn our devices into tools of control.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic tries to hide Claude's AI actions. Devs hate it (320 pts)]]></title>
            <link>https://www.theregister.com/2026/02/16/anthropic_claude_ai_edits/</link>
            <guid>47033622</guid>
            <pubDate>Mon, 16 Feb 2026 11:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2026/02/16/anthropic_claude_ai_edits/">https://www.theregister.com/2026/02/16/anthropic_claude_ai_edits/</a>, See on <a href="https://news.ycombinator.com/item?id=47033622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Anthropic has updated Claude Code, its AI coding tool, changing the progress output to hide the names of files the tool was reading, writing, or editing. However, developers have pushed back, stating that they need to see which files are accessed.</p>
<p>Version 2.1.20 collapsed the output so that instead of showing, for example, the file names and how many lines were read, it would just print "Read 3 files (ctrl+o to expand)," according to a&nbsp;<a href="https://symmetrybreak.ing/blog/claude-code-is-being-dumbed-down/" target="_blank" rel="nofollow">post</a>&nbsp;complaining that "Claude Code is being dumbed down." The full details can still be accessed with the keyboard shortcut, but constantly invoking this is annoying and impractical.</p>
<p>Developers have many reasons for wanting to see the file names, such as for security, for knowing immediately if Claude is pulling context from the wrong files, and for easy audit of past activity by scrolling through conversation. "When I'm working on a complex codebase, knowing what context Claude is pulling helps me catch mistakes early and steer the conversation," one person wrote.</p>

    

<p>There's also a financial impact. If developers spot that Claude is going down a wrong track, they can interrupt and avoid wasting tokens.</p>

        


        

<p>&nbsp;A&nbsp;<a href="https://github.com/anthropics/claude-code/issues/21151" target="_blank" rel="nofollow">GitHub issue</a>&nbsp;on the subject drew a&nbsp;<a href="https://github.com/anthropics/claude-code/issues/21151#issuecomment-3812828803" target="_blank" rel="nofollow">response</a>&nbsp;from Boris Cherny, creator and head of Claude Code at Anthropic, that "this isn't a vibe coding feature, it's a way to simplify the UI so you can focus on what matters, diffs and bash/mcp outputs." He suggested that developers "try it out for a few days" and said that Anthropic's own developers "appreciated the reduced noise."</p>
<p>Cherny said that developers who wanted more detail could enable verbose mode. Responses were lackluster, with one person writing: "Verbose mode is not a viable alternative, there's way too much noise."</p>

        

<p>Another observation was that the new default output, such as "searched for 2 patterns, read 3 files," conveys no useful information. "It's not a nice simplification, it's an idiotic removal of valuable information,"&nbsp;<a href="https://github.com/anthropics/claude-code/issues/21151#issuecomment-3844716512" target="_blank" rel="nofollow">said</a>&nbsp;a user.</p>
<p>Cherny responded to the feedback by making changes. "We have repurposed the existing verbose mode setting for this," he&nbsp;<a href="https://github.com/anthropics/claude-code/issues/21151#issuecomment-3887552218" target="_blank" rel="nofollow">said</a>, so that it "shows file paths for read/searches. Does not show full thinking, hook output, or subagent output (coming in tomorrow's release)."</p>
<p>The problem with this is that making verbose mode less verbose is a bad change for those who wanted the full details.</p>

        

<p>Cherny also participated in a lengthy&nbsp;<a href="https://news.ycombinator.com/item?id=46978710" target="_blank" rel="nofollow">discussion</a>&nbsp;on Hacker News. "Claude has gotten more intelligent, it runs for longer periods of time, and it is able to more agentically use more tools... The amount of output this generates can quickly become overwhelming in a terminal, and is something we hear often from users," he said.</p>
<ul>

<li><a href="https://www.theregister.com/2026/02/13/anthropic_series_g/">Investors shove another $30B into the Anthropic money furnace</a></li>

<li><a href="https://www.theregister.com/2026/02/13/anthropic_c_compiler/">OK, so Anthropic's AI built a C compiler. That don't impress me much</a></li>

<li><a href="https://www.theregister.com/2026/02/13/cloudflare_markdown_for_ai_crawlers/">Cloudflare turns websites into faster food for AI agents</a></li>

<li><a href="https://www.theregister.com/2026/02/12/30_chrome_extensions_ai/">30+ Chrome extensions disguised as AI chatbots steal users' API keys, emails, other sensitive data</a></li>
</ul>
<p>Those users who want the collapsed output seem to be mostly absent from the discussion. "I can't tell you how many times I benefited from seeing the files Claude was reading, to understand how I could interrupt and give it a little more context... saving thousands of tokens,"&nbsp;<a href="https://news.ycombinator.com/item?id=46982115" target="_blank" rel="nofollow">said</a>&nbsp;one response.</p>
<p>Cherny said that the repurposed verbose mode was the solution, and that Claude Code will still default to the condensed view.</p>
<p>The debate is important because if AI tools like Claude Code hide what they are doing from developers (or other users), mistakes are more likely to slip through. "I'm a Claude user who has been burned lately by how opaque the system has become,"&nbsp;<a href="https://news.ycombinator.com/item?id=46988932" target="_blank" rel="nofollow">said</a>&nbsp;another developer. "Right now Claude cannot be trusted to get things right without constant oversight and frequent correction, often for just a single step. For people like me, this is make or break. If I cannot follow the reasoning, read the intent, or catch logic disconnects early, the session just burns through my token quota."</p>
<p>Claude Code changes frequently, so it is likely that this aspect will be further tweaked, but there is not yet any indication that it will revert to the old behavior. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MessageFormat: Unicode standard for localizable message strings (141 pts)]]></title>
            <link>https://github.com/unicode-org/message-format-wg</link>
            <guid>47033328</guid>
            <pubDate>Mon, 16 Feb 2026 10:26:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/unicode-org/message-format-wg">https://github.com/unicode-org/message-format-wg</a>, See on <a href="https://news.ycombinator.com/item?id=47033328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">MessageFormat Working Group</h2><a id="user-content-messageformat-working-group" aria-label="Permalink: MessageFormat Working Group" href="#messageformat-working-group"></a></p>
<p dir="auto">Welcome to the home page for the MessageFormat Working Group, a subgroup of the <a href="https://cldr.unicode.org/" rel="nofollow">Unicode CLDR-TC</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Charter</h2><a id="user-content-charter" aria-label="Permalink: Charter" href="#charter"></a></p>
<p dir="auto">The MessageFormat Working Group (MFWG) is tasked with developing and supporting an industry standard
for the representation of localizable message strings.
MessageFormat is designed to support software developers, translators, and end users with fluent messages
and locally-adapted presentation for data values
while providing a framework for increasingly complex features, such as gender, inflections, and speech.
Our goal is to provide an interoperable syntax, message data model, and associated processing that is
capable of being adopted by any presentation framework or programming environement.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Unicode MessageFormat Standard</h2><a id="user-content-the-unicode-messageformat-standard" aria-label="Permalink: The Unicode MessageFormat Standard" href="#the-unicode-messageformat-standard"></a></p>
<p dir="auto">The <a href="https://github.com/unicode-org/message-format-wg/blob/main/spec">Unicode MessageFormat Standard</a> is a stable part of CLDR.
It was approved by the CLDR Technical Committee
and is recommended for implementation and adoption.
The normative version of the specification is published as a part of <a href="https://www.unicode.org/reports/tr35/" rel="nofollow">TR35</a>.
This repository contains the editor's copy.</p>
<p dir="auto"><strong>Unicode MessageFormat</strong> is sometimes referred to as <em>MessageFormat 2.0</em>,
since it replaces earlier message formatting capabilities built into ICU.</p>
<p dir="auto">Some <em>default functions</em> and items in the <code>u:</code> namespace are still in Draft status.
Feedback from users and implementers might result in changes to these capabilities.</p>
<p dir="auto">The MessageFormat Working Group and CLDR Technical Committee welcome any and all feedback,
including bugs reports,
implementation reports,
success stories,
feature requests,
requests for clarification,
or anything that would be helpful in supporting or enhancing the specification and
promoting widespread adoption.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sharing Feedback</h2><a id="user-content-sharing-feedback" aria-label="Permalink: Sharing Feedback" href="#sharing-feedback"></a></p>
<p dir="auto">Do you have feedback on the specification or any of its elements? <a href="https://github.com/unicode-org/message-format-wg/issues/new?labels=Preview-Feedback&amp;projects=&amp;template=tech-preview-feedback.md&amp;title=%5BFEEDBACK%5D+">file an issue here</a></p>
<p dir="auto">We invite feedback about implementation difficulties,
proposed functions or options
real-life use-cases,
requirements for future work,
tooling,
runtime APIs,
localization workflows,
and other topics.</p>
<ul dir="auto">
<li>General questions and thoughts → <a href="https://github.com/unicode-org/message-format-wg/discussions">post a discussion thread</a>.</li>
<li>Actionable feedback (bugs, feature requests) → <a href="https://github.com/unicode-org/message-format-wg/issues">file a new issue</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Participation / Joining the Working Group</h2><a id="user-content-participation--joining-the-working-group" aria-label="Permalink: Participation / Joining the Working Group" href="#participation--joining-the-working-group"></a></p>
<p dir="auto">We are looking for participation from software developers, localization engineers and others with experience
in Internationalization (I18N) and Localization (L10N).
If you wish to contribute to this work, please review the information about the Contributor License Agreement below.</p>
<p dir="auto">To follow this work:</p>
<ol dir="auto">
<li>Apply to join our <a href="https://groups.google.com/a/chromium.org/forum/#!forum/message-format-wg" rel="nofollow">mailing list</a></li>
<li>Watch this repository (use the "Watch" button in the upper right corner)</li>
</ol>
<p dir="auto">To contribute to this work, in addition to the above:</p>
<ol dir="auto">
<li>Each individual MUST have a copy of the CLA on file. See below.</li>
<li>Individuals who are employees of Unicode Member organizations SHOULD contact their member representative.
Individuals who are not employees of Unicode Member organizations MUST contact the chair to request Invited Expert status.
Employees of Unicode Member organizations MAY also apply for Invited Expert status,
subject to approval from their member representative.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Copyright &amp; Licenses</h3><a id="user-content-copyright--licenses" aria-label="Permalink: Copyright &amp; Licenses" href="#copyright--licenses"></a></p>
<p dir="auto">Copyright © 2019-2025 Unicode, Inc. Unicode and the Unicode Logo are registered trademarks of Unicode, Inc. in the United States and other countries.</p>
<p dir="auto">A CLA is required to contribute to this project - please refer to the <a href="https://github.com/unicode-org/message-format-wg/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> file (or start a Pull Request) for more information.</p>
<p dir="auto">The contents of this repository are governed by the Unicode <a href="https://www.unicode.org/copyright.html" rel="nofollow">Terms of Use</a> and are released under <a href="https://github.com/unicode-org/message-format-wg/blob/main/LICENSE">LICENSE</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3.5: Towards Native Multimodal Agents (317 pts)]]></title>
            <link>https://qwen.ai/blog?id=qwen3.5</link>
            <guid>47032876</guid>
            <pubDate>Mon, 16 Feb 2026 09:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwen.ai/blog?id=qwen3.5">https://qwen.ai/blog?id=qwen3.5</a>, See on <a href="https://news.ycombinator.com/item?id=47032876">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[picol: A Tcl interpreter in 500 lines of code (115 pts)]]></title>
            <link>https://github.com/antirez/picol</link>
            <guid>47032235</guid>
            <pubDate>Mon, 16 Feb 2026 08:04:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/antirez/picol">https://github.com/antirez/picol</a>, See on <a href="https://news.ycombinator.com/item?id=47032235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">Picol is a Tcl-alike interpreter in 500 lines of code that I released 15th of March 2007. Recentely I looked at the source code and realized this was a better C programming example compared to what I recalled, so I'm putting this on GitHub to archive it, together with the main points of the original article.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rules</h2><a id="user-content-rules" aria-label="Permalink: Rules" href="#rules"></a></p>
<p dir="auto">When I built this code, I had some rule in mind:</p>
<ul dir="auto">
<li>I wanted to use more or less my normal C style. In Picol you'll find normal C spacing and even comments.</li>
<li>I wanted to write an interpreter with a design similar to a real one. One of the few useful things you can do with Picol is to learn how to write a Tcl interpreter if you are a newbie programmer, I guess, so the point was to write a simple to understand program, not just a <em>short</em> program.</li>
<li>The resulting interpreter should be able to run some kind of non trivial program: to just set few vars and print hello world was not an option.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The resulting interpreter: Picol</h2><a id="user-content-the-resulting-interpreter-picol" aria-label="Permalink: The resulting interpreter: Picol" href="#the-resulting-interpreter-picol"></a></p>
<p dir="auto">The parser is very similar to the Tcl one, Picol supports interpolation as well, for example you can write:</p>
<div dir="auto" data-snippet-clipboard-copy-content="set a &quot;pu&quot;
set b {ts}
$a$b &quot;Hello World!&quot;"><pre><span>set</span> a <span><span>"</span>pu<span>"</span></span>
<span>set</span> b {ts}
<span>$a$b</span> <span><span>"</span>Hello World!<span>"</span></span></pre></div>
<p dir="auto">Note that Picol has an interactive shell! so just launch it without arguments to start to play (to compile the code use <code>gcc -O2 -Wall -o picol picol.c</code>).</p>
<p dir="auto">To run a program stored in a file, use: <code>picol filename.tcl</code>.</p>
<p dir="auto">Probably the parser could be rewritten in order to take less space, currently it takes almost 250 lines of code: this is too much and leaves little room for all the rest. On the other side, it's a decent example about writing parsers by hand.</p>
<p dir="auto">A Raw list of the supported features:</p>
<ul dir="auto">
<li>Interpolation, as seen above. You can also write <code>"2+2 = [+ 2 2]"</code> or <code>"My name is: $foobar"</code>.</li>
<li>Procedures, with return. Like Tcl if return is missing the result of the last command executed is returned.</li>
<li><code>If</code>, <code>if .. else ..</code>, <code>while</code> with <code>break</code> and <code>continue</code>.</li>
<li>Recursion.</li>
<li>Variables inside procedures are limited in scope like Tcl, i.e. there are real call frames in Picol.</li>
<li>The following other commands: <code>set</code> <code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>==</code> <code>!=</code> <code>&gt;</code> <code>&lt;</code> <code>&gt;=</code> <code>&lt;=</code> <code>puts</code>.</li>
</ul>
<p dir="auto">This is an example of programs Picol can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="proc fib {x} {
    if {== $x 0} {
        return 0
    }
    if {== $x 1} {
        return 1
    }
    return [+ [fib [- $x 1]] [fib [- $x 2]]]
}

puts [fib 20]
that of course will output fib(20). Another example:
proc square {x} {
    * $x $x
}"><pre><span>proc</span> <span>fib</span> {x} {
    <span>if</span> {== <span>$x</span> 0} {
        <span>return</span> 0
    }
    <span>if</span> {== <span>$x</span> 1} {
        <span>return</span> 1
    }
    <span>return</span> [+ [fib [- <span>$x</span> 1]] [fib [- <span>$x</span> 2]]]
}

<span>puts</span> [fib 20]
that of course will output fib(20). Another example:
<span>proc</span> <span>square</span> {x} {
    * <span>$x</span> <span>$x</span>
}</pre></div>
<p dir="auto">Or:</p>
<div data-snippet-clipboard-copy-content="set a 1
while {<= $a 10} {
    if {== $a 5} {
        puts {Missing five!}
        set a [+ $a 1]
        continue
    }
    puts &quot;I can compute that $a*$a = [square $a]&quot;
    set a [+ $a 1]
}"><pre><code>set a 1
while {&lt;= $a 10} {
    if {== $a 5} {
        puts {Missing five!}
        set a [+ $a 1]
        continue
    }
    puts "I can compute that $a*$a = [square $a]"
    set a [+ $a 1]
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design</h2><a id="user-content-design" aria-label="Permalink: Design" href="#design"></a></p>
<ul dir="auto">
<li>It's pretty straightforward, the first important part you see in the source code is an hand written parser. The main function of the parser is <code>picolGetToken</code> that just calls functions able to parse the different parts of a Tcl program and return in the parsing structure the type of the token and start/end pointers in order to extract it.</li>
</ul>
<p dir="auto">This parsing function is in turn used by <code>picolEval</code> in order to execute the program. Every token is used either to form a new argument if a separator token was found before, or concatenated to the last argument (this is how interpolation is performed in Picol). Once an EOL (end of line) token is returned picolEval will call the command looking it up in a linked list of commands stored inside the interpreter structure.</p>
<p dir="auto">Variables and commands substitution is performed by <code>picolEval</code> itself. The parser is able to return variables and commands tokens already stripped by <code>$</code> and <code>[]</code>, so all it's required to do is to lookup the variable in the call frame and substitute the value with the token, or to recursively call <code>picolEval</code> if it's a command substitution, using the result instead of the original token.</p>
<p dir="auto">Commands are described by a name and a pointer to a C function implementing the command. In the command structure there is also a private data void pointer used in order to store data private to the command. This makes you able to implement multiple Picol commands using a single C function. User defined procedures are just like commands, but they are implemented by passing as private data the argument list and the body of the procedure, so a single C function is able to implement all the existing user defined procedures.</p>
<p dir="auto">Procedures call is trivial. The interpreter structure contains a call frame structure having more or less just a pointer to a liked list of variables (that are in turn structures with two fileds: name and value). When a procedure is called a new call frame is created and put at the top of the old one. When the procedure returns the top call frame is destroyed.</p>
<p dir="auto"><strong>Inside every large program there is a small program trying to get out -- Sir Tony Hoare.</strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I want to wash my car. The car wash is 50 meters away. Should I walk or drive? (1294 pts)]]></title>
            <link>https://mastodon.world/@knowmadd/116072773118828295</link>
            <guid>47031580</guid>
            <pubDate>Mon, 16 Feb 2026 06:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.world/@knowmadd/116072773118828295">https://mastodon.world/@knowmadd/116072773118828295</a>, See on <a href="https://news.ycombinator.com/item?id=47031580">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building SQLite with a small swarm (102 pts)]]></title>
            <link>https://kiankyars.github.io/machine_learning/2026/02/12/sqlite.html</link>
            <guid>47031268</guid>
            <pubDate>Mon, 16 Feb 2026 05:38:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kiankyars.github.io/machine_learning/2026/02/12/sqlite.html">https://kiankyars.github.io/machine_learning/2026/02/12/sqlite.html</a>, See on <a href="https://news.ycombinator.com/item?id=47031268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
      
    

    <article>
  <small>February 12, 2026</small>
  <h2 id="tldr">tl;dr</h2>

<p>I tasked Claude, Codex, and Gemini to build a <a href="https://github.com/kiankyars/sqlite">SQLite-like engine</a> in Rust.</p>

<ul>
  <li><strong>~19k~</strong> lines of code.</li>
  <li>Parser, planner, volcano executor, pager, b+trees, wal, recovery, joins, aggregates, indexing, transaction semantics, grouped aggregates, and stats-aware planning all implemented.</li>
  <li>282 unit tests, all passing.</li>
</ul>

<hr>

<h2 id="background">background</h2>

<p>Treat software engineering like distributed systems, and force coordination with: git, lock files, tests, and merge discipline.</p>

<hr>

<h2 id="harness">harness</h2>

<div><pre><code>├── AGENT_PROMPT.md       // main agent task prompt
├── BOOTSTRAP_PROMPT.md   // bootstrap (initialization) prompt
├── COALESCE_PROMPT.md    // deduplication prompt for coalescer agent
├── launch_agents.sh      // launches all agents and sets up isolated workspaces
├── agent_loop.sh         // per-agent loop/worker script
├── restart_agents.sh     // restarts agents
└── coalesce.sh           // invokes the coalescing script
</code></pre></div>

<hr>

<h2 id="workflow">workflow</h2>

<ol>
  <li><strong>bootstrap phase</strong>: one Claude run generates baseline docs, crate skeleton, and test harness.
    <div><pre><code> ├── Cargo.toml         // crate manifest
 ├── DESIGN.md          // architecture design notes
 ├── PROGRESS.md        // test &amp; build progress
 ├── README.md          // project overview
 ├── agent_logs         // per-agent log files
 ├── crates             // workspace subcrates
 ├── current_tasks      // lock files
 ├── notes              // inter-agent notes
 ├── target             // build artifacts
 └── test.sh            // test harness script
</code></pre></div>
  </li>
  <li><strong>worker phase</strong>: six workers loop forever (<code>2x Claude</code>, <code>2x Codex</code>, <code>2x Gemini</code>).</li>
</ol>

<hr>

<h2 id="loop">loop</h2>

<ol>
  <li>Each agent pulls latest main.</li>
  <li>Claims one scoped task.</li>
  <li>Implements + tests against sqlite3 as oracle.</li>
  <li>Updates shared progress/notes.</li>
  <li>Push.</li>
</ol>

<hr>

<h2 id="analysis">analysis</h2>

<h3 id="coordination-tax">coordination tax</h3>

<ul>
  <li><strong>84 / 154 commits (54.5%)</strong> were lock/claim/stale-lock/release coordination.</li>
  <li>Demonstrates parallel-agent throughput depends heavily on lock hygiene and stale-lock cleanup discipline.</li>
</ul>

<h3 id="what-helped-most">what helped most</h3>

<p>Two things looked decisive:</p>

<ul>
  <li><strong>oracle-style validation + high test cadence</strong> (<code>cargo test ...</code> and <code>./test.sh --fast</code>/full runs captured in <code>PROGRESS.md</code>).</li>
  <li><strong>strong module boundaries</strong> (<code>parser -&gt; planner -&gt; executor &lt;-&gt; storage</code>) so agents could work on orthogonal slices with fewer merge collisions.</li>
</ul>

<h3 id="redundancy">redundancy</h3>

<p>I implemented a coalescer with gemini to clean duplication/drift, since that is the largest problem with parallel agents. However, it only ran once at the end of the project, so it was never actually used during the run itself. I have a cron job which runs it daily, but gemini couldn’t complete the entire de-deuplication when I ran it during the expirement itself, which is to say it stopped mid-way through.</p>

<h3 id="takeaways">takeaways</h3>

<ul>
  <li>Parallelism is great, but only with strict task boundaries.</li>
  <li>Shared state docs (PROGRESS.md, design notes) are part of the runtime, not “documentation.”</li>
  <li>Tests are the anti-entropy force.</li>
  <li>Give agents a narrow interface, a common truth source, and fast feedback, and you get compounding throughput on real systems code.</li>
</ul>

<hr>

<h2 id="replication">replication</h2>

<p>To replicate this setup:</p>

<div><pre><code>git clone git@github.com:kiankyars/parallel-ralph.git
<span>mv </span>parallel-ralph/sqlite <span>.</span>
<span>chmod </span>700 sqlite/<span>*</span>.sh
./sqlite/launch_agents.sh
</code></pre></div>

<p>restart agents:</p>

<div><pre><code>./sqlite/restart_agents.sh claude/codex/gemini
</code></pre></div>

<p>coalesce agent:</p>



<p>Assumes you have the relevant CLIs installed (<code>claude</code>, <code>codex</code>, <code>gemini</code>), plus <code>screen</code>, <code>git</code>, Rust toolchain, and <code>sqlite3</code>.</p>

<hr>

<h2 id="limitations">limitations</h2>

<ul>
  <li>The documentation in the repo became enormous, <code>PROGRESS.md</code> became 490 lines and look at the sheer amount of <a href="https://github.com/kiankyars/sqlite/tree/main/notes">notes</a>; all this to say that the coalesce agent must be run as often as the other agents.</li>
  <li>There isn’t a great way to record token usage since each platform uses a different format, so I don’t have a grasp on which agent pulled the most weight.</li>
</ul>

<hr>

<h2 id="future-work">future work</h2>

<ul>
  <li>Track “substantive run rate”, since many are rate-limited/nothing happened.</li>
  <li>Only Claude adds itself as a co-author to each commit and I did not do that for Codex and Gemini, so I need to add a commit message for Gemini and Codex.</li>
  <li>Adding more strict observability because probably a lot of errors were due to the rate limit being hit mid work and then just not being able to like put then essentially at that point there was like only half finished work pushed.</li>
</ul>

<hr>

<h2 id="inspiration">inspiration</h2>

<ul>
  <li>https://cursor.com/blog/scaling-agents</li>
  <li>https://www.anthropic.com/engineering/building-c-compiler</li>
</ul>

<hr>

<h2 id="appendix">appendix</h2>

<h3 id="code-size-snapshot">code size snapshot</h3>

<table>
  <thead>
    <tr>
      <th>Language</th>
      <th>Files</th>
      <th>Lines</th>
      <th>Non-blank/Non-comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Rust</strong></td>
      <td>14</td>
      <td>18,650</td>
      <td>~16,155</td>
    </tr>
    <tr>
      <td><strong>Shell</strong></td>
      <td>1</td>
      <td>199</td>
      <td>~139</td>
    </tr>
    <tr>
      <td><strong>Total</strong></td>
      <td><strong>15</strong></td>
      <td><strong>18,849</strong></td>
      <td><strong>~16,294</strong></td>
    </tr>
  </tbody>
</table>

<p><em>154 commits between 2026-02-10 and 2026-02-12.</em></p>

<h3 id="usage">usage</h3>

<p>Gemini does not offer a way to monitor usage with their CLI. It’s also not on a weekly usage basis, but rather a 24-hour usage basis. For codex, I used 100% of the Pro Plan weekly usage, which is currently on a 2x promotion. I used 70% of the Claude Pro weekly usage.</p>

<ul>
  <li>codex
    <ul>
      <li><img src="https://kiankyars.github.io/imgs/2026-02-12-sqlite/sqlite1.png" alt=""></li>
    </ul>
  </li>
  <li>claude
    
  </li>
</ul>

<h3 id="disclaimer">disclaimer</h3>

<ul>
  <li>codex wrote the first draft for this post.</li>
</ul>

<hr>

<h2 id="citation">citation</h2>

<div><pre><code><span>@misc</span><span>{</span><span>kyars2026sqlite</span><span>,</span>
  <span>author</span> <span>=</span> <span>{Kian Kyars}</span><span>,</span>
  <span>title</span> <span>=</span> <span>{Building SQLite With a Small Swarm}</span><span>,</span>
  <span>year</span> <span>=</span> <span>{2026}</span><span>,</span>
  <span>month</span> <span>=</span> <span>feb</span><span>,</span>
  <span>day</span> <span>=</span> <span>{12}</span><span>,</span>
  <span>howpublished</span> <span>=</span> <span>{\url{https://kiankyars.github.io/machine_learning/2026/02/16/sqlite.html}}</span><span>,</span>
  <span>note</span> <span>=</span> <span>{Blog post}</span>
<span>}</span>
</code></pre></div>

</article>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arm wants a bigger slice of the chip business (143 pts)]]></title>
            <link>https://www.economist.com/business/2026/02/12/arm-wants-a-bigger-slice-of-the-chip-business</link>
            <guid>47030271</guid>
            <pubDate>Mon, 16 Feb 2026 02:36:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2026/02/12/arm-wants-a-bigger-slice-of-the-chip-business">https://www.economist.com/business/2026/02/12/arm-wants-a-bigger-slice-of-the-chip-business</a>, See on <a href="https://news.ycombinator.com/item?id=47030271">Hacker News</a></p>
Couldn't get https://www.economist.com/business/2026/02/12/arm-wants-a-bigger-slice-of-the-chip-business: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Pink noise reduces REM sleep and may harm sleep quality (105 pts)]]></title>
            <link>https://www.pennmedicine.org/news/pink-noise-reduces-rem-sleep-and-may-harm-sleep-quality</link>
            <guid>47029397</guid>
            <pubDate>Mon, 16 Feb 2026 00:35:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pennmedicine.org/news/pink-noise-reduces-rem-sleep-and-may-harm-sleep-quality">https://www.pennmedicine.org/news/pink-noise-reduces-rem-sleep-and-may-harm-sleep-quality</a>, See on <a href="https://news.ycombinator.com/item?id=47029397">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript-heavy approaches are not compatible with long-term performance goals (155 pts)]]></title>
            <link>https://sgom.es/posts/2026-02-13-js-heavy-approaches-are-not-compatible-with-long-term-performance-goals/</link>
            <guid>47029339</guid>
            <pubDate>Mon, 16 Feb 2026 00:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sgom.es/posts/2026-02-13-js-heavy-approaches-are-not-compatible-with-long-term-performance-goals/">https://sgom.es/posts/2026-02-13-js-heavy-approaches-are-not-compatible-with-long-term-performance-goals/</a>, See on <a href="https://news.ycombinator.com/item?id=47029339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    
<p>This post is a (very long) opinion piece, albeit one that I hope is somewhat substantiated by the accompanying anecdotes. The TL;DR is that in my experience the title is a correct statement the vast majority of the time, and we should favour more server-centric approaches instead, when possible.</p>
<details>
<summary>What I mean by “JS-heavy approaches“</summary>

<p>I use this term to refer to any approach that relies on shipping large quantities of JS to the browser, for the browser to execute as an intrinsic part of using the web app, often in the critical path. These sorts of approaches are usually associated with Single-page Applications (SPAs), but do unfortunately pop up in Multi-page Applications (MPAs) as well.</p>
</details>

<h2 id="background">Background</h2>
<h3 id="my-experience">My experience</h3>
<p>In case you don’t know me, hi! 👋 I work full-time on web performance at <a href="https://automattic.com/">Automattic</a>, as part of a small PerfOps team whose mission is to improve performance across the various things we build or work on as a company. Our team also builds and maintains our company’s performance monitoring infrastructure, but the majority of our time is spent finding and fixing performance issues across our various stacks, all the way from optimising DB queries to fixing janky CSS animations.</p>
<p>Personally, I specialize on the browser side of things. As part of this work, I have had more than my share of debugging sessions for loading performance issues, runtime performance problems, bundle size increases, framework-specific issues like expensive re-renders in <code>React</code>, etc., and there are some underlying root causes I’ve seen over and over again. These problems are not necessarily insurmountable, but they do add up to reveal some important gaps and outright falsehoolds in the narrative about the “modern web development approach” that we’ve been sold over the years.</p>
<h3 id="caveats">Caveats</h3>
<p>I don’t think I’ll necessarily bring any groundbreaking revelations to the table, nor do I come with a large corpus of data to back anything up as the unquestionable truth. This is all based on my own experience, and may not apply to what any given project is doing, particularly if it’s a small web app maintained by a focused, dedicated team.</p>
<p>Also note that I’ll be jumping between different aspects of web performance throughout the post, without much thought. That’s because while they’re all distinct, they’re all important! You don’t want to have an application that takes forever to load, any more than one that is sluggish at rendering the characters the user is typing, or one that has a disruptive delay on in-app navigations.</p>
<h3 id="the-pitch-and-the-reality-of-frameworks">The pitch and the reality of frameworks</h3>
<p>With the caveats out of the way, let’s look at the main topic: the long-term performance characteristics of “modern” development, which most of the time involves frameworks of some sort, like <code>React</code>.</p>
<details>
<summary>Yes, <code>React</code> is a framework</summary>

<p>I know there’s some discussion on whether <code>React</code> is a framework or not, with some folks — notably, the authors and maintainers — insisting on calling it a library. I don’t know why they do it, but it’s frankly wrong, as far as agreed-upon terms and shared vocabulary go.</p>
<p>In Computer Science, the <a href="https://en.wikipedia.org/wiki/Software_framework">commonly accepted distinction between the two</a> is that a library is something that your code calls into, while a framework is something that you relinquish control unto, so that <strong>it</strong> calls <strong>your code</strong> instead, at the appropriate time. That is, a framework works through Inversion of Control, and a library does not.</p>
<p>By this definition, <code>React</code> is a framework, because an idiomatic <code>React</code> application overwhelmingly consists of code that gets called by <code>React</code>, and lives under its control.</p>
</details>

<p>Frameworks like <code>React</code> are often perceived as accelerators, or even as the only sensible way to do web development. There’s this notion that a more “modern” stack (read: JS-heavy, where the JS ends up running on the user’s browser) allows you to be more agile, release more often with fewer bugs, make code more maintainable, and ultimately launch better sites. In short, the claim is that this approach will offer huge improvements to developer experience, and that these DevEx benefits will trickle down to the user.</p>
<p>But over the years, this narrative has proven to be unrealistic, at best. In reality, for any decently sized JS-heavy project, you should expect that what you build will be slower than advertised, it will keep getting slower over time while it sees ongoing work, and it will take more effort to develop and especially to maintain than what you were led to believe, with as many bugs as any other approach.</p>
<p>Where it comes to performance, the important thing to note is that a JS-heavy approach (and particularly one based on <code>React</code> &amp; friends) will most likely not be a good starting point; in fact, it will probably prove to be a performance minefield that you will need to keep revisiting, risking a detonation with every new commit.</p>
<h3 id="roadmap">Roadmap</h3>
<p>In this post, I’ll start by looking at some of the underlying causes for the most common categories of problems I’ve seen, and follow that up with mitigation strategies you can adopt to prevent some of them.</p>
<p>After that, I’ll reflect on whether JS-heavy approaches are worth it, whether performance is optional, and then explore server-centric development as an alternative, and why it tends to do better.</p>
<p>I’ll then finish up with my plea to our industry to change the way we do things.</p>
<h2 id="the-problems">The problems</h2>
<h3 id="dependencies-are-expensive">Dependencies are expensive</h3>
<p>One of the more immediate problems is that JS-heavy development approaches often deeply rely on the <code>npm</code> global package repository for runtime JS (i.e., the JS that the user’s browser will run). While this can be a great shortcut for getting something out the door quickly, it can come with some less-than-obvious costs, often measured in bytes on your bundle.</p>
<blockquote>
<p><strong>Note:</strong> while bundle size isn’t a metric that your users experience directly, most of the time it correlates strongly with ones that do (such as the various paint metrics), particularly in scenarios where large portions of your application get loaded as part of the critical path.</p>
</blockquote>
<p>It’s unfortunately common to pull large packages into your bundle, such as huge libraries for time and timezone management, extensive collections of JS utility functions, and entire component systems to kickstart your UI building. Some of these packages are well-designed, and built to be small and modular, so that you only pay for what you use. Many of them are not, however, and end up being monolithic, or close to it, due to their design (e.g. fluent APIs like <code>moment</code>), due to heavily reusing themselves internally (e.g. <code>lodash-es</code>), or due to failing to account for what bundlers need in order to perform effective tree-shaking.</p>
<p>Large dependencies are often bad enough on their own, but sadly, it gets worse: many of these packages grow larger over time. Which means that not only do we carry a ball and chain around, it keeps getting heavier!</p>
<ul>
<li>For example, <code>moment</code> has had a 10% increase in size over the last 5 years. Not outrageous, but lots of small increases like this across a number of dependencies do add up.</li>
<li><code>react-dom</code> has steadily grown over the years, and going from v18 to v19 alone results in a 33% increase in bundle size for a basic Hello World app.</li>
</ul>
<p>These increases are often hidden under the hood, and not something you’d easily come across, as most tools don’t give you this information by default:</p>
<ul>
<li><code>npm</code> won’t tell you how large a package is when you install or update it.</li>
<li>Many bundlers don’t show your bundle size at the end of a build (although some of the more recent ones like <code>Vite</code> or <code>Rsbuild</code>, do).</li>
<li>Your IDE or editor won’t tell you how much an import will add to your bundle unless you’ve specifically added a plugin for this.</li>
<li>If you have a tool like <code>dependabot</code> set up on your repo, it will helpfully point out that new versions are available and even prepare the upgrade PRs for you — but it won’t warn you that the new version is 20% larger.</li>
</ul>
<p>And to make matters worse, sticking to the previous version of a package is often not an option, because the new version includes security or bug fixes, or because another package is forcing you to upgrade a shared dependency.</p>
<p>But let’s say you were mindful when picking your dependencies, and you carefully vet each version upgrade not to balloon your bundle size. You’re also cautious about not adding too much code yourself. Does that mean your web app will be fast?</p>
<h3 id="its-easy-to-make-it-slow">It’s easy to make it slow</h3>
<p>A robust, developer-friendly system should make it easier to do the right thing than the wrong thing. It’s usually not feasible to make mistakes outright impossible, but it’s generally enough to ensure that following the wrong approach involves extra friction.</p>
<p>Unfortunately, where it comes to performance, JS-heavy development approaches often behave the opposite way: they make it easier to do the wrong thing than the right thing.</p>
<p>I’ll focus on <code>React</code> and <code>Redux</code> in some of my examples since that is what I have the most experience with, but much of this applies to other frameworks and to JS-heavy approaches in general. All of the following are issues I came across multiple times, in separate codebases:</p>
<ul>
<li>It’s often much easier to add things on a top-level component’s context and reuse them throughout the app, than it is to add them only where needed. But doing this means you’re paying the cost before (and whether) you need it.</li>
<li>It’s much simpler to add something as a synchronous top-level import and force it to be present on every code path, than it is to load it conditionally and deal with the resulting asynchronicity.</li>
<li>Setting up a bundler to produce one monolithic bundle is trivial, whereas splitting things up per route with shared bundles for the common bits often involves understanding and writing some complex configuration.</li>
<li>It’s less work to build a <code>Redux</code> reducer that blindly rewrites an entire subtree and leads to unnecessary rerenders, than it is to have it check to see if changes are needed.</li>
<li>It’s much more straightforward to write a <code>Redux</code> selector that blindly runs a <code>.map()</code> to generate a new array everytime, than it is to memoise the results correctly.</li>
<li>If you forget to use memoisation on a single prop, your component might now rerender every time something innocuous happens. And if it’s a root-adjacent component, that means potentially a large portion of your render tree needs to be recalculated.</li>
<li>It’s a lot less work to load your entire application state (i.e., all reducers) in the critical path, than it is to split things up so that only the relevant portions of state get loaded on each route.</li>
<li>It’s often much easier to wrap each of your SVG icons in JSX than it is to set things up to load them from an external <code>.svg</code> file with SVG <code>&lt;use&gt;</code>.</li>
</ul>
<p>This is compounded by the fact that developers are often taught the wrong approach by tutorials or other documentation resources. In an effort to make things easier to understand and learn, the code is kept simple — and as we’ve seen, the straightforward strategy is often the wrong one, at least where performance is concerned. Sometimes the docs go on to mention that they’re showing a simplified snippet of code and production code might need to be different. Sometimes they don’t.</p>
<p>These approaches are <strong>fragile</strong>, and make it easy for anyone to break performance at any time.</p>
<p>Which leads me to the next point. How likely is it that an initially fast JS-heavy app remains fast throughout development?</p>
<h3 id="its-hard-to-keep-it-fast">It’s hard to keep it fast</h3>
<p>If a system is fragile, it means that it even if it starts out solid, it will be hard to keep it intact as time goes by.</p>
<p>The unfortunate reality is that some of these performance issues are watershed moments, in that a single wrong move can effectively render all your previous efforts to avoid a given class of performance problems moot:</p>
<ul>
<li>If you’ve carefully made every usage of a library conditional and asynchronous, it only takes a single static import somewhere in the critical path for the library to now need to be parsed, compiled, and executed before your site gets a chance to render.</li>
<li>If you’ve painstakingly removed every instance of a large monolithic library like <code>moment</code>, it only takes a single import for it to come back in its entirety.</li>
<li>If you’ve isolated an endpoint from as much of the rest of the application as possible, it might only take a single import in that endpoint or in the shared core for a huge amount of code to pour back in.</li>
<li>It only takes a single, poorly-coded <code>Redux</code> selector or reducer to potentially affect performance on every action.</li>
</ul>
<p>These problems may be obvious to you as an experienced developer, but they might not be to someone who joined the team recently, or a coder whose experience mostly involves other frameworks or libraries. <strong>Any system that relies on developer discipline</strong> to maintain performance (or any other characteristic, really) <strong>is a fragile thing that requires constant vigilance</strong>, particularly for large teams or projects.</p>
<p>So while it may be easy to resolve the situation by rolling back the change or fixing the new code, the reality is that if you’re not carefully monitoring your web app, you might miss the problem entirely and unwittingly leave things in a worse state than before, despite all the work you had done to prevent the problem in the first place.</p>
<p>I’ve seen it happen time and time again over the years; some kinds of performance improvement efforts don’t tend to last for very long, because it’s just so easy to break things again.</p>
<p><em>“Okay”</em>, you might think, <em>“but these frameworks and friends claim to be focused on developer experience, which means they surely must come with some great tools for debugging performance issues too, right?”</em></p>
<p>Unfortunately, that’s often not the case.</p>
<h3 id="its-hard-to-debug-performance-issues">It’s hard to debug performance issues</h3>
<p>Performance is never easy to debug, even at the best of times.</p>
<p>That said, over the years, browser engines have put an incredible amount of effort into building robust web development tooling. Browser Dev Tools pretty much let you peer into every aspect of your site’s performance, and even into what the browser itself is doing while it loads and renders your site. They show you easy to understand waterfalls for resource loading, and helpful flamecharts for JS execution and rendering. They can throttle CPU and network, and run detailed profiles that combine all sorts of information. They can even now analyse those profiles for you, to point out problems and offer suggestions! These tools easily match and often exceed their server-side counterparts, and they’re a wonderful perk that we rely on daily.</p>
<p>Which is why it’s incredibly frustrating when frameworks like <code>React</code>, which again ostensibly pride themselves on being developer-friendly, throw all of that away to build their own Dev Tools. Poorly.</p>
<p>This is not the case with every framework, mind you; for instance, <code>Preact</code> takes the right approach, in <strong>enhancing</strong> the existing browser DevTools with framework-specific information. So if you run a browser profile on your <code>Preact</code> web app with their Dev Tools enabled, you’ll get some helpful markers added to the native browser profile, indicating when a component renders, for example. This lets you make sense of what’s happening in the browser layer as a result of the framework layer, so you can more quickly zero in on the code that’s seemingly causing a lot of garbage collection passes, or the component that’s triggering layout refreshes at an inconvenient time.</p>
<figure>
  <p><img loading="lazy" src="https://sgom.es/posts/2026-02-13-js-heavy-approaches-are-not-compatible-with-long-term-performance-goals/preact-dev-tools.jpg" alt="Preact-level timing information gets added to browser profiles, and is available to built-in functionality like the aggregate table of where time was spent. Great stuff! 👍">
  </p>
  <figcaption><code>Preact</code>-level timing information gets added to browser profiles, and is available to built-in functionality like the aggregate table of where time was spent. Great stuff! 👍</figcaption>
</figure>





<p>But that’s not the case with <code>React</code>: it has its own tools, which are entirely separate. You can run a <code>React</code> profile, but it doesn’t create a browser profile to go with it, and you have no way of combining <code>React</code> information with the JS sampling information from the browser to get a complete picture of where time is being spent. This means that often, even if you manage to figure out that the problem is in, say, a particular component’s commit to the DOM, you’re unable to understand why. You might need to resort to reproducing the problem twice (once for each profiler), carefully comparing the two separate profiles, and performing some leaps of logic.</p>
<figure>
  <p><img loading="lazy" src="https://sgom.es/posts/2026-02-13-js-heavy-approaches-are-not-compatible-with-long-term-performance-goals/react-dev-tools.jpg" alt="With React DevTools, browser profiles don’t include any React-level timing information 😞">
  </p>
  <figcaption>With <code>React</code> DevTools, browser profiles don’t include any <code>React</code>-level timing information 😞</figcaption>
</figure>





<p>In addition, the <code>React</code> DevTools profiler can be hard to use (from a UI point of view), and is missing essential information, like an aggregate view of where time was spent.</p>
<p>And this is not just a problem with <code>React</code> Dev Tools. The debugging information you need is often not available, and even when it is, you’re frequently forced to jump through extra hoops:</p>
<ul>
<li>In a server-side rendered <code>React</code> app, if your hydration fails and your application needs to render from scratch (a performance problem, in that you’re losing the SSR optimisation), the error messages are often a useless <code>Did not expect server HTML to contain a &lt;div&gt; in &lt;main&gt;.</code> or similar, even when running a debug build. Good luck finding that <code>&lt;div&gt;</code>!</li>
<li><code>Redux</code> Dev Tools don’t include any mechanism to help you find and debug slow reducers or selectors.</li>
<li>Debugging performance issues often requires you to find out about and switch to the “profiling“ build of your frameworks, if they exist at all. If you use a debug build instead, your timings won’t be meaningful, leading you to spend time trying to fix phantom issues; and if you use a production build, you’ll be missing the symbols that help you make sense of what’s happening.</li>
<li>To my knowledge, none of the bundlers default to building a map of what’s inside your bundle, for help with bundle size debugging if needed. Most of the time you need to find out about, install, and configure a separate plugin manually.</li>
</ul>
<p>Now, don’t get me wrong: the web platform can be a challenging performance environment on its own, with plenty of quirks to be aware of. But frameworks rarely address those definitively (if at all), and in fact make debugging performance <strong>harder</strong> than the baseline, because they add additional layers of abstraction.</p>
<p>The bigger the mismatch between a framework’s programming model and the underlying platform’s, the bigger the opportunity for complicated performance problems to arise in the interface between the two. So it sure would be nice if they would at least help with that part!</p>
<h2 id="mitigating-the-problems">Mitigating the problems</h2>
<p>So now that we know the problems and how easy it is for them to show up, what can we do about it? If you’re stuck building a JS-heavy client-side app, don’t despair! There are still actions you can take to improve your chances of making it faster, and keeping it from getting too slow over time.</p>
<p>Be warned, though: in my experience, you shouldn’t expect a consistently performant app, even if you follow all of these suggestions. But following them will help with stopping the bleeding and keeping things from getting too bad without you noticing.</p>
<h3 id="before-you-start">Before you start</h3>
<ul>
<li><strong>Make sure everyone is on the same page about performance.</strong> You might need to convince some non-technical folks by showing them industry studies that link page performance to business metrics like conversions. As for developers, you may need to remind them that their development devices and connections aren’t a good benchmark for “good enough”, since most users <a href="https://infrequently.org/2025/11/performance-inequality-gap-2026/">will unfortunately be relying on something significantly worse</a>.</li>
<li><strong>Define your performance budget.</strong> While there are some standard numbers across the industry, such as the thresholds Google recommends for <a href="https://web.dev/articles/vitals#core-web-vitals">its Core Web Vitals metrics</a>, not every project is the same. Depending on your audience and use-case, you may need to be more (or less) ambitious in your goals, or pick different metrics entirely. The key thing is to define them ahead of time, and stick to them during development, avoiding the temptation to move the goalposts to fit the outcome.</li>
<li><strong>Carefully choose your architecture and tech stack.</strong> Now’s a good time to figure out whether your client-side application should have a server-side aspect to it, to speed up initial renders. This is especially important if you have API calls in your critical path, and doubly so if they’re sequential in any way. Also, take the opportunity to research what’s out there, and pick the frameworks, libraries, and general approach that best fit your needs.</li>
</ul>
<h3 id="early-in-development">Early in development</h3>
<ul>
<li><strong>Set up code splitting.</strong> A lot of development environments give you this essentially for free, but many others unfortunately do not.<ul>
<li>Make sure that your bundler outputs multiple chunks, that are split in a reasonable way (e.g. per route).</li>
<li>Ensure that bundles for other routes don’t get loaded, or if they do, that it happens lazily, very late into the page loading process (e.g. after the <code>load</code> event). This is to ensure that they don’t compete with critical path resources for network and CPU availability.</li>
<li>Make sure you have things set up so that shared chunks are automatically created, with code that’s common to most bundles. There’s a balance to be found between fewer, larger chunks with more unused code for any given route, and having a multitude of tiny little chunks that all need to be fetched. Thankfully, that balance has been made easier to reach since we got widespread HTTP/2 usage, but do be mindful that there are still performance benefits to be had in keeping things down to fewer critical path resources.</li>
<li>Alternatively, you can create the shared bundles by hand, but keep in mind that this is an additional maintenance cost.</li>
</ul>
</li>
<li><strong>Set up bundle size tracking.</strong> While bundle size isn’t a particularly meaningful metric, it’s an easy one to track, and one that can serve as an early warning to look for actual performance issues.<ul>
<li>Have your bundler output bundle size when you build locally (e.g. <code>npm run build</code>), as a first step.</li>
<li>Once you have a CI (Continuous Integration) environment, add a build size action to it. For something like GitHub Actions, you can find pre-made ones that are fairly easy to set up (e.g. <a href="https://github.com/preactjs/compressed-size-action"><code>compressed-size-action</code></a>).</li>
<li>Once that’s in place, make sure that your CI adds a comment to pull requests where a large increase is detected. The earlier a developer learns about the problem, the better the chances they’ll fix it.</li>
</ul>
</li>
<li><strong>Set up linting.</strong> There aren’t many kinds of performance issues (or potential ones) that can be detected like this, but it’s hugely beneficial for those that can, since developers find out about them early in their work.<ul>
<li>Set up a list of “forbidden” imports, like <code>lodash</code> if you’re using <code>lodash-es</code>, as well as any packages you may have had trouble with in the past and want to avoid entirely.</li>
<li>Set up rules for packages that include both modular and monolithic imports, so that the monolithic ones are banned.</li>
<li>If you have a package or a portion of your application that you want to be loaded dynamically (i.e., with a dynamic <code>import()</code>), set up a rule to prevent static imports.</li>
<li>Keep updating the list as time goes by. Whenever you fix a problem or complete a performance-related migration away from something, consider adding new rules to help prevent the problem from resurfacing.</li>
</ul>
</li>
<li><strong>Set up performance tracking.</strong> You can use a tool like <code>puppeteer</code> or <code>playwright</code> to perform full page loads and in-app navigations, and measure how long things take. This is easily an order of magnitude more difficult than the previous suggestions, but it’s the best way I’m aware of to get somewhat meaningful performance numbers during development.<ul>
<li>Start by setting things up for running tests locally, on demand (e.g. <code>npm run test:perf</code>). Make sure you test both initial loading performance and in-app navigations, and that you use both network and CPU throttling when doing so.</li>
<li>Once that’s working, move it to CI, and run it on every pull request.</li>
<li>The tricky part here is making the numbers reliable, which can be incredibly difficult, depending on your CI environment. For some of them, it may actually be a bit of a lost cause, because they don’t give you any way to achieve reasonable stability.<ul>
<li>On the test suite side of things, you can try calibrating performance by running a pre-test CPU benchmark to determine an appropriate CPU throttling multiplier, as well as ensuring that only a single test runs at a time.</li>
<li>On the CI side of things, you can try to reduce the amount of virtualisation (ideally, to zero), and other concurrent work.</li>
<li>On the hardware side of things, if that’s an option, you can go as far as trying to force a fixed amount of RAM (when virtualising), as well as a fixed CPU speed.</li>
</ul>
</li>
<li>Once you manage to make it stable, with reasonably reliable numbers, have your CI add comments to pull requests whenever a large shift is detected.</li>
<li>If you can, make sure to keep historical data, so that you can visualise long-term trends. Small increases often go by unnoticed (particularly if your data is noisy), but they do add up over time.</li>
</ul>
</li>
</ul>
<h3 id="once-you-launch">Once you launch</h3>
<ul>
<li><strong>Add RUM (Real User Monitoring) performance tracking.</strong> This is the gold standard in performance monitoring, since it reflects what your real users are experiencing, regardless of what your expectations were.<ul>
<li>If your web app is large enough to be featured in the <a href="https://developer.chrome.com/docs/crux">CrUX</a> dataset, start by looking at those numbers. They’re fairly limited for client-side apps, and they won’t give you much in the way of debugging information, but they’ll help you calibrate your expectations with reality.</li>
<li>Choose a RUM vendor and integrate their client code into your app, or roll out your own system. The latter obviously involves more work, but it might help you tailor things better to your application, so that you have the metrics that make the most sense to you (e.g. when the first image rendered, if you’re building an image-focused site).</li>
<li>Consider including debug information in your app. While you don’t want your RUM code to grow huge, and you probably don’t want to collect lots of data for every user action, there may be some useful info around the metrics themselves that will help with debugging. For example, you could complement the Largest Contentful Paint value with ancillary information like what type of element the LCP target was, the external resource it related to (if any), or the portion of the UI it came from.</li>
</ul>
</li>
<li><strong>Collect performance feedback.</strong> If you have a support team, make sure they know who to reach for complaints about performance. Depending on your product, you may find that most users are non-technical and unable to communicate the problem clearly, but you’ll get a general idea of what the pain points are, and where to target your improvement work.</li>
</ul>
<h2 id="are-js-heavy-approaches-worth-it">Are JS-heavy approaches worth it?</h2>
<p>If that seems like a lot of overhead to you, we’re definitely in agreement. It’s a lot of running to stay in the same place, and it’s probably not even going to be such a great spot to stick around in anyway.</p>
<p>Implementing all of these mitigations will take a significant amount of time, and keeping the ongoing vigilance will force you to either have a dedicated team that actively tries to counter these effects, or to somehow educate all your developers on all of the pitfalls, and convince them to keep this level of vigilance themselves, constantly.</p>
<p>This level of overhead may be feasible if your organisation is really large or well-funded, but it’s less doable for everyone else.</p>
<p>Given all of this cost and the prospect of bad performance, at some point we should probably look at the benefits we’re getting in return for these JS-heavy approaches. We’ve talked about the promises, but what is reality like?</p>
<h3 id="is-react-worth-it">Is React worth it?</h3>
<p>I’m going to make things a bit more concrete for this section, and talk about <code>React</code> and its ecosystem specifically. They’re dominant, they’ve been around for a while, and they’re what I’ve had the most SPA experience with, so they’re the best examples I’ve got.</p>
<p>I do get the appeal of building in <code>React</code>, really, I do. It’s a fun programming model, and there are inherent organisational benefits to the component paradigm it pushes so hard. There’s also a lot you can reuse out there, and maybe that gives you the confidence that you’ll spend less time in the initial implementation, leaving you longer for improvements and polish.</p>
<p>And that’s fair, you probably won’t find a larger ecosystem of components, libraries, and complementary frameworks, ready to use! But beyond generally poor performance, they come with another pretty big caveat: they don’t seem to last very long. In my experience, there isn’t much in the way of stability in the <code>React</code> ecosystem, or even the JS ecosystem as a whole. Perhaps that’s changing lately, but historically, I’ve seen multiple large applications end up rewriting massive portions of their code after a while, not because of product reasons, but because of dependencies:</p>
<ul>
<li><code>React</code> hooks were meant to be an alternative to class-based components, but their adoption by the broader ecosystem forced substantial rewrites, as keeping class-based components (which can’t use hooks directly) in your code became less feasible.</li>
<li>The concurrency changes in <code>React</code> 18 forced many components to be rewritten to avoid practices that were formerly fine.</li>
<li>The deprecation of Enzyme forced projects to rewrite their entire test suites.</li>
<li>Some long-lived projects went through multiple state management architectures, as the favoured approach changed over the years, from prop drilling, to <code>Flux</code> or <code>MobX</code>, to <code>Redux</code>, to contexts, etc.</li>
</ul>
<p>Of course, I’m a developer as well, and I understand that tradeoffs are needed. When we’re building a project, there are a lot of different concerns that need to be carefully weighed so that it actually launches at some point, and in a half-decent state. So it may well be justifiable to accept all of the JS ecosystem instability, as well as some later code churn, and to view them as a reasonable price to pay for getting off the ground quickly. That’s fair.</p>
<p>That still leaves us with the issue of performance, though.</p>
<h2 id="is-performance-optional">Is performance optional?</h2>
<p>So what about performance, then? If we’re dealing with code churn, new features, and bugfixes, then we probably won’t have a lot of time for it, since as we saw, it comes with a lot of overhead in JS-heavy apps.</p>
<p>So maybe we decide that performance isn’t <strong>that</strong> important to us, and we’re better off forgoing some of the vigilance and monitoring above. If we get to it, great! If we don’t, the app will still work, and that’s what matters. And that can be a perfectly reasonable decision; less work is a good thing, as is not having to worry about performance!</p>
<p>But in the end, someone else will be making a decision: our audience. They’ll be choosing whether to use our site or not, and maybe whether to pay for it. That choice will involve how quickly they can get things done, and how “good” our site feels to use, which is non-trivially impacted by how fast it is for them.</p>
<p>In all likelihood, an unmonitored JS-heavy web app will not work well for all of our users. It’ll work great for some, sure, those with fast devices and connections that are similar to the typical developer’s, but chances are our audience is much broader than that, with all kinds of devices and connections. And so:</p>
<ul>
<li>Do we want to exclude entire classes of folks who can’t afford a high-end phone / laptop and a fast connection, and make them look at long loading screens?</li>
<li>Are we okay with having large portions of our audience see our sites through the lens of a sluggish experience, where every action makes them question whether they tapped the wrong thing or <em>“the site is just being slow again“</em>?</li>
<li>Do we want to keep sending megabytes of JavaScript down the wire (that probably doesn’t even get cached for very long because of a rapid release cycle), and inflating a pay-as-you-go customer’s bills with <strong>our code</strong> instead of <strong>their content</strong>?</li>
</ul>
<p>Is it worth it to give up on providing a great experience to a broad range of users? Opinions will vary, I’m sure, but my answer would be an emphatic “no”. We can do better than that.</p>
<h2 id="the-real-alternative-server-side-work">The real alternative: server-side work</h2>
<p>So as we’ve seen, there’s a whole host of problems when we build our web apps primarily out of JS. Most of them stem from just how expensive JS can be to run on users’ devices, and how easy it is to accidentally arrive at a place where the level of required work exceeds a device’s ability to do it in an acceptable timeframe.</p>
<details>
<summary>Just why is JS so expensive?</summary>

<p>Not all bytes are created equal, and JS is byte-for-byte the heaviest resource for a browser to handle. Parsing and compiling it are both expensive, with its complex syntax and type flexibility. When it finally does run, it mostly does so on a single thread, and can therefore only take advantage of a single core in your device’s CPU or SoC — and if it happens to be a little core in a heterogenous core architecture, it might be very slow indeed.</p>
</details>

<p>The obvious alternative is to shift all or much of this work to the server, so that even if it’s still JS (e.g. <code>nodejs</code>), it can take advantage of more powerful CPUs, dramatically higher power budgets (no battery life to worry about!), longer-lived bytecode caches, and better multi-core utilisation.</p>
<p>A server-centric architecture serves browsers with ready-to-use page content, instead of a recipe and some ingredients that allow them to build that content themselves, at their own expense.</p>
<h3 id="server-side-rendering-with-frameworks">Server-side rendering with frameworks</h3>
<p>However, not all server-side work is equally effective. While some of these JS-heavy architectures do involve a server-side rendering aspect to them, it’s often really only a band-aid, and a poorly-fitting one at that, which negates a lot of the benefits I was talking about.</p>
<p>The best example of this is probably monolithic hydration, where you render the whole page on the server, and ship both the initial markup and the application code to the user. The idea there is that the browser can use the markup to show the page quickly, without waiting for the JS, which only “hydrates“ the page with interactivity afterwards.</p>
<p>While this approach can definitely help with initial paint times, you’re still potentially shipping megabytes of JS down the wire to an unwitting user’s browser. That browser is still going to have to fetch, parse, compile, and run all that code on boot, which may end up resulting in a significant delay between the application being visible and actually being usable. So even if you solve the paint issue, there’s still the interactivity one — and you may have unwittingly made that one worse, if you’re now shipping even more JS down the wire to hydrate that initial state!</p>
<p>Partial hydration strategies can help lessen these costs, but you still need to be mindful of just how much JS is going to run at any one time and keep a watchful eye on that, which means one more thing to worry about and track.</p>
<h3 id="server-centric-models">Server-centric models</h3>
<p>The real alternative, as I see it, is to switch to a server-centric programming model, with the majority of the code intended never to leave the server. It can still be JS, too, if that’s what you prefer! Just not JS you will ever need to ship down the wire to someone’s device.</p>
<p>The server is a much more predictable and scalable environment, and more importantly, you have some level of control over it. Performance logging and debugging can be a lot easier since the bulk of the code is running on machines you manage in some way, not a vast array of wildly heterogenous computing devices entirely outside of your control. And most of the time you don’t need to worry about code size at all!</p>
<p>In a HTML-based approach, if your processing power and your content distribution strategy are sufficient for your needs, it becomes somewhat irrelevant if the user is running a high-end desktop computer with a blazingly fast CPU, or a bargain bin phone with an SoC that costs less than that desktop’s CPU cooler. Your web app will demand a lot less of your users’ devices and connections, providing a great experience to everyone.</p>
<h3 id="some-examples-of-server-centric-approaches">Some examples of server-centric approaches</h3>
<p>The obvious one is simple full-page navigation, which is the foundational architecture of the web. It has a proven track record of consistently hitting performance goals when paired with solid, globally-distributed hosting infrastructure that reduces network latency. Following this approach doesn’t mean that you can’t use JS; but you should lean towards keeping it as a sprinkle on top for some interactive bits here and there, not the engine the entire thing runs on. You can use isolated JS scripts, or other approaches like progressively-enhanced web components, and you can use as much or as little of a build process as you like to transform between source code and production code.</p>
<p>Beyond that, we’ve had various kinds of JS-lite architectures over the years that still do a great job, and have only been getting better. For example, the approach of substituting parts of the page with a new server-rendered HTML payload (made popular by e.g. <code>turbolinks</code>) still works really well, and now even has some level of native support with the <a href="https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API">View Transitions API</a>. HTML-enhancing approaches like <code>htmx</code> or WordPress’s Interactivity API are another kind of JS-lite approach, and can also serve as the foundation for many different kinds of projects.</p>
<h3 id="server-centric-isnt-a-good-fit-for-everything">Server-centric isn’t a good fit for everything</h3>
<p>Of course, we need to be realistic: not all web apps can be built like this, and some of them only really make sense as primarily client-side affairs. A highly interactive application like an editor is one such example, with server roundtrips ideally kept to a minimum, so that the user can focus on what they’re writing and not have to worry about a delay every time they click on something. They might even be willing to tolerate a longer page loading time than usual, if they see it as enough of a “destination”, and if their upcoming task (e.g. writing a post) is going to take long enough to justify the wait.</p>
<p>But even in the context of an entirely client-side application, there are many different approaches you could take, and in 2026, <code>React</code> is honestly one of the least compelling ones. It’s big, it’s slow, it’s riddled with performance pitfalls, and it’s not even fresh and exciting. It does have a huge ecosystem you can rely on, which is a really important point — albeit one that suggests that if that’s the deciding factor for you, you might be prioritising development speed above your users’ needs.</p>
<p>If you really need to go client-side, give one of the other frameworks a try! The framework runtimes are often smaller, they use finer-grained reactivity (which helps with modularity and UI responsiveness), and some of them use fundamentally different approaches that get away from the performance minefield of having to diff large trees all the time. A few have been around for a while, too, so you’re not necessarily relying on unproven dependencies.</p>
<h2 id="lets-change-the-way-we-do-things">Let’s change the way we do things</h2>
<p>To sum things up: in my experience, JS-heavy web apps usually start in a poor place, performance-wise, and they tend to get worse over time. Mitigating this requires significant overhead that is expensive to set up and maintain, and will usually still lead to performance degradation anyway — albeit at a slower rate.</p>
<p>The touted benefits are also not as obvious to me as they’re made out to be. Do frameworks actually make us more agile, and lead to a better overall user experience, or do they just trade one set of problems for another? Do they actually save us development time and effort in the long run, or does dependency-driven code churn in lasting projects undo much of that? I can’t answer any of this definitively, but I do feel that there’s a bit of a blind spot around the costs we’re paying, especially long-term.</p>
<p>Despite this, the industry perception seems to be that JS-heavy approaches are “fine”, performance-wise, and that the benefits are worth it. I find this overly optimistic, as none of the JS-heavy applications I’ve looked at in my work has ever managed to consistently hit good performance over time. Selection bias? Maybe, but <a href="https://infrequently.org/2025/11/performance-inequality-gap-2026/#content-trends">the wider landscape doesn’t provide much of a counterpoint to my anecdotes</a>.</p>
<p>I really do believe that the amount of effort needed to maintain a good level of performance in a healthy, long-running JS-heavy project is so high that it won’t be sustainable long term, in most cases. At some point, you’ll likely learn that your optimisation work was undone by unrelated changes, despite the monitoring you set up. Even if the developer that introduced the problem realised it, when deadlines clash with performance goals, the deadlines typically win.</p>
<p>This is why I’m a strong advocate for more robust systems, that make it harder to break performance in the first place.</p>
<p>Now, don’t get me wrong: building things server-side isn’t a panacea, nor is it the best fit for all projects. It’s certainly possible to build something slow on the server, and performance degradation can occur there as well. All systems have their performance challenges, and the server is no exception.</p>
<p>But maintaining a good level of performance does tend to be easier on the server, from what I’ve seen, because you’re keeping the expensive work where it makes sense: the powerful dedicated servers you own, manage, or rent; and not the slow, underpowered devices that your audience have access to. It’s also often much easier to track and log what’s happening on the server, which will greatly help with finding and fixing performance bugs, and even application logic ones. And there’s a lot less to send over the wire too, if your application code never leaves the server.</p>
<p>It’s time to stop lying to ourselves that JS-heavy client-side approaches are <strong>the way</strong> to develop a web app nowadays. We really should stop reflexively reaching for JS frameworks for everything we build, and to stop basing our architecture decisions on the stack we’re most familiar with.</p>
<p>Many use-cases are simply better suited to a server-side, HTML-centric approach, because the performance tradeoffs more closely align with the expected usage — and that really should be a part of the decision process. Instead of habitually <code>npm install</code>ing some framework boilerplate, we should take a moment to consider if we shouldn’t instead be building a multi-page application on the server.</p>
<p>Will client-side rendering really be helping, or will the user be stuck waiting for API calls anyway?</p>
<p>Will the user offset the upfront loading wait by later saving time on a number of in-app navigations or actions, or will they <a href="https://infrequently.org/2025/11/performance-inequality-gap-2026/#are-spas-working%3F">leave once they’ve done the couple of things they meant to</a>?</p>
<p>Will they get any benefit at all, as we borrow their device to do the work we could be handling on our servers?</p>
<p>We really owe it to our users to do better as an industry, because right now we’re often building the way we want to, not the way they need us to.</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I don't think AGI is imminent (125 pts)]]></title>
            <link>https://dlants.me/agi-not-imminent.html</link>
            <guid>47028923</guid>
            <pubDate>Sun, 15 Feb 2026 23:34:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dlants.me/agi-not-imminent.html">https://dlants.me/agi-not-imminent.html</a>, See on <a href="https://news.ycombinator.com/item?id=47028923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root"><p>February 14, 2026</p><div><ul><li><a href="#issue-1-cognitive-primitives-embodied-cognition">Issue 1: cognitive primitives, embodied cognition</a></li><li><a href="#what-about-world-models">What about world models?</a></li><li><a href="#benchmarking-the-gap">Benchmarking the gap</a></li><li><a href="#addendum-gemini-3-deep-think-and-inference-time-co">Addendum: Gemini 3 Deep Think and inference-time compute added after initial publication</a></li><li><a href="#issue-2-architecture">Issue 2: Architecture</a></li><li><a href="#revision-chain-of-thought-changes-this-picture-add">Revision: Chain of Thought changes this picture added after initial publication</a></li><li><a href="#the-discourse-problem">The Discourse Problem</a></li><li><a href="#research-labs-and-secrecy">Research Labs and Secrecy</a></li><li><a href="#what-does-this-mean">What does this mean?</a></li></ul></div><p>The CEOs of OpenAI and Anthropic have both claimed that human-level AI is just around the corner — and at times, that it's already here. These claims have generated enormous public attention. There has been some technical scrutiny of these claims, but critiques rarely reach the public discourse. This piece is a sketch of my own thinking about the boundary of transformer-based large language models and human-level cognition. I have an MS degree in Machine Learning from over a decade ago, and I don't work in the field of AI currently, but I am well-read on the underlying research. If you know more than I do about these topics, please <a href="https://dlants.me/about.html">reach out</a> and let me know, I would love to develop my thinking on this further.</p><h2 id="issue-1-cognitive-primitives-embodied-cognition"><a href="#issue-1-cognitive-primitives-embodied-cognition">Issue 1: cognitive primitives, embodied cognition</a></h2><p>Research in evolutionary neuroscience has identified a set of cognitive primitives that are <a href="https://pubmed.ncbi.nlm.nih.gov/39932126/">hardwired into vertebrate brains</a>: some of these are a sense of number, object permanence, causality, spatial navigation, and the ability to distinguish animate from inanimate motion. These capacities are <a href="https://www.nature.com/articles/s41598-024-64396-8">shared across vertebrates</a>, from fish to ungulates to primates, pointing to a common evolutionary origin hundreds of millions of years old.</p><p>Language evolved on top of these primitives — a tool for communication where both speaker and listener share the same cognitive foundation. Because both sides have always had these primitives, language takes them for granted and does not state them explicitly.</p><p>Consider the sentence "Mary held a ball." To understand it, you need to know that Mary is an animate entity capable of intentional action, that the ball is a separate, bounded, inanimate object with continuous existence through time, that Mary is roughly human-sized and upright while the ball is small enough to fit in her hand, that her hand exerts an upward force counteracting gravity, that the ball cannot pass through her palm, that releasing her grip would cause the ball to fall, and that there is one Mary and one ball, each persisting as the same entity from moment to moment, each occupying a distinct region of three-dimensional space. All of that is what a human understands from four words, and none of it is in the text. Modern LLMs are now trying to reverse-engineer this cognitive foundation from language, which is an extremely difficult task.</p><p>I find this to be useful framing for understanding many of the observed limitations of current LLM architectures. For example, transformer-based language models <a href="https://arxiv.org/abs/2410.05229">can't reliably do multi-digit arithmetic</a> because they have no number sense, only statistical patterns over digit tokens. They <a href="https://arxiv.org/abs/2309.12288">can't generalize simple logical relationships</a> — a model trained on "A is B" can't infer "B is A" — because they lack the compositional, symbolic machinery.</p><p>One might object: modern AIs are now being trained on video, not just text. And it's true that video prediction can teach something like object permanence. If you want to predict the next frame, you need to model what happens when an object passes behind an occluder, which is something like a representation of persistence. But I think the reality is more nuanced. Consider a shell game: a marble is placed under one of three cups, and the cups are shuffled. A video prediction model might learn the statistical regularity that "when a cup is lifted, a marble is usually there." But actually tracking the marble through the shuffling requires something deeper — a commitment to the marble as a <em>persistent entity</em> with a continuous trajectory through space. That's not merely a visual pattern.</p><p>The shortcomings of visual models align with this framing. Early GPT-based vision models failed at even basic spatial reasoning. Much of the recent progress has come from generating large swaths of synthetic training data. But even in this, we are trying to learn the physical and logical constraints of the real world from visual data. The results, predictably, are fragile. A model trained on synthetic shell game data could probably learn to track the marble. But I suspect that learning would not generalize to other situations and relations — it would be shell game tracking, not object permanence.</p><p>Developmental psychologist <a href="https://harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf">Elizabeth Spelke's research on "core knowledge"</a> has shown that infants — including blind infants — represent objects as bounded, cohesive, spatiotemporally continuous entities. This isn't a learned visual skill. It appears to be something deeper: a fundamental category of representation that the brain uses to organize all sensory input. Objects have identity. They persist. They can't teleport or merge. This "object-ness" likely predates vision itself — it's rooted in hundreds of millions of years of organisms needing to <em>interact with things in the physical world</em>, and I think this aspect of our evolutionary "training environment" is key to our robust cognitive primitives. Organisms don't merely observe reality to predict what happens next. They perceive in order to act, and they act in order to perceive. Object permanence allows you to track prey behind an obstacle. Number sense lets you estimate whether you're outnumbered. Logical composition enables tool construction and use. Spatial navigation helps you find your way home. Every cognitive primitive is directly linked to action in a rich, multisensory, physical world.</p><p>As Rodney Brooks <a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">has pointed out</a>, even human dexterity is a tight coupling of fine motor control and rich sensory feedback. Modern robots do not have nearly as rich of sensory information available to them. While LLMs have benefited from vast quantities of text, video, and audio available on the internet, we simply don't have large-scale datasets of rich, multisensory perception coupled to intentional action. Collecting or generating such data is extremely challenging.</p><h2 id="what-about-world-models"><a href="#what-about-world-models">What about world models?</a></h2><p>What if we built simulated environments where AIs could gather embodied experience? Would we be able to create learning scenarios where agents could learn some of these cognitive primitives, and could that generalize to improve LLMs? There are a few papers that I found that poke in this direction.</p><p>Google DeepMind's <a href="https://arxiv.org/abs/2512.04797">SIMA 2</a> is one. Despite the "embodied agent" branding, SIMA 2 is primarily trained through behavioral cloning: it watches human gameplay videos and learns to predict what actions they took. The reasoning and planning come from its base model (Gemini Flash-Lite), which was pretrained on internet text and images — not from embodied experience. There is an RL self-improvement stage where the agent does interact with environments, but this is secondary; the core intelligence is borrowed from language pretraining. SIMA 2 reaches near-human performance on many game tasks, but what it's really demonstrating is that a powerful language model can be taught to output keyboard actions.</p><p>Can insights from world-model training actually transfer to and improve language understanding? DeepMind's researchers explicitly frame this as a trade off between two competing objectives: "embodied competence" (acting effectively in 3D worlds) and "general reasoning" (the language and math abilities from pretraining). They found that baseline Gemini models, despite being powerful language models, achieved only 3-7% success rates on embodied tasks — demonstrating that embodied competence is not something that emerges from language pretraining. After fine-tuning on gameplay data, SIMA 2 achieved near-human performance on embodied tasks while showing "only minor regression" on language and math benchmarks. But notice the framing: the <em>best case</em> is that embodied training doesn't <em>hurt</em> language ability too much. There's no evidence that it <em>improves</em> it. The two capabilities sit in separate regions of the model's parameter space, coexisting but not meaningfully interacting. LLMs have billions of parameters, and there is plenty of room in those weights to predict language and to model a physical world <em>separately</em>. Bridging that gap — using physical understanding to actually improve language reasoning — remains undemonstrated.</p><p>DeepMind's <a href="https://arxiv.org/abs/2509.24527">Dreamer 4</a> also hints at this direction. Rather than borrowing intelligence from a language model, Dreamer 4 learns a world model from gameplay footage, then trains an RL agent within that world model through simulated rollouts where the agent takes actions, observes consequences provided by the world model, and updates its policy. This is genuinely closer to perception-action coupling: the agent learns <em>through</em> acting. However, the goal of this research is not general intelligence — it's sample-efficient control for robotics. The agent is trained and evaluated on predefined task milestones (get wood, craft pickaxe, find diamond), scored by a learned reward model. Nobody has tested whether the representations learned through this sort of training generalize to reasoning, language, or anything beyond the specific control tasks they were trained on. The gap between "an agent that learns to get diamonds in Minecraft through simulated practice" and "embodied experience that produces transferable cognitive primitives" is enormous and entirely unexplored.</p><p>As far as I understand, we don't know how to:</p><ul><li><p>embed an agent in a perception-action coupled training environment</p></li><li><p>create an objective and training process that leads it to learn cognitive primitives like spatial reasoning or object permanence</p></li><li><p>leverage this to improve language models or move closer to general artificial intelligence</p></li></ul><p>Recent benchmarking work underscores how far we are. Stanford's <a href="https://arxiv.org/abs/2511.20937">ENACT benchmark</a> (2025) tested whether frontier vision-language models exhibit signs of embodied cognition — things like affordance recognition, action-effect reasoning, and long-horizon memory. The results were stark: current models lag significantly behind humans, and the gap <em>widens</em> as tasks require longer interaction horizons.</p><p>In short: world models are a genuinely exciting direction, and they could be the path to learning foundational primitives like object permanence, causality, and affordance. But this work is still in the absolute earliest stages. Transformers were an incredible leap forward, which is why we can now have things like the ENACT benchmark which better illustrate the boundaries of cognition. I think this area is really promising, but research in this space could easily take decades.</p><p>I will also mention that the most prominent "world model" comes from Yann LeCun, who recently left Meta to start <a href="https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/">AMI Labs</a>. His <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">Joint Embedding Predictive Architecture (JEPA)</a> is a representation learning method: it trains a Vision Transformer on video data, masking parts of the input and predicting their abstract representations rather than their raw pixels. The innovation is predicting in representation space rather than input space, which lets the model focus on high-level structure and ignore unpredictable low-level details. This is a genuine improvement over generative approaches for learning useful embeddings. But despite the "world model" branding, JEPA's actual implementations (I-JEPA, V-JEPA, V-JEPA 2) are still training on passively observed video — not on agents embedded in physics simulations. There is no perception-action coupling, no closed-loop interaction with an environment. JEPA is a more sophisticated way to learn from observation, but by the logic of the argument above, observation alone is unlikely to yield the cognitive primitives that emerge from acting in the world.</p><h2 id="benchmarking-the-gap"><a href="#benchmarking-the-gap">Benchmarking the gap</a></h2><p>The <a href="https://arcprize.org/">ARC-AGI benchmark</a> offers an important illustration of where these primitives show up. ARC tasks are grid-based visual puzzles that test abstract reasoning: spatial composition, symmetry, relational abstraction, and few-shot generalization. They require no world knowledge or language — just the ability to infer abstract rules from a handful of examples and apply them to novel cases. Humans solve these tasks trivially, usually in under two attempts. When <a href="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025">ARC-AGI-2</a> launched in March 2025, pure LLMs scored 0% and frontier reasoning systems achieved only single-digit percentages. By the end of the year, <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">refinement-loop systems</a> — scaffolding that wraps a model in iterative generate-verify-refine cycles — pushed scores to <a href="https://poetiq.ai/posts/arcagi_verified/">54% on the semi-private eval</a> and as high as 75% on the public eval using GPT-5.2, surpassing the 60% human average. But the nature of this progress matters as much as the numbers.</p><p>The nature of this progress is telling: the top standalone model without <a href="https://arxiv.org/abs/2601.10904">refinement</a> scaffolding — Claude Opus 4.5 — scores 37.6%. It takes a <a href="https://poetiq.ai/posts/arcagi_verified/">refinement harness</a> running dozens of iterative generate-verify-refine cycles at $30/task to push that to 54%, and a combination of GPT-5.2's strongest reasoning mode plus such a harness to reach 75%. This is not behavior that comes out of the core transformer architecture — it is scaffolded brute-force search, with each percentage point requiring substantially more compute. The <a href="https://arcprize.org/competitions/2025/">ARC Prize Grand Prize</a> at 85% remains unclaimed.</p><p>ARC is important because it illustrates the kind of abstract reasoning that seems central to intelligence. For humans, these capabilities arose from embodied experience. It's conceivable that training methods operating in purely abstract or logical spaces could teach an agent similar primitives without embodiment. We simply don't know yet. Research in this direction is just beginning, catalyzed by benchmarks like ARC that are sharpening our understanding of the boundary between what LLMs do and what intelligence actually requires. Notably, the benchmark itself is evolving in this direction <a href="https://arxiv.org/abs/2601.10904">ARC-AGI-3</a> introduces interactive reasoning challenges requiring exploration, planning, memory, and goal acquisition — moving closer to the perception-action coupling that I argue is central to intelligence.</p><p>It's worth addressing a common counterargument here: AI models have saturated many benchmarks in recent years, and we have to keep introducing new ones. Isn't this just moving the goalposts? I don't think this framing is true - benchmark saturation is exactly how we learn what a benchmark was actually measuring. Creating different benchmarks in response is not goalpost-moving — it's the normal process of refining our instruments and understanding. The "G" in AGI stands for "general" — truly general intelligence should transfer from one reasoning task to another. If a model had genuinely learned abstract reasoning from saturating one benchmark, the next benchmark testing similar capabilities should be easy, not devastating. The fact that each new generation of benchmarks consistently exposes fundamental failures is itself evidence about the nature of the gap. The ARC benchmark series illustrates this well: the progression from ARC-AGI-1 to ARC-AGI-3 didn't require heroic effort to find tasks that stump AI while remaining easy for humans - it just required refining the understanding of where the boundary lies. Tasks that are trivially easy for humans but impossible for current models are <em>abundant</em> (see multi-digit arithmetic, above). The benchmark designers aren't hunting for exotic edge cases; they're mapping a vast territory of basic cognitive capability that AI simply doesn't have.</p><h2 id="addendum-gemini-3-deep-think-and-inference-time-co"><a href="#addendum-gemini-3-deep-think-and-inference-time-co">Addendum: Gemini 3 Deep Think and inference-time compute <em>added after initial publication</em></a></h2><p>I didn't realize while writing this piece that Google DeepMind released <a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/">Gemini 3 Deep Think</a> (February 12, 2026), which scored <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">84.6% on ARC-AGI-2</a> — just shy of the 85% Grand Prize threshold. For context, the base Gemini 3 Pro model scores 31.1%. The entire 53-point gap is inference-time compute: extended reasoning chains, parallel hypothesis exploration, and search.</p><p>This result is significant. While I wasn't able to find details about the architecture behind this particular model, the <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">ARC Prize team's earlier analysis of 2025 submissions</a> identifies "refinement loops" — iterative generate-verify-refine cycles — as the central theme driving progress. The intelligence is coming from scaffolding rather than from the base model having learned general abstract reasoning. As the ARC Prize team put it:</p><blockquote><p>For the ARC-AGI-1/2 format, we believe the Grand Prize accuracy gap is now primarily bottlenecked by engineering while the efficiency gap remains bottlenecked by science and ideas. ARC Prize stands for open AGI progress, and, as we've previously committed, we will continue to run the ARC-AGI-2 Grand Prize competition in 2026 to track progress towards a fully open and reproducible solution.</p></blockquote><blockquote><p>As good as AI reasoning systems are, they still exhibit many flaws and inefficiencies necessary for AGI. We still need new ideas, like how to separate knowledge and reasoning, among others. And we'll need new benchmarks to highlight when those new ideas arrive.</p></blockquote><p>I am now really curious about how the agents will fare with AGI-3, which comes out in March 2026. Are refinement loops / search / extended CoT chains effective at general reasoning? My guess is that these techniques are specifically fitting to the geometric pattern format of AGI 1 and 2, and we'll see a big drop-off in performance on AGI-3, which will be recovered over time as teams adjust their scaffolding to the new challenges.</p><h2 id="issue-2-architecture"><a href="#issue-2-architecture">Issue 2: Architecture</a></h2><p>The transformer architectures powering current LLMs are strictly feed-forward. Information flows from tokens through successive layers to the output, and from earlier tokens to later ones, but never backward. This is partly because backpropagation — the method used to train neural networks — <a href="https://www.offconvex.org/2016/12/20/backprop/">requires acyclic computation graphs</a>. But there's also a hard practical constraint: these models have hundreds of billions of parameters and are trained on trillions of tokens, and rely heavily on reusing computation. When processing token N+1, an LLM reuses all the computation from tokens 1 through N (a technique called KV caching). This is what makes training and inference tractable at scale. But it also means the architecture is locked into a one-directional flow — processing a new token can never revisit or revise the representations of earlier ones. Any architecture that allowed backward flow would compromise this caching, requiring novel computational techniques to make it tractable at scale.</p><p>Human brains function in a fundamentally different way. The brain is not a feed-forward pipeline. Activations reverberate through recurrent, bidirectional connections, eventually settling into stable patterns. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864796/">For every feedforward connection in the visual cortex, there is a reciprocal feedback connection</a> carrying contextual information back to earlier processing stages. When you recognize a face, it's not the output of a single forward pass — it's the result of distributed activity that echoes back and forth between regions until the system converges on an interpretation.</p><p>This is not to say that the human brain architecture is <em>necessary</em> to reach general intelligence. But the contrast helps contextualize just how constrained current LLM architectures are. There's a growing body of <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983">peer-reviewed theoretical work</a> formalizing these constraints. Merrill and Sabharwal have <a href="https://arxiv.org/abs/2207.00729">shown</a> that fixed-depth transformers with realistic (log-precision) arithmetic fall within the complexity class TC⁰ — which means they provably cannot recognize even regular languages or determine whether two nodes in a graph are connected. These are formally simple problems, well within the reach of basic algorithms, that transformers provably cannot solve in a single forward pass. This isn't an engineering limitation to be overcome with more data or compute — it's a mathematical property of the architecture itself. And Merrill and Sabharwal go further, arguing that this is a consequence of the transformer's high parallelizability: any architecture that is as parallelizable — and therefore as scalable — will hit similar walls.</p><p>What might alternative architectures look like? Gary Marcus has long advocated for other approaches, like <a href="https://arxiv.org/abs/2002.06177">neurosymbolic AI</a> — hybrid systems that combine neural networks with explicit symbolic reasoning modules for logic, compositionality, and variable binding. I think that neural architectures with feedback connections — networks that are not strictly feed-forward but allow information to flow backward and settle into stable states — could learn to represent cognitive primitives. The challenge, as discussed above, is that such architectures break the computational shortcuts that make current transformers trainable and deployable at scale. In either case, getting neurosymbolic, recurrent or bidirectional neural networks to work at the scale of modern LLMs is an open engineering and research problem.</p><h2 id="revision-chain-of-thought-changes-this-picture-add"><a href="#revision-chain-of-thought-changes-this-picture-add">Revision: Chain of Thought changes this picture <em>added after initial publication</em></a></h2><p>A <a href="https://news.ycombinator.com/item?id=47030078">reader pointed out</a> that chain of thought effectively invalidates the feed-forward argument, since we are never doing a single feed-forward pass, but instead repeated passes where preceding tokens are fed back into the network. As such, the transformer can use its own context window as a working space to solve a more complex class of problems. After this, I found a <a href="https://arxiv.org/abs/2310.07923">follow-up paper by the same authors</a> (Merrill &amp; Sabharwal, ICLR 2024) that confirms this. While a single forward pass through a transformer is limited to TC⁰, allowing the model to generate intermediate "chain of thought" tokens — where each token is the output of a new forward pass conditioned on all previous tokens — fundamentally extends its computational power. Specifically, with a polynomial number of CoT steps, a transformer can solve any problem in P.</p><p>This matters because modern "reasoning" models (OpenAI's o-series, Anthropic's Claude with extended thinking, DeepSeek R1) do exactly this: they generate long chains of intermediate reasoning tokens before producing an answer. The theoretical result says that this approach, in principle, overcomes the TC⁰ barrier I described above.</p><p>I'll admit I was a victim of anti-AI media hype on this point. I was sold on the architecture argument after reading a <a href="https://www.wired.com/story/ai-agents-math-doesnt-add-up/">Wired article</a> and <a href="https://arxiv.org/pdf/2507.07505">an accompanying paper</a> that brushed off CoT's impact on complexity, arguing that the base operation still carries the limited complexity and that token budgets are too small. In hindsight, that doesn't really address the formal result.</p><p>That said, there are important caveats. First, the theoretical result is about expressive power — what a transformer with CoT <em>could</em> compute with the right weights — not about what models actually <em>learn</em> to do. As the authors themselves note: "our lower bounds do not directly imply transformers can learn to use intermediate steps effectively." Whether current training methods (including reinforcement learning) can actually teach models to exploit this theoretical capacity is an open question.</p><p>Second, the P result works by showing that a transformer can encode the transitions of any <em>specific</em> Turing machine, with the CoT tokens serving as the tape. But AGI would require something more demanding: the feed-forward network would need to encode a <em>universal</em> Turing machine — one capable of reading a novel problem, constructing a solution strategy, and executing it. (Some smart) humans can do this. Whether a fixed-depth transformer can learn to do this through CoT, even in principle, is a much stronger claim than "CoT reaches P."</p><p>Furthermore, the systems achieving the highest scores on ARC-AGI-2 — like Gemini 3 Deep Think at 84.6% — go beyond simple sequential chain of thought. They use parallel hypothesis exploration, search over candidate solutions, and iterative refinement loops. This is a genuine extension to the feed-forward architecture: the transformer is no longer operating alone but is embedded in a broader program that orchestrates multiple inference passes, evaluates their outputs, and steers the search. In the original version of this piece, I suggested that alternative architectures with feedback connections might be needed. What's actually emerging is something different — the feedback is happening <em>outside</em> the model, in scaffolding that wraps the transformer in a loop. Whether this external scaffolding can ultimately substitute for the kind of internal recurrence I was imagining remains to be seen, but the progress is harder to dismiss than I initially thought.</p><p>So the architecture argument is weaker than I originally stated, but it isn't entirely gone. The theoretical ceiling has been raised from TC⁰ to P, which is a significant expansion. Whether models can actually reach that ceiling through current training methods, and whether P is sufficient for the kind of flexible, general reasoning that characterizes intelligence, remain open questions.</p><h2 id="the-discourse-problem"><a href="#the-discourse-problem">The Discourse Problem</a></h2><p>Most people encounter AGI through CEO proclamations. Sam Altman <a href="https://openai.com/index/reflections/">claims</a> that OpenAI knows how to build superintelligent AI. Dario Amodei <a href="https://darioamodei.com/machines-of-loving-grace">writes</a> that AI could be "smarter than a Nobel Prize winner across most relevant fields" by 2026. These are marketing statements from people whose companies depend on continued investment in the premise that AGI is imminent. They are not technical arguments.</p><p>Meanwhile, the actual research community tells a different story. A <a href="https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-FINAL.pdf">2025 survey by the Association for the Advancement of Artificial Intelligence (AAAI)</a>, surveying 475 AI researchers, found that 76% believe scaling up current AI approaches to achieve AGI is "unlikely" or "very unlikely" to succeed. The researchers cited specific limitations: difficulties in long-term planning and reasoning, generalization beyond training data, causal and counterfactual reasoning, and embodiment and real-world interaction. This is an extraordinary disconnect.</p><p>Consider the <a href="https://ai-2027.com/">AI 2027</a> scenario, perhaps the most widely-discussed AGI forecast of 2025. The <a href="https://www.aifuturesmodel.com/">underlying model's</a> first step is automating coding, which is entirely based on an extrapolation of the <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">METR study</a> on coding time horizons. The METR study collects coding tasks that an AI can complete with a 50% success rate, and tracks how the duration of those tasks grows over time. But task duration is not a measure of task complexity. As the ARC-AGI benchmarks illustrate, there are classes of problems that take humans only seconds to solve but that require AI systems thousands of dollars of compute and dozens of iterative refinement cycles to approach — and even then, the 85% Grand Prize threshold remains unmet. The focus on common coding tasks strongly emphasizes <em>within distribution</em> tasks, which are well-represented within the AI training set. The 50% success threshold also allows one to ignore precisely the tricky, out of distribution, short tasks that agents may not be making any progress on at all. The second step within the 2027 modeling is agents developing "research taste". My take is that research taste is going to rely heavily on the short-duration cognitive primitives that the ARC highlights but the METR metric does not capture.</p><p>I'd encourage anyone interested in this topic to seek out technical depth. Understand what these systems actually can and can't do. The real story is fascinating - it's about the fundamental nature of intelligence, and how far we still have to go to understand it.</p><h2 id="research-labs-and-secrecy"><a href="#research-labs-and-secrecy">Research Labs and Secrecy</a></h2><p>Betting against AI is difficult currently, due to the sheer amount of capital being thrown at it. One thing I've spent a lot of time thinking about is — what if there's a lab somewhere out there that's about to crack this? Maybe there are labs — even within OpenAI and Anthropic themselves — that are already working on all of these problems and keeping them secret?</p><p>But the open questions described above are not the kind of problem a secret lab can solve. They are long-standing problems that span multiple different fields — embodied cognition, evolutionary neuroscience, architecture design and complexity theory, training methodology and generalizability. Solving problems like this requires a global research community working across disciplines over many years, with plenty of dead ends along the way. This is high-risk, low-probability-of-reward, researchers-tinkering-in-a-lab kind of work. It's not a sprint towards a finish line.</p><p>This also helps us frame what AI companies are actually doing. They're buying up GPUs, building data centers, expanding product surface area, securing more funding. They are scaling up the current paradigm, which doesn't really have bearing on the fundamental research that can make progress in the problems highlighted above.</p><h2 id="what-does-this-mean"><a href="#what-does-this-mean">What does this mean?</a></h2><p>I'm not saying that AGI is impossible, or even that it won't come within our lifetime. I fully believe neural networks, using appropriate architectures and training methods, can represent cognitive primitives and reach superhuman intelligence. They can probably do this without repeating our long evolutionary history, by training in simulated logical / symbolic simulations that have little to do with the physical world. I am also not saying that LLMs aren't useful. Even the current technology is fundamentally transforming our society (see <a href="https://dlants.me/ai-mid.html">AI is not mid - a response to Dr. Cottom’s NYT Op-Ed</a>)</p><p>We have to remember though that neural networks have their origins in the 1950's. Modern backpropagation was popularized in 1986. Many of the advances that made modern GPTs possible were discovered gradually over the following decades:</p><ul><li><p>Long Short-Term Memory (LSTM) networks, which solved the vanishing gradient problem for sequence modeling — Hochreiter and Schmidhuber, 1997</p></li><li><p>Attention mechanisms, which allowed models to dynamically focus on relevant parts of their input — Bahdanau et al., 2014</p></li><li><p>Residual connections (skip layers), which made it possible to train networks hundreds of layers deep — He et al., 2015</p></li><li><p>The transformer architecture itself, which combined attention with parallelizable training to replace recurrent networks entirely — Vaswani et al., 2017</p></li></ul><p>Transformers have fundamental limitations. They are very powerful, and they have taught us a lot about what general intelligence is. We are gaining a more and more crisp understanding of where the boundaries lie. But solving these problems will require research, which is a non-linear processs full of dead ends and plateaus. It could take decades, and even then we might discover new and more nuanced issues.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peter Thiel: 2,436 emails with Epstein from 2014 to 2019 (238 pts)]]></title>
            <link>https://jmail.world/wiki/peter-thiel</link>
            <guid>47028369</guid>
            <pubDate>Sun, 15 Feb 2026 22:29:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jmail.world/wiki/peter-thiel">https://jmail.world/wiki/peter-thiel</a>, See on <a href="https://news.ycombinator.com/item?id=47028369">Hacker News</a></p>
<div id="readability-page-1" class="page"><!--$?--><template id="B:1"></template><!--/$--><title>Peter Thiel — The Jmail Encyclopedia</title><meta name="description" content="Peter Thiel — The Jmail Encyclopedia. Sourced from 2,429 emails across 1,345 threads in the Jmail archive."><meta name="author" content="Luke Igel"><meta name="author" content="Riley Walz"><meta name="creator" content="Luke Igel &amp; Riley Walz"><meta name="publisher" content="Kino AI"><meta name="robots" content="index, follow"><meta name="googlebot" content="index, follow"><meta property="og:title" content="Peter Thiel — The Jmail Encyclopedia"><meta property="og:description" content="Peter Thiel — The Jmail Encyclopedia. Sourced from 2,429 emails across 1,345 threads in the Jmail archive."><meta property="og:url" content="https://jmail.world/wiki/peter-thiel"><meta property="og:site_name" content="Jmail"><meta property="og:image" content="https://jmail.world/api/og/jwiki/peter-thiel"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Peter Thiel — The Jmail Encyclopedia"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@jmailarchive"><meta name="twitter:title" content="Peter Thiel — The Jmail Encyclopedia"><meta name="twitter:description" content="Peter Thiel — The Jmail Encyclopedia. Sourced from 2,429 emails across 1,345 threads in the Jmail archive."><meta name="twitter:image" content="https://jmail.world/api/og/jwiki/peter-thiel"></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magnus Carlsen Wins the Freestyle (Chess960) World Championship (352 pts)]]></title>
            <link>https://www.fide.com/magnus-carlsen-wins-2026-fide-freestyle-world-championship/</link>
            <guid>47028227</guid>
            <pubDate>Sun, 15 Feb 2026 22:17:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fide.com/magnus-carlsen-wins-2026-fide-freestyle-world-championship/">https://www.fide.com/magnus-carlsen-wins-2026-fide-freestyle-world-championship/</a>, See on <a href="https://news.ycombinator.com/item?id=47028227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="377e8e4" data-element_type="widget" data-elementor-type="wp-post" data-elementor-id="43662" data-elementor-post-type="post" data-widget_type="theme-post-content.default">
				<div data-id="815ffae" data-element_type="widget" data-widget_type="image.default">
				<p><img fetchpriority="high" decoding="async" width="1000" height="605" src="https://www.fide.com/wp-content/uploads/FS-D03.jpg" alt="" srcset="https://www.fide.com/wp-content/uploads/FS-D03.jpg 1000w, https://www.fide.com/wp-content/uploads/FS-D03-300x182.jpg 300w, https://www.fide.com/wp-content/uploads/FS-D03-768x465.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px">															</p>
				</div>
				<div data-id="13c5e19" data-element_type="widget" data-widget_type="text-editor.default">
									<p><strong>Magnus Carlsen</strong> (Norway) is the 2026 FIDE Freestyle Chess World Champion. A draw in the fourth and final game against <strong>Fabiano Caruana</strong> (USA) was enough to seal a 2.5–1.5 match victory in Weissenhaus, Germany.</p><p><span>&nbsp;</span><span>The decisive moment came in game three. Carlsen won from a dead lost position, turning the match in his favor. Entering the final game, he needed only a draw and achieved it in an equal endgame after Caruana missed late chances to mount a comeback. Both finalists qualified for the 2027 FIDE Freestyle Chess World Championship.</span></p>								</div>
				<div data-id="3d4b435" data-element_type="widget" data-widget_type="image.default">
				<p><img decoding="async" width="1000" height="661" src="https://www.fide.com/wp-content/uploads/FS-D03-Carlsen-Caruana.jpg" alt="" srcset="https://www.fide.com/wp-content/uploads/FS-D03-Carlsen-Caruana.jpg 1000w, https://www.fide.com/wp-content/uploads/FS-D03-Carlsen-Caruana-300x198.jpg 300w, https://www.fide.com/wp-content/uploads/FS-D03-Carlsen-Caruana-768x508.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px">															</p>
				</div>
				<div data-id="cb9359c" data-element_type="widget" data-widget_type="text-editor.default">
									<p>The 2026 tournament marks the first official FIDE-recognized Freestyle Chess World Championship. World number one Carlsen had previously attempted to win the FIDE Fischer Random World Championship without success. In Weissenhaus, he secured the official FIDE Freestyle Chess title – his 21st world title across formats.</p><p><span>&nbsp;</span><span>In the match for third place, <strong>Nodirbek Abdusattorov</strong> (Uzbekistan) defeated <strong>Vincent Keymer</strong> (Germany). Abdusattorov secured the match by drawing a winning position in the final game, also ensuring qualification for the 2027 championship.</span></p>								</div>
				<div data-id="27aa71b" data-element_type="widget" data-widget_type="image.default">
				<p><img decoding="async" width="1000" height="670" src="https://www.fide.com/wp-content/uploads/FS-D03-Abdusattorov-Keymer.jpg" alt="" srcset="https://www.fide.com/wp-content/uploads/FS-D03-Abdusattorov-Keymer.jpg 1000w, https://www.fide.com/wp-content/uploads/FS-D03-Abdusattorov-Keymer-300x201.jpg 300w, https://www.fide.com/wp-content/uploads/FS-D03-Abdusattorov-Keymer-768x515.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px">															</p>
				</div>
				<div data-id="55d317e" data-element_type="widget" data-widget_type="text-editor.default">
									<p><span><strong>Hans Niemann</strong> (USA) took fifth place with a 2–0 victory over <strong>Arjun Erigaisi</strong> (India), while <strong>Levon Aronian</strong> (USA) won his Armageddon against <strong>Javokhir Sindarov</strong> (Uzbekistan) to take seventh place.</span></p><p>In the women’s exhibition match, <strong>Bibisara Assaubayeva</strong> (Kazakhstan) prevailed over <strong>Alexandra Kosteniuk</strong> (Switzerland) after winning the third game and drawing the fourth.</p>								</div>
				<div data-id="b1752d4" data-element_type="widget" data-widget_type="image.default">
				<p><img loading="lazy" decoding="async" width="1000" height="670" src="https://www.fide.com/wp-content/uploads/FS-D03-Kosteniuk-Assaubayeva.jpg" alt="" srcset="https://www.fide.com/wp-content/uploads/FS-D03-Kosteniuk-Assaubayeva.jpg 1000w, https://www.fide.com/wp-content/uploads/FS-D03-Kosteniuk-Assaubayeva-300x201.jpg 300w, https://www.fide.com/wp-content/uploads/FS-D03-Kosteniuk-Assaubayeva-768x515.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px">															</p>
				</div>
				<div data-id="20a76b5" data-element_type="widget" data-widget_type="text-editor.default">
									<p><strong>Key facts</strong>:</p><ul><li><span>⁠</span><span>⁠<span>Location: Weissenhaus, Germany</span></span></li><li><span><span>⁠</span><span>Dates: February 13–15, 2026</span></span></li><li><span><span>⁠</span><span>Prize fund: $300,000</span></span></li><li><span><span>⁠</span><span>Winner’s prize: $100,000</span></span></li><li><span><span>⁠</span><span>Top three qualify for 2027</span></span></li></ul><p><span>&nbsp;</span><span>A full report is available <a href="https://www.freestyle-chess.com/news/magnus-carlsen-wins-fide-freestyle-chess-world-championship/"><strong>[HERE]</strong></a>.</span></p><p><strong>Written by Till Behrend</strong></p><p><strong>Photos: Lennart Ootes and Steve Bonhage / Freestyle Chess</strong></p><p>Official website:&nbsp;<strong><a href="https://www.freestyle-chess.com/">https://www.freestyle-chess.com/</a></strong></p>								</div>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I’m joining OpenAI (1324 pts)]]></title>
            <link>https://steipete.me/posts/2026/openclaw</link>
            <guid>47028013</guid>
            <pubDate>Sun, 15 Feb 2026 21:54:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steipete.me/posts/2026/openclaw">https://steipete.me/posts/2026/openclaw</a>, See on <a href="https://news.ycombinator.com/item?id=47028013">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" data-astro-cid-vj4tpspi="">  <p><strong>tl;dr: I’m joining OpenAI to work on bringing agents to everyone. <a href="https://openclaw.ai/">OpenClaw</a> will move to a foundation and stay open and independent.</strong></p>
<p>The last month was a whirlwind, never would I have expected that my playground project would create such waves. The internet got weird again, and it’s been incredibly fun to see how my work inspired so many people around the world.</p>
<p>There’s an endless array of possibilities that opened up for me, countless people trying to push me into various directions, giving me advice, asking how they can invest or what I will do. Saying it’s overwhelming is an understatement.</p>
<p>When I started exploring AI, my goal was to have fun and inspire people. And here we are, the lobster is taking over the world. My next mission is to build an agent that even my mum can use. That’ll need a much broader change, a lot more thought on how to do it safely, and access to the very latest models and research.</p>
<p>Yes, I could totally see how OpenClaw could become a huge company. And no, it’s not really exciting for me. I’m a builder at heart. I did the whole creating-a-company game already, poured 13 years of my life into it and learned a lot. What I want is to change the world, not build a large company and teaming up with OpenAI is the fastest way to bring this to everyone.</p>
<p>I spent last week in San Francisco talking with the major labs, getting access to people and unreleased research, and it’s been inspiring on all fronts. I want to thank all the folks I talked to this week and am thankful for the opportunities.</p>
<p>It’s always been important to me that OpenClaw stays open source and given the freedom to flourish. Ultimately, I felt OpenAI was the best place to continue pushing on my vision and expand its reach. The more I talked with the people there, the clearer it became that we both share the same vision.</p>
<p>The community around OpenClaw is something magical and OpenAI has made strong commitments to enable me to dedicate my time to it and already sponsors the project. To get this into a proper structure I’m working on making it a foundation. It will stay a place for thinkers, hackers and people that want a way to own their data, with the goal of supporting even more models and companies.</p>
<p>Personally I’m super excited to join OpenAI, be part of the frontier of AI research and development, and continue building with all of you.</p>
<p>The claw is the law.</p>
<p><img src="https://steipete.me/assets/img/2026/openclaw/clawcon.jpg" alt="ClawCon" loading="lazy"></p> </article></div>]]></description>
        </item>
    </channel>
</rss>