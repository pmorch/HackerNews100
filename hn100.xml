<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 07 Jun 2024 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How Does GPT-4o Encode Images? (124 pts)]]></title>
            <link>https://www.oranlooney.com/post/gpt-cnn/</link>
            <guid>40608269</guid>
            <pubDate>Fri, 07 Jun 2024 12:54:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oranlooney.com/post/gpt-cnn/">https://www.oranlooney.com/post/gpt-cnn/</a>, See on <a href="https://news.ycombinator.com/item?id=40608269">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <img src="https://www.oranlooney.com/post/gpt-cnn_files/lead.jpg">
      
      
      
      <hr>
      <ul>
        
        <li><time>June 5, 2024</time></li>
        <li>
          <a href="https://www.oranlooney.com/tags/machine-learning/">
            <i></i>
            Machine Learning
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/llm/">
            <i></i>
            LLM
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/cnn/">
            <i></i>
            CNN
          </a>&nbsp;
        </li>
      </ul>
      

      

<p>Here’s a <a href="https://openai.com/api/pricing/">fact</a>: GPT-4o charges 170 tokens to process each <code>512x512</code> tile
used in high-res mode. At ~0.75 tokens/word, this suggests a picture is worth
about 227 words—only a factor of four off from the traditional saying.</p>

<p>(There’s also an 85 tokens charge for a low-res ‘master thumbnail’ of each picture
and higher resolution images are broken into many such <code>512x512</code> tiles,
but let’s just focus on a single high-res tile.)</p>

<p>OK, but <em>why</em> 170? It’s an oddly specific number, isn’t it? OpenAI uses round
numbers like “\$20” or “\$0.50” in their pricing, or powers of 2 and 3 for their
internal dimensions. Why choose a numbers like 170 in this instance?</p>

<p>Numbers that are just dropped into a codebase without explanation are called
“<a href="https://en.wikipedia.org/wiki/Magic_number_(programming)">magic numbers</a>” in programming, and 170 is a pretty glaring magic
number.</p>

<p>And why are image costs even being converted to token counts anyway? If it were
just for billing purposes, wouldn’t it be less confusing to simply list the
cost per tile?</p>

<p>What if OpenAI chose 170, not as part of some arcane pricing strategy, but
simply because it’s literally true? What if image tiles are in fact represented
as 170 consecutive embedding vectors? And if so, how?</p>

<h2 id="embeddings">Embeddings</h2>

<p>The first thing to recall about the transformer model is that it operates on
vectors, not discrete tokens. The inputs <em>have</em> to be vectors, or the dot
product similarity at the heart of the transformer wouldn’t make any sense. The
whole concept of tokens is a pre-processing step: text is converted to tokens
and tokens are converted to embedding vectors by an embedding model before they
even hit the first layer of the transformer model.</p>

<p>For example, Llama 3 uses 4,096 feature dimensions internally. Consider the
sentence, “My very educated mother just served us nine pizzas.” It gets
converted into 10 integer tokens (counting the period) by <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a>, then
those are each converted into 4,096-dimensional vectors by an embedding model,
resulting in a <code>10x4096</code> matrix. That’s the “real” input into a transformer
model.</p>

<p>But there’s no law that says that these vectors <strong>must</strong> come from a text embedding
model. It’s a strategy that works well for text data, but if we have data in a
different format that we want to feed into a transformer then we can simply use
a different embedding strategy.</p>

<p>We know that OpenAI has been thinking along these lines because in 2021 they
released the <a href="https://openai.com/index/clip/">CLIP embedding model</a>. CLIP embeds both text and images
into the same semantic vector space, allowing you to use cosine similarity to
find images related to text strings, or images which are semantically similar
to other images. You can try the <a href="https://huggingface.co/spaces/vivien/clip">demo</a> on hugging face to get a feel for
how it works:</p>

<p><a href="https://huggingface.co/spaces/vivien/clip"><img src="https://www.oranlooney.com/post/gpt-cnn_files/clip_demo.png"></a></p>

<p>However, CLIP embeds the entire image as a single vector, not 170 of them.
GPT-4o must be using a different, more advanced strategy internally to
represent images (and likewise video, voice, and other kinds of data; that’s
why it’s “omnimodal.”)</p>

<p>Let’s see if we can’t deduce what that strategy might be for image data in
particular.</p>

<h2 id="number-of-feature-dimensions">Number of Feature Dimensions</h2>

<p>Let’s start by guesstimating the number of dimensions used internally by GPT-4o
to represent embedding vectors. We can’t know the real number for certain
because it’s proprietary, but we can make some reasonable assumptions.</p>

<p>OpenAI seems to likes powers of 2, sometimes with a single factor of 3 mixed
in. For example, they used 1,536 for <a href="https://openai.com/index/new-and-improved-embedding-model/">ada-002</a> embeddings or 3,072 for
<a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">text-embedding-3-large</a>. GPT-3 is known to use <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">12,288 dimensions
throughout</a>. It’s probable that GPT-4o either kept or increased that
parameter.</p>

<p>It doesn’t seem likely that the number of embeddings would have gone down from
GPT-3 to GPT-4o, but it’s possible. Releases like GPT-4 Turbo were actually
faster and cheaper than earlier version, and a reduction in embedding dimension
may have been part of that if the developers had benchmarks showing that the
smaller size was just as good in terms of quality.</p>

<blockquote>
<p>“Interest rates may go up, they may go down, or they may stay the same. I’m
sorry, but I really can’t be any more vague than that.”
—Alan Greenspan</p>
</blockquote>

<p>Given all that, it’s likely that the number of feature dimensions used inside
of GPT-4o is one of these:</p>

<div>
    <table>
        <thead>
            <tr>
                <th>Dimension</th>
                <th>Prime Factors</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>$1{,}536$</td>
                <td>$3 \cdot 2^9$</td>
            </tr>
            <tr>
                <td>$2{,}048$</td>
                <td>$2^{11}$</td>
            </tr>
            <tr>
                <td>$3{,}072$</td>
                <td>$3 \cdot 2^{10}$</td>
            </tr>
            <tr>
                <td>$4{,}096$</td>
                <td>$2^{12}$</td>
            </tr>
            <tr>
                <td>$12{,}228$</td>
                <td>$3 \cdot 2^{12}$</td>
            </tr>
            <tr>
                <td>$16{,}384$</td>
                <td>$2^{14}$</td>
            </tr>
            <tr>
                <td>$24{,}576$</td>
                <td>$3 \cdot 2^{13}$</td>
            </tr>
        </tbody>
    </table>
</div>

<p>For the sake of argument, I’ll assume that GPT-4o is using 12,228 for the
dimension of its embedding vectors. It doesn’t really matter if we’re off by a
factor of 2 or 4; the same arguments will work.</p>

<h2 id="embedding-images">Embedding Images</h2>

<p>Image tiles are square, so are likely represented by a square grid of tokens.
170 is very close to $13 \times 13$. The extra token could a single embedding vector
which encodes a kind of gestalt impression of the entire image, exactly as
CLIP does (and similar to their strategy of using an 85 token “master
thumbnail” for each image.)</p>

<p>So, the question is, how do we go from <code>512x512x3</code> to <code>13x13x12228</code>?</p>

<h3 id="strategy-1-raw-pixels">Strategy 1: Raw Pixels</h3>

<p>Here’s an extremely simple way to stuff an image into a vector space:</p>

<ol>
<li>Divide the <code>512x512</code> image into a <code>8x8</code> grid of “mini-tiles.”</li>
<li>Each mini-tile is <code>64x64x3</code>; flatten it a vector of dimension 12,228.</li>
<li>Each mini-tile is a single embedding vector.</li>
<li>The entire image tile is represented as 64 consecutive embedding vectors.</li>
</ol>

<p>There are two problems with this approach:</p>

<ol>
<li>64 ≠ 170, and</li>
<li>it’s extremely stupid.</li>
</ol>

<p>By “extremely stupid” I mean that it doesn’t make any sense to embed using raw
RGB values and then just cross your fingers and hope the transformer will sort it out.
Transformers aren’t really designed to handle the spatial structure of 2D images,
especially not when it’s embedded in such a braindead way as this.</p>

<p>To see why, imagine the image is shifted a few pixel to the left. The dot
product between the embedding vectors of the original and shifted images would
immediately drop close to zero. The same would happen if we resize the image.</p>

<p>Ideally we’d want a model that was robust to these kinds of transforms—we’d
like it to have translational and scale invariance, to use the technical jargon.</p>

<h3 id="strategy-2-cnn">Strategy 2: CNN</h3>

<p>Luckily, there already exists a model with those characteristics, with over a
decade-long track record of successfully handling image data: the
<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a>. (Here, I’m using the term to describe the
broad family of deep learning models which use convolution layers somewhere
inside them.)</p>

<p>Just to get a sense of what the options are, let’s take look at a classic CNN
architecture introduced in 2012, <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a>:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/alexnet.png"></p>

<p>The basic building blocks are:</p>

<ol>
<li>Convolution Layer. These scan over an image in $k \times k$ sized blocks,
training a small neural network.</li>
<li>Max Pool Layer. These also look at $k \times k$ block, but simply take the
maximum value from each.</li>
</ol>

<p>You should spot two key trends as we move into the deeper layers of the
network: the height and width get smaller, while the number of “channels”
(sometimes called “filters”) gets larger. That means we’re incrementally
digesting many low-level features into fewer high level concepts until, at the
very end, AlexNet has turned the entire image into a single categorical concept
representing something like a “cat” or “dog.” CNNs are essentially funnels that
squeeze the lemons of raw pixels into the lemonade of semantic vectors.</p>

<p>If you’re following my somewhat strained analogy, you should see how a CNN can
turn an image into a single embedding vector. To see how (and why) a CNN can
turn an image into many embedding vectors, let’s take a look at a slightly
newer (circa 2018) CNN architecture, one that’s a little closer in spirit to
what we’ll need for GPT-4o. It’s called <a href="https://arxiv.org/abs/1804.02767">YOLO</a>, short for “You Only Look
Once.”</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/yolo_v3.png"></p>

<p>Here, the notation “xN” means that the entire block is repeated N times. YOLOv3
is 10 times as deep as AlexNet but is still very similar in some regards. It
has a somewhat more modern design: stride 2 convolutional layers instead
of max pooling layers to reduce dimensionality, residual layers to
preserve good gradients in very deep networks, etc.</p>

<p>But the key difference is that it doesn’t reduce the image to a single flat
vector, but stops at <code>13x13</code>. There are no fully connected layers after that;
the output of YOLOv3 is in fact 169 different vectors, laid out in a <code>13x13</code>
grid, each of dimension 1,024, and each representing the class (and some
bounding box data we’ll ignore) of the object found in or near a particular
cell of the grid. This means that YOLO doesn’t see just one object in the
image—it can see many in a single pass. That’s why it’s said to “only
look once.”</p>

<p>These examples give us a rough sense of what GPT-4o’s (hypothetical) image
embedding CNN might be shaped like. All we have to do now is play a little game
of connect the dots: how do we go from <code>512x512x3</code> to <code>13x13x12228</code> using
standard CNN layers?</p>

<p>The moves in this game are the standard building blocks we’ve seen in the
above CNN architectures. We can choose the layer types and play around with
hyperparameters like kernel size, stride length, padding strategy, etc. Note
that we ignore things like residual layers, repeated blocks, batch/layer normalization,
or <code>1x1</code> convolutional layers as these don’t affect the overall tensor size.</p>

<p>The goal is to suggest a workable CNN architecture that connects the known
input size (<code>512x512</code> images with 3 RGB color channels) to the assumed output
shape (<code>13x13</code> embedding vectors with 12,228 dimensions each.)</p>

<p>I tried
<a href="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative.png" target="_blank">several</a>
<a href="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative4.png" target="_blank">different</a>
<a href="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative2.png" target="_blank">variations</a>,
but most of these required special cases on one or more layers to “fit.” Until
I found this one, which steps down elegantly with no special cases at all:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative3.png"></p>

<p>It very neat, isn’t it? It’s almost identical to AlexNet, and it steps down
from from 512 to 13 in five identical repeating blocks, while simultaneously
quadrupling the number of channels with each block to hit 12,228 on the bottom
layer. Unfortunately, it also feels a little outdated due to the <code>5x5</code> kernels
and max pool layers. AlexNet was a breakthrough in 2012 but I would be suprised
if OpenAI was using something similar in 2024.</p>

<p>Here’s an alternative that <em>almost</em> worked (got to <code>12x12</code> instead of <code>13x13</code>)
while staying closer to the more modern YOLO design:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative4.png"></p>

<p>While impossible to prove, these speculative designs demonstrate that there are
plausible CNN architectures that could represent an image as a $k \times k$
grid of embedding vectors.</p>

<h2 id="experimental-validation">Experimental Validation</h2>

<p>Does GPT-4o really see a <code>13x13</code> grid of embedding vectors? I invented
a task, loosely inspired by <a href="https://en.wikipedia.org/wiki/Zener_cards">Zener cards</a>, to test this. The task is to
identify the color and shape for every symbol on a grid in an image.</p>

<p>A <a href="https://gist.github.com/olooney/07850f0a2f0fcaac973ffabac765454a">simple program</a> generates test grids that look like this:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/zener_5x5.png"></p>

<p>I then used this prompt to obtain comparison data:</p>

<pre><code>"""Divide this image into a {k}x{k} grid and describe the shape and color of
each cell. Report your results as a JSON array of arrays (no objects or row
labels) and use all lowercase."""
</code></pre>

<p>If the <code>13x13</code> hypothesis is correct, GPT-4o should do well on this task
up to about 13 and then performance should degrade. The CNN isn’t literally
dividing the image up into a grid so it might start to degrade before then,
and might struggle more with even grid sizes than odd because the cells won’t
be aligned.</p>

<p>However, that is not in fact what happens. Instead, performance is perfect for
<code>5x5</code> grids and smaller (e.g. <a href="https://www.oranlooney.com/post/gpt-cnn_files/zener_2x2.png"><code>2x2</code></a>, <a href="https://www.oranlooney.com/post/gpt-cnn_files/zener_3x3.png"><code>3x3</code></a>, <a href="https://www.oranlooney.com/post/gpt-cnn_files/zener_4x4.png"><code>4x4</code></a>) but begins
to degrade rapidly after that. By the time we reach <code>13x13</code> it was no better
than chance.</p>

<p>For example, here is a <code>7x7</code> grid:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/zener_7x7.png"></p>

<p>For which GPT-4o returned this:</p>

<pre><code>[
    ["blue diamond", "purple triangle", "red plus", "purple circle", "orange square", "purple circle", "blue circle"],
    ["blue diamond", "red circle", "green plus", "red square", "orange diamond", "blue triangle", "purple plus"],
    ["blue diamond", "orange triangle", "red square", "orange square", "blue plus", "purple circle", "blue circle"],
    ["green diamond", "blue circle", "orange circle", "green circle", "purple diamond", "green triangle", "orange diamond"],
    ["purple square", "purple circle", "green plus", "green diamond", "green circle", "blue triangle", "purple triangle"],
    ["red plus", "red triangle", "purple circle", "blue triangle", "orange triangle", "red diamond", "orange diamond"],
    ["orange plus", "blue diamond", "green triangle", "green plus", "green triangle", "purple diamond", "purple square"]
]
</code></pre>

<p>It got <sup>38</sup>⁄<sub>49</sub> correct—an accuracy of 76%. The exact pattern of hits and
misses looks like this (yellow is correct, purple incorrect):</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/zener_7x7_results.png"></p>

<p>Performance continues to degrade as the grid size increases and by the time we
get to the <code>13x13</code> grid:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/zener_13x13.png"></p>

<p>The results are no better than chance:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/zener_13x13_results.png"></p>

<p>Does that mean I was wrong about 169 tokens representing a <code>13x13</code> grid?
Yes. Yes it does. My disappointment is immeasurable and my day is ruined.</p>

<blockquote>
<p>“The great tragedy of science: the slaying of a beautiful hypothesis by an
ugly fact.” —Thomas Huxley</p>
</blockquote>

<p>But the <code>5x5</code> grid results are suggestive. GPT-4o really can keep track of 25
distinct objects and their absolute positions within in an image. Maybe the
basic concept is right; I just got the dimension wrong. It would be easy
to tack on another couple of layers to our CNN to get down to <code>5x5</code> instead
of <code>13x13</code>:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/gpt4o_speculative5.png"></p>

<p>How could we structure the output to reach 170 tokens if we assume we only use
<code>5x5</code> grids and smaller?</p>

<h2 id="pyramid-strategy">Pyramid Strategy</h2>

<p>One way to get close to both 85 and 170 is to assume that we encode the image
in a series of increasingly granular levels, like a pyramid. We start with one
embedding vector to capture a gestalt impression of the whole image, add a
<code>3x3</code> to capture left/middle/right and top/middle/bottom, then adding a <code>5x5</code>,
<code>7x7</code> etc.</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/grids.png"></p>

<p>This strategy gets us very close to 85 tokens for the ‘master thumbnail’ if
we stop at <code>7x7</code>:</p>

<p>$1^2 + 3^2 + 5^2 + 7^2 = 1 + 9 + 25 + 49 = 84$</p>

<p>And very close to 170 if we add one final <code>9x9</code> grid:</p>

<p>$1^2 + 3^2 + 5^2 + 7^2 + 9^2 = 1 + 9 + 25 + 49 + 81 = 165$</p>

<p>If we throw in an <em>ad hoc</em> <code>2x2</code> grid for the <code>512x512</code> tile and assume one
special <code>&lt;|image start|&gt;</code> for each, we can get a perfect match:</p>

<p>$1 + 1^2 + 3^2 + 5^2 + 7^2 = 1 + 1 + 9 + 25 + 49 = 85$</p>

<p>$1 + 1^2 + 2^2 + 3^2 + 5^2 + 7^2 + 9^2 = 1 + 1 + 4 + 9 + 25 + 49 + 81 = 170$</p>

<p>This scheme lacks any sort of delimiters for the start and end of a row, but
I think that could be handled with positional encoding similar to the way
<a href="https://blog.eleuther.ai/rotary-embeddings/">RoPE</a> is used to encode position information for text tokens, but in 2D.</p>

<p>The above takes only odd grid sizes and goes well beyond <code>5x5</code>; given that the
Zener grid performance starts to fall off after <code>5x5</code> this does not entirely
concord with the evidence.</p>

<p>As an alternative, we could try taking all the grids (even and odd) up to <code>5x5</code>:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/grids2.png"></p>

<p>This approach gives us 55 tokens:</p>

<p>$1^2 + 2^2 + 3^2 + 4^2  + 5^2 = 55$</p>

<p>If we assume 3 tokens per mini-tile and a delimiter token between each, we
can get to 170:</p>

<p>$3 \times (1^2 + 2^2 + 3^2 + 4^2  + 5^2) + 5 = 170$</p>

<p>This isn’t fully satisfactory on numerological grounds but does jive well with
the empirical results. The pyramid strategy has a lot of intuitive appeal—it
feels like an almost “obvious” way to encode spatial information at
different zoom levels - and may explain why it does so well with the <code>5x5</code> grid
and below and so poorly on <code>6x6</code> and above.</p>

<p>It’s maddening that every hypothesis seems to come tantalizingly close to
explaining everything but the numbers never quite seem to work out neatly…
Still, these pyramid strategies are the best I’ve been able to come up with.</p>

<h2 id="optical-character-recognition">Optical Character Recognition</h2>

<p>The one thing that none of the above hypotheses explain is how GPT-4o is doing
OCR. YOLO and CLIP can’t natively do OCR, and the strategies suggested above
seem like they would struggle for the same reasons. I mean, if it can’t read
off 36 symbols in a neat <code>6x6</code> grid from an image, it certainly can’t read off
a several hundred text characters flawlessly.</p>

<p>State-of-the-art OCR engines like <a href="https://en.wikipedia.org/wiki/Tesseract_(software)">Tesseract</a> do a great deal of work to
find bounding boxes and strips of characters, and then run specialized
character recognition models along those strips, one character or word at a
time. They aren’t just big CNNs.</p>

<p>And yet GPT-4o patently <em>can</em> do high-quality OCR: it can transcribe long
blocks of text, read handwritten text, or text which has been shifted, rotated,
projected, or partially occluded.</p>

<p>I have a simple theory to explain that: I think OpenAI is running
<a href="https://en.wikipedia.org/wiki/Tesseract_(software)">Tesseract</a> (or their own in-house OCR) and feeding the identified text
into the transformer alongside the image data. I mean, that’s what I would do.</p>

<p>This would explain why the early versions were so easily confused by text
hidden in images: from its POV, that text <em>was</em> part of the prompt. (This is
fixed now; GPT-4o is good at ignoring malicious prompts hidden inside images.)</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/malicious_penguin.png" alt="GPT-4o passes the malicious penguin test."></p>

<p>However, this does not explain why there’s no charge per token for the text
found in an image.</p>

<p>Interestingly enough, it’s actually <em>more efficient</em> to send text as images: A
<code>512x512</code> image with a small but readable font can easily fit 400-500 tokens
worth of text, yet you’re only charged for 170 input tokens plus the 85 for the
‘master thumbnail’ for a grand total of 255 tokens—far less than the
number of words on the image.</p>

<p>This theory explains why there is additional latency when processing images.
The CNN would be essentially instantaneous, but 3rd-party OCR would add
additional time. By the way, (and I’m not saying this proves anything) but the
Python environment used by the OpenAI code interpreter has <a href="https://pypi.org/project/pytesseract/">PyTesseract</a>
installed. You can literally just ask it to run PyTesseract on any image you’ve
uploaded to get a second opinion.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Well, we’ve made a lot of speculative hay out of what is essentially only one
morsel of hard fact: that OpenAI used the magic number 170.</p>

<p>However, there does seem to be a complete plausible approach—very much in
line with other CNN architectures such as YOLO—for mapping from image tiles
to embedding vectors.</p>

<p>As such, I don’t think 170 tokens is just an approximation used to bill for
roughly the amount of compute it takes to process an image. And I don’t think
they’re concatenating layers to join image and text data the way some other
multi-modal models do.</p>

<p>No, I think GPT-4o is <em>literally</em> representing <code>512x512</code> images as 170
embedding vectors, using an CNN architecture that’s a mixture of CLIP and YOLO
to embed the image directly into the transformer’s semantic vector space.</p>

<p>When I started this article, I was entirely convinced that I had cracked it
entirely, that I was going to find that the 170 tokens were for a <code>13x13</code> grid
and one additional “gestalt impression” token. That got blown out of the water
when performance on the Zener task started to degrade after <code>5x5</code>—whatever
they’re doing internally, it seems to be a lot smaller than <code>13x13</code>.</p>

<p>Still the analogy to YOLO is compelling, and the performance on the <code>5x5</code> Zener
task all but confirms that they’re doing some kind of grid. This theory has a
lot of predictive power in other areas as well: it explains how GPT-4o is able
to handle multiple images, and tasks like comparing two images, for example. It
explains how it’s able to see multiple objects in the same image, but gets
overwhelmed when there are too many objects in a busy scene. It explains
why GPT-4o seems extremely vague about the absolute and relative positions of
separate objects within the scene, and why it can’t count objects accurately
in images: when an object spans two adjacent grid cells the same classes are
activated in both so it’s not sure if it’s one object or two.</p>

<p>Ironically, the one thing this theory can’t cleanly explain is the question
which motivated this article in the first place: why 170 tokens in particular?
The pyramid theory (<code>1x1 + 2x2 + 3x3 + 4x4 + 5x5</code>) was the best I was able to
come up with, and it’s not particularly neat.</p>

<p>I’d love to hear from anyone who has a theory that fits a little better (or
even actual knowledge, assuming it doesn’t run afoul of an NDA!)</p>

<!-- https://unsplash.com/photos/black-and-gray-camera-on-white-table-Y5dd6hLkn-8 -->

<h2 id="postscript-alpha-channel-shenanigans">Postscript: Alpha Channel Shenanigans</h2>

<p>One other thing I noticed while working on this project is that GPT-4o
<em>ignores</em> the alpha channel, resulting in somewhat counter-intuitive behavior.</p>

<p>When I say, “ignores”, I don’t mean that it gets rid of transparency by
compositing it onto some default background, the way an image editor might
when converting PNG to JPG. No, I mean it literally just grabs the RGB channels
and ignores the alpha channel.</p>

<p>We can illustrate this with four carefully prepared images. For convenience,
I’ve used HTML and CSS to display these images on top of a checkerboard
pattern—the images themselves have flat, transparent backgrounds.
However, half have transparent <em>black</em> backgrounds, and half have transparent
<em>white</em> backgrounds.</p>

<p>What do I mean by “transparent black” or “transparent white?” Well, when we
represent an RGBA color with four bytes, the RGB bytes are still there even
when alpha is 100%. Thus, <code>(0, 0, 0, 255)</code> and <code>(255, 255, 255, 255)</code> are in
some sense different colors, even though there’s no situation where a correct
renderer would display them differently since they’re both 100% transparent.</p>

<p>Let’s ask GPT-4o what it “sees” on these four images:</p>

<div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/black_on_transparent_black.png" alt="MODICUM"></p><p>Black Text on Transparent Black Background</p>
            <p>GPT-4o Reads: “”</p>
        </div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/black_on_transparent_white.png" alt="ENORMOUS"></p><p>Black Text on Transparent White Background</p>
            <p>GPT-4o Reads: “ENORMOUS”</p>
        </div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/white_on_transparent_black.png" alt="SCINTILLA"></p><p>White Text on Transparent Black Background</p>
            <p>GPT-4o Reads: “SCINTILLA”</p>
        </div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/white_on_transparent_white.png" alt="GIGANTIC"></p><p>White Text on Transparent White Background</p>
            <p>GPT-4o Reads: “”</p>
        </div>
</div>

<p>What’s going on here? The pattern that emerges is that GPT-4o can read the
text if and only if the text color is different than the “color” of the
transparent background.</p>

<p>This tells us that GPT-4o <em>disregards</em> the alpha channel and only looks at the
RGB channels. To it, transparent black is black, transparent white is white.</p>

<p>We can see this even more clearly if we mess with an image to preserve the
three RGB channels while setting the alpha channel to 100%. Here’s a little
Pillow function to do that:</p>

<pre><code>from PIL import Image

def set_alpha(image, output_path, alpha_value):
    # copy the image and ensure it's RGBA
    image = image.convert("RGBA")

    # set the alpha channel of every pixel to the given value
    pixels = image.getdata()
    new_pixels = [(r, g, b, alpha_value) for r, g, b, a in pixels]
    image.putdata(new_pixels)

    return image
</code></pre>

<p>I used that to make the two images below; they have identical RGB data, and
only differ in the alpha channel:</p>

<div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/platypus.png" alt="Visible Platypus"></p><p>Alpha Channel = 255</p>
        </div>
    <div>
            <p><img src="https://www.oranlooney.com/post/gpt-cnn_files/platypus_hidden.png" alt="Hidden Platypus"></p><p>Alpha Channel = 0</p>
        </div>
</div>

<p>GPT-4o has no trouble seeing the hidden platypus:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/chatgpt_hidden_platypus_test.png" alt="GPT-4o passes the hidden platypus test."></p>

<p>You can try downloading the
<a href="https://www.oranlooney.com/post/gpt-cnn_files/platypus_hidden.png" download=""><code>hidden_platypus.png</code></a>
image and dropping it into ChatGPT yourself; it will correctly describe it.
You may also note the image is 39.3 KB, the same size as
<a href="https://www.oranlooney.com/post/gpt-cnn_files/platypus.png" download=""><code>platypus.png</code></a>
even though PNG compression should have made it much smaller if it was really
a perfectly blank, transparent image. Or you can use the above function to
set the alpha channel back to 255, recovering the original image.</p>

<p>I’m not sure if this is bug but it’s certainly <a href="https://en.wikipedia.org/wiki/Principle_of_least_astonishment">surprising</a> behavior; in
fact, it feels like something a malicious user could use to smuggle information
past humans and directly to GPT-4o. However, GPT-4o is <em>much</em> better at
detecting and ignoring malicious prompts hidden in images than GPT-4v was:</p>

<p><img src="https://www.oranlooney.com/post/gpt-cnn_files/malicious_dogs.png"></p>

<p>(You can find other examples of GPT-4o successfully detecting and
ignoring malicious prompts hidden in images in my <a href="https://olooney.github.io/image_tagger/gallery/index.html">gallery of GPT-4o test
images</a> generated by my <a href="https://github.com/olooney/image_tagger"><code>image_tagger</code></a> utility.)</p>

<p>So, even if it is a bug, it’s not obvious it can be exploited. Still, it would
be less surprising in GPT-4o “saw” the same thing that a human would in a
browser.</p>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cancel Adobe if you are a creative under NDA with your clients (491 pts)]]></title>
            <link>https://twitter.com/Stretchedwiener/status/1798153619285708909</link>
            <guid>40607442</guid>
            <pubDate>Fri, 07 Jun 2024 11:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Stretchedwiener/status/1798153619285708909">https://twitter.com/Stretchedwiener/status/1798153619285708909</a>, See on <a href="https://news.ycombinator.com/item?id=40607442">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Ice - open source bar manager for macOS (435 pts)]]></title>
            <link>https://github.com/jordanbaird/Ice</link>
            <guid>40605532</guid>
            <pubDate>Fri, 07 Jun 2024 05:11:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jordanbaird/Ice">https://github.com/jordanbaird/Ice</a>, See on <a href="https://news.ycombinator.com/item?id=40605532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/jordanbaird/Ice/blob/main/Ice/Assets.xcassets/AppIcon.appiconset/icon_256x256.png"><img src="https://github.com/jordanbaird/Ice/raw/main/Ice/Assets.xcassets/AppIcon.appiconset/icon_256x256.png" width="200" height="200"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">Ice</h2><a id="user-content-ice" aria-label="Permalink: Ice" href="#ice"></a></p>
</div>
<p dir="auto">Ice is a powerful menu bar management tool. While its primary function is hiding and showing menu bar items, it aims to cover a wide variety of additional features to make it one of the most versatile menu bar tools available.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/337269729-e256575b-880c-49ee-90a1-b6eabd4a7868.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMzcyNjk3MjktZTI1NjU3NWItODgwYy00OWVlLTkwYTEtYjZlYWJkNGE3ODY4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiN2Q5NGYwOTVkODFmNjY2MjhmOWUzY2FkNjc2MTJkMjJiY2UxZjljMjIwM2E0NGJiMzg2MDc3Yzk1NTYzZGImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZAQaQeWfoC14wJuXpebfc2vSdt0FCoyRe0YbnYtszDU"><img src="https://private-user-images.githubusercontent.com/90936861/337269729-e256575b-880c-49ee-90a1-b6eabd4a7868.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMzcyNjk3MjktZTI1NjU3NWItODgwYy00OWVlLTkwYTEtYjZlYWJkNGE3ODY4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiN2Q5NGYwOTVkODFmNjY2MjhmOWUzY2FkNjc2MTJkMjJiY2UxZjljMjIwM2E0NGJiMzg2MDc3Yzk1NTYzZGImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZAQaQeWfoC14wJuXpebfc2vSdt0FCoyRe0YbnYtszDU" alt="Banner"></a></p>
<p dir="auto"><a href="https://github.com/jordanbaird/Ice/releases/latest"><img src="https://camo.githubusercontent.com/01a1e5eace1f29146439e52ca905bb1944101aa8e1439ad28016634ab09d4cf2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f776e6c6f61642d6c61746573742d627269676874677265656e3f7374796c653d666c61742d737175617265" alt="Download" data-canonical-src="https://img.shields.io/badge/download-latest-brightgreen?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d2da2d8531376e348989bf9b38fc124d66edb8d942c7b5124d2f05648d5f6531/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532d626c75653f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/d2da2d8531376e348989bf9b38fc124d66edb8d942c7b5124d2f05648d5f6531/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532d626c75653f7374796c653d666c61742d737175617265" alt="Platform" data-canonical-src="https://img.shields.io/badge/platform-macOS-blue?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6fa7e0d369a9b73e179ef17bef802dc7f20905b64989f2b74028c9727936970/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726571756972656d656e74732d6d61634f5325323031342532422d6661346534393f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/e6fa7e0d369a9b73e179ef17bef802dc7f20905b64989f2b74028c9727936970/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726571756972656d656e74732d6d61634f5325323031342532422d6661346534393f7374796c653d666c61742d737175617265" alt="Requirements" data-canonical-src="https://img.shields.io/badge/requirements-macOS%2014%2B-fa4e49?style=flat-square"></a>
<a href="https://jordanbaird.gumroad.com/l/ice" rel="nofollow"><img src="https://camo.githubusercontent.com/9bd6b0db10c755135ba6f7fa7e3f6771510682e448650ee11a55b98b9a1cac24/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f537570706f72742532302545322539442541342545462542382538462d3841324245323f7374796c653d666c61742d737175617265" alt="Support" data-canonical-src="https://img.shields.io/badge/Support%20%E2%9D%A4%EF%B8%8F-8A2BE2?style=flat-square"></a>
<a href="https://icemenubar.app/" rel="nofollow"><img src="https://camo.githubusercontent.com/cd06400f307d1a54e0c33e65a7d4541a059345a59cc24a0f809914b5025fc38a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f576562736974652d3031354642413f7374796c653d666c61742d737175617265" alt="Website" data-canonical-src="https://img.shields.io/badge/Website-015FBA?style=flat-square"></a>
<a href="https://github.com/jordanbaird/Ice/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/07b911e5dcbb76995245bb90ea5b4153f7c2f2f2ac8b688c0f4514da4e7f978a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f7264616e62616972642f4963653f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/jordanbaird/Ice?style=flat-square"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Ice is currently in active development. Some features have not yet been implemented. Download the latest release <a href="https://github.com/jordanbaird/Ice/releases/latest">here</a> and see the roadmap below for upcoming features.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Simply <code>Command</code> + drag your menu bar items to rearrange them.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jordanbaird/Ice/blob/main/Resources/rearranging.gif"><img src="https://github.com/jordanbaird/Ice/raw/main/Resources/rearranging.gif" alt="Rearranging" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Installation</h3><a id="user-content-manual-installation" aria-label="Permalink: Manual Installation" href="#manual-installation"></a></p>
<p dir="auto">Download the "Ice.zip" file from the <a href="https://github.com/jordanbaird/Ice/releases/latest">latest release</a> and move the unzipped app into your <code>Applications</code> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<p dir="auto">If you use <a href="https://brew.sh/" rel="nofollow">Homebrew</a>, you can install Ice using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install jordanbaird-ice"><pre>brew install jordanbaird-ice</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features/Roadmap</h2><a id="user-content-featuresroadmap" aria-label="Permalink: Features/Roadmap" href="#featuresroadmap"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Menu bar item management</h3><a id="user-content-menu-bar-item-management" aria-label="Permalink: Menu bar item management" href="#menu-bar-item-management"></a></p>
<ul>
<li> Hide menu bar items</li>
<li> "Always Hidden" menu bar section</li>
<li> Show hidden items when hovering over the menu bar</li>
<li> Show hidden items when an empty area in the menu bar is clicked</li>
<li> Show hidden items by scrolling or swiping in the menu bar</li>
<li> Automatically rehide</li>
<li> Hide application menus when they overlap with shown menu bar items</li>
<li> Drag and drop interface to arrange individual items</li>
<li> Search menu bar items</li>
<li> Display hidden items in a separate bar (e.g. for MacBooks with the notch)</li>
<li> Custom spacing between items</li>
<li> Profiles for menu bar layout</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Menu bar appearance</h3><a id="user-content-menu-bar-appearance" aria-label="Permalink: Menu bar appearance" href="#menu-bar-appearance"></a></p>
<ul>
<li> Menu bar tint (solid and gradient)</li>
<li> Menu bar shadow</li>
<li> Menu bar border</li>
<li> Custom menu bar shapes (rounded and/or split)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hotkeys</h3><a id="user-content-hotkeys" aria-label="Permalink: Hotkeys" href="#hotkeys"></a></p>
<ul>
<li> Toggle individual menu bar sections</li>
<li> Toggle application menus</li>
<li> Show/hide section divider icons</li>
<li> Temporarily show individual menu bar items</li>
<li> Enable/disable auto rehide</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other</h3><a id="user-content-other" aria-label="Permalink: Other" href="#other"></a></p>
<ul>
<li> Launch at login</li>
<li> Automatic updates</li>
<li> Menu bar widgets</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why the name?</h2><a id="user-content-why-the-name" aria-label="Permalink: Why the name?" href="#why-the-name"></a></p>
<p dir="auto">Because your menu bar becomes like ice, allowing your menu bar items to slide away 🧊🧊🧊</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why not support earlier operating systems?</h2><a id="user-content-why-not-support-earlier-operating-systems" aria-label="Permalink: Why not support earlier operating systems?" href="#why-not-support-earlier-operating-systems"></a></p>
<p dir="auto">There are a number of system APIs that Ice uses that are only available starting with macOS 14.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Gallery</h2><a id="user-content-gallery" aria-label="Permalink: Gallery" href="#gallery"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Choose your own icon to show in the menu bar</h4><a id="user-content-choose-your-own-icon-to-show-in-the-menu-bar" aria-label="Permalink: Choose your own icon to show in the menu bar" href="#choose-your-own-icon-to-show-in-the-menu-bar"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316565649-ded03af0-f6cf-4bc1-a094-af0cf32ead96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjU2NDktZGVkMDNhZjAtZjZjZi00YmMxLWEwOTQtYWYwY2YzMmVhZDk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyYjBmNDc2NzY4ZDA4MDkzODk0YzMwMGY0NzY3ODJmNGEyZjcxZDUyMzI1MzM2ODg3NjczZjUyNzYzYjEzNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OdzlaKbWN6tW8FHldIAzabXMTfl8l3ujByE9iSkkms0"><img src="https://private-user-images.githubusercontent.com/90936861/316565649-ded03af0-f6cf-4bc1-a094-af0cf32ead96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjU2NDktZGVkMDNhZjAtZjZjZi00YmMxLWEwOTQtYWYwY2YzMmVhZDk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyYjBmNDc2NzY4ZDA4MDkzODk0YzMwMGY0NzY3ODJmNGEyZjcxZDUyMzI1MzM2ODg3NjczZjUyNzYzYjEzNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OdzlaKbWN6tW8FHldIAzabXMTfl8l3ujByE9iSkkms0" alt="Custom Icon Example"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Customize the menu bar's appearance</h4><a id="user-content-customize-the-menu-bars-appearance" aria-label="Permalink: Customize the menu bar's appearance" href="#customize-the-menu-bars-appearance"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316568231-99a59cc7-914a-4f8a-af4b-11a5193534da.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjgyMzEtOTlhNTljYzctOTE0YS00ZjhhLWFmNGItMTFhNTE5MzUzNGRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0YTMzY2Q3MDY5NTdiYWE3OWNkY2ViYjI0ZDMzODlkZTI5MDU3OWFiZDI3NmFhZThhNGZhNjI2Y2NlZmMyMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.15HYZuDRgDllKmNA3U944pqxgRskhkMERX1c0EePsmk"><img src="https://private-user-images.githubusercontent.com/90936861/316568231-99a59cc7-914a-4f8a-af4b-11a5193534da.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjgyMzEtOTlhNTljYzctOTE0YS00ZjhhLWFmNGItMTFhNTE5MzUzNGRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0YTMzY2Q3MDY5NTdiYWE3OWNkY2ViYjI0ZDMzODlkZTI5MDU3OWFiZDI3NmFhZThhNGZhNjI2Y2NlZmMyMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.15HYZuDRgDllKmNA3U944pqxgRskhkMERX1c0EePsmk" alt="Menu Bar Appearance Example 1"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316570684-ba3bd8d9-0eeb-447b-8a62-1bc811319132.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NzA2ODQtYmEzYmQ4ZDktMGVlYi00NDdiLThhNjItMWJjODExMzE5MTMyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk0ZGEzYzQ3NWJmMjcwMjcwMmVkMzE2Mjc0YTg3ODUzZGU3NDYxOGJlZTNkMWQwMjdiYTEyNTJmYzM3MGZjZDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8hYg4P60AFYj_4Oub82t_J7hClwIjWHijaofXPOqXbA"><img src="https://private-user-images.githubusercontent.com/90936861/316570684-ba3bd8d9-0eeb-447b-8a62-1bc811319132.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NzA2ODQtYmEzYmQ4ZDktMGVlYi00NDdiLThhNjItMWJjODExMzE5MTMyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk0ZGEzYzQ3NWJmMjcwMjcwMmVkMzE2Mjc0YTg3ODUzZGU3NDYxOGJlZTNkMWQwMjdiYTEyNTJmYzM3MGZjZDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8hYg4P60AFYj_4Oub82t_J7hClwIjWHijaofXPOqXbA" alt="Menu Bar Appearance Example 2"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Ice is available under the <a href="https://github.com/jordanbaird/Ice/blob/main/LICENSE">MIT license</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CPU Energy Meter: A tool for measuring energy consumption of Intel CPUs (136 pts)]]></title>
            <link>https://github.com/sosy-lab/cpu-energy-meter</link>
            <guid>40604596</guid>
            <pubDate>Fri, 07 Jun 2024 01:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sosy-lab/cpu-energy-meter">https://github.com/sosy-lab/cpu-energy-meter</a>, See on <a href="https://news.ycombinator.com/item?id=40604596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">CPU Energy Meter</h2><a id="user-content-cpu-energy-meter" aria-label="Permalink: CPU Energy Meter" href="#cpu-energy-meter"></a></p>
<p dir="auto"><a href="https://gitlab.com/sosy-lab/software/cpu-energy-meter/pipelines" rel="nofollow"><img src="https://camo.githubusercontent.com/3b2e2e44e005c60ee6d71f8c6809f83c193ed224c1d0b42bb2223967c4564a6b/68747470733a2f2f6769746c61622e636f6d2f736f73792d6c61622f736f6674776172652f6370752d656e657267792d6d657465722f6261646765732f6d61696e2f706970656c696e652e737667" alt="Build Status" data-canonical-src="https://gitlab.com/sosy-lab/software/cpu-energy-meter/badges/main/pipeline.svg"></a>
<a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/1aaf5e71abd8dbe565cc0ed7ba58a18597fa5a29ac1f05fc2008cf6006b39a2f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d636c617573652d627269676874677265656e2e737667" alt="BSD-3-Clause License" data-canonical-src="https://img.shields.io/badge/license-BSD--3--clause-brightgreen.svg"></a>
<a href="https://github.com/sosy-lab/cpu-energy-meter/releases"><img src="https://camo.githubusercontent.com/dd46eab6b8c7c31d612c1a172019367238c4833663dc6d26dcbfb1d715b93742/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f736f73792d6c61622f6370752d656e657267792d6d657465722e737667" alt="Releases" data-canonical-src="https://img.shields.io/github/release/sosy-lab/cpu-energy-meter.svg"></a>
<a href="https://zenodo.org/badge/latestdoi/46493895" rel="nofollow"><img src="https://camo.githubusercontent.com/4b9e215a2babde7837ff8b29e2f952dffc23d9f1e276e5a581f87a5bdd7d1b56/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f34363439333839352e737667" alt="DOI via Zenodo" data-canonical-src="https://zenodo.org/badge/46493895.svg"></a></p>
<p dir="auto">CPU Energy Meter is a Linux tool that allows to monitor power consumption of Intel CPUs
at fine time granularities (few tens of milliseconds).
Power monitoring is available for the following power domains:</p>
<ul dir="auto">
<li>per package domain (CPU socket)</li>
<li>per core domain (all the CPU cores on a package)</li>
<li>per uncore domain (uncore components, e.g., integrated graphics on client CPUs)</li>
<li>per memory node (memory local to a package, server CPUs only)</li>
<li>per platform (all devices in the platform that receive power from integrated
power delivery mechanism, e.g., processor cores, SOC, memory, add-on or
peripheral devices)</li>
</ul>
<p dir="auto">To do this, the tool uses a feature of Intel CPUs that is called <a href="https://en.wikipedia.org/wiki/Running_average_power_limit" rel="nofollow">RAPL (Running Average Power Limit)</a>,
which is documented in the <a href="https://software.intel.com/en-us/articles/intel-sdm" rel="nofollow">Intel Software Developers Manual</a>, Volume 3B Chapter 14.9.
RAPL is available on CPUs from the generation <a href="https://en.wikipedia.org/wiki/Sandy_Bridge" rel="nofollow">Sandy Bridge</a> and later.
Because CPU Energy Meter uses the maximal possible measurement interval
(depending on the hardware this is between a few minutes and an hour),
it causes negligible overhead.</p>
<p dir="auto">CPU Energy Meter is a fork of the <a href="https://software.intel.com/en-us/articles/intel-power-gadget-20" rel="nofollow">Intel Power Gadget</a>
and developed at the <a href="https://www.sosy-lab.org/" rel="nofollow">Software Systems Lab</a>
of the <a href="https://www.uni-muenchen.de/" rel="nofollow">Ludwig-Maximilians-Universität München (LMU Munich)</a>
under the <a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/LICENSE">BSD-3-Clause License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">For Debian or Ubuntu the easiest way is to install from our <a href="https://launchpad.net/~sosy-lab/+archive/ubuntu/benchmarking" rel="nofollow">PPA</a>:</p>
<div data-snippet-clipboard-copy-content="sudo add-apt-repository ppa:sosy-lab/benchmarking
sudo apt install cpu-energy-meter"><pre><code>sudo add-apt-repository ppa:sosy-lab/benchmarking
sudo apt install cpu-energy-meter
</code></pre></div>
<p dir="auto">Alternatively, you can download our <code>.deb</code> package from <a href="https://github.com/sosy-lab/cpu-energy-meter/releases">GitHub</a>
and install it with <code>apt install ./cpu-energy-meter*.deb</code>.</p>
<p dir="auto">Dependencies of CPU Energy Meter are <a href="https://sites.google.com/site/fullycapable/" rel="nofollow">libcap</a>,
which is available on most Linux distributions in package <code>libcap</code> (e.g., Fedora)
or <code>libcap2</code> (e.g, Debian and Ubuntu: <code>sudo apt install libcap2</code>),
and a Linux kernel with the MSR module (available by default)</p>
<p dir="auto">Alternatively, for running CPU Energy Meter from source (quick and dirty):</p>
<div data-snippet-clipboard-copy-content="sudo apt install libcap-dev
sudo modprobe msr
make
sudo ./cpu-energy-meter"><pre><code>sudo apt install libcap-dev
sudo modprobe msr
make
sudo ./cpu-energy-meter
</code></pre></div>
<p dir="auto">It is also possible (and recommended) to run CPU Energy Meter without root.
To do so, the following needs to be done:</p>
<ul dir="auto">
<li>Load kernel modules <code>msr</code> and <code>cpuid</code>.</li>
<li>Add a group <code>msr</code>.</li>
<li>Add a Udev rule that grants access to <code>/dev/cpu/*/msr</code> to group <code>msr</code> (<a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/debian/additional_files/59-msr.rules">example</a>).</li>
<li>Run <code>chgrp msr</code>, <code>chmod 2711</code>, and <code>setcap cap_sys_rawio=ep</code> on the binary (<code>make setup</code> is a shortcut for this).</li>
</ul>
<p dir="auto">The provided Debian package in our <a href="https://launchpad.net/~sosy-lab/+archive/ubuntu/benchmarking" rel="nofollow">PPA</a>
and on <a href="https://github.com/sosy-lab/cpu-energy-meter/releases">GitHub</a> does these steps automatically
and lets all users execute CPU Energy Meter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to use it</h2><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it" href="#how-to-use-it"></a></p>
<div data-snippet-clipboard-copy-content="cpu-energy-meter [-d] [-e sampling_delay_ms] [-r]"><pre><code>cpu-energy-meter [-d] [-e sampling_delay_ms] [-r]
</code></pre></div>
<p dir="auto">The tool will continue counting the cumulative energy use of all supported CPUs
in the background and will report a key-value list of its measurements when it
receives SIGINT (Ctrl+C):</p>
<div data-snippet-clipboard-copy-content="+--------------------------------------+
| CPU-Energy-Meter            Socket 0 |
+--------------------------------------+
Duration                  2.504502 sec
Package                   3.769287 Joule
Core                      0.317749 Joule
Uncore                    0.010132 Joule
DRAM                      0.727783 Joule
PSYS                     29.792603 Joule"><pre><code>+--------------------------------------+
| CPU-Energy-Meter            Socket 0 |
+--------------------------------------+
Duration                  2.504502 sec
Package                   3.769287 Joule
Core                      0.317749 Joule
Uncore                    0.010132 Joule
DRAM                      0.727783 Joule
PSYS                     29.792603 Joule
</code></pre></div>
<p dir="auto">To get intermediate measurements, send signal <code>USR1</code> to the process.</p>
<p dir="auto">Optionally, the tool can be executed with parameter <code>-r</code>
to print the output as a raw (easily parsable) list:</p>
<div data-snippet-clipboard-copy-content="cpu_count=1
duration_seconds=3.241504
cpu0_package_joules=4.971924
cpu0_core_joules=0.461182
cpu0_uncore_joules=0.053406
cpu0_dram_joules=0.953979
cpu0_psys_joules=38.904785"><pre><code>cpu_count=1
duration_seconds=3.241504
cpu0_package_joules=4.971924
cpu0_core_joules=0.461182
cpu0_uncore_joules=0.053406
cpu0_dram_joules=0.953979
cpu0_psys_joules=38.904785
</code></pre></div>
<p dir="auto">The parameter <code>-d</code> adds debug output.
By default, CPU Energy Meter computes the necessary measurement interval automatically,
this can be overridden with the parameter <code>-e</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uganda's surveillance state is built on national ID cards (140 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system</link>
            <guid>40603692</guid>
            <pubDate>Thu, 06 Jun 2024 23:36:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system">https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system</a>, See on <a href="https://news.ycombinator.com/item?id=40603692">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make your program slower with threads (2014) (140 pts)]]></title>
            <link>https://brooker.co.za/blog/2014/12/06/random.html</link>
            <guid>40603625</guid>
            <pubDate>Thu, 06 Jun 2024 23:28:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2014/12/06/random.html">https://brooker.co.za/blog/2014/12/06/random.html</a>, See on <a href="https://news.ycombinator.com/item?id=40603625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>How much do context switches matter?</p>

<p>Years ago, while taking a numerical methods course, I wrote some code to calculate the expected number of shared birthdays in a group. The code is very simple: each attempt constructs a vector of N birthdays, then counts the duplicates. The outer loop runs millions of attempts, and calculates the mean number of shared birthdays across all the samples. It’s little more than a tight loop around a pseudo-random number generator.</p>

<p>I was also learning about threading at the time, and decided that I could speed up my program by running it on the lab’s shiny dual-core machine. I knew that communicating between threads was expensive, so I had each of my threads calculate their attempts in parallel, and merge the results right at the end. I was expecting a great speedup. Much to my disappointment, though, the multi-threaded version was slower. Much, much, slower.</p>

<p>Much like the <a href="http://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a> runs counter to our intuition about statistics, the behavior of bad multi-threaded programs runs counter to our intuition about computer performance. We’re used to computers being much faster than they used to be, and single-threaded efficiency mattering less than it used to in most cases. Counter to that intuition, the gap between <em>good</em> and <em>bad</em> multithreaded programs has gotten worse over time.</p>

<p>To illustrate just how bad it can be, I replicated my program from back then. It’s not much more than a multi-threaded tight loop around <em>random(3)</em>. It’s nice and quick single-threaded: running 10 million attempts in under 7 seconds. Going up to two threads makes it a bit faster, down to less than 6 seconds. When we hit three threads (on my four core Haswell E3-1240), it all goes horribly wrong:</p>

<p><img src="https://s3.amazonaws.com/mbrooker-blog-images/threads_bar.png" alt=""></p>

<p>To figure out what’s wrong, we can turn to Linux’s excellent <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf</a> tool. Running the 1-thread and 4-thread versions with <em>perf stat</em> make it obvious that something’s going on. For 1 thread:</p>

<div><pre><code>     3,788,352 L1-dcache-load-misses #0.03% of all L1-dcache hits
43,399,424,441 instructions  #1.46  insns per cycle
           734 context-switches
</code></pre></div>

<p>and for four threads:</p>

<div><pre><code>  4,110,904,396 L1-dcache-load-misses #6.88% of all L1-dcache hits
248,853,610,160 instructions # 0.51  insns per cycle
     15,993,647 context-switches
</code></pre></div>

<p>Two things are going wrong here. One is that we’re seeing a more L1 cache misses with more threads, but the bigger issue is that we’re seeing <em>a whole lot more</em> context switches. The effect of both of these is visible in the much lower <em>instructions per cycle</em> of the second version. There’s no nice constant for the cost of a context switch, but a good modern estimate is around 3μs. Multiplying 3μs by 16 million context switches gives 48 seconds, which is a good hint that we’re headed in the right direction. So, what’s causing the context switches?</p>

<p>Back to <em>perf</em>, this time running <em>perf record</em> on the processes, followed by <em>perf report</em>. First, the top few rows for the single-threaded version:</p>

<div><pre><code># Overhead   Command  Shared Object    Symbol
# ........  ........  ..............   ........................
62.01%  birthday  libc-2.19.so         [.] msort_with_tmp.part.0
11.40%  birthday  libc-2.19.so         [.] __memcpy_sse2        
10.19%  birthday  birthday             [.] simulate
</code></pre></div>

<p>We’re spending 62% of the time sorting the array, which is used to find the duplicates. That’s about what I would have guessed. What about the version with four threads?</p>

<div><pre><code># Overhead   Command  Shared Object  Symbol
# ........  ........  .............  ............
46.80%  birthday  [kernel.kallsyms]  [k] _raw_spin_lock   
 8.86%  birthday  libc-2.19.so       [.] __random           
 3.42%  birthday  libc-2.19.so       [.] __lll_lock_wait_private
 3.23%  birthday  [kernel.kallsyms]  [k] try_to_wake_up       
 2.95%  birthday  libc-2.19.so       [.] __random_r        
 2.79%  birthday  libc-2.19.so       [.] msort_with_tmp.part.0
 2.10%  birthday  [kernel.kallsyms]  [k] futex_wake 
 1.46%  birthday  [kernel.kallsyms]  [k] system_call  
 1.35%  birthday  [kernel.kallsyms]  [k] get_futex_value_locked 
 1.15%  birthday  [kernel.kallsyms]  [k] futex_wait_setup  
 1.14%  birthday  [kernel.kallsyms]  [k] futex_wait 
</code></pre></div>

<p>Well, that’s suspicious. There aren’t any locks in my code, but there are a whole lot of references to locks in the trace. <em>raw_spin_lock</em> is obviously a candidate, and it’s suspicious to see so many <a href="http://en.wikipedia.org/wiki/Futex">futex</a>-related calls. Something’s taking locks, and the fact that <em>random</em> is near the top of the list makes it a likely candidate. Before we dive in there, though, let’s confirm that we’re doing a lot of syscalls:</p>

<div><pre><code>sudo perf stat -e 'syscalls:sys_e*' ./birthday
</code></pre></div>

<p>Which spits out a long list of system calls, most (like <em>mmap</em>) with just a handful of hits. There are two huge outliers:</p>

<div><pre><code>46,889,267 syscalls:sys_enter_futex
46,889,267 syscalls:sys_exit_futex
</code></pre></div>

<p>That confirms it, something’s taking a lot of futexes. Knowing whether it’s <em>random</em> or not requires a dive into the <em>glibc</em> source, which nearly instantly <a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/random.c;h=c75d1d96adecf5ac894ca752a4c54647014bd746;hb=9752c3cdbce2b3b8338abf09c8b9dd9e78908b8a#l194">reveals something suspicious</a>:</p>

<div><pre><code> /* POSIX.1c requires that there is mutual exclusion for the `rand' and
  `srand' functions to prevent concurrent calls from modifying common
   data.  */
</code></pre></div>

<p>And, <a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/random.c;h=c75d1d96adecf5ac894ca752a4c54647014bd746;hb=9752c3cdbce2b3b8338abf09c8b9dd9e78908b8a#l292">just a little bit further down</a>:</p>

<div><pre><code> __libc_lock_lock (lock);
 (void) __random_r (&amp;unsafe_state, &amp;retval);
 __libc_lock_unlock (lock);
</code></pre></div>

<p>Getting rid of the locks means getting rid of one of two things: shared state, or the necessity to prevent concurrent modification to that state. It seems like the former is easier: reasoning about a data-race-safe PRNG is tricky. There are a many good ways to get rid of shared state in the PRNG. Linux has one particularly convenient way: the C library exposes a reentrant random number generator called <a href="http://man7.org/linux/man-pages/man3/random_r.3.html">random_r</a> (which is used by <em>random</em>, as you can see from the snippet above). Dropping <em>random_r</em> in place of <em>random</em> has an amazing effect:</p>

<p><img src="https://s3.amazonaws.com/mbrooker-blog-images/threads_bar_second.png" alt=""></p>

<p>As expected, the context switches are way down and instructions per cycle is nicely improved:</p>

<div><pre><code>     4,166,540 L1-dcache-load-misses  # 0.04% of all L1-dcache hits
40,201,461,769 instructions # 1.43  insns per cycle
           572 context-switches
</code></pre></div>

<p>I recognize that spinning on a tight loop on <em>random</em> is a contrived example, but it’s not too far away from reality. Many programs that multi-thread for performance end up with library or system calls inside relatively tight loops. Our intuition about these things tends to follow <a href="http://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a>. At worst, it’s tempting to think, these things count as a non-parallel portion of code and lower the maximum achievable parallel speedup. In the real world, though, that’s not the case. Multi-threaded programs can, and very often do, run much more slowly than the equivalent single-threaded program.</p>

<p>It’s just another thing that makes writing multi-threaded code difficult.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PSA: How to keep using adblockers on Chrome and Chromium (121 pts)]]></title>
            <link>https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376</link>
            <guid>40602886</guid>
            <pubDate>Thu, 06 Jun 2024 22:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376">https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376</a>, See on <a href="https://news.ycombinator.com/item?id=40602886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-manifest-v2-chrome-md">
    <article itemprop="text">
<ol dir="auto">
<li>google's manifest v3 has no analouge to the <code>webRequestBlocking</code> API, which is neccesary for (effective) adblockers to work</li>
<li>starting in chrome version 127, the transition to mv3 will start cutting off the use of mv2 extensions alltogether</li>
<li>this will inevitably piss of enterprises when their extensions don't work, so the <code>ExtensionManifestV2Availability</code> key was added and will presumably stay forever after enterprises complain enough</li>
</ol>
<p dir="auto">You can use this as a regular user, which will let you keep your mv2 extensions even after they're supposed to stop working</p>
<p dir="auto"><h2 dir="auto">Linux</h2><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto">In a terminal, run:</p>
<div dir="auto"><pre>sudo mkdir -p /etc/opt/chrome/policies/managed /etc/opt/chromium/policies/managed
<span>echo</span> <span><span>'</span>{ "ExtensionManifestV2Availability": 2 }<span>'</span></span> <span>|</span> sudo tee /etc/opt/chrome/policies/managed/policy.json /etc/opt/chromium/policies/managed/policy.json</pre></div>
<p dir="auto"><h2 dir="auto">ChromeOS</h2><a id="user-content-chromeos" aria-label="Permalink: ChromeOS" href="#chromeos"></a></p>
<ul dir="auto">
<li>enable developer mode</li>
<li>upon rebooting, go into vt2 (shift+ctrl+right arrow function key)</li>
<li>log in as root</li>
<li>type in <code>/usr/libexec/debugd/helpers/dev_features_rootfs_verification &amp;&amp; reboot</code></li>
<li>upon rebooting, go into vt2 again and log in as root</li>
<li>run the commands from the linux section</li>
</ul>
<p dir="auto"><h2 dir="auto">Windows</h2><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Open regedit, and create <code>Software\Policies\Google\Chrome\ExtensionManifestV2Availability</code> as a dword set to <code>0x00000002</code></p>
<p dir="auto"><h2 dir="auto">MacOS</h2><a id="user-content-macos" aria-label="Permalink: MacOS" href="#macos"></a></p>
<p dir="auto">In a terminal, run:
<code>defaults write com.google.Chrome ExtensionManifestV2Availability 2</code></p>
<p dir="auto">(note that i haven't tested this for mac. please let me know if it doesn't work)</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The problem with lying is keeping track of all the lies (101 pts)]]></title>
            <link>https://materialize.com/blog/engineering/keeping-track-lies/</link>
            <guid>40602603</guid>
            <pubDate>Thu, 06 Jun 2024 21:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://materialize.com/blog/engineering/keeping-track-lies/">https://materialize.com/blog/engineering/keeping-track-lies/</a>, See on <a href="https://news.ycombinator.com/item?id=40602603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div>
				<p><img src="https://res.cloudinary.com/mzimgcdn/image/upload/v1673447206/recursion-in-materialize.png" alt="The Problem with Lying is Keeping Track of All the Lies"></p>
				
				<p>Or why clear consistency guarantees are how to stay sane when programming distributed systems.</p>
				</div>
			<p><img src="https://res.cloudinary.com/mzimgcdn/image/upload/v1673447206/recursion-in-materialize.png" alt="The Problem with Lying is Keeping Track of All the Lies"></p></div><div data-sb-field-path=".markdown_content" data-theme="light"><p>“The real difficulty with lying is that you have to keep track of all the lies that you’ve told, and to whom” is a quote I once read that I can’t definitively source (it’s… inconsistently attributed to Mark Twain). It’s stuck with me because it captures the logic as to why it’s so hard to be productive as a programmer in a world of weak isolation models.</p>
<p><em>[Author’s note: database communities use the term “isolation,” and distributed systems communities use the term “strong consistency,” to refer to overlapping concepts. In the rest of this post, I will stick to the database terminology because this is all their fault in the first place.]</em></p>
<h2 id="anomalies">(Anoma)lies</h2>
<p>If you lie to someone, you have to remember all the other things you’ve told to everyone else, and game out who might be talking to whom. Then, you have to reason about how you could get caught. This slows down your thinking and mental agility. Similarly, if you work with data platforms that do <em>not</em> provide strong isolation, you have to carefully consider how this might lead to error states or end-user visible inconsistencies. You’re potentially telling “lies,” and you need to keep track of them.</p>
<p>This slows down your development velocity. Most of your time is spent reasoning about architecture diagrams. You might be giving an inconsistent read to an unsuspecting client. You have to keep track of what services are not communicating through the database. I concede that the “lie” metaphor might be provocative, but it’s a good approximation for what an “anomaly” is in practice. And lying is a solid framework for understanding the concept of database consistency.</p>
<p>Some databases with weak isolation are correctly documented, because they promise nearly nothing, and deliver on this minimal promise. That’s not a lie (“I didn’t <em>say</em> I was going to check, you read into it…”). But in practice, this is misleading for developers. At the very least, it slows them down. As I’ll show later, even the most sophisticated database programmers often get contorted by the subtleties of weak isolation models.</p>
<p>Fundamentally, programming atop weak isolation demands a significant amount of work from developers. The case for building atop strong isolation is this: it enables local reasoning. The other dubiously sourced Mark Twain quote is “If you tell the truth, you don’t have to remember anything”. Databases with strong isolation are almost like oracles. They tell the truth all the time.</p>
<p>Translated to distributed systems, in this context you can interpret “isolation” quite literally: it allows programmers of a single query to reason about that query in isolation. On the other hand, weak isolation requires global reasoning, which means that every programmer writing queries against the system must be on the same page at all times.</p>
<p>When you give an inconsistent read, <a href="https://buttondown.email/jaffray/archive/tpc-see/" target="_blank" rel="external">whether you will get away with it</a> depends on which reads might end up conflicting downstream between other systems. And in a world where the database is accessed by multiple clients, you always need to reason about how they interact further downstream. Any errors will propagate outwards. This means that for any code change, the user has to consider the context of all the other queries that might hit the database.</p>
<h2 id="a-precedence-graphs-of-anomalies">A precedence graphs of (anoma)lies</h2>
<p>Let’s model this formally. One strong isolation level is serializability, which can be achieved in two different ways. First, you can use a database that guarantees serializability. Or second, you can take all the queries that could run on the database, and construct a directed <a href="https://en.wikipedia.org/wiki/Precedence_graph" target="_blank" rel="external">precedence graph</a>, then check this graph for cycles.</p>
<p>This is a nice definition. You can have a set of queries that are conflict-serializable even if they run on a system that provides weaker guarantees. This is because they are cleverly designed not to interfere! A database that provides serializable isolation ensures that no transactions could ever cause a cycle.</p>
<p>But if you have a database that only provides snapshot isolation, it won’t catch one particular shape of cycle, called write skew. You can still ensure that the end result has no anomalies by manually inspecting the set of transactions you run. But this checking process is hard!</p>
<p>In practice, few people are actually doing this with great success (let alone using <a href="https://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2009/Papers/p492-fekete.pdf" target="_blank" rel="external">the formal algorithm</a>). But given that Oracle only provides snapshot isolation (unhelpfully called “serializable” for historical reasons), there’s plenty of lore around what to be careful of when looking at the set of transactions. On this topic, consult your local Oracle DBA for more information.</p>
<p>As database guarantees get weaker than snapshot isolation, you have a wider set of anomalies that could potentially occur. That leads to even more hard-to-catch shapes in the precedence graph. This requires a wider set of checks that consider the complete set of all possible transactions running against a database. If your database is running in read committed mode (the postgres default), you have to ensure that it doesn’t allow phantom reads, lost updates, or unrepeatable reads, which is difficult<sup><a href="#fn1" id="ref1">1</a>.</sup></p>
<h2 id="honesty-is-often-the-best-policy">Honesty is often the best policy</h2>
<p>Does this line up with all the checks you’re running across your distributed infrastructure? In practice, nobody is doing this to the formal standards of rigor. Nor are they incorporating the checks as part of <em>every</em> change to <em>every</em> database query. But you’re probably reasoning quite a bit about the common transaction paths. You’re drawing out full architecture diagrams and investigating any bugs with distributed traces. You’re looking for inconsistencies and patching them with some fencing around your queries.</p>
<p>My point is this: it is extremely wasteful. The hard truth is that global reasoning is the most expensive thing of all. It involves humans scheduling meetings and staring at the complete set of all possible transactions. Then they must review the proposed transactions by other programmers. And the most expensive part, by far, is the salary hours your employees dump into this process.</p>
<p>That said, weak isolation is not something to categorically exclude. Imagine you’re working on distributed infrastructure at unprecedented scales at one of the largest companies in the world. It might make sense to build bespoke high throughput infrastructure that has to make some careful tradeoffs in exchange for performance.</p>
<p>The FBI and CIA have involved and convoluted protocols to keep their lies straight. But is this an ideal pursuit for a database programmer? There’s an easier way to keep the answers straight. You can build a process to ensure that all subsequent changes do not create any inadvertent anomalies. However, it’s not something to take on casually: it’s a tool of last resort, when you’ve really hit the performance bottlenecks of strongly isolated systems.</p>
<p>Most developers building data infrastructure have the task of&nbsp;<em>presenting upwards</em>. They are in the business of&nbsp;<em>building a database-like internal service</em>. Once they get down to building their own inverted database with stream processors, Redis caches, or queues, they’re on the hook for delivering isolation guarantees. At the very least, they must correctly document and help their teams use the database correctly.</p>
<h2 id="enough-with-the-anomalies">Enough with the anomalies!</h2>
<p>In the particular case of streaming, <a href="https://materialize.com/blog/eventual-consistency-isnt-for-streaming/" target="_blank" rel="external">isolation in stream processing is particularly difficult. </a>Stream processors are typically deployed in situations where the inputs are <em>unbounded</em> and the computation is continuous. Many systems with weak isolation guarantees are designed with the informal goal of eventual consistency (i.e. we’ll get around to the truth… at some point).</p>
<p>But this doesn’t fit well with stream processing: if the inputs aren’t ever settled, eventual consistency could very well result in the outputs never being settled. That’s a large departure from most people’s expectations. Eventual consistency sets a potentially acceptable expectation that deviations are bounded and temporary. That’s very different from the situation of deviations being permanent and unbounded.</p>
<p>It’s possible, using stream processors, caches, key-value stores, and custom programs, to build a system that gives <a href="https://materialize.com/blog/operational-consistency/" target="_blank" rel="external">clear correctness guarantees to end-users</a>. But it’s <a href="https://www.scattered-thoughts.net/writing/internal-consistency-in-streaming-systems/" target="_blank" rel="external">certainly not trivial</a>. This guarantee is <em>strict serializability.</em> Strict serializability is the isolation guarantee that fits best with people’s natural intuitions around concurrency control, and <a href="https://materialize.com/blog/operational-consistency/" target="_blank" rel="external">the one that we deliver at Materialize</a>.</p>
<p>At Materialize, we’ve put in <a href="https://materialize.com/blog/virtual-time-consistency-scalability/" target="_blank" rel="external">quite a lot of work</a> to build a system that is trustworthy, and <a href="https://materialize.com/blog/operational-attributes/" target="_blank" rel="external">we are clear about what that means for you</a>. We’re betting that most of you don’t want to become consistency experts, and certainly don’t want to acquire that expertise during the course of an incident retro. Who wants to keep track of all those lies?</p>
<p>If you’re tired of keeping track of all those lies, <a href="https://console.materialize.com/account/sign-up" target="_blank" rel="external">sign up for a free trial</a> of Materialize to leverage strong consistency.</p>

<p><sup id="fn1">1. Sometimes this can get quite subtle: for instance, Postgres supports an intermediate level called repeatable read. While repeatable read theoretically allows for phantom reads,&nbsp;Postgres goes one step further (https://www.postgresql.org/docs/current/transaction-iso.html). The Postgres implementation disallows phantoms. Since the ANSI standard defines four anomalies, from the table it looks like Postgres’ repeatable read implementation is as good as the serializable implementation, right? And if you do any performance benchmarking, repeatable read is faster than serializable. In practice, serializable is such a large performance hit that few people run Postgres in serializable mode. But not so fast. There is another, secret anomaly, unknown to the ANSI committee, called&nbsp;g2-item (https://news.ycombinator.com/item?id=23500134). And in repeatable read mode,&nbsp;Postgres allows it (https://jepsen.io/analyses/postgresql-12.3). So you’ll have to check your precedence graphs for that one.<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup></p></div><div data-theme="dark"><h2 data-sb-field-path=".heading">


<span><!-- HTML_TAG_START -->More Articles<!-- HTML_TAG_END --></span>


<!-- HTML_TAG_START --><!-- HTML_TAG_END --></h2>
			<div><div><p>General</p>

			<a href="https://materialize.com/blog/engineering/qa-process-overview"><h3 data-sb-field-path=".heading"><!-- HTML_TAG_START -->Testing Materialize: Our QA Process<!-- HTML_TAG_END --></h3></a>
			<p>The following blog will show you we keep our customers and developers happy with our rigorous QA process, including our tools and testing methods.</p>
			</div><div><p>Technical Article</p>

			<a href="https://materialize.com/blog/engineering/materialize-and-memory"><h3 data-sb-field-path=".heading"><!-- HTML_TAG_START -->Materialize and Memory<!-- HTML_TAG_END --></h3></a>
			<p>We reduced memory requirements for many users by nearly 2x, resulting in significant cost-savings. </p>
			</div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stallman Was Right (102 pts)]]></title>
            <link>https://twitter.com/snowden/status/1798728673698443638</link>
            <guid>40602455</guid>
            <pubDate>Thu, 06 Jun 2024 21:10:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/snowden/status/1798728673698443638">https://twitter.com/snowden/status/1798728673698443638</a>, See on <a href="https://news.ycombinator.com/item?id=40602455">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing's starliner docks at ISS after five thrusters unexpectedly shut off (114 pts)]]></title>
            <link>https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917</link>
            <guid>40601776</guid>
            <pubDate>Thu, 06 Jun 2024 20:06:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917">https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917</a>, See on <a href="https://news.ycombinator.com/item?id=40601776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After back-to-back failed launch attempts, a crewed Starliner capsule finally <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/boeing-first-crewed-starliner-launch-iss-nasa-tense-1851512733&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/boeing-first-crewed-starliner-launch-iss-nasa-tense-1851512733">launched on Wednesday</a></span> to deliver two NASA astronauts to low Earth orbit. Boeing is testing its capsule’s ability to transport crews to the International Space Station (ISS), with Starliner set to dock to the orbital lab on Thursday.<br></p><div data-video-id="197027" data-monetizable="true" data-position="sidebar" data-video-title="Sonos First Ever Headphones Are Too Expensive For What They Offer" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="154" data-playlist="197027,197012,196999" data-current="197027"><div><p>Sonos First Ever Headphones Are Too Expensive For What They Offer</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/197027/197027_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22588.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p><strong>Update: June 6, 1:45 p.m. ET: </strong>Starliner missed its first docking opportunity at 12:15 p.m. ET due to technical issues, prompting NASA to target another docking window starting at around 1:30 p.m. ET. Five of the spacecraft thrusters failed during its approach, and four were subsequently recovered. As a result, NASA astronaut Butch Wilmore had to assume manual control of the crew capsule for a while before it went back into its autonomous mode. The capsule finally docked with the ISS at 1:34 p.m. ET following a nail-biting journey.</p><p><strong>Update: June 6, 1:27 p.m. ET: </strong>Starliner’s docking has not gone smoothly, the spacecraft developed trouble with its reaction control system thrusters.</p><p><strong>Update: June 6, 12:40 p.m. ET</strong>: NASA has waved off Starliner’s first docking opportunity and is now targeting a backup at 1:33 p.m. ET.</p><p><em>Original article follows.</em></p><p>Starliner is scheduled to autonomously dock to the forward-facing port of the space station’s Harmony module at approximately 12:15 p.m. on June 6, where it will remain for about a week. The hatch is set to open at 2 p.m. ET to allow the crew to board the ISS.</p><p>The docking maneuver will be broadcast live on NASA’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://nasa.gov/nasatv&quot;,{&quot;metric25&quot;:1}]]" href="http://nasa.gov/nasatv" target="_blank" rel="noopener noreferrer">website</a></span>, as well as the space agency’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.youtube.com/@NASA&quot;,{&quot;metric25&quot;:1}]]" href="https://www.youtube.com/@NASA" target="_blank" rel="noopener noreferrer">YouTube channel</a></span>. The live stream is set to begin at 9:30 a.m. ET, and you can tune in  through the feed below.<br></p><p>Boeing’s Starliner capsule launched atop United Launch Alliance’s Atlas V rocket, carrying NASA astronauts Butch Wilmore and Suni Williams. “Two bold NASA astronauts are well on their way on this historic first test flight of a brand-new spacecraft,” NASA Administrator Bill Nelson said in a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nasa.gov/news-release/liftoff-nasa-astronauts-pilot-first-starliner-crewed-test-to-station/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nasa.gov/news-release/liftoff-nasa-astronauts-pilot-first-starliner-crewed-test-to-station/" target="_blank" rel="noopener noreferrer">statement</a></span>. “Human spaceflight is a daring task—but that’s why it’s worth doing.”</p><p>The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/review-boeing-nasa-starliner-program-1851448548&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/review-boeing-nasa-starliner-program-1851448548">Crewed Flight Test</a></span> is part of NASA’s Commercial Crew Program and is meant to transport crew and cargo to and from the International Space Station (ISS) under a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://oig.nasa.gov/docs/IG-20-005.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://oig.nasa.gov/docs/IG-20-005.pdf" target="_blank" rel="noopener noreferrer">$4.3 billion</a></span> contract with the space agency. NASA’s other commercial partner, SpaceX, has so far launched eight crews to the space station.</p><p>During Starliner’s first crewed flight, Boeing will monitor a series of automatic spacecraft maneuvers while NASA will monitor space station operations throughout the flight. Meanwhile, the astronaut crew on board will be testing the environmental control system, the displays and control system, and maneuvering the thrusters, as well as other features of the spacecraft. </p><p>The two astronauts will provide a virtual tour of the Starliner capsule from space, which will be broadcast live on Saturday, June 8 at 8:50 a.m. on <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://nasa.gov/nasatv&quot;,{&quot;metric25&quot;:1}]]" href="http://nasa.gov/nasatv" target="_blank" rel="noopener noreferrer">NASA TV</a></span>.</p><p>It’s been a struggle for Boeing to reach this stage with its Starliner program, which has been marred by technical hiccups and delays over the past few years. Throughout it all, NASA remained dedicated to seeing its crew ride on board the company’s crew capsule.</p><p>“For many of us, this is a career-defining moment bringing on a new crew transportation capability for our agency and our nation,” Steve Stich, manager, Commercial Crew Program, at NASA’s Johnson Space Center in Houston, said in a statement. “We are going to take it one step at a time, putting Starliner through its paces, and remaining vigilant until Butch and Suni safely touch down back on Earth at the conclusion of this test flight.”</p><p><em>For more spaceflight in your life, follow us on </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://gizspaceflight/&quot;,{&quot;metric25&quot;:1}]]" href="http://gizspaceflight/" target="_blank" rel="noopener noreferrer"><em>X</em></a></span><em> and bookmark Gizmodo’s dedicated </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/science/spaceflight&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/science/spaceflight"><em>Spaceflight page</em></a></span><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HP bricks customers laptops with faulty automatic BIOS upgrade (158 pts)]]></title>
            <link>https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3</link>
            <guid>40601711</guid>
            <pubDate>Thu, 06 Jun 2024 20:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3">https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3</a>, See on <a href="https://news.ycombinator.com/item?id=40601711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text" id="bodyDisplay_7">
			
				
					
					
						<blockquote><hr><a href="https://h30434.www3.hp.com/t5/user/viewprofilepage/user-id/7882955">@Wojtek0586</a>&nbsp;wrote:<p>Hi Dioxaz, thanks for Your work!</p><p><span>It can be very useful!</span></p><p><span>I am surprised by the fact that HP has not made an official statement on this topic. These laptops are approaching 3 years old, we have literally a dozen or so days left in which they are covered by the manufacturer's warranty. What about units that are already out of warranty? The official update broke working equipment, who will pay for the repair? The scale of the phenomenon seems to be very large and concerns the entire world. Could this be planned obsolescence of the product?</span></p><hr></blockquote><p>It looks like there has been an official statement <a href="https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/m-p/9085148/highlight/true#M661377" target="_self">according to a previous post</a>. But I have no idea of where this was published. Me too can't believe we're the only ones in those threads who ended up with bricked 455 G7s, this incident must have struck potentially millions of machines. However I can't seem to find anything related on the internet for the moment.</p><p>Fun fact: when this incident struck me a few days ago with two users at work reporting bricked machines due to a bad BIOS update, I thought I was the only one, started to panic and doubt my IT asset management skills. Just think of it, that dreaded BIOS update even escaped the GPO I made preventing driver updates, and we're indeed running a WSUS server with manually approved updates. How could that be? Well, a smile came back to my face when a teammate alerted me that HP deleted the BIOS update from their servers. Then I saw this thread, then another one, and another one... and I finally realised. <span title=":slightly_smiling_face:">🙂</span></p><p>And yep, my initial searches leading me to clueless people advising to use a GPO to prevent BIOS updates. I don't think they have any idea of what they're talking about (especially with that dreaded “<strong>Native OS Firmware Update Service</strong>” functionality). Speaking of which, that will be our next task: setting that “Native OS Firmware Update Service” to off on all machines we can in order to disable the UEFI Capsule BIOS update for good.</p>
					
				
			
			
				
			
			
				
			
			
			
			
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI in software engineering at Google: Progress and the path ahead (213 pts)]]></title>
            <link>https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/</link>
            <guid>40601116</guid>
            <pubDate>Thu, 06 Jun 2024 18:59:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/">https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/</a>, See on <a href="https://news.ycombinator.com/item?id=40601116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20240606">
                    
                    


<div>
        <p data-block-key="ygj61">In 2019, a software engineer — at Google or indeed anywhere else — would have heard of advances in machine learning, and how deep learning has become remarkably effective in fields such as computer vision or language translation. However, most of them would not have imagined, let alone experienced, the ways in which machine learning might benefit what <i>they</i> do.</p><p data-block-key="arj8a">Just five years later, in 2024, there is widespread enthusiasm among software engineers about how AI is helping write code. And a significant number of those have used ML-based autocomplete, whether it is using company internal tools at large companies, e.g., <a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">Google’s internal code completion</a>, or via commercially available products.</p><p data-block-key="5afae">In this blog, we present our newest AI-powered improvements within the context of the continuing transformation of Google’s internal software development tools, and discuss further changes that we expect to see in the coming 5 years. We also present our methodology on how to build AI products that deliver value for professional software development. Our team is responsible for the software development environments where Google engineers spend the majority of their time, including <i>inner loop</i> (e.g., IDE, <a href="https://abseil.io/resources/swe-book/html/ch19.html" target="_blank" rel="noopener noreferrer">code review</a>, <a href="https://developers.google.com/code-search" target="_blank" rel="noopener noreferrer">code search</a>), as well as <i>outer loop</i> surfaces (e.g., <a href="https://developers.google.com/issue-tracker" target="_blank" rel="noopener noreferrer">bug management</a>, planning). We illustrate that improvements to these surfaces can directly impact developer productivity and satisfaction, both metrics that we <a href="https://ieeexplore.ieee.org/document/9159122" target="_blank" rel="noopener noreferrer">monitor carefully</a>.</p>
    </div>

                    
                    


<div>
        <h2 data-block-key="ygj61">The challenge</h2><p data-block-key="3lfr4">An ongoing challenge in this domain is that AI technology is evolving quickly and it is hard to predict which ideas to explore first. There is often a significant gap between technically feasible demos and successful productization. We approach deployment of ideas to products with three guidelines:</p><ol><li data-block-key="kapf"><i>Prioritize by technical feasibility and impact</i>: Work on ideas wherein both technical feasibility has already been established and high (measureable) impact on engineers’ workflows is expected.</li><li data-block-key="5v1pu"><i>Learn quickly, to improve UX and model quality</i>: Focus on iterating quickly and extracting lessons learned, while safeguarding developer productivity and happiness. User experience is just as important as model quality.</li><li data-block-key="6ha0d"><i>Measure effectiveness</i>: As our goal is to increase <a href="https://ieeexplore.ieee.org/document/10372494" target="_blank" rel="noopener noreferrer">productivity</a> and satisfaction metrics, we need to extensively monitor these metrics.</li></ol>
    </div>

                    
                    


<div>
        <h2 data-block-key="ygj61">Applying LLMs to software development</h2><p data-block-key="1pl70">With the advent of <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">transformer</a> architectures, we started exploring how to apply LLMs to software development. LLM-based inline code completion is the most popular application of AI applied to software development: it is a natural application of LLM technology to use the code itself as training data. The UX feels natural to developers since word-level autocomplete has been a core feature of IDEs for many years. Also, it’s possible to use a rough measure of impact, e.g., the percentage of new characters written by AI. For these reasons and more, it made sense for this application of LLMs to be the first to deploy.</p><p data-block-key="9qvi8">Our <a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">earlier blog</a> describes the ways in which we improve user experience with code completion and how we measure impact. Since then, we have seen continued fast growth similar to <a href="https://arxiv.org/abs/2402.04141" target="_blank" rel="noopener noreferrer">other enterprise contexts</a>, with an acceptance rate by software engineers of 37%<footnote id="836bf139-6026-4010-9080-cf46311a03a0">[836bf1]</footnote> assisting in the completion of 50% of code characters<footnote id="5f21d2a5-56e2-4ad8-b41c-d21f3452375e">[5f21d2]</footnote>. In other words, the same amount of characters in the code are now completed with AI-based assistance as are manually typed by developers. While developers still need to spend time reviewing suggestions, they have more time to focus on code design.</p><p data-block-key="b4iqc">Key improvements came from both the models —&nbsp;larger models with improved coding capabilities, heuristics for constructing the context provided to the model, as well as tuning models on usage logs containing acceptances, rejections and corrections — and the UX. This cycle is essential for learning from practical behavior, rather than synthetic formulations.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-Hero1.width-800.png" alt="AIforSE-Hero1">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-Hero1.width-800.png" alt="AIforSE-Hero1" loading="lazy">
      
        <p data-block-key="qeeti"><i>Improving AI-based features in coding tools (e.g., in the IDE) with historical high quality data across tools and with usage data capturing user preferences and needs.</i></p>
      
    </picture>
  



                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-LineGraph2.width-800.png" alt="AIforSE-LineGraph2">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-LineGraph2.width-800.png" alt="AIforSE-LineGraph2" loading="lazy">
      
        <p data-block-key="qeeti"><i>Continued increase of the fraction of code created with AI assistance via code completion, defined as the number of accepted characters from AI-based suggestions divided by the sum of manually typed characters and accepted characters from AI-based suggestions. Notably, characters from copy-pastes are not included in the denominator.</i></p>
      
    </picture>
  



                    
                    


<div>
        <p data-block-key="ygj61">We use our extensive and high quality logs of internal software engineering activities across multiple tools, which we have curated over many years. This data, for example, enables us to represent fine-grained code edits, build outcomes, edits to resolve <a href="https://en.wikipedia.org/wiki/Software_build" target="_blank" rel="noopener noreferrer">build</a> issues, code copy-paste actions, fixes of pasted code, code reviews, edits to fix reviewer issues, and change submissions to a repository. The training data is an aligned corpus of code with task-specific annotations in input as well as in output. The design of the data collection process, the shape of the training data, and the model that is trained on this data was described in our <a href="https://blog.research.google/2023/05/large-sequence-models-for-software.html">DIDACT</a> blog. We continue to explore these powerful datasets with newer generations of foundation models available to us (discussed more below).</p><p data-block-key="7v102">Our next significant deployments were <a href="https://research.google/blog/resolving-code-review-comments-with-ml/">resolving code review comments</a> (&gt;8% of which are now addressed with AI-based assistance) and automatically adapting pasted code to the surrounding context (now responsible for ~2% of code in the IDE<footnote id="5f21d2a5-56e2-4ad8-b41c-d21f3452375e">[5f21d2]</footnote>). Further deployments include instructing the IDE to perform code edits with natural language and predicting <a href="https://research.google/blog/safely-repairing-broken-builds-with-ml/">fixes to build failures</a>. Other applications, e.g., <a href="https://arxiv.org/abs/2405.13565" target="_blank" rel="noopener noreferrer">predicting tips for code readability</a> following a similar pattern are also possible.</p><p data-block-key="a6led">Together, these deployed applications have been successful, highly-used applications at Google, with measurable impact on productivity in a real, industrial context.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    <div>
      <video playsinline="" muted="true" loop="true" preload="auto">
        <source src="https://storage.googleapis.com/gweb-research2023-media/media/AIforSE-ExampleIDE3.mp4" type="video/mp4">
      </video>
      <div aria-label="Video Play/pause">
        <p><span>play silent looping video</span>
          <span>pause silent looping video</span>
        </p>
        
      </div>
      
      <p data-block-key="a3e6t"><i>A demonstration of how a variety of AI-based features can work together to assist with coding in the IDE.</i></p>
      
    </div>
  



                    
                    


<div>
        <h2 data-block-key="kyiao">What we’ve learned</h2><p data-block-key="d7ocb">Our work so far has taught us several things:</p><ol><li data-block-key="fijbk">We achieved the highest <i>impact with UX that naturally blends into users’ workflows</i>. In all the above examples, a suggestion is presented to the user, taking them to the next step in their workflow with one tab or click. Experiments requiring the user to remember to trigger the feature have failed to scale.</li><li data-block-key="92tm">We observe that with AI-based suggestions, the <i>code author increasingly becomes a reviewer</i>, and it is <a href="https://research.google/pubs/if-its-what-i-wanted-thats-great-but-if-its-not-i-just-wasted-time-unpacking-the-perceived-costsbenefits-of-ml-enhanced-developer-tooling/">important to find a balance</a> between <a href="https://arxiv.org/abs/2205.06537" target="_blank" rel="noopener noreferrer">the cost of review and added value</a>. We typically address the tradeoff with acceptance rate targets.</li><li data-block-key="b83m8">Quick iterations with <i>online A/B experiments are key</i>, as offline metrics are often only rough proxies of user value. By surfacing our AI-based features on internal tooling, we benefit greatly from being able to easily launch and iterate, measure usage data, and ask users directly about their experience through UX research.</li><li data-block-key="g6lv"><i>High quality data</i> from activities of Google engineers across software tools, including interactions with our features, is essential for our model quality.</li></ol><p data-block-key="hc39">We observe across features that it’s <i>important to optimize for conversion</i> from the opportunity (mostly a user activity, shown at the top of the funnel below) to impact (applied AI assistance, at the bottom of the funnel), while removing bottlenecks from intermediate steps of the funnel by leveraging UX and model improvements.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-FunnelFinal4.width-800.png" alt="AIforSE-FunnelFinal4">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-FunnelFinal4.width-800.png" alt="AIforSE-FunnelFinal4" loading="lazy">
      
        <p data-block-key="ubz65"><i>An opportunity funnel starting from SWE actions down to actual application of ML-based suggestions. Opportunities are lost if the model prediction is not confident enough, the model doesn’t respond or responds too late, the prediction is subpar, the user doesn’t notice the prediction, and so on. We use UX and model improvements to harvest as many opportunities as we can.</i></p>
      
    </picture>
  



                    
                    


<div>
        <h2 data-block-key="kyiao">What’s next</h2><p data-block-key="5cg9f">Encouraged by our successes so far, we are doubling down on bringing the latest foundation models (<a href="https://blog.google/technology/ai/google-gemini-ai/" target="_blank" rel="noopener noreferrer">Gemini series</a>) infused with the developer data (as part of <a href="https://blog.research.google/2023/05/large-sequence-models-for-software.html">DIDACT</a>, mentioned above) to power existing and new applications of ML to software engineering in Google.</p><p data-block-key="44ic3">Across the industry, ML-based code completion has provided a major boost for software developers. While there are still opportunities to improve code generation, we expect the next wave of benefits to come from ML assistance in a broader range of software engineering activities, such as testing, code understanding and code maintenance; the latter being of particular interest in enterprise settings. These opportunities inform our own ongoing work. We also highlight two trends that we see in the industry:</p><ol><li data-block-key="6emfq">Human-computer interaction has moved towards natural language as a common modality, and we are seeing a shift towards using language as the interface to software engineering tasks as well as the gateway to informational needs for software developers, all integrated in IDEs.</li><li data-block-key="2pi30">ML-based automation of larger-scale tasks — from diagnosis of an issue to landing a fix — has begun to show initial evidence of feasibility. These possibilities are driven by innovations in <i>agents</i> and <i>tool use</i>, which permit the building of systems that use one or more LLMs as a component to accomplish a larger task.</li></ol><p data-block-key="dfn6u">To expand on the above successes toward these next generation capabilities, the community of practitioners and researchers working in this topic would benefit from common benchmarks to help move the field towards practical engineering tasks. So far, benchmarks have been focused mostly around code generation (e.g., <a href="https://github.com/openai/human-eval" target="_blank" rel="noopener noreferrer">HumanEval</a>). In an enterprise setting, however, benchmarks for a wider range of tasks could be particularly valuable, e.g., code migrations and production debugging. Some benchmarks, such as one for bug resolution (e.g., <a href="https://www.swebench.com/" target="_blank" rel="noopener noreferrer">SWEBench</a>), and prototypes targeting those benchmarks (e.g., from <a href="https://www.cognition-labs.com/blog" target="_blank" rel="noopener noreferrer">Cognition AI</a>) have been published. We encourage the community to come together to suggest more benchmarks to span a wider range of software engineering tasks.</p>
    </div>

                    
                    


<div>
        <h2 data-block-key="kyiao">Acknowledgements</h2><p data-block-key="98em"><i>This project is the result of work of many people from the Google Core Systems &amp; Experiences team and Google Deepmind, and</i> <a href="https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html" target="_blank" rel="noopener noreferrer"><i>DIDACT</i></a><i>. This article was co-authored with Boris Bokowski (Google Coding Tools Director), Petros Maniatis (research), Ambar Murillo (UXR), and Alberto Elizondo (UXD). Deep gratitude goes to contributors to the various features: Adam Husting, Ahmed Omran, Alexander Frömmgen, Ambar Murillo, Ayoub Kachkach, Brett Durrett, Chris Gorgolewski, Charles Sutton, Christian Schneider, Danny Tarlow, Damien Martin-Guillerez, David Tattersall, Elena Khrapko, Evgeny Gryaznov, Fredde Ribeiro, Gabriela Surita, Guilherme Herzog, Henrik Muehe, Ilia Krets, Iris Chu, Juanjo Carin, Kevin Villela, Kristóf Molnár, Lera Kharatyan, Madhura Dudhgaonkar, Marcus Revaj, Nimesh Ghelani, Niranjan Tulpule, Pavel Sychev, Siddhant Sanyam, Stanislav Pyatykh, Stoyan Nikolov, Ugam Kumar, Tobias Welp, Vahid Meimand, Vincent Nguyen, Yurun Shen, and Zoubin Ghahramani. Thanks to Tom Small for creating graphics for this post.</i></p>
    </div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dragonfly: A large vision-language model with multi-resolution zoom (134 pts)]]></title>
            <link>https://www.together.ai/blog/dragonfly-v1</link>
            <guid>40600775</guid>
            <pubDate>Thu, 06 Jun 2024 18:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.together.ai/blog/dragonfly-v1">https://www.together.ai/blog/dragonfly-v1</a>, See on <a href="https://news.ycombinator.com/item?id=40600775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><p>We are excited to announce the launch of Dragonfly, a breakthrough instruction-tuning Vision-language architecture, that enhances fine-grained visual understanding and reasoning about image regions. </p><p>We are releasing the Dragonfly architecture, which uses multi-resolution zoom-and-select to enhance multi-modal reasoning while being context-efficient. We are also launching two new open-source models &nbsp;<a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-v1">Llama-3-8b-Dragonfly-v1</a> a general-domain model trained on 5.5 million image-instruction pairs and <a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-Med-v1">Llama-3-8b-Dragonfly-Med-v1</a> finetuned on additional 1.4 biomedical image-instruction data. Dragonfly demonstrates promising performance on vision-language benchmarks like commonsense visual QA and image captioning. Dragonfly-Med outperforms prior models, including Med-Gemini on multiple medical imaging tasks, showcasing its capabilities for high-resolution medical data.</p><p>Figure 1: Generated example from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eeab92ebdcff8cda76f2_image%20(5).png" loading="lazy" alt=""></p><figcaption><strong>Question</strong>: Summarize the visual contents of the image.<br>‍<strong>Dragonfly Response</strong>: In the heart of a vibrant skatepark, a skateboarder is caught in a moment of pure exhilaration. The skateboarder, dressed in a black t-shirt adorned with a yellow graphic and black pants, is suspended in mid-air, performing an impressive trick on a concrete ramp. The skateboarder's arms are outstretched, adding balance to the daring stunt. The skatepark itself is a concrete playground, with the skateboarder's ramp being the main focus. In the background, palm trees sway gently, adding a touch of nature to the urban setting. A few spectators can be seen in the distance, their attention riveted on the airborne skateboarder. The image captures not just a moment, but a story of skill, courage, and the joy of skateboarding.</figcaption></figure><p>Figure 2: Generated examples from &nbsp;Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eed892ebdcff8cda8fc9_image%20(9).png" loading="lazy" alt=""></p><figcaption><strong>Question</strong>: Write a detailed radiology note based on the chest X-ray.<br>‍<strong>Gold Answer</strong>: AP upright and lateral views of the chest were provided. Left chest wall pacer pack is again seen with leads extending into the right heart. Abandoned pacing leads are also noted in the right chest wall extending into the right heart. The heart remains moderately enlarged. Lung volumes are low, with equivocal ground-glass opacity on the frontal view, which appears less conspicuous on the lateral view most likely attributable to underpenetrated technique. No gross evidence for pneumonia or pulmonary edema. No large effusions are seen. There is no pneumothorax. Bony structures are intact.<br>‍<strong>Dragonfly Response</strong>: PA and lateral views of the chest provided. Left chest wall pacer device is again noted with leads extending into the region the right atrium and right ventricle. The heart remains mildly enlarged. There is mild hilar congestion without frank edema. No convincing signs of pneumonia. No large effusion or pneumothorax. Cardiomediastinal silhouette is stable. Bony structures are intact </figcaption></figure><h2>Dragonfly Model Architecture</h2><p><a href="https://arxiv.org/abs/2406.00977"><em>Arxiv Paper</em></a><em> with technical details and </em><a href="https://github.com/togethercomputer/Dragonfly"><em>codebase</em></a><em> for architecture implementation</em></p><p>Dragonfly is special because:</p><ol role="list"><li>Dragonfly employs two key strategies: multi-resolution visual encoding and zoom-in patch selection, which enables the model to focus more fine-grained details on image regions and provide better commonsense reasoning. Despite the model being optimized for capturing fine-grained image details, the model achieves good zero-shot performance at standard image-understanding benchmarks such as visual question-answering or natural-image-captioning.</li><li>We demonstrate our model's understanding and reasoning capabilities on biomedical tasks, which often require a fine-grained understanding of high-resolution image regions. By finetuning our general-domain model on a biomedical instruction-tuning dataset with 1.4 million biomedical image-text pairs, Dragonfly-med, our biomedical version of the model, achieves state-of-the-art or competitive performance on multiple biomedical benchmarks, including visual question answering, image captioning, and radiology report generation.</li></ol><p>Figure 3: Overview of the Dragonfly architecture</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66614db787f49c335e5c4118_image3.png" loading="lazy" alt=""></p></figure><p><strong>Multi-resolution Visual Encoding:</strong> Dragonfly processes images at multiple resolutions—low, medium, and high. Each image, depending on its resolution, is divided into sub-images that are encoded into visual tokens. These tokens are then projected into a language space, forming a concatenated sequence that feeds into the LLM. This method allows the model to handle larger images efficiently and improves the granularity of visual data processing.</p><p><strong>Zoom-in Patch Selection:</strong> To further refine the model's focus on important visual details, Dragonfly employs a selective approach for high-resolution images. It uses a novel zoom-in patch selection strategy to identify and retain only those high-resolution sub-images that provide the most significant visual information. This is achieved by comparing summary embeddings of medium and high-resolution sub-images to select the most semantically relevant patches. This targeted selection reduces redundancy and focuses on crucial content areas, thereby enhancing the overall model efficiency and fine-grained understanding on these areas.</p><p>Together, these strategies enable Dragonfly to achieve a detailed and efficient visual understanding, making it particularly adept at processing complex image data in specific domains.</p><p>Figures 4 and 5: Generated examples from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f9c35d300e04e8ef323f_image%20(6).png" loading="lazy" alt=""></p><figcaption><strong>Figure 4 Question:</strong> Summarize the visual content of the image first and then answer the questions. What process does this diagram portray and which solar body is portrayed in this diagram?<br><strong>Dragonfly Response:</strong> The diagram portrays the process of photosynthesis in plants, which is a fundamental biological process where plants convert light energy from the sun into chemical energy in the form of glucose. The solar body portrayed in this diagram is the sun, which is shown at the top left corner, providing the light energy that is essential for photosynthesis.<p><strong>Figure 5 Question:</strong> Summarize the image and why it is funny?<br><strong>Dragonfly Response:</strong> The image is a humorous take on the Mona Lisa painting, featuring a dog in the place of the human subject. The dog's expression, combined with the iconic pose and attire, creates a playful and amusing effect. The painting is executed in the same style as the original Mona Lisa, with similar brushwork and color palette, which adds to the humor by juxtaposing the serious art historical context with the light-hearted subject matter.</p></figcaption></figure><h2>Dragonfly Model Evaluation</h2><p>We evaluate Dragonfly trained based on LLaMA-8B on five popular vision-language benchmarks that require strong commonsense reasoning and detailed image understanding, AI2D, ScienceQA, MMMU, MMVet, and POPE. AI2D and ScienceQA evaluate visual commonsense reasoning in the science domain. MMMU and MMVet focus on providing a comprehensive evaluation of the vision-language capabilities. POPE evaluates the object-level hallucination of the regional details. Dragonfly achieved competitive performance compared with other popular vision-language models, demonstrating the effectiveness of Dragonfly on commonsense reasoning and fine-grained understanding of image regions. The results are presented in the following table:</p><div>
<table>
  <tbody><tr>
    <th>Model</th>
    <th>AI2D</th>
    <th>ScienceQA</th>
    <th>MMMU</th>
    <th>MMVet</th>
    <th>POPE</th>
  </tr>
  <tr>
    <td>VILA</td>
    <td>-</td>
    <td>68.2</td>
    <td></td>
    <td>34.9</td>
    <td>85.5</td>
  </tr>
  <tr>
    <td>LLaVA-v1.5 (Vicuna-7B)</td>
    <td>54.8</td>
    <td>70.4</td>
    <td>35.3</td>
    <td>30.5</td>
    <td>85.9</td>
  </tr>
  <tr>
    <td>LLaVA-v1.6 (Mistral-7B)</td>
    <td>60.8</td>
    <td>72.8</td>
    <td>33.4</td>
    <td>44.8</td>
    <td>86.7</td>
  </tr>
  <tr>
    <td>QWEN-VL-chat</td>
    <td>52.3</td>
    <td>68.2</td>
    <td>35.9</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Dragonfly (LLaMA-8B)</td>
    <td>63.6</td>
    <td>80.5</td>
    <td>37.8</td>
    <td>35.9</td>
    <td>91.2</td>
  </tr>
</tbody></table>
</div><h2>Dragonfly-Med</h2><p>We partnered with the Zou group at Stanford Medicine to finetune Dragonfly on additional 1.4 million biomedical image instructions to develop the biomedical version, Dragonfly-Med. </p><p>Figures 6 and 7: Generated examples from &nbsp;Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f97f1204614802b7b80f_image%20(7).png" loading="lazy" alt=""></p><figcaption><strong>Figure 4 Question: </strong>Please provide brief description of the image.<br><strong>Gold Answer: </strong>Abdominal enhanced CT shows a ring-like enhanced and central heterogeneous tumor in the pelvis.<br><strong>Dragonfly Response:</strong> Computed tomography scan of the abdomen and pelvis showing a large lobulated mass in the pelvic cavity.<br><strong><br>Figure 5 Question:</strong> Explain the visual content of the image.<br><strong>Gold Answer:</strong> GROSS: HEMATOLOGIC: Spleen: Chronic Myelogenous Leukemia: Gross natural color whole spleen massively enlarged.<br><strong>Dragonfly Response:</strong> HEMATOLOGIC: Spleen: Chronic Myelogenous Leukemia: Gross natural color external view of massively enlarged spleen with multiple infarcts seen from capsule.</figcaption></figure><p>Dragonfly-Med is evaluated on both visual question-answering, medical image captioning, and clinical report generation evaluation. Our model outperforms Med-Gemini, which was finetuned a customized version of Gemini 1.5 with about 5x more image-instruction data, on all the three question-answering datasets. The following two tables show the results on these benchmarks. The first table shows the performance on question-answering benchmarks and the second table presents the performance of biomedical image caption and report generation. </p><div>
<table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th>LLaVA-Med</th>
    <th>Med-Gemini</th>
    <th>SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="2">VQA-RAD</td>
    <td>Acc (closed)</td>
    <td>84.2</td>
    <td>69.7</td>
    <td>87.1</td>
    <td>77.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>50.1</td>
    <td>62.1</td>
    <td>59.6</td>
  </tr>
  <tr>
    <td rowspan="2">SLAKE</td>
    <td>Acc (closed)</td>
    <td>83.2</td>
    <td>84.8</td>
    <td>91.6</td>
    <td>90.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>75.8</td>
    <td>89.3</td>
    <td>88.8</td>
  </tr>
  <tr>
    <td rowspan="2">Path-VQA</td>
    <td>Acc (closed)</td>
    <td>91.7</td>
    <td>83.3</td>
    <td>91.7</td>
    <td>92.3</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>58.7</td>
    <td>62.7</td>
    <td>67.6</td>
  </tr>
</tbody></table>
</div><p>‍</p><div><table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th colspan="2">BiomedGPT SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="3">IU X-Ray</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">28.5<p>12.9</p><p>40.1</p></td>
    <td rowspan="3">44.8<p>24.2</p><p>43.5</p></td>
    <td rowspan="3">28.5<p>29.7</p><p>58.8</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">Peir Gross</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">36.0<p>15.4</p><p>122.7</p></td>
    <td rowspan="3">36.0<p>15.4</p><p>122.7</p></td>
    <td rowspan="3">40.3<p>38.3</p><p>179.9</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">ROCO</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">18.2<p>7.8</p><p>24.2</p></td>
    <td rowspan="3">18.2<p>7.8</p><p>24.2</p></td>
    <td rowspan="3">19.3<p>15.1</p><p>40.1</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">MIMIC CXR</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">23.8<p>14.2</p><p>14.7</p></td>
    <td rowspan="3">33.5<p>19.0</p><p>50.9</p></td>
    <td rowspan="3">24.2<p>22.6</p><p>47.2</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
</tbody></table></div><p>‍</p><p>Dragonfly-Med outperforms the existing state-of-the-art models across multiple benchmarks on the Path-VQA dataset for both accuracy (closed) and token F1 metrics. It attains an accuracy of 90.4% on the SLAKE dataset, which is close to the current state-of-the-art of 91.6%. Notably, Dragonfly-Med outperforms Med-Gemini, a model finetuned from Gemini-1.5 on 7 million biomedical data samples, on all VQA tasks we evaluated. On the image captioning task, Dragonfly-Med achieves state-of-the-art or competitive results on several metrics across these datasets. Notably, on the Peir Gross and ROCO datasets, Dragonfly-Med outperforms existing methods on all three metrics: ROUGE-L, METEOR, and CIDEr. Some of the baseline models are much larger than our current implementation. The zoom-and-select architecture of Dragonfly is especially powerful for medical image understanding as medical images are often very large and the salient regions are found in small patches.</p><h2>Conclusion and Future Work</h2><p>Dragonfly architecture provides a potential research direction on zooming in image regions to focus more selected fine-grained visual information. We trained two checkpoints based on LLaMA3-8B-Instruct and achieved promising results on both general-domain and biomedical-domain tasks. We hope this work could benefit the research community to explore more open-sourced multimodal research and apply AI on real-world problems.</p><p>We will keep improving the comprehensive capabilities of open-source multimodal models. In the future, we will explore new architectures, better visual encoding strategies, more comprehensive studies on how the data mixture should be and more scientific domains to provide benefits for broader fields.</p><h2>Acknowledgements</h2><p>We especially appreciate the contributions and collaborations of our partner, Stanford Medicine. We would also like to acknowledge the following resources that were instrumental in the development of Dragonfly:</p><ul role="list"><li>Meta LLaMA3: We utilized the LLaMA3 as our language model backbone for our current two checkpoints.</li><li>CLIP: Our current visual backbone is CLIP model from OpenAI</li><li>Our codebase is built upon codebases of Otter and LLaVA-UHD.</li></ul></div><div><div><p><h2>Q: Should I use the RedPajama-V2 Dataset out of the box?</h2></p></div><div><p>RedPajama-V2 is conceptualized as a pool of data that serves as a foundation for creating high quality datasets. The dataset is thus not intended to be used out of the box and, depending on the application, data should be filtered out using the quality signals that accompany the data. With this dataset, we take the view that the optimal filtering of data is dependent on the intended use. Our goal is to provide all the signals and tooling that enables this.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interview with Lola De La Mata about tinnitus (114 pts)]]></title>
            <link>https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/</link>
            <guid>40600748</guid>
            <pubDate>Thu, 06 Jun 2024 18:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/">https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/</a>, See on <a href="https://news.ycombinator.com/item?id=40600748">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Multi-disciplinary artist Lola De La Mata speaks to Patrick Clarke about how her experience with severe tinnitus and vertigo, groundbreaking work in the field of audiology, and stigma around hearing loss in the musical community fed into her new album Oceans On Azimuth</p><div>




<p>“At least I had some good sake,” jokes Lola De La Mata. “The rest is quite unfortunate.” The English-born, French-Spanish multi-disciplinary artist is recalling a dinner in 2019 that would change her life and her artistic practice forever. As she recalls, a staff member plugged in the restaurant’s electric piano without checking that the master fader was at 0, which resulted in a deafening noise. De La Mata had already been experiencing low-level tinnitus for years prior – she blames the heavy metal shows that her parents took her to growing up – but the feedback in the restaurant was so severe that it would leave her dealing with catastrophic consequences. “It was a rupture,” she explains. “Afterwards my hearing in my left ear kept going through notches, I’d completely lose my hearing, or low floating tones would cover [other people’s] speech. Sometimes I’d wake up because I was hearing all this thunder, a cracking sound.”</p>



<p>The thunder, she was later informed by audiologists, was a good thing – the sound of her ear trying to repair itself. This was of scant solace, given that as well as extreme tinnitus, De La Mata developed vertigo. “I do have a chronic health condition, which made it difficult to pinpoint if it was that that was suddenly getting worse, or whether it was [the damage to the ear] that was causing neurological changes, but I literally couldn’t walk straight; I was having what looked like strokes where I would collapse.” A violinist, she was told by doctors to give up playing. When the COVID pandemic arrived a few months in, she was forced to shield because of ultimately false suspicions that she had MS. “I got really frustrated,” De La Mata says. “I wasn’t getting any of the answers I wanted. It was, ‘Your hearing is fine, you’re young, you’re healthy,’ and it’s like, well clearly I’m not if I can’t walk and people are feeding me.”&nbsp;</p>




<p>Over a year later, when she had recovered to the point that she could hear again through her left ear, though not as well as before, she was able to start processing her situation. First came a composition called ‘KOH-klee-uh’ via SA Recordings in 2022, the leading release in a four-part series called <em>The Hearing Experience</em> in which artists were invited to explore their relationship with the act of listening. An uneasy and often jarring mixture high-pitched rings, deep scrapes and dull thuds, she employed tuning forks used during the Rhine &amp; Weber hearing test, as well as a Canna Sonora, a rare instrument consisting of aluminium poles arranged vertically across a rack. “There’s a node in the centre, and when you rub [a pole], it activates a vibration. When you approach it with your hand, the vibration actually swarms into your palm, then you shoot it back. It’s absolutely wild.” In the process it delivers a ringing noise, equal parts grating and transfixing, that accurately mimics the sound of tinnitus.</p>



<iframe width="560" height="315" src="https://www.youtube.com/embed/h7umtdzHM6Q?si=J2os4pDmr8xKFLTs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>



<p>“At the beginning it was not a good project, it was just, ‘I hear a high-pitched sound, here’s a high-pitched sound!’” she laughs. The plan at that point was to create a number of pieces relating to different parts of the ear, but over time her scope broadened to become her new album <em>Oceans On Azimuth</em>, a visceral and intense thing that bristles with life in the same way her eardrum thundered as it tried to force itself back into shape. At the record’s launch at the Stephen Lawrence Gallery in Greenwich, a week before she speaks to tQ, De La Mata displays accompanying sculptures based on a 3D rendering of her ear canal while Jono Heale from the hearing protection company ACS takes similar moulds from attendees to form part of a future piece. Part of the venue is devoted to instruments arranged in preparation for De La Mata to perform following a panel discussion with Heale and the abstract turntablist Maria Chávez. There’s no Canna Sonora – only two of them currently exist in the country – but a number of the other instruments have been specially crafted for the project.</p>



<p>She begins her live performance by walking slowly through the crowd bearing one of them, a metal gong in the shape of an ear. It spins on its string as she strikes it, making the clanging sounds spin around our ears. As the resonances fade each time, the sounds of our surroundings – church bells, sirens, the crash of a dropped glass – feel amplified as they fill the emerging gaps. That is until De La Mata takes to the theremin, playing it first at a shrieking high, then low like a chainsaw. Suddenly it stops, and as a new electronic bass tone slowly builds the musician moves over to a violin, partly wrapped in tinfoil, which she plays scratchily and discordantly. She thumps the instrument with the bow, then scrapes its strings with the bow upside down. She scrapes the bow again against the rim of some hi-hats and knocks it against the inside of ear-shaped metal loops. Turning to her laptop she unleashes a monstrous barrage of beeps and gurgles and screeches from which a muffled rhythm gradually emerges. Occasionally recorded voices can be heard atop the fray. One is an academic-sounding figure who explains the concept of impedance matching – the way we have a ‘middle ear’ whose role is to take the vibrations that enter the ear and translate them so they can be interpreted through the liquid of the cochlea, through which sound travels differently. Elsewhere, a woman’s voice riffs on the phrase ‘pink noise’ – referring to an ambient sound that can be used in the treatment of tinnitus – which De La Mata has warped into a fragmented whirl.</p>




<figure><img decoding="async" width="683" height="1024" src="https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-683x1024.jpg" alt="Lola De La Mata holding an ear-shaped gong while stood in a cave" srcset="https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-683x1024.jpg 683w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-200x300.jpg 200w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-768x1152.jpg 768w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-1024x1536.jpg 1024w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour.jpg 1280w" sizes="(max-width: 683px) 100vw, 683px" loading="lazy" fetchpriority="low"><figcaption>Photo by Rosie Terry Toogood</figcaption></figure>



<p>The voices belong to two employees of The Hudspeth Laboratory Of Sensory Neuroscience in New York, biophysicist Francesco Gianoli and resident musicologist Lana Norris respectively. De La Mata had come across the institution when, a few months after ‘KOH-klee-uh’ and her first foray into making music around her tinnitus, she searched online for academics in the field of audiology. She was intrigued that the laboratory’s director A.J. Hudspeth “kept employing words from artistic fields, like how the way hair bundles are distributed on the cochlea is almost like a piano in reverse. There was something that was accessible, not just academic.” You can hear that same impulse in Gianoli’s explanation of impedance matching, too, included on the album and in her live performance, where he compares the role of the middle ear to the bridge of a violin. “He wrote a master’s paper about it, and how there’s a possibility of creating the perfect bridge for each specific violin if you put in the right equations with the thickness of wood, the type, the width, etcetera.”</p>



<p>Throughout the project, De La Mata continues, she felt a duty to focus on this sense of the organic and the tangible, and not to rest too heavily on the digital. She became fascinated by the work of Maryanne Amacher, a composer and artist who worked with phenomena called auditory distortion products, or otoacoustic emissions, in which the ears themselves produce audible sound. “There’s a capacity of ‘playing the ear’,” as De La Mata puts it. When hearing the Canna Sonora, for instance, if you turn your head, it can feel like you’re catching extra tones that are being played, “like there’s a little moth fluttering on your ear,” whereas what’s really happening is that “your ear is pushing its own tone back out, it’s like a combination of the tones.”</p>



<p>She also started meeting with female double bass players (two of whom, Gwen Reed and Marianne Schofield, appear on <em>Oceans On Azimuth</em>). “There’s something about this oversized instrument that means if you’re a woman, who tend to be smaller than men, it means that your hands are used differently, your reach is different and the way your hips fit into the instrument, and that actually changes the vibration of the instrument.” There were parallels to be found, she explains, with the way in which different people’s ears will interpret sound differently due to their individual physiology. “We did a lot of experimentation, weaving tinfoil through [a double bass] to give it more distortion, slapping it and hitting it. I was just desperate to get into the ear. There’s a bit of, I don’t know if it’s the right word, but magic to how the ear functions, so it was nice to include instruments that felt surreal.”</p>




<figure><img decoding="async" width="1024" height="769" src="https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1024x769.jpg" alt="A black and white photo of Lola De La Mata with an experimental instrument" srcset="https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1024x769.jpg 1024w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-300x225.jpg 300w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-768x577.jpg 768w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1536x1154.jpg 1536w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px" loading="lazy" fetchpriority="low"></figure>



<p>Hudspeth was fascinated, and so within a couple of months of De La Mata’s reaching out, he invited her to join him in New York, which she did so thanks to an Arts Council grant. “I had already been meeting with instrument makers, so I already had some prototypes, so by the time I approached them I had something to come along with and discuss,” she continues. “They were desperate to show me all the things they’d built, these little studios, these mini-anechoic chambers, towers and towers of equipment.” One room contained zebrafish, a species who hear using hairs on their backs that are almost identical to the hairs on a human cochlea, and who are born translucent so that scientists can observe their nerves’ growth first hand. Elsewhere, the scientists showed her the 3D printed chambers they were developing to recreate the environment of a cochlea, “which was interesting because I’d been 3D printing all of my weird stuff. There was a lot of overlap.”</p>



<p>De La Mata had things to offer in return, pointing out how they might make use of soundproof rooms to deal with the noise produced by the fans of their machinery. Thanks to her experiments with an electromagnetic field recorder, which highlighted how noisy the lab could get, they were able to ascertain where previous experiments had run into issues. In turn, it also altered her view of science. “I’ve learned that it’s all about failure, and that they welcome it. People would open discussions by saying, ‘We have no idea how this thing works, we’re not there and we might never be there.’ And just as there’s a lot of failure in science, there’s a lot of failure in our communication, which actually allows spaces for questions, space to dream and innovate. So there was this common shared language, but we just had slightly different definitions of what it all meant, which was exciting. The last time I went to the lab, I met the head of the Paris branch of research at the Curie Institute that looks at hair cells and hair cell growth, and the way he’s studying it is by using etching techniques. In New York, they’re using photographic techniques as a way of measuring the movement of a hair cell. There’s no sure way or absolute truth to anything that they do, it’s all about experimentation and improvisation, which is not so different to what I’m doing.”&nbsp;</p>



<p>The most mind-blowing moment, not only for De La Mata but the scientists too, came when they managed to actually record the sounds that she heard in her ears – which now appear as ‘Left Ear’ and ‘Right Ear’ which begin sides A and B on the album – and in doing so opened up questions about the nature of tinnitus itself. “The NHS definition is that it’s a phantom sound that your brain is creating, that it isn’t something ‘real’, so you should try to ignore it.” By having De La Mata place her ear into an anechoic chamber, with an ultra-sensitive microphone perched in her ear canal, they were able to provide significant evidence to the contrary. “After the first recording of it, it was ‘There’s no way, this isn’t possible.’” They tried again with her breath held, and again with her tensing her ears, and again with other members of staff, but each time it became apparent that yes, the noises De La Mata hears are seemingly something physical. More intriguingly still, the two women whose ears were recorded, De La Mata and Lana Norris – the musicologist whose voice appears on the album’s ‘PINK Noise’, and who is also a choral director – were the only two people whose ears were found to produce spontaneous otoacoustic emissions. “It’s something to do with hormone difference, but they don’t really know why,” De La Mata says. Present in most children but believed to fade over time, they’re also found far more in musicians than in other adults, for reasons yet unknown. It all raises a lot of questions. “What I have is tinnitus by the definition we have now, but maybe that’s not correct. Maybe it’s something else,” De La Mata wonders aloud.&nbsp;</p>







<p>Given the scale of the unanswered questions, De La Mata has started a creative practice PhD to take things even further. For now, however, we have <em>Oceans On Azimuth</em>, a record that not only distils the complexity and nuance of its subject into an immediate and engaging listen, but also represents a major breach in the stigma that still surrounds hearing loss in the musical community. “If you’re told you have tinnitus, you’re often then told, ‘It’s the end of your career, you can’t be a musician anymore’, or ‘You’re not at optimal capacity’. There’s always a fear when you feel like you’re diverging from the normal with a body part or with health, or that you’re hearing something ‘wrong’. It’s something I experienced going to tinnitus support groups. Some people there were talking about tinnitus for the first time with someone other than themselves. There were people who had lost their job, who couldn’t see friends because they couldn’t deal with having any other noise. There were people who were suicidal.”&nbsp;</p>



<p><em>Oceans On Azimuth</em>, however, offers an alternative way of viewing things: an embrace of difference and subjectivity and a rejection of a binary idea of ‘good’ and ‘bad’ ears. “It was interesting to have that other way of thinking about it. The more I speak to biophysicists, the more I’m learning that although you create something very crisp and perfect when you’re thinking about the purity of sound, your ear will still always add distortion to it.”&nbsp; Her tinnitus is retriggered all the time by the reality of everyday life, but now, she says, “I’ve learnt to accept it.”</p>



<p><em><a href="https://loladelamata.bandcamp.com/album/oceans-on-azimuth">Lola De La Mata’s new album Oceans On Azimuth is out now</a></em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let rand = main as usize (2022) (134 pts)]]></title>
            <link>https://codeandbitters.com/main-as-usize/</link>
            <guid>40600384</guid>
            <pubDate>Thu, 06 Jun 2024 17:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeandbitters.com/main-as-usize/">https://codeandbitters.com/main-as-usize/</a>, See on <a href="https://news.ycombinator.com/item?id=40600384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                <p>The other day I made a joke on twitter, and learned some interesting things about raw pointers in Rust.</p>
<p>The abridged joke goes something like this:</p>
<blockquote>
<p>Yosh: What do you mean Rust doesn't ship with <code>rand</code> built-in?</p>
</blockquote>
<blockquote>
<p>Me: ASLR to the rescue!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
</blockquote>
<p><img src="https://codeandbitters.com/images/rust_logo_hand_drawn_1b.png" alt="Rust Logo" width="320" height="256"></p>
<h4 id="part-1-explaining-the-joke">Part 1: Explaining the joke<a href="#part-1-explaining-the-joke"> §</a>
</h4>
<p>Explaining the joke is bad form, but there is some valuable technical detail here.</p>
<p>The most important thing is ASLR (Address Space Layout Randomization). When software has memory safety bugs like buffer overflows, it's easy for an attacker to blast hostile data into a process's stack. That hostile data could replace the address that the currently running function will return to, giving the attacker the ability to execute some arbitrary instructions.</p>
<p>This would be bad. One of the mitigations that engineers came up with is to have each program execute at a randomized virtual address, to make such attacks harder. It's debatable whether this is effective at turning away attacks, but that's the goal, and ASLR is enabled on almost every operating system in use today.</p>
<p>Let me annotate my joke program a little bit:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// By calling this "rand", I'm pretending that this line of code is
</span><span>    </span><span>// a random number generator.
</span><span>    </span><span>let</span><span> rand = main as </span><span>usize</span><span>;
</span><span>    </span><span>//          ^        ^
</span><span>    </span><span>//          |        \-- that address as a pointer-sized integer
</span><span>    </span><span>//          |
</span><span>    </span><span>//          \-- the address of the main function
</span><span>
</span><span>    dbg!(rand); </span><span>// print out the result
</span><span>}
</span></code></pre>
<p>This program assumes that ASLR is used by the host operating system, which caused the program to run at a random address. Our program observes the address that <code>main</code> is located at, and uses that as our random value.</p>
<p>It's reasonable to wonder whether the address of <code>main</code> might be a static value, or that the Rust compiler might use some static address rather than recomputing at runtime due to ASLR. This isn't the case, though: experimentally we can verify that the value does change on each execution.</p>
<p>This is a dirty hack, and I don't recommend doing this in real programs. ASLR isn't a good random number generator. The address doesn't change that much, and under some conditions may not change at all. Even in the best circumstances, a program can only acquire one random value this way, so two different modules both using this trick would use the same value. Real random number generators are fast and readily available (at least, on any platform capable of using ASLR). Please use a well-regarded RNG istead of a hack like this.</p>
<h4 id="part-2-i-learn-things-about-rust-pointers">Part 2: I learn things about Rust pointers<a href="#part-2-i-learn-things-about-rust-pointers"> §</a>
</h4>
<p>The code I above isn't exactly what I posted on twitter. The original post looked like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>*const fn</span><span>() as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
<p>This wasn't well-written code. The two-step cast is just habit, because of situations like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>print_address</span><span>(</span><span>int_ref</span><span>: &amp;</span><span>u32</span><span>) {
</span><span>    </span><span>let</span><span> px = int_ref as </span><span>*const u32 </span><span>as </span><span>usize</span><span>;
</span><span>    dbg!(px);
</span><span>}
</span></code></pre>
<p>In many cases Rust won't allow us to cast a reference address directly to an integer; we have to go by way of a raw pointer.</p>
<p>Function pointers don't work the same way, though.</p>
<p>I didn't realize that there is no such thing as a "raw function pointer" in Rust. <code>fn()</code> is itself a pointer type, so <code>*const fn()</code> is a raw pointer to a function pointer, which doesn't make sense in this context.</p>
<p>Since there is no syntax for "raw function pointer", the compiler will let you substitute any other raw pointer type. Several tutorials use <code>foo as *const ()</code> (pointer to unit) to temporarily hold an untyped function pointer.</p>
<p>For those expecting to the usual Rust guard rails, it's surprising that the compiler allows casting between arbitrary raw pointer types outside of an <code>unsafe</code> block. This feels really dangerous— even though we can't do anything with the pointer outside of an <code>unsafe</code> block, creating a raw pointer usually implies that an unsafe block will eventually do something with it. I kind of wish that this pointer casting required <code>unsafe</code>, just because this code should send up red flags, and probably deserves a close look during code review.</p>
<p>I feel a little bad that while making a joke by doing something that's a little evil, I accidentally inserted a really evil cast that is highly misleading to the reader.</p>
<p>If I wanted to go all-in on the evil cast I could do something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>*const </span><span>rand::rngs::OsRng as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
<p>A few people pointed out that there are a few instances of programs that actually do try to harvest some randomness from the program's address. There's probably a place for a dirty hack like that, but I would feel a bit icky if I ever published code like that myself.</p>
<p>Thanks to @yoshuawuyts for setting up the joke, and to @eddyb for pointing out my pointer mistake, and everyone else who commented!</p>
<p>Comments? Get in touch on <a href="https://twitter.com/codeandbitters">twitter: @codeandbitters</a></p>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Extracting concepts from GPT-4 (361 pts)]]></title>
            <link>https://openai.com/index/extracting-concepts-from-gpt-4/</link>
            <guid>40599749</guid>
            <pubDate>Thu, 06 Jun 2024 17:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/extracting-concepts-from-gpt-4/">https://openai.com/index/extracting-concepts-from-gpt-4/</a>, See on <a href="https://news.ycombinator.com/item?id=40599749">Hacker News</a></p>
Couldn't get https://openai.com/index/extracting-concepts-from-gpt-4/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: XPipe, a brand-new type of remote file browser and shell connection hub (117 pts)]]></title>
            <link>https://xpipe.io</link>
            <guid>40599419</guid>
            <pubDate>Thu, 06 Jun 2024 16:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xpipe.io">https://xpipe.io</a>, See on <a href="https://news.ycombinator.com/item?id=40599419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><p>Manage all your servers from your local desktop. No remote setup required.</p></div><div><p><img alt="Connection hub" fetchpriority="high" loading="eager" width="1280" height="720" decoding="async" data-nimg="1" srcset="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbanner.png&amp;w=1920&amp;q=75 1x, https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbanner.png&amp;w=3840&amp;q=75 2x" src="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbanner.png&amp;w=3840&amp;q=75"></p></div></div><div><div><p><h2>Features</h2></p></div><div><div><p><img alt="File Manager" loading="lazy" width="1280" height="720" decoding="async" data-nimg="1" srcset="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbrowser.png&amp;w=1920&amp;q=100 1x, https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbrowser.png&amp;w=3840&amp;q=100 2x" src="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fbrowser.png&amp;w=3840&amp;q=100"></p></div><div><h2>XPipe quickly summarized</h2><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>A remote file browser that provides a workflow optimized for professionals</li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>A quick terminal launcher that can boot you into a shell session in your favorite terminal</li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>Complete SSH support which includes SSH configs, agent integration, tunnels, key files, and more</li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>Full support for various container runtimes like docker, podman, LXD, and more running remotely</li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>A versatile scripting system, allowing for custom shell scripts, init scripts, templates, and more</li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="none" width="20" height="20"><circle cx="12" cy="15" r="5"></circle></svg>Synchronization and sharing of your connection information via the git integration</li></ul></div></div></div><div><div><p><h2>Highlights</h2><h3>Explore what makes XPipe stand out.</h3></p></div><div><div><p><img alt="" loading="lazy" width="484" height="309" decoding="async" data-nimg="1" srcset="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard1.png&amp;w=640&amp;q=100 1x, https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard1.png&amp;w=1080&amp;q=100 2x" src="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard1.png&amp;w=1080&amp;q=100"></p><div><h5>Your tools first</h5><p>XPipe fully integrates with your toolbox of favourite applications like terminals, editors, git clients, and more. You can keep using them in conjunction with XPipe and don't have to adjust to anything new.</p></div></div><div><p><img alt="" loading="lazy" width="544" height="468" decoding="async" data-nimg="1" srcset="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard3.png&amp;w=640&amp;q=100 1x, https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard3.png&amp;w=1200&amp;q=100 2x" src="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard3.png&amp;w=1200&amp;q=100"></p><div><h5>Natively cross-platform</h5><p>XPipe is at home on every system. You have access to the same feature set everywhere without compromises. Native installers and portable archives for all operating systems are available.</p></div></div><div><p><img alt="" loading="lazy" width="486" height="398" decoding="async" data-nimg="1" srcset="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard2.png&amp;w=640&amp;q=100 1x, https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard2.png&amp;w=1080&amp;q=100 2x" src="https://xpipe.io/_next/image?url=%2Fassets%2Fimages%2FHomePage%2Fcard2.png&amp;w=1080&amp;q=100"></p><div><h5>Prioritized security</h5><p>All your data is stored securely on your local system. Additionally, XPipe can also integrate with your password manager to fetch secrets. All your vault data can also be synced via your own remote git repository.</p></div></div></div></div><div><p><h2>Showcase</h2><h3>XPipe summarized in clips</h3></p></div><div><div><p><h2>Demo</h2><h3>XPipe in action</h3></p></div><div><p><iframe src="https://www.youtube.com/embed/wjd3E0EN2xk" frameborder="0" allowfullscreen="" title="Embedded youtube"></iframe></p></div></div><div><p><h2>Frequently asked questions</h2><h3>Answers to the most common questions</h3></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen2 LLM Released (245 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwen2/</link>
            <guid>40599018</guid>
            <pubDate>Thu, 06 Jun 2024 16:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwen2/">https://qwenlm.github.io/blog/qwen2/</a>, See on <a href="https://news.ycombinator.com/item?id=40599018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen.jpg#center" width="100%"></figure><p><a href="https://github.com/QwenLM/Qwen2" target="_blank">GITHUB</a>
<a href="https://huggingface.co/Qwen" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/organization/qwen" target="_blank">MODELSCOPE</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><h2 id="introduction">Introduction</h2><p>After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:</p><ul><li>Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and <strong>Qwen2-72B</strong>;</li><li>Having been trained on data in <strong>27</strong> additional languages besides English and Chinese;</li><li>State-of-the-art performance in a large number of benchmark evaluations;</li><li>Significantly improved performance in coding and mathematics;</li><li>Extended context length support up to <strong>128K</strong> tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.</li></ul><p>We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!</p><h2 id="model-information">Model Information</h2><p>The Qwen2 series include base and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B. We illustrate the key information of the models in the following table:</p><table><thead><tr><th>Models</th><th>Qwen2-0.5B</th><th>Qwen2-1.5B</th><th>Qwen2-7B</th><th>Qwen2-57B-A14B</th><th>Qwen2-72B</th></tr></thead><tbody><tr><td># Params</td><td>0.49B</td><td>1.54B</td><td>7.07B</td><td>57.41B</td><td>72.71B</td></tr><tr><td># Non-Emb Params</td><td>0.35B</td><td>1.31B</td><td>5.98B</td><td>56.32B</td><td>70.21B</td></tr><tr><td>GQA</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>Tie Embedding</td><td>True</td><td>True</td><td>False</td><td>False</td><td>False</td></tr><tr><td>Context Length</td><td>32K</td><td>32K</td><td>128K</td><td>64K</td><td>128K</td></tr></tbody></table><p>Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.</p><p>In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation.
However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks.
In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle in a Haystack</a> task. Notably, when augmented with YARN, both Qwen2-7B-Instruct and Qwen2-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens.</p><p>Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:</p><table><thead><tr><th>Regions</th><th>Languages</th></tr></thead><tbody><tr><td>Western Europe</td><td>German, French, Spanish, Portuguese, Italian, Dutch</td></tr><tr><td>Eastern &amp; Central Europe</td><td>Russian, Czech, Polish</td></tr><tr><td>Middle East</td><td>Arabic, Persian, Hebrew, Turkish</td></tr><tr><td>Eastern Asia</td><td>Japanese, Korean</td></tr><tr><td>South-Eastern Asia</td><td>Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog</td></tr><tr><td>Southern Asia</td><td>Hindi, Bengali, Urdu</td></tr></tbody></table><p>Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models’ proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.</p><h2 id="performance">Performance</h2><p>Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model Qwen2-72B.
In terms of base language models, Qwen2-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities.
Benefiting from meticulously curated datasets and optimized training methods, Qwen2-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b.jpg#center" width="100%"></figure><p>After extensive large-scale pre-training, we conduct post-training to further enhance Qwen’s intelligence, bringing it closer to human. This process further improves the model’s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model’s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as <a href="https://arxiv.org/pdf/2308.01825">rejection sampling</a> for math, execution feedback for coding and instruction-following, back-translation for creative writing, <a href="https://arxiv.org/pdf/2401.12474">scalable oversight</a> for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel <a href="https://arxiv.org/pdf/2405.17931">Online Merging Optimizer</a> to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b-instruct.jpg#center" width="100%"></figure><p>We comprehensively evaluate Qwen2-72B-Instruct on 16 benchmarks across various domains. Qwen2-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, Qwen2-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.</p><p>In terms of smaller models, our Qwen2 models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, Qwen2-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-7b.jpg#center" width="100%"></figure><h2 id="highlights">Highlights</h2><h2 id="coding--mathematics">Coding &amp; Mathematics</h2><p>We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from <a href="https://qwenlm.github.io/blog/codeqwen1.5/">CodeQwen1.5</a>, resulting in significant improvements in Qwen2-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, Qwen2-72B-Instruct has reflects stronger capabilities in solving mathematic problems.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-code-math.jpg#center" width="100%"></figure><h2 id="long-context-understanding">Long Context Understanding</h2><p>In Qwen2, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like <a href="https://arxiv.org/abs/2309.00071">YARN</a> or <a href="https://arxiv.org/abs/2402.17463">Dual Chunk Attention</a>.</p><p>The figure below shows our test results on the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle in a Haystack</a>. Notably, Qwen2-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.</p><p>Additionally, it’s worth noting the impressive capabilities of other models in the series: Qwen2-7B-Instruct nearly flawlessly handles contexts up to 128k in length, Qwen2-57B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.</p><p>Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see <a href="https://qwenlm.github.io/blog/qwen-agent-2405/">our dedicated blog post on this topic</a>.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2_needle_in_haystack.png#center" width="100%"></figure><h2 id="safety-and-responsibility">Safety and Responsibility</h2><p>The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from <a href="https://github.com/verazuo/jailbreak_llms/tree/main">Jailbreak</a> and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the Qwen2-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.</p><table><thead><tr><th>Language</th><th></th><th>Illegal Activity</th><th></th><th></th><th>Fraud</th><th></th><th></th><th>Pornography</th><th></th><th></th><th>Privacy Violence</th><th></th></tr></thead><tbody><tr><td></td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td></tr><tr><td>zh</td><td><strong>0%</strong></td><td>13%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>17%</td><td><strong>0%</strong></td><td><strong>43%</strong></td><td>47%</td><td>53%</td><td><strong>0%</strong></td><td>10%</td><td><strong>0%</strong></td></tr><tr><td>en</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>23%</td><td><strong>0%</strong></td><td><strong>37%</strong></td><td>67%</td><td>63%</td><td><strong>0%</strong></td><td>27%</td><td>3%</td></tr><tr><td>ar</td><td><strong>0%</strong></td><td>13%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>15%</strong></td><td>26%</td><td><strong>15%</strong></td><td>3%</td><td>13%</td><td><strong>0%</strong></td></tr><tr><td>es</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td>3%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td><strong>48%</strong></td><td>64%</td><td>50%</td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td></tr><tr><td>fr</td><td><strong>0%</strong></td><td>3%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td><td>19%</td><td>7%</td><td><strong>0%</strong></td><td>27%</td><td><strong>0%</strong></td></tr><tr><td>ko</td><td><strong>0%</strong></td><td>4%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td>8%</td><td>4%</td><td>17%</td><td>29%</td><td><strong>10%</strong></td><td><strong>0%</strong></td><td>26%</td><td>4%</td></tr><tr><td>pt</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td><td><strong>47%</strong></td><td>57%</td><td><strong>47%</strong></td><td><strong>4%</strong></td><td>26%</td><td><strong>4%</strong></td></tr><tr><td>th</td><td><strong>0%</strong></td><td>10%</td><td><strong>0%</strong></td><td>7%</td><td>23%</td><td><strong>3%</strong></td><td>13%</td><td>17%</td><td><strong>10%</strong></td><td>13%</td><td><strong>7%</strong></td><td><strong>7%</strong></td></tr><tr><td>vi</td><td><strong>0%</strong></td><td>4%</td><td><strong>0%</strong></td><td>4%</td><td>11%</td><td><strong>0%</strong></td><td><strong>22%</strong></td><td>26%</td><td><strong>22%</strong></td><td><strong>0%</strong></td><td><strong>0%</strong></td><td><strong>0%</strong></td></tr><tr><td>Average</td><td><strong>0%</strong></td><td>8%</td><td><strong>0%</strong></td><td>3%</td><td>11%</td><td><strong>2%</strong></td><td><strong>27%</strong></td><td>39%</td><td>31%</td><td>3%</td><td>16%</td><td><strong>2%</strong></td></tr></tbody></table><h2 id="developing-with-qwen2">Developing with Qwen2</h2><p>Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.</p><p>For a long time, a lot of friends have been supporting the development of Qwen, including finetuning (<a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a>, <a href="https://github.com/hiyouga/LLaMA-Factory">Llama-Factory</a>, <a href="https://github.com/yangjianxin1/Firefly">Firefly</a>, <a href="https://github.com/modelscope/swift">Swift</a>, <a href="https://github.com/InternLM/xtuner">XTuner</a>), quantization (<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>, <a href="https://github.com/casper-hansen/AutoAWQ">AutoAWQ</a>, <a href="https://github.com/intel/neural-compressor">Neural Compressor</a>), deployment (<a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/sgl-project/sglang">SGL</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, <a href="https://github.com/openvinotoolkit/openvino">OpenVino</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a>), API platforms (<a href="https://www.together.ai/">Together</a>, <a href="https://fireworks.ai/">Fireworks</a>, <a href="https://openrouter.ai/">OpenRouter</a>), local run (<a href="https://github.com/ml-explore/mlx">MLX</a>, <a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a>, <a href="https://ollama.com/">Ollama</a>, <a href="https://lmstudio.ai/">LM Studio</a>), Agent and RAG Frameworks (<a href="https://www.llamaindex.ai/">LlamaIndex</a>, <a href="https://www.crewai.com/">CrewAI</a>, <a href="https://github.com/OpenDevin/OpenDevin/">OpenDevin</a>) , Evaluation (<a href="https://chat.lmsys.org/">LMSys</a>, <a href="https://opencompass.org.cn/home">OpenCompass</a>, <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>), model training (<a href="https://huggingface.co/cognitivecomputations">Dolphin</a>, <a href="https://github.com/OpenBuddy/OpenBuddy">Openbuddy</a>) etc. For how to use Qwen2 with the third-party frameworks, please refer to the respective documentation as well as our <a href="https://qwen.readthedocs.io/en/latest/">official documentation</a>.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/logo-v3.jpg#center" width="80%"></figure><p>Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.</p><h2 id="license">License</h2><p>This time, we change the licenses of our models to different ones. While Qwen2-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, and Qwen2-57B-A14B, turn to adopt <strong>Apache 2.0</strong>! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of Qwen2 all around the world.</p><h2 id="whats-next-for-qwen2">What’s Next for Qwen2?</h2><p>We are training larger Qwen2 models to further explore model scaling along with our recent data scaling. Additionally, we extend the Qwen2 language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!</p><h2 id="citation">Citation</h2><p>We are going to release the technical report for Qwen2 very soon. Feel free to give us a cite!</p><pre tabindex="0"><code>@article{qwen2,
  title={Qwen2 Technical Report},
  year={2024}
}
</code></pre><h2 id="appendix">Appendix</h2><h2 id="base-language-model-evaluation">Base Language Model Evaluation</h2><p>The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.</p><p>The datasets for evaluation include:</p><p><strong>English Tasks</strong>: MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)</p><p><strong>Coding Tasks</strong>: EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p><p><strong>Math Tasks</strong>: GSM8K (4-shot), MATH (4-shot)</p><p><strong>Chinese Tasks</strong>: C-Eval(5-shot), CMMLU (5-shot)</p><p><strong>Multilingual Tasks</strong>: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p><h3 id="qwen2-72b-performance">Qwen2-72B performance</h3><table><thead><tr><th>Datasets</th><th>DeepSeek-V2</th><th>Mixtral-8x22B</th><th>Llama-3-70B</th><th>Qwen1.5-72B</th><th>Qwen1.5-110B</th><th><strong>Qwen2-72B</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td>#Activated Params</td><td>21B</td><td>39B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td>#Params</td><td>236B</td><td>140B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>78.5</td><td>77.8</td><td>79.5</td><td>77.5</td><td>80.4</td><td><strong>84.2</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>49.5</td><td>52.8</td><td>45.8</td><td>49.4</td><td><strong>55.6</strong></td></tr><tr><td>GPQA</td><td>-</td><td>34.3</td><td>36.3</td><td>36.3</td><td>35.9</td><td><strong>37.9</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>35.9</td><td>32.3</td><td>29.3</td><td>34.9</td><td><strong>43.1</strong></td></tr><tr><td>BBH</td><td>78.9</td><td>78.9</td><td>81.0</td><td>65.5</td><td>74.8</td><td><strong>82.4</strong></td></tr><tr><td>HellaSwag</td><td>87.8</td><td><strong>88.7</strong></td><td>88.0</td><td>86.0</td><td>87.5</td><td>87.6</td></tr><tr><td>WindoGrande</td><td>84.8</td><td>85.0</td><td><strong>85.3</strong></td><td>83.0</td><td>83.5</td><td>85.1</td></tr><tr><td>ARC-C</td><td>70.0</td><td><strong>70.7</strong></td><td>68.8</td><td>65.9</td><td>69.6</td><td>68.9</td></tr><tr><td>TruthfulQA</td><td>42.2</td><td>51.0</td><td>45.6</td><td><strong>59.6</strong></td><td>49.6</td><td>54.8</td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.7</td><td>46.3</td><td>48.2</td><td>46.3</td><td>54.3</td><td><strong>64.6</strong></td></tr><tr><td>MBPP</td><td>73.9</td><td>71.7</td><td>70.4</td><td>66.9</td><td>70.9</td><td><strong>76.9</strong></td></tr><tr><td>EvalPlus</td><td>55.0</td><td>54.1</td><td>54.8</td><td>52.9</td><td>57.7</td><td><strong>65.4</strong></td></tr><tr><td>MultiPL-E</td><td>44.4</td><td>46.7</td><td>46.3</td><td>41.8</td><td>52.7</td><td><strong>59.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.2</td><td>83.7</td><td>83.0</td><td>79.5</td><td>85.4</td><td><strong>89.5</strong></td></tr><tr><td>MATH</td><td>43.6</td><td>41.7</td><td>42.5</td><td>34.1</td><td>49.6</td><td><strong>51.1</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>81.7</td><td>54.6</td><td>65.2</td><td>84.1</td><td>89.1</td><td><strong>91.0</strong></td></tr><tr><td>CMMLU</td><td>84.0</td><td>53.4</td><td>67.2</td><td>83.5</td><td>88.3</td><td><strong>90.1</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mulit-Exam</td><td>67.5</td><td>63.5</td><td>70.0</td><td>66.4</td><td>75.6</td><td><strong>76.6</strong></td></tr><tr><td>Multi-Understanding</td><td>77.0</td><td>77.7</td><td>79.9</td><td>78.2</td><td>78.2</td><td><strong>80.7</strong></td></tr><tr><td>Multi-Mathematics</td><td>58.8</td><td>62.9</td><td>67.1</td><td>61.7</td><td>64.4</td><td><strong>76.0</strong></td></tr><tr><td>Multi-Translation</td><td>36.0</td><td>23.3</td><td><strong>38.0</strong></td><td>35.6</td><td>36.2</td><td>37.8</td></tr></tbody></table><h3 id="qwen2-57b-a14b">Qwen2-57B-A14B</h3><table><thead><tr><th>Datasets</th><th>Jamba</th><th>Mixtral-8x7B</th><th>Yi-1.5-34B</th><th>Qwen1.5-32B</th><th><strong><strong>Qwen2-57B-A14B</strong></strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>52B</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>67.4</td><td>71.8</td><td><strong>77.1</strong></td><td>74.3</td><td>76.5</td></tr><tr><td>MMLU-Pro</td><td>-</td><td>41.0</td><td><strong>48.3</strong></td><td>44.0</td><td>43.0</td></tr><tr><td>GPQA</td><td>-</td><td>29.2</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>23.2</td><td>-</td><td>28.8</td><td><strong>33.5</strong></td></tr><tr><td>BBH</td><td>45.4</td><td>50.3</td><td><strong>76.4</strong></td><td>66.8</td><td>67.0</td></tr><tr><td>HellaSwag</td><td><strong>87.1</strong></td><td>86.5</td><td>85.9</td><td>85.0</td><td>85.2</td></tr><tr><td>Winogrande</td><td>82.5</td><td>81.9</td><td><strong>84.9</strong></td><td>81.5</td><td>79.5</td></tr><tr><td>ARC-C</td><td>64.4</td><td><strong>66.0</strong></td><td>65.6</td><td>63.6</td><td>64.1</td></tr><tr><td>TruthfulQA</td><td>46.4</td><td>51.1</td><td>53.9</td><td>57.4</td><td><strong>57.7</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>37.2</td><td>46.3</td><td>43.3</td><td><strong>53.0</strong></td></tr><tr><td>MBPP</td><td>-</td><td>63.9</td><td>65.5</td><td>64.2</td><td><strong>71.9</strong></td></tr><tr><td>EvalPlus</td><td>-</td><td>46.4</td><td>51.9</td><td>50.4</td><td><strong>57.2</strong></td></tr><tr><td>MultiPL-E</td><td>-</td><td>39.0</td><td>39.5</td><td>38.5</td><td><strong>49.8</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>59.9</td><td>62.5</td><td><strong>82.7</strong></td><td>76.8</td><td>80.7</td></tr><tr><td>MATH</td><td>-</td><td>30.8</td><td>41.7</td><td>36.1</td><td><strong>43.0</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>-</td><td>83.5</td><td><strong>87.7</strong></td></tr><tr><td>CMMLU</td><td>-</td><td>-</td><td>84.8</td><td>82.3</td><td><strong>88.5</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>-</td><td>56.1</td><td>58.3</td><td>61.6</td><td><strong>65.5</strong></td></tr><tr><td>Multi-Understanding</td><td>-</td><td>70.7</td><td>73.9</td><td>76.5</td><td><strong>77.0</strong></td></tr><tr><td>Multi-Mathematics</td><td>-</td><td>45.0</td><td>49.3</td><td>56.1</td><td><strong>62.3</strong></td></tr><tr><td>Multi-Translation</td><td>-</td><td>29.8</td><td>30.0</td><td>33.5</td><td><strong>34.5</strong></td></tr></tbody></table><h3 id="qwen2-7b">Qwen2-7B</h3><table><thead><tr><th>Datasets</th><th>Mistral-7B</th><th>Gemma-7B</th><th>Llama-3-8B</th><th>Qwen1.5-7B</th><th>Qwen2-7B</th></tr></thead><tbody><tr><td># Params</td><td>7.2B</td><td>8.5B</td><td>8.0B</td><td>7.7B</td><td>7.6B</td></tr><tr><td># Non-emb Params</td><td>7.0B</td><td>7.8B</td><td>7.0B</td><td>6.5B</td><td>6.5B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>64.2</td><td>64.6</td><td>66.6</td><td>61.0</td><td><strong>70.3</strong></td></tr><tr><td>MMLU-Pro</td><td>30.9</td><td>33.7</td><td>35.4</td><td>29.9</td><td><strong>40.0</strong></td></tr><tr><td>GPQA</td><td>24.7</td><td>25.7</td><td>25.8</td><td>26.7</td><td><strong>31.8</strong></td></tr><tr><td>Theorem QA</td><td>19.2</td><td>21.5</td><td>22.1</td><td>14.2</td><td><strong>31.1</strong></td></tr><tr><td>BBH</td><td>56.1</td><td>55.1</td><td>57.7</td><td>40.2</td><td><strong>62.6</strong></td></tr><tr><td>HellaSwag</td><td><strong>83.2</strong></td><td>82.2</td><td>82.1</td><td>78.5</td><td>80.7</td></tr><tr><td>Winogrande</td><td>78.4</td><td><strong>79.0</strong></td><td>77.4</td><td>71.3</td><td>77.0</td></tr><tr><td>ARC-C</td><td>60.0</td><td><strong>61.1</strong></td><td>59.3</td><td>54.2</td><td>60.6</td></tr><tr><td>TruthfulQA</td><td>42.2</td><td>44.8</td><td>44.0</td><td>51.1</td><td><strong>54.2</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>37.2</td><td>33.5</td><td>36.0</td><td><strong>51.2</strong></td></tr><tr><td>MBPP</td><td>51.1</td><td>50.6</td><td>53.9</td><td>51.6</td><td><strong>65.9</strong></td></tr><tr><td>EvalPlus</td><td>36.4</td><td>39.6</td><td>40.3</td><td>40.0</td><td><strong>54.2</strong></td></tr><tr><td>MultiPL-E</td><td>29.4</td><td>29.7</td><td>22.6</td><td>28.1</td><td><strong>46.3</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>52.2</td><td>46.4</td><td>56.0</td><td>62.5</td><td><strong>79.9</strong></td></tr><tr><td>MATH</td><td>13.1</td><td>24.3</td><td>20.5</td><td>20.3</td><td><strong>44.2</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>47.4</td><td>43.6</td><td>49.5</td><td>74.1</td><td><strong>83.2</strong></td></tr><tr><td>CMMLU</td><td>-</td><td>-</td><td>50.8</td><td>73.1</td><td><strong>83.9</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>47.1</td><td>42.7</td><td>52.3</td><td>47.7</td><td><strong>59.2</strong></td></tr><tr><td>Multi-Understanding</td><td>63.3</td><td>58.3</td><td>68.6</td><td>67.6</td><td><strong>72.0</strong></td></tr><tr><td>Multi-Mathematics</td><td>26.3</td><td>39.1</td><td>36.3</td><td>37.3</td><td><strong>57.5</strong></td></tr><tr><td>Multi-Translation</td><td>23.3</td><td>31.2</td><td><strong>31.9</strong></td><td>28.4</td><td>31.5</td></tr></tbody></table><h3 id="qwen2-05b--qwen2-15b">Qwen2-0.5B &amp; Qwen2-1.5B</h3><table><thead><tr><th>Datasets</th><th>Phi-2</th><th>Gemma-2B</th><th>MiniCPM</th><th>Qwen1.5-1.8B</th><th>Qwen2-0.5B</th><th>Qwen2-1.5B</th></tr></thead><tbody><tr><td>#Non-Emb Params</td><td>2.5B</td><td>2.0B</td><td>2.4B</td><td>1.3B</td><td>0.35B</td><td>1.3B</td></tr><tr><td>MMLU</td><td>52.7</td><td>42.3</td><td>53.5</td><td>46.8</td><td>45.4</td><td><strong>56.5</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>15.9</td><td>-</td><td>-</td><td>14.7</td><td>21.8</td></tr><tr><td>Theorem QA</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.9</td><td><strong>15.0</strong></td></tr><tr><td>HumanEval</td><td>47.6</td><td>22.0</td><td><strong>50.0</strong></td><td>20.1</td><td>22.0</td><td>31.1</td></tr><tr><td>MBPP</td><td><strong>55.0</strong></td><td>29.2</td><td>47.3</td><td>18.0</td><td>22.0</td><td>37.4</td></tr><tr><td>GSM8K</td><td>57.2</td><td>17.7</td><td>53.8</td><td>38.4</td><td>36.5</td><td><strong>58.5</strong></td></tr><tr><td>MATH</td><td>3.5</td><td>11.8</td><td>10.2</td><td>10.1</td><td>10.7</td><td><strong>21.7</strong></td></tr><tr><td>BBH</td><td><strong>43.4</strong></td><td>35.2</td><td>36.9</td><td>24.2</td><td>28.4</td><td>37.2</td></tr><tr><td>HellaSwag</td><td><strong>73.1</strong></td><td>71.4</td><td>68.3</td><td>61.4</td><td>49.3</td><td>66.6</td></tr><tr><td>Winogrande</td><td><strong>74.4</strong></td><td>66.8</td><td>-</td><td>60.3</td><td>56.8</td><td>66.2</td></tr><tr><td>ARC-C</td><td><strong>61.1</strong></td><td>48.5</td><td>-</td><td>37.9</td><td>31.5</td><td>43.9</td></tr><tr><td>TruthfulQA</td><td>44.5</td><td>33.1</td><td>-</td><td>39.4</td><td>39.7</td><td><strong>45.9</strong></td></tr><tr><td>C-Eval</td><td>23.4</td><td>28.0</td><td>51.1</td><td>59.7</td><td>58.2</td><td><strong>70.6</strong></td></tr><tr><td>CMMLU</td><td>24.2</td><td>-</td><td>51.1</td><td>57.8</td><td>55.1</td><td><strong>70.3</strong></td></tr></tbody></table><h2 id="instruction-tuned-model-evaluation">Instruction-tuned Model Evaluation</h2><h3 id="qwen2-72b-instruct">Qwen2-72B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Llama-3-70B-Instruct</th><th>Qwen1.5-72B-Chat</th><th><strong>Qwen2-72B-Instruct</strong></th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>82.0</td><td>75.6</td><td><strong>82.3</strong></td></tr><tr><td>MMLU-Pro</td><td>56.2</td><td>51.7</td><td><strong>64.4</strong></td></tr><tr><td>GPQA</td><td>41.9</td><td>39.4</td><td><strong>42.4</strong></td></tr><tr><td>TheroemQA</td><td>42.5</td><td>28.8</td><td><strong>44.4</strong></td></tr><tr><td>MT-Bench</td><td>8.95</td><td>8.61</td><td><strong>9.12</strong></td></tr><tr><td>Arena-Hard</td><td>41.1</td><td>36.1</td><td><strong>48.1</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>77.3</td><td>55.8</td><td><strong>77.6</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>81.7</td><td>71.3</td><td><strong>86.0</strong></td></tr><tr><td>MBPP</td><td><strong>82.3</strong></td><td>71.9</td><td>80.2</td></tr><tr><td>MultiPL-E</td><td>63.4</td><td>48.1</td><td><strong>69.2</strong></td></tr><tr><td>EvalPlus</td><td>75.2</td><td>66.9</td><td><strong>79.0</strong></td></tr><tr><td>LiveCodeBench</td><td>29.3</td><td>17.9</td><td><strong>35.7</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td><strong>93.0</strong></td><td>82.7</td><td>91.1</td></tr><tr><td>MATH</td><td>50.4</td><td>42.5</td><td><strong>59.7</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>61.6</td><td>76.1</td><td><strong>83.8</strong></td></tr><tr><td>AlignBench</td><td>7.42</td><td>7.28</td><td><strong>8.27</strong></td></tr></tbody></table><h3 id="qwen2-57b-a14b-instruct">Qwen2-57B-A14B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Mixtral-8x7B-Instruct-v0.1</th><th>Yi-1.5-34B-Chat</th><th>Qwen1.5-32B-Chat</th><th><strong>Qwen2-57B-A14B-Instruct</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>71.4</td><td><strong>76.8</strong></td><td>74.8</td><td>75.4</td></tr><tr><td>MMLU-Pro</td><td>43.3</td><td>52.3</td><td>46.4</td><td><strong>52.8</strong></td></tr><tr><td>GPQA</td><td>-</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>TheroemQA</td><td>-</td><td>-</td><td>30.9</td><td><strong>33.1</strong></td></tr><tr><td>MT-Bench</td><td>8.30</td><td>8.50</td><td>8.30</td><td><strong>8.55</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.1</td><td>75.2</td><td>68.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td>59.5</td><td><strong>74.6</strong></td><td>67.9</td><td>70.9</td></tr><tr><td>MultiPL-E</td><td>-</td><td>-</td><td>50.7</td><td><strong>66.4</strong></td></tr><tr><td>EvalPlus</td><td>48.5</td><td>-</td><td>63.6</td><td><strong>71.6</strong></td></tr><tr><td>LiveCodeBench</td><td>12.3</td><td>-</td><td>15.2</td><td><strong>25.5</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>65.7</td><td><strong>90.2</strong></td><td>83.6</td><td>79.6</td></tr><tr><td>MATH</td><td>30.7</td><td><strong>50.1</strong></td><td>42.4</td><td>49.1</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>76.7</td><td>80.5</td></tr><tr><td>AlignBench</td><td>5.70</td><td>7.20</td><td>7.19</td><td><strong>7.36</strong></td></tr></tbody></table><h3 id="qwen2-7b-instruct">Qwen2-7B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Llama-3-8B-Instruct</th><th>Yi-1.5-9B-Chat</th><th>GLM-4-9B-Chat</th><th>Qwen1.5-7B-Chat</th><th>Qwen2-7B-Instruct</th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>68.4</td><td>69.5</td><td><strong>72.4</strong></td><td>59.5</td><td>70.5</td></tr><tr><td>MMLU-Pro</td><td>41.0</td><td>-</td><td>-</td><td>29.1</td><td><strong>44.1</strong></td></tr><tr><td>GPQA</td><td><strong>34.2</strong></td><td>-</td><td><strong>-</strong></td><td>27.8</td><td>25.3</td></tr><tr><td>TheroemQA</td><td>23.0</td><td>-</td><td>-</td><td>14.1</td><td><strong>25.3</strong></td></tr><tr><td>MT-Bench</td><td>8.05</td><td>8.20</td><td>8.35</td><td>7.60</td><td><strong>8.41</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Humaneval</td><td>62.2</td><td>66.5</td><td>71.8</td><td>46.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td><strong>67.9</strong></td><td>-</td><td>-</td><td>48.9</td><td>67.2</td></tr><tr><td>MultiPL-E</td><td>48.5</td><td>-</td><td>-</td><td>27.2</td><td><strong>59.1</strong></td></tr><tr><td>Evalplus</td><td>60.9</td><td>-</td><td>-</td><td>44.8</td><td><strong>70.3</strong></td></tr><tr><td>LiveCodeBench</td><td>17.3</td><td>-</td><td>-</td><td>6.0</td><td><strong>26.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.6</td><td><strong>84.8</strong></td><td>79.6</td><td>60.3</td><td>82.3</td></tr><tr><td>MATH</td><td>30.0</td><td>47.7</td><td><strong>50.6</strong></td><td>23.2</td><td>49.6</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>45.9</td><td>-</td><td>75.6</td><td>67.3</td><td><strong>77.2</strong></td></tr><tr><td>AlignBench</td><td>6.20</td><td>6.90</td><td>7.01</td><td>6.20</td><td><strong>7.21</strong></td></tr></tbody></table><h3 id="qwen2-05b-instruct--qwen2-15b-instruct">Qwen2-0.5B-Instruct &amp; Qwen2-1.5B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Qwen1.5-0.5B-Chat</th><th><strong>Qwen2-0.5B-Instruct</strong></th><th>Qwen1.5-1.8B-Chat</th><th><strong>Qwen2-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU</td><td>35.0</td><td><strong>37.9</strong></td><td>43.7</td><td><strong>52.4</strong></td></tr><tr><td>HumanEval</td><td>9.1</td><td><strong>17.1</strong></td><td>25.0</td><td><strong>37.8</strong></td></tr><tr><td>GSM8K</td><td>11.3</td><td><strong>40.1</strong></td><td>35.3</td><td><strong>61.6</strong></td></tr><tr><td>C-Eval</td><td>37.2</td><td><strong>45.2</strong></td><td>55.3</td><td><strong>63.8</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>14.6</td><td><strong>20.0</strong></td><td>16.8</td><td><strong>29.0</strong></td></tr></tbody></table><h2 id="multilingual-capability-of-instruction-tuned-models">Multilingual capability of instruction-tuned models</h2><p>We compare Qwen2 instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:</p><ul><li><a href="https://github.com/nlp-uoregon/mlmm-evaluation">M-MMLU</a> from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh)</li><li><a href="https://arxiv.org/abs/2210.03057">MGSM</a>: math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn</li></ul><p>The results are averaged over languages for each benchmark and shown as follows:</p><table><thead><tr><th>Models</th><th>M-MMLU (5-shot)</th><th>MGSM (0-shot, CoT)</th></tr></thead><tbody><tr><td><strong><em>Proprietary LLMs</em></strong></td><td></td><td></td></tr><tr><td>GPT-4-0613</td><td>78.0</td><td>87.0</td></tr><tr><td>GPT-4-Turbo-0409</td><td>79.3</td><td>90.5</td></tr><tr><td>GPT-4o-0513</td><td>83.2</td><td>89.6</td></tr><tr><td>Claude-3-Opus-20240229</td><td>80.1</td><td>91.0</td></tr><tr><td>Claude-3-Sonnet-20240229</td><td>71.0</td><td>85.6</td></tr><tr><td><strong><em>Open-source LLMs</em></strong></td><td></td><td></td></tr><tr><td>command-r-plus-110b</td><td>65.5</td><td>63.5</td></tr><tr><td>Qwen1.5-7B-Chat</td><td>50.0</td><td>37.0</td></tr><tr><td>Qwen1.5-32B-Chat</td><td>65.0</td><td>65.0</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>68.4</td><td>71.7</td></tr><tr><td><strong>Qwen2-7B-Instruct</strong></td><td><strong>60.0</strong></td><td><strong>57.0</strong></td></tr><tr><td><strong>Qwen2-57B-A14B-Instruct</strong></td><td><strong>68.0</strong></td><td><strong>74.0</strong></td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td><strong>78.0</strong></td><td><strong>86.6</strong></td></tr></tbody></table><p>For human evaluation, we compare Qwen2-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):</p><table><thead><tr><th>Models</th><th>ar</th><th>es</th><th>fr</th><th>ko</th><th>th</th><th>vi</th><th>pt</th><th>id</th><th>ja</th><th>ru</th><th>Average</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>4.15</td><td>4.31</td><td>4.23</td><td>4.23</td><td>4.01</td><td>3.98</td><td>4.09</td><td>4.40</td><td>3.85</td><td>4.25</td><td>4.15</td></tr><tr><td>GPT-4o-0513</td><td>3.55</td><td>4.26</td><td>4.16</td><td>4.40</td><td>4.09</td><td>4.14</td><td>3.89</td><td>4.39</td><td>3.72</td><td>4.32</td><td>4.09</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.44</td><td>4.08</td><td>4.19</td><td>4.24</td><td>4.11</td><td>3.84</td><td>3.86</td><td>4.09</td><td>3.68</td><td>4.27</td><td>3.98</td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td>3.86</td><td>4.10</td><td>4.01</td><td>4.14</td><td>3.75</td><td>3.91</td><td>3.97</td><td>3.83</td><td>3.63</td><td>4.15</td><td>3.93</td></tr><tr><td>GPT-4-0613</td><td>3.55</td><td>3.92</td><td>3.94</td><td>3.87</td><td>3.83</td><td>3.95</td><td>3.55</td><td>3.77</td><td>3.06</td><td>3.63</td><td>3.71</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>2.52</td><td>4.07</td><td>3.47</td><td>2.37</td><td>3.38</td><td>2.90</td><td>3.37</td><td>3.56</td><td>2.75</td><td>3.24</td><td>3.16</td></tr></tbody></table><p>Grouped by task types, the results are shown as follows:</p><table><thead><tr><th>Models</th><th>Knowledge</th><th>Understanding</th><th>Creation</th><th>Math</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>3.64</td><td>4.45</td><td>4.42</td><td>3.81</td></tr><tr><td>GPT-4o-0513</td><td>3.76</td><td>4.35</td><td>4.45</td><td>3.53</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.42</td><td>4.29</td><td>4.35</td><td>3.58</td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td>3.41</td><td>4.07</td><td>4.36</td><td>3.61</td></tr><tr><td>GPT-4-0613</td><td>3.42</td><td>4.09</td><td>4.10</td><td>3.32</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>3.37</td><td>3.67</td><td>3.89</td><td>2.97</td></tr></tbody></table><p>These results demonstrate the strong multilingual capabilities of Qwen2 instruction-tuned models.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX's Starship rocket completes test flight, successfully splashes down (112 pts)]]></title>
            <link>https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html</link>
            <guid>40598833</guid>
            <pubDate>Thu, 06 Jun 2024 15:47:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html">https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html</a>, See on <a href="https://news.ycombinator.com/item?id=40598833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107425233" data-test="InlineImage"><p>SpaceX's Starship launches its fourth flight test from the company's Boca Chica launchpad, designed to eventually send astronauts to the moon and beyond, near Brownsville, Texas, U.S. , in this handout picture obtained on June 6, 2024.&nbsp;</p><p>Spacex | Via Reuters</p></div><div><p>SpaceX completed a test flight of its Starship rocket for the first time on Thursday, as the company pushed development of the mammoth vehicle past new milestones.</p><p>"Our first ever ship landing burn after a launch into space ... that was incredible," SpaceX communications manager Dan Huot said on the company's broadcast.</p><p><a href="https://www.cnbc.com/elon-musk/">Elon Musk's</a> company launched Starship at about 8:50 a.m. ET from its Starbase facility near Boca Chica, Texas.</p></div><div id="ArticleBody-InlineImage-107425169" data-test="InlineImage"><p>The SpaceX Starship launches during its fourth flight test from Boca Chica beach on June 06, 2024 in Brownsville, Texas.&nbsp;</p><p>Brandon Bell | Getty Images</p></div><div><p>A few minutes after launch, the rocket's booster successfully splashed down in the Gulf of Mexico, a new milestone for its development. This was the first time SpaceX returned the booster in one piece — a controlled return of the booster is key to the company's long-term goal of being able to launch and land Starship regularly, a practice it's made routine with its Falcon 9 rockets.</p><p>About an hour after the launch, Starship visibly survived reentry through the Earth's atmosphere, and the company confirmed that the rocket splashed down in the Indian Ocean to complete the mission. Starship appeared to withstand external damage during the intense heat of reentry, with debris apparent on the broadcast.</p><p>"Splashdown confirmed!" SpaceX posted on social media after the flight.</p><p>There were no people on board the fourth Starship spaceflight. The company's leadership has previously emphasized that SpaceX expects to fly hundreds of Starship missions before the rocket launches with any crew.</p></div><div id="ArticleBody-InlineImage-107425157" data-test="InlineImage"><p>The sun sets behind the SpaceX Starship ahead of its fourth flight test at Boca Chica beach on June 05, 2024 in Brownsville, Texas.&nbsp;</p><p>Brandon Bell | Getty Images</p></div><div><p>The Starship system is designed to be fully reusable and aims to become a new method of flying cargo and people beyond Earth. The rocket is also critical to NASA's plan to return astronauts to the moon. SpaceX won a multibillion-dollar contract from the agency to use Starship as a crewed lunar lander as part of NASA's Artemis moon program.</p><p>Shortly after the flight, NASA Administrator Bill Nelson congratulated SpaceX on its progress.</p><p>"We are another step closer to returning humanity to the Moon through Artemis — then looking onward to Mars," Nelson wrote in a social media post.</p><p>SpaceX has flown the full Starship rocket system on three spaceflight tests previously, with launches in <a href="https://www.cnbc.com/2023/04/20/spacex-starship-orbital-launch-attempt-live-updates.html">April 2023</a>, <a href="https://www.cnbc.com/2023/11/18/spacex-starship-second-rocket-launch.html">November</a> and <a href="https://www.cnbc.com/2024/03/14/spacex-starship-rocket-third-test-flight-launch.html">March</a>. Each of the test flights has achieved more milestones than the last, but in each result prior to Thursday the rocket was destroyed before the flight's end.</p><p>During the company's third test flight SpaceX tested new capabilities including opening and closing the payload door once in space —&nbsp;which would be how the rocket deploys payloads such as satellites on future missions — and&nbsp;<a href="https://www.cnbc.com/2023/12/05/spacex-plans-nasa-refueling-demonstration-for-next-starship-launch.html">transferring fuel&nbsp;</a>during the flight in a NASA demonstration.</p><p>SpaceX heavily emphasizes an approach of building "on what we've learned from previous flights" in its approach to develop Starship. The company says its strategy focuses on "recursive improvement" to the rocket, where even test flights with fiery outcomes represent progress toward its goal of a fully reusable rocket that can deliver people to the moon and Mars.</p><p>Musk last year said he expected the company to <a href="https://www.cnbc.com/2023/04/29/elon-musk-spacexs-starship-costing-about-2-billion-this-year.html">spend about $2 billion on Starship development</a> in 2023.</p></div><h2><a id="headline0"></a>The rocket</h2><div id="Placeholder-ArticleBody-Video-107029162" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000237329" aria-labelledby="Placeholder-ArticleBody-Video-107029162"><p><img src="https://image.cnbcfm.com/api/v1/image/107013976-1644535900772-S20_Chopstick_Stack_Desktop.jpg?v=1647016871&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Why Starship is indispensable for the future of SpaceX"><span></span><span></span></p></div><div><p>Starship is both the tallest and most powerful rocket ever launched. Fully stacked on the Super Heavy booster, Starship stands 397 feet tall and is about 30 feet in diameter.</p><p>The Super Heavy booster, which stands 232 feet tall, is what begins the rocket's journey to space. At its base are 33 Raptor engines, which together produce 16.7 million pounds of thrust – about double the 8.8 million pounds of thrust of NASA's Space Launch System rocket, which <a href="https://www.cnbc.com/2022/11/16/watch-live-nasa-launches-artemis-1-moon-mission.html">launched for the first time in 2022</a>.</p><p>Starship itself, at 165 feet tall, has six Raptor engines – three for use while in the Earth's atmosphere and three for operating in the vacuum of space.</p><p>The rocket is powered by liquid oxygen and liquid methane. The full system requires more than 10 million pounds of propellant for launch.</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>