<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Dec 2024 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Gemini 2.0: our new AI model for the agentic era (253 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</link>
            <guid>42388783</guid>
            <pubDate>Wed, 11 Dec 2024 15:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42388783">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  
  <div>
          
            <p>Dec 11, 2024</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
  
  <div data-summary-id="ai_summary_2" data-component="uni-ai-generated-summary" data-analytics-module="{
    &quot;event&quot;: &quot;module_impression&quot;,
    &quot;module_name&quot;: &quot;ai_summary&quot;,
    &quot;section_header&quot;: &quot;CTA&quot;
  }">
          <h2>Bullet points</h2>
          <ul>
<li>Google DeepMind introduces Gemini 2.0, a new AI model designed for the "agentic era."</li>
<li>Gemini 2.0 is more capable than previous versions, with native image and audio output and tool use.</li>
<li>Gemini 2.0 Flash is available to developers and trusted testers, with wider availability planned for early next year.</li>
<li>Google is exploring agentic experiences with Gemini 2.0, including Project Astra, Project Mariner, and Jules.</li>
<li>Google is committed to building AI responsibly, with safety and security as key priorities.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
</div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="Text &quot;Gemini 2.0&quot; in front of a futuristic blue and black abstract background">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to A message from our CEO" href="#ceo-message" id="ceo-message-anchor">A message from our CEO</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini 2.0" href="#gemini-2-0" id="gemini-2-0-anchor">Introducing Gemini 2.0</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.0 Flash" href="#gemini-2-0-flash" id="gemini-2-0-flash-anchor">Gemini 2.0 Flash</a>
        </li>
        
        <li>
          <a aria-label="link to Project Astra" href="#project-astra" id="project-astra-anchor">Project Astra</a>
        </li>
        
        <li>
          <a aria-label="link to Project Mariner" href="#project-mariner" id="project-mariner-anchor">Project Mariner</a>
        </li>
        
        <li>
          <a aria-label="link to Agents for developers" href="#agents-for-developers" id="agents-for-developers-anchor">Agents for developers</a>
        </li>
        
        <li>
          <a aria-label="link to Agents in games" href="#ai-game-agents" id="ai-game-agents-anchor">Agents in games</a>
        </li>
        
        <li>
          <a aria-label="link to Building responsibly" href="#building-responsibly" id="building-responsibly-anchor">Building responsibly</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
    }" data-date-modified="2024-12-11T15:30:20.983616+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd"><b>A note from Google and Alphabet CEO Sundar Pichai:</b></p><p data-block-key="efphd">Information is at the core of human progress. It’s why we’ve focused for more than 26 years on our mission to organize the world’s information and make it accessible and useful. And it’s why we continue to push the frontiers of AI to organize that information across every input and make it accessible via any output, so that it can be truly useful for you.</p><p data-block-key="4oebp">That was our vision when <a href="https://blog.google/technology/ai/google-gemini-ai/">we introduced Gemini 1.0 last December</a>. The first model built to be natively multimodal, Gemini 1.0 and 1.5 drove big advances with multimodality and long context to understand information across text, video, images, audio and code, and process a lot more of it.</p><p data-block-key="2huik">Now millions of developers are building with Gemini. And it’s helping us reimagine all of our products — including all 7 of them with 2 billion users — and to create new ones. <a href="https://notebooklm.google/">NotebookLM</a> is a great example of what multimodality and long context can enable for people, and why it’s loved by so many.</p><p data-block-key="60rf2">Over the last year, we have been investing in developing more agentic models, meaning they can understand more about the world around you, think multiple steps ahead, and take action on your behalf, with your supervision.</p><p data-block-key="ejiii">Today we’re excited to launch our next era of models built for this new agentic era: introducing Gemini 2.0, our most capable model yet. With new advances in multimodality — like native image and audio output — and native tool use, it will enable us to build new AI agents that bring us closer to our vision of a universal assistant.</p><p data-block-key="bh7ok">We’re getting 2.0 into the hands of developers and trusted testers today. And we’re working quickly to get it into our products, leading with Gemini and Search. Starting today our Gemini 2.0 Flash experimental model will be available to all Gemini users. We're also launching a new feature called <a href="https://blog.google/products/gemini/google-gemini-deep-research/">Deep Research</a>, which uses advanced reasoning and long context capabilities to act as a research assistant, exploring complex topics and compiling reports on your behalf. It's available in Gemini Advanced today.</p><p data-block-key="eqmfh">No product has been transformed more by AI than Search. Our AI Overviews now reach 1 billion people, enabling them to ask entirely new types of questions — quickly becoming one of our most popular Search features ever. As a next step, we’re bringing the advanced reasoning capabilities of Gemini 2.0 to AI Overviews to tackle more complex topics and multi-step questions, including advanced math equations, multimodal queries and coding. We started limited testing this week and will be rolling it out more broadly early next year. And we’ll continue to bring AI Overviews to more countries and languages over the next year.</p><p data-block-key="aaa8b">2.0’s advances are underpinned by decade-long investments in our differentiated full-stack approach to AI innovation. It’s built on custom hardware like Trillium, our sixth-generation TPUs. TPUs powered 100% of Gemini 2.0 training and inference, and today Trillium is <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">generally available</a> to customers so they can build with it too.</p><div data-block-key="7gbvh"><p>If Gemini 1.0 was about organizing and understanding information, Gemini 2.0 is about making it much more useful. I can’t wait to see what this next era brings.</p><p>-Sundar</p></div><hr></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Introducing Gemini 2.0: our new AI model for the agentic era</h2><p data-block-key="7o7kh"><i>By Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind on behalf of the Gemini team</i></p><p data-block-key="ecj6n">Over the past year, we have continued to make incredible progress in artificial intelligence. Today, we are releasing the first model in the Gemini 2.0 family of models: an experimental version of Gemini 2.0 Flash. It’s our workhorse model with low latency and enhanced performance at the cutting edge of our technology, at scale.</p><p data-block-key="6o7oe">We are also sharing the frontiers of our agentic research by showcasing prototypes enabled by Gemini 2.0’s native multimodal capabilities.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Gemini 2.0 Flash</h2><p data-block-key="7lrrf">Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p><img alt="A chart comparing Gemini models and their capabilities" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_narrow_light2x.gif">
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd">Our goal is to get our models into people’s hands safely and quickly. Over the past month, we’ve been sharing early, experimental versions of Gemini 2.0, getting great feedback from developers.</p><p data-block-key="964c">Gemini 2.0 Flash is available now as an experimental model to developers via the Gemini API in <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp">Google AI Studio</a> and <a href="https://cloud.google.com/generative-ai-studio">Vertex AI</a> with multimodal input and text output available to all developers, and text-to-speech and native image generation available to early-access partners. General availability will follow in January, along with more model sizes.</p><p data-block-key="a4e0l">To help developers build dynamic and interactive applications, we’re also releasing a new Multimodal Live API that has real-time audio, video-streaming input and the ability to use multiple, combined tools. More information about 2.0 Flash and the Multimodal Live API can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog</a>.</p><h3 data-block-key="dsvak">Gemini 2.0 available in Gemini app, our AI assistant</h3><p data-block-key="en32v">Also starting today, <a href="https://gemini.google.com/">Gemini</a> users globally can access a chat optimized version of 2.0 Flash experimental by selecting it in the model drop-down on desktop and mobile web and it will be available in the Gemini mobile app soon. With this new model, users can experience an even more helpful Gemini assistant.</p><p data-block-key="ea033">Early next year, we’ll expand Gemini 2.0 to more Google products.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Unlocking agentic experiences with Gemini 2.0</h2><p data-block-key="78rfe">Gemini 2.0 Flash’s native user interface action-capabilities, along with other improvements like multimodal reasoning, long context understanding, complex instruction following and planning, compositional function-calling, native tool use and improved latency, all work in concert to enable a new class of agentic experiences.</p><p data-block-key="3k97r">The practical application of AI agents is a research area full of exciting possibilities. We’re exploring this new frontier with a series of prototypes that can help people accomplish tasks and get things done. These include an update to Project Astra, our research prototype exploring future capabilities of a universal AI assistant; the new Project Mariner, which explores the future of human-agent interaction, starting with your browser; and Jules, an AI-powered code agent that can help developers.</p><p data-block-key="c8gij">We’re still in the early stages of development, but we’re excited to see how trusted testers use these new capabilities and what lessons we can learn, so we can make them more widely available in products in the future.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="Fs0t6SdODd8" data-index-id="10" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Gemini 2.0 supercut video" src="https://i.ytimg.com/vi_webp/Fs0t6SdODd8/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/Fs0t6SdODd8/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/Fs0t6SdODd8/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Project Astra: agents using multimodal understanding in the real world</h2><p data-block-key="fdk7b">Since we introduced <a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a> at I/O, we’ve been learning from trusted testers using it on Android phones. Their valuable feedback has helped us better understand how a universal AI assistant could work in practice, including implications for safety and ethics. Improvements in the latest version built with Gemini 2.0 include:</p><ul><li data-block-key="4arno"><b>Better dialogue:</b> Project Astra now has the ability to converse in multiple languages and in mixed languages, with a better understanding of accents and uncommon words.</li><li data-block-key="1f3oh"><b>New tool use:</b> With Gemini 2.0, Project Astra can use Google Search, Lens and Maps, making it more useful as an assistant in your everyday life.</li><li data-block-key="9f826"><b>Better memory:</b> We’ve improved Project Astra’s ability to remember things while keeping you in control. It now has up to 10 minutes of in-session memory and can remember more conversations you had with it in the past, so it is better personalized to you.</li><li data-block-key="68bh5"><b>Improved latency:</b> With new streaming capabilities and native audio understanding, the agent can understand language at about the latency of human conversation.</li></ul><p data-block-key="4qful">We’re working to bring these types of capabilities to Google products like <a href="http://gemini.google.com/">Gemini</a> app, our AI assistant, and to other form factors like glasses. And we’re starting to expand our trusted tester program to more people, including a small group that will soon begin testing Project Astra on prototype glasses.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="hIIlJt8JERI" data-index-id="13" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Project Astra demo video" src="https://i.ytimg.com/vi_webp/hIIlJt8JERI/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/hIIlJt8JERI/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/hIIlJt8JERI/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Project Mariner: agents that can help you accomplish complex tasks</h2><p data-block-key="b0b0b">Project Mariner is an early research prototype built with Gemini 2.0 that explores the future of human-agent interaction, starting with your browser. As a research prototype, it’s able to understand and reason across information in your browser screen, including pixels and web elements like text, code, images and forms, and then uses that information via an experimental Chrome extension to complete tasks for you.</p><p data-block-key="d0nh0">When evaluated against the <a href="https://arxiv.org/abs/2401.13919">WebVoyager benchmark</a>, which tests agent performance on end-to-end real world web tasks, Project Mariner <a href="http://deepmind.google/technologies/project-mariner">achieved a state-of-the-art result of 83.5%</a> working as a single agent setup.</p><p data-block-key="8mvv5">It’s still early, but Project Mariner shows that it’s becoming technically possible to navigate within a browser, even though it’s not always accurate and slow to complete tasks today, which will improve rapidly over time.</p><p data-block-key="2n0oq">To build this safely and responsibly, we’re conducting active research on new types of risks and mitigations, while keeping humans in the loop. For example, Project Mariner can only type, scroll or click in the active tab on your browser and it asks users for final confirmation before taking certain sensitive actions, like purchasing something.</p><p data-block-key="ch96g">Trusted testers are starting to test Project Mariner using an experimental Chrome extension now, and we’re beginning conversations with the web ecosystem in parallel.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="2XJqLPqHtyo" data-index-id="16" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Mariner demo video" src="https://i.ytimg.com/vi_webp/2XJqLPqHtyo/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/2XJqLPqHtyo/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/2XJqLPqHtyo/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Jules: agents for developers</h2><p data-block-key="216k6">Next, we’re exploring how AI agents can assist developers with Jules — an experimental AI-powered code agent that integrates directly into a GitHub workflow. It can tackle an issue, develop a plan and execute it, all under a developer’s direction and supervision. This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.</p><p data-block-key="acjnm">More information about this ongoing experiment can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog post</a>.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p>

        
        
          
            <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Jules_GIF_3D_10_1.mp4" type="video/mp4" title="Animation of Jules coding assistant" alt="Jules">
              Video format not supported
            </video>
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Agents in games and other domains</h2><p data-block-key="384g8">Google DeepMind has a <a href="https://deepmind.google/discover/blog/agent57-outperforming-the-human-atari-benchmark/">long</a> <a href="https://deepmind.google/research/breakthroughs/alphago/">history</a> of using games to help AI models become better at following rules, planning and logic. Just last week, for example, we introduced <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, our AI model that can create an endless variety of playable 3D worlds — all from a single image. Building on this tradition, we’ve built agents using Gemini 2.0 that can help you navigate the virtual world of video games. It can reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation.</p><p data-block-key="b3sa9">We're collaborating with leading game developers like Supercell to explore how these agents work, testing their ability to interpret rules and challenges across a diverse range of games, from strategy titles like “Clash of Clans” to farming simulators like “Hay Day.”</p><p data-block-key="2p0a">Beyond acting as virtual gaming companions, these agents can even tap into Google Search to connect you with the wealth of gaming knowledge on the web.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="IKuGNHJBGsc" data-index-id="22" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Navi demo video" src="https://i.ytimg.com/vi_webp/IKuGNHJBGsc/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/IKuGNHJBGsc/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/IKuGNHJBGsc/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="of3wa">In addition to exploring agentic capabilities in the virtual world, we’re experimenting with agents that can help in the physical world by applying Gemini 2.0's spatial reasoning capabilities to robotics. While it’s still early, we’re excited about the potential of agents that can assist in the physical environment.</p><p data-block-key="ba0kt">You can learn more about these research prototypes and experiments at <a href="http://labs.google/">labs.google</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Building responsibly in the agentic era</h2><p data-block-key="6j36d">Gemini 2.0 Flash and our research prototypes allow us to test and iterate on new capabilities at the forefront of AI research that will eventually make Google products more helpful.</p><p data-block-key="fvsj8">As we develop these new technologies, we recognize the responsibility it entails, and the many questions AI agents open up for safety and security. That is why we are taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.</p><p data-block-key="angf4">For example:</p><ul><li data-block-key="crrpj">As part of our safety process, we’ve worked with our Responsibility and Safety Committee (RSC), our longstanding internal review group, to identify and understand potential risks.</li><li data-block-key="2uku6">Gemini 2.0's reasoning capabilities have enabled major advancements in our AI-assisted red teaming approach, including the ability to go beyond simply detecting risks to now automatically generating evaluations and training data to mitigate them. This means we can more efficiently optimize the model for safety at scale.</li><li data-block-key="81t7m">As Gemini 2.0’s multimodality increases the complexity of potential outputs, we’ll continue to evaluate and train the model across image and audio input and output to help improve safety.</li><li data-block-key="clba0">With Project Astra, we’re exploring potential mitigations against users unintentionally sharing sensitive information with the agent, and we’ve already built in privacy controls that make it easy for users to delete sessions. We’re also continuing to research ways to ensure AI agents act as reliable sources of information and don’t take unintended actions on your behalf.</li><li data-block-key="7r7pa">With Project Mariner, we’re working to ensure the model learns to prioritize user instructions over 3rd party attempts at prompt injection, so it can identify potentially malicious instructions from external sources and prevent misuse. This prevents users from being exposed to fraud and phishing attempts through things like malicious instructions hidden in emails, documents or websites.</li></ul><p data-block-key="f9e42">We firmly believe that the only way to build AI is to be responsible from the start and we'll continue to prioritize making safety and responsibility a key element of our model development process as we advance our models and agents.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Gemini 2.0, AI agents and beyond</h2><p data-block-key="35lej">Today’s releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we’re looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI.</p></div>
  

  
    














<uni-related-content-tout title="Gemini 2.0: Our latest, most capable AI model yet" cta="See more" summary="See how Gemini 2.0 and our research prototypes work — and how they’ll help make our Google products more helpful." hideimage="False" eyebrow="Collection" fullurl="https://blog.google/products/gemini/google-gemini-ai-collection-2024/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp" alt="gemini social share collection" sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" aria-label="Sign up to receive weekly news and stories from Google." data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
        
        
        <p>You are already subscribed to our newsletter.</p>
      </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PeerTube mobile app: discover videos while caring for your attention (131 pts)]]></title>
            <link>https://joinpeertube.org/news/peertube-app</link>
            <guid>42388488</guid>
            <pubDate>Wed, 11 Dec 2024 15:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joinpeertube.org/news/peertube-app">https://joinpeertube.org/news/peertube-app</a>, See on <a href="https://news.ycombinator.com/item?id=42388488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, at Framasoft (bonjour!), we publish the very first version of the PeerTube Mobile app for android and iOS. A lot of care went into its conception, to help a wider audience watch videos and discover platforms, while not getting their attention (and data) exploited.</p>
<h4>Another step into PeerTube growth</h4>
<p>Even though we have been developing and maintaining the PeerTube software for 7 years, we, <a href="https://framasoft.org/" target="_blank" rel="noopener noreferrer">at Framasoft, are far from being an IT company</a>. First because <strong>we are a not-for-profit</strong> (funded through donations, you can support us <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a>), and then because <strong>our goal is, actually, to help others educate themselves on digital issues, surveillance capitalism</strong>, etc. and to give them tools that helps them get digitally emancipated.</p>
<p><strong>Developing PeerTube has been, to us, an (happy) accident</strong>. We wanted to show that with one paid developer (for the first six years, then two), very little means (~ €650,000 over 7 years) and lots of community contributions, we can create a radical alternative to YouTube and Twitch. It also took a lot of patience. From the get go, <strong>we knew we needed to aim for a slow but steady pace of growth</strong> for the software, the network of video platforms it federates, the whole ecosystem and the audiences it reached.</p>

<p>Videos and live-streams are increasingly watched on mobile devices. We knew <strong>the next step to widen the audience of the PeerTube network of platforms was to develop a mobile client</strong>. Last year, we decided to hire <a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/" target="_blank" rel="noopener noreferrer">Wicklow (who completed his last internship, before graduating, here with us)</a>, to train him on mobile technologies, develop a mobile app, while continuing to get familiar with PeerTube's core code.</p>
<h4>Getting funded and getting help</h4>
<p>This was (and still is) a big decision: a new hire needs to be funded (our huge thanks to <a href="https://nlnet.nl/" target="_blank" rel="noopener noreferrer">NLnet</a> and the <a href="https://nlnet.nl/entrust/" target="_blank" rel="noopener noreferrer">NGI0 Entrust program</a>!), and we want to stay a small structure, so we don't have lots of room in our team. In hindsight, though, we believe it was the right one.</p>
<p>We surrounded ourselves with <a href="https://www.zenika.com/" target="_blank" rel="noopener noreferrer">Zenika</a>, to get help on architecture and experience on mobile strategy. We soon realized that peer-to-peer video sharing wouldn't be a wise strategy on mobile devices. After benchmarking different technologies, Wicklow picked Flutter for the development.</p>
<p><a href="https://www.lacooperativedesinternets.fr/" target="_blank" rel="noopener noreferrer">La Coopérative des Internets (French design workers-owed-company)</a>, helped us pinpoint the relevant user experience and design an app fit for videos on the fediverse. <strong>We decided, for the first release, to limit the scope of the app to the "spectator use-case"</strong>: browsing and watching videos.</p>
<p>We plan to share all their reports soon (early 2025), as soon as we put in the final touches. We hope that sharing this expertise and experience will help other FLOSS initiatives in their endeavor.</p>
<p>In the meanwhile, the PeerTube Mobile app is (as always with us) Free-libre and open-source, and you can <a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">find the source code here on our repository</a>.</p>
<p><img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-welcome.jpg" alt="image welcome page">
  <img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-video-player.jpg" alt="image player">
</p>
<h4>Fediverse complexities made simple</h4>
<p>This preparatory work helped us realize that a mobile client was <strong>an amazing opportunity to simplify the PeerTube experience</strong>. PeerTube is not a video platform: it's a network of video platforms, each with their own rules, means and focus, that can choose to federate with others (or not).</p>
<p>It is, by design, more complex than a centralized platform. One of the main feedback we got from video enthusiasts was</p>
<blockquote>
<p>"I don't know where to get an account. I don't know where to search &amp; find videos" (even though we maintain <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a>).</p>
</blockquote>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/2024-12-sepia-search-screenshot-EN.jpg" title="" alt="">  </figure>

<h5>Local account</h5>
<p>Within a mobile client, we can create some kind of local account, directly on your device, so you get your watch-list, playlists, faves, etc. <strong>It saves you the hassle of finding a platform where you'd need to create an account</strong> if you just want to enjoy video content.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-watch-later.jpg" title="" alt="">  </figure>

<h5>Explore platforms</h5>
<p>We can also include a search engine and an interface to explore the federation of PeerTube platforms and find videos suited to your interest. Not everyone knows <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> (and other fediverse search engines) exists: <strong>you get it from the get go, in your pocket</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore.jpg" title="" alt="">  </figure>

<h5>Highlighting platforms' diversity</h5>
<p>Finally, we can present content in a way that highlights the platforms, and show you where the videos/channels you watch are hosted. Differentiating platforms is <strong>a practical, visual way of introducing the concept of federation</strong> to a wider audience.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore-2.jpg" title="" alt="">  </figure>

<h4>Designing out dark patterns</h4>
<p>Humility check: a small French nonprofit will never have Google's workforce nor Amazon's money (and vice versa). But <strong>we have an edge: we are not constrained by surveillance capitalism rules</strong>, and its captology models.</p>
<blockquote>
<p>Neither PeerTube nor the mobile app have any interest into grabbing your attention, forcefeeding you ads and milking behavioural and personal data from you.</p>
</blockquote>
<p>That is how <strong>we freed the design from toxic design patterns such as doom scrolling, curated feeds, needy notifications and so on</strong>.</p>
<p>It might sound obvious, but it takes real effort to concieve an interface cleaned from what has unfortunately became the new normal. Even more if you need to keep it familiar enough so it says easy to use.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-show-more.jpg" title="" alt="">  </figure>

<h4>A very first build, limited by (play &amp; i) stores</h4>
<p>We knew beforehand that <strong>fitting into Google's PlayStore and Apple AppStore would be a challenge</strong>. They clearly weren't ready to host a client for (not-a-platform but) a network of autonomous video-sharing platforms, published by a small French nonprofit, funded through its independent donation website.</p>
<p>We knew about the <a href="https://github.com/sschueller/peertube-android/issues/302" target="_blank" rel="noopener noreferrer">issues encountered by Thorium</a> (another PeerTube mobile client). We got help and advices from Gabe, who develops <a href="https://owncast.online/" target="_blank" rel="noopener noreferrer">the streaming tool Owncast</a> (may your keyboard always repel crumbs and click smoothly), and <a href="https://laurenshof.online/owncast-and-the-app-store/" target="_blank" rel="noopener noreferrer">encountered many obstacles</a>... We knew about all that but, oh my Tux, it was a wild ride.</p>
<p>After jumping though hoops, here we are, you can download the PeerTube mobile app here:</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
<h4>(un-)Limiting the federation</h4>
<p>To get through Apple's (and, in a lesser way, Google's) validation processes, we had to present the mobile app with a curated "allowlist" of PeerTube platforms that meet their standards.</p>
<p>Here is the state of those limitations right now:</p>
<ul>
<li><strong>Apple AppStore</strong>: limited to a very strict allowlist. Truth be told, a week before release, we are still unsure of being validated. Once we manage it, we'll see how to widen the list &amp; let users add platforms they want</li>
<li><strong>Google Play Store</strong>: limited allowlist, but users can already add the platforms they want. We plan to widen the allowlist next</li>
<li><strong>F-Droid</strong> (coming soon) and direct download apk: all PeerTube platforms we have indexed on <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> are available. If an instance isn't declared to our index or is moderated, you can add it manually.</li>
</ul>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-plaforms.jpg" title="" alt="">  </figure>

<p>We cannot stress enough how <strong>their stores are not ready for independent solidarity-oriented networks</strong>. For exemple, a small "support us" donation link in our website footer or even on one of the allowed platforms triggered a "nope" from Apple.</p>
<p>And that's consistent: as seen in <a href="https://en.wikipedia.org/wiki/Epic_Games_v._Apple" target="_blank" rel="noopener noreferrer">their fight with Epic</a> (owners of Fortnite) Apple take their share in every in-app purchases. They have an economic interest to keep your expenses enclosed in their ecosystem. Please, please: consider getting your freedom back ;).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/expected-nothing.jpg" title="" alt="">  </figure>

<h4>Coming soon, in the PeerTube App</h4>
<p>Fitting into Apple's (and Google's) very small boxes took time and energy, more than what we expected. We decided to release a first (incomplete) version of the app in December anyway, and gradually improve on it.</p>
<p>Here are the <strong>features we plan to develop and share for the PeerTube app</strong>:</p>
<ul>
<li>Soon (early 2024)
<ul>
<li>Finalize and publish design and mobile strategy reports</li>
<li>Publish documentation</li>
<li>Play video in background</li>
<li>Log in to one's account, gets subscriptions, comment videos</li>
<li>next video recommandation</li>
<li>improve on the limited platforms list situation</li>
</ul>
</li>
<li>Then (mid 2024 (if funded))
<ul>
<li>adapt to tablets</li>
<li>adapt to TVs (AndroidTV... AppleTV will depend on their limitations)</li>
<li>Watch offline (for downloadable content)</li>
</ul>
</li>
</ul>
<p>Right now, we are still waiting to secure funding for those mid-2024 features (for which we have requested a NLnet grant).</p>
<p>Depending on the app success and usage, <strong>we would love to add the content creator usecase to the app</strong>. But that's a big one: upload and publish a video, manage one's content, create a livestream, etc. We are still wondering <strong>where, when and how to get funds for this undertaking</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/PeerTube-app-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<h4>Care, Share and Contribute!</h4>
<p><strong>This is the part where we need you</strong>.</p>
<p>We hope you will <strong>enjoy this app, download and use it, and share it</strong> with your friends. This is a new gateway to promote PeerTube content, get audience to fabulous content creators, entice them to share more and boost that virtious loop.</p>
<p>This app is also <strong>a way of showcasing how media could be presented</strong>, when they are made with care for your agency and attention. More than ever: <strong>sharing is caring</strong>.</p>
<p>You can also <strong>contribute by reporting bugs</strong> (within the app), helping on the code (<a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">here is the git repository</a>), and translating the interface. This is an important one: right now, the App is only available in English and French. <strong><a href="https://weblate.framasoft.org/projects/peertube-app/peertube-app/" target="_blank" rel="noopener noreferrer">Your language contributions are welcomed</a> here on our translation platform</strong>.</p>
<p>Obviously, we plan to maintain the app, add translations, implement bugfixes and security updates when needed: but this has a cost. <strong>We need to secure Framasoft's 2025 budget</strong> to make Wicklow's position permanent in our team (which is a priority to us). <strong>Our donation campaign is active right now</strong>, you can add your support <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a> (and thanks!).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/20-ans-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<hr>
<p>You can help us continue to improve PeerTube by sharing this information, <a href="https://ideas.joinpeertube.org/" target="_blank" rel="noopener noreferrer">suggesting improvements</a> and, if you can afford it, making <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">a donation to Framasoft</a>, the association that develops PeerTube.</p>
<p>Thanks in advance for your support!<br>
Framasoft</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Get Distracted (137 pts)]]></title>
            <link>https://calebhearth.com/dont-get-distracted</link>
            <guid>42388354</guid>
            <pubDate>Wed, 11 Dec 2024 14:57:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calebhearth.com/dont-get-distracted">https://calebhearth.com/dont-get-distracted</a>, See on <a href="https://news.ycombinator.com/item?id=42388354">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
      
<section>
  <iframe src="https://www.youtube.com/embed/UBdBoWAtLNI" frameborder="0" allowfullscreen=""></iframe>
</section>

<p><em>Be sure to check out these <a href="https://calebhearth.com/images/dont-get-distracted-sketchnotes--stephanie-nemeth.jpg">Sketchnotes</a> from a 15 minute version of this talk from <a href="http://codelandconf.com/">Codeland</a> by <a href="https://twitter.com/stephaniecodes">Stephanie Nemeth</a></em></p>

<p>I’m going to tell you about how I took a job building software to kill people.</p>

<p>But don’t get distracted by that; I didn’t know at the time.</p>

<p>Even before I’d walked across the stage for graduation, I accepted an offer for an internship. It paid half again as much as the most I’d ever gotten in the highest paying job up to that point. Not to mention that I’d spent years in college with low paid student jobs or living only on student loans.</p>

<p>I’d be joining a contracting company for the Department of Defense. The Department of Defense, or DOD, is the part of the government made up by the military in the United States. The DOD outsources all sorts of things, from <a href="https://www.defense.gov/News/Contracts/Contract-View/Article/1177766/">P-8A Multi-mission Maritime aircraft to blue, shade 451, Poly/wool cloth</a>.</p>

<p>At the time, I thought nothing of the fact that I’d be supporting the military. Besides, they’re the good folks right? My dad was in the military. So was my grandfather. It was good money, a great opportunity, and a close friend of mine had gotten me the gig. Life was good.</p>

<p>I showed up for my first day of work in North Virginia, or NoVA as the industry likes to call it. I met the team of other interns, then learned what the team would be building: a tool to use phones to find WiFi signals.</p>

<p>It seemed pretty cool compared to what I’d built to up that point. The most complicated thing was an inventory management system. It didn’t concern itself overmuch with persistence. Who needs to stop and restart a program anyhow? The data was right there in memory and if you forgot how many Aerosmith CDs you had, who cares? It got me an A on the assignment, and that was all that mattered.</p>

<p>Honestly, the idea of finding WiFi routers based on the signal strength seemed pretty intimidating at the time. The idea impressed me.</p>

<p>But don’t get distracted by all this; the software was intended to kill people.</p>

<p>I joined the team after they’d already gotten started on the project. The gist of the tool was that it would look at how WiFi signal strength changed as your phone moved around. If the signal strength got stronger, you were getting closer. If it got weaker, you were moving away. To find this information, we’d collect two pieces of information for each WiFi access point in range. The phone’s geolocation information and the WiFi signal strength.</p>

<p>To predict the actual location of the WiFi signal, we used a convolution of two algorithms. Both of them relied on the Free Space Path Loss equation. For our purposes, FSPL calculates how far away a phone is from a WiFi signal based on loss in signal strength. It assumes that there is only empty air, or “free space”, between the access point and the phone.</p>

<p>The first algorithm was R^2. It measured the difference between the signal strength we’d observed and the expected signal strength at each distance in a search grid based on Free Space Path Loss. Locations with the lowest R^2 error rate were the most likely location.</p>

<p>We’d combine that calculation with a Gaussian estimate. Now I spent two or three days last week trying to understand Gaussian estimates for this talk and couldn’t. The best documentation on them are still research papers. I do know that it creates a probability curve. The high points of the curve are distances where the access point is likely to be, and low points are unlikely. The curve started with a probability hole of low values. They represented low likelihood that the phone is standing right next to the signal. The curve then increased to high probabilities further out. It decreased to near zero even further. The algorithm adjusted the width and height of these two curves by consulting past measurements. It created a heat map of probabilities for the signal source.</p>

<p>We’d normalize the probabilities for each location in the search grid. A combination of probabilities was more correct than either algorithm itself.</p>

<p>We stored this probability matrix for each location a phone collected from. Using these, if we collected readings while moving in a straight line, we could tell you how far away the WiFi was. If you turned a corner, we could also add direction, so you could find it in 2D space. If you climbed some stairs, we’d show you altitude as well. The technology was the most interesting project I’d ever worked on.</p>

<p>But don’t let that distract you; it was designed to kill people.</p>

<p>I mentioned that I had a software engineering degree at this point. My teammates were earlier in their education careers. Most of them were a year or two into their four year programs, also a mix of Computer Science and Math majors. My expertise was in the design and process of building software, while theirs was more in high level mathematics or the theory of computer use.</p>

<p>I helped to translate the working algorithms they’d designed in MatLab to the Java code we needed to run on the phones.</p>

<p>And let’s be honest, we spent plenty of time deciding whether we preferred Eclipse or NetBeans. Can I say how happy I am that as a Ruby developer, I’ve not had to figure out where to put a .jar file in over half a decade?</p>

<p>One such example is calculating distance between two points. As these things go, it was in the deepest level of each loop. We were using great circle distance, which is the way you measure the shortest distance between two points on a sphere, such as Earth. Did we need to use this complicated measurement over the meager distances a WiFi network can operate over? Absolutely not. Did I mention this was over half a decade ago? We weren’t always making the best choices. Anyhow, we needed to calculate these distances. The function doing it was being hit hundreds of thousands of time for each collection point, often with the same two locations. It was a very slow process.</p>

<p>We solved that by implementing a dictionary of latitude/longitude pairs to distances. This at least meant we didn’t re-do those calculations. This and other optimizations we made sped up the performance from seven minutes to a few seconds.</p>

<p>But don’t get distracted; that performance increase made it faster to kill people.</p>

<p>The accuracy of the locations wasn’t fantastic. I don’t remember exactly what it was before we focused on improving this, but an average error about 45 feet sticks in my head.</p>

<p>That’s a little longer than a Tyrannosaurus Rex nose to tail, or more concretely the length of a shipping container.</p>

<p>That’s significant when the WiFi range for 802.11n is only about 100 feet. That means we could be up to almost half the range of the router off from where it was.</p>

<p>I talked about the Gaussian estimation, the two curves from the second algorithm. We hard-coded numbers that defined this curve. They were only starting points, but they were starting points every time we made the calculation.</p>

<p>A Genetic Algorithm is a type of program that produces a set of values that optimize for a desired result. It’s a perfect fit for tuning these hard coded values to get more accurate results.</p>

<p>Each of the Gaussian estimation values will is a gene. The set of values is a genome in Genetic Algorithm parlance. The genes were our 3 constants.</p>

<p>A fitness function is what Genetic Algorithms use to measure performance of a genome. For the dataset of readings I was using in the GA, I knew the actual location of the access points. That meant I could run the geolocation algorithm with each genome’s values in place as the fitness function. The result would be the distance between the actual location and the one calculated by the GA-derived values.</p>

<p>Genetic Algorithms take a set of genomes, called a generation, and keep a certain percentage of top performers.</p>

<p>These top performers “survive” to the next generation as copies. Sometimes the algorithm mutates these copies by adding a Gaussian random value to each gene. This means that there was a chance that any of the copied genomes would have each gene changed slightly. That way they would have a chance of performing better or worse. New random genomes are created for the remaining spots in the new generation.</p>

<p>We saved the top performers across all populations. When the GA ended I could take a look at the values and select the best performer.</p>

<p>I let this genetic algorithm run over the weekend. It was able to increase the accuracy from 40-odd feet to about 10.3 feet, 25% of the error from the original. That is less than the GPS accuracy on the smartphones collecting data (which is about 16 feet according to GPS.gov). This is too accurate, so it may have over-optimized against the test data and might not be as accurate against other data sets. This is called overfitting and the way around it is to have separate sets of training and test data. Did I mention this was five years ago? I didn’t even know what overfitting was back then.</p>

<p>I loved this. Genetic algorithms, R^2, and Gaussian estimation are the kind of thing that they tell you you’ll never need to use again once you graduate. But we were using them for a real world project! It was great.</p>

<p>But don’t let that distract you; this accuracy made it easier for the software to help kill people.</p>

<p>The tracking now worked accurately and quickly. The next feature was to add tracking a moving WiFi access point. I briefly wondered why an access point would be moving, but that question wasn’t as interesting as figuring out how to track it.</p>

<p>We made use of Kalman Filters to observe state variables: the position, velocity, and acceleration of the WiFi signal. Given these and the time since the last measurement, a Kalman Filter is able to improve the current prediction with surprising accuracy, filtering out “noise” data automatically.</p>

<p>Each time we ran the real time algorithm, we’d also feed this to the Kalman Filter.  With only that information, it was able to produce an estimate that is a weighted average of previous data. A new predicted location that was more accurate than the calculated value.</p>

<p>At the same time, we added the ability to track more than one WiFi signal. We’d filter our collection of readings by the unique identifier of each access point. The filtered datasets each went through the full algorithm to produce predictions.</p>

<p>We used the APIs on the phone to read the signal strength of all WiFi access points in range. We were able to track multiple hotspots, basically whatever we could see in the WiFi network list. It was all very exciting. These seem like academic problems, but we were getting to use them in a real project! Being a programmer was going to be great.</p>

<p>But don’t let that distract you; this meant we could kill multiple more accurately people.</p>

<p>We’d been working with the project owner throughout this process.</p>

<p>He was laissez-faire about most things. He might check in once a day then going back to his main job in the area of the building dedicated to classified work.</p>

<p>Whenever we hit one of these milestones, we’d tell him. He’d be happy about it, but a question always came up. He wanted it to sniff for the signals put out by phones in addition to WiFi hotspots. This is a much harder problem from a technical perspective. The functionality necessary to do this is “promiscuous mode”, a setting on the wireless network controller. Neither iPhone nor Android supported that option. We’d need to jailbreak or root the phone regardless of the platform. We looked for packages that we could use that would let us do this to “sniff” the packets that devices sent back to routers. The closest we ever found was a SourceForge project that seemed promising. We didn’t fully understand its use and it wasn’t well documented.</p>

<p>We told the project owner that we’d get to it later. None of us thought it was that important: we had the technology to find WiFi Access Points working. That was the goal right? Each time we’d demonstrate the new exciting tech we’d built though, the same question came up.</p>

<ul>
  <li>We got WiFi hotspots located! Great, does it find phones?</li>
  <li>It’s taking seconds instead of minutes! Great, does it find phones?</li>
  <li>We looked into it finding phones, it seems unlikely but maybe! Ok, we’ll come back to it.</li>
  <li>We got moving targets working! Great, does it find phones?</li>
</ul>

<p>I had been distracted.</p>

<p>All of the cool problems we were solving: finding nodes, speeding things up, making more accurate predictions. It was all so cool, so much fun. I hadn’t thought about why we were putting all this work into finding a better place to sit and get good WiFi. That doesn’t even make sense if you look at it for more than a few seconds.</p>

<p><em>Does it find phones.</em></p>

<p>This was never about finding better WiFi. We were always finding phones. Phones carried by people. Remember I said I was working for a Department of Defense contractor? The DoD is the military. I was building a tool for the military to find people based on where their phones where, and shoot them.</p>

<p>I tried to rationalize this then. The military is in place to protect Truth, Justice, and the American Way. But this was the same time that we found out the government had been spying on Americans in the US with drones. They’d also lent out that technology to federal, state, and local law enforcement agencies nearly 700 times to run missions. The military and government do things that I know I don’t agree with pretty often. I didn’t want to be a part of building something used to kill people, especially since I knew I’d never know who it was killing, let alone have a say.</p>

<p>I rationalize it now too. We were interns, and we didn’t even have clearance. The projects this company did for the government were classified Top Secret. I wasn’t allowed to know what they were. My code probably got thrown away and forgotten. Probably.</p>

<p>This was an extreme example of code used in a way that the creator did not intend it. The project owner conveniently left out its purpose was when explaining the goals. I conveniently didn’t focus too much on that part. It was great pay for me at the time. It was a great project. Maybe I just didn’t want to know what it would be used for. I got distracted.</p>

<p>There are other examples of when code is used in ways it wasn’t intended, and of code that does bad things.</p>

<p>A year and a day ago, a developer named Bill Sourour <a href="https://medium.freecodecamp.org/the-code-im-still-ashamed-of-e4c021dff55e" title="The Code I'm Still Ashamed Of">wrote a blog post</a>. It opened with the line: “If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical.”</p>

<p>Bill had been asked to create a quiz that would almost always give a result that benefitted his client. Bill worked in Canada, and in Canada there are laws in place that limit how pharmaceutical companies can advertise prescription drugs to customers. Anyone could learn about the general symptoms a given drug addressed, but only patients with prescriptions could get specific information about the drug.</p>

<p>Because of this law, the quiz was posing as a general information site and not an advertisement for a specific drug. If the user didn’t answer that either they were allergic to the drug or already taking it, every quiz result suggested this specific drug. That’s what the requirements said to do, and that’s what Bill coded up.</p>

<p>The project manager did a quick test before submitting the website to the client. She told Bill that the quiz was broken: it always had the same answer. “Those were the requirements,” Bill responded. “Oh. Ok.”</p>

<p>A little while later, Bill got an email from a colleague that had a link to a news article. A young woman had taken the drug that Bill had built this quiz for. She had killed herself. It turns out that one of the main side effects of the drug were severe depression and suicidal thoughts.</p>

<p>Nothing Bill did was illegal. Like me, Bill was a young developer making great money. The purpose of the site was to push a particular drug - that’s why it was being built. He chalked it up to marketing. He never intended for this to happen. Maybe Bill got distracted too.</p>

<p>As his conclusion, Bill writes:</p>

<figure>
  <blockquote>
  <p>As developers, we are often one of the last lines of defense against potentially dangerous and unethical practices.</p>
  <p>We’re approaching a time where software will drive the vehicle that transports your family to soccer practice. There are already AI programs that help doctors diagnose disease. It’s not hard to imagine them recommending prescription drugs soon, too.</p>
  <p>The more software continues to take over every aspect of our lives, the more important it will be for us to take a stand and ensure that our ethics are ever-present in our code.</p>
  <p>Since that day, I always try to think twice about the effects of my code before I write it. I hope that you will too.</p>
  </blockquote>
  </figure>

<p>Bill’s story isn’t that far off from mine, but there are still other examples.</p>

<p>Earlier this year, a story came out that Uber had built into its ridesharing app code they call “greyball”. It’s a feature of their VTOS (or violation of terms of service) tool that can populate the screen with fake cars when the app is opened by users in violation of the terms of service.</p>

<p>In a statement, Uber said, “This program denies ride requests to users who are violating our terms of service — whether that’s people aiming to physically harm drivers, competitors looking to disrupt our operations, or opponents who collude with officials on secret ‘stings’ meant to entrap drivers.”</p>

<p>In practice, as <a href="https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html" title="How Uber Deceives the Authorities Worldwide">The New York Times reports</a>, it was used in Portland to avoid code enforcement officers working to build a case against Uber for operating without a license. When triggered by Uber’s logic, it populates the app with cars that don’t exist, with fake drivers who quickly cancel after accepting a ride.</p>

<p>I am not a lawyer, but it seems like this is likely an obstruction of justice, itself a crime outside of Uber’s unlawful operations in Portland. Greyball is used even today, though mostly outside the United States. I’m a huge fan of ridesharing - though I use a competitor in Austin and Boston called Fasten<sup id="fnref:fasten" role="doc-noteref"><a href="#fn:fasten" rel="footnote">1</a></sup> rather than the much larger Uber or Lyft. But it’s not uncommon to see in the news these days articles about heinous things these drivers are doing. Greyball may have enabled some of those.</p>

<p>Again, it’s an unintended consequence of a tool built. Maybe the greyball internal pitch was to “greyball” users who were in violation of the terms of service. People who were under 18, or who didn’t pay to clean up their late night explosive accidents one too many times for example. Rather than block them, probably causing them to create a new account, they could be put into an alternate dimension where for some reason they just couldn’t ever get a ride. That’s fine, right?</p>

<p>If these developers had thought about the worst possible case for how this could be used, maybe obstruction of an investigation into Uber’s shady dealings would have come up in that conversation and it could have been addressed early on. Maybe they were distracted by the face value of the request from looking deeper at the purpose and uses.</p>

<p>There’s all sorts of things as well that aren’t as black and white (if you’ll excuse the pun). <a href="http://www.deidrariggs.com/2016/11/30/is-instagram-listening-in-on-you/" title="Is Instagram Listening In On You?">Apps that always listen to the microphone to tailor ads to you based on what you say near your phone</a>, <a href="https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html" title="Sean Parker Unloads on Facebook">websites designed to exploit psychology to take up as much of your time and attention as possible</a>, and any number of apps that opt you into mailing lists when you sign up or purchase something. These aren’t nearly as obviously bad, but at least in my opinion they’re still kind of shady.</p>

<p>This value system is different for others. We don’t always agree as individuals what is right and wrong, or even with what should be legal or illegal.</p>

<p>There are actually words for things that society decides are good or bad versus what you or I individually believe: ethics and morals. While modern philosophy more or less uses these terms interchangeably, a common understanding at least between us will be important later.</p>

<p>Ethics are imposed by an outside group. A society, a profession, a community such as ours or even where you live. Religions provide ethical systems, as do groups of friends. Societies in whatever form define right and wrong, good and bad, and imposes those on its members. Ethics in societies such as local, state, and national groups are often, but not always, coded into laws.</p>

<p>Morals are a more personal version of the same thing. Society as a whole imposes its mores on smaller communities, and all of that trickles down to the individual level. That’s not to say that your morals can’t conflict with the ethics of society. For example, you might think that freedom of speech is a basic human right, but live somewhere that defacing religious or political objects is considered wrong.</p>

<p>Let’s not get distracted by morals and ethics yet, though. We’ll come back to them.</p>

<p>The unifying factor in all of the stories I’ve told is that a developer wrote the code that did these unethical or immoral things. As a profession, we have a superpower: we can make computers do things. We build tools, and ultimately some responsibility lies with us to think through how those tools will be used. Not just what their intention is, but also what misuses might come out of them. None of us wants to build things that will be used for evil.</p>

<p>The Association for Computing Machinery is a society dedicated to advancing computing as a science &amp; profession. ACM includes this in their <a href="https://www.acm.org/about-acm/acm-code-of-ethics-and-professional-conduct#imp1.2">Code of Ethics and Professional Conduct</a>:</p>

<figure>
  <blockquote><p>
  Well-intended actions, including those that accomplish assigned duties, may lead to harm unexpectedly. In such an event the responsible person or persons are obligated to undo or mitigate the negative consequences as much as possible. One way to avoid unintentional harm is to carefully consider potential impacts on all those affected by decisions made during design and implementation.</p></blockquote></figure>

<p>So how can we “carefully consider potential impacts”? Honestly, I don’t have any answers to this. I don’t think that there really is a universal answer yet, because if we had it I have to believe we’d not be building these dangerous pieces of software.</p>

<p>I do have a couple of ideas though. One I got from my friend Schneems is to add to the planning process a step where we come up with the worst possible uses of our software. In opting in folks to an email list by default, the worst case might be that we send them a bunch of unwanted email and they unsubscribe. Maybe they even stop being a customer. As Schneems said: “Am I willing to sell my hypothetical startup’s soul for a bigger mailing list, when that might be all that keeps the company afloat? Yeah, no problem.” That makes sense to me. I don’t think it’s the best practice, but in the end it’s not physically hurting anyone. If I had sat down and thought about what the WiFi location app could be used for in the worst case, I would have come to a very different conclusion.</p>

<p>Actually, thinking about the worst possible uses of code could probably be a fun exercise. You might come up with some pretty wacky examples like “If we send Batman an email and he happens to have notifications on his iPhone for new emails, he might be looking at the notification when the Riddler drives by in the Riddler Car and he might not catch him before he gets off his witty one liner at the crime scene. Riddle me this, riddle me that, who’s afraid of the big, black bat?.” This isn’t so plausible, but it shows that these exercises can go down all sorts of different paths that aren’t obvious at a glance.</p>

<p>Another, the thing that I think I should have done, and that we can all do more of, is to simply not take requests at face value. The project owner at the Defense contractor I worked at didn’t spell out what the reason for the code was. But at least in retrospect, it wasn’t a big leap of logic. “We’re going to build an app to find WiFi signals” is all true, but it’s not the whole truth. Asking them, or myself, “why” enough times probably would have led me to a much earlier understanding. Why? To find the sources. Why? To go to them. Why? Why? Why?</p>

<p>Comedian Kumail Nanjiani, best known for the TV show Silicon Valley and his recent film The Big Sick, took to Twitter recently on this subject.</p>

<figure>
  <blockquote>
  <p>I know there's a lot of scary stuff in the world right now, but this is something I've been thinking about that I can't get out of my head.</p>
  <p>As a cast member on a show about tech, our job entails visiting tech companies, conferences, etc. We meet people eager to show off new tech.</p>
  <p>Often we'll see tech that is scary. I don't mean weapons. I mean altering video, tech that violates privacy, stuff with obvious ethical issues.</p>
  <p>And we'll bring up our concerns to them. We are realizing that ZERO consideration seems to be given to the ethical implications of tech.</p>
  <p>They don't even have a pat rehearsed answer. They are shocked at being asked. </p><p>Which means nobody is asking those questions.</p>
  <p>"We're not making it for that reason but the way people choose to use it isn't our fault. Safeguards will develop." But tech is moving so fast.</p>
  <p>That there is no way humanity or laws can keep up. We don't even know how to deal with open death threats online.</p>
  <p>Only "Can we do this?" Never "should we do this?" We've seen that  same blasé attitude in how Twitter or Facebook deal with abuse and fake news.</p>

  <p>Tech has the capacity to destroy us. We see the negative effect of  social media. No ethical considerations are going into dev of  tech.</p>

  <p>You can't put this stuff back in the box. Once it's out there, it's out there. And there are no guardians. It's terrifying. The end.</p>
  </blockquote>
  <figcaption>
  Kumail Nanjiani <a href="https://twitter.com/i/moments/930118384225800192?ref_src=twsrc%5Etfw">November 1, 2017</a>
  </figcaption>
  </figure>

<p>It’s a major problem when we’re given so much power in tech, but we’re not doing anything to ensure that we use it safely. Thinking about what we’re doing and being careful not to build things that can be used maliciously is really important.</p>

<p>Make your own decisions. Make your own choices. Make your own judgement.</p>

<figure>
  <blockquote>
  <p>For engineers in particular, we develop systems. But the systems we develop can be used for different things. The software that I was using in Iraq is the same you’d use in marketing. It’s the same tools. It’s the same analysis.</p>
  <p>I guess technologists should realize that we have an ethical obligation to make decisions that go beyond just meeting deadlines for creating a product. Let’s actually take some chunks of time time and think ‘what are the consequences of this system? How can this be used? How can it be misused?’ Let’s try to figure out how we can mitigate a software system from being misused, or decide whether or not you want to implement it at all. There are systems that if misused can be very dangerous.</p>
  </blockquote>
  <figcaption>
  Chelsea Manning in
  <cite>
  <a href="https://www.wnyc.org/story/chelsea-manning-life-after-prison/" title="Chelsea Manning on Life After Prison fron The New Yorker Radio Hour">
  The New Yorker Radio Hour
</a> (31:55-33:40)
  </cite>
  </figcaption>
  </figure>

<p>Don’t get distracted by deadlines and feature requests. Think about the consequences of what you’re building. Build in safeguards to prevent misuse, or don’t build it at all because it’s too dangerous.</p>

<p>I’m asking you to do something about this. Well, I guess I’m asking you to not do something because of this. It’s only fair that we talk a bit about how and when to take a stand.</p>

<p>Let’s say I had a time machine and could go back in time to 2011 and do it all over again. I already have the foreknowledge that this tool is unethical. I’ve accepted this job. I’ve moved across the country from Tempe, Arizona to Brookville, Maryland. I’ve driven the two hour commute to Sterling, Virginia, home of <a href="https://www.governmentcontractswon.com/department/defense/sterling_va_virginia.asp">395 defense contractors awarded 16.8 trillion dollars in contracts over the past 16 years</a>. It’s my first job out of school, and it’s my first day. I don’t have my clearance so I’m an intern. My new project owner introduces me to the team then pulls me into a side room to give me an overview of the project. What do I say?</p>

<p>I think the first thing is to establish a mutual understanding of the task. It’s entirely possible at this point that I don’t understand what the actual thing is, and that I’m overreacting. I ask “Why are we finding these signals” and the project owner says “We want to find people’s cell phones.” “Who’s finding them, and why?” I ask. “I don’t know, probably some soldiers in the Middle East.” “Why?” I repeat. “I can’t tell you that.”</p>

<p>“I can’t tell you that” is something I got a lot from this project owner. It’s code for “I have clearance and I know things about this project. I know what you’re asking and I know the answer but I am not allowed to tell you.”</p>

<p>At this point, I think we have a mutual understanding. The task is to help soldiers find people’s phones, probably attached to those people. The reason is left unsaid but we both know.</p>

<p>This organization is a defense contractor. They build things for the military. It is their core competency. They’re not not going to do this…. On the other hand, I care a lot about not killing people. The company’s goal is to build things for the military. If my goal is not to let this happen, then there isn’t a good fit for me at this company. This probably means that the worst case here is that I’m going to leave today without a job. Either I’ll say no and they’ll fire me, or I’ll say “that’s not something I’m comfortable with, best of luck” and quit. These are the worst case scenarios, not necessarily what will happen.</p>

<p>Before saying no then, I need to consider: Can I afford to leave here without a job financially? Am I likely to be able to rely on my network to get me another job? Have I built up a trust with my employer where I can go to them with this type of thing and feel confident that I’ll be heard out? The answer to these questions was no for me in 2011. Sometimes, something is important enough that you should still do something, but there’s a lot that goes into these decisions. I’d like to think that I would still say no.</p>

<p>Let’s look at another situation, where someone did the ethical thing. A developer we’ll call Alice received a strange request. We want to identify weak passwords in the system to notify users to change them. We’d like you to run a password cracking program on the very, very, large password database.</p>

<p>This was a long time ago, before aged passwords were common. Expiring old passwords wasn’t a straightforward option. Alice thought this was a weird request, but said that if the appropriate paperwork was completed she would be willing.</p>

<p>Alice received the completed paperwork and ran the password crack. The next request was “We’d like the list of users along with their weak passwords”. Alice knew that her coworkers had a valid desire to help customers improve their passwords. She also knew that users often re-used passwords. Combining the email and password into one report could allow someone to log into the customers’ accounts on other websites.</p>

<p>Alice pointed this out to her manager, and together they worked with the CSA team to design an email that didn’t include the password. Customers received notifications about their weak passwords, and there was less risk of the report falling into malicious hands. No one was fired and Alice built up trust within her team.</p>

<p>Different scenarios need different ways of analyzing what you should do. In some cases,  the right thing to do say nothing and build the product. It isn’t a simple thing to make this decision.</p>

<p>But don’t get distracted by having to think through it. Sometimes your code can kill people.</p>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://www.theatlantic.com/video/index/541797/anil-dash-tech-ethics">Does Technology Need to Be Ethical?</a>
Anil Dash briefly talks to The Atlantic about tech ethics.</li>
  <li><a href="http://www.bbc.com/news/technology-35639549">Is your smartphone listening to you?</a>
The BBC addresses whether tech companies use your phone to listen to what you’re saying while not using their apps, and if they even can.</li>
  <li><a href="http://www.hcpro.com/HOM-236942-5728/know-your-ethical-obligations-regarding-coding-and-documentation">Know your ethical obligations regarding coding and documentation</a>
A blog post on how to define your ethical obligations as a programmer, some ways of dealing with them, and some real world examples</li>
  <li><a href="https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-project.html">‘The Business of War’: Google Employees Protest Work for the Pentagon</a>
Google employees ask Google’s CEO not to build software to help the military build warfare technology</li>
  <li><a href="https://www.theverge.com/2015/10/8/9481651/volkswagen-congressional-hearing-diesel-scandal-fault">Volkswagen America’s CEO blames software engineers for emissions cheating scandal</a>
VW’s CEO blames software engineers for changing how diesel emissions are reported during tests. Even if they were doing what they were told, someone up the chain can throw the blame back onto the programmers.</li>
</ul>

<h2 id="glossary-of-terms">Glossary of Terms</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator">Gaussian Estimation</a>
Used in the WiFi geolocation algorithm to estimate free space path loss using probability density estimation. It said “it’s less likely to be nearby if you’re looking for it and more likely to be further out”.</li>
  <li><a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3">Genetic Algorithm</a>
A type of machine learning or searching that is inspired by the theory of evolution. It uses randomization to create populations of individuals and tests their fitness to determine which ones will reproduce. It was used to increase accuracy in the geolocation algorithm.</li>
  <li><a href="https://heroku.com/">Heroku</a>
A cloud platform as a service (PaaS) supporting several programming languages, that is used as a web application deployment model. It supports Java, Node.js, Scala, Clojure, Python, PHP, and Go.</li>
  <li><a href="https://schneems.com/2017/06/12/bayes-is-bae/">Kalman Filter</a>
A Kalman Filter can be used any time you have a model of motion and some noisy data that you want to produce a more accurate prediction.</li>
  <li><a href="https://stackoverflow.com/a/1988826/218211">Memoization</a>
Remembering the result of a calculation based on its arguments, and then using that result instead of re-calculating if the same method call is made with the same arguments. Caleb’s team did this with a hash/dictionary/map (different names for the same thing) that contained the location pairs as keys and the distance between them as values.</li>
  <li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R2 Algorithm</a>
Used in the WiFi geolocation algorithm to measure the difference between expected and actual signal strength for each point in a search grid. The smallest difference is the most likely to be the correct distance from the source.</li>
</ul>


    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The PayPal Mafia is taking over America's government (132 pts)]]></title>
            <link>https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</link>
            <guid>42387549</guid>
            <pubDate>Wed, 11 Dec 2024 13:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government">https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</a>, See on <a href="https://news.ycombinator.com/item?id=42387549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main" id="content"><article data-test-id="Article" id="new-article-template"><div data-test-id="standard-article-template"><section><p><span><a href="https://www.economist.com/business" data-analytics="sidebar:section"><span>Business</span></a></span><span> | <!-- -->All-in on Donald Trump</span></p><h2>America’s right-wing tech bros are celebrating Donald Trump’s victory</h2></section><section><figure><img alt="Palace of Fine Arts at night with the Golden Gate Bridge in the background, San Francisco, California, USA. " fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="(min-width: 960px) 700px, 95vw" srcset="https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg"><figcaption><span>Photograph: Getty Images</span></figcaption></figure></section><div><div><p><time datetime="2024-12-10T17:55:20.458Z"> <!-- -->Dec 10th 2024</time><span>|</span><span>SAN FRANCISCO</span></p></div><section data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">O</span><small>n the night</small> of December 7th San Francisco’s Palace of Fine Arts, with its lakeside colonnade echoing a Roman ruin, turned into Mar-a-Lago, as <a href="https://www.economist.com/united-states/2024/11/21/how-donald-trump-could-win-the-future">Silicon Valley’s newly emboldened right-wingers</a> gathered for a Christmas bash organised by the All-In podcast. The festive good cheer did not extend to everyone;&nbsp;<i>The Economist</i> was made to feel most unwelcome. But not before being privy to a riotous celebration of how a clique of billionaires—the so-called PayPal Mafia—helped clinch Donald Trump’s election victory and has taken Washington by storm.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/elon-musk" data-analytics="tags:elon_musk"><span>Elon Musk</span></a><a href="https://www.economist.com/topics/donald-trump" data-analytics="tags:donald_trump"><span>Donald Trump</span></a><a href="https://www.economist.com/topics/united-states" data-analytics="tags:united_states"><span>United States</span></a></nav></p></div></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making memcpy(NULL, NULL, 0) well-defined (144 pts)]]></title>
            <link>https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</link>
            <guid>42387013</guid>
            <pubDate>Wed, 11 Dec 2024 12:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined">https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</a>, See on <a href="https://news.ycombinator.com/item?id=42387013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>Undefined behavior (UB) in the C programming language is a regular source of heated discussions among programmers. On the one hand, UB can be important for compiler optimizations. On the other hand, it makes is easy to introduce bugs that lead to security issues.</p><p>The good news is that <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3322.pdf">N3322</a> has been accepted for C2y, which will remove undefined behavior from one particular corner of the C language, making all of the following well-defined:</p><pre><code>memcpy(NULL, NULL, 0);
memcmp(NULL, NULL, 0);
(int *)NULL + 0;
(int *)NULL - 0;
(int *)NULL - (int *)NULL;</code></pre><p>This only applies when a null pointer is combined with a "zero-length" operation. The following are still undefined:</p><pre><code>memcpy(NULL, NULL, 4);
(int *)NULL + 4;</code></pre><p>The removal of this undefined behavior is not expected to have any negative impact on performance. In fact, the reverse is true.</p><h2>Motivation</h2><p>The examples above are somewhat silly because they hard-code a <code>NULL</code>/<code>nullptr</code> constant. However, it is easy to run into this situation with a pointer that is only sometimes null. For example, consider a typical representation for a string with a known length:</p><pre><code>struct str {
   char *data;
   size_t len;
};</code></pre><p>An empty string would usually be represented as <code>(struct str) { .data = NULL, .len = 0 }</code>, with the <code>data</code> pointer being <code>NULL</code>. Now, consider a function that checks if two strings are equal:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0;
}</code></pre><p>This implementation looks very reasonable at first glance. However, it exhibits undefined behavior if both of the inputs are empty strings. In that case, we will call <code>memcmp(NULL, NULL, 0)</code>, which is undefined behavior according to the C standard.</p><p>This kind of UB introduces the risk that the compiler will optimize away following null pointer checks. For example, GCC will happily remove the <code>dest == NULL</code> branch in the following code, while Clang deliberately does not perform this optimization:</p><pre><code>int test(char *dest, const char *src, size_t len) {
   memcpy(dest, src, len);
   if (dest == NULL) {
       // This branch will be removed by GCC due to undefined behavior.
   }
}</code></pre><p>The correct way to write the <code>str_eq</code> function is as follows:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          (str1-&gt;len == 0 ||
           memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0);
}</code></pre><p>The new code is correct, but worse in every other way:</p><ul><li>It increases code size, by requiring an extra check at each inlined call-site.</li><li>It decreases performance, by redundantly checking something <code>memcmp</code> has to handle anyway.</li><li>It increases code complexity.</li></ul><p>At the same time, there is no useful way in which the C library can make use of this undefined behavior to provide a more efficient implementation. This is the kind of UB that benefits nobody, and should be removed from the language.</p><h2>Null pointer arithmetic</h2><p>The original proposal was focused on removing UB for memory library calls, but an early reviewer pointed out that this is not sufficient. After all, we also need to take into account how these library functions are implemented.</p><p>For example, let's consider a typical implementation for a <code>memcpy</code>-like function:</p><pre><code>void copy(char *dst, const char *src, size_t n) {
   for (const char *end = src + n; src &lt; end; src++) {
       *dst++ = *src;
   }
}</code></pre><p>This function exhibits undefined behavior when called as <code>copy(NULL, NULL, 0)</code>, because <code>NULL + 0</code> is undefined behavior in C.</p><p>To avoid this, and make the overall language self-consistent, we need to define <code>NULL + 0</code> as returning <code>NULL</code> and <code>NULL - NULL</code> as returning 0. This also aligns C with C++ semantics, where this was already well-defined.</p><h2>Opposition</h2><p>When this proposal was discussed at two WG14 meetings, the opposition didn't come from the direction I expected.</p><p>The most broadly controversial part of the proposal was to define <code>NULL - NULL</code> as returning 0. The reason for this is that when address spaces get involved (which are not part of standard C, but may be implemented as an extension), there may be multiple representations of a null pointer. Making sure that subtracting two "different" nulls still results in zero might require the generation of additional code, breaking the premise that this change is entirely free.</p><p>However, the most vocal opposition came from a static analysis perspective: Making null pointers well-defined for zero length means that static analyzers can no longer unconditionally report <code>NULL</code> being passed to functions like <code>memcpy</code>—they also need to take the length into account now. If an <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3089.pdf"><code>_Optional</code> qualifier</a> is introduced in the future, <code>memcpy</code> arguments would have to be qualified with it. GCC is considering the introduction of a <a href="https://gcc.gnu.org/pipermail/gcc-patches/2024-November/668505.html"><code>nonnull_if_nonzero</code></a> attribute to represent the new pre-condition.</p><p>After the seemingly negative discussion, I was somewhat surprised that the vote not only went strongly in favor of the change, but also came with a recommendation to implementers to apply the change <a href="https://www.open-std.org/jtc1/sc22/wg14/www/previous.html">retroactively</a> to old standard versions. This means that, once compilers and C libraries have implemented the change, it should apply even without specifying the <code>-std=c2y</code> flag.</p><h2>Compiler builtins</h2><p>I work on the middle-end of the <a href="https://llvm.org/">LLVM</a> compiler toolchain. Being far removed from any "user-facing" parts of the compiler, I am generally not involved with standardization efforts.</p><p>The reason I got involved here at all is the specification for LLVM's internal memcpy intrinsic:</p><blockquote><div><p>The <code>llvm.memcpy.*</code> intrinsics copy a block of memory from the source location to the destination location, which must either be equal or non-overlapping. [...]</p><p>If <code>&lt;len&gt;</code> is 0, it is no-op modulo the behavior of attributes attached to the arguments. [...]</p></div></blockquote><p>The <code>llvm.memcpy</code> intrinsic may lower to a call to the <code>memcpy</code> function, which is treated as a "compiler runtime builtin" here, even though it is ultimately also provided by the C library.</p><p>When used as a builtin, LLVM requires that both <code>memcpy(x, x, s)</code> and <code>memcpy(NULL, NULL, 0)</code> are well-defined, even though the C standard says they are UB. GCC and MSVC have similar assumptions.</p><p>Making <code>memcpy(NULL, NULL, 0)</code> officially well-defined removes one of the assumptions, while the <code>memcpy(x, x, s)</code> case remains for now. Allowing this was originally also part of the proposal, but was later dropped, because it didn't fit well with the other changes.</p><p>In a weird turn of events, this change to the C standard came about because Rust developers kept nagging me about the mismatch between LLVM and C semantics.</p><h2>Acknowledgements</h2><p>This paper was a collaboration with Aaron Ballman, who also drove the discussion during the actual WG14 meetings. Special thanks go to David Stone, whose early feedback radically changed the direction of the proposal from memory library calls in particular to "zero-length" operations in general.</p>
          
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astronomy Photographer of the Year 2024 winners (119 pts)]]></title>
            <link>https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024</link>
            <guid>42385761</guid>
            <pubDate>Wed, 11 Dec 2024 07:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024">https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42385761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
    
  

  



  




  

  

  

  <main role="main">
    
    <div id="block-rmg-theme-content">
  
    
      <article data-content-type="topic">

  
    

  
  <div>
              
              <div>
              <p>
          <h2>
            The overall winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-09/OS-348550-1%20Distorted%20Shadows%20of%20the%20Moon%27s%20Surface%20Created%20by%20an%20Annular%20Eclipse%20v2_0.jpg?itok=NnUAJz-h" width="1200" height="675" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Ryan Imperio
      </span>
                  </p></div>
      <div>
                  <h3>
            Distorted Shadows of the Moon's Surface Created by an Annular Eclipse by Ryan Imperio
      </h3>
                <div>
          
            <p>Photographer Ryan Imperio from the United States has been named the overall winner of Astronomy Photographer of the Year 16.</p><p>The image is a <a href="https://www.rmg.co.uk/stories/topics/beads-sunlight-photographing-annular-solar-eclipse-astronomy-photographer-year" data-entity-type="node" data-entity-uuid="415695bc-72b7-4740-a1af-726249f49004" data-entity-substitution="canonical" title="Beads of sunlight: photographing an annular solar eclipse" data-gtm-name="CTA" data-gtm-detail="formatted content">composite of more than 30 separate photographs of the Sun</a>, taken in Texas during the annular solar eclipse of 14 October 2023.</p><p>Together the photographs capture the fleeting optical illusion known as 'Baily's beads', which occurs when sunlight shines through the valleys and craters of the Moon.</p><p>“What an innovative way to map the Moon’s topography at the point of third contact during an annular solar eclipse," competition judge Kerry-Ann Lecky Hepburn says. "This is an impressive dissection of the fleeting few seconds during the visibility of the Baily’s beads. This image left me captivated and amazed. It’s exceptional work deserving of high recognition. Congratulations!”</p><p><a href="https://www.rmg.co.uk/stories/topics/beads-sunlight-photographing-annular-solar-eclipse-astronomy-photographer-year" data-entity-type="node" data-entity-uuid="415695bc-72b7-4740-a1af-726249f49004" data-entity-substitution="canonical" title="Beads of sunlight: photographing an annular solar eclipse" data-gtm-name="CTA" data-gtm-detail="formatted content">Learn more about the winning image</a></p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/our-sun-2024" data-entity-type="node" data-entity-uuid="9ce1c711-01dc-45a0-afc0-966a5252bb2f" data-entity-substitution="canonical" title="Our Sun 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Our Sun shortlist</a><br>&nbsp;</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
        <h2>
            Never miss a shooting star
      </h2>        
            <p><span>Sign up to our monthly space newsletter for amazing space stories, astronomy guides and more from Royal Museums Greenwich.</span></p>
      
      </div>
              <div>
              <p>
          <h2>
            Skyscapes category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Tasman%20Gems%20%C2%A9%20Tom%20Rae%20%E2%80%93%20Astronomy%20Photographer%20of%20the%20Year%202024%20Skyscapes%20.jpg?itok=aKcF1NY1" width="1200" height="761" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Rae
      </span>
                  </p></div>
      <div>
                  <h3>
            Tasman Gems by Tom Rae
      </h3>
                <div>
          
            <p>"It’s very challenging to create this sort of composition without tipping the balance in favour of either foreground or background. Here, the grass and central rock retain wonderful detail, as do the midground mountains, but the vibrance and detail in the cosmic background shine through as well.&nbsp;</p><p>"Even the airglow adds to the image, which is no easy feat! As well as being technically impressive, the balance also produces a sort of surreal quality. A slightly dream-like connection between the Earth-bound and the celestial."</p><p>– Ed Bloomer, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/skyscapes-2024" data-entity-type="node" data-entity-uuid="144ebd7a-1f52-4905-affd-4eaa7e2fb0b9" data-entity-substitution="canonical" title="Skyscapes 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Skyscapes shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Galaxies category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/G-68776-63-03518%20Echoes%20of%20the%20Past%20%C2%A9%20Bence%20T%C3%B3th%20and%20P%C3%A9ter%20Felt%C3%B3ti.jpg?itok=J4yknJ6w" width="1200" height="705" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Bence Tóth and Péter Feltóti
      </span>
                  </p></div>
      <div>
                  <h3>
            Echoes of the Past by Bence Tóth and Péter Feltóti
      </h3>
                <div>
          
            <p>"Galaxies are among the most amazing phenomena you can observe with a telescope. Each is unique, but some are more special than others. Centaurus A is one of the most extraordinary of its kind, and this image certainly stands out among galaxy photos. Thanks to accurate photon collection, precise image processing and cooperation between fellow astrophotographers, a photo taken on a challenging expedition has become one of the best captures of this object to date."</p><p>– László Francsics, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/galaxies-2024" data-entity-type="node" data-entity-uuid="6afa0eb3-4926-4ec9-9dce-41a80bb8e208" data-entity-substitution="canonical" title="Galaxies 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Galaxies shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Our Moon category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Shadow%20Peaks%20of%20Sinus%20Iridum%20%C2%A9%20G%C3%A1bor%20Bal%C3%A1zs%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Our%20Moon.jpg?itok=86_Tl91h" width="1200" height="865" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Gábor Balázs
      </span>
                  </p></div>
      <div>
                  <h3>
            Shadow Peaks of Sinus Iridum by Gábor Balázs
      </h3>
                <div>
          
            <p>"This is a very impressive image. Sinus Iridum, known as the 'Bay of Rainbows', is about 260 km in diameter and is bordered by several smaller craters, showcasing the Moon’s rugged terrain.</p><p>"The detailed capture of Pythagoras, noted for its depth and complex features, is enhanced by the phenomenon of libration, where slight oscillations in the Moon’s orientation allow Earth-bound observers a glimpse of areas typically hidden from view. This image not only highlights the capabilities of modern astrophotography equipment but also offers a vivid illustration of lunar surface features, contributing valuable insights into lunar geology."</p><p>– Yuri Beletsky, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/our-moon-2024" data-entity-type="node" data-entity-uuid="87bb24ef-6d32-4c1f-8e29-4f969533135c" data-entity-substitution="canonical" title="Our Moon 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Our Moon shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Aurorae category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Queenstown%20Aurora%20%C2%A9%20Larryn%20Rae%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Aurorae.jpg?itok=D3ZS7yl8" width="1200" height="722" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Larryn Rae
      </span>
                  </p></div>
      <div>
                  <h3>
            Queenstown Aurora by Larryn Rae
      </h3>
                <div>
          
            <p>"This extraordinary panoramic image captures an aurora with rare red and pink hues over New Zealand. This is a phenomenon typically seen near the poles but appears here due to intense solar activity. The vivid red colours are produced at high altitudes when charged particles from solar flares and coronal mass ejections interact with oxygen in Earth’s atmosphere.&nbsp;</p><p>"Red aurorae are less common than green ones, which occur at lower altitudes where there is more oxygen to interact with and a higher density of atoms. This makes the observation more unique and special; an event typically associated with significant solar activity and usually only visible under clear, dark sky conditions."</p><p>– Yuri Beletsky, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/aurorae-2024" data-entity-type="node" data-entity-uuid="c5015161-7fe5-4911-b5b0-06d2f5c94be0" data-entity-substitution="canonical" title="Aurorae 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Aurorae shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Planets, Comets and Asteroids category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/PCA-195480-170%20On%20Approach%20%C2%A9%20Tom%20Williams%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Planets%20Comets%20and%20Asteroids.jpg?itok=KQkif32_" width="1200" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Williams
      </span>
                  </p></div>
      <div>
                  <h3>
            On Approach by Tom Williams
      </h3>
                <div>
          
            <p>"Venus shares very little with Earth-bound observers. Its highly reflective clouds show no detail when using conventional imaging methods. This photographer, however, has managed to tease a startling level of detail out of the phases shown here. Although the colours used are false, they are not too far from the natural colour of the planet. The thoughtful compositional work in the accurate scaling of the three phases is just the icing on the cake."</p><p>– Steve Marsh, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/planets-comets-asteroids-2024" data-entity-type="node" data-entity-uuid="e5ad6088-ae9e-4510-9af1-7b24fa53ffd9" data-entity-substitution="canonical" title="Planets, Comets and Asteroids 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Planets, Comets and Asteroids shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            People and Space category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/High-tech%20Silhouette%20%C2%A9%20Tom%20Williams%20%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20People%20and%20Space.jpg?itok=jM2P8Gei" width="935" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Williams
      </span>
                  </p></div>
      <div>
                  <h3>
            High-tech Silhouette by Tom Williams
      </h3>
                <div>
          
            <p>"This dramatic image serves as a powerful reminder of humanity’s ongoing presence in space. The photographer has done a great job in perfectly timing this shot so that the International Space Station is silhouetted against the backdrop of the Sun’s eastern solar limb.&nbsp;</p><p>"The photograph beautifully showcases the dynamic and active nature of the Sun, bringing it to life in a captivating way. Yet among that, your eye is permanently fixed on the tiny human-made spacecraft making its way across, emphasizing its significance amid the grandeur of the Sun. It’s a worthy winner of this category."</p><p>– Melissa Brobby, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/people-space-2024" data-entity-type="node" data-entity-uuid="2da63a75-3b58-440b-829c-032441f64fbb" data-entity-substitution="canonical" title="People and Space 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full People and Space shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Stars and Nebulae category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/SNR%20G107.5-5.2%2C%20Unexpected%20Discovery%20%28the%20Nereides%20Nebula%20in%20Cassiopeia%29%20%C2%A9%20Marcel%20Drechsler%2C%20Bray%20Falls%2C%20Yann%20Sainty%2C%20Nicolas%20Martino%20and%20Richard%20Galli.jpg?itok=8-dm1sfe" width="1099" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Marcel Drechsler, Bray Falls, Yann Sainty, Nicolas Martino and Richard Galli
      </span>
                  </p></div>
      <div>
                  <h3>
            SNR G107.5-5.2, Unexpected Discovery (the Nereides Nebula in Cassiopeia) by Marcel Drechsler, Bray Falls, Yann Sainty, Nicolas Martino and Richard Galli
      </h3>
                <div>
          
            <p>"The hits keep on coming from this team with another stunning revelation for us. Who knew this fantastic and delicate structure was there all along in one of the best-known constellations in the night sky? The thoughtful processing and clever use of colouring really make the supernova remnant pop against its background. It’s one of those images that you can stare into for hours and still find more detail. Stunning!"</p><p>– Steve Marsh, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/stars-nebulae-2024" data-entity-type="node" data-entity-uuid="6f020dfb-7c21-4b6a-881d-a9d37c656966" data-entity-substitution="canonical" title="Stars and Nebulae 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Stars and Nebulae shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            The Sir Patrick Moore Prize for Best Newcomer
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/BN-312741-1%20SH2-308%20Dolphin%20Head%20Nebula%20%C2%A9%20Xin%20Feng%20and%20Miao%20Gong%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Best%20Newcomer.jpg?itok=xhy4VGLX" width="1200" height="799" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Xin Feng and Miao Gong
      </span>
                  </p></div>
      <div>
                  <h3>
            SH2-308: Dolphin Head Nebula by Xin Feng and Miao Gong
      </h3>
                <div>
          
            <p>"The Dolphin Head Nebula is a bubble of hydrogen pushed out from a very luminous Wolf-Rayet star. Stellar winds of over 1,500 km per second make the region rather more lively than even its animal namesake.</p><p>"This image is wonderfully detailed, and really displays the three-dimensional nature of the nebula. It is vibrant, without losing the very delicate surrounding structures, and you can clearly make out another little planetary nebula bubble (called PN G234.9-09.7) towards the bottom of the dolphin’s head, which is rarely imaged with any clarity. Very, very impressive work from any astrophotographer, let alone a newcomer."</p><p>– Ed Bloomer, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/best-newcomer-2024" data-entity-type="node" data-entity-uuid="edf7197f-cdc7-44b1-87f6-d96c3da5f132" data-entity-substitution="canonical" title="Best Newcomer 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Best Newcomer shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            The Annie Maunder Prize for Image Innovation
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-09/Anatomy%20of%20a%20Habitable%20Planet%20%C2%A9%20Sergio%20Di%CC%81az%20Ruiz%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Annie%20Maunder%20Prize.jpg?itok=xMIVsv5Y" width="1200" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Sergio Díaz Ruiz
      </span>
                  </p></div>
      <div>
                  <h3>
            Anatomy of a Habitable Planet by Sergio Díaz Ruiz 
      </h3>
                <div>
          
            <p>"This strangely familiar representation of the Earth transforms scientific data through colour mapping to highlight the devastation already inflicted on our world. The image poignantly emphasizes the significant environmental challenges we face and the urgent need to protect and preserve our planet."</p><p>– Victoria Lane, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/image-innovation-prize-2024" data-entity-type="node" data-entity-uuid="637e10c4-4c30-4c15-8aa0-3935111d2857" data-entity-substitution="canonical" title="Annie Maunder Prize for Image Innovation 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Annie Maunder Prize shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Young Competition
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/NGC%201499%2C%20a%20Dusty%20California%20%C2%A9%20Daniele%20Borsari%20-%20Astronomy%20Photographer%20of%20the%20Year%20Young.jpg?itok=33mHEQiH" width="1200" height="995" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Daniele Borsari
      </span>
                  </p></div>
      <div>
                  <h3>
            NGC 1499, a Dusty California by Daniele Borsari
      </h3>
                <div>
          
            <p>"This incredibly beautiful image was very popular with the panel. Not least because it captures a nebula, atmospheric gases and has extraordinary balance of light, composition and structure. The future of astronomy photography being fearlessly, and openly, taken forward by a new generation."</p><p>– Neal White, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/young-competition-2024" data-entity-type="node" data-entity-uuid="9792c8b9-097c-4d3c-b3e5-4bbfb43fee7f" data-entity-substitution="canonical" title="Young Competition 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Young Competition shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
                  <h2>
            Find out more about the competition
      </h2>
                        <div>
                                                <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2020-11/PS-8826-5_Ineffable%20%C2%A9%20Alyn%20Wallace_0.jpg?itok=TNy3jqPy" alt="" loading="lazy"></p><p>Competition</p>                                      </div>
                  <div>
                      
                                            <p>
                        Key dates, prizes and details on how to enter Astronomy Photographer of the Year
                      </p>
                    </div>
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/Collection-13-cover-rmg-publication-banner.jpg?v=1730196645" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    £30.00
                                                                              </p>
                                            <p>
                        Astronomy Photographer of the Year: Collection 13 is a stunning gift for admirers of astrophotography. The competition's official book, this spectacular astronomy photography book showcases the most spectacular space photography, taken from locations across the globe...
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/migrations/G-28529-27_Winner%20and%20Overall%20Winner_Andromeda%20Galaxy%20at%20Arm_s%20Length%20%C2%A9%20Nicolas%20Lefaudeux_2.jpg?itok=QhE33xgR" alt="" loading="lazy"></p><p>Past winners</p>                                      </div>
                  <div>
                      
                                            <p>
                        Astronomy Photographer of the Year has been held every year since 2009. Take a journey back in space and time and explore all the past winning images
                      </p>
                    </div>
                                  </div>
                              </div>
      </div>
              <div>
                  <h2>
            Get into astrophotography
      </h2>
                        <div>
                                                <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2023-09/Monika%20Deviat%20silhouetted%20taking%20a%20photo%20at%20night.png?itok=85Mib32v" alt="" loading="lazy"></p><p>Course</p>                                      </div>
                  <div>
                      
                                            <p>
                        Learn how to take images of the night sky in the Royal Observatory Greenwich's online introductory astrophotography course
                      </p>
                    </div>
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/2025-GTTNS-RMG-publication.jpg?v=1730285783" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    £6.99
                                                                              </p>
                                            <p>
                        
Annually, Guide to the Night Sky is the bestselling stargazing handbook to the planets, stars, and constellations visible from the northern hemisphere...
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-06/A%20woman%20looks%20at%20the%20photographs%20on%20display%20in%20the%20Astronomy%20Photographer%20of%20the%20Year%20exhibition%20at%20the%20National%20Maritime%20Museum%20%28T3782-127%29.jpg?itok=JEquG48o" alt="" loading="lazy"></p><p>Inspiration</p>                                      </div>
                  <div>
                      
                                            <p>
                        Want to get into astrophotography but don’t know where to begin? Photographers from Astronomy Photographer of the Year reveal their top tips
                      </p>
                    </div>
                                  </div>
                              </div>
      </div>
              <div>
      <div>
                      <p>
      <h2>
            Our partners
      </h2>
    </p>
                    </div>
      <div>
                              <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2021-08/Liberty_Specialty_Markets_black%5B22%5D.png?itok=8_BWwpaC" width="277" height="96" alt="Liberty Specialty Markets black" loading="lazy">



      
</p>
            </div>
                      <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2022-06/BBC%20Sky%20At%20Night%20logo.png?itok=aLkJU--4" width="193" height="96" alt="BBC Sky at Night logo in black" loading="lazy">



      
</p>
            </div>
                        </div>
    </div>
          </div>

</article>


  </div>  </main>

  
    

  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You need 4 colors (102 pts)]]></title>
            <link>https://www.iamsajid.com/colors/</link>
            <guid>42385357</guid>
            <pubDate>Wed, 11 Dec 2024 06:31:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iamsajid.com/colors/">https://www.iamsajid.com/colors/</a>, See on <a href="https://news.ycombinator.com/item?id=42385357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <pre id="code">:root {
        --color-primary: hsl(0, 50%, 90%);
        --color-secondary: hsl(0, 50%, 10%);
        --color-tertiary: hsl(60, 80%, 20%);
        --color-accent: hsl(300, 80%, 20%);
    }
.dark {
        --color-primary: hsl(0, 50%, 10%);
        --color-secondary: hsl(0, 50%, 90%);
        --color-tertiary: hsl(60, 80%, 80%);
        --color-accent: hsl(300, 80%, 80%);
    }</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge Refuses to Allow Sale of Infowars to The Onion (360 pts)]]></title>
            <link>https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html</link>
            <guid>42384921</guid>
            <pubDate>Wed, 11 Dec 2024 04:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html">https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html</a>, See on <a href="https://news.ycombinator.com/item?id=42384921">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Case Against Google's Claims of "Quantum Supremacy" (112 pts)]]></title>
            <link>https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/</link>
            <guid>42384768</guid>
            <pubDate>Wed, 11 Dec 2024 04:22:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/">https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/</a>, See on <a href="https://news.ycombinator.com/item?id=42384768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>The 2019 paper “<a href="https://www.nature.com/articles/s41586-019-1666-5">Quantum supremacy using a programmable superconducting processor</a>”&nbsp; asserted that Google’s Sycamore quantum computer,&nbsp; with 53 qubits and a depth of 20, performed a specific computation in about 200 seconds. According to Google’s estimate, a state-of-the-art classical supercomputer would require approximately 10,000 years to complete the same computation.</p>
<p>The Google experiment had two major components:</p>
<ol>
<li><strong>The “Fidelity Claims”</strong>: Assertions regarding the fidelity of the samples produced by the quantum computer.</li>
<li><strong>The “Supremacy Claims”</strong>: Assertions that translated fidelity into a measure of advantage over classical computation.</li>
</ol>
<p>There are valid reasons to question both of these claims in the context of Google’s 2019 experiment. In my view, these claims may reflect serious methodological mistakes rather than an objective scientific reality. I do not recommend treating Google’s past or future claims as a solid foundation for policy-making decisions.</p>
<p>Below is a brief review of the case against Google’s 2019 claims of quantum supremacy:</p>
<p><strong>A) The “Supremacy” Assertions: Flawed Estimation of Classical Running Time</strong></p>
<p><strong>A.1)</strong> The claims regarding classical running times were off by 10 orders of magnitude.</p>
<p><strong>A.2)</strong> Moreover, the Google team was aware that better classical algorithms existed. They had developed more sophisticated classical algorithms for one class of circuits and subsequently changed the type of circuits used for the “supremacy demonstration” just weeks before the final experiment.</p>
<p><strong>A.3)</strong> The 2019 Google paper states, <em>“Quantum processors have thus reached the regime of quantum supremacy. We expect that their computational power will continue to grow at a double-exponential rate.”</em> It is surprising to encounter such an extraordinary claim in a scientific paper.</p>
<h3><strong>B) The “Fidelity” Assertions: Statistically Unreasonable Predictions Indicating Methodological Flaws</strong></h3>
<p>The google paper relies on a very simple <em>a priori</em> prediction of the fidelity based on the error-rates of individual components. (Formula (77).)</p>
<p><strong>B.1)</strong> The agreement between the <em>a priori</em> prediction and the actual estimated fidelity is statistically implausible (“too good to be true”): It is unlikely that the fidelities of samples from hundreds of circuits would agree within 10-20% with a simple formula based on the multiplication of the fidelities of individual components.&nbsp; In my opinion, this suggests a methodologically flawed optimization process, such as the one described in item C.&nbsp;&nbsp;</p>
<p><strong>B.2)</strong> The Google team provided a statistical explanation for this agreement based on three premises. The first premise is that the fidelities for the individual components are exact up to&nbsp; ±20%. The second premise is that this ±20% instability is unbiased. The third premise is that all these fidelities for individual components are statistically independent. These premises are unreasonable and they contradict various other experimental findings.</p>
<p><strong>B.3)</strong> As of now, the error rates for individual components have not been released by the Google team. (Most recently, in May 2023, they promised “to push” for this data.) Analysis of the partial data provided for readout errors reinforces these concerns.</p>
<h3><strong>C) The Calibration Process: Evidence of Undocumented Global Optimization</strong></h3>
<p>According to the Google paper, calibration was performed prior to running the random circuit experiments and was based on the behavior of 1- and 2-qubit circuits. This process involved modifying the definitions of 1-gates and 2-gates to align with how the quantum computer operates.</p>
<p><strong>C.1)</strong> Statistical evidence suggests that the calibration process involved a methodologically flawed <em>global</em> optimization process. (This concern applies even to Google’s assertions about the fidelity of the smallest 12-qubit circuits.)</p>
<p><strong>C.2)</strong> Non-statistical evidence also supports this claim. For example, contrary to the description provided by the Google team, it was revealed that they supplied an outdated calibration version (for the experimental circuits) to the Jülich Research Center scientists involved in the experiment. This calibration was later further modified after the experiment was conducted. (This discrepancy is also reflected in a <a href="https://youtu.be/-ZNEzzDcllU?si=-CiGRIUn3rz7Sc0m">video released</a> by Google particularly between <a href="https://youtu.be/-ZNEzzDcllU?si=fTVx-zCzXVLRtKG3&amp;t=133">2:13-3:07</a>.)</p>
<p><strong>C.3)</strong> The Google team has not disclosed their calibration programs, citing them as a commercial secret. For technical reasons, they were also unable to share the<em> inputs</em> for the calibration program, although they promised to do so in future experiments—a promise that has not yet been fulfilled.</p>
<p><a href="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp"><img data-attachment-id="27584" data-permalink="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/google-slide13-2/" data-orig-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp" data-orig-size="1024,560" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="google-slide13" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=300" data-large-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=640" src="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp" alt="google-slide13" width="549" height="300" srcset="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=549&amp;h=300 549w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=150&amp;h=82 150w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=300&amp;h=164 300w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=768&amp;h=420 768w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp 1024w" sizes="(max-width: 549px) 100vw, 549px"></a></p>
<p><span>A slide from my 2019 lecture “<a href="https://youtu.be/p18P1y8GD9U?si=1nNQmHGFgdxTAln1">The Google quantum supremacy demo</a>” (<a href="https://gilkalai.wordpress.com/2019/12/27/the-google-quantum-supremacy-demo/">post</a>), highlights that the error rates for two-qubit gates <img src="https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="e_g"> have not yet been provided by the Google team as of today (Dec. 2024).</span></p>
<h3><strong>D) Comparing Google with IBM</strong></h3>
<p>As far as we know, there is a significant gap (in favor of Google) between what IBM quantum computers—which are in some ways more advanced than Google’s quantum computers—can achieve for random circuit sampling and what Google claims, even for circuits with 7–12 qubits. While one might argue that Google’s devices or team are simply better, in my view, this gap more likely reflects methodological issues in Google’s experiments.</p>
<h3><strong>E) (Not) Adopting Suggestions for Better Control</strong></h3>
<p>In our discussions with the Google team, they endorsed several of our suggestions for future experiments aimed at improving control over the quality of their experiments. However, in practice, later experiments did not implement any of these suggestions. Moreover, the structure of these later experiments makes them even harder to verify compared to the 2019 experiment. Additionally, unlike the 2019 experiment, the data for a subsequent random circuit sampling experiment does not include the amplitudes computed for the experimental circuits, further complicating efforts to scrutinize the results.</p>
<h3><strong>F) My Personal Conclusion</strong></h3>
<p>Google Quantum AI’s claims&nbsp; (including published ones) should be approached with caution, particularly those of an extraordinary nature. These claims may stem from significant methodological errors and, as such, may reflect the researchers’ expectations more than objective scientific reality. I do not recommend treating Google’s past or future claims as a solid basis for policy-making decisions.</p>
<h3><strong>G) Remarks</strong></h3>
<p><strong>G.1)</strong> Google’s supremacy claims (from the 2019 paper) have been refuted in a series of papers by several groups. This began with work by IBM researchers Pednault et al. shortly after Google’s original paper was published and continued with studies by Pan and Zhang; Pan, Chen, and Zhang; Kalachev, Panteleev, and Yung; Gao et al.; Liu et al.; and several other groups. For further details, see <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">this post</a> and the associated comment section, as well as <a href="https://gilkalai.wordpress.com/2022/08/06/ordinary-computers-can-beat-googles-quantum-computer-after-all/">this post</a>.&nbsp;</p>
<p><strong>G.2)</strong> Google now acknowledges that using the tensor network contraction method, their 2019 53-qubit result can be computed classically in less than 200 seconds. However, in their more recent 2023/24 paper, <em>“Phase Transitions…”</em> (see Table 1), they claim that with 67 to 70 qubits, classical supercomputers would require many years to generate 1 million such bitstrings, even with tensor network contraction.</p>
<p><strong>G.3)</strong> Items B) and C) highlights methodological issues with Google’s fidelity assertions, even for 12-qubit circuits. These concerns persist independently of the broader question of quantum supremacy for larger circuits, where the fidelity assertions are taken at face value.</p>
<p><strong>G.4)</strong> For a more comprehensive view of our study of Google’s fidelity claims, refer to the following papers:</p>
<ul>
<li>Y. Rinott, T. Shoham, and G. Kalai, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/08/sts836.pdf">Statistical Aspects of the Quantum Supremacy Demonstration,</a> (2020) Statistical Science&nbsp; (2022)</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/10/cc22a19.pdf"><span dir="ltr" role="presentation">Google’s 2019 “Quantum Supremacy” Claims:&nbsp;</span><span dir="ltr" role="presentation">Data, Documentation, &amp; Discussion</span></a> (2022) (see <a href="https://gilkalai.wordpress.com/2022/10/07/the-google-supremacy-experiment-data-information-discussions-and-three-questions/">this post</a>).</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://arxiv.org/abs/2305.01064">Questions and Concerns About Google’s Quantum Supremacy Claim</a> (2023) (see <a href="https://gilkalai.wordpress.com/2023/05/31/questions-and-concerns-about-googles-quantum-supremacy-claim/">this post</a>).</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://arxiv.org/abs/2404.00935">Random circuit sampling: Fourier expansion and statistics</a>. (2024) (see <a href="https://gilkalai.wordpress.com/2024/04/02/random-circuit-sampling-fourier-expansion-and-statistics/">this post</a>)</li>
</ul>
<p>These papers describe an ongoing project with Yosi Rinott and Tomer Shoham, supported by Ohad Lev and Carsten Voelkmann. Together with Carsten, we plan to expand our study and apply our tools to other experiments. Additionally, see my earlier paper:</p>
<ul>
<li>G. Kalai, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2020/08/laws-blog2.pdf">The argument against quantum computers, the quantum laws of nature, and Google’s supremacy claims,</a><em> (2020) The Intercontinental Academia Laws: Rigidity and Dynamics </em>(M. J. Hannon and E. Z. Rabinovici, eds.), World Scientific, 2024. arXiv:2008.05188.</li>
</ul>
<p><strong>G.5)</strong> There is also supporting evidence for Google’s 2019 claims, such as a 2020 replication by a group from the University of Science and Technology of China (USTC) and later verifications of some of Google’s fidelity estimations.</p>
<p><strong>G.6)</strong> There are some additional concerns regarding the Google experiment. In particular, there are problematic discrepancies between the experimental data, the Google noise model, and simulations.</p>
<p><strong>G.7)</strong> In my opinion, the main current challenge for experimental quantum computing is to improve the quality of two-qubit gates and other components, as well as to carefully study the quality of quantum circuits in the 5–20 qubit regime. Experiments on quantum error correction for larger circuits are also important.&nbsp;</p>
<h3><strong>H) Hype and Bitcoin</strong></h3>
<p>I usually don’t mind “hype” as a reflection of scientists’ enthusiasm for their work and the public’s excitement about scientific endeavors. However, in the case of Google, some caution is warranted, as the premature claims in 2019 may have had significant consequences. For example, following the 2019 “supremacy” announcement, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/08/bc666.png">the value of Bitcoin dropped</a> (around October 24, 2019, after a period of stability) from roughly $9,500 to roughly $8,500 in just a few days, representing a loss for investors of more than ten billion dollars. (The value today is around $100,000.) Additionally, Google’s assertions may have imposed unrealistic challenges on other quantum computing efforts and encouraged a culture of undesirable scientific methodologies.</p>
<p><a href="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png"><img data-attachment-id="27644" data-permalink="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/bnp5/" data-orig-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png" data-orig-size="1287,821" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="BNP5" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=300" data-large-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=640" src="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png" alt="BNP5" width="446" height="284" srcset="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=446&amp;h=284 446w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=890&amp;h=568 890w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=150&amp;h=96 150w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=300&amp;h=191 300w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=768&amp;h=490 768w" sizes="(max-width: 446px) 100vw, 446px"></a></p>
<p><span>Sergio Boixo, Hartmut Neven, and John Preskill in a video</span> <a href="https://youtu.be/l_KrC1mzd0g?si=3BBjgIRHlscg_phC">“Quantum next leap: Ten septillions years beyond-classic”</a></p>
<p><strong>I) Update (Dec. 10): The Wind in the Willow<br></strong><br>Yesterday, Google Quantum AI <a href="https://blog.google/technology/research/google-willow-quantum-chip/">announced</a> that their “Willow” quantum computer “performed a standard benchmark computation in under five minutes that would take one of today’s fastest supercomputers 10 septillion (that is, 10^25) years.” As far as I know there is no paper with the details. Google AI team announced also the appearance in <em>Nature</em> of their recent paper on distance-5 and distance-7 surface codes. It is asserted that the distance-7 codes exhibit an improvement of a factor of 2.4 compared to the physical qubits. The ratio of improvement Λ from distance-5 to distance-7 is 2.14. (We mentioned it in an August post following a <a href="https://gilkalai.wordpress.com/2024/08/21/five-perspectives-on-quantum-supremacy/#comment-99295">comment</a> by phan ting.)</p>
<p>We did not study yet these particular claims by Google Quantum AI, but my general conclusion apply to them “Google Quantum AI’s claims (including published ones) should be approached with caution, particularly those of an extraordinary nature. These claims may stem from significant methodological errors and, as such, may reflect the researchers’ expectations more than objective scientific reality.” (Our specific contention points are relevant to Google’s newer supremacy experiments but not directly to the quantum error-correction experiment.) </p>
<p>There is a nice very positive <a href="https://scottaaronson.blog/?p=8525">blog post over SO</a> about the new developments where Scott wrote: “besides the new and more inarguable Google result, IBM, Quantinuum, QuEra, and USTC have now all also reported Random Circuit Sampling experiments with good results.” For me, the gap between Google and IBM for RCS is a serious additional reason not to take the Google assertions seriously (item D) and and if I am wrong I will gladly stand corrected.</p>



											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alzheimer's study shows ketone bodies help clear misfolded proteins (166 pts)]]></title>
            <link>https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/</link>
            <guid>42383840</guid>
            <pubDate>Wed, 11 Dec 2024 01:48:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/">https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/</a>, See on <a href="https://news.ycombinator.com/item?id=42383840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Ketone bodies are produced by the body to provide fuel during fasting, and are thought to have roles in regulating cellular processes and aging mechanisms beyond energy production. Research by Buck Institute scientists, including experiments in the nematode <em>Caenorhabditis elegans</em>, and in mouse models, provides what they suggest is a direct molecular mechanism for the regulation of misfolded proteins by ketone bodies and related metabolites. The results, they said, indicate that ketone bodies, including β-hydroxybutyrate (βHB), may be considered powerful signaling metabolites affecting brain function in aging and Alzheimer’s disease (AD). The findings also point to potential metabolism-related mechanistic targets for therapeutic development in aging and in AD.</p>
<p>Reporting on their work in <em>Cell Chemical Biology</em> (“<a href="http://dx.doi.org/10.1016/j.chembiol.2024.11.001" target="_blank" rel="noopener">β-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer disease brain</a>”), senior author John Newman, MD, PhD, a Buck Institute assistant professor, and colleagues stated, “Here, we provide a direct molecular mechanism for the regulation of misfolded proteins by ketone bodies and related metabolites … Together, these data provide foundational evidence for a novel mechanistic component of ketone body biology.”</p>
<p>Ketone bodies are a class of lipid-derived small molecule metabolites that include acetone, acetoacetate, and (R)-β-hydroxybutyrate (R-βHB), the authors noted. “The primary function of acetoacetate and R-βHB production is to provide cellular energy to extrahepatic tissues during periods of reduced&nbsp;glucose availability, such as fasting, starvation, high-intensity exercise, and ketogenic diet.”</p>
<p>Previous studies have shown that boosting ketone bodies through diet, exercise, and supplementation can be good for brain health and cognition, both in rodents and humans. “There is clear preclinical literature support, and early clinical data, for ketogenic therapies in aging and AD. … ketogenic diet and exogenous ketones have been shown to improve cognitive and motor behavior in several mouse models of AD,” the authors stated. “Early human studies of ketogenic compounds have improved cognitive scores in patients with mild to moderate AD.”</p>
<p>The researcher’s newly reported work demonstrated that ketone bodies and similar metabolites have profound effects on the proteome and protein quality control in the brain. Working in cells, in mouse models of AD and aging, and in the model organism <em>C. elegans,</em>&nbsp;the findings indicated that the ketone body β-hydroxybutyrate (βHB) interacts directly with misfolded proteins, altering their solubility and structure so they can be cleared from the brain through the process of autophagy.</p>
<p>In addition to testing the changing solubility and structure of proteins in test tubes, the researchers also studied the effects of ketone bodies in model organisms. To assess whether the solubility changes caused by ketone bodies helped improve models of pathological aggregation, the&nbsp;investigators fed ketone bodies to nematode worms that were genetically modified to express the human equivalent of amyloid beta, which causes amyloid plaques. “The amyloid beta affects muscles and paralyzes the worms,” said Sidharth Madhavan, a PhD candidate and lead author on the study. “Once they were treated with ketone bodies the animals recovered their ability to swim. It was really exciting to see such a dramatic impact in a whole animal.”</p>
<p>When the team fed a ketone ester to mice, they found that the ketone ester treatment resulted in clearance rather than pathological aggregation of insoluble proteins. The investigators in addition generated detailed proteome-wide solubility maps from their experiments in test tubes and from their mouse experiments.</p>
<figure id="attachment_305108" aria-describedby="caption-attachment-305108"><img fetchpriority="high" decoding="async" src="https://www.genengnews.com/wp-content/uploads/2024/12/low-res-300x300.jpeg" alt="B-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer’s disease brain. [Sid Madhavan, Buck Institute for Research on Aging]" width="300" height="300" srcset="https://www.genengnews.com/wp-content/uploads/2024/12/low-res-300x300.jpeg 300w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-150x150.jpeg 150w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-420x420.jpeg 420w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-696x696.jpeg 696w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-600x600.jpeg 600w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res.jpeg 700w" sizes="(max-width: 300px) 100vw, 300px"><figcaption id="caption-attachment-305108">B-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer’s disease brain. [Sid Madhavan, Buck Institute for Research on Aging]</figcaption></figure><p>Newman noted an existing theory that the ketone body-based improvements are caused by increased energy to the brain or a reduction in brain inflammation, with reported improvements in amyloid plaques in mouse models being an indirect by-product. “Now we know that’s not the whole story,” he said. “Ketone bodies interact with damaged and misfolded proteins directly, making them insoluble so they can be pulled from the cell and recycled.”</p>
<p>Newman said the study highlights a new form of metabolic regulation of protein quality control. “This is not just about ketone bodies,” he said. “We tested similar metabolites in test tubes and a bunch of them had similar effects. In some cases, they performed better than β-hydroxybutyrate. It’s beautiful to imagine that changing metabolism results in this symphony of molecules cooperating together to improve brain function.”</p>
<p>While acknowledging that other mechanisms like energy supply are also important to brain health, Newman calls the discovery new biology. “It’s a new link between metabolism in general, ketone bodies, and aging,” he said. “Directly linking changes in a cell’s metabolic state to changes in the proteome is really exciting.”</p>
<p>Given that proteostatic mechanisms such as autophagy are known to be activated by nutrient deprivation, the authors noted, it’s not surprising that evolutionary pressures would encourage the clearance of pathogenic proteins during ketosis to promote cellular health in organisms needing additional substrate for ATP production. “In this situation, ketone bodies are janitors of damaged proteins, chaperoning away molecular waste so organisms can operate at peak molecular fitness,” they pointed out.</p>
<p>Noting that ketone bodies are easy to manipulate experimentally and therapeutically, Newman added, “This might be a powerful avenue to assist with global clearing of damaged proteins. We’re just scratching the surface as to how this might be applied to brain aging and neurodegenerative disease.”</p>
<p>In their paper, the team further concluded, “We show that βHB-induced insolubility leads to clearance of highly insolubilized proteins in vivo, likely via βHB communication with cellular protein degradation pathways. This work identifies βHB as a global regulator of cytosolic protein solubility, and identifies new metabolism-related mechanistic targets for therapeutic development in aging and AD.”</p>
<p>Madhavan is now pursuing whether ketone bodies and related metabolites have similar effects outside the brain, such as in the gut, and suggests that the key next step will be to test this new protein quality control mechanism in people to help guide how best to apply it therapeutically.</p>
		    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electric (Postgres sync engine) beta release (275 pts)]]></title>
            <link>https://electric-sql.com/blog/2024/12/10/electric-beta-release</link>
            <guid>42383136</guid>
            <pubDate>Wed, 11 Dec 2024 00:04:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electric-sql.com/blog/2024/12/10/electric-beta-release">https://electric-sql.com/blog/2024/12/10/electric-beta-release</a>, See on <a href="https://news.ycombinator.com/item?id=42383136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-3d98f719=""><p>With version <a href="https://github.com/electric-sql/electric/releases" target="_blank" rel="noreferrer"><code>1.0.0-beta.1</code></a> the Electric sync engine is now in BETA!</p><p>If you haven't checked out Electric recently, it's a great time to <a href="https://electric-sql.com/docs/intro">take another look</a>.</p><h2 id="what-is-electric" tabindex="-1">What is Electric? <a href="#what-is-electric" aria-label="Permalink to &quot;What is Electric?&quot;">​</a></h2><p><a href="https://electric-sql.com/product/electric">Electric</a> is a Postgres sync engine. We do real-time <a href="https://electric-sql.com/docs/guides/shapes">partial replication</a> of Postgres data into local apps and services.</p><p>Use Electric to swap out data <em>fetching</em> for <a href="https://electric-sql.com/use-cases/data-sync">data <em>sync</em></a>. Build apps on instant, real-time, local data. Without having to roll your own sync engine or change your stack.</p><p>We also develop <a href="https://electric-sql.com/product/pglite">PGlite</a>, a lightweight WASM Postgres you can run in the browser.</p><h2 id="the-path-to-beta" tabindex="-1">The path to BETA <a href="#the-path-to-beta" aria-label="Permalink to &quot;The path to BETA&quot;">​</a></h2><p>Six months ago, we <a href="https://electric-sql.com/blog/2024/07/17/electric-next">took on a clean re-write</a>.</p><p><a href="https://github.com/electric-sql/archived-electric-next/commit/fc406d77caca923d1fb595d921102f25c7ce3856" target="_blank" rel="noreferrer">First commit</a> was on the 29th June 2024. <a href="https://github.com/electric-sql/electric/pulls?q=is%3Apr+is%3Aclosed" target="_blank" rel="noreferrer">600 pull requests later</a>, we're ready for adoption into production apps.</p><h2 id="production-ready" tabindex="-1">Production ready <a href="#production-ready" aria-label="Permalink to &quot;Production ready&quot;">​</a></h2><figure><img src="https://electric-sql.com/assets/logo-strip.ByW8Eccm.svg"><img src="https://electric-sql.com/assets/logo-strip.sm.BZnVxeMp.svg"><img src="https://electric-sql.com/assets/logo-strip.xs.CqHd4PCW.svg"><img src="https://electric-sql.com/assets/logo-strip.xxs.CX2ul4h3.svg"></figure><p>Electric and PGlite are being used in production by companies including <a href="https://firebase.google.com/docs/data-connect" target="_blank" rel="noreferrer">Google</a>, <a href="https://database.build/" target="_blank" rel="noreferrer">Supabase</a>, <a href="https://trigger.dev/launchweek/0/realtime" target="_blank" rel="noreferrer">Trigger.dev</a>, <a href="https://ottogrid.ai/" target="_blank" rel="noreferrer">Otto</a> and <a href="https://www.doorboost.com/" target="_blank" rel="noreferrer">Doorboost</a>.</p><blockquote><p>We use ElectricSQL to power <a href="https://trigger.dev/launchweek/0/realtime" target="_blank" rel="noreferrer">Trigger.dev Realtime</a>, a core feature of our product. When we execute our users background tasks they get instant updates in their web apps. It's simple to operate since we already use Postgres, and it scales to millions of updates per day.<br><em>— <a href="https://www.linkedin.com/in/mattaitken1985" target="_blank" rel="noreferrer">Matt Aitken</a>, Founder &amp; CEO, <a href="https://trigger.dev/" target="_blank" rel="noreferrer">Trigger.dev</a></em></p></blockquote><blockquote><p>At <a href="https://ottogrid.ai/" target="_blank" rel="noreferrer">Otto</a>, we built a spreadsheet product where every cell operates as its own AI agent. ElectricSQL enables us to reliably stream agent updates to our spreadsheet in real-time and efficiently manage large spreadsheets at scale. It has dramatically simplified our architecture while delivering the performance we need for cell-level reactive updates.<br><em>— <a href="https://x.com/SullyOmarr" target="_blank" rel="noreferrer">Sully Omar</a>, Co-founder &amp; CEO, <a href="https://ottogrid.ai/" target="_blank" rel="noreferrer">Otto</a></em></p></blockquote><blockquote><p>At <a href="https://www.doorboost.com/" target="_blank" rel="noreferrer">Doorboost</a> we aggregate millions of rows from a dozen platforms, all of which gets distilled down to a simple dashboard. With Electric we have been able to deliver this dashboard in milliseconds and update live. Moving forward, we will be building all our products using Electric.<br><em>— <a href="https://am.linkedin.com/in/vacheasatryan" target="_blank" rel="noreferrer">Vache Asatryan</a>, CTO, <a href="https://doorboost.com/" target="_blank" rel="noreferrer">Doorboost</a></em></p></blockquote><h3 id="scalable" tabindex="-1">Scalable <a href="#scalable" aria-label="Permalink to &quot;Scalable&quot;">​</a></h3><p>So many real-time sync systems demo well but break under real load.</p><p>Electric has been <a href="https://electric-sql.com/docs/api/http">engineered from the ground up</a> to handle high-throughput workloads, like <a href="https://trigger.dev/launchweek/0/realtime" target="_blank" rel="noreferrer">Trigger.dev</a>, with low latency and flat resource use. You can stream real-time data to <strong>millions of concurrent users</strong> from a single commodity Postgres.</p><p>The chart below is from our cloud <a href="https://electric-sql.com/docs/reference/benchmarks">benchmarks</a>, testing Electric's memory usage and latency with a single Electric service scaling real-time sync from 100k to 1 million concurrent clients under a sustained load of 960 writes/minute. Both memory usage and latency are essentially <em>flat</em>:</p><figure></figure><p>You can also see how large-scale apps built with Electric feel to use with our updated <a href="https://electric-sql.com/demos/linearlite"> Linearlite</a> demo. This is a <a href="https://linear.app/" target="_blank" rel="noreferrer">Linear</a> clone that loads 100,000k issues and their comments through Electric into PGlite (~150mb of data). Once loaded, it's fully interactive and feels instant to use:</p><figure><p><a href="https://linearlite.examples.electric-sql.com/" target="_blank"><img src="https://electric-sql.com/assets/linearlite-screenshot.-uynhOsT.png"></a></p><figcaption> Screenshot of Linearlite. <a href="https://linearlite.examples.electric-sql.com/" target="_blank"> Open the demo</a></figcaption></figure><h2 id="easy-to-adopt" tabindex="-1">Easy to adopt <a href="#easy-to-adopt" aria-label="Permalink to &quot;Easy to adopt&quot;">​</a></h2><p>We've iterated a lot on our APIs to make them as simple and powerful as possible. There should be no breaking changes in minor or patch releases moving forward.</p><p>We've updated our <a href="https://electric-sql.com/docs/intro">Documentation</a>, with a new <a href="https://electric-sql.com/docs/quickstart">Quickstart</a> and guides for topics like:</p><ul><li>how to do <a href="https://electric-sql.com/docs/guides/auth">auth</a></li><li>how to handle <a href="https://electric-sql.com/docs/guides/writes">local writes</a></li><li>how to do <a href="https://electric-sql.com/docs/guides/shapes">partial replication with Shapes</a></li><li>how to <a href="https://electric-sql.com/docs/guides/deployment">deploy Electric</a></li><li>how to <a href="https://electric-sql.com/docs/guides/client-development">write your own client</a> for any language or environment</li></ul><p>We have <a href="https://electric-sql.com/docs/api/clients/typescript">client libraries</a>, <a href="https://electric-sql.com/docs/integrations/react">integration docs</a>, <a href="https://electric-sql.com/demos">demo apps</a> and <a href="https://electric-sql.com/demos#technical-examples">technical examples</a> showing how to use Electric with different patterns and frameworks:</p><h4 id="interactive-demos" tabindex="-1">Interactive demos <a href="#interactive-demos" aria-label="Permalink to &quot;Interactive demos&quot;">​</a></h4><div><div target="_blank"><p><a href="https://electric-sql.com/demos/notes"><img src="https://electric-sql.com/img/demos/notes-demo.png"></a></p></div><div target="_blank"><p><a href="https://electric-sql.com/demos/pixel-art"><img src="https://electric-sql.com/img/demos/pixel-art-demo.png"></a></p></div></div><h3 id="incrementally" tabindex="-1">Incrementally <a href="#incrementally" aria-label="Permalink to &quot;Incrementally&quot;">​</a></h3><p>You can adopt Electric one component and one route at a time. Wherever you have code doing something like this:</p><div><p><span>tsx</span></p><pre tabindex="0"><code><span><span>import</span><span> React, { useState, useEffect } </span><span>from</span><span> 'react'</span></span>
<span></span>
<span><span>const</span><span> MyComponent</span><span> =</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span><span>  const</span><span> [</span><span>items</span><span>, </span><span>setItems</span><span>] </span><span>=</span><span> useState</span><span>([])</span></span>
<span></span>
<span><span>  useEffect</span><span>(() </span><span>=&gt;</span><span> {</span></span>
<span><span>    const</span><span> fetchItems</span><span> =</span><span> async</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span><span>      const</span><span> response</span><span> =</span><span> await</span><span> fetch</span><span>(</span><span>'https://api.example.com/v1/items'</span><span>)</span></span>
<span><span>      const</span><span> data</span><span> =</span><span> await</span><span> response.</span><span>json</span><span>()</span></span>
<span></span>
<span><span>      setItems</span><span>(data)</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    fetchItems</span><span>()</span></span>
<span><span>  }, [])</span></span>
<span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>List</span><span> items</span><span>=</span><span>{ items } /&gt;</span></span>
<span><span>  )</span></span>
<span><span>}</span></span></code></pre></div><p>Swap it out for code like this (replacing the <code>fetch</code> in the <code>useEffect</code> with <a href="https://electric-sql.com/docs/integrations/react"><code>useShape</code></a>):</p><div><p><span>tsx</span></p><pre tabindex="0"><code><span><span>import</span><span> { useShape } </span><span>from</span><span> '@electric-sql/react'</span></span>
<span></span>
<span><span>const</span><span> MyComponent</span><span> =</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span><span>  const</span><span> { </span><span>data</span><span>: </span><span>items</span><span> } </span><span>=</span><span> useShape</span><span>({</span></span>
<span><span>    url: </span><span>'https://electric.example.com/v1/shapes'</span><span>,</span></span>
<span><span>    params: {</span></span>
<span><span>      table: </span><span>'items'</span></span>
<span><span>    }</span></span>
<span><span>  })</span></span>
<span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>List</span><span> items</span><span>=</span><span>{ items } /&gt;</span></span>
<span><span>  )</span></span>
<span><span>}</span></span></code></pre></div><p>This works with <em>any</em> Postgres <a href="https://electric-sql.com/docs/guides/deployment">data model and host</a>, any data type, extension and Postgres feature. Including <a href="https://github.com/pgvector/pgvector" target="_blank" rel="noreferrer">pgvector</a>, <a href="https://postgis.net/" target="_blank" rel="noreferrer">PostGIS</a>, sequential IDs, unique constraints, etc. You don't have to change your data model or your migrations to use Electric.</p><h3 id="with-your-existing-api" tabindex="-1">With your existing API <a href="#with-your-existing-api" aria-label="Permalink to &quot;With your existing API&quot;">​</a></h3><p>Because Electric syncs <a href="https://electric-sql.com/docs/api/http">over HTTP</a>, you can use it together <a href="https://electric-sql.com/blog/2024/11/21/local-first-with-your-existing-api">with your existing API</a>.</p><p>This allows you to handle concerns like <a href="https://electric-sql.com/docs/guides/auth">auth</a> and <a href="https://electric-sql.com/docs/guides/writes">writes</a> with your existing code and web service integrations. You don't need to codify your auth logic into database rules. You don't need to replace your API endpoints and middleware stack.</p><h2 id="take-another-look" tabindex="-1">Take another look <a href="#take-another-look" aria-label="Permalink to &quot;Take another look&quot;">​</a></h2><p>With this BETA release, Electric is stable and ready for prime time use. If you haven't checked it out recently, it's a great time to take another look.</p><h3 id="signup-for-early-access-to-electric-cloud" tabindex="-1">Signup for early access to Electric Cloud <a href="#signup-for-early-access-to-electric-cloud" aria-label="Permalink to &quot;Signup for early access to Electric Cloud&quot;">​</a></h3><p>We're also building <a href="https://electric-sql.com/product/cloud">Electric Cloud</a>, which provides managed Electric hosting (for those that don't want to <a href="https://electric-sql.com/docs/guides/deployment">host Electric themselves</a>).</p><p>If you're interested in using Electric Cloud, you can sign up for early access here:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WPEngine, Inc. vs. Automattic– Order on Motion for Preliminary Injunction (268 pts)]]></title>
            <link>https://www.courtlistener.com/docket/69221176/64/wpengine-inc-v-automattic-inc/</link>
            <guid>42382829</guid>
            <pubDate>Tue, 10 Dec 2024 23:20:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.courtlistener.com/docket/69221176/64/wpengine-inc-v-automattic-inc/">https://www.courtlistener.com/docket/69221176/64/wpengine-inc-v-automattic-inc/</a>, See on <a href="https://news.ycombinator.com/item?id=42382829">Hacker News</a></p>
Couldn't get https://www.courtlistener.com/docket/69221176/64/wpengine-inc-v-automattic-inc/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Most Expensive Eating Disorder (101 pts)]]></title>
            <link>https://desmolysium.com/bryan-johnson-the-worlds-most-expensive-eating-disorder/</link>
            <guid>42382607</guid>
            <pubDate>Tue, 10 Dec 2024 22:55:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://desmolysium.com/bryan-johnson-the-worlds-most-expensive-eating-disorder/">https://desmolysium.com/bryan-johnson-the-worlds-most-expensive-eating-disorder/</a>, See on <a href="https://news.ycombinator.com/item?id=42382607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="1109504" data-element_type="widget" data-widget_type="theme-post-content.default">
			
<p><em><strong>Disclaimer: </strong>I have never personally met Bryan Johnson. Reading this again, I realized that this article is quite harsh at times. Overall, I do like him and I am grateful that someone like Bryan exists as he is propelling the whole field of longevity forward by making it more popular. I also like the way he deals with criticism.</em> <em>While reading this article, please keep in mind that this is an opinion piece, and some of the points I make are hypothetical.</em></p>



<p>In my early twenties, I had a multi-year-long period during which my body fat was ultra-low (4.8% on the only DEXA scan I had done during that time). I became very obsessive and tolerated an unbelievable amount of self-torture to reach my goals. I also developed very weird eating habits and was rigid about my meal timing – I could not stand breaking my fasts by even a few minutes.</p>



<p>My need for control was extreme and I meticulously tried to control my environment. In retrospect, I am sure that keeping my body fat that low was actually the single biggest contributor to my 90-degree personality turn (as compared to my adolescence) – in other words, I behaved like I behaved more <em>because </em>of my eating disorder than the other way around.</p>



<p>During this time, my blood pressure (roughly 105mmHg), resting heart rate (low 40s when sleeping), and body temperature (roughly 35.8-36.5) were all very low. I had a fixed caloric intake (2000-2100kcal/day) and whenever I overshot it I made sure to balance it out over the next couple of days. My face looked pale and gaunt. My <a href="https://desmolysium.com/vitality/">vitality</a> was non-existent.</p>



<p>Just like Bryan, I was never underweight because I had a fair amount of muscle. I had an eating disorder concealed under the veil of “health and fitness”. However, what was really going on was starvation. Nowadays, I quickly recognize it when someone else has an eating disorder.</p>



<p>When body fat gets to very low levels, for evolutionary reasons, people are set to become obsessive, rigid, and engage in OCD-like behaviors intended to help with foraging. We see this in about every mammal we study, and Bryan Johnson is no exception.</p>



<figure><img fetchpriority="high" decoding="async" width="1024" height="768" src="https://desmolysium.com/wp-content/uploads/2024/11/bryan-1x-wpcf_1024x768.jpeg" alt="" srcset="https://desmolysium.com/wp-content/uploads/2024/11/bryan-1x-wpcf_1024x768.jpeg 1024w, https://desmolysium.com/wp-content/uploads/2024/11/bryan-1x-wpcf_1024x768-300x225.jpeg 300w, https://desmolysium.com/wp-content/uploads/2024/11/bryan-1x-wpcf_1024x768-768x576.jpeg 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Upon starvation, sympathetic nervous system activity falls, leading to low blood pressure (<a href="https://desmolysium.com/midodrine/">as far as I know, Bryan’s is around 105mmHg systolic</a>), heart rate, (Bryan’s is in the low 40s while sleeping), and body temperature. Sympathetic activity is the biggest contributor to the <a href="https://desmolysium.com/diet/" data-type="post" data-id="3641" target="_blank" rel="noreferrer noopener">basal metabolic rate </a>(how many calories you burn while doing nothing). Whenever we eat less than we expend, the nervous system decreases sympathetic activity. In starvation states, the nervous system shuts it down to very low levels in a desperate attempt to conserve energy.</p>



<p>In anorexia nervosa (including atypical anorexia nervosa – the same state without being underweight), body temperature often falls to 36C – 35.5C, so borderline hypothermia. This lowering of the hypothalamic temperature setpoint is also a frantic last-ditch effort to have the body expend less energy.</p>



<p>Bryan just released a newsletter this week “bragging” that his body temperature hovers around 34.8C, which means outright hypothermia. For the last couple of months, I worked in emergency medicine and whenever someone had outright (acute) hypothermia we would put the patient into the observatory critical care unit. Mammalian enzymes have evolved to function well within a very narrow temperature range and when the temperature is considerably above or below, kinetic equilibria are going to be perturbed.</p>



<p>To quote from his newsletter: <em>“My health program has me metabolically cold plunging. Since starting Blueprint, my body temp has cooled 4°F in 3 yrs, now at 94.8°F. This technically qualifies as minor hypothermia. To put this into context, it takes swimming more than a mile in ice to achieve an equivalent temperature reduction as mine.”</em></p>



<p>Well done, Bryan.</p>



<p>While, in theory, having a colder body temperature may be related to longevity (simply put, the lower the temperature the slower chemical reactions happen), in the “wild” this may be a double-edged sword because at lower temperatures the immune system is less capable (predisposing to infections &amp;<a href="https://desmolysium.com/cancer/" data-type="post" data-id="6369" target="_blank" rel="noreferrer noopener"> cancer</a>) and also repair processes happen more slowly. Most importantly, at these temperatures brain function is quite impaired because of the perturbation of the delicate balance of ion kinetics. </p>



<p>In other words, an average animal with hypothermia in the wild is usually dead long before its maximum lifespan kills it. However, Bryan is not like an animal in the “wild” but rather living in his own “lab”.</p>



<p>Further hints of him being in a state of starvation are his carotenemia, which nearly every anorexia patient has, and his remarkable pallor – most people think that he is so pale because he simply avoids the sun. Actually, his body is so cold and his blood circulation is so centralized that there is little cutaneous blood flow, therefore the red color of hemoglobin does not “shine” through the skin causing his vampire-like color.</p>



<p>Also, in the interviews I have seen of him, his blinking frequency is quite high, <em>potentially</em> because he is so low in body fat that his lacrimal ducts cannot manufacture the lipid portion of tear fluid properly. Keratoconjunctivitis sicca (dry eyes) is relatively common in restrictive eating disorders. </p>



<p>Bryan’s (excessive?) exercise regimen in combination with his nutritional state may be causally related to the impairments of his heart and musculoskeletal system (some data and more on that later). Hyperactivity and “excessive drive for exercise” are hallmark signs of the most common type of anorexia. Relatedly and paradoxically, if we experimentally starve mice, instead of “saving energy by doing nothing” they run or work themselves to death. The restlessness associated with starvation is presumably an evolutionary adaptation to help with foraging for food and may be caused by the <a href="https://desmolysium.com/leptin/" data-type="post" data-id="7467" target="_blank" rel="noreferrer noopener">low leptin levels</a> associated with very low levels of body fat.</p>



<p>In sum, it is not too far-fetched to assume that as Bryan started to control “Evening-Bryan”, he gradually fell into an eating disorder. The resulting neurobiochemical makeup may be causally intertwined with the evolution of many of his current ritualistic behaviors.</p>



<div><p><strong><em>Subscribe to the Desmolysium newsletter and get access to three exclusive articles!</em></strong></p></div>



<h2><strong>What are my overall thoughts on Bryan? </strong></h2>



<p>The part I disagree with most is his self-starvation. I would like for Bryan to communicate openly how much “like shit” he feels all the time (fatigued; weak; cold; hungry; etc.). This is not due to his program but primarily due to his insufficient <a href="https://desmolysium.com/diet/" data-type="post" data-id="3641" target="_blank" rel="noreferrer noopener">caloric intake</a>, and more importantly, combined with having ultra-low body-fat levels seemingly year-round. From a longevity &amp; health perspective, he would do well with <a href="https://desmolysium.com/leptin/" data-type="link" data-id="https://desmolysium.com/leptin/" target="_blank" rel="noreferrer noopener">leptin injections</a> (or simply gaining a little body fat). However, they would change his neurobiochemistry probably in a way that is not conducive to maintaining his rigid lifestyle.</p>



<p>Someone with a blood pressure and body temperature this low is definitely not “healthier than most 18-year-olds”. The metrics he is using to judge that are quite theoretical but more on some of his real data later. </p>



<p>Furthermore, he claims that he is “aging more slowly than 88% of 18-year-olds”. He uses a number of epigenetic clocks (e.g., PCHorvath 1, PC Phenoage, etc.) to determine his speed of aging. Evidence suggests that they advance faster during periods of rapid growth and development. Given that puberty is probably the time period with the fastest rate of aging in life, beating 18-year-olds is not an accomplishment. Furthermore, most of the epigenetic clocks are quite faulty. For example, when someone fasts for a couple of days, these clocks tend to age-reverse by a couple of years (for a short time only). Bryan’s “starvation mode” may be keeping these clocks down artificially and a couple of months on a normal diet may drive these clocks up by a decade or more.</p>



<p>Moreover, Bryan engages in quite extreme (relative) caloric restriction. Caloric restriction may be helpful for <a href="https://desmolysium.com/longevity/" data-type="post" data-id="2845" target="_blank" rel="noreferrer noopener">longevity</a> (and this is a big MAY in already long-lived mammals such as great apes, elephants, and dolphins – particularly if they live in the wild full of pathogens and caloric restriction leads to immunosuppression and poor wound healing), it means sacrificing most of his (biological) <a href="https://desmolysium.com/vitality/" data-type="post" data-id="427" target="_blank" rel="noreferrer noopener">vitality</a>.</p>



<p>Of note, as of 2024, he is eating roughly 2200kcal per day, which to some may seem far from “starvation”. When it comes to starvation, it is all about <em>relative </em>rather than absolute energy intake. For example, a person who runs a marathon a day would soon be in “starvation mode” on a diet below 4000+ kcal/d whereas some people who just sit on the couch all day may get by fine with only 1500 kcal/d. Bryan engages in a very intense exercise regimen, has very low body fat levels and therefore great <a href="https://desmolysium.com/insulin/" data-type="post" data-id="3037" target="_blank" rel="noreferrer noopener">insulin sensitivity</a> (i.e., he can tolerate more carbs as his nutrient-partitioning is presumably excellent), and also takes a very high dose of <a href="https://desmolysium.com/hormones/" data-type="post" data-id="3636" target="_blank" rel="noreferrer noopener">thyroid hormones</a>. In sum, he would probably need around 3000-4000kcal for proper maintenance of his body weight (without his nervous system desperately having to shut down sympathetic activity).</p>



<p>The prolonged starvation and the associated <a href="https://desmolysium.com/hormones/" data-type="post" data-id="3636" target="_blank" rel="noreferrer noopener">hormonal decline</a> <em>may</em> be responsible for the white matter hyperintensities in his brain (early onset grey matter atrophy), and not just his internal jugular vein stenosis as he believes. Cortical atrophy has been reported in primate studies looking at the effects of starvation on longevity (I am sure Bryan is aware of this but once you have an eating disorder it is sort of hard to stop…).</p>



<p>I do not believe in the validity of his sleep data. In starvation, sympathetic nervous system activity and <a href="https://desmolysium.com/neurotransmitters/">noradrenaline</a> levels get very low, which causes sleep resting heart rate to drop and heart rate variability to increase. While having a low RHR and high HRV <em>can</em> be suggestive of restorative sleep (e.g., as they do with increased cardiovascular fitness or with <a href="https://desmolysium.com/cortisol/" data-type="post" data-id="7466" target="_blank" rel="noreferrer noopener">low cortisol levels</a>), the change in these parameters can also be suggestive of an abysmally low sympathetic activity. Hence, one could argue that Bryan “cheats” his way to a great sleep score by having a pathologically low sympathetic tone, which the sleep trackers presumably mistake for being in a state of deep relaxation and having a very restorative sleep.&nbsp;</p>



<p>I also think he is sometimes dishonest about where his on-paper results are coming from. He takes potent anti-aging drugs (<a href="https://desmolysium.com/rapamycin/">rapamycin</a>, metformin, <a href="https://desmolysium.com/rapamycin/">acarbose</a>, 17-alpha estradiol), uses a bunch of hormones (<a href="https://desmolysium.com/thyroid/">thyroid hormones way higher than replacement doses</a>; <a href="https://desmolysium.com/testosterone/">TRT in the past</a>; growth hormone in the past), and even gene therapies. His nutty pudding has little to do with his “results”, yet this is what is being fed (and sold) to the masses.</p>



<p>While the testosterone dosage was still listed on his website, the way he looked while taking “TRT” looked nothing like a “testosterone replacement dose” but rather like a (mild?) steroid cycle, particularly at his low caloric intake. Remember, the man is 45 and not 18. In his defense, he does look much bigger than he actually is because his body fat is very low, enhancing muscle definition.</p>



<figure><img decoding="async" width="1024" height="700" src="https://desmolysium.com/wp-content/uploads/2024/11/photo-1024x700.jpg" alt="" srcset="https://desmolysium.com/wp-content/uploads/2024/11/photo-1024x700.jpg 1024w, https://desmolysium.com/wp-content/uploads/2024/11/photo-300x205.jpg 300w, https://desmolysium.com/wp-content/uploads/2024/11/photo-768x525.jpg 768w, https://desmolysium.com/wp-content/uploads/2024/11/photo-1536x1050.jpg 1536w, https://desmolysium.com/wp-content/uploads/2024/11/photo.jpg 1653w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Given that veins vasoconstrict in non-hot water, I would guess that he somehow artificially enhanced vasodilation before the photo shoot, either by taking a vasodilator (e.g., <a href="https://desmolysium.com/atherosclerosis/" data-type="post" data-id="6358" target="_blank" rel="noreferrer noopener">Viagra</a>), using a blowdryer or sauna, and/or getting in a pump through volume training.</p>



<p>It seems that he is spending a lot of his time &amp; effort on maintaining his hair (huge kudos for admitting that!). He should not need to do that if his body was functioning like an 18-year-old. It is true that androgenic alopecia is mostly a time-course disease due to a genetic susceptibility to DHT (among other things) but Bryan is using topical 5-alpha-reductase inhibitors, and if Bryan truly set back his true aging clock he probably would not be battling with ongoing hair loss (despite a plethora of interventions), or at least it would mostly halt – which does not seem to be the case. </p>



<p>And through his implementation of the vast amounts of cosmetic interventions he is concealing his true speed of aging. However, I love the fact that he shares these as normally people do these in private and like to credit their “good genes” for their looks. I do think that Bryan is quite good-looking, and unlike most people who do a lot of cosmetic interventions, in my opinion, he did more good than harm overall.</p>



<p>I do think that what he does is interesting. I read his protocol and watched a few of his videos. The only thing I adopted myself was taking 1mg of lithium per day. I would be interested in measuring my nighttime erections and seeing how they change as my sex hormones, calories, body fat, stress levels, leptin levels, etc change.</p>



<p>The part about Botox injections into his penis was superb and ever since seeing that my opinion of him was swayed from rather negative to rather positive – particularly because he is being public about this. I do think that this single tweet might have opened up a new industry…</p>



<figure><img decoding="async" width="1024" height="976" src="https://desmolysium.com/wp-content/uploads/2024/11/penis-1024x976.jpg" alt="" srcset="https://desmolysium.com/wp-content/uploads/2024/11/penis-1024x976.jpg 1024w, https://desmolysium.com/wp-content/uploads/2024/11/penis-300x286.jpg 300w, https://desmolysium.com/wp-content/uploads/2024/11/penis-768x732.jpg 768w, https://desmolysium.com/wp-content/uploads/2024/11/penis.jpg 1506w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Even though this article is full of criticism, more and more I find myself liking him than not liking him. I particularly like him calling out AG1 and other shitty companies. I also like his “shot-gun” approach: “Let us try everything at once and see what we find.” </p>



<p>I am also impressed by how well and courageous Bryan reacts to the vast amounts of hate and criticism he gets. We are social primates and social humiliation and ostracism trigger deep-seated circuits of fight or flight – Bryan does neither and it does take a certain greatness to react the way he does (though I am sure he has PR consultants).</p>



<p>Much of his undertaking is probably for his own ego, which seems to be hungry for attention (I could be wrong!). He says that his main goal is all about “not dying” and surviving until superintelligence hits. To me, this seems questionable. Firstly, he is around 45 years old and most predictions about the advent of AGI are way shorter than 40-50 years. Why then take these risks by plunging into the unknown? </p>



<p>Secondly, it seems that quite a few people hate him, perhaps unjustified but still. Given that the US society has many lunatics, by being so public-facing he exposes himself as a target of hate crime, meaning that he incurs the additional risk of homicide, which may even be his biggest risk of death. He has roughly half a billion $, if not more. If “Don’t die!” is really his top and only priority, why not just take your money &amp; your interventions and spend the rest of your time within a very sheltered property in New Zealand without telling anyone?</p>



<div><p><strong><em>Subscribe to the Desmolysium newsletter and get access to three exclusive articles!</em></strong></p></div>



<h2><strong>Some more data</strong><strong></strong></h2>



<p>Sure, he is in the “top 1%” in some parameters (e.g., fat mass, inflammation, VO2 max, bone mass – for which he has probably used teriparatide) but he is much worse off in a number of other parameters that cannot be improved via “effort” (e.g., physical training, medications). Below is cherry-picked data taken from his <a href="https://protocol.bryanjohnson.com/">Blueprint website</a> revealing some of his weaknesses, which he rarely discusses as far as I am aware. Disclaimer: I picked the most negative data I could find and there is definitively a wealth of positive data!</p>



<ul>
<li>He has 1.37 liters of subcutaneous fat. This is incredibly low and approaches body-builder-level territory. He calls this “optimal”. Subcutaneous fat is <em>healthy</em> – it is the visceral fat that is bad.</li>
</ul>







<ul>
<li>Some of his heart parameters are like that of a 70-year-old (“LV septal A’ mitral: age 70+; Aortic root diameter age 70+; LA E’ laterobasal: age 70+; RVSP: age 70; LV sepal E/E’: age 55). Despite being a doctor, I had to look up some of them myself. I will briefly explain what these mean. 
<ul>
<li>“LV septal A mitral” is a marker of diastolic function in the left ventricle. His value of age 70+ means that there is stiffness in the heart muscle, meaning that the heart can not relax as well during diastole, often associated with older age. </li>



<li>Aortic root diameter: This is the diameter of the lowest part of the aorta. The diameter increases with age due to normal “wear and tear” (e.g., <a href="https://desmolysium.com/atherosclerosis/" data-type="post" data-id="6358" target="_blank" rel="noreferrer noopener">hypertension</a>, overexercising). A large aortic root diameter predisposes to aortic dissection, which is very rare, but usually deadly when it occurs.</li>



<li>“LA E’ laterobasal” is very similar to the first value. This measures the relaxation velocity of the lateral basal wall of the left atrium during early diastole (vs. the first value measures something similar but for the left ventricle instead of the atrium). It seems that the left side of his heart is not the best at relaxing (i.e., becoming soft while being filled with blood).</li>



<li>“RVSP” stands for right ventricular systolic pressure, which is an estimate of the pressure in the pulmonary artery, which is mildly elevated. My non-cardiologist understanding of the heart tells me that this could be related to the diastolic weakness of the left side of his heart. Pulmonary hypertension is a serious matter. Bryan has not commented on his pulmonary pressures as far as I am aware. </li>



<li>“LV sepal E/E’”: He scored age 55 meaning that his heart is about a decade stiffer than what we would expect for his age.</li>



<li>Note: It is possible that his bad heart markers are due to a combination of overexercising + undereating. If this is indeed the case, then they will continue to get worse over the next couple of years unless Bryan changes key aspects of his protocol. Of note, structural changes to the heart are commonly observed in long-standing anorexia nervosa. It is also possible that his values are the way they are mostly due to genetic predisposition. To say for sure, we would need data on how these values changed since Bryan started Blueprint.</li>
</ul>
</li>
</ul>







<ul>
<li>Most of his brain markers are exactly around his biological age (“White Matter Hyperintensities: age 48; Ventricular volume: age 48; Cortical grey vol: age 45; AI T1 brain age: age 44; RAVENS PM: age 41, Total Cerebral WMV: age 37; WASO: age 37). </li>
</ul>







<ul>
<li>His joints and tendons do not seem to be in the best shape, particularly the important joints of the knee and shoulder. Next to the heart, these impairments of some parts of the musculoskeletal apparatus are probably among his weakest areas. I suspect that some of these are the result of under-eating + over-exercising + under-resting:
<ul>
<li>“Bilateral shoulder labral degenerative tears with paralabral cysts.” They are unlikely to get better other than with labrum surgery.</li>



<li>“Tendinosis on both sites of the supraspinatus rotator cuff (shoulder) with subacromial bursitis (chronic inflammation in R&gt;L shoulders).” Chronic degeneration of some shoulder tendons. May improve with time.</li>



<li>“Bilateral knee chondromalacia patella with left knee mild to moderate effusion/bursitis.” Cartilage wear beneath the kneecap to the point that there is inflammatory fluids in his knees. This is unlikely to get better because cartilage barely heals – at least with conventional treatment. I would be interested to know whether the stem cell treatments he did made any difference.</li>



<li>“Bilateral hip gluteus medius tendinosis.” Chronic degeneration of the tendon of the glute medius – probably due to overtraining. The glute medius is a very important and neglected muscle and I am sure Bryan is aware of that – perhaps a little too much.</li>



<li>“Bilateral Hip Cam impingement syndrome.” Can predispose to osteoarthritis of the hips.</li>
</ul>
</li>
</ul>







<ul>
<li>His hearing is not the best (Right ear normal freq: age 61; Left ear normal freq: age 51; Right ear EHF: age 60; Left ear EHF: age 32).</li>
</ul>







<ul>
<li>His telomere length is that of a 42-year old so roughly what we would expect.</li>
</ul>







<ul>
<li>In 2021 he writes: “Body fat increased from 3.5% to 6% due to transitioning to three meals a day (10-16 hour fast), from one meal a day (~22 hr fast).” Most people cannot even imagine what it <em>feels like </em>to be 3.5% body fat, which is in life-threatening territory by the way.</li>
</ul>







<p>Somewhere buried on his website he writes: “Severe headache symptoms causing to wake in the night, on acetazolamide 250 mg since 25th Jan 2022.” However, acetazolamide is not on the supplements/medication list he shares on the Blueprint protocol (<a href="https://protocol.bryanjohnson.com/#step-2-supplements" data-type="link" data-id="https://protocol.bryanjohnson.com/#step-2-supplements">link</a>). Is this list just the stuff that he takes for health and longevity? What else is he taking that is not included in this list? </p>



<p>There is one video of him measuring his brain activity using his Kernel device while on <a href="https://desmolysium.com/ketamine/" data-type="link" data-id="https://desmolysium.com/ketamine/" target="_blank" rel="noreferrer noopener">ketamine</a>. I would not be surprised if he were actually on psychopharmaceuticals as well, potentially to combat his neurobiological depression stemming from self-starvation (I am using the term “depression” from a neurobiological perspective – <a href="https://desmolysium.com/antidepressants/" data-type="link" data-id="https://desmolysium.com/antidepressants/" target="_blank" rel="noreferrer noopener">depression is not the same as “suffering”</a>), which eventually hits nearly everyone going down that road.</p>



<h4>Sources &amp; further information</h4>



<ul>
<li><strong>Article:</strong>&nbsp;<a href="https://protocol.bryanjohnson.com/" data-type="link" data-id="https://protocol.bryanjohnson.com/" target="_blank" rel="noreferrer noopener">Blueprint Protocol</a></li>
</ul>



<h4><strong>Disclaimer</strong></h4>



<p><em>The content available on this website is based on the author’s individual research, opinions, and personal experiences. It is intended solely for informational and entertainment purposes and does not constitute medical advice. The author does not endorse the use of supplements, pharmaceutical drugs, or hormones without the direct oversight of a qualified physician. People should never disregard professional medical advice or delay in seeking it because of something they have read on the internet.</em></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Don't let your billion-dollar ideas die (126 pts)]]></title>
            <link>https://ideaharbor.xyz</link>
            <guid>42382506</guid>
            <pubDate>Tue, 10 Dec 2024 22:45:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ideaharbor.xyz">https://ideaharbor.xyz</a>, See on <a href="https://news.ycombinator.com/item?id=42382506">Hacker News</a></p>
Couldn't get https://ideaharbor.xyz: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Arctic tundra is now emitting more carbon than it absorbs, US agency says (128 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/dec/10/arctic-tundra-carbon-shift</link>
            <guid>42382470</guid>
            <pubDate>Tue, 10 Dec 2024 22:42:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/dec/10/arctic-tundra-carbon-shift">https://www.theguardian.com/world/2024/dec/10/arctic-tundra-carbon-shift</a>, See on <a href="https://news.ycombinator.com/item?id=42382470">Hacker News</a></p>
Couldn't get https://www.theguardian.com/world/2024/dec/10/arctic-tundra-carbon-shift: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GM exits robotaxi market, will bring Cruise operations in house (365 pts)]]></title>
            <link>https://www.cnbc.com/2024/12/10/gm-halts-funding-of-robotaxi-development-by-cruise.html</link>
            <guid>42381637</guid>
            <pubDate>Tue, 10 Dec 2024 21:19:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/12/10/gm-halts-funding-of-robotaxi-development-by-cruise.html">https://www.cnbc.com/2024/12/10/gm-halts-funding-of-robotaxi-development-by-cruise.html</a>, See on <a href="https://news.ycombinator.com/item?id=42381637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ArticleBody-InlineImage-108024233" data-test="InlineImage"><p>A in San Francisco, California, US, on Thursday Aug. 10, 2023.</p><p>David Paul Morris | Bloomberg | Getty Images</p></div><div><p>After spending more than $10 billion on its robotaxi unit, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/GM/">General Motors</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> is abandoning its Cruise driverless ride-hailing service.</p><p>The Detroit automaker on Tuesday said it will no longer fund its Cruise division's robotaxi development and will instead fold the unit into its broader tech team. GM shares rose 2.3% in extended trading. </p><p>"Cruise was well on its way to a robotaxi business — but when you look at the fact you're deploying a fleet, there's a whole operations piece of doing that," GM CEO Mary Barra said on a call Tuesday. Barra said GM would instead focus on the development of autonomous systems for use in personal vehicles.</p><p>GM cited the increasingly competitive robotaxi market, capital allocation priorities and the considerable time and resources necessary to grow the business as reasons for its decision.</p><p>The company will combine the majority-owned Cruise LLC with GM technical teams. Barra, who also serves as board chair of Cruise, said the companies have yet to determine how many employees will move to GM. Cruise has nearly 2,300 employees, a GM spokesperson told CNBC.</p><p>GM acquired Cruise in 2016. The automaker currently owns about 90% of Cruise and has agreements with other shareholders that will raise its ownership to more than 97%, GM said <a href="https://news.gm.com/home.detail.html/Pages/news/us/en/2024/dec/1210-gm.html" target="_blank">in a statement</a>. GM anticipates it will complete the acquisition of remaining Cruise shares from outside shareholders by early 2025, CFO Paul Jacobson said Tuesday.</p><p>GM's current annual expenditure on Cruise amounted to about $2 billion, and the restructuring would cut that by more than half, Jacobson said.</p><p>Honda, an outside investor in Cruise, told CNBC that it had planned to launch a driverless ride-hail service in Japan in early 2026, but will now re-assess those plans and make adjustments if needed.</p><p>"Honda remains committed to various research and development initiatives aimed at providing new mobility solutions to our customers in Japan," a Honda spokesperson said on Tuesday. Honda said its total investment in Cruise was $852 million.</p><p>Cruise founder Kyle Vogt, who left the company in November 2023, <a href="https://x.com/kvogt/status/1866612270815494639" target="_blank">posted on X</a> after the announcement, "In case it was unclear before, it is clear now: GM are a bunch of dummies." </p><p>An early entrant in the U.S. robotaxi market, Cruise <a href="https://www.cnbc.com/2023/10/26/cruise-pauses-all-driverless-operations-after-collisions-suspensions.html">grounded its driverless operations</a> in October 2023, shortly before Vogt's departure. The National Highway Traffic Safety Administration fined Cruise $1.5 million after the company failed to disclose details of a <a href="https://www.cnbc.com/2023/10/03/driverless-cruise-car-traps-woman-after-hit-and-run-incident.html">serious crash</a> that month involving a pedestrian.</p><p>A third-party probe into the <a href="https://www.cnbc.com/2024/01/25/gm-cruise-probe-finds-poor-leadership-at-center-of-incident-response.html">incident</a> ordered by GM and Cruise found that culture issues, ineptitude and poor leadership fueled regulatory oversights that led to the accident. The probe also investigated allegations of a cover-up by Cruise leadership but found no evidence to support those claims.</p><p>In July of this year, GM announced that it would <a href="https://www.cnbc.com/2024/07/23/gm-cruise-origin-autonomous-vehicle-indefinitely-delayed.html">indefinitely delay</a> production of the Origin autonomous vehicle as its Cruise self-driving unit attempted&nbsp;<a href="https://www.cnbc.com/2024/04/09/gms-cruise-to-relaunch-vehicles-with-human-drivers-in-phoenix.html">to relaunch operations.</a> At that point, Cruise began to focus on using the next-generation Chevrolet Bolt for development of its autonomous vehicles.</p><p>As Cruise's operations were on hold, its robotaxi rivals gained ground.</p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>-owned Waymo has begun to operate commercial robotaxi services across several major U.S. metro areas, with the company last week announcing its plans to <a href="https://www.cnbc.com/2024/12/05/waymo-announces-robotaxi-expansion-to-miami.html">expand into Miami</a>. Chinese autonomous vehicle makers including Pony.ai and WeRide have rolled out in overseas markets as well.</p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-11"><a href="https://www.cnbc.com/quotes/TSLA/">Tesla</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, meanwhile, showed off design concepts for a self-driving Cybercab at <a href="https://www.cnbc.com/2024/10/10/elon-musk-hypes-a-30000-tesla-cybercab-robovan-at-robotaxi-event.html">an event in October</a>. Tesla still classifies the Autopilot and Full Self-Driving software in its vehicles as "partially automated driving systems," which require a human to be ready to steer or brake at all times. In an&nbsp;<a href="https://www.cnbc.com/2024/10/23/tesla-tsla-q3-2024-earnings-report.html">October earnings call</a>, Tesla CEO <a href="https://www.cnbc.com/elon-musk/">Elon Musk</a> said the company will launch a self-driving ride-hailing service in California and Texas as early as 2025.</p><p>SoftBank-funded Wayve is testing its autonomous vehicles in San Francisco, and&nbsp;<a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a>-owned Zoox is also testing its autonomous vehicles, which do not feature steering wheels, in several U.S. cities including San Francisco.</p><p>SoftBank's Vision Fund was also an investor in Cruise, with a nearly 20% stake, until GM <a href="https://www.cnbc.com/2022/03/18/gm-to-buy-softbanks-stake-in-cruise-self-driving-unit.html">repurchased</a> the shares for $2.1 billion in 2022.</p><p><strong>WATCH: </strong><a href="https://www.cnbc.com/video/2024/12/05/uber-and-lyft-drop-on-news-waymo-is-expanding-to-miami.html">Uber and Lyft drop on news Waymo is expanding to Miami</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's trusted execution environment blown wide open by new BadRAM attack (135 pts)]]></title>
            <link>https://arstechnica.com/information-technology/2024/12/new-badram-attack-neuters-security-assurances-in-amd-epyc-processors/</link>
            <guid>42381406</guid>
            <pubDate>Tue, 10 Dec 2024 20:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/information-technology/2024/12/new-badram-attack-neuters-security-assurances-in-amd-epyc-processors/">https://arstechnica.com/information-technology/2024/12/new-badram-attack-neuters-security-assurances-in-amd-epyc-processors/</a>, See on <a href="https://news.ycombinator.com/item?id=42381406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2065895">
  
  <header>
  <div>
      

      

      <p>
        Attack bypasses AMD protection promising security, even when a server is compromised.
      </p>

      
    </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>One of the oldest maxims in hacking is that once an attacker has physical access to a device, it’s game over for its security. The basis is sound. It doesn’t matter how locked down a phone, computer, or other machine is; if someone intent on hacking it gains the ability to physically manipulate it, the chances of success are all but guaranteed.</p>
<p>In the age of cloud computing, this widely accepted principle is no longer universally true. Some of the world’s most sensitive information—health records, financial account information, sealed legal documents, and the like—now often resides on servers that receive day-to-day maintenance from unknown administrators working in cloud centers thousands of miles from the companies responsible for safeguarding it.</p>
<h2>Bad (RAM) to the bone</h2>
<p>In response, chipmakers have begun baking protections into their silicon to provide assurances that even if a server has been physically tampered with or infected with malware, sensitive data funneled through virtual machines can’t be accessed without an encryption key that’s known only to the VM administrator. Under this scenario, admins inside the cloud provider, law enforcement agencies with a court warrant, and hackers who manage to compromise the server are out of luck.</p>
<p>On Tuesday, an international team of researchers unveiled BadRAM, a proof-of-concept attack that completely undermines security assurances that chipmaker AMD makes to users of one of its most expensive and well-fortified microprocessor product lines. Starting with the AMD Epyc 7003 processor, a feature known as SEV-SNP—short for <a href="https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf">Secure Encrypted Virtualization and Secure Nested Paging</a>—has provided the cryptographic means for certifying that a VM hasn’t been compromised by any sort of backdoor installed by someone with access to the physical machine running it.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>If a VM has been backdoored, the cryptographic <a href="https://www.redhat.com/en/blog/attestation-confidential-computing">attestation</a> will fail and immediately alert the VM admin of the compromise. Or at least that’s how SEV-SNP is designed to work. BadRAM is an attack that a server admin can carry out in minutes, using either about $10 of hardware, or in some cases, software only, to cause DDR4 or DDR5 memory modules to misreport during bootup the amount of memory capacity they have. From then on, SEV-SNP will be permanently made to suppress the cryptographic hash attesting its integrity even when the VM has been badly compromised.</p>
<p>“BadRAM completely undermines trust in AMD's latest Secure Encrypted Virtualization (SEV-SNP) technology, which is widely deployed by major cloud providers, including Amazon AWS, Google Cloud, and Microsoft Azure,” members of the research team wrote in an email. “BadRAM for the first time studies the security risks of bad RAM—rogue memory modules that deliberately provide false information to the processor during startup. We show how BadRAM attackers can fake critical remote attestation reports and insert undetectable backdoors into _any_ SEV-protected VM.”</p>
<h2>Compromising the AMD SEV ecosystem</h2>
<p>On a <a href="https://badram.eu/">website</a> providing more information about the attack, the researchers wrote:</p>
<blockquote><p>Modern computers increasingly use encryption to protect sensitive data in DRAM, especially in shared cloud environments with pervasive data breaches and insider threats. AMD's Secure Encrypted Virtualization (SEV) is a cutting-edge technology that protects privacy and trust in cloud computing by encrypting a virtual machine's (VM's) memory and isolating it from advanced attackers, even those compromising critical infrastructure like the virtual machine manager or firmware.</p>
<p>We found that tampering with the embedded SPD chip on commercial DRAM modules allows attackers to bypass SEV protections—including AMD’s latest SEV-SNP version. For less than $10 in off-the-shelf equipment, we can trick the processor into allowing access to encrypted memory. We build on this BadRAM attack primitive to completely compromise the AMD SEV ecosystem, faking remote attestation reports and inserting backdoors into any SEV-protected VM.</p>
<p>In response to a vulnerability report filed by the researchers, AMD has already shipped patches to affected customers, a company spokesperson said. The researchers say there are no performance penalties, other than the possibility of additional time required during boot up. The BadRAM vulnerability is tracked in the industry as CVE-2024-21944 and AMD-SB-3015 by the chipmaker.</p></blockquote>

<h2>A stroll down memory lane</h2>
<p>Modern dynamic random access memory for servers typically comes in the form of DIMMs, short for <a href="https://en.wikipedia.org/wiki/DIMM">Dual In-Line Memory Modules</a>. The basic building block of these rectangular sticks are capacitors, which, when charged, represent a binary 1 and, when discharged, represent a 0. The capacitors are organized into cells, which are organized into arrays of rows and columns, which are further arranged into ranks and banks. The more capacitors that are stuffed into a DIMM, the more capacity it has to store data. Servers usually have multiple DIMMs that are organized into channels that can be processed in parallel.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>For a server to store or access a particular piece of data, it first must locate where the bits representing it are stored in this vast configuration of transistors. Locations are tracked through addresses that map the channel, rank, bank row, and column. For performance reasons, the task of translating these physical addresses to DRAM address bits—a job assigned to the memory controller—isn’t a one-to-one mapping. Rather, consecutive addresses are spread across different channels, ranks, and banks.</p>
<p>Before the server can map these locations, it must first know how many DIMMs are connected and the total capacity of memory they provide. This information is provided each time the server boots, when the BIOS queries the SPD—short for <a href="https://en.wikipedia.org/wiki/Serial_presence_detect">Serial Presence Detect</a>—chip found on the surface of the DIMM. This chip is responsible for providing the BIOS basic information about available memory. BadRAM causes the SPD chip to report that its capacity is twice what it actually is. It does this by adding an extra addressing bit.</p>
<p>To do this, a server admin need only briefly connect a specially programmed <a href="https://www.raspberrypi.com/">Raspberry Pi</a> to the SPD chip just once.</p>
<figure>
    <p><img width="1993" height="1219" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper.jpg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper.jpg 1993w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-640x391.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-1024x626.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-768x470.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-1536x939.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-980x599.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/badram-hardware-spd-tamper-1440x881.jpg 1440w" sizes="auto, (max-width: 1993px) 100vw, 1993px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The researchers' Raspberry Pi connected to the SPD chip of a DIMM.

              <span>
          Credit:

          
          De Meulemeester et al.

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<h2>Hacking by numbers, 1, 2, 3</h2>
<p>In some cases, with certain DIMM models that don't adequately lock down the chip, the modification can likely be done through software. In either case, the modification need only occur once. From then on, the SPD chip will falsify the memory capacity available.</p>
<p>Next, the server admin configures the operating system to ignore the newly created "ghost memory," meaning the top half of the capacity reported by the compromised SPD chip, but continue to map to the lower half of the real memory. On Linux, this configuration can be done with the `memmap` kernel command-line&nbsp;parameter. The researchers' paper, titled <a href="https://badram.eu/badram.pdf">BadRAM: Practical Memory Aliasing Attacks on Trusted Execution Environments</a>, provides many more details about the attack.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Next, a script developed as part of BadRAM allows the attacker to quickly find the memory locations of ghost memory bits. These aliases give the attacker access to memory regions that SEV-SNP is supposed to make inaccessible. This allows the attacker to read and write to these protected memory regions.</p>
<p>Access to this normally fortified region of memory allows the attacker to copy the cryptographic hash SEV-SNP creates to attest to the integrity of the VM. The access also permits the attacker to boot an SEV-compliant VM that has been backdoored. Normally, this malicious VM would trigger a warning in the form of a cryptographic hash. BadRAM allows the attacker to replace this attestation failure hash with the attestation success hash collected earlier.</p>
<p>The primary steps involved in BadRAM attacks are:</p>
<ol>
<li>Compromise the memory module to lie about its size and thus trick the CPU into accessing the nonexistent ghost addresses that have been silently mapped to existing memory regions.</li>
<li>Find aliases. These addresses map to the same DRAM location.</li>
<li>Bypass CPU Access Control. The aliases allow the attacker to bypass memory protections that are supposed to prevent the reading of and writing to regions storing sensitive data.</li>
</ol>

<h2>Beware of the ghost bit</h2>
<p>For those looking for more technical details, Jesse De Meulemeester, who along with Luca Wilke was lead co-author of the paper, provided the following, which more casual readers can skip:</p>
<blockquote><p>In our attack, there are two addresses that go to the same DRAM location; one is the original address, the other one is what we call the alias.</p>
<p>When we modify the SPD, we double its size. At a low level, this means all memory addresses now appear to have one extra bit. This extra bit is what we call the "ghost" bit, it is the address bit that is used by the CPU, but is not used (thus ignored) by the DIMM. The addresses for which this "ghost" bit is 0 are the original addresses, and the addresses for which this bit is 1 is the "ghost" memory.</p>
<p>This explains how we can access protected data like the launch digest. The launch digest is stored at an address with the ghost bit set to 0, and this address is protected; any attempt to access it is blocked by the CPU. However, if we try to access the same address with the ghost bit set to 1, the CPU treats it as a completely new address and allows access. On the DIMM side, the ghost bit is ignored, so both addresses (with ghost bit 0 or 1) point to the same physical memory location.</p>
<p>A small example to illustrate this:</p>
<p>Original SPD: 4 bit addresses:<br>
CPU: address 1101 -&gt; DIMM: address 1101</p>
<p>Modified SPD: Reports 5 bits even though it only has 4:<br>
CPU: address 01101 -&gt; DIMM: address 1101<br>
CPU: address 11101 -&gt; DIMM: address 1101</p>
<p>In this case 01101 is the protected address, 11101 is the alias. Even though to the CPU they seem like two different addresses, they go to the same DRAM location.</p></blockquote>
<p>As noted earlier, some DIMM models don't lock down the SPD chip, a failure that likely makes software-only modifications possible. Specifically, the researchers found that two DDR4 models made by Corsair contained this flaw.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>In a statement, AMD officials wrote:</p>
<blockquote><p>AMD believes exploiting the disclosed vulnerability requires an attacker either having physical access to the system, operating system kernel access on a system with unlocked memory modules, or installing a customized, malicious BIOS. AMD recommends utilizing memory modules that lock Serial Presence Detect (SPD), as well as following physical system security best practices. AMD has also released firmware updates to customers to mitigate the vulnerability.</p></blockquote>
<p>Members of the research team are from KU Leuven, the University of Lübeck, and the University of Birmingham. Specifically, they are:</p>
<ul>
<li><a href="https://www.esat.kuleuven.be/cosic/people/person/?u=u0156850">Jesse De Meulemeester</a> (<a href="https://www.esat.kuleuven.be/cosic/">COSIC</a>, <a href="https://www.esat.kuleuven.be/english">Department of Electrical Engineering</a>, <a href="https://www.kuleuven.be/english/kuleuven/index.html">KU Leuven</a>)</li>
<li><a href="https://luca-wilke.com/">Luca Wilke</a> (<a href="https://www.its.uni-luebeck.de/en/institute">University of Lübeck</a>)</li>
<li><a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/oswald-david">David Oswald</a> (<a href="https://www.birmingham.ac.uk/">University of Birmingham</a>)</li>
<li><a href="https://www.its.uni-luebeck.de/en/staff/thomas-eisenbarth">Thomas Eisenbarth</a> (<a href="https://www.its.uni-luebeck.de/en/institute">University of Lübeck</a>)</li>
<li><a href="https://www.esat.kuleuven.be/cosic/people/person/?u=u0018159">Ingrid Verbauwhede</a> (<a href="https://www.esat.kuleuven.be/cosic/">COSIC</a>, <a href="https://www.esat.kuleuven.be/english">Department of Electrical Engineering</a>, <a href="https://www.kuleuven.be/english/kuleuven/index.html">KU Leuven</a>)</li>
<li><a href="https://vanbulck.net/">Jo Van Bulck</a> (<a href="https://distrinet.cs.kuleuven.be/">DistriNet</a>, <a href="https://wms.cs.kuleuven.be/cs/english">Department of Computer Science</a>, <a href="https://www.kuleuven.be/english/kuleuven/index.html">KU Leuven</a>)</li>
</ul>
<p>The researchers tested BadRAM against the Intel SGX, a competing microprocessor sold by AMD's much bigger rival promising integrity assurances comparable to SEV-SNP. The classic, now-discontinued version of the SGX did allow reading of protected regions, but not writing to them. The current Intel Scalable SGX and Intel TDX processors, however, allowed no reading or writing. Since a comparable Arm processor wasn't available for testing, it's unknown if it's vulnerable.</p>
<p>Despite the lack of universality, the researchers warned that the design flaws underpinning the BadRAM vulnerability may creep into other systems and should always use the mitigations AMD has now put in place.</p>
<p>"Since our BadRAM primitive is generic, we argue that such countermeasures should be considered when designing a system against untrusted DRAM," the researchers wrote in their paper. "While advanced hardware-level attacks could potentially circumvent the currently used countermeasures, further research is required to judge whether they can be carried out in an impactful attacker model."</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/dan-goodin/"><img src="https://arstechnica.com/wp-content/uploads/2018/10/Dang.jpg" alt="Photo of Dan Goodin"></a></p>
  </div>

  <div>
    

    <p>
      Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Dan is based in San Francisco. Follow him at <a href="https://infosec.exchange/@dangoodin" rel="me">here</a> on Mastodon and <a href="https://bsky.app/profile/dangoodin.bsky.social">here</a> on Bluesky. Contact him on Signal at DanArs.82.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/information-technology/2024/12/new-badram-attack-neuters-security-assurances-in-amd-epyc-processors/#comments" title="80 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    80 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/space/2024/12/intrigue-swirls-as-blue-origin-races-toward-year-end-deadline-for-new-glenn/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/1733771570623-768x432.jpg" alt="Listing image for first story in Most Read: In a not-so-subtle signal to regulators, Blue Origin says New Glenn is ready" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running Durable Workflows in Postgres Using DBOS (175 pts)]]></title>
            <link>https://supabase.com/blog/durable-workflows-in-postgres-dbos</link>
            <guid>42379974</guid>
            <pubDate>Tue, 10 Dec 2024 18:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://supabase.com/blog/durable-workflows-in-postgres-dbos">https://supabase.com/blog/durable-workflows-in-postgres-dbos</a>, See on <a href="https://news.ycombinator.com/item?id=42379974">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p><img alt="Running Durable Workflows in Postgres using DBOS" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 100vw, (max-width: 1200px) 50vw, 33vw" srcset="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=256&amp;q=100 256w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=384&amp;q=100 384w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=640&amp;q=100 640w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=750&amp;q=100 750w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=828&amp;q=100 828w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=1080&amp;q=100 1080w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=1200&amp;q=100 1200w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=1920&amp;q=100 1920w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=2048&amp;q=100 2048w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=3840&amp;q=100 3840w" src="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Fdbos%2Fog.png&amp;w=3840&amp;q=100"></p><p>Michael Stonebraker is the inventor of Postgres and a Turing Award winner. His latest venture is <a href="https://www.dbos.dev/">DBOS</a>, a three-year joint research project between Stanford and MIT. The DBOS team have built a Durable Workflow engine using Postgres. It's one of the of the more elegant designs I've seen, leveraging the features of Postgres to keep it lightweight and fast.</p>
<p>The DBOS team have released a Supabase integration, so you can use your Postgres database as a durable workflow engine.</p>
<!-- -->
<p><strong>Continue reading or just get started?</strong></p>
<p>I really love the design of DBOS, so I'm going to write more below. Their design is aligned with our philosophy at Supabase: “just use Postgres”. I'll take you through the lower-level details in the rest of this post. If you just want to get started using DBOS with Supabase, get started using their tutorial:</p>
<p><a href="https://docs.dbos.dev/integrations/supabase">Use DBOS With Supabase →</a></p>

<p>Let's start with a common situation where a workflow is useful: you're running an e-commerce platform where an order goes through multiple “steps”:</p>
<!-- -->
<p>The process is simple, but writing a <em>robust</em> program for this is surprisingly difficult. Some potential problems:</p>
<ul>
<li>You get to step 2, “Check Inventory”, and you're out of stock. You need to wait 24 hours for the new inventory before you can ship it. You need that “step” to sleep for a day.</li>
<li>Your program crashes during step 3, “Ship Order”, and it doesn't record that you've shipped the inventory. You end up sending the same order twice.</li>
</ul>
<p>A Durable Workflow Engine helps with these problems (and more). There are a few on the market that provide different architectures like <a href="https://trigger.dev/">Trigger.dev</a>, <a href="https://www.inngest.com/">Inngest</a>, <a href="https://www.windmill.dev/">Windmill</a>, <a href="https://temporal.io/">Temporal</a>, and <a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a>.</p>
<p>DBOS offers a relatively unique approach to Workflows, storing the state in your own Postgres database. Let's explore how DBOS does it.</p>

<p>DBOS is a platform where you can write your programming logic in serverless functions (similar to Supabase Edge Functions). Functions can be written in either <a href="https://docs.dbos.dev/python/programming-guide">Python</a> or <a href="https://docs.dbos.dev/typescript/programming-guide">TypeScript</a>.</p>
<h3 id="creating-workflows-with-decorators">Creating workflows with decorators</h3>
<p>One thing that's different to Supabase Edge Functions is the ability to add <strong>decorators</strong> to your Functions with <code>DBOS.step()</code> and <code>DBOS.workflow()</code>:</p>
<!-- -->
<p>When you do this, DBOS stores the “state” of every step in Postgres:</p>
<!-- -->
<p>This is the part I find the most interesting! If you're a gamer, it's a bit like having a “<a href="https://en.wikipedia.org/wiki/Saved_game">save point</a>” in your programs. If a Function fails at any point, a new Function can start, picking up at the last checkpoint.</p>
<!-- -->
<h3 id="storing-function-state-in-postgres">Storing function state in Postgres</h3>
<p>When you create an application with DBOS, they create a new database inside your Postgres cluster for storing this state.</p>
<p>Using their “Widget Store” example, you can see two new databases -</p>
<ol>
<li><code>widget_store</code>: for storing the application data</li>
<li><code>widget_store_dbos_sys</code>: for storing the workflow state.</li>
</ol>
<!-- -->
<p>The <code>widget_store_dbos_sys</code> database holds the workflow state:</p>
<!-- -->
<h3 id="workflow-logic"><strong>Workflow logic</strong></h3>
<p>The DBOS team were kind enough to share some of the logic with me about how their workflow engine works:</p>
<ol>
<li>When a workflow starts, it generates a unique ID and stores it in a Postgres <code>workflow_status</code> table with a <code>PENDING</code> status. It also stores its inputs in Postgres.</li>
<li>Each time a step completes, it stores its output in a Postgres <code>operation_outputs</code> table.</li>
<li>When a workflow completes, it updates its status in the Postgres <code>workflow_status</code> table to <code>SUCCESS</code> (or to <code>ERROR</code>, if it threw an uncaught exception).</li>
</ol>
<h3 id="error-logic">Error logic</h3>
<p>If a program is interrupted, on restart the DBOS library launches a background thread that resumes all incomplete workflows from the last completed step.</p>
<ol>
<li>It queries Postgres to find all <code>PENDING</code> workflows, then starts each one. Because workflows are just Python functions, it can restart a workflow by simply calling the workflow function with its original inputs, retrieved from Postgres.</li>
<li>As a workflow re-executes, before trying each step, it first checks Postgres to see if that step was previously executed. If it finds the step in Postgres, it doesn't re-execute the step, instead re-using its original output.</li>
<li>Eventually, the workflow reaches the first step whose output isn't stored in Postgres and resumes execution from there - “resuming from the last completed step.”</li>
</ol>
<p>All this works because workflows are deterministic, so they can re-execute them using stored step outputs to recover their pre-interruption state.</p>
<h2 id="the-benefits-of-using-postgres">The benefits of using Postgres</h2>
<p>DBOS isn't the first to create a workflow engine. Others in the market include <a href="https://temporal.io/">Temporal</a> and <a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a>. DBOS provides a number of benefits over workflow engines that use external orchestrators like AWS Step Functions:</p>
<h3 id="performance">Performance</h3>
<p>Because a step transition is just a Postgres write (~1ms) versus an async dispatch from an external orchestrator (~100ms), it means DBOS is <a href="https://www.dbos.dev/blog/dbos-vs-aws-step-functions-benchmark">25x faster than AWS Step Functions</a>:</p>
<!-- -->
<h3 id="exactly-once-execution">Exactly-once execution</h3>
<p>DBOS has a special <code>@DBOS.Transaction</code> decorator. This runs the entire step inside a Postgres transaction. This guarantees exactly-once execution for databases transactional steps.</p>
<h3 id="idempotency">Idempotency</h3>
<p>You can set an idempotency key for a workflow to guarantee it executes only once, even if called multiple times with that key. Under the hood, this works by setting the workflow's unique ID to your idempotency key.</p>
<h3 id="other-postgres-features">Other Postgres features</h3>
<p>Since it's all in Postgres, you get all the tooling you're familiar with. Backups, GUIs, CLI tools - you name it. It all “just works”.</p>

<p>To get started with DBOS and Supabase, check out their official integration docs:</p>
<p><a href="https://docs.dbos.dev/integrations/supabase">Use DBOS With Supabase →</a></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Digital consumption keeps me from getting better at my job (288 pts)]]></title>
            <link>http://sibervepunk.com/digital-consumption.html</link>
            <guid>42379970</guid>
            <pubDate>Tue, 10 Dec 2024 18:46:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://sibervepunk.com/digital-consumption.html">http://sibervepunk.com/digital-consumption.html</a>, See on <a href="https://news.ycombinator.com/item?id=42379970">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>
    <time datetime="2024-07-31 00:00:00 +0000">31.07.2024</time>
  </p>

  

  <blockquote>
  <p>know thyself</p>
</blockquote>

<p>There is a new lifestyle imposed on almost the entire world, willingly or unwillingly, perhaps by powerful people or by many small people that want to be powerful, which somehow affects all ordinary people: a consumption-oriented life. Fast consumption, constant consumption, more consumption.</p>

<p>I don’t have much to say about the “shopping” side of this consumption craze because it’s a topic that’s been around for many years, born out of  <em>-ism</em> movements and studied numerous times through  <em>-ology</em> disciplines. It has been the subject of public service announcements, romantic comedies, and personal development books. The public has been constantly educated about it for years. Two guys known as The Minimalists and some “smart” people like Marie Kondo made a fortune out of this movement. Personally, I believe I am a conscious consumer, and the shopping craze doesn’t affect me much, so I want to look at the other, often-discussed side of the issue.</p>

<p>The consumption I will discuss is digital content, information, and emotion/thought consumption. I know there are social science studies that delve into the intersections and background connections of all these consumptions, but as an ordinary person, I want to talk about the effects on my own life, particularly my professional development.</p>

<p>Although it has been on my mind for a long time, I haven’t been able to read a comprehensive book based on these studies (<em>the reason being the vicious cycle based on this topic</em>), but I have consumed plenty of content… I’ve watched various TED talks, several indie YouTuber videos with a wholesome background, selling personal development under the hood on their newly launched channels, and of course, read tweets… I’ve also had plenty of opportunities to observe myself.</p>

<p>At this point I am convinced that fast consumption is harmful to the brain, mind, and soul.</p>

<p>The main reason I pursue this topic is that, aside from all the side effects in personal life, it also prevents me from being better at my profession as a brain-worker as Jules Payot puts it.</p>

<p>In disciplines like software engineering, constantly improving oneself and being in a state of continuous learning is an inevitable process. Even if you don’t put in extra effort and just try to do your job, you have to learn a new concept or technology. If you do put in the extra effort, you become someone who does their job better. Since graduating from undergraduate studies <em>(which marks exactly one year as I write this post)</em>, putting in extra effort has been my top priority. Working more, reading more, knowing more. In addition to technical studies, I also read about and received advice on soft skills related to “software crafting.” One of my first mistakes, I think, was taking every kind of advice from everyone. Even if I didn’t implement them directly, these pieces of advice took up space in my mind, and thinking “what if that’s better” prevented me from putting any of them into practice.</p>

<p>The problem with online advice is that the person writing the blog post is doing so entirely from their own perspective and lifestyle. They have no idea about you, and you have no idea about them. There’s no guarantee that what works for them will work for you. Moreover, you don’t get a chance to question causality, you just read the advice, consume it, and move on. It takes up space in your mind and on your to-do list, but you don’t get a chance to internalize or filter this topic. You don’t even realize that you should actually do so.</p>

<p>One of the pieces of advice I took without realizing it was to systematize the mentioned studies, work regularly, and similar. Once that idea put in my mind, things became complicated for me. While working full-time, I had to balance my personal life and stick to the plan.</p>

<p>No matter how much your willpower sticks to the plan, your health, developments in your life, and your brain, which sometimes refuses to accept more information, don’t always stick to the plan. When this happens, it becomes difficult to establish the system I mentioned, and you start looking for more advice, reading more blogs. <strong>You find yourself in a quest for productivity, feeling productive because of the quest, but not really doing any productive work.</strong></p>

<p>Advice also has the effect of reducing creativity and problem-solving skills. When I have a problem, technical or other, the first thing I do is research the solution. As a result, I don’t get enough chance to think about my problem, let alone produce a solution, and I don’t fully understand the problem.</p>

<p>I’ve “consumed” what I should do and how I should do it many times from different people. With all this information occupying my brain, I no longer had the energy and resources to produce a tangible output.</p>

<hr>

<p>Because of my profession, I like learning different concepts from different fields. I am particularly curious about the low-level infrastructures and systems behind high-level tools, and I am aware of the contribution of knowing these to doing my job well. However, because of the constant rush and haste imposed by social media in my life, I can’t devote enough time to these resources. Because I am so used to seeing information, quickly taking it in, and moving on to another topic. Because the short content I constantly consume, whether written or visual, has made me accustomed to this. <br>
While reading a technically deep book, re-reading the same page feels like a waste of time; every piece of information I have to go over because I couldn’t understand it at once makes me feel inadequate and late because I can’t just scroll past it.</p>

<p>I want to know everything, immediately, quickly. Since this is not humanly possible, I end up doing nothing.</p>

<p>I can’t think long-term; I can’t stop myself from thinking that working on a book for 6 months, doing its projects, is a huge waste of time for me, and because I already feel late, I find myself, yet again, in a cycle.</p>

<hr>

<p>When I’m focused solely on consuming, my ability to produce naturally decreases. I include speaking, being able to express oneself, and having a good command of words in this context. After knowing myself as someone who has always been good with words for years, seeing that I can’t choose the right word when speaking, or that I can’t convey the message or information I want to give more clearly and simply when writing, naturally bothers me. Although it is said that software development is an antisocial job, you constantly need to communicate with people, either in writing or verbally, and you need to express what you have done and what you will do well. I am approaching the point of losing this skill by consuming instead of producing.</p>

<hr>

<p>While all this disrupts progress and confuses my mind, I also have to deal with the physical and mental side effects of fast consumption. Difficulty concentrating, lack of focus, inability to understand what I read, stress, anxiety. I see these kinds of complaints from many people lately, and in my opinion, our biggest common ground is digital content consumption.</p>

<p>The relativity of time is a reality I feel to the core while doom scrolling. Besides the lost time, there’s the confusion after realizing it and putting the phone down, trying to get my dazed mind back to normal. And then, not finding anything to do, not being able to putting yourself together, and reaching for the phone again.</p>

<p>Everyone has seen the articles about the brain’s approach to social media content, which offers a quick, easily accessible way that makes you happy or, even if it doesn’t make you happy, offers an escape from the thing that makes you unhappy.</p>

<p>When you put these into words or write them down, it bothers you a lot, but I think knowing yourself is the most important thing to do before changing yourself. I know what I’m doing wrong, and now it’s documented in front of me. I also know what I need to work on.</p>



<p>We are talking about the harms, but I have always been fascinated by the opportunities the internet offers. Being able to communicate and chat with someone from anywhere in the world within seconds is an invaluable blessing. It just takes a little effort to filter to see and reach the right people’s content. Otherwise, I don’t think completely withdrawing would be very beneficial in my industry and the era I live in.</p>

<hr>

<p>I won’t go against what I mentioned so I won’t end this problem-filled post with advice or plans. That’s why I started with the quote “know thyself.” I just tried to see and make the problem tangible. I will stop researching what I can do for a while. First, I plan to clear my mind of clutter, quit this fast and excessive consumption habit I have acquired without realizing it, and then learn how to consume slowly and gradually. I have enough raw information to discover how to do all this myself; I will now give myself the opportunity to process it.</p>

</article><p>
    Thanks for reading. If you have any feedback or would like to discuss
    further, I would be happy to hear from you. <br>
    <!-- insert social links (twitter and email) -->
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canvas (162 pts)]]></title>
            <link>https://openai.com/index/introducing-canvas/</link>
            <guid>42379361</guid>
            <pubDate>Tue, 10 Dec 2024 18:01:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-canvas/">https://openai.com/index/introducing-canvas/</a>, See on <a href="https://news.ycombinator.com/item?id=42379361">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-canvas/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Devin is now generally available (144 pts)]]></title>
            <link>https://www.cognition.ai/blog/devin-generally-available</link>
            <guid>42378994</guid>
            <pubDate>Tue, 10 Dec 2024 17:30:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cognition.ai/blog/devin-generally-available">https://www.cognition.ai/blog/devin-generally-available</a>, See on <a href="https://news.ycombinator.com/item?id=42378994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> 
<p><img src="https://www.cognition.ai/_astro/generally-available.CHJ0T34J_Z1rnVDi.webp" alt="Devin is generally available" width="1600" height="900" loading="lazy" decoding="async"></p>
<p> Today we’re making Devin generally available starting at $500 a month for engineering teams, which includes: </p>
<ul>
<li>No seat limits</li>
<li>Access to Devin’s Slack integration, IDE extension, and <a href="https://www.cognition.ai/blog/dec-24-product-update#for-repetitive-engineering-tasks-try-the-devin-api-with-structured-inputoutput">API</a></li>
<li>Onboarding session &amp; support from the Cognition engineering team</li>
</ul>
<p> All engineering teams can now start working with Devin at <a href="https://app.devin.ai/" rel="noopener noreferrer" target="_blank">app.devin.ai</a>. </p>
<h2>  </h2> 
<p> While Devin can be an all-purpose tool, we recommend starting with: </p>
<ul>
<li><strong>Small frontend bugs and edge cases</strong> - tag Devin in Slack threads</li>
<li><strong>Creating first-draft PRs for backlog tasks</strong> - assign Devin tasks from your todo list at the start of your day</li>
<li><strong>Making targeted code refactors</strong> - use the Devin IDE extension (for VSCode and forks) to point Devin to parts of the code you want edited or upgraded</li>
</ul>
<p> Devin has helped teams with everything from building integrations to migrating and maintaining documentation. Devin is versatile, but works best when you: </p>
<ul>
<li>Give Devin tasks that you know how to do yourself</li>
<li>Tell Devin how to test or check its own work</li>
<li>Keep sessions under ~3 hours and break down large tasks</li>
<li>Share detailed requirements upfront</li>
<li>Invest in coaching Devin by providing feedback in chat and accepting suggested <a href="https://docs.devin.ai/Onboard_Devin/knowledge">Knowledge</a>, or adding your own Knowledge manually</li>
</ul>
<h2>  </h2> 
<p> Slack is the primary interface for spinning up Devin sessions, so you can quickly tag @devin to offload smaller tasks and fix bugs when they’re reported. </p>
<p> Try asking Devin to do the first pass on the next 3rd party integration, refactor, or codebase question you have. Devin messages you when it’s done, so you can review Devin’s PR whenever convenient. Devin responds automatically to your Github PR comments. </p>
<lite-youtube videoid="xTbpGq09g88"> <a href="https://youtube.com/watch?v=xTbpGq09g88"> <span></span> </a> </lite-youtube>  

<p> Hand off async work to Devin directly from your IDE with ⌘ G. The Devin extension (Beta Feature available for VScode and forks) allows you to checkout Devin’s PRs and review and accept Devin’s code directly in your IDE. </p>
<lite-youtube videoid="0e6Vx4rngm8"> <a href="https://youtube.com/watch?v=0e6Vx4rngm8"> <span></span> </a> </lite-youtube>  
<h2>  </h2> 
<p> To showcase Devin in action, we’re sharing sessions where Devin resolves issues on a few of our favorite open-source repositories. Devin often needs guidance but we share these examples to show how we use Devin to speed our own workflows. </p>
<p> <em><strong>Anthropic MCP</strong></em> </p>
<p> In this session, Devin identifies the cause of a user-reported issue. We liked how it read the MCP spec in the browser to understand “capability negotiation” and tested its changes end-to-end in the browser. The changes weren’t perfect, so the maintainers gave some feedback on the PR which we addressed with a second Devin session. </p>
<p> First session: <a href="https://app.devin.ai/sessions/266955553baf40cfa7fdd32d42ab219d">https://app.devin.ai/sessions/266955553baf40cfa7fdd32d42ab219d</a> </p>
<p> Second session: <a href="https://app.devin.ai/sessions/807168f5f9874d47a4c1965bf7afc9df">https://app.devin.ai/sessions/807168f5f9874d47a4c1965bf7afc9df</a> </p>
<p> <a href="https://github.com/modelcontextprotocol/inspector/pull/105">https://github.com/modelcontextprotocol/inspector/pull/105</a> </p>
<p> <img src="https://cdn.sanity.io/images/2mc9cv2v/production/c7c887019189c18355d40cf904fba5448f21d5f5-2014x168.png?w=1600&amp;fit=max" alt="Devin's PR to Anthropic MCP"> </p>

<p> <em><strong>Zod</strong></em> </p>
<p> This PR adds a new feature to the popular library Zod. Devin planned collaboratively with the user, implemented the feature across multiple files and wrote tests – we were very impressed! There was a merge conflict which we manually resolved because Devin tends to struggle with that. </p>
<p> <a href="https://app.devin.ai/sessions/51826709fcd3457abc4be25e587c790c">https://app.devin.ai/sessions/51826709fcd3457abc4be25e587c790c</a> </p>
<p> <a href="https://github.com/colinhacks/zod/pull/3893">https://github.com/colinhacks/zod/pull/3893</a> </p>
<p> <img src="https://cdn.sanity.io/images/2mc9cv2v/production/3411a1a87aa611878f7658e15649177b428bf706-1736x168.png?w=1600&amp;fit=max" alt="Devin's PR to Zod"> </p>

<p> <em><strong>Google</strong></em> </p>
<p> A user of the Go Github client wished it propagated response objects even on HTTP errors. These small chores are annoying for human engineers because testing is often more effort than the fix itself. Devin required a few iterations to get it right and we manually cleaned up a few stray edits using the VS Code extension. The biggest timesaver here was Devin writing and running the unit tests. </p>
<p> <a href="https://app.devin.ai/sessions/1b2f7ce6e3b44942b3ac1f518eac7c22">https://app.devin.ai/sessions/1b2f7ce6e3b44942b3ac1f518eac7c22</a> </p>
<p> <a href="https://github.com/google/go-github/pull/3369">https://github.com/google/go-github/pull/3369</a> </p>
<p> <img src="https://cdn.sanity.io/images/2mc9cv2v/production/18a73299e1ab51f155303433d963ed6069e18480-1854x168.png?w=1600&amp;fit=max" alt="Devin's PR to Google"> </p>

<p> <em><strong>Llama Index</strong></em> </p>
<p> Devin fixes a bug where the implementation of the Anthropic tokenizer followed the protocol incorrectly. It found the correct fix first try and wrote a unit test too. A PR comment from the maintainer requested a small stylistic change which we fixed manually. </p>
<p> <a href="https://app.devin.ai/sessions/3d66de6feed946efbadf8a58698caafc">https://app.devin.ai/sessions/3d66de6feed946efbadf8a58698caafc</a> </p>
<p> <a href="https://github.com/run-llama/llama_index/pull/17201">https://github.com/run-llama/llama_index/pull/17201</a> </p>
<p> <img src="https://cdn.sanity.io/images/2mc9cv2v/production/dc90640bf91bd47cdea29e98fc3b60aaf7cd8107-2042x168.png?w=1600&amp;fit=max" alt="Devin's PR to Llama Index"> </p>

<p> <em><strong>Karpathy’s nanoGPT</strong></em> </p>
<p> This change is just a one-liner. But we wouldn’t trust this change without testing – which Devin did nicely by writing an ad-hoc testing script. </p>
<p> <a href="https://app.devin.ai/sessions/9e0c3255385c463f838f5b2f4413b92f">https://app.devin.ai/sessions/9e0c3255385c463f838f5b2f4413b92f</a> </p>
<p> <a href="https://github.com/karpathy/nanoGPT/pull/578">https://github.com/karpathy/nanoGPT/pull/578</a> </p>
<p> <img src="https://cdn.sanity.io/images/2mc9cv2v/production/5a31675b672e516ea4fcb974ecfedb90b78a0c01-1712x168.png?w=1600&amp;fit=max" alt="Devin's PR to Karpathy's nanoGPT"> </p>
<h2>  </h2> 
<p> You can start working with Devin today at <a href="https://app.devin.ai/" rel="noopener noreferrer" target="_blank">app.devin.ai</a>. </p>
<p> For more information about <a href="https://devin.ai/enterprise">Devin Enterprise</a>, reach out to our Sales team <a href="https://cognition.ai/get-started#company">here</a>. </p> </div></div>]]></description>
        </item>
    </channel>
</rss>