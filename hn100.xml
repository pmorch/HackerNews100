<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 09 Jul 2023 07:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Perl first commit: a “replacement” for Awk and sed (138 pts)]]></title>
            <link>https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</link>
            <guid>36650120</guid>
            <pubDate>Sun, 09 Jul 2023 00:24:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1">https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</a>, See on <a href="https://news.ycombinator.com/item?id=36650120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" data-hotkey="y">Permalink</a></p>


<div>
  <p><a id="browse-at-time-link" href="https://github.com/Perl/perl5/tree/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-48b3911f-a378-408f-be27-09a795a734bd" for="browse-at-time-link" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
    <p>
      a "replacement" for awk and sed
    </p>

    <div><pre>[  Perl is kind of designed to make awk and sed semi-obsolete.  This posting
   will include the first 10 patches after the main source.  The following
   description is lifted from Larry's manpage. --r$  ]

   Perl is a interpreted language optimized for scanning arbitrary text
   files, extracting information from those text files, and printing
   reports based on that information.  It's also a good language for many
   system management tasks.  The language is intended to be practical
   (easy to use, efficient, complete) rather than beautiful (tiny,
   elegant, minimal).  It combines (in the author's opinion, anyway) some
   of the best features of C, sed, awk, and sh, so people familiar with
   those languages should have little difficulty with it.  (Language
   historians will also note some vestiges of csh, Pascal, and even
   BASIC-PLUS.) Expression syntax corresponds quite closely to C
   expression syntax.  If you have a problem that would ordinarily use sed
   or awk or sh, but it exceeds their capabilities or must run a little
   faster, and you don't want to write the silly thing in C, then perl may
   be for you.  There are also translators to turn your sed and awk
   scripts into perl scripts.</pre></div>

  <div>
  <include-fragment src="/Perl/perl5/branch_commits/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" id="async-branches-list">
    
    <ul>
      <li>Loading branch information<span></span></li>
    </ul>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California needs real math education, not gimmicks (167 pts)]]></title>
            <link>https://www.noahpinion.blog/p/california-needs-real-math-education</link>
            <guid>36650010</guid>
            <pubDate>Sun, 09 Jul 2023 00:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noahpinion.blog/p/california-needs-real-math-education">https://www.noahpinion.blog/p/california-needs-real-math-education</a>, See on <a href="https://news.ycombinator.com/item?id=36650010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" width="716" height="290.1373626373626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:1456,&quot;resizeWidth&quot;:716,&quot;bytes&quot;:183684,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>In an </span><a href="https://vimeo.com/65921206" rel="">old Saturday Night Live skit</a><span>, Chevy Chase, portraying Gerald Ford, says “It was my understanding that there would be no math.” Half a century later, it seems like this has become America’s national motto. Even as high-tech manufacturing has migrated relentlessly to China, plenty of Americans seem to think that they — or anyone — should be able to flourish in a modern economy without a functional understanding of mathematics. U.S. high school math scores </span><a href="https://www.bloomberg.com/opinion/articles/2019-12-12/u-s-schools-do-fine-in-international-rankings-except-in-math?sref=R8NfLgwS" rel="">lag significantly</a><span> behind other countries, even though scores in reading and science are above average, and our country is </span><a href="https://www.bloomberg.com/view/articles/2020-07-29/u-s-will-need-talented-refugees-if-skilled-immigrants-won-t-come?sref=R8NfLgwS" rel="">utterly dependent </a><span>on a continuous inflow of foreign talent for a number of critical STEM fields. Math, out of all subjects, seems to hold a special terror for Americans, who often seem to view the subject as a test of innate intelligence rather than </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">a skill that can be acquired and honed</a><span> through hard work. </span></em></p><p><em><span>In response to lagging math scores, educators in California have been trying to water down math education — banning students from taking algebra in 8th grade, replacing advanced algebra classes with “data science” courses that don’t even teach the algebra required to understand basic statistics, and so on. I see this as an extremely wrongheaded move, and I’ve been meaning to write about it for a while. But data analyst </span><a href="https://twitter.com/ArmandDoma" rel="">Armand Domalewski</a><span>, a friend of mine, has been following the issue far more closely than I have, and has been outspoken about it on social media, so I thought I would ask him to take a crack at saying what needs to be said.</span></em></p><p>One of the strangest things about California is that it is simultaneously one of the technology capitals of the world and has some of the worst math scores for children in the entire United States. In practice, California has relied on a combination of pockets of home grown math excellence and imported math whizzes from around the globe to bridge the gap between the math skills it needs and the math skills it has. It works, somewhat—but we can and should do better by all of the kids in our state.</p><p>We have a chance to do exactly that with the release of a new California Math Framework (C.M.F.), a document used by the state to establish math curricula for all public schools in California. Unfortunately, that process has been hijacked by a “math reform” movement led by Stanford Professor Dr. Jo Boaler, who claims to advocate for a more inclusive way of teaching that would replace memorizing times tables with real-world problem-solving. Her worldview has gained credence in influential educational circles because, to many people, including myself, the basic premise is extremely appealing: replacing rote memorization with creative problem solving, making math more inclusive to all kinds of students, embracing a growth mindset, etc. all sound lovely. And honestly, I don’t think at a high level that these concepts are wrong—what is broken, however, is the specific implementation of these ideas as advocated by Dr. Boaler and implemented by California education policymakers.</p><div><p><span>Math </span><em>should </em><span>be more inclusive. Math </span><em>should </em><span>be more engaging. I think one of the biggest mistakes both Dr. Boaler’s supporters and detractors have made in this debate is to try to slot what should be a practical, fact based argument about optimal math education into an ideological struggle invoking silly phrases like Woke Math. Dr. Boaler and her fans did not make math education worse by being too left wing in their math—whatever that even means—but by sloppy with their science and lazy with their facts. You don’t make math education better by advocating for changes based on lies, and unfortunately, that is exactly what happened here. </span></p><p><span>Two of the major policy changes proposed in the draft CMF are already showing indications of disaster. The first is moving Algebra education out of 8th Grade.</span></p></div><div><p><span>One of the ideas underpinning the California Math Framework is the notion that math needs to be “detracked”—instead of allowing some students to take Algebra I in 8th grade, it would require all students to enroll in the same math curriculum until the 9th grade. Advocates argue&nbsp; this promotes equity, while detractors argue that it diminishes excellence. Years ago, I supported San Francisco’s efforts to rework the curriculum based on that argument—I believed those who said it would improve educational outcomes for the kids struggling the most without hurting those who were already succeeding. </span></p><p><span>I was wrong. Not only did pushing out 8th grade Algebra hurt kids who were at the top of their class by forcing them to pay for private classes or other workarounds to get the credits they needed to apply for UCs, the claim that it would help outcomes for kids who were struggling turned out to be a bald faced lie.</span></p></div><p><span>In 2017, </span><a href="http://www.sfusdmath.org/uploads/2/4/0/9/24098802/historic_shifts_in_math_show_promise.pdf" rel="">SFUSD</a><span> claimed a "dramatic increase in student comprehension" and a drop in Algebra 1 repeaters from 40% to 7%, and credited detracking. An analysis by</span><a href="https://www.familiesforsanfrancisco.com/updates/inequity-in-numbers" rel=""> Families for San Francisco</a><span> found that this claim was utter nonsense. Algebra 1 grades did not improve at all, and the only reason the repeat rate went down was because SFUSD straight up </span><em>eliminated the requirement that you had take an exit exam in order to progress!</em><span>&nbsp;</span></p><p>Not only that, but the group was unable to replicate the 40% to 7% drop using the data provided by SFUSD through a records request, and no other independent entity has been able to validate that number as well.</p><p><span>Page 6 of this </span><a href="https://static1.squarespace.com/static/60412a3a51d4863950d1bdf2/t/616e2f823696906267609f3f/1634611077888/Report-+Inequity+in+Numbers.pdf" rel="">report</a><span> is particularly damning–SFUSD provides numbers that, when reverse engineered with some basic Algebra, would imply a class size of 2475 students when the actual class size was 4011. What happened to the other 1,536 students? The shoddiness of this evidence did not stop Dr. Boaler from touting this as a major success for her ideas, and until very recently, did not stop the CMF from citing it as a major argument in favor of detracking.</span></p><div><p><span>Not only did detracking not achieve its stated goals of advancing math equity in San Francisco, it actually harmed Black and brown students. By the end of 10th grade, Algebra 2 enrollments of Black and brown students declined, since their families were less likely to afford the expensive work arounds that white and Asian families pursued. Instead, most of the district’s Black and Latino students ended up in a diluted “compression” course that lacked about 75% of the state’s</span><a href="https://www.cde.ca.gov/ci/ma/cf/documents/mathfwprecalculus.pdf" rel=""> precalculus</a><span> “+” standards, where the “+” standards are defined as “additional mathematics to prepare students for advanced courses,” making it difficult for students to pursue more advanced math in college. (Which is why, counter the claims of some detracking advocates, the UCs do not officially credit this compression course as “advanced math.”) </span></p><p><span>The result? They’re grim. If you compare </span><a href="https://www.educationnext.org/san-franciscos-detracking-experiment/" rel="">statewide results against SFUSD results on California’s Smarter Balanced tests,</a><span> which assess student performance across the state, you see that between 2015 and 2019, at the state level, the eleventh-grade Black-White student&nbsp; gap grew by 11 points—from 94 to 105—while in SFUSD, the gap expanded by 15 points (from 143 to 158). The outcomes are even worse for Hispanic students. The Hispanic-White gap at the state level gap grew by only 5 points, but in SF, it grew by 31 points.</span></p></div><p><span>As with all education data, there are always a million variables and you can never conclusively say that a single policy change caused a specific outcome, but at the very least, it is hard to argue that these 8th Grade Algebra changes advocated by Boaler helped SFUSD, and even harder to argue they serve </span><em>as a model for our state.&nbsp;</em></p><p>Unfortunately, that is not the only controversial policy change being pitched in the CMF: another poorly conceived notion is the replacement of the second year of Algebra with “data science.”</p><p><span>For decades, American math curriculum has followed a standard sequence: arithmetic, algebra, geometry, algebra II, precalculus and trigonometry, and calculus. The University of California required you to take three years of high-school math, culminating in Algebra II. In October 2020, the UC Board of Admissions and Relations with Schools (BOARS) </span><a href="https://senate.universityofcalifornia.edu/_files/committees/boars/documents/statement-on-mathematics-preparation-for-uc.pdf" rel="">recommended allowing alternatives</a><span> to the second year of algebra—including data science. Courses like “</span><a href="https://www.introdatascience.org/" rel="">Introduction to Data Science</a><span>,” developed by UCLA and “</span><a href="https://hsdatascience.youcubed.org/" rel="">Explorations in Data Science</a><span>,” developed by Dr. Boaler, started popping up. The argument was that these classes would teach data skills relevant to the 21st century, such as collecting and analyzing data on “real-world topics,” in contrast to Algebra II, which Boaler said was as relevant as “</span><a href="https://www.latimes.com/opinion/story/2019-10-23/math-high-school-algebra-data-statistics" rel="">sock darning and shorthand</a><strong><span>.” </span></strong><span>And look, in theory, this sounds nice. I mean, my job is literally data analyst—I analyze, evaluate, and interpret data for a living! When I first heard about this, I was thrilled. But when I thought about it a bit more, it gave me pause—the skills I use daily as a data analyst are based on a foundation of Algebra and Calculus. It didn’t quite make sense to me how you could </span><em>replace </em><span>Algebra II with data science—the formulas that make up linear regression, for example, don’t make any sense unless you have at least a basic grasp of algebra.&nbsp; Logarithms and trigonometric functions are pretty core to doing data science work! So I started digging into what was actually being taught in these “data science” courses and was…frankly, I was horrified.</span></p><p><span>While UC Admissions requirements state that Algebra II alternatives still have to “build on” certain core concepts in Algebra II, in practice this does not seem to be enforced. The “Introduction to Data Science” produced by UCLA contains </span><a href="https://www.ucladatascienceed.org/wp-content/uploads/California-Common-Core-Mathematics-Standards-addressed-by-IDS.pdf" rel="">very little Algebra II</a><strong> </strong><span>and “Explorations in Data Science” only claims to</span><a href="https://docs.google.com/presentation/d/e/2PACX-1vQ4tVbSk5qZsgARWwctjKa6joNKKYi7_jzi2-hkDyr7yGzQSQgKCndzhfaICVooye55ZZqCBEVXpxSv/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" rel=""> teach the portions of Algebra II that overlap with statistics</a><span>, leaving huge swathes of math necessary for an eventual career in STEM completely untouched.&nbsp;</span></p><p><span>Frankly, reading the CMF does not give me the impression that its authors have a strong understanding of what data science </span><em>is</em><span>, exactly. It includes phrases like “the numbers are staggering: around 1.7 megabytes of digital data were created and stored every second for every person on Earth in 2020, and the vast majority of data goes unanalyzed.’ As </span><a href="https://sites.google.com/view/publiccommentsonthecmf/#h.w46loj4uaiev" rel="">Dr. Brian Conrad</a><span> points out, this is a nonsense statement. Is 1.7MB a large amount? (No, literally one JPEG can be that size.) Most of that is likely video, how exactly should that “data” be “analyzed”? And how do the authors even know it’s not being analyzed? After all, every video uploaded to YouTube gets tossed into an algorithm that produces viewing metrics, there are data scientists analyzing those uploads for trends, etc? (The authors probably found it by googling “impressive big data stats'', which spat out that statistic as the first result at the time the CMF was drafted.).&nbsp;</span></p><p><span>I wish this ignorance of the subject matter was limited to cute illustrative examples, but unfortunately, it permeates the basic thinking and structure of the document. The core issue of the CMF’s “data science” section is that it claims to be discussing data science while it is actually discussing </span><em>data literacy. </em><span>Don’t get me wrong—data literacy is good! Society would be better off if more people understood how to clean data or read a poll accurately. </span><em>But this is not data science and it is not math. </em><span>&nbsp;The CMF is replete with </span><a href="https://drive.google.com/file/d/1QI9XDw77ZlvwtcnLn_rKbhaWHo2UX-E2/view" rel="">statements</a><span> like “high-school data-science class students can learn to clean data sets – removing any data that is incorrect, corrupted, incorrectly formatted, duplicated, or incorrect in some other way [...] High school students can also learn to download and upload data, and develop the more sophisticated “data moves” that are important to learn if students are tackling real data sets.'' This teaches you how to use Excel, sure—but it does not teach you how regressions work, how statistical tests work, the multivariable calculus and linear algebra </span><em>you need to do the job of an actual data scientist!</em></p><p><span>In response to this, science and math professors across the state </span><a href="https://sites.google.com/view/mathindatamatters/home" rel="">have</a><span> been </span><a href="https://edsource.org/2022/proposed-mathematics-pathways-for-california-high-school-students-raise-equity-concerns/674400" rel="">raising</a><span> </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">alarms.</a><span> In response, Dr. Boaler turned to a tactic she often relies on—trying to wrap her ideas in the context of a broader culture war, painting critics as stodgy conservatives fighting her efforts to make math more equitable and diverse. She described her critics as those resisting change. The notion that teaching this version of data science rather than Algebra II is somehow more equitable permeates the CMF in often bizarre ways.&nbsp;</span></p><p><span>The CMF says data science is more </span><a href="https://sites.google.com/view/publiccommentsonthecmf/" rel="">equitable</a><span> than other STEM fields because “data scientists work together to address uncertainty in data while avoiding bias.”</span></p><p>Err, what? I’ve met many data scientists who do not work together or address uncertainty in data while avoiding bias, and many non-data science STEM professionals who do. There is absolutely nothing inherent to data science that makes it more collaborative or unbiased than other STEM fields…</p><div><p><span>It goes on to say…“Traditional mathematics lessons that have taught the subject as a set of procedures to follow have resulted in widespread disengagement as students see no relevance for their lives. This is particularly harmful for students of color and for girls…The data science field provides opportunities for equitable practice, with multiple opportunities for students to pursue answers to wonderings and to accept the reality that all students can excel in data science fields.'' </span></p><p><span>I agree that traditional math as currently taught does disengage a lot of students, and in particular women. But there is absolutely no evidence offered in the CMF to suggest that data science education would somehow be different, and there is something profoundly weird about the suggestion that students of color and girls can excel in data science fields but not excel in other fields of mathematics. The primary reason girls, for example, diverge from boys in math performance is </span><em><a href="https://techcrunch.com/2016/01/05/why-stems-future-rests-in-the-hands-of-12-year-old-girls/" rel="">because society teaches them that math is not for girls.</a></em><span> That is not something swapping out actual math for a watered down “data science” course can solve, and it’s pretty gross for people to claiming to be trying to make math more equitable for women and people of color to be pushing a program that will actually make them </span><em>worse at math. </em><span>Dr. Boaler and the CMF are basically saying “women and people of color aren’t doing as well in math, so we should just </span><em>give up on teaching them actual math</em><span>. It’s bananas!</span></p></div><p><span>But don’t take my white, male word for it—a group of Black UC faculty members in data science-related fields wrote a </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">letter</a><span> stating, “‘Introduction to Data Science’...make[s] claims that they specifically support learning for women and minorities, which are not only baseless, but fail to appreciate that they actually do the opposite and harm students from such groups by steering them away from being prepared for STEM majors.”</span></p><p><span>There’s a reason why these folks have been joined by other Black mathematicians around the country, such as </span><a href="https://www.chronicle.com/article/the-divider" rel="">Dr. Jelani Nelson,</a><span> in pushing back fiercely against the ideas around 8th Grade Algebra and data science proposed in the CMF. (And a reason, perhaps, that Dr. Boaler </span><a href="https://nypost.com/2022/04/08/stanford-prof-calls-cops-on-berkeley-prof-who-exposed-her-5k-hour-consulting-fee/" rel="">threatened to call the police on him for it!</a><span>) There’s a reason why Stanford Mathematics professor Dr. Brian Conrad&nbsp; wrote, in a comprehensive </span><a href="https://sites.google.com/view/publiccommentsonthecmf/?ref=stanfordreview.org#h.ns5n6hdqa4x8" rel="">takedown</a><span> of the CMF you really should read, that “whatever author is responsible for such a myopic view of mathematics should never again be involved in the setting of public policy guidance on math education.” There’s a reason why the </span><em><span>authors of papers Dr. Boaler cites to </span><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">back up her work consistently say she has misread and misrepresented their work</a></em><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">,</a><span> and that it does not support the claims she is making. And the reason, simply, is that her ideas have not worked. Forcing all children to defer Algebra until 9th grade,trying to squeeze two years of schooling into one year of a watered down “compression course” rejected by the University of California for not meeting its </span><a href="https://icas-ca.org/wp-content/uploads/2020/05/ICAS-Statement-Math-Competencies-2013.pdf" rel="">standards</a><span>, and replacing Algebra II with a glorified data literacy course masquerading as a “data science” course does not help high achieving kids or struggling kids or any kids in between—it hurts them all.</span></p><p><span>California students need different answers on math–what we’ve been doing for the past few decades hasn’t worked. But that doesn’t mean we should embrace the ideas embodied in the current CMF draft, which were built on decades of </span><a href="https://www.nonpartisaneducation.org/Review/Articles/v8n1.pdf" rel="">shoddy and dishonest academic research,</a><span> and throw up our hands at the notion of teaching underperforming kids advanced math entirely. The good news is that there are answers out there—we can learn from </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">other countries teach math differently than we do,</a><span> we can integrate findings from the </span><a href="https://www.cis.org.au/publication/myths-that-undermine-maths-teaching/" rel="">“science of math”</a><span>, and more. Our kids deserve a better California Math Framework than the one we’re being offered now—let’s get it done.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Unlearning Challenge (127 pts)]]></title>
            <link>https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</link>
            <guid>36649710</guid>
            <pubDate>Sat, 08 Jul 2023 23:14:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html">https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</a>, See on <a href="https://news.ycombinator.com/item?id=36649710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-667019077470952746">
<p><span>Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google</span>

</p><p>
Deep learning has recently driven tremendous progress in a wide array of applications, ranging from <a href="https://imagen.research.google/">realistic image generation</a> and <a href="https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html">impressive retrieval systems</a> to <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">language models that can hold human-like conversations</a>. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google’s AI <a href="https://ai.google/responsibility/principles/">Principles</a>, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy.
</p> <p>
Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it’s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [<a href="https://arxiv.org/abs/1610.05820">1</a>, <a href="https://arxiv.org/abs/2112.03570">2</a>] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction">membership inference attacks</a> (MIAs). This can raise privacy concerns, as it implies that even if an individual's data is deleted from a database, it may still be possible to infer whether that individual's data was used to train a model. 
</p>
<p>
Given the above, <em>machine unlearning</em> is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the "forget set" — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples <em>while maintaining</em> other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data.
</p>
<p>
Today we're thrilled to announce that we've teamed up with a broad group of academic and industrial researchers to organize the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a>, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms.
</p>




<h2>Machine unlearning applications</h2>


<p>
Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (e.g., due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. 
</p>
<p>
The field of machine unlearning is related to other areas of machine learning such as <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a>, <a href="https://arxiv.org/abs/1802.07569">life-long learning</a>, and <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">fairness</a>. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (e.g., demographics, age groups, etc.).
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png" imageanchor="1"><img data-original-height="405" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png"></a></td></tr><tr><td><b>Anatomy of unlearning.</b> An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the "forget set"). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.</td></tr></tbody></table>



<h2>Challenges of machine unlearning</h2>


<p>
The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model’s utility (e.g., accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while <a href="https://arxiv.org/abs/2007.02923">adding noise</a> to the weights achieves forgetting at the expense of utility. 
</p>
<p>
Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some <a href="https://arxiv.org/abs/1911.04933">works</a> report the classification accuracy on the samples to unlearn, <a href="https://proceedings.mlr.press/v119/wu20b.html">others</a> report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [<a href="https://arxiv.org/abs/2302.09880">4</a>, <a href="https://arxiv.org/abs/2010.10981">5</a>, <a href="https://arxiv.org/abs/2005.02205">6</a>].
</p>
<p>
We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we've teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge.
</p>




<h2>Announcing the first Machine Unlearning Challenge</h2>


<p>
We are pleased to announce the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>, which will be held as part of the <a href="https://neurips.cc/Conferences/2023/CompetitionTrack">NeurIPS 2023 Competition Track.</a> The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities.
</p>
<p>
The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a> and run between mid-July 2023 and mid-September 2023. As part of the competition, today we're announcing the availability of the <a href="https://github.com/unlearning-challenge/starting-kit">starting kit</a>. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset.
</p>
<p>
The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we'll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png" imageanchor="1"><img data-original-height="400" data-original-width="2000" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png"></a></td></tr><tr><td>Excerpt images from the <a href="https://github.com/microsoft/FaceSynthetics">Face Synthetics</a> dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.</td></tr></tbody></table>



<p>
For evaluating forgetting, we will use tools inspired by MIAs, such as <a href="https://arxiv.org/abs/2112.03570">LiRA</a>. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be <em>unable</em> to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. 
</p>



<h2>Conclusion</h2>


<p>
Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community.
</p>



<h2>Acknowledgements</h2>


<p>
<em>The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio C. S. Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.</em>
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Suck at Excel (2015) [video] (118 pts)]]></title>
            <link>https://www.youtube.com/watch?v=0nbkaYsR94c</link>
            <guid>36649047</guid>
            <pubDate>Sat, 08 Jul 2023 21:41:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=0nbkaYsR94c">https://www.youtube.com/watch?v=0nbkaYsR94c</a>, See on <a href="https://news.ycombinator.com/item?id=36649047">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Open Letter to Tim O’Reilly to Free the Perl Camel (128 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36648949</link>
            <guid>36648949</guid>
            <pubDate>Sat, 08 Jul 2023 21:27:44 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36648949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>This is (was?) O'Reilly's stance on the matter:<p><a href="https://web.archive.org/web/20180425080044/http://archive.oreilly.com/pub/a/oreilly/perl/usage" rel="nofollow noreferrer">https://web.archive.org/web/20180425080044/http://archive.or...</a></p><p>The Perl Camel Usage and Trademark Information</p><p>As most of you probably know, O'Reilly started putting animal images on the covers of our books about thirteen years ago. To millions of readers, the animals mean O'Reilly. They've become our signature "trade dress." We've also trademarked the association between particular animals and the subject of their books. After all, the only reason that people think of camels in association with Perl is because we used a camel on the cover of Programming Perl.</p><p>We recognize that things do get more complicated, though, when an image like the camel is so widely known that it comes to symbolize not just our products but also the entire Perl language. This is a good thing, and we want it to continue. But trademark law is sticky on this point. If a trademark isn't "protected" (by letters asking people not to use it, or by licenses that allow them to use it only in specific ways), it gets into the public domain and loses its protected status. If this happened, anyone could use the camel without restriction, including in ways that were detrimental to the language. For example, you might imagine a company creating a Perl-compatible language, branding it with a camel, and pushing it as the "official Perl" in an attempt to drive Larry Wall's Perl out of existence.</p><p>Another important issue is that a brand is strong in proportion to two things: its ubiquity and its distinctiveness. It's important that, just as we want one version of Perl (so we don't have the fragmentation that was the downfall of UNIX), we have one symbol for Perl. To protect the integrity and impact of that symbol, we need to maintain some artistic control over what kinds of camel images are used. We believe that "one camel" will strengthen the overall Perl brand.</p><p>In short, we're walking a fine line, trying to make the camel as available as possible as a symbol for Perl while protecting it as a trademark. So, here's our policy on using the camel image:</p><p>Non-commercial use</p><p>We will license the camel image widely for open source products and non-commercial sites related to Perl, requiring only an acknowledgement of its trademark status and a link to www.perl.com. To request the camel artwork, please send email to permissions@oreilly.com, indicating where, how, and for what purpose you plan to use the image. Please note that we generally do not allow alterations of the Perl camel artwork.</p><p>Some non-commercial sites currently using the Perl camel:</p><p>(snipped)</p><p>We also offer the Programming Republic of Perl logo for some non-commercial sites. Feel free to download these logos for use on your pages. Please make the logo a link to www.perl.com.</p><p>Some sites using the Programming Republic of Perl logo:</p><p>(snipped)</p><p>We may also license the Perl camel image for some commercial products and sites related to Perl. To inquire about the use of a camel image on any commercial product or site, please send email to permissions@oreilly.com with a description of the product or web site, indicating where and how you'd like to use the camel.</p><p>We've also created "Powered by Perl" buttons that any site using Perl may use on web pages. Feel free to download and use these buttons. Please make the buttons link to www.perl.com.</p><p>And the Camel FAQ:</p><p><a href="https://web.archive.org/web/20180123132933/http://archive.oreilly.com/pub/a/oreilly/perl/usage/faq.html" rel="nofollow noreferrer">https://web.archive.org/web/20180123132933/http://archive.or...</a></p><p>Q: So are you saying that O'Reilly has trademarked an entire animal?</p><p>A: No. When a company receives a trademark, it receives protection for a symbol in a particular category of products or services. For example, Owens Corning has trademarked the color pink. The whole color? No, only for insulation. O'Reilly has protected the camel image for books and online publications related to the Perl language, and related product and services. The only reason an association exists between camels and the Perl programming language is because we've used a camel image on our Perl-related products.</p><p>Q: Do you just own the particular Camel on the cover of Programming Perl, or all camels?</p><p>A: We own the particular camel image shown above, which has lead to an association between camels and the Perl language. If someone were to use a different camel on their Perl book, there could be confusion over which one "The Camel Book" referred to, and we might need to step in and stop use of that camel image. That's how trademarks work, helping to protect confusion in the marketplace.</p><p>Q: I want to design a T-shirt with the Perl camel on it. Do I need to get your permission?</p><p>A: Yes. But we're willing to make allowances for those of you who have creative ideas and want to do something fun for your friends. So, if the lifetime print run of the T-shirt design is less than 100, you may consider permission automatically granted. For larger print runs, please ask first. We promise to answer quickly!</p><p>Q: Why isn't your trademark just restricted to books?</p><p>A: We also do conferences, software, research, and online publishing in Perl, and we use the camel image for those things as well. We may want to camel-brand other Perl-related products in the future.</p><p>Q: I want to use $camel as a variable name in a Perl program. Do I need to acknowledge the trademark?</p><p>A: No.</p><p>Q: I want to use a cartoon camel as the logo for my software product. Is that okay?</p><p>A: It depends on what your product is, how it was developed, and how you intend to distribute it. Please send email to permissions@oreilly.com, with information about what you'd like to do, and we'll get back to you.</p><p>Q: I want to place a picture of a camel on my Perl web page. Am I allowed to do that? Do I have to use your camel?</p><p>A:Yes, as long as your page is non-commercial, and the context in which the camel is placed portrays Perl in a positive light. You will need to include the following language in small text somewhere on the page where the camel appears:</p><p>"The Perl camel image is a trademark of O'Reilly Media, Inc. Used with permission."</p><p>Please make the "O'Reilly Media, Inc." part of the statement a link to our home page (<a href="http://www.oreilly.com/" rel="nofollow noreferrer">http://www.oreilly.com</a>).</p><p>We'd encourage you to use the Perl camel we use, as it has wide recognition as "the Perl camel." But if you have another camel you'd like to use on a non-commercial site we generally would not object, so as long as the image is in no way derogatory.</p><p>Please note: If you use the "Powered by Perl" or the "Programming Republic of Perl" buttons, please make those active links to <a href="http://www.perl.com/" rel="nofollow noreferrer">http://www.perl.com</a>, not the O'Reilly home page.</p><p>Q: What is the Programming Republic of Perl logo?</p><p>A: The Programming Republic of Perl logo was developed some years ago for non-commercial use on web sites, and serves as a pointer to www.perl.com. Feel free to use it on any non-commercial pages. You can find it on the main Perl Camel Usage and Trademark Information page.</p><p>Q: Where can I find out more about camels?</p><p><a href="http://www.sandiegozoo.org/animalbytes/t-camel.html" rel="nofollow noreferrer">http://www.sandiegozoo.org/animalbytes/t-camel.html</a></p><p>If you have questions or comments about the Perl camel or any other O'Reilly trademarks, or if you want to use one of our trademarks in some way that we haven't explicitly described on this page, please send a detailed request to permissions@oreilly.com. For more information, see the Perl Camel FAQ.
              </p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open sourcing the Nginx playground (115 pts)]]></title>
            <link>https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</link>
            <guid>36648821</guid>
            <pubDate>Sat, 08 Jul 2023 21:11:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/">https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=36648821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! In 2021 I released a small playground for testing nginx configurations
called <a href="https://nginx-playground.wizardzines.com/">nginx playground</a>. There’s a
<a href="https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/">blog post about it here</a>.</p>

<p>This is an extremely short post to say that at the time I didn’t make it open source,
but I am making it open source now. It’s not a lot of code but maybe it’ll be
interesting to someone.</p>

<p>Here’s <a href="https://github.com/jvns/nginx-playground/">the github repo</a>. The
frontend is in <code>static/</code> and the backend is in <code>api/</code>. The README is mostly an
extended apology for the developer experience and note that the project is
unmaintained. But I did test that the build instructions work!</p>

<h3 id="why-didn-t-i-open-source-this-before">why didn’t I open source this before?</h3>

<p>I’m not very good at open source. Some of the problems I have with open sourcing things are:</p>

<ul>
<li>I dislike (and am very bad at) maintaining open source projects – I usually
ignore basically all feature requests and most bug reports and then feel bad about it.
I handed off maintainership to both of the open source projects that I
started (<a href="https://github.com/rbspy/rbspy">rbspy</a> and <a href="https://github.com/rust-bpf/rust-bcc">rust-bcc</a>) to other people who are doing a MUCH better job than I ever did.</li>
<li>Sometimes the developer experience for the project is pretty bad</li>
<li>Sometimes there’s configuration in the project (like the <code>fly.toml</code> or the
analytics I have set up) which don’t really make sense for other people to
copy</li>
</ul>

<h3 id="new-approach-don-t-pretend-i-m-going-to-improve-it">new approach: don’t pretend I’m going to improve it</h3>

<p>In the past I’ve had some kind of belief that I’m going to improve the problems
with my code later. But I haven’t touched this project in more than a year and
I think it’s unlikely I’m going to go back to it unless it breaks in some dramatic way.</p>

<p>So instead of pretending I’m going to improve things, I decided to just:</p>

<ul>
<li>tell people in the README that the project is unmaintained</li>
<li>write down all the security caveats I know about</li>
<li>test the build instructions I wrote to make sure that they work (on a fresh machine, even!)</li>
<li>explain (but do not fix!!) some of the messy parts of the project</li>
</ul>

<h3 id="that-s-all">that’s all!</h3>

<p>Maybe I will open source more of my tiny projects in the future, we’ll see!
Thanks to <a href="https://www.changeset.nyc/">Sumana Harihareswara</a> for helping me
think through this.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Htmx Gaining in Popularity? (127 pts)]]></title>
            <link>https://trends.builtwith.com/javascript/Htmx</link>
            <guid>36648817</guid>
            <pubDate>Sat, 08 Jul 2023 21:10:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trends.builtwith.com/javascript/Htmx">https://trends.builtwith.com/javascript/Htmx</a>, See on <a href="https://news.ycombinator.com/item?id=36648817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div>
<div>
<p>
<label for="tk">Top 10k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="hk">Top 100k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="m">Top 1m</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="ei">All Internet</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
</div>

<div>
<div>
<p><img data-src="https://deo39crpw7zzn.cloudfront.net/thumb/0c-50-90-ed-cz-c1/n" alt="Htmx" width="48" height="48">
</p>
<div>
<p>Gives you access to AJAX, CSS Transitions, WebSockets and Server Sent Events directly in HTML, using attributes.</p>
<p><a href="https://htmx.org/" target="_blank" rel="nofollow noopener">https://htmx.org</a> </p>
<p><a href="https://trends.builtwith.com/javascript">JavaScript Libraries and Functions</a></p>
</div>
</div>
<div>
<h5>Htmx Customers</h5>
<p>
Get access to data on <a href="https://trends.builtwith.com/websitelist/Htmx/Historical">9,204 websites</a> that are Htmx Customers. We know of <a href="https://trends.builtwith.com/websitelist/Htmx">7,188 live websites</a> using Htmx and an additional 2,016 sites that used Htmx historically and <a href="https://trends.builtwith.com/websitelist/Htmx/Switzerland">178 websites in Switzerland</a>.</p>
<p>
<a href="https://trends.builtwith.com/websitelist/Htmx">
<svg>
<use xlink:href="#icon-arrow-alt-circle-down"></use></svg>
Download Lead List</a>
</p>
</div>
</div>
<div>
<div>
<p>
<h5><svg><use xlink:href="#icon-award"></use></svg>Htmx Awards</h5>
</p>
</div>
<div>
<div>
<ul>
</ul></div> <p>No awards yet.</p>
</div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Personal ChatGPT Using Your Data (259 pts)]]></title>
            <link>https://github.com/raghavan/PdfGptIndexer</link>
            <guid>36648794</guid>
            <pubDate>Sat, 08 Jul 2023 21:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/raghavan/PdfGptIndexer">https://github.com/raghavan/PdfGptIndexer</a>, See on <a href="https://news.ycombinator.com/item?id=36648794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">PdfGptIndexer</h2>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">PdfGptIndexer is an efficient tool for indexing and searching PDF text data using OpenAI's GPT-2 model and FAISS (Facebook AI Similarity Search). This software is designed for rapid information retrieval and superior search accuracy.</p>
<h2 tabindex="-1" dir="auto">Libraries Used</h2>
<ol dir="auto">
<li><a href="https://github.com/deanmalmgren/textract">Textract</a> - A Python library for extracting text from any document.</li>
<li><a href="https://github.com/huggingface/transformers">Transformers</a> - A library by Hugging Face providing state-of-the-art general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG).</li>
<li><a href="https://python.langchain.com/" rel="nofollow">Langchain</a> - A text processing and embeddings library.</li>
<li><a href="https://github.com/facebookresearch/faiss">FAISS (Facebook AI Similarity Search)</a> - A library for efficient similarity search and clustering of dense vectors.</li>
</ol>
<h2 tabindex="-1" dir="auto">Installing Dependencies</h2>
<p dir="auto">You can install all dependencies by running the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install langchain openai textract transformers langchain faiss-cpu"><pre>pip install langchain openai textract transformers langchain faiss-cpu</pre></div>
<h2 tabindex="-1" dir="auto">How It Works</h2>
<p dir="auto">The PdfGptIndexer operates in several stages:</p>
<ol dir="auto">
<li>It first processes a specified folder of PDF documents, extracting the text and splitting it into manageable chunks using a GPT-2 tokenizer from the Transformers library.</li>
<li>Each text chunk is then embedded using the OpenAI GPT-2 model through the LangChain library.</li>
<li>These embeddings are stored in a FAISS index, providing a compact and efficient storage method.</li>
<li>Finally, a query interface allows you to retrieve relevant information from the indexed data by asking questions. The application fetches and displays the most relevant text chunk.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png"><img src="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png" alt="Untitled-2023-06-16-1537"></a></p>
<h2 tabindex="-1" dir="auto">Advantages of Storing Embeddings Locally</h2>
<p dir="auto">Storing embeddings locally provides several advantages:</p>
<ol dir="auto">
<li>Speed: Once the embeddings are stored, retrieval of data is significantly faster as there's no need to compute embeddings in real-time.</li>
<li>Offline access: After the initial embedding creation, the data can be accessed offline.</li>
<li>Compute Savings: You only need to compute the embeddings once and reuse them, saving computational resources.</li>
<li>Scalability: This makes it feasible to work with large datasets that would be otherwise difficult to process in real-time.</li>
</ol>
<h2 tabindex="-1" dir="auto">Running the Program</h2>
<p dir="auto">To run the program, you should:</p>
<ol dir="auto">
<li>Make sure you have installed all dependencies.</li>
<li>Clone the repository to your local machine.</li>
<li>Navigate to the directory containing the Python script.</li>
<li>Replace "&lt;OPENAI_API_KEY&gt;" with your actual OpenAI API key in the script.</li>
<li>Finally, run the script with Python.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python3 pdf_gpt_indexer.py"><pre><span>python3</span> <span>pdf_gpt_indexer</span>.<span>py</span></pre></div>
<p dir="auto">Please ensure that the folders specified in the script for PDF documents and the output text files exist and are accessible. The query interface will start after the embeddings are computed and stored. You can exit the query interface by typing 'exit'.</p>
<h2 tabindex="-1" dir="auto">Exploring Custom Data with ChatGPT</h2>
<p dir="auto">Check out the post <a href="https://devden.raghavan.studio/p/chatgpt-using-your-own-data" rel="nofollow">here</a> for a comprehensive guide on how to utilize ChatGPT with your own custom data.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn Electronics by Practice (261 pts)]]></title>
            <link>https://beletronics.wordpress.com/</link>
            <guid>36647364</guid>
            <pubDate>Sat, 08 Jul 2023 18:35:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beletronics.wordpress.com/">https://beletronics.wordpress.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36647364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
<div>
<div>

<p>(residence of electronics enthusiasts)</p></div>


<nav aria-label="Primary" data-wp-interactive="" data-wp-context="{ &quot;core&quot;: { &quot;navigation&quot;: { &quot;isMenuOpen&quot;: { &quot;click&quot;: false, &quot;hover&quot;: false }, &quot;overlay&quot;: true, &quot;roleAttribute&quot;: &quot;&quot; } } }">
			<div id="modal-3" aria-label="Menu" data-wp-bind--aria-modal="selectors.core.navigation.isMenuOpen" data-wp-bind--role="selectors.core.navigation.roleAttribute" data-wp-effect="effects.core.navigation.initMenu" tabindex="-1" data-micromodal-close="" data-wp-class--has-modal-open="selectors.core.navigation.isMenuOpen" data-wp-class--is-menu-open="selectors.core.navigation.isMenuOpen" data-wp-on--keydown="actions.core.navigation.handleMenuKeydown" data-wp-on--focusout="actions.core.navigation.handleMenuFocusout">
							<ul><li><a href="https://beletronics.wordpress.com/mission/"><span>Mission</span></a></li><li><a href="https://beletronics.wordpress.com/about/"><span>About</span></a></li></ul>
						</div></nav></div>




</header>


<p><h2>Learn electronics by&nbsp;practice</h2></p>



<main>

<div>
<p>This website is dedicated to electronics enthusiasts and aspirants who believe that true knowledge comes through the persistence of constant practice.</p>



<p>The content is divided into parts presented hereafter. </p>



<figure><a href="https://beletronics.wordpress.com/learn-basic-electronics/"><img decoding="async" data-attachment-id="52" data-permalink="https://beletronics.wordpress.com/home/basic/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/basic.png" data-orig-size="1497,729" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basic" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" alt="Learn basic electronics" width="498" height="243" srcset="https://beletronics.files.wordpress.com/2022/05/basic.png?w=498 498w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=996 996w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=768 768w" sizes="(max-width: 498px) 100vw, 498px"></a><figcaption><a href="https://beletronics.wordpress.com/learn-basic-electronics/">Learn basic electronics – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/pong-game/"><img decoding="async" width="3660" height="1275" data-attachment-id="1345" data-permalink="https://beletronics.wordpress.com/home/pong-2/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png" data-orig-size="3660,1275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pong-2" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" alt="" srcset="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024 1024w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=2048 2048w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><a href="https://beletronics.wordpress.com/pong-game/">Create a legendary Pong game – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/cp4u-processor/"><img decoding="async" data-attachment-id="2594" data-permalink="https://beletronics.wordpress.com/home/cp4u/" data-orig-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png" data-orig-size="2494,1636" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cp4u" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" src="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" alt="" width="475" height="311" srcset="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=475 475w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=948 948w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=150 150w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300 300w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=768 768w" sizes="(max-width: 475px) 100vw, 475px"></a><figcaption><a href="https://beletronics.wordpress.com/cp4u-processor/">Create a 4-bit processor – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/z80-computer/"><img decoding="async" data-attachment-id="1841" data-permalink="https://beletronics.wordpress.com/home/z80/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg" data-orig-size="4007,2093" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone XS Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1653660645&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.25&quot;,&quot;iso&quot;:&quot;320&quot;,&quot;shutter_speed&quot;:&quot;0.016666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;,&quot;latitude&quot;:&quot;45.812958333333&quot;,&quot;longitude&quot;:&quot;15.942352777778&quot;}" data-image-title="z80" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" src="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" alt="" width="445" height="232" srcset="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=445 445w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=888 888w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=150 150w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300 300w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=768 768w" sizes="(max-width: 445px) 100vw, 445px"></a><figcaption><a href="https://beletronics.wordpress.com/z80-computer/">Create a complete Z80 computer – click to read</a></figcaption></figure>
			
			
			</div></main>



<div>
<!-- You can start editing here. -->

	<h3 id="comments">
		6 responses to “Learn electronics by&nbsp;practice”	</h3>

	

	<ol>
			<li id="comment-2">
			<article id="div-comment-2">
				<!-- .comment-meta -->

				<div>
					<p>Very nice 🙂</p>
<p id="comment-like-2" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=2&amp;_wpnonce=9ed7fde271" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-2">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=2#respond" data-commentid="2" data-postid="36" data-belowelement="div-comment-2" data-respondelement="respond" data-replyto="Reply to Heidobito" aria-label="Reply to Heidobito">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-3">
			<article id="div-comment-3">
				<!-- .comment-meta -->

				<div>
					<p>Thank you!</p>
<p id="comment-like-3" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=3&amp;_wpnonce=08fa246496" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-3">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=3#respond" data-commentid="3" data-postid="36" data-belowelement="div-comment-3" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-11">
			<article id="div-comment-11">
				<!-- .comment-meta -->

				<div>
					<p>Respect!<br>
Thank you for your effort and enthusiasm!</p>
<p id="comment-like-11" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=11&amp;_wpnonce=f04ba9290e" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-11">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=11#respond" data-commentid="11" data-postid="36" data-belowelement="div-comment-11" data-respondelement="respond" data-replyto="Reply to Tomislav Valecic" aria-label="Reply to Tomislav Valecic">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-12">
			<article id="div-comment-12">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Tomislav!</p>
<p id="comment-like-12" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=12&amp;_wpnonce=ddb5fad259" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-12">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=12#respond" data-commentid="12" data-postid="36" data-belowelement="div-comment-12" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-13">
			<article id="div-comment-13">
				<!-- .comment-meta -->

				<div>
					<p>Thank you, Bele! You’re the best.</p>
<p id="comment-like-13" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=13&amp;_wpnonce=0f7e0dc91d" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-13">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=13#respond" data-commentid="13" data-postid="36" data-belowelement="div-comment-13" data-respondelement="respond" data-replyto="Reply to Danko Kozar" aria-label="Reply to Danko Kozar">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-14">
			<article id="div-comment-14">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Danko. Means a lot, especially from you!</p>
<p id="comment-like-14" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=14&amp;_wpnonce=f8e8a70822" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-14">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=14#respond" data-commentid="14" data-postid="36" data-belowelement="div-comment-14" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	</ol>

	

	<div id="respond">
		<h3 id="reply-title">Leave a Reply <small></small></h3><form action="https://beletronics.wordpress.com/wp-comments-post.php" method="post" id="commentform" novalidate="">


<div>
	<p><label for="comment">Enter your comment here…</label></p>
</div>

		<div id="comment-form-identity">
	<div id="comment-form-nascar">
		<p>Fill in your details below or click an icon to log in:</p>
		<ul>
			
			<li>
				<a href="#comment-form-load-service:WordPress.com" id="postas-wordpress" title="Login via WordPress.com">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg>				</a>
			</li>
			<li>
				<a href="#comment-form-load-service:Facebook" id="postas-facebook" title="Login via Facebook">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg>				</a>
			</li>
		</ul>
	</div>

	<div id="comment-form-guest">
			<p><a href="https://gravatar.com/site/signup/" target="_blank">				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="Gravatar" width="25">
</a>			</p>

				<div>
				<div>
					<p><label for="email">Email <span>(required)</span> <span>(Address never made public)</span></label></p>
				</div>
				<div>
					<p><label for="author">Name <span>(required)</span></label></p>
				</div>
				<div>
					<p><label for="url">Website</label></p>
				</div>
			</div>
			
		</div>

	<div id="comment-form-wordpress">
			<p><img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="WordPress.com Logo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your WordPress.com account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg></span>
		</p>
					</div>
	
		</div>

	<div id="comment-form-facebook">
			<p><img src="" alt="Facebook photo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your Facebook account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg></span>
		</p>
					</div>
	
		</div>


	<div id="comment-form-load-service">
		<div><p>Cancel</p></div>
		<p>Connecting to %s</p>
	</div>

</div>



<div id="comment-form-subscribe">
	<p> <label id="subscribe-label" for="subscribe">Notify me of new comments via email.</label></p><p> <label id="subscribe-blog-label" for="subscribe_blog">Notify me of new posts via email.</label></p></div>

	






</form>	</div><!-- #respond -->
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Container Training (136 pts)]]></title>
            <link>https://container.training/</link>
            <guid>36647211</guid>
            <pubDate>Sat, 08 Jul 2023 18:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://container.training/">https://container.training/</a>, See on <a href="https://news.ycombinator.com/item?id=36647211">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <table>
      <tbody><tr><td colspan="3">Container Training</td></tr>
      <tr><td colspan="3">Note: while some workshops are delivered in other languages, slides are always in English.</td></tr>

      <tr><td colspan="3">Free Kubernetes intro course</td></tr>

      <tr>
      	<td>Getting Started With Kubernetes and Container Orchestration</td>
      	<td><a href="https://qconuk2019.container.training/"></a></td>
      	<td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
      </tr>
      <tr>
        <td>This is a live recording of a 1-day workshop that took place at QCON London in March 2019.</td>
      </tr>
      <tr>
        <td>If you're interested, we can deliver that workshop (or longer courses) to your team or organization.</td>
      </tr>
      <tr>
        <td>Contact <a href="mailto:jerome.petazzoni@gmail.com">Jérôme Petazzoni</a> to make that happen!</td>
      </tr>

      

      
        <tr><td colspan="3">Past workshops</td></tr>

        
          <tr>
            <td>Opérer Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 18th-19th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Kubernetes avancé (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 8th-16th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Packaging et CI/CD pour Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 11th-12th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Fondamentaux Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 4th-7th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Docker intensif (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered September 27th-29th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        

        
          <tr>
            <td>... and at least <a href="https://container.training/past.html">111 more</a>.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Recorded workshops</td></tr>

        
          <tr>
            <td>Getting started with Kubernetes and container orchestration</td>
            <td><a href="https://pycon2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/watch?v=J08MrW2NC1Y"></a></td>
          </tr>
          <tr>
            <td>Delivered May 1st, 2019 at PyCon in Cleveland, OH.</td>
          </tr>
        
          <tr>
            <td>Getting Started With Kubernetes and Container Orchestration</td>
            <td><a href="https://qconuk2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
          </tr>
          <tr>
            <td>Delivered March 8th, 2019 at QCON in London.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="http://qconsf2017intro.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLgqTum8MkspG_8VzGl6C07"></a></td>
          </tr>
          <tr>
            <td>Delivered November 16th, 2017 at QCON SF in San Francisco, CA.</td>
          </tr>
        
          <tr>
            <td>Deploying and scaling microservices with Docker and Kubernetes</td>
            <td><a href="http://osseu17.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLrsyydCzxWrIP_1-wkcSHS"></a></td>
          </tr>
          <tr>
            <td>Delivered October 26th, 2017 at Open Source Summit Europe in Prague.</td>
          </tr>
        
          <tr>
            <td>Deploying &amp; Scaling microservices with Docker Swarm</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=DABbqyJeG_E"></a></td>
          </tr>
          <tr>
            <td>Delivered July 25th, 2017 at devopsdays in Minneapolis, MN.</td>
          </tr>
        
          <tr>
            <td>Deploy and scale containers with Docker native, open source orchestration</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=EuzoEaE6Cqs"></a></td>
          </tr>
          <tr>
            <td>Delivered May 18th, 2017 at PyCon in Portland, OR.</td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Applications with Docker Swarm</td>
            <td><a href="http://lisa16t1.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviIDDhr8vIwCN1wkyNGXjbbc"></a></td>
          </tr>
          <tr>
            <td>Delivered December 6th, 2016 at LISA in Boston, MA.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and containers</td>
            <td><a href="https://us.pycon.org/2016/site_media/media/tutorial_handouts/DockerSlides.pdf"></a></td>
            <td><a href="https://www.youtube.com/watch?v=ZVaRK10HBjo"></a></td>
          </tr>
          <tr>
            <td>Delivered May 29th, 2016 at PyCon in Portland, OR.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Self-paced tutorials</td></tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="https://container.training/intro-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Container Orchestration with Docker and Swarm</td>
            <td><a href="https://container.training/swarm-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Microservices with Docker and Kubernetes</td>
            <td><a href="https://container.training/kube-selfpaced.yml.html"></a></td>
          </tr>
        
      

      

      <tr><td></td></tr>

      <tr>
        <td>
          Maintained by Jérôme Petazzoni (<a href="https://twitter.com/jpetazzo">@jpetazzo</a>) and <a href="https://github.com/jpetazzo/container.training/graphs/contributors">contributors</a>.
        </td>
      </tr>
    </tbody></table>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Use Pascal? (234 pts)]]></title>
            <link>https://castle-engine.io/why_pascal</link>
            <guid>36646890</guid>
            <pubDate>Sat, 08 Jul 2023 17:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://castle-engine.io/why_pascal">https://castle-engine.io/why_pascal</a>, See on <a href="https://news.ycombinator.com/item?id=36646890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="_modern_clean_language_to_develop_maintainable_applications">2.1. Modern clean language to develop maintainable applications</h3>
<div>
<ul>
<li>
<p>Object Pascal is a modern programming language. It supports classes, units, properties, generics, interfaces, reflection, closures…​ Everything you expect from a modern OOP language.</p>
</li>
<li>
<p>The syntax puts emphasis on readable code.</p>
</li>
<li>
<p>The language is type-safe. E.g. special types for booleans, strings, chars, sets, enums, ranges. Type conversions are either really safe, or have to be done explicitly.</p>
</li>
<li>
<p>There are additional run-time checks, e.g. array range checking, integer overflow checking, assertions, memory leak checking. Notes:</p>
<div>
<ul>
<li>
<p>You can turn off these checks in <em>release</em> version, but use them in <em>debug</em>. When compiling using CGE build tool / editor, we have debug / release modes that automatically do this for you.</p>
</li>
<li>
<p><a href="https://github.com/michaliskambi/modern-pascal-introduction/wiki/What-are-range-and-overflow-checks-(and-errors)-in-Pascal">What are range and overflow checks (and errors) in Pascal</a></p>
</li>
<li>
<p><a href="https://castle-engine.io/detecting_memory_leaks_using_heaptrc">Detecting Memory Leaks</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_fast">2.2. Fast</h3>
<div>
<ul>
<li>
<p>It is compiled to a native code and so is fast <em>"out of the box"</em>. There’s seldom any need to do low-level optimizations.</p>
</li>
<li>
<p>But if you need to, language can be as low-level as you want. E.g. you can use pointers, do pointer math, write OS and CPU-specific code, even add pieces in assembly. You can work on the same level as C or C++ does.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
But you will probably not need to get too "low level" in usual applications. E.g. <a href="https://castle-engine.io/">Castle Game Engine</a> has <strong>zero assembler code</strong> to maximize portability and code readability and we’re still fast.
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p>Compilation is also fast.</p>
<div>
<p><img src="https://castle-engine.io/images/not_resized/pascal-fast-compilation.webp" alt="pascal fast compilation">
</p>
</div>
<p>2.5 seconds to get desktop build, 10.1 seconds to get Android build <strong>of a new project, opened for the 1st time</strong>. Try to match that with your engine :)</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_cross_platform">2.3. Cross-platform</h3>
<div>
<ul>
<li>
<p>Desktop (Windows, Linux, macOS, Raspberry Pi, FreeBSD, probably every Unix…​),</p>
</li>
<li>
<p>mobile (Android, iOS),</p>
</li>
<li>
<p>consoles (Nintendo Switch, special in CGE),</p>
</li>
<li>
<p>web (both WebAssembly and JS (using pas2js)).</p>
</li>
</ul>
</div>

</div>
<div>
<h3 id="_welcoming">2.4. Welcoming</h3>
<div>
<ul>
<li>
<p>In <a href="https://castle-engine.io/">Castle Game Engine</a> case, engine code and game code are in the same language. Every user is contributor!</p>
</li>
<li>
<p>And the engine is open-source.</p>
</li>
</ul>
</div>
<p>Don’t hesitate to fork CGE to adjust it to your needs.</p>
</div>
<div>
<h3 id="_general_purpose">2.5. General purpose</h3>
<p>There are existing libraries (units) in Pascal for everything:</p>
<div>
<ul>
<li>
<p>database</p>
</li>
<li>
<p>XML, JSON</p>
</li>
<li>
<p>A.I.</p>
</li>
<li>
<p>blockchain</p>
</li>
<li>
<p>networking</p>
</li>
</ul>
</div>
<p>Moreover you can easily integrate with (link to) any existing library with C API. Any renderer, sound library, physics - we can use everything.</p>

</div>
<div>
<h3 id="_ecosystem_of_tools">2.6. Ecosystem of tools</h3>
<div>
<ul>
<li>
<p><a href="https://www.freepascal.org/">FPC</a> - Free Pascal Compiler, open-source.</p>
</li>
<li>
<p><a href="https://www.lazarus-ide.org/">Lazarus</a> - IDE for Pascal, on top of FPC, also open-source.</p>
</li>
<li>
<p><a href="https://www.embarcadero.com/products/Delphi">Delphi</a> - commercial compiler and IDE for Pascal.</p>
</li>
<li>
<p><a href="https://castle-engine.io/vscode">VS Code</a> support - CGE, as well as many others in the Pascal ecosystem, explicitly support integration with VS Code.</p>
</li>
</ul>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How and Why I Stopped Buying New Laptops (2020) (242 pts)]]></title>
            <link>https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</link>
            <guid>36646791</guid>
            <pubDate>Sat, 08 Jul 2023 17:47:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/">https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</a>, See on <a href="https://news.ycombinator.com/item?id=36646791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/X60-on-its-side-white_hu753fd43d283e60bf1aefb8ea5c1440fe_3642029_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<p>Being an independent journalist – or an office worker if you wish – I always reasoned that I needed a decent computer and that I need to pay for quality. Between 2000 and 2017, I consumed three laptops that I bought new and which cost me around 5,000 euros in total – roughly 300 euros per year over the entire period. The average useful life of my three laptops was 5.7 years.</p>
<p>In 2017, somewhere between getting <a href="https://solar.lowtechmagazine.com/2018/09/how-to-build-a-low-tech-website/">my office</a>, I decided not to buy any more new laptops. Instead, I switched to a 2006 second-hand machine that I purchased online for 50 euros and which does everything that I want and need. Including a new battery and a simple hardware upgrade, I invested less than 150 euros.</p>
<p>If my 2006 laptop lasts as long as my other machines – if it runs for another 1.7 years – it will have cost me only 26 euros per year. That’s more than 10 times less than the cost of my previous laptops. In this article, I explain my motivations for not buying new laptops, and how you could do the same.</p>
<h2 id="energy-and-material-use-of-a-laptop">Energy and material use of a laptop</h2>
<p>Not buying new laptops saves a lot of money, but also a lot of resources and environmental destruction. According to the most recent life cycle analysis, it takes 3,010 to 4,340 megajoules of primary energy to make a laptop – this includes mining the materials, manufacturing the machine, and bringing it to market. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>Each year, we purchase between 160 and 200 million laptops. Using the data above, this means that the production of laptops requires a yearly energy consumption of 480 to 868 petajoules, which corresponds to between one quarter and almost half of all solar PV energy produced worldwide in 2018 (2,023 petajoules). <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> The making of a laptop also involves a high material consumption, which includes a wide variety of minerals that may be considered scarce due to different types of constraints: economic, social, geochemical, and geopolitical. <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>The <a href="https://solar.lowtechmagazine.com/2009/06/the-monster-footprint-of-digital-technology/">production of microchips is a very energy- and material-intensive process</a>, but that is not the only problem. The high resource use of laptops is also because they have a very short lifespan. Most of the 160-200 million laptops sold each year are replacement purchases. The average laptop is replaced every 3 years (in business) to five years (elsewhere). <sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup> My 5.7 years per laptop experience is not exceptional.</p>
<h2 id="laptops-dont-change">Laptops don’t change</h2>
<p>The study cited dates from 2011, and it refers to a machine made in 2001: a Dell Inspiron 2500.  You are forgiven for thinking that this “most recent life cycle analysis of a laptop” is outdated, but it’s not. A 2015 research paper discovered that the embodied energy of laptops is static over time. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
<p>The scientists disassembled 11 laptops of similar size, made between 1999 and 2008, and weighed the different components. Also, they measured the silicon die area for all motherboards and 30 DRAM cards produced over roughly the same period (until 2011). They found that the mass and material composition of all key components – battery, motherboard, hard drive, memory – did not change significantly, even though manufacturing processes became more efficient in energy and material use.</p>
<p>The reason is simple: improvements in functionality balance the efficiency gains obtained in the manufacturing process. Battery mass, memory, and hard disk drive mass decreased per unit of functionality but showed roughly constant totals per year. The same dynamic explains why newer laptops don’t show lower operational electricity consumption compared to older laptops. New laptops may be more energy-efficient per computational power, but these gains are offset by more computational power. <a href="https://solar.lowtechmagazine.com/2018/01/bedazzled-by-energy-efficiency/">Jevon’s paradox</a> is nowhere as evident as it is in computing.</p>
<h2 id="the-challenge">The challenge</h2>
<p>All this means that there’s no environmental or financial benefit whatsoever to replacing an old laptop with a new one. On the contrary, the only thing a consumer can do to improve their laptop’s ecological and economic sustainability is to use it for as long as possible. This is facilitated by the fact that laptops are now a mature technology and have more than sufficient computational power. One problem, though. Consumers who try to keep working on their old laptops are likely to end up frustrated. I shortly explain my frustrations below, and I’m pretty confident that they are not exceptional.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The three new laptops I used from 2000 to 2017." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/3-laptops-white_hud91339aa34e3ccb38ac1a26db4506f2c_3604453_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The three new laptops I used from 2000 to 2017. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-first-laptop-apple-ibook-2000-2005">My first laptop: Apple iBook (2000-2005)</h2>
<p>In 2000, when I was working as a freelance science and tech journalist in Belgium, I bought my first laptop, an Apple iBook. Little more than two or three years later, the charger started malfunctioning. When informed of the price for a new charger, I was so disgusted with Apple’s sales practices – chargers are very cheap to produce, but Apple sold them for a lot of money – that I refused to buy it. Instead, I managed to keep the charger working for a few more years, first by putting it under the weight of books and furniture, and when that didn’t work anymore, by putting it in a firmly tightened clamp.</p>
<h2 id="my-second-laptop-ibm-thinkpad-r52-2005-2013">My second laptop: IBM ThinkPad R52 (2005-2013)</h2>
<p>When the charger eventually died entirely in 2005, I decided to look for a new laptop. I had only one demand: it should have a charger that lasts or is at least cheap to replace. I found more than I was looking for. I bought an <a href="http://www.thinkwiki.org/wiki/Category:R52" target="_blank">IBM Thinkpad R52</a>, and it was love at first use. My IBM laptop was the Apple iBook counterpart, not just in terms of design (a rectangular box available in all colours as long as it’s black). More importantly, the entire machine was built to last, built to be reliable, and built to be repairable.</p>
<p><a href="https://solar.lowtechmagazine.com/2019/06/how-to-make-wind-power-sustainable-again/">Circular and modular products are all the hype these days</a>, its lifetime could be extended endlessly by gradually repairing and replacing every part that it consists of. The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
<blockquote>
<p>The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
</blockquote>
<p>My Thinkpad was more expensive to buy than my iBook, but at least I didn’t spend all that money on a cute design but a decent computer. The charger gave no problems, and when I lost it during a trip and had to buy a new one, I could do so for a fair price. Little did I know that my happy purchase was going to be a once-in-a-lifetime experience.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The IBM ThinkPad R52 from 2005." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/Thinkpad-r52-white_hu02c8b35b61eabe045fd7fdd9f38bf53d_4952865_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The IBM ThinkPad R52 from 2005. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-third-laptop-lenovo-thinkpad-t430-2013-2017">My third laptop: Lenovo Thinkpad T430 (2013-2017)</h2>
<p>Fast forward to 2013. I am now living in Spain and I’m running Low-tech Magazine. I’m still working on my IBM Thinkpad R52, but there are some problems on the horizon. First of all, Microsoft will soon force me to upgrade my operating system, because support for Windows XP is to end in 2014. I don’t feel like spending a couple of hundred euros on a new operating system that would be too demanding for my old laptop anyway. Furthermore, the laptop had gotten a bit slow, even after it had been restored to its factory settings. In short, I fell into the trap that the hardware and software industries have set up for us and made the mistake of thinking that I needed a new laptop.</p>
<p>Having been so fond of my Thinkpad, it was only logical to get a new one. Here’s the problem: in 2005, shortly after I had bought my first Thinkpad, Lenovo, a Chinese manufacturer that is now the largest computer maker in the world, bought IBM’s PC business. Chinese companies don’t have a reputation for building quality products, especially not at the time. However, since Lenovo was still selling Thinkpads that looked almost identical to those built by IBM, I decided to try my luck and bought a <a href="http://www.thinkwiki.org/wiki/Category:T430" target="_blank">Lenovo Thinkpad T430</a> in April 2013. At a steep price, but I assumed that quality had to be paid for.</p>
<p>My mistake was clear from the beginning. I had to send the new laptop back twice because its case was deformed. When I finally got one that didn’t wobble on my desk, I quickly ran into another problem: the keys started breaking off. I can still remember my disbelief when it happened for the first time. The IBM Thinkpad is known for its robust keyboard. If you want to break it, you need a hammer. Lenovo obviously didn’t find that so important and had quietly replaced the keyboard with an inferior one. Mind you, I can be an aggressive typist, but I have never broken any other keyboard.</p>
<p>I grumpily ordered a replacement key for 15 euros. In the months after that, replacement keys became a recurring cost. After spending more than 100 euros on plastic keys, which would soon break again, I calculated that my keyboard had 90 keys and that replacing them all just once would cost me 1,350 euros. I stopped using the keyboard altogether, temporarily finding a solution in an external keyboard. However, this was impractical, especially for working away from home – and why else would I want a laptop?</p>
<p>There was no getting around it anymore: I needed a new laptop. Again. But which one? For sure it would not be one made by Lenovo or Apple.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/broken-keyboard-white_huaf323a230a1b423848610266c7325189_7786692_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros.  <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-fourth-laptop-ibm-thinkpad-x60s-2017-now">My fourth laptop: IBM Thinkpad X60s (2017-now)</h2>
<p>Not finding what I was looking for, I decided to go back in time. By now, it had dawned on me that new laptops are of inferior quality compared to older laptops, even if they carry a much higher price tag.  I found out that Lenovo switched keyboards around 2011 and started searching auction sites for Thinkpads built before that year. I could have changed back to my ThinkPad R52 from 2005, but by now, I had become accustomed to a Spanish keyboard, and the R52 had a Belgian one.</p>
<p>In April 2017, I settled on a used <a href="http://www.thinkwiki.org/wiki/Category:X60s" target="_blank">Thinkpad X60s</a> from 2006. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> As of December 2020, the machine is in operation for almost 4 years and is 14 years old – three to five times older than the average laptop. If I loved my Thinkpad R52 from 2005, I adore my Thinkpad X60s from 2006. It’s just as sturdily built – it already survived a drop from a table on a concrete floor – but it’s much smaller and also lighter: 1.43 kg vs. 3.2 kg.</p>
<p>My 2006 Thinkpad X60s does everything I want it to do. I use it to write articles, do research, and maintain the websites. I have also used it on-stage to give lectures, projecting images on a large screen. There’s only one thing missing on my laptop, especially nowadays, and that’s a webcam. I solve this by firing up the cursed 2013 laptop with the broken keys whenever I need to, happy to give it some use that doesn’t involve its keyboard. It could also be solved by a switch to the <a href="http://www.thinkwiki.org/wiki/Category:X200" target="_blank">Thinkpad X200</a> from 2008, which is a newer version of the same model and has a webcam.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-x60s-white_hu30a06554e31ecbc7d8a684e77fe1da4d_3965389_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="how-to-make-an-old-laptop-run-like-its-new">How to make an old laptop run like it’s new</h2>
<p>Not buying any more new laptops is not as simple as buying a used laptop. It’s advisable to upgrade the hardware, and it’s essential to downgrade the software. There are two things you need to do:</p>
<h2 id="1-use-low-energy-software">1. Use low energy software</h2>
<p>My laptop runs on <a href="https://www.linuxliteos.com/" target="_blank">Linux Lite</a>, one of several open-source operating systems <a href="https://lotoftech.com/10-best-lightweight-operating-system-for-old-computers/" target="_blank">specially designed to work on old computers</a>. The use of a Linux operating system is not a mere suggestion. There’s no way you’re going to revive an old laptop if you stick to Microsoft Windows or Apple OS because the machine would freeze instantly. Linux Lite does not have the flashy visuals of the newest Apple and Windows interfaces, but it has a familiar graphical interface and looks anything but obsolete. It takes very little space on the hard disk and demands even less computing power. The result is that an old laptop, despite its limited specifications, runs smoothly. I also use light browsers: <a href="https://vivaldi.com/" target="_blank">Vivaldi</a> and <a href="https://astian.org/en/midori-browser/" target="_blank">Midori</a>.</p>
<p>Having used Microsoft Windows for a long time, I find Linux operating systems to be remarkably better, even more so because they are free to download and install. Furthermore, Linux operating systems do not steal your personal data and do not try to lock you in, like the newest operating systems from both Microsoft and Apple do. That said, even with Linux, obsolescence cannot be ruled out. For example, Linux Lite will stop its support for 32-bit computers in 2021, which means that I will soon have to look for an alternative operating system, or buy a slightly younger 64-bit laptop.</p>
<h2 id="2-replace-the-hard-disk-drive-with-a-solid-state-drive">2. Replace the hard disk drive with a solid-state drive</h2>
<p>In recent years, solid-state drives (SSD) have become available and affordable, and they are much faster than hard disk drives (HDD). Although you can revive an old laptop by merely switching to a light-weight operating system, if you also replace the hard disk drive with a solid-state drive, you’ll have a machine that is just as fast as a brand new laptop. Depending on the storage capacity you want, an SSD will cost you between 20 euro (120 GB) and 100 euro (960 GB).</p>
<p>Installment is pretty straightforward and well documented online. Solid-state drives run silently and are more resistant to physical shock, but they have a shorter life expectancy than hard disk drives. Mine is now working for almost 4 years. It seems that both from an environmental and financial viewpoint, an old laptop with SSD is a much better choice than buying a new laptop, even if the solid-state drive needs replacement now and then.</p>
<h2 id="spare-laptops">Spare laptops</h2>
<p>Meanwhile, my strategy has evolved. I have bought two identical models for a similar price, in 2018 and early 2020, to use as spare laptops. Now I plan to keep working on these machines for as long as possible, having more than sufficient spare parts available. Since I bought the laptop, it had two technical issues. After roughly a year of use, the fan died. I had it repaired overnight in a tiny and messy IT shop run by a Chinese man in Antwerp, Belgium. He said that my patched fan would run for another six months, but it’s still working more than two years later.</p>
<p>Then, last year, my X60s suddenly refused to charge its battery, an issue that had also appeared with my cursed 2013 laptop. It seems to be a common problem with Thinkpads, but I could not solve it yet. Neither did I really have to because I had a spare laptop ready and started using that one whenever I needed or wanted to work outside.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Three identical 2006 laptops, all in working order, for less than 200 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-white_hu3766708df1a6ba8bdbeb0add86f0194b_6142617_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Three identical 2006 laptops, all in working order, for less than 200 euros. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<div>
<figure data-imgstate="dither">
<img alt="Image: Inside the Thinkpad X60s. Source: Hardware Maintenance Manual." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-inside_huad20844be5ac8ba7ef947df62b823a02_178972_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png"> </figure>

</div>
<h2 id="the-magical-sd-card">The magical SD-card</h2>
<p>Now to introduce you to my magical SD-card, which is another hardware upgrade that facilitates the use of old (but also new) laptops. Many people have their personal documents stored on their laptop’s hard drive and then make backups to external storage media if all goes well. I do it the other way around.</p>
<p>I have all my data on a 128 GB SD-card, which I can plug into any of the Thinkpads that I own. I then make monthly backups of the SD-card, which I store on an external storage medium, as well as regular backups of the documents that I am working on, which I temporarily store on the drive of the laptop that I am working on. This has proven to be very reliable, at least for me: I have stopped losing work due to computer problems and insufficient backups.</p>
<p>The other advantage is that I can work on any laptop that I want and that I’m not dependent on a particular machine to access my work. You can get similar advantages when you keep all your data in the cloud, but the SD-card is <a href="https://solar.lowtechmagazine.com/2015/10/why-we-need-a-speed-limit-for-the-internet/">the more sustainable option</a>, and it works without internet access.</p>
<p>Hypothetically, I could have up to two hard drive failures in one day and keep working as if nothing happened. Since I am now using both laptops alternately – one with battery, the other one without – I can also leave them at different locations and cycle between these places while carrying only the SD-card in my wallet. Try that with your brand new, expensive laptop. I can also use my laptops together if I need an extra screen.</p>
<p>In combination with a hard disk drive, the SD-card also increases the performance of an old laptop and can be an alternative to installing a solid-state drive. My spare laptop does not have one and it can be slow when browsing heavy-weight websites. However, thanks to the SD-card, opening a map or document happens almost instantly, as does scrolling through a document or saving it. The SD-card also keeps the hard disk running smoothly because it’s mostly empty. I don’t know how practical using an SD-card is for other laptops, but all my Thinkpads have a slot for them.</p>
<h2 id="the-costs">The costs</h2>
<p>Let’s make a complete cost calculation, including the investment in spare laptops and SD-card, and using today’s prices for both solid-state drives and SD-cards, which have become much cheaper since I have bought them:</p>
<ul>
<li>ThinkPad X60s: 50 euro</li>
<li>ThinkPad X60s spare laptop: 60 euro</li>
<li>ThinkPad X60 spare laptop: 75 euro</li>
<li>Two replacement batteries: 50 euro</li>
<li>240 GB solid-state drive: 30 euro</li>
<li>128 GB SD-card: 20 euro</li>
<li>Total: 285 euros</li>
</ul>
<p>Even if you buy all of this, you only spent 285 euros. For that price, you may be able to buy the crappiest new laptop on the market, but it surely won’t get you two spare laptops. If you manage to keep working with this lot for ten years, your laptop costs would be 28.5 euros per year. You may have to replace a few solid-state drives and SD-cards, but it won’t make much difference. Furthermore, you save the ecological damage that is caused by the production of a new laptop every 5.7 years.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My laptop needs are met for the foreseeable future." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-2-white_hu2dc6db9faa5724cc3c1faebbecf747bd_5522562_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My laptop needs are met for the foreseeable future. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="dont-take-it-too-far">Don’t take it too far</h2>
<p>Although I have used my Thinkpad X60s as an example, the same strategy works with other Thinkpad models – <a href="http://www.thinkwiki.org/wiki/ThinkPad_History" target="_blank">here’s an overview of all historical models</a> – and laptops from other brands (which I know nothing about). If you prefer not to buy on auction sites, you can walk to the nearest pawnshop and get a used laptop with a guarantee. The chances are that you don’t even need to buy anything, as many people have old laptops lying around.</p>
<p>There’s no need to go back to a 2006 machine. I hope it’s clear that I am trying to make a statement here, and I probably went as far back as one can while keeping things practical. My first try was a used ThinkPad X30 from 2002, but that was one step too far. It uses a different charger type, it has no SD-card slot, and I could not get the wireless internet connection working. For many people, it may serve to choose a somewhat younger laptop. That will give you a webcam and a 64-bit architecture, which makes things easier. Of course, you can also try to beat me and go back to the 1990s, but then you’ll have to do without USB and wireless internet connection.</p>
<p>Your choice of laptop also depends on what you want to do with it. If you use it mainly for writing, surfing the web, communication, and entertainment, you can do it as cheaply as I did. If you do graphical or audiovisual work, it’s more complicated, because in that case, you’re probably an Apple user. The same strategy could be applied, on a somewhat younger and more expensive laptop, but it would suggest switching from a Mac to a Linux operating system. When it comes to office applications, Linux is clearly better than its commercial alternatives. For a lack of experience, I cannot tell you if that holds for other software as well.</p>
<h2 id="this-is-a-hack-not-a-new-economical-model">This is a hack, not a new economical model</h2>
<p>Although capitalism could provide us with used laptops for decades to come, the strategy outlined above should be considered a hack, not an economical model. It’s a way to deal with or escape from an economic system that tries to force you and me to consume as much as possible. It’s an attempt to break that system, but it’s not a solution in itself. We need another economical model, in which we build all laptops like pre-2011 Thinkpads. As a consequence, laptop sales would go down, but that’s precisely what we need. Furthermore, with today’s computing efficiency, we could significantly reduce the operational and embodied energy use of a laptop if we reversed the trend towards ever higher functionality.</p>
<p>Significantly, hardware and software changes drive the fast obsolescence of computers, but the latter has now become the most crucial factor. A computer of 15 years old has all the hardware you need, but it’s not compatible with the newest (commercial) software. This is true for operating systems and every type of software, from games to office applications to websites. Consequently, to make laptop use more sustainable, the software industry would need to start making every new version of its products lighter instead of heavier. The lighter the software, the longer our laptops will last, and we will need less energy to use and produce them.</p>
<p>Kris De Decker</p>
<p>Images: Jordi Manrique Corominas, Adriana Parra, Roel Roscam Abbing</p>
<p>Proofreading: Eric Wagner</p>
</div>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An ARM Assembler Written in Lisp (130 pts)]]></title>
            <link>http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</link>
            <guid>36646277</guid>
            <pubDate>Sat, 08 Jul 2023 16:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237">http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</a>, See on <a href="https://news.ycombinator.com/item?id=36646277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>You can write machine-code functions in uLisp with the help of the ARM assembler written in Lisp, and I’ve recently updated it to make it more compact. It will now fit on a board with about 2000 objects of workspace, with room to spare to write assembler programs and run them.</p>
<p>This post describes how the latest version of the ARM assembler works. The aim is to help anyone who wants to extend the assembler to cater for ARM instructions that it doesn’t currently support. It will also be helpful if you want to write an assembler for another processor, or even design your own processor and write an assembler for it; Lisp is an excellent language to do this. For example, a printout of the whole ARM assembler fits on two A4 pages.</p>
<h3>Instruction encodings</h3>
<p>The starting point for writing an assembler is to get hold of a summary of the processor’s table of instruction encodings. For the ARM Thumb instruction set these are as follows:</p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x0</span> to <span>#x8</span>.</em></p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x9</span> to <span>#xF</span>.</em></p>
<p>You can see from these diagrams that the 16-bit instructions are arranged into consistent field patterns. This is true of most processor instruction sets, but some are more orderly than others (RISC-V is a nightmare!).</p>
<h4>An example - LSL</h4>
<p>As an example, consider the first instruction in the first table, LSL immediate:</p>

<p>This consists of:</p>
<ul>
<li>The four-bit value <span>#b0000</span>.</li>
<li>A one-bit <em>op</em> code, which is 0 for LSL and 1 for LSR.</li>
<li>An <em>immed5</em> value, which is a 5-bit integer from 0 to 31 giving the size of the left shift.</li>
<li>
<em>Lm</em>, which is a value from 0 to 7 representing the source register R0 to R7.</li>
<li>
<em>Ld</em>, which is a value from 0 to 7 representing the destination register R0 to R7.</li>
</ul>
<h3>Emitting bit fields</h3>
<p>The first function we need is <strong>emit</strong>, which takes a specification defining the widths of the bit fields, and a list of arguments, and packs the values of the arguments into the bit fields:</p>
<pre><code>(defun emit (bits &amp;rest args)
  (let ((word 0) (shift -28))
    (mapc #'(lambda (value)
              (let ((width (logand (ash bits shift) #xf)))
                (incf shift 4)
                (unless (zerop (ash value (- width))) (error "Won't fit"))
                (setq word (logior (ash word width) value))))
          args)
    word))
</code></pre>
<p>The first argument, <strong>bits</strong>, is a 32-bit hexadecimal number in which each hex digit specifies the width of the next bit field. The function <strong>emit</strong> reads the hex digits in <strong>bits</strong> from left to right, packs the appropriate number of bits from each argument into <strong>word</strong>, and then returns the result.</p>
<p>For example, the bit fields for the LSL instruction could be specified by:</p>
<pre><code>#x41533000
</code></pre>
<p>To make it easier to process the bit fields the widths are left-aligned, so you should add zeros to make the <strong>bits</strong> parameter eight hex digits.</p>
<p>The remaining arguments are the values to be packed into the bit fields. If any argument won’t fit into the corresponding bit field the error <strong>Won’t fit</strong> will be displayed.</p>
<p>So for example, to emit the op code for the instruction:</p>
<pre><code>LSL r7, r4, #31
</code></pre>
<p>evaluate:</p>
<pre><code>&gt; (emit #x41533000 0 0 31 4 7)
2023
</code></pre>
<p>If you print this as a 16-bit binary number with:</p>
<pre><code>&gt; (format t "~16,'0b" 2023)
0000011111100111
</code></pre>
<p>you can see that the values have been put into the correct fields as required.</p>
<h3>Specifying registers</h3>
<p>The next step is to be able to specify registers as r0 to r15, or their synonyms <strong>sp</strong> (for r13), <strong>lr</strong> (for r14), and <strong>pc</strong> (for r15). This is handled by the function <strong>regno</strong>:</p>
<pre><code>(defun regno (sym)
  (case sym (sp 13) (lr 14) (pc 15)
    (t (read-from-string (subseq (string sym) 1)))))
</code></pre>
<p>For example:</p>
<pre><code>&gt; (regno 'r12)
12
</code></pre>
<p>Finally, we can now define the LSL instruction as the convenient Lisp function <strong>$lsl</strong> as follows:</p>
<pre><code>(defun $lsl (argd argm immed5)
  (emit #x41533000 0 0 immed5 (regno argm) (regno argd))
</code></pre>
<p>This allows us to specify the instruction using syntax that’s close to ARM assembler syntax:</p>
<pre><code>&gt; ($lsl 'r7 'r4 31)
2023
</code></pre>
<p>I’ve used the convention that functions representing ARM instructions are prefixed by a $ sign; otherwise there would be a problem with instructions that are also existing Lisp functions, such as <strong>push</strong> and <strong>pop</strong>.</p>
<h3>Handling addressing modes</h3>
<p>The final complication is that some instruction mnemonics can generate different op codes, depending on the types of their arguments.</p>
<p>For example, there’s also a variant of LSL that shifts a register Rd by the shift value specified in the register Rs:</p>

<p>Using this syntax, the following assembler instruction shifts the value in R7 by the value in R1:</p>
<pre><code>LSL r7, r1
</code></pre>
<p>The block of register-to-register instructions that include LSL is handled by the routine <strong>reg-reg</strong>:</p>
<pre><code>(defun reg-reg (op argd argm)
  (emit #xa3300000 op (regno argm) (regno argd)))
</code></pre>
<p>Finally, we need to modify <strong>$lsl</strong> to include the register-to-register variant:</p>
<pre><code>(defun $lsl (argd argm &amp;optional arg2)
  (cond
   ((numberp arg2)
    (lsl-lsr-0 0 arg2 argm argd))
   ((numberp argm)
    (lsl-lsr-0 0 argm argd argd))
   (t
    (reg-reg #b0100000010 argd argm))))
</code></pre>
<p>where <strong>lsl-lsr-0</strong> is defined as:</p>
<pre><code>(defun lsl-lsr-0 (op immed5 argm argd)
  (emit #x41533000 0 op immed5 (regno argm) (regno argd)))
</code></pre>
<p>This expanded version of <strong>$lsl</strong> also handles the two-argument case where the source and destination registers are the same in an immediate shift; for example:</p>
<pre><code>($lsl 'r1 31)
</code></pre>
<h3>Running the assembler</h3>
<p>To run the assembler in uLisp you use the built-in command <strong>defcode</strong>, which generates an assembler listing, and puts the machine code into RAM so you can execute it as if it’s a normal Lisp function.</p>
<h4>Greatest Common Divisor example</h4>
<p>For example, to assemble a machine-code routine <strong>gcd</strong> to calculate Greatest Common Divisor you’d evaluate:</p>
<pre><code>; Greatest Common Divisor
(defcode gcd (x y)
  swap
  ($mov 'r2 'r1)
  ($mov 'r1 'r0)
  again
  ($mov 'r0 'r2)
  ($sub 'r2 'r2 'r1)
  ($blt swap)
  ($bne again)
  ($bx 'lr))
</code></pre>
<p>and you could then call:</p>
<pre><code>&gt; (gcd 3287 3460)
173
</code></pre>
<h3>Running the assembler in Common Lisp</h3>
<p>You can also run the ARM assembler in a standard Common Lisp implementation. The Common Lisp version of the ARM Assembler includes the following <strong>defcode</strong> macro that lets you assemble an ARM function and print the machine code, like the <strong>defcode</strong> special form built into uLisp:</p>
<pre><code>(defparameter *pc* 0)

(defmacro defcode (&amp;body body)
  (let ((*print-pretty* t) (assembler (cddr body)))
    (dotimes (pass 2)
      (setq *pc* 0)
      (mapc
       #'(lambda (ins)
           (cond
            ((atom ins)
             (unless (zerop pass) (format t "~4,'0x      ~(~a~)~%" *pc* ins))
             (set ins *pc*))
            ((listp (eval ins))
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (first (eval ins)) ins)
               (format t "~4,'0x ~4,'0x~%" (+ *pc* 2) (second (eval ins))))
             (incf *pc* 4))
            (t
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (eval ins) ins))
             (incf *pc* 2))))
       assembler)
      nil)))
</code></pre>
<p>Evaluating the <strong>Greatest Common Divisor example</strong> above generates the following output:</p>
<pre><code>0000      swap
0000 000A ($mov 'r2 'r1)
0002 0001 ($mov 'r1 'r0)
0004      again
0004 0010 ($mov 'r0 'r2)
0006 1A52 ($sub 'r2 'r2 'r1)
0008 DBFA ($blt swap)
000A D1FB ($bne again)
000C 4770 ($bx 'lr)
</code></pre>
<p>In this case you obviously won’t be able to run the machine code.</p>
<h3>Resources</h3>
<p>For both versions of the assembler see: <a href="https://github.com/technoblogy/lisp-arm-assembler">https://github.com/technoblogy/lisp-arm-assembler</a>.</p>
<p>For more information see <a href="http://www.ulisp.com/show?2YRU">ARM assembler overview</a>.</p>
<p>For a list of the ARM Thumb instructions supported by the assembler see: <a href="http://www.ulisp.com/show?30B8">ARM assembler instructions</a>.</p>
<p>For ARM assembler examples see: <a href="http://www.ulisp.com/show?30BD">ARM assembler examples</a>.</p>
<h3>Update</h3>
<p>6th July 2023: Updated the <strong>defcode</strong> macro to handle forward references.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse-engineering the 8086 processor's address and data pin circuits (109 pts)]]></title>
            <link>https://www.righto.com/2023/07/8086-pins.html</link>
            <guid>36645821</guid>
            <pubDate>Sat, 08 Jul 2023 16:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2023/07/8086-pins.html">https://www.righto.com/2023/07/8086-pins.html</a>, See on <a href="https://news.ycombinator.com/item?id=36645821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2795813864611579036" itemprop="description articleBody">


<p>The Intel 8086 microprocessor (1978) started the x86 architecture that continues to this day.
In this blog post, I'm focusing on a small part of the chip: the address and data pins that connect the chip to
external memory and I/O devices.
In many processors, this circuitry is straightforward, but it is complicated in the 8086 for two reasons.
First, Intel decided to package the 8086 as a 40-pin DIP, which didn't provide enough pins for all the functionality.
Instead, the 8086 multiplexes address, data, and status.
In other words, a pin can have multiple roles, providing an address bit at one time and a data bit at another time.</p>
<p>The second complication is that the 8086 has a 20-bit address space (due to its infamous segment registers), while the
data bus is 16 bits wide.
As will be seen, the "extra" four address bits have more impact than you might expect.
To summarize, 16 pins, called AD0-AD15, provide 16 bits of address and data.
The four remaining address pins (A16-A19) are multiplexed for use as status pins,
providing information about what the processor is doing for use by other parts of the system.
You might expect that the 8086 would thus have two types of pin circuits, but it turns out that there are four
distinct circuits, which I will discuss below.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/die-labeled.jpg"><img alt="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." height="623" src="https://static.righto.com/images/8086-ad-pins/die-labeled-w700.jpg" title="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." width="700"></a></p><p>The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version.</p>
<p>The microscope image above shows the silicon die of the 8086.
In this image, the metal layer on top of the chip is visible, while the silicon and polysilicon underneath are obscured.
The square pads around the edge of the die are connected by tiny bond wires to the chip's 40 external pins.
The 20 address pins are labeled: Pins AD0 through AD15 function as
address and data pins. Pins A16 through A19 function as address pins and status pins.<span id="fnref:ad"><a href="#fn:ad">1</a></span>
The circuitry that controls the pins is highlighted in red.
Two internal busses are important for this discussion: the 20-bit AD bus (green) connects the AD pins to the rest of the CPU,
while the 16-bit C bus (blue) communicates with the registers.
These buses are connected through a circuit that can swap the byte order or shift the value.
(The lines on the diagram are simplified; the real wiring twists and turns to fit the layout.
Moreover, the C bus (blue) has its bits spread across the width of the register file.)</p>
<h2>Segment addressing in the 8086</h2>
<p>One goal of the 8086 design was to maintain backward compatibility with the earlier 8080 processor.<span id="fnref:compatibility"><a href="#fn:compatibility">2</a></span>
This had a major impact on the 8086's memory design, resulting in the much-hated segment registers.
The 8080 (like most of the 8-bit processors of the early 1970s) had a 16-bit address space, able to access 64K (65,536 bytes) of memory,
which was plenty at the time.
But due to the exponential growth in memory capacity described by Moore's Law, it was clear that the 8086 needed to
support much more. Intel decided on a 1-megabyte address space, requiring 20 address bits.
But Intel wanted to keep the 16-bit memory addresses used by the 8080.</p>
<p>The solution was to break memory into segments. Each segment was 64K long, so a 16-bit offset was sufficient to access memory
in a segment.
The segments were allocated in a 1-megabyte address space, with the result that you could access a megabyte of memory, but
only in 64K chunks.<span id="fnref:pointers"><a href="#fn:pointers">3</a></span>
Segment addresses were also 16 bits, but were shifted left by 4 bits (multiplied by 16) to support the 20-bit address space.</p>
<p>Thus, every memory access in the 8086 required a computation of the physical address.
The diagram below illustrates this process: the logical address consists of the segment base address and the offset within the segment.
The 16-bit segment register was shifted 4 bits and added to the 16-bit offset to yield the 20-bit physical memory address.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/physical-address-generation.jpg"><img alt="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." height="260" src="https://static.righto.com/images/8086-ad-pins/physical-address-generation-w500.jpg" title="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." width="500"></a></p><p>The segment register and the offset are added to create a 20-bit physical address.  From <a href="http://www.bitsavers.org/components/intel/_dataBooks/1981_iAPX_86_88_Users_Manual.pdf">iAPX 86,88 User's Manual</a>, page 2-13.</p>
<p>This address computation was not performed by the regular ALU (Arithmetic/Logic Unit), but by a separate adder that
was devoted to address computation.
The address adder is visible in the upper-left corner of the die photo.
I will discuss the address adder in more detail below.</p>
<h2>The AD bus and the C Bus</h2>
<p>The 8086 has multiple internal buses to move bits internally, but the relevant ones are the AD bus and the C bus.
The AD bus is a 20-bit bus that connects the 20 address/data pins to the internal circuitry.<span id="fnref:patent"><a href="#fn:patent">4</a></span>
A 16-bit bus called the C bus provides the connection between
the AD bus, the address adder and some of the registers.<span id="fnref:registers"><a href="#fn:registers">5</a></span>
The diagram below shows the connections.
The AD bus can be connected to the 20 address pins through latches. The low 16 pins can also be used for data input, while the upper 4 pins
can also be used for status output.
The address adder performs the 16-bit addition necessary for segment arithmetic. Its output is shifted left by four bits
(i.e. it has four 0 bits appended), producing the 20-bit result.
The inputs to the adder are provided by registers, a constant ROM that holds small constants such as +1 or -2, or the C bus.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/buses.jpg"><img alt="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." height="311" src="https://static.righto.com/images/8086-ad-pins/buses-w350.jpg" title="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." width="350"></a></p><p>My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins.</p>
<p>The shift/crossover circuit provides the interface between these two buses, handling the 20-bit to 16-bit conversion. The busses can be connected in three ways: direct, crossover, or shifted.<span id="fnref:swapping"><a href="#fn:swapping">6</a></span>
The direct mode connects the 16 bits of the C bus to the lower 16 bits of the address/data pins.
This is the standard mode for transferring data between the 8086's internal circuitry and the data pins.
The crossover mode performs the same connection but swaps the bytes. This is typically used for unaligned memory accesses, where the low memory byte corresponds to
the high register byte, or vice versa.
The shifted mode shifts the 20-bit AD bus value four positions to the right.
In this mode, the 16-bit output from the address adder goes to the 16-bit C bus.
(The shift is necessary to counteract the 4-bit shift applied to the address adder's output.)
Control circuitry selects the right operation for the shift/crossover circuit at the right time.<span id="fnref:shift"><a href="#fn:shift">7</a></span></p>
<p>Two of the registers are invisible to the programmer but play an important role in memory accesses.
The <code>IND</code> (Indirect) register specifies the memory address; it holds the 16-bit memory offset in a segment.
The <code>OPR</code> (Operand) register holds the data value.<span id="fnref:prefetch"><a href="#fn:prefetch">9</a></span>
The <code>IND</code> and <code>OPR</code> registers are not accessed directly by the programmer; the microcode for a machine instruction moves the appropriate
values to these registers prior to the write.</p>
<h2>Overview of a write cycle</h2>
<p>I hesitate to present a timing diagram, since I may scare off of my readers,
but the 8086's communication is designed around a four-step bus cycle.
The diagram below shows simplified timing for a write cycle, when the 8086 writes to memory or an I/O device.<span id="fnref:timing"><a href="#fn:timing">8</a></span>
The external bus activity is organized as four states, each one clock cycle long: T1, T2, T3, T4.
These T states are very important since they control what happens on the bus.
During T1, the 8086 outputs the address on the pins. During the T2, T3, and T4 states, the 8086 outputs the data word on the pins.
The important part for this discussion is that the pins are multiplexed depending on the T-state: the pins provide the address during T1 and data during
T2 through T4.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/write-cycle.jpg"><img alt="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="130" src="https://static.righto.com/images/8086-ad-pins/write-cycle-w700.jpg" title="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="700"></a></p><p>A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>There two undocumented T states that are important to the bus cycle.
The physical address is computed in the two clock cycles before T1 so the address will be available in T1.
I give these "invisible" T states the names TS (start) and T0.</p>
<h2>The address adder</h2>
<!--
This computation is a bit tricky because the input buses and the adder are 16 bits, but the physical address is 20 bits.
-->
<p>The operation of the address adder is a bit tricky since the 16-bit adder must generate a 20-bit physical address.
The adder has two 16-bit inputs: the B input is connected to the upper registers via the B bus, while the C input is connected to the C bus.
The segment register value is transferred over the B bus to the adder during the second half
of the TS state (that is, two clock cycles before the bus cycle becomes externally visible during T1).
Meanwhile, the address offset is transferred over the C bus to the adder, but the adder's C input shifts the value four bits to the right,
discarding the four low bits. (As will be explained later, the pin driver circuits latch these bits.)
The adder's output is shifted left four bits and transferred to the AD bus during the second half of T0. 
This produces the upper 16 bits of the 20-bit physical memory address.
This value is latched into the address output flip-flops at the start of T1, putting the computed address on the pins.
To summarize, the 20-bit address is generated by storing the 4 low-order bits during T0 and then the 16 high-order sum bits
during T1.</p>
<p>The address adder is not needed for segment arithmetic during T1 and T2.
To improve performance, the 8086 uses the adder during this idle time to increment or decrement memory addresses.
For instance, after popping a word from the stack, the stack pointer needs to be incremented by 2.
The address adder can do this increment "for free" during T1 and T2, leaving the ALU available for other operations.<span id="fnref:pipelining"><a href="#fn:pipelining">10</a></span>
Specifically, the adder updates the memory address in <code>IND</code>, incrementing it or decrementing it as appropriate.
First, the <code>IND</code> value is transferred over the B bus to the adder during the second half of T1.
Meanwhile, a constant (-3 to +2) is loaded from the Constant ROM and transferred to the adder's C input.
The output from the adder is transferred to the AD bus during the second half of T2.
As before, the output is shifted four bits to the left. However, the shift/crossover circuit between the AD bus and the C bus
is configured to shift four bits to the right, canceling the adder's shift.
The result is that the C bus gets the 16-bit sum from the adder, and this value is stored in the <code>IND</code> register.<span id="fnref:predecrement"><a href="#fn:predecrement">11</a></span>
For more information on the implemenation of the address adder, see my <a href="https://www.righto.com/2020/08/reverse-engineering-adder-inside-intel.html">previous blog post</a>.</p>
<!-- 
The use of the address pins is closely tied to the 8086's external timing.
The diagram below shows how a typical bus cycle is divided into four "T" states, each one corresponding to one clock cycle.
During T1, the CPU puts the memory address on the bus using the address pins.
During T3 and T4, the CPU writes to memory by putting the data value on the data pins.
Alternatively, the CPU reads from memory by reading the data value during T3 and T4.
State T2 acts a buffer period to ensure that memory and the CPU don't try to write to the bus at the same time.

![A typical bus cycle consists of four T states. Diagram from The 8086 Family Users Manual, figure 4-5.](bus-cycle.jpg "w500")
-->

<h2>The pin driver circuit</h2>
<p>Now I'll dive down to the hardware implementation of an output pin.
When the 8086 chip communicates with the outside world, it needs to provide relatively high currents.
The tiny logic transistors can't provide enough current, so the chip needs to use large output transistors.
To fit the large output transistors on the die, they are constructed of multiple wide transistors in parallel.<span id="fnref:ratio"><a href="#fn:ratio">12</a></span>
Moreover, the drivers use a somewhat unusual "superbuffer" circuit with two transistors: one to pull the output high, and one to pull the output low.<span id="fnref:superbuffer"><a href="#fn:superbuffer">13</a></span></p>
<p>The diagram below shows the transistor structure for one of the output pins (AD10), consisting of three
parallel transistors between the output and +5V, and five parallel transistors between the output and ground.
The die photo on the
left shows the metal layer on top of the die. This shows the power and ground wiring and the connections to
the transistors.
The photo on the right shows the die with the metal layer removed, showing the underlying silicon and the
polysilicon wiring on top.
A transistor gate is formed where a polysilicon wire crosses the doped silicon region. 
Combined, the +5V transistors are equivalent to about 60 typical transistors, while the ground transistors are
equivalent to about 100 typical transistors.
Thus, these transistors provide substantially more current to the output pin.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-transistor.jpg"><img alt="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." height="402" src="https://static.righto.com/images/8086-ad-pins/output-transistor-w800.jpg" title="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." width="800"></a></p><p>Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon.</p>
<h3>Tri-state output driver</h3>
<p>The output circuit for an address pin uses a tri-state buffer, which allows the output to be disabled
by putting it into a high-impedance "tri-state" configuration.
In this state, the output is not pulled high or low but is left floating.
This capability allows the pin to be used for data input.
It also allows external devices to device can take control of the bus, for instance, to perform
DMA (direct memory access).</p>
<p>The pin is driven by two large MOSFETs, one to pull the output high and one to pull it low.
(As described earlier, each large MOSFET is physically multiple transistors in parallel, but I'll ignore that for now.)
If both MOSFETs are off, the output floats, neither on nor off.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-circuit.jpg"><img alt="Schematic diagram of a &quot;typical&quot; address output pin." height="230" src="https://static.righto.com/images/8086-ad-pins/output-circuit-w400.jpg" title="Schematic diagram of a &quot;typical&quot; address output pin." width="400"></a></p><p>Schematic diagram of a "typical" address output pin.</p>
<p>The tri-state output is implemented by driving the MOSFETs with two "superbuffer"<span id="fnref:and"><a href="#fn:and">15</a></span> AND gates.
If the <code>enable</code> input is low, both AND gates produce a low output and both output transistors are off.
On the other hand, if <code>enable</code> is high, one AND gate will be on and one will be off.
The desired output value is loaded into a flip-flop to hold it,<span id="fnref:clock"><a href="#fn:clock">14</a></span>
and the flip-flop turns one of the output transistors on, driving the output pin high or low as appropriate.
(Conveniently, the flip-flop provides the data output Q and the inverted data output <span>Q</span>.)
Generally, the address pin outputs are enabled for T1-T4 of a write but only during T1 for a read.<span id="fnref:enable"><a href="#fn:enable">16</a></span></p>
<p>In the remainder of the discussion, I'll use the tri-state buffer symbol below, rather than showing the implementation of the buffer.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-simplified.jpg"><img alt="The output circuit, expressed with a tri-state buffer symbol." height="137" src="https://static.righto.com/images/8086-ad-pins/output-simplified-w350.jpg" title="The output circuit, expressed with a tri-state buffer symbol." width="350"></a></p><p>The output circuit, expressed with a tri-state buffer symbol.</p>
<h3>AD4-AD15</h3>
<p>Pins AD4-AD15 are "typical" pins, avoiding the special behavior of the top and bottom pins, so I'll discuss them first.
The behavior of these pins is that the value on the AD bus is latched by the circuit and then put on the output pin
under the control of the <code>enaable</code> signal.
The circuit has three parts: a multiplexer to select the output value, a flip-flop to hold the output value, and a tri-state driver to
provide the high-current output to the pin.
In more detail, the multiplexer selects either the value on the AD bus or the current output from the flip-flop.
That is, the multiplexer can either load a new value into the flip-flop or hold the existing value.<span id="fnref:implementation"><a href="#fn:implementation">17</a></span>
The flip-flop latches the input value on the falling edge of the clock, passing it to the output driver.
If the enable line is high, the output driver puts this value on the corresponding address pin.</p>
<!-- datasheet: output low tested at 2.0mA, output high tested at -400 microamps -->

<p><a href="https://static.righto.com/images/8086-ad-pins/ad415.jpg"><img alt="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." height="129" src="https://static.righto.com/images/8086-ad-pins/ad415-w400.jpg" title="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." width="400"></a></p><p>The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit.</p>
<p>For a write, the circuit latches the address value on the bus during the second half of T0 and puts it on the pins during T1.
During the second half of the T1 state, the data word is transferred from the <code>OPR</code> register over the C bus to the AD bus and loaded
into the AD pin latches.
The word is transferred from the latches to the pins during T2 and held for the remainder of the bus cycle.</p>
<h3>AD0-AD3</h3>
<p>The four low address bits have a more complex circuit because these address bits are latched from the bus before the address adder computes its sum, as described earlier.
The memory offset (before the segment addition) will be on the C bus during the second half of TS and is loaded into the lower
flip-flop. This flip-flop delays these bits for one clock cycle and then they are loaded into the upper flip-flop.
Thus, these four pins pick up the offset prior to the addition, while the other pins get the result of the segment addition.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad03.jpg"><img alt="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." height="174" src="https://static.righto.com/images/8086-ad-pins/ad03-w500.jpg" title="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." width="500"></a></p><p>The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum.</p>
<p>For data, the AD0-AD3 pins transfer data directly from the AD bus to the pin latch, bypassing the delay that was used to get the address bits.
That is, the AD0-AD3 pins have two paths: the delayed path used for addresses during T0 and the direct path otherwise used for data.
Thus, the multiplexer has three inputs: two for these two paths and a third loop-back input to hold the flip-flop value.</p>
<!--
ad-latch-load loads the  AD0-15 latches.
-->

<!--
If the memory access was an instruction fetch,
the address adder is immediately reused to update the instruction pointer (program counter).
In the second half of T1, one input of the address adder is loaded with the instruction pointer increment from the constant ROM (2 if a word was fetched).
This value is added to the instruction pointer value and
the updated instruction pointer value is written back in the second half of T2.
A similar process is used for other memory accesses that update a pointer, such as stack operations or string operations.

If another bus cycle follows, the T3 and T4 states act like the T0 and T1 states described above, preparing the next memory address.
Thus, address calculation is pipelined in the 8086: the address adder performs the segment computation during the last half of the previous bus cycle, so the physical memory address will be ready at the start of the bus cycle.

For a memory write, the address latches are reloaded during T1, loading them with the `OPR` register???
-->

<h3>A16-A19: status outputs</h3>
<p>The top four pins (A16-A19) are treated specially, since they are not used for data.
Instead, they provide processor status during T2-T4.<span id="fnref:status"><a href="#fn:status">18</a></span> The pin latches for these
pins are loaded with the address during T0 like the other pins, but loaded with status instead of data during T1.
The multiplexer at the input to the latch selects the address bit during T0 and the status bit during T1, and
holds the value otherwise.
The schematic below shows how this is implemented for A16, A17, and A19.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad1619.jpg"><img alt="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." height="115" src="https://static.righto.com/images/8086-ad-pins/ad1619-w400.jpg" title="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." width="400"></a></p><p>The output circuit for AD16, AD17, and AD19 selects either an address output or a status output.</p>
<p>Address pin A18 is different because it indicates the current status of the interrupt enable flag bit.
This status is updated every clock cycle, unlike the other pins.
To implement this, the pin has a different circuit that isn't latched,
so the status can be updated continuously.
The clocked transistors act as "pass transistors", passing the signal through when active.
When a pass transistor is turned off, the following logic gate holds the previous value due to the capacitance of the
wiring.
Thus, the pass transistors provide a way of holding the value through the clock cycle.
The flip-flops are implemented with pass transistors internally, so in a sense the circuit below is a flip-flop
that has been "exploded" to provide a second path for the interrupt status.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad18.jpg"><img alt="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." height="162" src="https://static.righto.com/images/8086-ad-pins/ad18-w540.jpg" title="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." width="540"></a></p><p>The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle.</p>
<h2>Reads</h2>
<p>A memory or I/O read also uses a 4-state bus cycle, slightly different from the write cycle.
During T1, the address is provided on the pins, the same as for a write.
After that, however, the output circuits are tri-stated so they float, allowing the external memory to put data on the bus.
The read data on the pin is put on the AD bus at the start of the T4 state.
From there, the data passes through the crossover circuit to the C bus. Normally the 16 data bits pass straight through to
the C bus, but the bytes will be swapped if the memory access is unaligned.
From the C bus, the data is written to the <code>OPR</code> register, a byte or a word as appropriate.
(For an instruction prefetch, the word is written to a prefetch queue register instead.)</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-cycle.jpg"><img alt="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="139" src="https://static.righto.com/images/8086-ad-pins/read-cycle-w600.jpg" title="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="600"></a></p><p>A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>To support data input on the AD0-AD15 pins, they have a circuit to buffer the input data and transfer it to the AD bus.
The incoming data bit is buffered by the two inverters and sampled when the clock is high.
If the enable' signal is low, the data bit is transferred to the AD bus when the clock is low.<span id="fnref:read-enable"><a href="#fn:read-enable">19</a></span>
The two MOSFETs act as a "superbuffer", providing enough current for the fairly long AD bus.
I'm not sure what the capacitor accomplishes, maybe avoiding a race condition if the data pin changes just as the clock goes low.<span id="fnref:race"><a href="#fn:race">20</a></span></p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-circuit.jpg"><img alt="Schematic of the input circuit for the data pins." height="134" src="https://static.righto.com/images/8086-ad-pins/read-circuit-w500.jpg" title="Schematic of the input circuit for the data pins." width="500"></a></p><p>Schematic of the input circuit for the data pins.</p>
<p>This circuit has a second role, precharging the AD bus high when the clock is low, if there's no data.
Precharging a bus is fairly common in the 8086 (and other NMOS processors) because NMOS transistors are better at pulling a
line low than pulling it high. Thus, it's often faster to precharge a line high before it's needed and then pull it low for a 0.<span id="fnref:adder"><a href="#fn:adder">21</a></span></p>
<p>Since pins A16-A19 are not used for data, they operate the same for reads as for writes: providing address bits and then status.</p>
<h2>The pin circuit on the die</h2>
<p>The diagram below shows how the pin circuitry appears on the die. The metal wiring has been removed to show the silicon and polysilicon.
The top half of the image is the input circuitry, reading a data bit from the pin and feeding it to the AD bus.
The lower half of the image is the output circuitry, reading an address or data bit from the AD bus and amplifying it for output
via the pad.
The light gray regions are doped, conductive silicon. The thin tan lines are polysilicon, which forms transistor gates where it crosses doped silicon.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/pin-labeled.jpg"><img alt="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." height="482" src="https://static.righto.com/images/8086-ad-pins/pin-labeled-w600.jpg" title="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." width="600"></a></p><p>The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was.</p>
<h2>A historical look at pins and timing</h2>
<p>The number of pins on Intel chips has grown exponentially, more than a factor of 100 in 50 years.
In the early days, Intel management was convinced that a 16-pin package was large enough for any integrated circuit.
As a result, the Intel 4004 processor (1971) was crammed into a 16-pin package.
Intel chip designer Federico Faggin<span id="fnref:faggin"><a href="#fn:faggin">22</a></span> describes 16-pin packages as a completely silly requirement that was throwing away
performance,
but the "God-given 16 pins" was like a religion at Intel.
When Intel was forced to use 18 pins by the 1103 memory chip, it "was like the sky had dropped from heaven"
and he had "never seen so many long faces at Intel."
Although the 8008 processor (1972) was able to use 18 pins, this low pin count still harmed performance by forcing pins to be used for multiple
purposes.</p>
<p>The Intel 8080 (1974) had a larger, 40-pin package that allowed it to have 16 address pins and 8 data pins.
Intel stuck with this size for the 8086, even though competitors used larger packages with more pins.<span id="fnref:ti"><a href="#fn:ti">23</a></span>
As processors became more complex, the 40-pin package became infeasible and the pin count rapidly expanded;
The 80286 processor (1982) had a 68-pin package, while the
i386 (1985) had 132 pins; the i386 needed many more pins because it had a 32-bit data bus and a 24- or 32-bit address bus.
The i486 (1989) went to 196 pins while the original Pentium had 273 pins.
Nowadays, a modern <a href="https://www.intel.com/content/www/us/en/products/sku/232167/intel-core-i913900ks-processor-36m-cache-up-to-6-00-ghz/specifications.html">Core I9 processor</a> uses the <a href="https://en.wikipedia.org/wiki/LGA_1700">FCLGA1700</a> socket with a whopping 1700 contacts.</p>
<p>Looking at the history of Intel's bus timing, the 8086's complicated memory timing goes back to the Intel 8008 processor (1972). Instruction execution in the 8008 went through
a specific sequence of timing states; each clock cycle was assigned a particular state number.
Memory accesses took three cycles:
the address was sent to memory during states T1 and T2, half of the address at a time since there were only 8 address pins.
During state T3, a data byte was either transmitted to memory or read from memory.
Instruction execution took place during T4 and T5.
State signals from the 8008 chip indicated which state it was in.</p>
<!-- http://www.bitsavers.org/components/intel/MCS8/Intel_8008_8-Bit_Parallel_Central_Processing_Unit_Rev1_Apr72.pdf -->

<p>The 8080 used an even more complicated timing system.
An instruction consisted of one to five "machine cycles", numbered M1 through M5, where each machine cycle corresponded to
a memory or I/O access. Each machine cycle consisted of three to five states, T1 through T5, similar to the 8008 states.
The 8080 had 10 different types of machine cycle such as instruction fetch, memory read, memory write, stack read or write,
or I/O read or write. The status bits indicated the type of machine cycle.
The 8086 kept the T1 through T4 memory cycle. Because the 8086 decoupled instruction prefetching from execution, it no
longer had explicit M machine cycles. Instead, it used status bits to indicate 8 types of bus activity such as instruction
fetch, read data, or write I/O.</p>
<h2>Conclusions</h2>
<p>Well, address pins is another subject that I thought would be straightforward to explain but turned out to be surprisingly
complicated.
Many of the 8086's design decisions combine in the address pins: segmented addressing, backward compatibility, and the small 40-pin package.
Moreover, because memory accesses are critical to performance, Intel put a lot of effort into this circuitry.
Thus, the pin circuitry is tuned for particular purposes, especially pin A18 which is different from all the rest.</p>
<p>There is a lot more to say about memory accesses and how the 8086's Bus Interface Unit performs them.
The process is very complicated, with interacting state machines for memory operation and instruction prefetches, as well
as handling unaligned memory accesses.
I plan to write more, so 
follow me on Twitter <a href="https://twitter.com/kenshirriff">@kenshirriff</a> or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.
I've also started experimenting with Mastodon recently as <a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="eb808e859883829999828d8dab84878f89929f8e98c5989b8a888e">[email&nbsp;protected]</span></a>
and Bluesky as <a href="https://staging.bsky.app/profile/righto.com">@righto.com</a> so you can follow me there too.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Langchain Is Pointless (293 pts)]]></title>
            <link>https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</link>
            <guid>36645575</guid>
            <pubDate>Sat, 08 Jul 2023 15:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/">https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</a>, See on <a href="https://news.ycombinator.com/item?id=36645575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's filled with crap like this:</p>

<pre><code>    for i in range(n_results, 0, -1):
        try:
            return self._collection.query(
                query_texts=query_texts,
                query_embeddings=query_embeddings,
                n_results=i,
                where=where,
                **kwargs,
            )
</code></pre>

<p>and this:</p>

<pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:
    texts = list(map(lambda x: x.replace("\n", " "), texts))
    embeddings = self.client.encode(texts, **self.encode_kwargs)
    return embeddings.tolist()
</code></pre>

<p>and this:</p>

<pre><code>class CharacterTextSplitter(TextSplitter):
    """Implementation of splitting text that looks at characters."""

def __init__(self, separator: str = "\n\n", **kwargs: Any):
    """Create a new TextSplitter."""
    super().__init__(**kwargs)
    self._separator = separator

def split_text(self, text: str) -&gt; List[str]:
    """Split incoming text and return chunks."""
    # First we naively split the large input into a bunch of smaller ones.
    if self._separator:
        splits = text.split(self._separator)
    else:
        splits = list(text)
</code></pre>

<p>In short: <a href="https://i.imgur.com/OffEJTR.gifv">https://i.imgur.com/OffEJTR.gifv</a></p>

<p>Embeddings is just a do-nothing wrapper for SentenceTransformers. Chroma is just a do-nothing wrapper for ChromaDB. It's filled with "helper" functions that just call normal Python functions. A dedicated TextSplitter that calls split() from builtins.py? What? Why? Templates are no more useful than calling .replace() on a string. "texts" are just strings and "documents" are just a pointless dict that contain "texts." Just load the strings from your datasource yourself. The README is both grandiose and vague. The documentation is out-of-date and inconsistent. The import footprint is weirdly massive--highly modularized but nothing seems to do anything that'd take more than a few CPU cycles. There's not really a standard interoperable datatype, so you're actually led further afield than if you had just clearly defined the simple lists and strings required for hitting an LLM. </p>

<p>The very concept of chaining operations when interacting with LLMs doesn't really make sense to me: it's basically one <code>requests</code> call to a generation backend, but it's not like it even handles websockets and streaming for you. Why chain together wrapper classes when you can just do the operations yourself?</p>

<p>This seems like a beginner's project that blew up because it's riding a tidal wave of interest in the broader topic.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flickr Foundation is building a new bridge between Flickr and Wikimedia Commons (131 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</link>
            <guid>36645448</guid>
            <pubDate>Sat, 08 Jul 2023 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/">https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</a>, See on <a href="https://news.ycombinator.com/item?id=36645448">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-97236">
	<div>
			


<p>We are pleased to announce a new partnership with the <a href="https://www.flickr.org/">Flickr Foundation</a> to extend the great work already done via the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool to make it even easier to upload CC-licensed images from Flickr into Wikimedia Commons.&nbsp;</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97298"><img decoding="async" loading="lazy" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231" alt="" width="256" height="231" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=768 768w" sizes="(max-width: 256px) 100vw, 256px" data-recalc-dims="1"></a></figure></div>


<p>Wikipedia is a foundational source of information on the internet. It provides content to Google and other search engines, social media platforms, voice assistants, and, increasingly, AI applications. To illustrate that information, we have Wikimedia Commons, the central visual platform for Wikipedia and one of the primary sources for open licensed visual content online. You may not know that one of the largest sources for Wikimedia Commons is Flickr.&nbsp;&nbsp;</p>



<p>Since 2004, Flickr has been one of the most popular platforms for photographers and amateurs to upload photographs, videos, illustrations, and more online. It is also one of the largest online repositories of Creative Commons-licensed content. Flickr members can assign a license to their uploads, including those Creative Commons licenses accepted on Wikimedia Commons: Attribution (CC-BY), Attribution-ShareAlike (CC-BY-SA), Public Domain Dedication (CC0), and the Public Domain Mark.</p>





<p>In 2008, Flickr launched the <a href="https://www.flickr.com/commons">Flickr Commons</a> program, to increase public access to photography collections held at libraries, museums, and archives around the world. Images in Flickr Commons are shared with something a bit different from a license. It’s an <em>assertion</em> called&nbsp; “no known copyright restrictions.”&nbsp; The program supports <a href="https://www.flickr.com/commons/institutions">over 100 member institutions</a>, including <a href="https://www.flickr.com/photos/usnationalarchives/">The U.S. National Archives</a>, <a href="https://www.flickr.com/photos/nasacommons/">NASA on Commons</a>, the <a href="https://www.flickr.com/photos/nlscotland/">National Library of Scotland</a>, and <a href="https://www.flickr.com/photos/reykjavikmuseumofphotography/">Ljósmyndasafn Reykjavíkur</a>.</p>



<p>In 2022, the <a href="https://www.flickr.org/">Flickr Foundation</a> was established. It’s a US 501(c)(3) non-profit organization with the objective of safeguarding Flickr and its tens of billions of photos for the future. It seeks to develop and sustain “…an accessible social and technical infrastructure to protect [this] invaluable collection.”&nbsp;<br>This bridge between Flickr and Wikimedia Commons—which we’ve started calling “<strong>Flickypedia</strong>”—is one of the flagship projects of the Flickr Foundation. Building in partnership with the Wikimedia Foundation, and supported by the Culture and Heritage team, we will be building on the utility of the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool, extending it, and then tending it for the long term.</p>



<p>This project has been mentioned in the 2023-2024 <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Annual_Plan/2023-2024/Goals/Equity">Wikimedia Foundation Annual Plan</a> under the Equity / Culture &amp; Heritage section, in <a href="https://web.archive.org/web/20230602045536/https://mailchi.mp/c6a1d40b1748/new-news-for-you-about-flickrorg">this mailing list update</a> from Flickr Foundation, and <a href="https://commons.wikimedia.org/wiki/Commons:Village_pump/Archive/2023/06#Flickr_Foundation_adopts_Flickr2Commons">in the ensuing discussion</a> on Wikimedia Commons’s Village Pump.</p>



<h2>About Flickr2Commons</h2>



<p>Flickr2Commons is a popular tool used by Wikimedia Commons contributors to upload single or multiple files from Flickr into Wikimedia Commons. It was created by <a href="https://en.wikipedia.org/wiki/Magnus_Manske">Magnus Manske</a>, and first launched in 2013, ten years ago! The tool allows for user authentication, checks for the required licenses, includes a metadata editing step, and then file transfer.&nbsp;</p>



<h2>Metrics important for Flickypedia</h2>



<p>In order to gauge the possible reach of Flickypedia, we wanted to understand Flickr2Commons metrics. Magnus helped pull together the stats to show that roughly 5.4M files have been uploaded by about 2K users since launch. Using the <a href="https://hashtags.wmcloud.org/">Wikimedia Hashtags</a> tool, we can also see how much Flickr2Commons is used today. In June 2023 only, for example, 71,689 files were uploaded by 147 users.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97279"><img decoding="async" loading="lazy" width="1024" height="427" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?resize=1024%2C427" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1920 1920w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1536 1536w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Number of edits (ie. uploads), with the Flickr2Commons tool in June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-06-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags tool</a>)</figcaption></figure></div>


<p>We were also able to discover the most active users of Flickr2Commons in the last six months, from January to June 2023.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97281"><img decoding="async" loading="lazy" width="1024" height="1024" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?resize=1024%2C1024" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1254 1254w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=150 150w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Most active users of Flickr2Commons January to June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-01-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags</a>)</figcaption></figure></div>


<p>It’s been great to collate all these usage statistics for Flickr2Commons—both the more recent numbers, but also in total over the last 10 years. Seeing it all together gives us a clear target for the new version to try to match.&nbsp;</p>



<p>It is also worth noting that another tool connects Flickr to Wikimedia Commons, called the UploadWizard. We’re bearing in mind that this means there will have been even more images from Flickr through that tool. Preparing these metrics has given us ideas on how we might make it even simpler to count into the future using Flickypedia.</p>



<h2>Our timeline</h2>



<p>The Flickypedia partnership project officially started in June 2023. We plan to spend the next six months or so building our Alpha (hopefully to show in October) and then Version 1.0 (hopefully December). Please <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">stay in touch</a> if you’d like to be involved in testing or have feedback about Flickr2Commons we should know about.</p>



<ul>
<li>For the July-December plan, please <a href="https://commons.wikimedia.org/wiki/Commons:Flickypedia">visit the project page</a> on Wikimedia Commons.</li>



<li>For feedback, please contact us via the <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">talk page</a> on Wikimedia Commons.</li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F07%2F07%2Fflickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spying on a smartphone remotely by the authorities: feasibility and operation (105 pts)]]></title>
            <link>https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</link>
            <guid>36644952</guid>
            <pubDate>Sat, 08 Jul 2023 14:49:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation">https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</a>, See on <a href="https://news.ycombinator.com/item?id=36644952">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                
<p>French lawmakers agreed to a justice reform bill that includes a provision granting police the power to remotely activate suspects' geolocation, microphone and camera (<a href="https://www.lemonde.fr/en/france/article/2023/07/06/france-set-to-allow-police-to-spy-through-phones_6044269_7.html" rel="noreferrer">source</a>).</p>
<p>Following the senators, the deputies also gave their green light to allow certain features of smartphones and other devices to be activated remotely, thus turning them into surveillance trackers.</p>
<p>The deputies determined a list of professions "protected" from any capture: journalists, doctors, notaries, and bailiffs, in addition to lawyers, magistrates and members of parliament.</p>
<p><strong>Technically, will it be possible for them to set up this type of surveillance on phones? If so, how will they go about it?</strong></p>
<p>I wonder how authorities can effectively activate microphones and cameras from popular smartphones using iOS or Android.</p>
<p>I am also wondering how can the privacy of individuals not involved in any criminal activity be safeguarded when such wide-reaching surveillance measures are implemented.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When an app asks for permissions, it should have a “feed fake data” option (935 pts)]]></title>
            <link>https://mastodon.gamedev.place/@Nifflas/110668040598715116</link>
            <guid>36644895</guid>
            <pubDate>Sat, 08 Jul 2023 14:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.gamedev.place/@Nifflas/110668040598715116">https://mastodon.gamedev.place/@Nifflas/110668040598715116</a>, See on <a href="https://news.ycombinator.com/item?id=36644895">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[iVentoy (221 pts)]]></title>
            <link>https://www.iventoy.com/en/index.html</link>
            <guid>36644806</guid>
            <pubDate>Sat, 08 Jul 2023 14:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iventoy.com/en/index.html">https://www.iventoy.com/en/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=36644806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentpage"> 


<h2>News</h2>

<p>
2023/07/05 ---  New release <a href="https://www.iventoy.com/en/download.html">iventoy-1.0.08 </a>
<span> <a href="https://www.iventoy.com/en/doc_news.html">More ...</a></span>
</p>

<h2>What is iVentoy</h2>
<p>   
    iVentoy is an enhanced version of the PXE server. <br>
    With iVentoy you can boot and install OS on multiple machines at the same time through the network.<br> 
    iVentoy is extremely easy to use, without complicated configuration, just put the ISO file in the specified location and select PXE boot in the client machine.<br>
    iVentoy supports x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI and ARM64 UEFI mode at the same time.<br>
    iVentoy support 110+ common types of OS (Windows/WinPE/Linux/VMware) (<a href="https://www.iventoy.com/en/isolist.html">list</a>)。
</p>

<br>

<h2>Features</h2>
<div>
<div>
<ul>
  <li>Simple to use <a href="https://www.iventoy.com/en/doc_start.html">(Get Started)</a>  </li>  
  <li>Cross-platform, can run in both Windows and Linux.</li>      
  <li>Specially optimized for PXE scenarios, with flexible functions.</li>      
  <li>Directly boot ISO files, no extraction needed.</li>
  <li>Native boot menu style for Legacy &amp; UEFI</li> 
  <li>Directory layout corresponded boot menu.</li> 
  <li>Supports Legacy BIOS and IA32/X86_64/ARM64 UEFI mode.</li>    
  <li>Supports 110+ common types of OS (Windows/WinPE/Linux/VMware) </li>      
  <li>System or ISO level boot password protection.</li>  
  <li>Multiple devices to install different OSs at the same time.</li>
</ul>
</div>

<div>
<ul>  

  <li>Device filtering by MAC address.</li>
  <li>Support querying MAC address filtering status.</li>
  <li>Support MAC address attribution query.</li>
  <li>Client device information. (Manufacture, product name etc.)</li>
  <li>Directly get ISO internal files with HTTP.<a href="https://www.iventoy.com/en/doc_http_url.html">Notes</a></li>  
  <li>File Injection feature. <a href="https://www.iventoy.com/en/doc_injection.html">Notes</a> </li>  
  <li>Windows auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Linux auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Variables Expansion supported for install script <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Automatically solve the driver missing during Linux installation.</li>
  
</ul>
</div>
</div>


<p><img src="https://www.iventoy.com/static/img/screen/boot_ground.png?v=4">
</p>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $182B Chip Maker: AMD's Labs – Full Documentary [video] (170 pts)]]></title>
            <link>https://www.youtube.com/watch?v=7H4eg2jOvVw</link>
            <guid>36644506</guid>
            <pubDate>Sat, 08 Jul 2023 14:02:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=7H4eg2jOvVw">https://www.youtube.com/watch?v=7H4eg2jOvVw</a>, See on <a href="https://news.ycombinator.com/item?id=36644506">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Thermochromic Breadboard (136 pts)]]></title>
            <link>https://www.improwis.com/projects/hw_ThermochromicBreadboard/</link>
            <guid>36644026</guid>
            <pubDate>Sat, 08 Jul 2023 13:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improwis.com/projects/hw_ThermochromicBreadboard/">https://www.improwis.com/projects/hw_ThermochromicBreadboard/</a>, See on <a href="https://news.ycombinator.com/item?id=36644026">Hacker News</a></p>
<div id="readability-page-1" class="page">

<hr><hr><a name="Why"></a><h2>Why
</h2>
<p>
When working on a&nbsp;<a href="https://en.wikipedia.org/wiki/Breadboard#Solderless_breadboard" title="Wikipedia link: Breadboard#Solderless_breadboard" target="_blank">solderless breadboard</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC"> (not only there, but... well...),
it often happens that a&nbsp;power rating of some part is exceeded or its cooling is insufficient.
It would be beneficial to see such situations before the&nbsp;parts express their distress with a
smoke signal.
</p>
<hr><a name="How"></a><h2>How
</h2>
<p>
A breadborad was modified with a&nbsp;<a href="https://en.wikipedia.org/wiki/Thermochromism" title="Wikipedia link: Thermochromism" target="_blank">thermochromic paint</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC">, mixed from a&nbsp;white
acrylic model-grade paint and a&nbsp;thermochromic pigment obtained from <a href="http://www.mutr.co.uk/" title="remote link: http://www.mutr.co.uk/" target="_blank">Middlesex University Teaching Resources</a><img src="data:image/gif;base64,R0lGODlhCgAKAIABAGZmZv///yH5BAEAAAEALAAAAAAKAAoAAAIVjA8Jx6FvVmrL1chu3c1h+mXRoxgFADs=">
webshop, using a&nbsp;makeshift pot made from the&nbsp;bottom part of a&nbsp;beverage can. 
The orange color was chosen on the&nbsp;basis of availability (read: mistakenly ordering four
oranges instead of intended four different hues.) The threshold temperature of the&nbsp;pigment
was chosen also on the&nbsp;basis of availability (they did not have any other than 29-30&nbsp;°C).
</p>
<p>
The active area, where the&nbsp;parts are located by at least one pin, was coated with the
mixed paint. Care was taken to not let too much of it drip into the&nbsp;pin holes.
The power buses were not painted.
</p>
<hr><a name="Results"></a><h2>Results
</h2>
<p>
The first tests were done during a&nbsp;particularly warm summer night, when the&nbsp;indoor temperature
reached close to threshold temperature of the&nbsp;pigment. At such conditions, the&nbsp;sensitivity of the
paint was outstanding; a&nbsp;250-milliwatt resistor loaded with 350&nbsp;milliwatts shown a&nbsp;color change around
its leg within several seconds.
</p>
<p>
The reverse change was much slower. Due to some hysteresis of the&nbsp;pigment and the&nbsp;closeness of the
room temperature to the&nbsp;threshold temperature, the&nbsp;thermal trace was present for a&nbsp;fairly long time
(minutes). Putting the&nbsp;board out of the&nbsp;window, where the&nbsp;temperature was slightly lower, markedly
accelerated restoring of the&nbsp;color.
</p>
<p>
The thermal trace (the discoloration of the&nbsp;pigment) tended to bleed around the&nbsp;board as its material
spread the&nbsp;heat. The width of the&nbsp;trace, and the&nbsp;speed of its spreading, can provide a&nbsp;visual clue
about how much is the&nbsp;part heating.
</p>
<p>
Caveat: The color change shows the&nbsp;part's color indirectly. There is a&nbsp;delay between the&nbsp;temperature
change of the&nbsp;part itself and temperature change of the&nbsp;board, which is usually provided via thermal
conduction through the&nbsp;part's legs. There is also a&nbsp;thermal differential between the&nbsp;part itself,
along its leg, and over the&nbsp;board.
</p>
<hr><a name="Possibleimprovements"></a><h2>Possible improvements
</h2>
<ul><li> Use a&nbsp;darker pigment with a&nbsp;different hue (most likely blue or black)
</li><li> Use a&nbsp;pigment with a&nbsp;different threshold temperature (e.g. 43&nbsp;°C, that could be optimal)
</li><li> Best: Use a&nbsp;pair of pigments with different colors and different thresholds, yielding two color changes at two temperatures
</li><li> Incorporate the&nbsp;pigment directly into the&nbsp;material the&nbsp;breadboard is made from
</li></ul><div><hr><div>
<table>
<tbody><tr><td colspan="2"><small>If you have any comments or questions about the topic, please let me know here:</small></td></tr>


<tr><td>Your name:</td><td></td></tr>
<tr><td>Your email:</td><td></td></tr>

<tr><td>Feedback:</td><td></td></tr>
<tr><td>&nbsp;</td><td></td></tr>

</tbody></table>
</div>
</div>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[If PEP 703 is accepted, Meta can commit three engineer-years to nogil CPython (576 pts)]]></title>
            <link>https://discuss.python.org/t/a-fast-free-threading-python/27903/99</link>
            <guid>36643670</guid>
            <pubDate>Sat, 08 Jul 2023 12:18:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/99">https://discuss.python.org/t/a-fast-free-threading-python/27903/99</a>, See on <a href="https://news.ycombinator.com/item?id=36643670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="A fast, free threading Python">
        <meta itemprop="articleSection" content="Ideas">
      <meta itemprop="keywords" content="">
      

          <div itemprop="comment" id="post_80" itemscope="" itemtype="http://schema.org/Comment">
              
<p>This will work as long as you give it an initial empty <code>Counter</code> to start with (otherwise it starts with <code>0</code> and complains)</p>
            </div>
          <div id="post_81" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Rosuav"><span itemprop="name">Rosuav</span></a>
                (Chris Angelico)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T15:35:51Z">
                    June 26, 2023,  3:35pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T15:35:51Z">
              <span itemprop="position">81</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Ah thanks. Anyhow, the idea is to minimize the work done in the single-threaded “gather” phase at the end, by having each thread individually count in a lock-free way.</p>
            </div>

            

            

          </div>
          <div id="post_82" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lieryan"><span itemprop="name">lieryan</span></a>
                (Lie Ryan)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:20:39Z">
                    June 26, 2023,  4:20pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:28:05Z">
              <span itemprop="position">82</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I don’t think that is true. If free threading is possible, the cat will be out of the bag, even developers that only cares about single threaded work will still be affected by threading issues. If a library starts a thread in the background for whatever reason, they can cause threading issue in my code even though I never subscribed for having threading problems.</p>
<p>Many libraries that had async-to-sync bridges spawns threads to simulate async tasks. Django, FastAPI, SQLAlchemy is just a few off the top of my head. And then there’s tools like IPython that starts a couple background threads for who knows what reasons.</p>
<p>Multithreading has a reputation for being hard. But really, I think they are considered hard <strong>because</strong> of the existence of free threading. Languages like Rust that doesn’t have free threading (or to be more precise, it has an almost free threading with some severe restrictions) actually fared better at making multithreading a lot easier to use.</p>
<p>The arena-based threading with subinterpreters I had mentioned earlier would be that similar sort of that almost-free threading with forced discipline.</p>
<p>One way to think of arena-based threading is that it’s basically like a dynamic/runtime borrow checker, enforcing acquisition of the arena locks before working with any objects owned by the arena. I think it can even be flexible enough to allow future experimentation with non standard arenas that have different borrowing rules.</p>
            </div>

            

            

          </div>
          <div id="post_83" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:25:27Z">
                    June 26, 2023,  4:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:25:27Z">
              <span itemprop="position">83</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>There are language-level tools like golang’s race condition detector, thread sanitizer, etc, which take the common mistakes and test for them. It’s also possible someone could implement something like a borrow checker or thread safety heuristics on top of python’s type system, e.g. with passthrough types along the lines of Mutable / Immutable / Shared / Local, and auditing nonlocal variable access or object types passed into threads.</p>

<p>This wouldn’t be the case with my proposal to make threads take a voluntary lock by default. In a sense, you could leave something like the GIL in place, but make it safe to release for specific threads while accessing python code/objects.</p>
            </div>

            

            

          </div>
          <div itemprop="comment" id="post_84" itemscope="" itemtype="http://schema.org/Comment">
              
<p>I don’t think anyone has demonstrated how this would happen, and I’d view it as a fundamental flaw in the implementation if it could.</p>
<p>I’m not saying it’s impossible, but I think it would be useful to have specific examples–even if only theoretical–before it’s considered a significant problem.</p>
            </div>
          <div id="post_85" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:33:27Z">
                    June 26, 2023,  4:33pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:33:27Z">
              <span itemprop="position">85</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I guess this is a bit too open ended. I think the thread in question can only interact with your code unintentionally if you happen to share a resource with that thread in an unsafe way, furthermore to be scary it would need to be an unsafe way that isn’t possible today. Even with the GIL another thread can already do a lot of things, like mess with your file descriptors, stdout, signals. And threads sharing access to any variable is already inconsistent for non-atomic ops at the Python level.</p>
            </div>

            

            

          </div>
          <div id="post_86" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:01:58Z">
                    June 26, 2023,  5:01pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:01:58Z">
              <span itemprop="position">86</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Can you provide a (hypothetical?) example?</p>
            </div>

            

            

          </div>
          <div id="post_87" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Eclips4"><span itemprop="name">Eclips4</span></a>
                (Kirill Podoprigora)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:13:12Z">
                    June 26, 2023,  5:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:13:12Z">
              <span itemprop="position">87</span>
              </span>
            </p></div>
            <p>Yep, CPython can switch threads in the middle of these two operations. So, there’s a problem.</p>

            

            

          </div>
          <div id="post_88" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ntessore"><span itemprop="name">ntessore</span></a>
                (Nicolas Tessore)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T21:16:35Z">
                    June 26, 2023,  9:16pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T21:16:35Z">
              <span itemprop="position">88</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>Again, of course. <sup><a href="#footnote-102472-1" id="footnote-ref-102472-1">[1]</a></sup> But I understood that <a href="https://discuss.python.org/u/pf_moore">@pf_moore</a> made the very fine point that due to specialisations we are discussing here (e.g. <code>BINARY_SUBSCR_DICT</code>), and hence the GIL, things which are nominally not thread-safe are effectively so in current CPython, because they are specialised to a single native instruction. And this, I think, only needs an explicit specification for whether or not such operations are to be considered effectively “atomic” or not. Otherwise, yes, these are just undiscovered bugs, currently protected by a CPython implementation detail.</p>
<hr>

<ol>
<li id="footnote-102472-1"><p>Except that there’s the timer. <a href="#footnote-ref-102472-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_89" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/tjreedy"><span itemprop="name">tjreedy</span></a>
                (Terry Jan Reedy)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:25Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:25Z">
              <span itemprop="position">89</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>… of a library starting a background thread.  Not exactly a library, but idlelib.run has</p>
<pre><code>    sockthread = threading.Thread(target=manage_socket,
                                  name='SockThread',
                                  args=((LOCALHOST, port),))
</code></pre>
<p>as a result of which <code>threading.activecount()</code> and <code>theading.enumerate()</code> returns are greater when running on IDLE.  Someone once asked why the difference on Stackoverflow.   (I have not idea whether no-gil will require any change to <code>manage_socket</code> or that chance of user code having a problem.)</p>
            </div>

            

            

          </div>
          <div id="post_90" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:51Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:51Z">
              <span itemprop="position">90</span>
              </span>
            </p></div>
            <div itemprop="text">
              <pre><code>import threading

THREAD_COUNT = 3
BY_HOW_MUCH = 1_000_000


class Incrementor:
    def __init__(self):
        self.c = 0

def incr(incrementor, by_how_much):
    for i in range(by_how_much):
        incrementor.c += 1

incrementor = Incrementor()

threads = [
    threading.Thread(target=incr, args=(incrementor, BY_HOW_MUCH))
    for i in range(THREAD_COUNT)
]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(incrementor.c)

</code></pre>
<p>prints 3 million when ran it on 3.10. Does it mean you can rely on <code>+=</code> being atomic when writing Python code? No! If you run it on 3.9 it prints between 1.5 and 2 million. Soon a Faster CPython team member can swoop in and change (not break!) it again.</p>
<p>BTW if Java and .net developers can have free threads, then so can we.</p>
            </div>

            

            

          </div>
          <div id="post_91" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/h-vetinari"><span itemprop="name">h-vetinari</span></a>
                (H. Vetinari)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T23:08:41Z">
                    June 26, 2023, 11:08pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T23:08:41Z">
              <span itemprop="position">91</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>The word “can” here translates to (potentially) decades of work, which was the case for Java:</p>

<p>Yes we “can” (and likely should), but it requires serious commitment, and off-hand “others do it too” is not helpful here.</p>
<p>In the context Java and threading, it’s worth noting how threads commonly need quite a lot of developer-facing infrastructure (e.g. thread pools) that’s probably very hard to make beginner-friendly / “Pythonic”, and that they’re on a similarly large multi-{year,person} effort to move from free threads to virtual threads<sup><a href="#footnote-102482-1" id="footnote-ref-102482-1">[1]</a></sup> under <a href="https://openjdk.org/projects/loom/" rel="noopener nofollow ugc">Project “Loom”</a> (where – arguably – the boundaries to async programming start getting blurred), and encapsulating a lot of that in simpler interfaces through <a href="https://openjdk.org/jeps/453" rel="noopener nofollow ugc">“structured concurrency”</a>, which we have already (at least through <code>trio</code>).</p>
<p>All that to say: if we argue “Java can”, then we should also look at where those choices have led them, and what they consider as “moving forward” from there. But realistically, we’re very far from an apples-to-apples comparison in any case, and it’s better to leave that rhetorical tool hanging in the shed.</p>
<hr>

<ol>
<li id="footnote-102482-1"><p>latest <a href="https://openjdk.org/jeps/444" rel="noopener nofollow ugc">incarnation</a> in JDK 21 <a href="#footnote-ref-102482-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_92" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T04:39:41Z">
                    June 27, 2023,  4:39am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T07:13:02Z">
              <span itemprop="position">92</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I meant it from the user perspective, referring to “maybe PEP-684 is better, because it’s safer” part of the discussion. For years Python was “parallel, but…” and now adding subinterpreters will help, but it won’t solve the entire problem.</p>
<p>PEP-703 on the other hand, goes pretty much all the way (though stop-the-world GC may still be a limitation) for those willing to learn how to use it and for those who already have experience with threads from other languages. Python “popularity” will increase with projects choosing it for a multicore program when it will become an option.</p>
<p>Will some users hurt themselves with free threading? I’ve been tracking nogil for a long while now and from what I’ve seen, for someone who writes threadsafe code already (but not native extensions) it will be really hard to run into trouble.</p>
            </div>

            

            

          </div>
          <div id="post_93" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T19:25:35Z">
                    June 27, 2023,  7:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T19:30:00Z">
              <span itemprop="position">93</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>There are some multithreading traps inside glibc (relevant for Linux) that are unfortunate and sort of perennial issues (not just in Python!). <a href="http://rachelbythebay.com/w/2017/01/30/env/" rel="noopener nofollow ugc">For example getenv and setenv</a>; glibc maintains that multithreaded programs must not use setenv, it’s not thread safe.</p>
<p>A library could start a thread, and the library wants to and would use C getenv in this thread (getenv is allowed according to glibc in a multithreaded program, following the usual logic).<br>
The user’s program then has a threading issue: they must not use setenv, that could possibly cause segfaults (Python has setenv interfaces through os.environ and os.putenv).</p>
<p>(Does this issue exist in Python already today? Is there some mitigating factor that I don’t know about? How do subinterpreters deal with this? It would be great if C getenv/setenv had a major revision to be somewhat compatible with threading.)</p>
            </div>

            

            

          </div>
          <div id="post_94" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ings"><span itemprop="name">ings</span></a>
                (Christoph)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:19:25Z">
                    June 27, 2023,  8:19pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:19:25Z">
              <span itemprop="position">94</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I just want to throw in a use case which has not yet been discussed here and in the discussions of PEP 703: GUI toolkits.</p>
<p>GUI toolkits are naturally using threads and therefore an approach where free threading is replaced by a different concept like sub interpreters or multiprocessing is problematic in the design &amp; architecture of GUI applications written in python (because the toolkits being exposed in python are not aware of such concepts and most likely offer plain threading for offloading computational workload from the GUI). I’m a user of the PySide (Qt for python) project, and the PySide devs did struggle with the GIL as explained here <a href="https://www.qt.io/blog/qt-for-python-5.15.0-is-out" rel="noopener nofollow ugc">Qt for Python 5.15.0 is out!</a> (and also the links inside the document).</p>
<p>The problem of when it’s better to release or not to release the GIL in the C extensions of GUI toolkits is not straightforward, sometimes counter-intuitive (at least to me) and often there is a compromise involved depending on most common use cases but with drawbacks for other use cases.</p>
<p>Moreover, when creating GUI applications in python, you start struggling with the GIL when you have huge workloads happening in the background. My use case is a computer vision GUI which acts as a monitor and development environment for remote embedded systems. It is similar to what <a href="https://discuss.python.org/u/lunixbochs">@lunixbochs</a> reported for the realtime audio use case - keeping latencies and stutters on an acceptable level is unnecessarily difficult when you have to fight against the GIL mechanism.</p>
            </div>

            

            

          </div>
          <div id="post_95" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:34:59Z">
                    June 27, 2023,  8:34pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:34:59Z">
              <span itemprop="position">95</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Wouldn’t this be a potential problem today? “Not thread-safe” doesn’t mean “not thread-safe only when used in a free-threaded environment.”</p>
<p>I’m not trying to be difficult, maybe a bit pedantic. The presence of the GIL can obscure threading bugs or make them rear their ugly heads less often, but it doesn’t make code thread-safe. <a href="https://discuss.python.org/u/colesbury">@colesbury</a>’s work to remove the GIL has done a lot to remove places in the interpreter, stdlib, and some third-party libraries that relied on the GIL (knowingly or not). Most (all? almost all?) of that work will have been in C code, not Python code.</p>
            </div>

            

            

          </div>
          <div id="post_96" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-28T20:06:44Z">
                    June 28, 2023,  8:06pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-28T20:06:44Z">
              <span itemprop="position">96</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I think it could be a problem today, don’t know, I have the same question. I’m here to be curious.</p>
<p>Again as an interesting anecdotal data point here is a port of that troublesome C code to Python: <a href="https://gist.github.com/bluss/a3d2ad94d55382f682897ee1efae6d74" rel="noopener nofollow ugc">mtenv.py</a><br>
Here’s how it runs:</p>
<ul>
<li>Using Python 3.11.3 this program loops for a long time without problem</li>
<li>I compiled and used nogil-3.12 commit 4526c07caee8f2e (current tip of the repo)<br>
and it runs 1-2 seconds before it <strong>segfaults</strong> in getenv just like the C code from 2017.</li>
</ul>
<p>This is a reduced example. It doesn’t look like a normal Python program, it has a strange shape so that it can reproduce a crash easily. But the fundamental elements can occur in normal Python programs - various C calls that libraries use that use getenv - let’s say mktime to use the example from that blog post - and for setenv we have plain interface to it in <code>os</code> (<code>os.getenv</code> is <em>not</em> a plain interface to C <code>getenv</code>).</p>
            </div>

            

            

          </div>
          <div id="post_97" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Yes, it is a problem today, without free threading. getenv + setenv thread safety is a problem for Python applications I run at work. We had to do a bunch of whackamole to work around segfaults resulting from extension libraries using getenv + setenv (for a while we gave up and used a terrible <code>LD_PRELOAD</code> hack)</p>

            

            

          </div>
          <div id="post_98" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/gpshead"><span itemprop="name">gpshead</span></a>
                (Gregory P. Smith)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:46:24Z">
                    July 7, 2023, 11:46pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:46:24Z">
              <span itemprop="position">98</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>&amp;</p>

<p>At the high level this is the kind of thing I’d <strong>love someone to try creating</strong> for per-subinterpreter-GIL use! This is also quite hard, but I assume there are interested folks out there.</p>
<p>Intuitively I <em>expect</em> this winds up being the same problem that needs to be solved for free threading <em>(which PEP-703 appears to do)</em>: our pure reference counting model is the most significant reason we have a GIL - in order to share objects between multiple threads you need to make the reference counts work without that single lock.</p>
<p>Someone really needs to try creating explicitly shared objects implementation for CPython and subinterpreters to prove or disprove it’s actual utility. In the absence of that, I wouldn’t point to it and suggest it is a <em>better</em> solution. I consider it an open avenue of future work. <em>(Even if we get free threading, performant explicit sharing would be something I expect many would appreciate having.)</em></p>
            </div>

            

            

          </div>
          <div id="post_99" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/carljm"><span itemprop="name">carljm</span></a>
                (Carl Meyer)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:53:54Z">
                    July 7, 2023, 11:53pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:53:54Z">
              <span itemprop="position">99</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>We’ve had a chance to discuss this internally with the right people. Our team believes in the value that nogil will provide, and we are committed to working collaboratively to improve Python for everyone.</p>
<p>If PEP 703 is accepted, Meta can commit to support in the form of three engineer-years (from engineers experienced working in CPython internals) between the acceptance of PEP 703 and the end of 2025, to collaborate with the core dev team on landing the PEP 703 implementation smoothly in CPython and on ongoing improvements to the compatibility and performance of nogil CPython.</p>
            </div>

            

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scams upon scams: The data-driven advertising grift (228 pts)]]></title>
            <link>https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</link>
            <guid>36643630</guid>
            <pubDate>Sat, 08 Jul 2023 12:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/">https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</a>, See on <a href="https://news.ycombinator.com/item?id=36643630">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6748">
	
	<!-- .entry-header -->

	<div>
		
<p>Digital advertising is a scam from top to bottom. In fact, it’s several scams stacked on top of each other, wearing a trenchcoat, and some of the foundations of fibs are so effective that otherwise reasonable people entirely buy into them. </p>



<h2>Data-driven ads are anything but</h2>



<p>I’ll start with a few examples of the data which is definitely held on me, and just how entirely bad my targeted advertising is. </p>



<p>Facebook know my age and date of birth. They have had this data since I signed up for the website, 15 years ago. They know exactly how old I am. They also know where I live. Hell, sometimes I used to check into places with my location on. Despite knowing I am way north of 30 and way south of Birmingham, they are incredibly keen on advertising me events explicitly limited to people under the age of 30 in the Birmingham area. </p>



<p>Google knew I wanted to buy a mattress. They knew this because I googled it. And I clicked through to a brand selling mattresses, and I bought myself a mattress. The brand know I googled said mattress. Google know I clicked through. From Google’s own analytics, they ought to know I bought the mattress. Since buying that mattress, I’ve been constantly advertised mattresses, especially the one I already own and they know I already own. </p>



<p>Some might claim that in fact the advertisers are being incredibly smart and they’re advertising me activities for women under 30 in Birmingham so I go and tell my friends who are under 30 in Birmingham to go and do that. But of course, Facebook would also know that I don’t have any friends in that demographic. Or maybe that mattress seller is trying to tell me to refer a friend to buy that mattress by reminding me that I own a very nice mattress. In which case, why isn’t it advertising the referral programme, which I know they have because I received several emails and a physical leaflet about it with the fucking mattress?</p>



<p>The more simple answer is that the advertisers aren’t being data driven at all. They’re ticking default boxes or casting wider nets. I’m getting advertised mattresses because I have ~an interest in mattresses~. I’m getting activities for women under 30 in Birmingham because I’m under 40 and on the same island as Birmingham.</p>



<p>For all the buzzwords about “data-driven” and “smart” and whatever else you want to call it, the advertisers are just going “eh, sounds about right” and letting a robot automate their job. </p>



<p>This, then, is the first grift in the chain. Despite claiming to their boss that they’re using “data-driven” advertising, they’re targeting their ads even less than taking out a quarter page in the local newspaper. </p>



<h2>The product: they could spy on you (but don’t)</h2>



<p>Everyone is rightly nervy about the sheer quantity of data that big companies hold on us. Social media companies know all about your demographic information, social connections and interests. Amazon knows exactly when you have an outbreak of aphids because you buy things to kill the nasty little beasties, and it probably also knows when you’ve had a nasty breakup because nobody listens to Fleetwood Mac’s Rumours on repeat at 3am when they’re in a good place. Google basically knows everything about you. </p>



<p>At least that’s the theory. And that’s the product that they’re selling to advertisers. They have an enormous dataset from which everything an advertiser could ever dream of about a person can be garnered. They’re the world’s biggest, bestest spy network, which means they have quality data to help <em>your</em> business be the biggest, bestest business reaching the biggest, bestest customers.</p>



<p>At least that’s what they say. </p>



<p>Actual spying requires actual spies. There’s a reason intelligence agencies are such big employers: they have all of their fancy spy computers, but they know they need to hire humans to actually deduce patterns and sort signal from noise. They’re aware that a human brain is always superior to a computer in figuring this out, so they get humans to do the work.</p>



<p>Meanwhile, tech companies break into hives at the thought of getting a human to do a job. Their ethos is that if a human can do a task, a machine can do that task better, and not cost them anything such as salary, pensions or or a basic level of respect. Tech companies are fatally allergic to getting a human to do a human job, so content moderation is largely an algorithm looking for the word “boobies”. A tech company would go into anaphylactic shock at the very notion of employing a human to analyse their vast dataset.</p>



<p>So it’s all machine learning, and the machines are very, very stupid. Have you ever looked at your list inferred interests on a social media platform? If you ever tweeted “I don’t like Game of Thrones, it’s not for me,” you’ll be classified as interested in Game of Thrones and possibly get served ads for it. These machines may also attempt to deduce your age, gender, and so forth based on half-baked crap fed into them, and it seldom comes up right. Maybe that’s why it thinks I’m under 30 and in Birmingham. Perhaps I internet in a Brummie accent. </p>



<p>It’s no wonder that on multiple occasions, big tech has been caught out completely making things up when communicating with advertisers, and they continue to do so. Facebook was famously found to have inflated or outright fabricated video metrics. GA4 very quietly admits that the data is padded out with machine learning. The data is a lie, and a lot of it is because they literally haven’t the first clue on what to do with it, they just need to steeple their fingers and act all evil so advertisers think they have it.</p>



<p>Advertisers, then, are getting served a steaming turd on a plate rather than the medium-rare filet mignon they were promised. </p>



<p>And meanwhile, the spies don’t even need that data, because your posts are public anyway.</p>



<p>But enough about that. The problem is this grift is, too, built upon a grift. </p>



<h2>Marketing science is a grift</h2>



<p>I work in marketing, for my sins. This is mostly why I’m so entirely down on the marketing industry and many of the people who work in it. I also happen to have an MSc in psychology – actual psychology! – with a focus on behaviour change. </p>



<p>On day 1 of your class about behaviour change in a science course, you learn that behaviour change is not a simple matter of information in, behaviour out. Human behaviour, and changing it, is big and complex. </p>



<p>Meanwhile, on your marketing courses, which I have had the misfortune to attend, the model of changing behaviour is pretty much this: information in, behaviour out.</p>



<p>The thing with the entire “science” of marketing is the underpinning theory base is basic common sense which has been treated with a bit of a brand makeover, turned into a couple of overcomplicated diagrams with some neologisms obscuring meaning. Digital marketing has become very popular because baked into it are a whole bunch of metrics so you have something to show your manager that you’re not spending the entire day tending your geraniums, but do the metrics really mean anything?</p>



<p>The metrics that marketers are told they need are marketed to them by the marketing department of a company that specialises in making products for marketers. And that company was probably started up by someone who worked in marketing. </p>



<p>Marketing theory is never tested rigorously. The <s>common sense</s> incredibly sound scientific view based on heaps of scientific evidence view – showing your ads to people more likely to buy your product is more efficient because they’re more likely to buy your product anyway – is entirely untested.</p>



<p>There’s an anecdote that a glitch with Facebook led to ads no longer being targeted over a period of several weeks. And absolutely nobody noticed because the metrics all looked normal, the engagement and purchasing was just the same.</p>



<p>There isn’t any evidence to suggest that an ad targeted to 35 year old men with children with an interest in football is any more likely to result in sales of Football Dad socks than a poster for Football Dad socks at a bus stop. But an entire industry is based on pretending that this is the case.</p>



<h2>tl;dr</h2>



<p>Facebook will try to sell you Football Dad socks even if you’re a 55 year old childfree woman who posted once about hating football, because that data is utterly useless. </p>



<p>Spies are probably reading your posts though, no matter how boring.</p>



<p>_</p>



<p><em>Enjoyed what you read? Consider&nbsp;<a href="https://www.patreon.com/user?u=2332646">becoming a Patron&nbsp;</a>or&nbsp;<a href="https://www.paypal.me/stavvers">leave a tip</a></em></p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Farmer ordered to pay $CAD82k after confusion over meaning of thumbs-up emoji (123 pts)]]></title>
            <link>https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514</link>
            <guid>36643278</guid>
            <pubDate>Sat, 08 Jul 2023 11:04:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514">https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514</a>, See on <a href="https://news.ycombinator.com/item?id=36643278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="ArticleWeb"><header></header><div data-component="LayoutContainer" id="body"><div><p>A Canadian judge has ordered a farmer to pay&nbsp;more than $CAD82,000 ($92,000) in damages following a legal battle over what the thumbs-up emoji means.</p><p>Chris Achter, the owner of a farming company in Swift Current, Saskatchewan, had sent a thumbs-up emoji in response to a photograph of a flax-buying contract from a grains buyer in 2021.</p><p>Months later, the buyer — which&nbsp;had been doing business with Mr Achter for several years — did not receive the flax as expected.</p><p>That started a dispute that led to "a far-flung search" to unearth what the thumbs-up emoji means, according to the June court ruling that surfaced in local media this week.</p><p>The buyer, South West Terminal, argued that the emoji implied acceptance of contractual terms, while Mr Achter said he used it only to indicate that he had received the contract, but not to indicate his agreement.</p><p>In a summary judgement that contained 24 instances of the emoji, Judge T J&nbsp;Keene resolved the issue by ruling that a&nbsp;thumbs-up emoji is enough to&nbsp;accept contractual terms.</p><p>He said: "I am satisfied on the balance of probabilities that Chris okayed or approved the contract just like he had done before except this time he used a thumbs-up emoji."</p><p>"In my opinion the signature requirement was met by the thumbs-up emoji originating from Chris and his unique cell phone," the judge said.</p><p><strong>Reuters</strong></p></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-08T10:25:05.000Z">7 hours ago</time><time data-component="Text">Sat 8 Jul 2023 at 10:25am</time></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am drowning in mutes: The current Threads experience (150 pts)]]></title>
            <link>https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help</link>
            <guid>36642932</guid>
            <pubDate>Sat, 08 Jul 2023 09:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help">https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help</a>, See on <a href="https://news.ycombinator.com/item?id=36642932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>After 2 days of Threads, my muted list has grown from empty to this.</p><p><span>An unholy concoction of celebrities, meme pages, and brands, making painfully unfunny&nbsp;</span><a href="https://www.theverge.com/2023/7/6/23784842/threads-instagram-tweets-post-name-thread" rel="nofollow ugc noopener">tweets</a><span>&nbsp;form the majority of this list, with religious groups, "gurus", and meta (no, not Meta, people just talking about the platform itself) tweets forming the rest.</span></p><p>This too, is true of the algorithmic feed. While Instagram Reels has an uncanny ability to focus in on what you enjoy (to the point where staring at a tractor meme for too long has caused my feed to orbit farming Reels), Threads shares none of this ability. A blast of whatever content the platform has, with little thought.</p><p>It seemed that you could batter it into submission by mass muting anything unfunny - but then a new wave of celebrities and brands appears. Keeping up becomes an impossible challenge.</p><p>And thus, unless this situation resolves itself in another way, I will eventually grow tired of muting people, and leave the platform.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bevy XPBD: A physics engine for the Bevy game engine (133 pts)]]></title>
            <link>https://joonaa.dev/blog/02/bevy-xpbd-0-1-0</link>
            <guid>36642867</guid>
            <pubDate>Sat, 08 Jul 2023 09:41:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joonaa.dev/blog/02/bevy-xpbd-0-1-0">https://joonaa.dev/blog/02/bevy-xpbd-0-1-0</a>, See on <a href="https://news.ycombinator.com/item?id=36642867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>I’ve been working on a Rust-based physics engine for about a year now, and a week ago I finally released it. Here is <a href="https://github.com/Jondolf/bevy_xpbd">Bevy XPBD</a> 0.1.0!</p>
<h2 id="what-is-bevy-xpbd">What is Bevy XPBD?</h2>
<p>Bevy XPBD is a 2D and 3D physics engine for <a href="https://bevyengine.org/">Bevy</a>, a refreshingly simple data-driven game engine built in Rust.</p>
<p>It uses a newer physics simulation method called <em>Extended Position Based Dynamics</em>, which provides unconditionally stable, time step independent, and physically accurate simulations. Unlike other physics engines in the Bevy ecosystem, Bevy XPBD is made specifically <em>for</em> Bevy <em>with</em> Bevy, and it uses the <em>Entity Component System</em> (ECS) for both the public API and the internals.</p>
<p>Bevy XPBD 0.1 already has tons of features, including these:</p>
<ul>
<li>Dynamic, kinematic and static rigid bodies</li>
<li>Collision detection and collision response</li>
<li>Collision events</li>
<li>Access to colliding entities</li>
<li>Sensor colliders</li>
<li>Collision layers</li>
<li>Restitution and friction</li>
<li>Gravity, external forces and torque</li>
<li>Joints</li>
<li>Built-in XPBD constraints and support for custom constraints</li>
<li>Modular plugin architecture, allowing you to swap existing functionality with custom implementations</li>
<li>Configurable timestep and subtepping</li>
</ul>
<h2 id="getting-started">Getting started</h2>
<p>If you are new to Rust or Bevy, you should go through Bevy’s <a href="https://bevyengine.org/learn/book/getting-started/">Getting started guide</a>.</p>
<p>Once you are ready, you need to add Bevy XPBD to your <code>Cargo.toml</code>. You should use <code>bevy_xpbd_2d</code> for 2D projects and <code>bevy_xpbd_3d</code> for 3D projects.</p>
<pre is:raw="" tabindex="0"><code><span><span>[</span><span>dependencies</span><span>]</span></span>
<span><span>bevy_xpbd_3d = </span><span>"0.1.0"</span></span></code></pre>
<p>By default, Bevy XPBD uses <code>f32</code> numbers. If you encounter instability or use a large number of substeps, you might want to use <code>f64</code> instead. You can disable the default features and manually specify the feature flags you want:</p>
<pre is:raw="" tabindex="0"><code><span><span>[</span><span>dependencies</span><span>]</span></span>
<span><span># Add 3D Bevy XPBD with double-precision floating point numbers</span></span>
<span><span>bevy_xpbd_3d = { version = </span><span>"0.1"</span><span>, default-features = </span><span>false</span><span>, features = [</span><span>"3d"</span><span>, </span><span>"f64"</span><span>] }</span></span></code></pre>
<p>Next, add the <code>PhysicsPlugins</code> plugin group. Bevy XPBD is highly modular, and this plugin group adds all of the default physics plugins to your application.</p>
<pre is:raw="" tabindex="0"><code><span><span>use</span><span> </span><span>bevy</span><span>::</span><span>prelude</span><span>::*</span><span>;</span></span>
<span><span>use</span><span> </span><span>bevy_xpbd_3d</span><span>::</span><span>prelude</span><span>::*</span><span>;</span></span>
<span></span>
<span><span>fn</span><span> </span><span>main</span><span>() {</span></span>
<span><span>    </span><span>App</span><span>::</span><span>new</span><span>()</span></span>
<span><span>        </span><span>.</span><span>add_plugins</span><span>(</span><span>DefaultPlugins</span><span>)</span></span>
<span><span>        </span><span>.</span><span>add_plugins</span><span>(</span><span>PhysicsPlugins</span><span>)</span></span>
<span><span>        // ...your other plugins, systems and resources</span></span>
<span><span>        </span><span>.</span><span>run</span><span>();</span></span>
<span><span>}</span></span></code></pre>
<p>Now you can use all of Bevy XPBD’s components and resources to build whatever you want! For example, adding a rigid body with a collider is as simple as spawning an entity with the <code>RigidBody</code> and <code>Collider</code> components:</p>
<pre is:raw="" tabindex="0"><code><span><span>fn</span><span> </span><span>setup</span><span>(</span><span>mut</span><span> commands</span><span>:</span><span> </span><span>Commands</span><span>) {</span></span>
<span><span>    commands</span><span>.</span><span>spawn</span><span>((</span><span>RigidBody</span><span>::</span><span>Dynamic</span><span>, </span><span>Collider</span><span>::</span><span>ball</span><span>(</span><span>0.5</span><span>)));</span></span>
<span><span>}</span></span></code></pre>
<p>To learn more, refer to the <a href="https://docs.rs/bevy_xpbd_3d/0.1.0/bevy_xpbd_3d/">official documentation</a> and check out the <a href="https://github.com/Jondolf/bevy_xpbd">GitHub repository</a> and the <a href="https://github.com/Jondolf/bevy_xpbd/tree/main/crates/bevy_xpbd_2d/examples">2D</a> and <a href="https://github.com/Jondolf/bevy_xpbd/tree/main/crates/bevy_xpbd_3d/examples">3D</a> examples.</p>
<h2 id="why-make-a-physics-engine-for-bevy">Why make a physics engine for Bevy?</h2>
<p>I started development purely out of interest. I had been playing around with Bevy for a little while when I saw a <a href="https://johanhelsing.studio/posts/bevy-xpbd">tutorial series by Johan Helsing</a> where he created a simple XPBD physics engine for Bevy from scratch. I had seen a <a href="https://www.youtube.com/watch?v=F0QwAhUnpr4">Two Minute Papers video</a> on the topic earlier, so I decided to follow the tutorial and build along.</p>
<p>The tutorial series wasn’t/isn’t finished yet, so I quickly reached the end. I had 2D objects colliding realistically with each other, but lots of features were missing: friction, 3D support, joints, and a lot of other common physics engine capabilities. I decided to continue developing the engine, reading academic papers on XPBD and taking inspiration from existing physics engines like <a href="https://rapier.rs/">Rapier</a>.</p>
<p>As I continued adding features and improving the API, I realized that I was actually starting to have a decently functional physics engine. I started seeing some interest in my engine on the <a href="https://discord.gg/bevy">Bevy Discord server</a>, and I realized that this was truly a lacking area in the ecosystem. Bevy doesn’t have an official physics engine yet, and <code>bevy_rapier</code> is essentially the only good option, but it’s just a wrapper over Rapier and doesn’t use Bevy’s ECS for any of its internals. People would prefer a more native solution that is made specifically for Bevy and uses the ECS directly instead of relying on a separate physics world and synchronizing a monolithic data structure with the Bevy world.</p>
<p>This is what I am trying to accomplish with Bevy XPBD. As far as I know, it is the only pure Bevy ECS physics engine that is currently maintained, and I hope to continue on this road to help improve the state of physics in the Bevy ecosystem.</p>
<h2 id="whats-next">What’s next?</h2>
<p>Bevy XPBD is already quite usable in many cases, but it is missing several important features, and there are some stability and performance issues. Keep in mind that the engine is very young and physics engines can be quite massive, so it can take a while to reach a production ready state.</p>
<p>0.2 will be released in a couple of weeks, right after the release of Bevy 0.11. Here are some of the features and improvements that I have in mind:</p>
<ul>
<li>Update to Bevy 0.11</li>
<li>Spatial queries
<ul>
<li>Ray casting</li>
<li>Shape casting</li>
<li>Point projection</li>
</ul>
</li>
<li>Filters for collisions and spatial queries (exclude entities etc.)</li>
<li>Multiple colliders per body</li>
<li>Collider offset and scale</li>
<li>Bug fixes and documentation improvements</li>
</ul>
<p>Further into the future, I am planning on implementing some of these features:</p>
<ul>
<li>Physics scale (e.g. pixels per meter)</li>
<li>Joint motors</li>
<li>Soft bodies and cloth simulation</li>
<li>Locking translational and rotational axes</li>
<li>Parallel solver to improve performance</li>
<li>Full cross-platform determinism</li>
<li>A website for WASM demos</li>
</ul>
<p>Note that after 0.2, development might be a lot more inconsistent for a while because of my studies, as I will be graduating from high school next year, and I will have my matriculation exams in September and March. However, I will try to keep up with Bevy releases and respond to issues and questions, and I hope to work on the engine more actively once I am done with my exams.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Huge thanks to everyone on the <a href="https://discord.gg/bevy">Bevy Discord server</a> who has been interested in the project and helped me get this far, and especially to <a href="https://johanhelsing.studio/">Johan Helsing</a> who inspired me to originally begin development and later helped me with the project. Without your help I would have probably stopped development and abandoned the project, and I am very glad I didn’t.</p>
<p>It’s crazy to me how we have reached over 120 stars on GitHub just a week after release, and I think it just shows how much the community wants a native physics engine that is made specifically for Bevy in an ergonomic ECS-based way.</p>
<p>I hope to continue working on Bevy XPBD as much as I can, and to continue being a part of the amazing community. Stay tuned for the next update in a couple of weeks when 0.2 is released!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ARM powered ThinkPad x13s, as a developer (106 pts)]]></title>
            <link>https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/</link>
            <guid>36642825</guid>
            <pubDate>Sat, 08 Jul 2023 09:34:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/">https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/</a>, See on <a href="https://news.ycombinator.com/item?id=36642825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        

            <p>The ARM powered Thinkpad x13s is the first truly premium Windows on ARM laptop.  Let's see how that actually stacks up as a software engineer!</p>

        <section>

            <ul>
                <li>
                    <a href="https://www.dev-eloper.com/author/devin/"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"></path></g></svg>
</a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2023-06-24">Jun 24, 2023</time>
                        <span><span>•</span> 13 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://www.dev-eloper.com/content/images/size/w300/2023/06/Thinkpadx13s_header2.jpg 300w,
                            https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_header2.jpg 600w,
                            https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_header2.jpg 1000w,
                            https://www.dev-eloper.com/content/images/size/w2000/2023/06/Thinkpadx13s_header2.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://www.dev-eloper.com/content/images/size/w2000/2023/06/Thinkpadx13s_header2.jpg" alt="Picture of my Thinkpad x13s">
            </figure>

    </header>

    <section>
        <h3 id="the-long-and-winding-intro-that-leads-to-your-door">The long and winding intro (That leads to your door)</h3><p>So, you're a software engineer who loves Windows (We exist, I swear). &nbsp;More than that, you're a software engineer who loves being able to work detached from a desk, free to work in coffee shops, on busses, out at a park, or really wherever your heart desires at any given moment. &nbsp;You know you COULD switch to a Mac laptop, such as the Macbook Pro with an M-series chip, to get that sweet, sweet battery life you desired, but either you need to work in the Windows ecosystem or maybe like me you just don't like MacOS. &nbsp;What do you do?</p><p>So, you start by looking around at laptops. &nbsp;Microsoft's own Surface Laptop Studio sure sounds slick, but the battery life is atrocious. &nbsp;Same for the Framework laptop, even though you'd love to support the company. &nbsp;There are some Asus laptops that seem like they'd fit the bill, but they're just so clunky and large, which gets in the way of the portable dream. &nbsp;That's when you see it: &nbsp;A new Lenovo that claims insane 28-hour battery life, all in a thirteen-inch form factor. &nbsp;That is EXACTLY what you need!</p><p>You order it without reading too much into it and wait for it to show up. &nbsp;The day finally arrives, and the small brown box opens to reveal a slick little black-ish (Maybe more of a dark gray) laptop, a 65-watt USB-C charging brick, and precious little else. &nbsp;You boot it up, set up your account, and start looking through the specs. &nbsp;32GB of RAM, Snapdragon 8cx Gen 3 processor, 1TB NVMe SS- wait what was that last one?</p><p>Oh right, as it turns out this slick little device runs Windows on ARM on the latest Snapdragon processor, the Snapdragon 8cx Gen 3. &nbsp;"Well fuck," you say disappointedly, "That might be a problem. &nbsp;Do my development tools even run on this OS? &nbsp;What about git? &nbsp;Am I going to actually be able to compile code on this?"</p><p>Well fear not, imaginary developer, for I have already bought this device to use as my own development machine, and I want to share with you what I've discovered in my exploration of the device. &nbsp;While this isn't my first WoA powered device (I've been a long-time user of the SQ1 Surface Pro X and the Galaxy Book2 before that) this is the first one I've really bought for the express purpose of software engineering. &nbsp;I want to dive into the pros, the cons, and just my overall impressions about the Thinkpad x13s, Windows on ARM in general, and life as a developer on one of these machines.</p><h3 id="introductory-statements">Introductory statements</h3><p>A few things to note first and foremost:</p><ol><li>I primarily work in the JVM language space. &nbsp;My preferred language (and the one I use in a professional capacity) is Kotlin. &nbsp;I also have over a decade of professional Java development under my belt. &nbsp;The JVM and its languages are what I know and where I do my work. &nbsp;I will do my best to speak to development using Windows native languages like the .net suite, but my scope there is limited.</li><li>I enjoy using Windows, but I'm not tied to it. &nbsp;Most of my professional life involves Linux rather than Windows, and most of my engineering effort takes place in WSL2. &nbsp;That being said, I spent many years before WSL ever launched running everything on just Windows.</li><li>Straightup, I very much enjoy this laptop, but I will not mince words with the issues I have with it. &nbsp;There are a number of glaring issues with the device that people need to be aware of, and most reviewers looking at devices like this are not software engineers and will not run into these issues.</li><li>This is a long-term review, following nearly a year with my own personal Thinkpad x13s device.</li></ol><p>OK, now that I've gotten a lengthy and boring intro out of the way, along with some overly long introductory statements, let's start actually reviewing this thing.</p><h3 id="about-my-device">About my device</h3><p>Ah, ok, wait, no hold on, one more thing, we should probably actually go over what device specs we're looking at here so we can be clear about what we're talking about. </p><!--kg-card-begin: markdown--><table>
<thead>
<tr>
<th>Component</th>
<th>Spec</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Snapdragon 8cx Gen 3 (3.0 GHz)</td>
</tr>
<tr>
<td>Storage</td>
<td>512GB SSD M.2 2242 PCIe Gen4 TLC Opal</td>
</tr>
<tr>
<td>Memory</td>
<td>32GB LPDDR4x 4266MHz (Dual Channel, Soldered)</td>
</tr>
<tr>
<td>Battery</td>
<td>49.5 Whr</td>
</tr>
<tr>
<td>Display</td>
<td>13.3" WUXGA (1920 x 1200), IPS, Anti-Glare, Touch, 72%NTSC, 300 nits, 60Hz, LED Backlight</td>
</tr>
<tr>
<td>Graphics</td>
<td>Integrated Qualcomm Adreno 690 Graphics</td>
</tr>
<tr>
<td>Camera</td>
<td>5MP RGB+IR with Microphone</td>
</tr>
<tr>
<td>WLAN</td>
<td>Qualcomm Wi-Fi 6E WCN6855 2x2 AX &amp; Bluetooth 5.1</td>
</tr>
<tr>
<td>OS</td>
<td>Windows 11 Pro on ARM</td>
</tr>
</tbody>
</table>
<!--kg-card-end: markdown--><h3 id="hardware">Hardware</h3><p>OK, let's talk about the hardware feel and build quality of this thing. &nbsp;The x13s chassis is made of 90% recycled magnesium and covered with a soft-touch black paint (I think? &nbsp;Maybe a polycarbonate?) layer. &nbsp;The keyboard has very little flex and is reasonably sized. &nbsp;The laptop screen is a 300-400 nit matte touch screen with a pair of hinges that very easily pass the one-handed open test.</p><p>The keyboard itself is very similar to the Macbook Air, both in size and feel. &nbsp;As is typical with Thinkpads, the track point and physical mouse buttons join the keyboard layout, which is useful for a certain kind of person. &nbsp;Personally, I prefer the touchpad, which is of a reasonable size. &nbsp;Certainly, the touchpad could be bigger, but the aforementioned physical mouse buttons take up space below the keyboard that could otherwise be taken up by a bigger trackpad. &nbsp;Additionally, the trackpad is just slick plastic rather than glass, which is a shame especially considering the price tag of the laptop, but we'll come back to that.</p><p>The screen on mine is the 300-nit touchscreen variant, which runs at 60Hz. &nbsp;Color-wise it's. . . fine, covering only 72% of the NTSC color gamut. &nbsp;Definitely not amazing, it's not something I'd ever recommend to someone who is doing photo or video editing. &nbsp;The 60hz refresh rate <em>feels</em> like a 60Hz refresh rate. &nbsp;If you've ever used a 90 or 120Hz display, you'll know what I mean. &nbsp;As with the plastic trackpad, the price of this laptop really demands better on this front.</p><p>In terms of I/O, we have two USB-C 3.2 Gen 2 ports (with display out), a 3.5mm headphone jack, a Kensington lock slot, and an optional SIM slot for the on-board 5G radios. &nbsp;I don't have the 5G enabled version as it wasn't available when I bought mine, which I do regret, but if it's even half as good as it is on the SPX then it'll be totally fine. Additionally, the onboard wifi is 6E which is a nice addition. &nbsp;All in all this isn't <em>terrible</em> I/O. &nbsp;People get too caught up in laptops missing USB-A ports, but for a travel-focused device I don't really see the appeal. &nbsp;Maybe it's my software engineer talking, but I haven't used a peripheral on a laptop that wasn't USB-C or Bluetooth since the mid 2010s.</p><p>In general, the laptop feels good to use. &nbsp;The magnesium chassis feels cool to the touch and solid at the same time. &nbsp;The keyboard is comfortable to type on with good key travel. &nbsp;The track point, mouse buttons, and trackpad are all good enough. &nbsp;The screen works fine in daylight, though the low nit count requires using light mode rather than dark mode to compensate. &nbsp;All in all, it's a nice little package.</p><figure><div><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_side.jpg" width="1024" height="768" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_side.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_side.jpg 1000w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_side.jpg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_header-2.jpg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_header-2.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_header-2.jpg 1000w, https://www.dev-eloper.com/content/images/size/w1600/2023/06/Thinkpadx13s_header-2.jpg 1600w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_header-2.jpg 2048w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_back.jpg" width="1024" height="768" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_back.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_back.jpg 1000w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_back.jpg 1024w" sizes="(min-width: 720px) 720px"></p></div></figure><h3 id="software-compatibility-and-emulation">Software compatibility and Emulation</h3><p>Easily the biggest question everyone is going to have about this laptop is "Can I run the software I need?" Given the ARM nature of the laptop that's a completely reasonable question to ask. &nbsp;I, obviously, cannot speak to every piece of software ever written, but I can talk about the good and the bad that I've seen.</p><p>We'll start with the restrictions. &nbsp;Unlike a traditional laptop, the x13s uses an ARM64 processor, meaning that nearly all software written for Windows in the last 30 years won't run natively on it. &nbsp;To compensate for this, Microsoft has two emulation layers in place to translate traditional x86 and x86_64 machine code to ARM64 machine code. &nbsp;There are limitations here, of course. &nbsp;Emulation is inherently slower than native code, there isn't any way around that. &nbsp;Secondly, the emulation layer is user-space only meaning drivers can't be emulated, so anything that relies on custom drivers is a no-go unless they've been recompiled for ARM. &nbsp;The final note is that OpenGL calls are translated to DirectX calls, but that only goes as far as OpenGL 3.3, and Vulkan is right out.</p><p>In terms of actual performance of the emulation, we're going to avoid hard numbers and talk more about "feel," because most of that time that's what you're going to experience. &nbsp;There are really 3 groups of programs that we need to talk about, and each one is going to have a different experience as they make use of different elements of the emulation. &nbsp;There are "office" applications, these are things like Slack, Outlook, or Chrome. &nbsp;Then there are "Performance" applications, for us software engineers these are going to be things like our IDE, compiler, database software, etc. &nbsp;Finally, there are "GPU-intensive" programs, which generally means games and photo/video editing, but can also mean GPU-reliant libraries like Tensorflow.</p><p>For "Office" applications, 8 times out of 10 you won't even realize you're not using a native program. &nbsp;Most of these applications don't rely on anything CPU or GPU-intensive and once the code is cached in the emulation layer it'll run basically without issue. &nbsp;However, those remaining two times are going to be <em>painful</em>. &nbsp;Chrome and Slack are great examples of this. &nbsp;Chrome is slow and frustrating to use, with delayed animations and noticeable input lag, it's such a frustration that I entirely gave up on Chrome in favor of Edge which runs natively (Though for those who want to avoid Chromium in general, Firefox also has native ARM64 Windows builds). &nbsp;Slack is frustrating in its own way, running fine one second, then freezing for 10-15 seconds at a time, usually when switching to a new conversation or channel. &nbsp;To make matters worse, Slack from the Microsoft Store installs as x86_64, which makes the problem significantly worse for whatever reason. &nbsp;If you need Slack either download the x86 version directly from their website, or run it as a browser tab.</p><p>"Performance" applications are more of a mixed bag. &nbsp;As a JVM focused developer, I primarily use IntelliJ as my IDE so I'm going to focus on that experience. &nbsp;Luckily, this works out because Jetbrains has, in the last few months, released ARM64-compatible versions of its products, including IntelliJ. &nbsp;As such I've used the x86 emulation version, a hacked together ARM64 solution using an ARM version of Java, and the fully native solution. &nbsp;The difference here is night and day between all three. &nbsp;The emulated version ran, and that's about as much as I'm willing to give it. &nbsp;It worked, you could open/compile using it, but it wasn't pleasant, and larger projects were noticably slower to interact with and navigate around. &nbsp;It also wasn't able to access WSL, meaning I was locked out of using Linux tooling all together. &nbsp; The hacked solution was immediately better, but like the emulated version, it wasn't able to access WSL, and it required a fair amount of extra setup to even get working that most users (even other software engineers) wouldn't have even known to do. &nbsp;The native solution solved these problems entirely and runs so well that you'd never know it wasn't an Intel/AMD machine.</p><p>But like I said, it's a mixed bag. &nbsp;Other tools, such as MySQL, node, git, etc. have no issue when running under emulation, at least in the context of development (I doubt I'd say the same if I was running production using emulation). &nbsp;Most of the time you won't even realize these are emulated! A good rule of thumb is "If there isn't a UI, the performance is going to be on-par with a native build."</p><p>Finally, "GPU-Intensive" applications are. . . &nbsp;not great. &nbsp;There does exist a Tensorflow library aimed at ARM platforms that you can import, but it's not official so you're at risk of that falling through. &nbsp;Other GPU intensive tasks like video and photo editing either lack support (I don't think there is a single high-quality video editor with Windows on ARM support) or are a nightmare to use (Photoshop has Windows on ARM support, but it's glitchy as hell). &nbsp;Games are almost entirely out, though I won't say 100% out. &nbsp;Starcraft 2 runs, but you wouldn't want to play beyond the 4 minute mark. &nbsp;Demeo on the other hand runs incredibly well to the point where they demoed the game at PAX on the x13s. At the end of the day the emulation works for GPU intensive apps, but not with enough consistency or performance to recommend those things.</p><p>Long story short, if you're using GPU heavy workflows you're probably going to be fine. &nbsp;More and more development tools are making their way over to being ARM native, but you should absolutely check if yours are before you commit. &nbsp;I should also mention that Visual Studio Code does have an ARM64 release, which would likely cover just about everybody in a pinch.</p><p>One thing I should note here is that this is really a Qualcomm problem rather than a Microsoft problem. &nbsp;The emulation layer is actually quite fantastic, and when you Windows on ARM on an M-series Mac you don't see most of these issues. &nbsp;Honestly, Microsoft has done an incredible job with their emulation layer, and I have to commend them on that.</p><h3 id="native-performance">Native Performance</h3><p>The performance of native applications, as I've already alluded, is totally fine. &nbsp;Geekbench and other benchmarks will tell you that the Snapdragon 8cx Gen 3 is at best on par with an anemic i5, but in practice I don't think I've ever noticed performance problems with native applications. &nbsp;Intellij, Edge, Spotify (Beta), WSA, WSL, and many other ARM compiled programs run without any issue. &nbsp;Far and away, my experience with the performance has been very positive, and hasn't impeded my software development experience in the slightest.</p><p>On top of that, more and more development tools are becoming available for Windows on ARM. &nbsp;For example, Java has had native builds for nearly 3 years now, and Node got Windows on ARM support as of Node 20. &nbsp;While the performance bottlenecks of these pipeline tools has been minimal, the more tools that move over, the better off we'll all be. &nbsp;As I mentioned before, Visual Studio Code has been ARM64 compatible since 2020, and Visual Studio since 2022, so between those and the Jetbrains suite of IDEs most every major language has at least one native IDE now.</p><h3 id="battery-life">Battery Life</h3><p>OK, so what about battery life? &nbsp;I started this whole post off repeating Lenovo's claim of a 28-hour battery life, so does it actually live up to that?</p><p>No, of course not, not in any meaningful way. &nbsp;As with all battery life claims, the 28 hours is under <strong><em>extremely specific</em></strong><em> </em>conditions. &nbsp;In the real world, though, the battery life is still impressive! &nbsp;Much of my job as an engineering manager is spent doing admin work in Jira and taking Zoom/Meet calls. &nbsp;On those days I get around 8-10 hours or so of battery life, though the video calls cut into that severely. &nbsp;On days I'm doing development work, I get closer to 5-6 hours. &nbsp;This is <em>significantly better</em> than the 2 hours of development work or 3 hours of admin-only work I get on my Framework (11th gen, 55wh battery). &nbsp; As with all things battery life, your mileage may vary, especially if you're toolchain is dependent on non-native code.</p><p>This might all sound like a kick in the teeth considering the 28-hour promised battery life, but it's a day an night difference against most Intel/AMD machines, and lines up with my experience doing dev work on an M1 Macbook Pro. &nbsp;It helps the device feel like a truly portable work device.</p><h3 id="issues">Issues</h3><p>Now it's time to talk about the less pleasant aspects of the x13s, though honestly it's nothing we haven't touched on before, and they all really come back to one thing: The Thinkpad x13s is <em>mad expensive.</em></p><p>How expensive, you ask? At the time of writing the cost of a same-spec x13s as mine has a price tag of $2466 USD. &nbsp;Bumping that up to a 1 TB SSD takes the total cost to $2898.00. &nbsp;That's a high price tag, even for high-end Intel/AMD Windows machines. &nbsp;The Surface Laptop Studio with an i7 and 32GB of RAM and 1 TB of space is $2699 (2199.99 on sale!). &nbsp;To compound that, the device lacks the high-end features one would expect from a device at this price point: &nbsp;Low max brightness, mediocre NTSC color gamut, plastic track pad — all of these are more in line with an $1200 price tag, not twice that!</p><p>Luckily (or perhaps shadily), you will rarely ever pay the full price for the Thinkpad x13s. &nbsp;As is tradition, Lenovo seems to be perpetually running deals, taking up to 40% off (at time of writing, 45% time of purchase), bringing the laptop to a much more reasonable $1479.60. &nbsp;That's still a high price tag for the device, but much closer to reasonable.</p><p>For a quick comparison, a Macbook Pro 14" with an M2 Pro processor, 1 TB of storage, and 32GB of RAM clocks in at $3099.00 (Edit: At some point after I published this, the prices reflected on Apple's site changed. &nbsp;This may have been due to some failure on my part when I wrote the article, but the accurate price for a 1TB 32GB RAM M2 pro is $2599.00. &nbsp;Thanks to <a href="https://news.ycombinator.com/user?id=justinclift&amp;ref=dev-eloper.com">justinclift</a> for the correction!). &nbsp;A refurbished Macbook Pro 14" with an M1 Pro and 1 TB of space comes in at $1999.00 at the low-end. &nbsp;While these undoubtedly out-perform the Thinkpad x13s, it's hard to argue that the performance difference for most users, even engineers, is worth the extra $500.</p><h3 id="round-up">Round-up</h3><p>So, where do we land here? &nbsp;We have a device with well above average battery life in real world conditions that, when running native code, is indistinguishable performance wise from your average Intel/AMD fair, and is perfectly capable of being a full-time software engineering machine for your average full stack web developer. &nbsp;It struggles with GPU tasks that aren't ARM optimized, lacks the power needed to really push through on more complicated emulation tasks, and lacks the premium fit and finish you would expect for a device that costs nearly $2500.</p><p>At the end of the day, this is a niche product that is best aimed at people who believe more in "Work from Anywhere" than "Work from Home." &nbsp;If you're a coffee shop warrior, or someone who believes that anywhere can be their office this might be an ideal companion if you'd rather not hop the OS divide to Apple. &nbsp;That's definitely the case for me, and I would happily recommend it to people who are tech-savvy enough to troubleshoot and work around issues, and patient enough wait for developers to produce native binaries when needed. &nbsp;But I will be the first to admit that I would not give this a <em>general</em> recommendation. &nbsp;There are enough rough edges left with Windows on ARM, especially when paired with the current Snapdragon series of processors, that it's still not the right solution for most.</p>
    </section>


</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software engineers hate code (325 pts)]]></title>
            <link>https://www.dancowell.com/software-engineers-hate-code/</link>
            <guid>36642796</guid>
            <pubDate>Sat, 08 Jul 2023 09:28:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dancowell.com/software-engineers-hate-code/">https://www.dancowell.com/software-engineers-hate-code/</a>, See on <a href="https://news.ycombinator.com/item?id=36642796">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>This is the best-kept secret of the software engineering profession: engineers hate code. Especially code written by other people. It's why they love working on greenfield projects so much. No code, no maintenance, no headaches!</p><p>Ever wondered why microservices took off in teams of all sizes? A microservice architecture is the perfect way to pretend that the code you wrote last month no longer exists! Now that it has been stuffed into a container and tucked behind a load balancer it's a <em>service</em> and we can forget all about it until it breaks, then we deprecate it as <em>legacy</em> and replace it with something new. Rolling green fields forever!</p><p>Got a question about how one of your dependencies works? You could look at its implementation, or the test suite, but most engineers prefer to go to the place that everyone congregates to talk about - but not look at - code. Stack Overflow is a great resource for finding the code needed to solve your problem without having to look at a lot of code yourself!</p><p>I'm sure you've been stuck for hours begging your colleagues to review your pull request. Why do you think it's taking so long? You're asking them to do the thing they hate most - look at someone else's code!</p><p>Most people reading this have at one point or another approved a non-trivial pull request with a simple "LGTM." You got tagged in on a bad day and didn't have the time to take a proper look at code written by someone else. You had work to do!</p><h2 id="only-one-thing-can-overcome-engineers-hatred-of-code-their-love-of-writing-code">Only one thing can overcome engineers' hatred of code: their love of <em>writing</em> code.</h2><p>Software engineers will lock themselves in a room and do nothing but write code for hours. Some forget to eat, sleep or poop.</p><p>Notable engineers online invest hours of time writing <em>about </em>their code, or about <em>how </em>they write code. Paradoxically, engineers love reading this stuff, even if they never read the example code attached!</p><p>Meetings of all kinds, technical and user documentation, testing, post-release monitoring, refactoring - all are common sources of frustration that cut into valuable time that could otherwise be spent writing code!</p><p>Engineers will spend enormous effort learning or building tools to help them write more code. In the past couple of years, we've seen an entirely new generation of tooling emerge that can actually write code by itself, turning 10x engineers into 1,000x engineers*!</p><figure><img src="https://www.dancowell.com/content/images/2023/07/use-ai.jpg" alt="" loading="lazy" width="1920" height="1080" srcset="https://www.dancowell.com/content/images/size/w600/2023/07/use-ai.jpg 600w, https://www.dancowell.com/content/images/size/w1000/2023/07/use-ai.jpg 1000w, https://www.dancowell.com/content/images/size/w1600/2023/07/use-ai.jpg 1600w, https://www.dancowell.com/content/images/2023/07/use-ai.jpg 1920w" sizes="(min-width: 720px) 720px"><figcaption>* Scientists observing engineers using AI in the wild have seen them writing 100x more SLOC/hour!</figcaption></figure><p>Rarely, you will encounter engineers who have learned to temper their baser instincts and instead find a sick kind of joy in reading, understanding, modifying and even <em>deleting</em> other peoples' code. We call these odd folks "Senior Engineers."</p><p>Senior engineers have learned through hard-won experience that writing code is the ultimate diminishing return.</p><p>They know that code becomes legacy the moment the first byte is saved to disk. The rolling green fields of their youth are a happy delusion, distracting from the cold, hard truth that all code demands maintenance. They have felt the pain of an unmaintained system breaking at the worst possible time.</p><p>There are a limited number of hours in the day. The more code that gets written, the more things there are to break, and more of those precious hours will be taken up by maintenance.</p><p>The only logical course of action is to minimize the amount of code in production at any given time. Senior engineers' passion for writing code has been augmented with an even stronger desire to delete it.</p><p>Code that doesn't exist can't hurt us, or the people we love.</p><p>It demands no maintenance; it causes no downtime; it requires no testing. Senior engineers understand that unnecessary code should be eliminated at all costs, and all new code must prove its worth before being allowed to live. This is part of what drives them to pore over other peoples' code and provide meticulous review.</p><p>This isn't to say that senior engineers are cynics. There's still beauty to be found in creating an elegant solution to a complex problem. The joy of creation hasn't diminished, but it has been tempered by an understanding that less is more, and that every line of code they write comes at a cost.</p><p>Senior engineers hate <em>extraneous</em> code. They hate seeing time and effort invested in building yet another solution to an already-solved problem. They hate code that doesn't <em>need</em> to exist; code that isn't providing value.</p><h2 id="be-mindful-of-the-cost-that-your-code-incurs">Be mindful of the cost that your code incurs.</h2><p>Don't write new code when you can use, improve or fix what already exists. If you must write new code, write only what you need to get the job done.</p><p>Understand your tools, and the systems your code runs on. Leverage the features of those systems to minimize the code you need to write, and by extension, the cost that it imposes on you and your team.</p><div><p>💡</p><div><p>Some artistic liberties have been taken in this post for the sake of the narrative, however the qualities described above reflect attitudes I've witnessed in engineers that I've worked with in the past.</p><p>There's more to being a senior engineer than just a healthy skepticism about writing code to solve a problem, but it's an important quality to develop, and will serve any engineer well.</p></div></div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Train an AI model once and deploy on any cloud (182 pts)]]></title>
            <link>https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/</link>
            <guid>36642315</guid>
            <pubDate>Sat, 08 Jul 2023 07:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/">https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/</a>, See on <a href="https://news.ycombinator.com/item?id=36642315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Organizations are increasingly adopting hybrid and multi-cloud strategies to access the latest compute resources, consistently support worldwide customers, and optimize cost. However, a major challenge that engineering teams face is operationalizing AI applications across different platforms as the stack changes. This requires MLOps teams to familiarize themselves with different environments and developers to customize applications to run across target platforms.</p>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. You can then deploy that AI application on any GPU-powered platform without code changes.<strong></strong></p>



<h2>Introducing the latest NVIDIA Virtual Machine Image</h2>



<p>The NVIDIA Cloud Native Stack Virtual Machine Image (VMI) is GPU-accelerated. It comes pre-installed with Cloud Native Stack, which is a reference architecture that includes upstream Kubernetes and the NVIDIA GPU Operator. NVIDIA Cloud Native Stack VMI enables you to build, test, and run GPU-accelerated containerized applications orchestrated by Kubernetes.</p>



<p>The NVIDIA GPU Operator automates the lifecycle management of the software required to expose GPUs on Kubernetes. It enables advanced functionality, including better GPU performance, utilization, and telemetry. Certified and validated for compatibility with industry-leading Kubernetes solutions, GPU Operator enables organizations to focus on building applications, rather than managing Kubernetes infrastructure.</p>



<p>NVIDIA Cloud Native Stack VMI is available on AWS, Azure, and GCP.</p>



<h2>Now Available: Enterprise support by NVIDIA</h2>



<p>For enterprise support for NVIDIA Cloud Native Stack VMI and GPU Operator, &nbsp;purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA partner</a>.</p>



<p>Developing AI solutions from concept to deployment is not easy. Keep your AI projects on track with NVIDIA AI Enterprise Support Services. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options. Additional services, including training and AI workload onboarding, are available.<strong></strong></p>



<h2>Run:ai is now certified on NVIDIA AI Enterprise</h2>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on their Atlas platform.&nbsp;This additional certification enables enterprises to accelerate the data science pipeline. They can focus on streamlining the development and deployment of predictive AI models to automate essential processes and gain rapid insights from data.</p>



<p>Run:ai provides an AI Computing platform that simplifies the access, management, and utilization of GPUs in cloud and on-premises clusters. Smart scheduling and advanced fractional GPU capabilities ensure that you get the right amount of compute for the job.</p>



<p>Run:ai Atlas includes GPU Orchestration capabilities to help researchers consume GPUs more efficiently. They do this by automating the orchestration of AI workloads and the management and virtualization of hardware resources across teams and clusters.</p>



<p>Run:ai can be installed on any Kubernetes cluster, to provide efficient scheduling and monitoring capabilities to your AI infrastructure. With the NVIDIA Cloud Native Stack VMI, you can add cloud instances to a Kubernetes cluster so that they become GPU-powered worker nodes of the cluster.</p>



<p>Here’s testimony from one of our team members: “As an engineer, without the NVIDIA Cloud Native Stack VMI, there is a lot of manual work involved. With the Cloud Native Stack VMI, it was two clicks and took care of provisioning Kubernetes and Docker and the GPU Operator. It was easier and faster to get started on my work.”</p>



<h2>Set up a Cloud Native Stack VMI on AWS</h2>



<p>In the AWS marketplace, <a href="https://aws.amazon.com/marketplace/pp/prodview-cucpqpmvqkajy?sr=0-2&amp;ref_=beagle&amp;applicationId=AWSMPContessa" data-wpel-link="external" target="_blank" rel="follow external noopener">launch an NVIDIA Cloud Native Stack VMI</a> using the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launch-marketplace-console.html" data-wpel-link="external" target="_blank" rel="follow external noopener">Launch an AWS Marketplace instance</a> instructions.</p>



<p>Ensure that the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-prerequisites/" data-wpel-link="external" target="_blank" rel="follow external noopener">necessary prerequisites</a> have been met and install Run:ai using the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-install/" data-wpel-link="external" target="_blank" rel="follow external noopener">Cluster Install</a> instructions. After the installation, on the <strong>Overview</strong> dashboard, you should see that the metrics begin to populate. On the <strong>Clusters</strong> tab, you should also see the cluster as connected.</p>



<p>Next, add a few command components to the kube-apiserver.yaml file to enable user authentication on the Run:ai platform. For more information, see <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=researcher+api#administration-user-interface-setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Administration User Interface Setup</a>.</p>



<p>By default, you can find the kube-apiserver.yaml file in the following directory:</p>


<div><pre title="">/etc/kubernetes/manifests/kube-apiserver.yaml
</pre></div>


<p>You can <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=oidc#mandatory-kubernetes-configuration" data-wpel-link="external" target="_blank" rel="follow external noopener">validate that the oidc commands were successfully applied</a> by the kube-apiserver. Look for the <code>oidc</code> commands in the output.</p>



<pre><code>spec:
&nbsp; containers:
&nbsp; - command:
&nbsp; &nbsp; - kube-apiserver
&nbsp; &nbsp; - --oidc-client-id=runai
&nbsp; &nbsp; - --oidc-issuer-url=https://app.run.ai/auth/realms/nvaie
&nbsp; &nbsp; - --oidc-username-prefix=-</code></pre>



<p>Set up the <a href="https://docs.run.ai/admin/admin-ui-setup/overview/#setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Unified UI</a> and <a href="https://docs.run.ai/admin/admin-ui-setup/project-setup/?h=projects" data-wpel-link="external" target="_blank" rel="follow external noopener">create a new project</a>. Projects help to dictate GPU quota guarantees for data scientists and researchers who are using the Run:ai platform.</p>



<p>Name the new project and give the project at least one assigned GPU. For this post, I created one project with a two-GPU quota and another project with no GPU quota, labeled <code>nvaie-high-priority</code> and <code>nvaie-low-priority</code>, respectively After the project is created, you can <a href="https://docs.run.ai/admin/researcher-setup/cli-install/?h=researcher+command+line+interface#install-runai-cli" data-wpel-link="external" target="_blank" rel="follow external noopener">install the Run:ai CLI tool</a>, which enables you to submit workloads to the cluster.</p>



<p>The following commands use the runai CLI to submit a job (job1 or job2) leveraging a Docker image called quickstart. Quickstart contains TensorFlow, CUDA, a model, and data that feeds in and trains the model. It leverages one GPU for training (-g 1) and is submitted on behalf of the low-priority or high-priority project denoted by the <code>-p</code> parameter. </p>



<p>Deploy a few test jobs to show some of Run:ai’s orchestration functionality by running:</p>


<div><pre title="">runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority 
runai submit job2 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-low-priority
</pre></div>


<p>You can check the status of the jobs by running:</p>


<div><pre title="">runai describe job job1 -p nvaie-high-priority
runai describe job job2 -p nvaie-low-priority
</pre></div>


<p>Both workloads are now training on the GPUs, as you can see on the <strong>Overview</strong> dashboard.</p>



<p>You can submit an additional workload to highlight your job preemption capabilities. Currently, the <code>nvaie-high-priority</code> project is guaranteed access to both GPUs since their Assigned GPU quota is set to 2. You can submit an additional workload for the <code>nvaie-high-priority</code> project and observe that you are preempting the <code>nvaie-low-priority</code> job.</p>



<p>The job preemption enables you to look at the <a href="https://docs.run.ai/Researcher/best-practices/convert-to-unattended/?h=checkpoint#checkpoints" data-wpel-link="external" target="_blank" rel="follow external noopener">checkpointing process</a> for the training workloads, save the current progress at the checkpoint, and then preempt the workload to remove it from the GPU. Save the training progress and free up the GPU for a higher-priority workload to run.</p>


<div><pre title="">runai submit job3 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority
</pre></div>


<p>You can check the status of the job by running:</p>


<div><pre title="">runai describe job job3 -p nvaie-high-priority
</pre></div>


<p>If you go back to the overview dashboard, you’ll see the two jobs running for the <code>nvaie-high-priority</code> project and the workload from <code>nvaie-low-priority</code> preempted and placed back into the pending queue. The workload in the pending queue is automatically rescheduled when a GPU becomes available.</p>



<p>To clean up your jobs, run the following commands:</p>


<div><pre title="">runai delete job job1 -p nvaie-low-priority 
runai delete job job2 job3 -p nvaie-high-priority 
</pre></div>


<h2>Summary</h2>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. Developers and MLOps can then deploy that AI application on any GPU-powered platform without code change. </p>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on its Atlas platform. You can purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Partner</a> to obtain enterprise support for NVIDIA VMI and GPU Operator. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options.</p>



<p>For more information, see the following resources:</p>



<ul>
<li><a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA AI Enterprise</a></li>



<li><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/nvidia_vmi" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA VMI</a></li>



<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA GPU Operator</a> </li>



<li><a href="https://www.run.ai/" data-wpel-link="external" target="_blank" rel="follow external noopener">Run:ai solutions</a></li>
</ul>
</div></div>]]></description>
        </item>
    </channel>
</rss>