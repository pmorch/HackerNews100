<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Nov 2024 00:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Llama 3.2 Interpretability with Sparse Autoencoders (200 pts)]]></title>
            <link>https://github.com/PaulPauls/llama3_interpretability_sae</link>
            <guid>42208383</guid>
            <pubDate>Thu, 21 Nov 2024 20:37:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PaulPauls/llama3_interpretability_sae">https://github.com/PaulPauls/llama3_interpretability_sae</a>, See on <a href="https://news.ycombinator.com/item?id=42208383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Llama 3 Interpretability with Sparse Autoencoders</h2><a id="user-content-llama-3-interpretability-with-sparse-autoencoders" aria-label="Permalink: Llama 3 Interpretability with Sparse Autoencoders" href="#llama-3-interpretability-with-sparse-autoencoders"></a></p>
<p dir="auto"><a href="https://github.com/PaulPauls/llama3_interpretability_sae"><img src="https://camo.githubusercontent.com/d8ab014b7c4fddd9e1be09aad531f865d6c10714656e450bd25916bee9184faf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d302e322d627269676874677265656e2e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-0.2-brightgreen.svg"></a>
<a href="https://www.python.org/downloads/release/python-3120/" rel="nofollow"><img src="https://camo.githubusercontent.com/301b3c09bd84634def9aedd025140e5490068ad6d3c1dbf3ee7e438bc18ecc10/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31322d626c75652e737667" alt="Python 3.12" data-canonical-src="https://img.shields.io/badge/Python-3.12-blue.svg"></a>
<a href="https://pytorch.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/5f73ee33476fce4b75d5433466697168447f41344deba992dfd8ca022f87898d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5079546f7263682d322e342e312d2532334545344332432e7376673f7374796c653d666c6174266c6f676f3d7079746f726368266c6f676f436f6c6f723d7768697465" alt="PyTorch 2.4.1" data-canonical-src="https://img.shields.io/badge/PyTorch-2.4.1-%23EE4C2C.svg?style=flat&amp;logo=pytorch&amp;logoColor=white"></a>
<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="nofollow"><img src="https://camo.githubusercontent.com/cc7eb8b5b76db2770a1185619fe6e84884eae824a79af854d418438ef3cce6e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541362539392532304c6c616d612d332e322d626c756576696f6c65742e737667" alt="Llama 3.2" data-canonical-src="https://img.shields.io/badge/%F0%9F%A6%99%20Llama-3.2-blueviolet.svg"></a>
<a href="https://huggingface.co/datasets/PaulPauls/openwebtext-sentences" rel="nofollow"><img src="https://camo.githubusercontent.com/6bf69a74cc81c38011dc538b7169597928a2644fb726da1eee000e8ea0ebc089/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446174617365742d4f70656e5765625465787453656e74656e6365732d6f72616e67652e737667" alt="Dataset" data-canonical-src="https://img.shields.io/badge/Dataset-OpenWebTextSentences-orange.svg"></a>
<a href="https://api.wandb.ai/links/paulpauls/pi9dpi2a" rel="nofollow"><img src="https://camo.githubusercontent.com/b3032f9b6ba38ec405b1fbb5e410082ce6acaeea33a5af19f9bfc831ffb0c9ae/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57656967687473253230262532304269617365732d4c6f67732d79656c6c6f772e737667" alt="Weights &amp; Biases" data-canonical-src="https://img.shields.io/badge/Weights%20&amp;%20Biases-Logs-yellow.svg"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sparse-autoencoder_light.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sparse-autoencoder_light.webp" alt="Sparse Autoencoder Animation"></a>
  <br>
  <em>Source: <a href="https://openai.com/index/extracting-concepts-from-gpt-4/" rel="nofollow">OpenAI - Extracting Concepts from GPT-4</a></em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Overview</h2><a id="user-content-project-overview" aria-label="Permalink: Project Overview" href="#project-overview"></a></p>
<p dir="auto">Modern LLMs encode concepts by superimposing multiple features into the same neurons and then interpeting them by taking into account the linear superposition of all neurons in a layer. This concept of giving each neuron multiple interpretable meanings they activate depending on the context of other neuron activations is called <em>superposition</em>. Sparse Autoencoders (SAEs) are models that are inserted into a trained LLM for the purpose of projecting the activations into a very large but very sparsely activated latent space. By doing so they attempt to untangle these superimposed representations into separate, clearly interpretable features for each neuron activation that each represent one clear concept - which in turn would make these neurons <em>monosemantic</em>. Such a mechanistic interpretability has proven very valuable for understanding model behavior, detecting hallucinations, analyzing information flow through models for optimization, etc.</p>
<p dir="auto">This project attempts to recreate this great research into mechanistic LLM Interpretability with Sparse Autoencoders (SAE) to extract interpretable features that was very successfully conducted and published by Anthropic, OpenAI and Google DeepMind a few months ago. The project aims to provide a full pipeline for capturing training data, training the SAEs, analyzing the learned features, and then verifying the results experimentally. Currently, the project provides all code, data, and models that were created by running the whole project pipeline once and creating a functional and interpretable Sparse Autoencoder for the Llama 3.2-3B model.</p>
<p dir="auto">Such a research project obviously requires a lot of computational resources (meaning money) and time that I don't necessarily have at my full disposal for a non-profit side project of mine. Therefore, the project - as I am releasing it now with version 0.2 - is in a good, efficient, and scalable state, but it is not final and will hopefully be updated and improved upon over time. Please feel free to contribute code or feedback or just let me know if you found a bug - thank you!</p>
<p dir="auto">This project is based primarily on the following research papers:</p>
<ul dir="auto">
<li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" rel="nofollow">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a> (Anthropic, May 2024)</li>
<li><a href="https://arxiv.org/abs/2406.04093" rel="nofollow">Scaling and Evaluating Sparse Autoencoders</a> (OpenAI, June 2024)</li>
<li><a href="https://arxiv.org/abs/2408.05147" rel="nofollow">Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2</a> (Google DeepMind, July 2024)</li>
</ul>
<p dir="auto">And the Open Source LLM Llama 3.2 that was used for the current state of the project:</p>
<ul dir="auto">
<li><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="nofollow">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</a></li>
<li><a href="https://github.com/meta-llama/llama-models">Llama Models</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Features</h3><a id="user-content-core-features" aria-label="Permalink: Core Features" href="#core-features"></a></p>
<p dir="auto">A complete end-to-end pipeline from activation capture to Sparse AutoEncoder (SAE) training, feature interpretation, and verification, written in pure PyTorch with minimal dependencies. Specifically:</p>
<ul dir="auto">
<li>Captures residual activations from large language models as SAE training dataset, using a custom sentence-split OpenWebText dataset variant</li>
<li>Preprocesses training data (prebatching, stat calculation) for efficient training</li>
<li>Supports distributed (multi-GPU, single-node) large-scale and efficient training of SAEs with captured and preprocessed activation data</li>
<li>Implements SAE training with auxiliary loss to prevent and revive dead latents, and gradient projection to stabilize training dynamics</li>
<li>Provides comprehensive logging, visualization, and checkpointing of SAE training through Weights &amp; Biases and console logging, including detailed logs for:
<ul dir="auto">
<li>Training progress (main/aux loss)</li>
<li>Validation results (main/aux loss)</li>
<li>Dead latent monitoring for debugging and analysis</li>
</ul>
</li>
<li>Offers interpretability analysis tools for feature extraction and semantic analysis of learned features by:
<ul dir="auto">
<li>Capturing inputs that maximally activate the sparse autoencoder latents</li>
<li>Cost-effectively analyzing them at scale using a Frontier LLM</li>
</ul>
</li>
<li>Provides a pure PyTorch implementation of Llama 3.1/3.2 chat and text completion without external dependencies (e.g., Fairscale) for general usage and result verification</li>
<li>Verifies SAE's impact on model behavior and enables feature steering of extracted semantic features through:
<ul dir="auto">
<li>Text and chat completion tasks</li>
<li>Optional Gradio interface for ease of use</li>
</ul>
</li>
<li><strong>All components are designed and implemented with scalability, efficiency, and maintainability in mind</strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Published Resources</h2><a id="user-content-published-resources" aria-label="Permalink: Published Resources" href="#published-resources"></a></p>
<p dir="auto">The following resource are available to reproduce the results of the current state of the project or to provide insight into the training:</p>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://huggingface.co/datasets/PaulPauls/openwebtext-sentences" rel="nofollow"><strong>OpenWebText Sentence Dataset</strong></a>:</p>
<ul dir="auto">
<li>Custom Version of the OpenWebText Dataset used for activation capture</li>
<li>All text from the original OpenWebText dataset</li>
<li>Sentences are stored individually now in parquet format for faster access</li>
<li>Maintains all original OpenWebText text and the order thereof</li>
<li>Sentences were split using NLTK 3.9.1 pre-trained "Punkt" tokenizer</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://drive.google.com/drive/folders/1GqMQz3d0z40TRg0codbpt_qacWSPftTZ?usp=sharing" rel="nofollow"><strong>Captured Llama 3.2-3B Activations</strong></a>:</p>
<ul dir="auto">
<li>25 million sentences worth of Llama 3.2-3B layer 23 residual activations</li>
<li>Size: 3.2 TB (compressed from 4 TB raw)</li>
<li>Split into 100 archives for more manageable download</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://api.wandb.ai/links/paulpauls/pi9dpi2a" rel="nofollow"><strong>SAE Training Log</strong></a>:</p>
<ul dir="auto">
<li>Weights &amp; Biases visualized log of training, validation and debug metrics</li>
<li>10 epochs of training, 10,000 logged steps</li>
<li>Includes train/val main losses, auxiliary losses, and dead latent stats during training</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://drive.google.com/file/d/1HEty3WPThHethMsRqS8duhdKrmdawxeu/view?usp=sharing" rel="nofollow"><strong>Trained 65,536 latents SAE Model</strong></a>:</p>
<ul dir="auto">
<li>Final Trained SAE model after 10 epochs with training log specified configuration</li>
<li>Trained on 6.5 Billion activations from Llama 3.2-3B layer 23</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Structure</h2><a id="user-content-project-structure" aria-label="Permalink: Project Structure" href="#project-structure"></a></p>
<p dir="auto">The project is organized into four main components:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Data Capture</h4><a id="user-content-1-data-capture" aria-label="Permalink: 1. Data Capture" href="#1-data-capture"></a></p>
<ul dir="auto">
<li><code>capture_activations.py</code> - Captures LLM residual activations</li>
<li><code>openwebtext_sentences_dataset.py</code> - Custom dataset for sentence-level processing</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. SAE Training</h4><a id="user-content-2-sae-training" aria-label="Permalink: 2. SAE Training" href="#2-sae-training"></a></p>
<ul dir="auto">
<li><code>sae.py</code> - Core SAE model implementation</li>
<li><code>sae_preprocessing.py</code> - Data preprocessing for SAE training</li>
<li><code>sae_training.py</code> - Distributed SAE training implementation</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. Interpretability</h4><a id="user-content-3-interpretability" aria-label="Permalink: 3. Interpretability" href="#3-interpretability"></a></p>
<ul dir="auto">
<li><code>capture_top_activating_sentences.py</code> - Identifies sentences that maximize feature activation</li>
<li><code>interpret_top_sentences_send_batches.py</code> - Builds and sends batches for interpretation</li>
<li><code>interpret_top_sentences_retrieve_batches.py</code> - Retrieves interpretation results</li>
<li><code>interpret_top_sentences_parse_responses.py</code> - Parses and analyzes interpretations</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Verification and Testing</h4><a id="user-content-4-verification-and-testing" aria-label="Permalink: 4. Verification and Testing" href="#4-verification-and-testing"></a></p>
<ul dir="auto">
<li><code>llama_3_inference.py</code> - Core inference implementation</li>
<li><code>llama_3_inference_text_completion_test.py</code> - Text completion testing</li>
<li><code>llama_3_inference_chat_completion_test.py</code> - Chat completion testing</li>
<li><code>llama_3_inference_text_completion_gradio.py</code> - Gradio interface for interactive testing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Poetry if not already installed
curl -sSL https://install.python-poetry.org | python3.12 -

# Clone the repository
git clone https://github.com/PaulPauls/llama3_interpretability_sae
cd llama3_interpretability_sae

# Install exact dependencies the project has been run with
poetry install --sync"><pre><span><span>#</span> Install Poetry if not already installed</span>
curl -sSL https://install.python-poetry.org <span>|</span> python3.12 -

<span><span>#</span> Clone the repository</span>
git clone https://github.com/PaulPauls/llama3_interpretability_sae
<span>cd</span> llama3_interpretability_sae

<span><span>#</span> Install exact dependencies the project has been run with</span>
poetry install --sync</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation Details &amp; Results</h2><a id="user-content-implementation-details--results" aria-label="Permalink: Implementation Details &amp; Results" href="#implementation-details--results"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Custom Llama Implementation</h4><a id="user-content-1-custom-llama-implementation" aria-label="Permalink: 1. Custom Llama Implementation" href="#1-custom-llama-implementation"></a></p>
<p dir="auto">The basis of this research is the custom Llama 3.1/3.2 transformer model implementation in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3/model_text_only.py"><code>llama_3/model_text_only.py</code></a>. It is based on the reference implementation from the llama models repository, however I've made several significant modifications to better suit this project. I rewrote the implementation to remove the heavy dependency on the Fairscale library - simply because I am not familiar with it and much more comfortable working directly with PyTorch, thereby hoping to avoid bugs or performance bottlenecks stemming from working with a library I am not familiar with. Similarly, I stripped out the multimodal functionality since investigating image interpretability would have added unnecessary complexity for this initial release.
The modified Llama 3.1/3.2 model implementation includes new constructor options that enable either activation capture or the injection of a trained Sparse Autoencoder (SAE) model at specific layers during inference. This functionality is implemented through the following constructor parameters:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class Transformer(nn.Module):
    def __init__(
        self,
        params: ModelArgs,
        store_layer_activ: list[int] | None = None,
        sae_layer_forward_fn: dict[int, callable] | None = None,
    ):"><pre><span>class</span> <span>Transformer</span>(<span>nn</span>.<span>Module</span>):
    <span>def</span> <span>__init__</span>(
        <span>self</span>,
        <span>params</span>: <span>ModelArgs</span>,
        <span>store_layer_activ</span>: <span>list</span>[<span>int</span>] <span>|</span> <span>None</span> <span>=</span> <span>None</span>,
        <span>sae_layer_forward_fn</span>: <span>dict</span>[<span>int</span>, <span>callable</span>] <span>|</span> <span>None</span> <span>=</span> <span>None</span>,
    ):</pre></div>
<p dir="auto">For the supporting code in the <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3"><code>llama_3/</code></a> directory, I made a pragmatic decision to keep most of the auxiliary files from the original Llama models repository unchanged. 95% of this auxiliary code is unused and only needed for the chat formatter, which is very dependent on interconnected imports. However, since it wasn't research critical did I decide not to rewrite it and simply carry over the files over.
The actual inference implementation is custom and contained in my <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/llama_3_inference.py"><code>llama_3_inference.py</code></a> module, which provides streaming capabilities for both chat and text completion tasks and will be primarily of use when testing and validating the results. The implementation allows for batched inference and features configurable temperature and top-p sampling parameters, with an automatic fallback to greedy sampling when the temperature is set to 0.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Data Capture</h4><a id="user-content-2-data-capture" aria-label="Permalink: 2. Data Capture" href="#2-data-capture"></a></p>
<p dir="auto">For the data capture I created a custom variant of the <a href="https://huggingface.co/datasets/Skylion007/openwebtext" rel="nofollow">OpenWebText</a> dataset that processes text at the sentence level, capturing 25 million sentences with a maximum length of 192 tokens each. This extensive data collection resulted in 4TB of raw activation data, which compresses to 3.2TB of tar.gz archives. In total I captured approximately 700 million activations from these 25 million contexts, with sentences averaging 27.3 tokens in length.</p>
<p dir="auto">While this dataset is roughly an order of magnitude smaller than those used by Anthropic or Google DeepMind (who both utilized around 8 billion unique activations), it still provides a substantial foundation for training an initial SAE model in my opinion. To compensate for the smaller dataset size I trained the SAE for 10 epochs, effectively processing the same number of total activations as Anthropic and Google DeepMind - the key difference being that my SAE encounters each activation 10 times instead of only once. This approach was simply a monetary constraints for a non-profit side project, as scaling to match their single-epoch approach would have increased my GCP bucket costs from approximately $80/month (for 3.2TB w/ traffic) to $800/month (for 32TB w/ traffic), not to mention the significantly higher costs for instance SSDs during training.</p>
<p dir="auto">The decision to process data at the sentence level was deliberate and grounded in several key considerations. Sentences represent natural linguistic units that contain complete thoughts and concepts, which I hope would lead to more interpretable and semantically meaningful features. This approach avoids artificial truncation of context and prevents <em>contextual bleed</em> of meanings across sentence boundaries, while still capturing essential contextual relationships within grammatically complete units. My thinking was that this makes it easier to attribute discovered features to specific linguistic or semantic phenomena. This approach was also chosen so that the training dataset contains activations adapted to the same linguistic units that I later intended to use for the interpretability analysis.</p>
<p dir="auto">Furthermore I specifically chose to process sentences without the 'beginning-of-sequence' (bos) token to avoid position-specific patterns, as my goal was to interpret features based on their meaning alone, independent of their position in a sequence. Since sentences represent natural semantic units that provide sufficient context for meaningful interpretation while remaining specific enough to identify distinct concepts, this aligns well with the goal of having a LLM analyze the semantic content that activates specific latents.</p>
<p dir="auto">From a technical implementation perspective, I captured residual stream activations after layer normalization from Llama 3.2-3B, specifically from layer 23 out of 28 (positioned just short of 5/6th through the model's depth, following OpenAI's implementation). The capture process utilizes a distributed implementation using NCCL for single-node multi-GPU inference, with asynchronous disk I/O handled through a separate process to prevent GPU processing bottlenecks. The entire data capture process took approximately 12 hours using 4x Nvidia RTX4090 GPUs.</p>
<p dir="auto">OpenAI's implementation also uses residual stream activations after layer normalization, with a context length of 64 tokens and sampling 5/6th-way throughout the GPT-4 model (though they sample more toward the middle layers for smaller models). Anthropic's implementation differs more substantially, using 8 billion MLP activation vectors from 40 million contexts (250 tokens per context) and applying them to residual stream activations at the middle layer of the Claude 3 Sonnet model. In hindsight, there could be a potential for improvement when capturing earlier layers in the next experiments for a relatively small 3B parameter model.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>Data Preprocessing</em></h5><a id="user-content-data-preprocessing" aria-label="Permalink: Data Preprocessing" href="#data-preprocessing"></a></p>
<p dir="auto">I am personally a big fan of data preprocessing if complicated batching is necessary. In this case, the challenge was to create batches of 1024 activations each, while handling activations of variable sequence lengths that necessitate carryover handling, potentially in a multiprocessing context. Given the high risk of batching bugs or I/O-related performance bottlenecks, I made the decision to implement a preprocessing phase rather than handling these complexities during training.</p>
<p dir="auto">Since preprocessing was already necessary, I also took the opportunity to calculate the mean tensor across all activations using Welford's algorithm. This algorithm was specifically chosen for its numerical stability and memory efficiency when processing very large datasets. The calculated mean serves as the initial value for the <code>b_pre</code> bias term in the SAE model. While OpenAI's implementation uses the geometric median instead of the mean for initializing <code>b_pre</code>, they only compute it for the first ~30,000 samples of their dataset rather than the complete set and given that the <code>b_pre</code> is optimizable anyway, I decided that using the mean as an initial approximation would be sufficient.</p>
<p dir="auto">The entire preprocessing pipeline is implemented with full CPU parallelization through multiprocessing, ensuring efficient processing of the large activation dataset. This approach simplifies the training process by providing clean, pre-batched data.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. SAE Implementation</h4><a id="user-content-3-sae-implementation" aria-label="Permalink: 3. SAE Implementation" href="#3-sae-implementation"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE DESIGN</em></h5><a id="user-content-sae-design" aria-label="Permalink: SAE DESIGN" href="#sae-design"></a></p>
<p dir="auto">The core of the Sparse Autoencoder implementation follows a straightforward encoder-decoder architecture with design choices following mainly the choices made by OpenAI. The full forward pass of the TopK Autoencoder includes two key bias terms: <code>b_pre</code> for both encoder and decoder (initialized as the mean calculated in the preprocessing as outlined in the prior section), and <code>b_enc</code> specifically for the encoder (initialized randomly). The complete forward pass can be described as:</p>
<p dir="auto">Encoder: <code>h = TopK(W_enc(x - b_pre) + b_enc)</code></p>
<p dir="auto">Decoder: <code>x^ = W_dec * h (+ h_bias) + b_pre</code></p>
<p dir="auto">Sparsity in the latent space is enforced through the TopK activation function, which retains only the k largest activations and sets the rest to zero. This approach directly controls sparsity without requiring an L1 penalty term for sparsity in the loss function as required by Anthropic's approach. The model includes an optional <code>h_bias</code> parameter that remains disabled during training but can be activated afterwards for feature steering, allowing dynamic manipulation of the latent space post-training.</p>
<p dir="auto">For numerical precision I chose to work with float32 dtype due to its quick and precise conversion compatibility with Llama's required bfloat16 dtype. Both formats share the same 1 sign bit and 8 exponent bits structure, differing only in their mantissa bits (23 vs 7), making conversions quick and accurate.</p>
<p dir="auto">My implementation differs from both Anthropic's and OpenAI's approaches in several ways. Anthropic uses a one-hidden-layer MLP with ReLU activation and enforces sparsity through an L1 penalty instead of TopK. They also worked with massively larger latent sizes (~1M, ~4M, and ~34M features), though their average number of active features per token remained below 300 for all SAEs. OpenAI's architecture is more similar to my implementation, but they experimented with latent sizes from 2^11 (2,048) up to 2^24 (16.7M) for GPT-4. Their experiments showed that larger latent sizes generally produced better loss and feature explainability, while lower k values (fewer active latents) led to more interpretable features.</p>
<p dir="auto">For this project - working with the 3B parameter Llama 3.2 model with a residual stream dimension of 3,072 - I chose a latent dimension of 2^16 (65,536) and a k value of 64. This decision aims to strike a balance between several factors: providing sufficient feature capacity at approximately 21x the residual stream dimension, maintaining computational efficiency as suggested by the OpenAI and Google DeepMind papers, and staying within the project's monetary constraints when training on ~8 billion activations for comparability. The k value of 64 was selected to achieve a good balance between reconstruction power and the strong sparsity needed for interpretable features.</p>
<p dir="auto">In hindsight however, as I outline in <a href="#6-future-work--improvements">section 6</a>, I would considerably increase the latent size and decrease the k value for future experiments to improve the variety and interpretability of the features and attempt to find efficiency improvements to stay within budget constraints. As a first full run of the project, I am however very happy with the chosen hyperparameters and results.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE TRAINING METHODOLOY</em></h5><a id="user-content-sae-training-methodoloy" aria-label="Permalink: SAE TRAINING METHODOLOY" href="#sae-training-methodoloy"></a></p>
<p dir="auto">The training configuration of the Sparse Autoencoder was chosen to balance efficiency and feature interpretability. The core hyperparameters reflect both the model architecture and the training dynamics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Set up configuration
d_model = 3072
n_latents = 2**16  # 65536
k = 64
k_aux = 2048
aux_loss_coeff = 1 / 32
dead_steps_threshold = 80_000  # ~1 epoch in training steps
sae_normalization_eps = 1e-6
batch_size = 1024
num_epochs = 10
early_stopping_patience = 10  # disabled
learning_rate = 5e-5
learning_rate_min = learning_rate / 5
optimizer_betas = (0.85, 0.9999)
optimizer_eps = 6.25e-10
dtype = torch.float32
dataloader_num_workers = 8
logs_per_epoch = 1000
train_val_split = 0.95"><pre><span># Set up configuration</span>
<span>d_model</span> <span>=</span> <span>3072</span>
<span>n_latents</span> <span>=</span> <span>2</span><span>**</span><span>16</span>  <span># 65536</span>
<span>k</span> <span>=</span> <span>64</span>
<span>k_aux</span> <span>=</span> <span>2048</span>
<span>aux_loss_coeff</span> <span>=</span> <span>1</span> <span>/</span> <span>32</span>
<span>dead_steps_threshold</span> <span>=</span> <span>80_000</span>  <span># ~1 epoch in training steps</span>
<span>sae_normalization_eps</span> <span>=</span> <span>1e-6</span>
<span>batch_size</span> <span>=</span> <span>1024</span>
<span>num_epochs</span> <span>=</span> <span>10</span>
<span>early_stopping_patience</span> <span>=</span> <span>10</span>  <span># disabled</span>
<span>learning_rate</span> <span>=</span> <span>5e-5</span>
<span>learning_rate_min</span> <span>=</span> <span>learning_rate</span> <span>/</span> <span>5</span>
<span>optimizer_betas</span> <span>=</span> (<span>0.85</span>, <span>0.9999</span>)
<span>optimizer_eps</span> <span>=</span> <span>6.25e-10</span>
<span>dtype</span> <span>=</span> <span>torch</span>.<span>float32</span>
<span>dataloader_num_workers</span> <span>=</span> <span>8</span>
<span>logs_per_epoch</span> <span>=</span> <span>1000</span>
<span>train_val_split</span> <span>=</span> <span>0.95</span></pre></div>
<p dir="auto">The loss function combines a main reconstruction loss resulting from the reconstruction error with a complex auxiliary loss designed to prevent and revive dead latents in the following way: <code>total_loss = main_loss + aux_loss_coeff * aux_loss</code>. Following OpenAI's approach, I set <code>aux_loss_coeff = 1/32</code>. Both losses are computed in normalized space to ensure equal contribution from all features regardless of their original scale, which helps maintain numerical stability throughout training.</p>
<p dir="auto">The auxiliary loss was proposed by OpenAI and plays a crucial role in preventing dead latents through a clever mechanism: it calculates the MSE between the main reconstruction residual (the difference between input and main reconstruction) and a special auxiliary reconstruction. This auxiliary reconstruction uses the same pre-activation latent as the main reconstruction, but it takes only the top-(aux-k) activation values from latents that haven't fired recently (which is tracked through the shared <code>stats_last_nonzero</code> tensor) and sends them through the decoder again to get this 'auxiliary reconstruction'. This gives these <code>k_aux = 2048</code> inactive latents that haven't even activated during the actual training where only the top <code>k</code> latents are used for the reconstruction a dedicated learning signal to capture information missed by these main latents. This makes the dead latents more likely to activate in future forward passes and thereby allows for reviving dead latents, keeping all latents alive and useful.</p>
<p dir="auto">The training only consider dead latents for the auxiliary loss. A latent is considered dead if it hasn't been activated in <code>dead_steps_threshold</code> training steps (set to 80,000 steps, approximately one epoch in my setup), which equates to no activation in the reconstruction of the last ~650M activations given a batch size of 8192. This threshold serves two purposes: it ensures the main loss has sufficient warm-up time before receiving an auxiliary loss signal, and it guarantees that we only attempt to revive latents that haven't fired once after seeing all unique training data activations.</p>
<p dir="auto">The training infrastructure employs distributed training using the NCCL backend for a single-node multi-GPU setup. Using 8x Nvidia RTX4090 GPUs for 10 epochs with a per-GPU batch size of 1024 (effective batch size 8192), I processed approximately 7 billion activations over a span of a little over 7 days. The number of epochs was chosen to match the total number of processed activations in Anthropic's and Google DeepMind's experiments. All training progress, including losses and debugging statistics about dead latents, was tracked comprehensively via Weights &amp; Biases.</p>
<p dir="auto">The parameters of the optimzier were carefully tuned for the specific challenges of training a sparse autoencoder where some features activate extremely rarely. I chose a base learning rate of 5e-5 after comparative testing showed it achieved similar optimization speed to higher rates while promising better fine-tuning potential for sparse features in later training stages. The learning rate follows a cosine annealing schedule down to a minimum of 1e-5 (1/5 of initial).
The AdamW configuration required special consideration for the sparse nature of the autoencoder:</p>
<ul dir="auto">
<li><code>beta_1 = 0.85</code> (lower than the typical 0.9 to make individual updates more meaningful given the large effective batch size of 8192 and sparse nature of the autoencoder)</li>
<li><code>beta_2 = 0.9999</code> (accommodates sparse activation patterns where some features might activate very rarely and therefore need longer momentum preservation)</li>
<li><code>eps = 6.25e-10</code> (provides sufficient numerical stability for float32 precision while allowing precise parameter updates needed for optimizing rare activation patterns)</li>
</ul>
<p dir="auto">Weight initialization and normalization were implemented with particular attention to training stability as recommended by the OpenAI paper. The encoder and decoder weights are initialized orthogonally (with the decoder as the transpose of the encoder) to ensure balanced, independent initial feature directions. Input features are normalized with a small epsilon term for training robustness. Following empirical findings from both the OpenAI paper and <a href="https://transformer-circuits.pub/2023/monosemantic-features" rel="nofollow">Bricken et al. [2023]</a>, the decoder weights are explicitly normalized to unit norm after initialization and each training step, as this improves MSE performance.</p>
<p dir="auto">A key implementation detail is the gradient projection via <code>project_decoder_grads()</code>, which maintains the unit-norm constraint on decoder weights by removing gradient components parallel to existing dictionary vectors. This projection helps stabilize training and prevents the autoencoder from learning redundant or degenerate features when identifying sparse patterns in the data:</p>
<div dir="auto" data-snippet-clipboard-copy-content="def project_decoder_grads(self):
    &quot;&quot;&quot;Project out gradient information parallel to dict vectors.&quot;&quot;&quot;
    # Compute dot product of decoder weights and their grads, then subtract the projection from the grads
    # in place to save memory
    proj = torch.sum(self.decoder.weight * self.decoder.weight.grad, dim=1, keepdim=True)
    self.decoder.weight.grad.sub_(proj * self.decoder.weight)"><pre><span>def</span> <span>project_decoder_grads</span>(<span>self</span>):
    <span>"""Project out gradient information parallel to dict vectors."""</span>
    <span># Compute dot product of decoder weights and their grads, then subtract the projection from the grads</span>
    <span># in place to save memory</span>
    <span>proj</span> <span>=</span> <span>torch</span>.<span>sum</span>(<span>self</span>.<span>decoder</span>.<span>weight</span> <span>*</span> <span>self</span>.<span>decoder</span>.<span>weight</span>.<span>grad</span>, <span>dim</span><span>=</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>)
    <span>self</span>.<span>decoder</span>.<span>weight</span>.<span>grad</span>.<span>sub_</span>(<span>proj</span> <span>*</span> <span>self</span>.<span>decoder</span>.<span>weight</span>)</pre></div>
<p dir="auto">My implementation differs from both Anthropic's and OpenAI's approaches in several ways. Anthropic uses a combination of L2 reconstruction error and L1 regularization for sparsity, implements periodic neuron resampling for dead neurons and modifies gradient updates for decoder weight normalization. While they used the same batch size of 8192, their approach to maintaining sparsity and handling dead neurons is quite different from my TopK implementation.</p>
<p dir="auto">OpenAI's implementation is much closer to mine but uses different Adam optimizer settings (<code>beta_1 = 0.9</code>, <code>beta_2 = 0.999</code>), maintains a constant learning rate, implements gradient clipping for stability and considers neurons dead much earlier (no activation after 10M tokens). They also use an EMA of weights instead of raw optimization weights and opted for an extremely large batch size of 131,072 tokens.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><em>SAE TRAINING RESULTS</em></h5><a id="user-content-sae-training-results" aria-label="Permalink: SAE TRAINING RESULTS" href="#sae-training-results"></a></p>
<p dir="auto">The training process ran for ~7 days on 8x Nvidia RTX4090 GPUs, demonstrating stable and efficient convergence throughout. The training progression showed a nice logarithmic decay in the loss function, ultimately achieving a final total normalized loss of approximately 0.144:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_train-total-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_train-total-loss.png" alt="SAE Training 2024-10-28, Train Total Loss"></a>
  <br>
  <em>Fig 1: SAE Training - Total Loss</em>
</p>
<p dir="auto">The validation loss was computed on a held-out 5% of the training data and exhibited a similar logarithmic decay pattern, though predictably less steep than the training loss:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_val-total-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_val-total-loss.png" alt="SAE Training 2024-10-28, Val Total Loss"></a>
  <br>
  <em>Fig 2: SAE Training - Validation Loss</em>
</p>
<p dir="auto">A particularly interesting aspect of the training dynamics emerged after the initial warm-up period of 80,000 training steps. At this point, about 40% of the latents were identified as "dead" - meaning they hadn't activated once so far. However, the auxiliary loss mechanism proved remarkably effective at reviving these dead latents quickly:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_dead-latents-ratio.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_dead-latents-ratio.png" alt="SAE Training 2024-10-28, Dead Latents Ratio"></a>
  <br>
  <em>Fig 3: Dead Latents Ratio - Rapid decrease after warm-up period, stabilizing at minimum threshold</em>
</p>
<p dir="auto">The auxiliary loss started quite high but also showed quick decay as it successfully revived dead latents:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/sae_training_2024-10-28_train-aux-loss.png"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/sae_training_2024-10-28_train-aux-loss.png" alt="SAE Training 2024-10-28, Train Aux Loss"></a>
  <br>
  <em>Fig 4: SAE Training - Aux Loss</em>
</p>
<p dir="auto">An interesting implementation detail emerged regarding the auxiliary loss calculation: it only triggers when at least k_aux (2,048) latents are dead, effectively establishing a soft lower bound of dead latents at approximately 3% (2,048/65,536), which is very visible in Figure 3. I initially implemented this condition as an optimization to avoid unnecessary auxiliary loss calculations when few latents were dead. Surprisingly to me, the auxiliary loss mechanism was so effective at reviving dead latents that it consistently drove the dead latent count toward this lower bound, particularly in the later stages of training where the auxiliary loss was frequently zero due to insufficient dead latents to trigger the calculation.</p>
<p dir="auto">One reason why I was surprised by such an effective revival of dead latents was that I expected a much higher percentage of dead latents. Anthropic and OpenAI both reported up to 65% dead latents in certain configurations, though admittedly their latent size was 1 to 2 orders of magnitude larger than mine. The effectiveness of the auxiliary loss implementation, combined with the gradient projection technique for stabilizing training dynamics appears to create a very robust training. For future experiments though, removing the minimum dead latents threshold for auxiliary loss calculation could potentially allow for even fewer dead latents, though I am happy with the results of the current implementation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Interpretability Analysis</h4><a id="user-content-4-interpretability-analysis" aria-label="Permalink: 4. Interpretability Analysis" href="#4-interpretability-analysis"></a></p>
<p dir="auto">The interpretability analysis approach builds upon methods established in Anthropic's research on scaling monosemanticity, but with a key difference in granularity. While Anthropic primarily focused on single-token analysis, this implementation captures and analyzes complete sentences - specifically the top 50 sentences that most strongly activate each latent. The activation strength is calculated using a mean and last-token aggregation across all tokens in a sentence, which is intended to hopefully provide a more holistic view of semantic activation patterns in Llama 3.2-3B's intermediate layers.</p>
<p dir="auto">As I already addressed in <a href="#2-data-capture">section 2</a>, the decision to use sentence-level analysis instead of token-level analysis was deliberate and based on the hopes of combining linguistic principles with a simple approach for a first release. Sentences represent natural linguistic units that contain complete thoughts and in my opinion provide a great balance between context and specificity. This approach prevents both the artificial truncation of context and the potential mixing of meanings across sentence boundaries (<em>contextual bleed</em>). To aggregate all latent activations in a sequence I primarily chose to rely on 2 methods:</p>
<ul dir="auto">
<li><code>mean</code> aggregation to hopefully reveal features that maintain a consistent activation throughout a sentence, highlighting a sustained semantic theme</li>
<li><code>last</code> aggregation (simply taking the last token's activations) to hopefully leverage an LLMs autoregressive nature and capture the final representation that has seen the whole sentence through self-attention</li>
</ul>
<p dir="auto">For the semantic analysis itself, I utilized the most advanced frontier LLM that is available to me at the time of this project: Claude 3.5 (specifically <code>claude-3-5-sonnet-20241022</code>) with a structured chain-of-thought prompt. I employed an automated approach that allows for scalable interpretation while hopefully maintaining semantic specificity. The prompt is supposed to guide Claude through specific analysis steps for all 50 supplied sentences:</p>
<ol dir="auto">
<li>Identify key words and phrases</li>
<li>Group thematic elements</li>
<li>Consider potential outliers</li>
<li>Provide a final semantic interpretation with a confidence score</li>
</ol>
<p dir="auto">This analysis pipeline is implemeneted in the three stages of <em>sending analysis requests in cost-effective batches</em>, <em>retrieving responses</em>, and <em>parsing and processing the semantic interpretations</em>. All intermediate data is preserved for reproducibility and further analysis:</p>
<ul dir="auto">
<li>Raw sentences, their activation aggregations, and OpenWebText indices are stored in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/captured_top_sentences"><code>capture_top_sentences/</code></a></li>
<li>Unprocessed semantic analysis responses are saved in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/top_sentences_last_responses"><code>top_sentences_last_responses/</code></a> and <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/top_sentences_mean_responses"><code>top_sentences_mean_responses/</code></a></li>
<li>Processed results mapping latents to their identified 'common_semantic' and 'certainty score' are stored in <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/latent_index_meaning"><code>latent_index_meaning/</code></a></li>
</ul>
<p dir="auto">To illustrate the analysis process let's take a look at latent #896, which was identified as representing "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology". Claude's detailed analysis included:</p>
<div data-snippet-clipboard-copy-content="<semantic_analysis>
1. Key Word Analysis:
    - Frequent terms: &quot;UN&quot;, &quot;United Nations&quot;, &quot;Secretary-General&quot;
    - Official titles: &quot;Special Rapporteur&quot;, &quot;Under-Secretary-General&quot;, &quot;Coordinator&quot;
    - Department names: &quot;UNDP&quot;, &quot;UNHCR&quot;, &quot;OCHA&quot;, &quot;UNODC&quot;

2. Thematic Grouping:
    - UN organizational structure references
    - UN personnel and positions
    - UN reports and documentation
    - UN agencies and bodies
    - UN operations and activities

3. Pattern Analysis:
    - All sentences reference UN entities, personnel, or activities
    - Formal institutional language
    - Heavy use of official titles and department names
    - References to official documents and reports

4. Strength Assessment:
    - 50 out of 50 sentences contain direct UN references
    - Mix of department names, personnel titles, and activities
    - Consistent institutional terminology
    - No outliers identified

5. Certainty Calculation:
    - 100% of sentences contain UN references
    - Very strong institutional terminology consistency
    - Clear organizational focus
    - Direct and explicit connections

The analysis reveals that every sentence relates to United Nations organizations, personnel, operations, or documentation, with extremely consistent institutional terminology and structure. The commonality is both obvious and comprehensive.
</semantic_analysis>

{
    &quot;common_semantic&quot;: &quot;References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology&quot;,
    &quot;certainty&quot;: 1.0
}"><pre lang="text"><code>&lt;semantic_analysis&gt;
1. Key Word Analysis:
    - Frequent terms: "UN", "United Nations", "Secretary-General"
    - Official titles: "Special Rapporteur", "Under-Secretary-General", "Coordinator"
    - Department names: "UNDP", "UNHCR", "OCHA", "UNODC"

2. Thematic Grouping:
    - UN organizational structure references
    - UN personnel and positions
    - UN reports and documentation
    - UN agencies and bodies
    - UN operations and activities

3. Pattern Analysis:
    - All sentences reference UN entities, personnel, or activities
    - Formal institutional language
    - Heavy use of official titles and department names
    - References to official documents and reports

4. Strength Assessment:
    - 50 out of 50 sentences contain direct UN references
    - Mix of department names, personnel titles, and activities
    - Consistent institutional terminology
    - No outliers identified

5. Certainty Calculation:
    - 100% of sentences contain UN references
    - Very strong institutional terminology consistency
    - Clear organizational focus
    - Direct and explicit connections

The analysis reveals that every sentence relates to United Nations organizations, personnel, operations, or documentation, with extremely consistent institutional terminology and structure. The commonality is both obvious and comprehensive.
&lt;/semantic_analysis&gt;

{
    "common_semantic": "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology",
    "certainty": 1.0
}
</code></pre></div>
<p dir="auto">From a cost perspective, this interpretability analysis proved remarkably efficient compared to the dataset capture, storage, and SAE training phases. Processing 24,828,558 input tokens and generating 3,920,044 output tokens with Claude 3.5 in batch mode cost only $66.74.</p>
<p dir="auto">While this approach to semantic analysis is relatively straightforward, it was chosen as a solid initial method for both feature interpretation and potential feature steering. The sentence-level analysis helps avoid uncertainties around when specific latents should be activated, though this simplicity does certainly come at the cost to result quality. I considered developing more sophisticated interpretation methods, but this seemed like a complex research challenge that could potentially be debated and refined for months. Anthropic for example is not only publishing great papers on this topic but also consistently high quality blog posts about it for years at <a href="https://transformer-circuits.pub/" rel="nofollow">transformer-circuits.pub</a>. So for this initial release, I opted for a simpler approach that validates my full pipeline first before potentially making improvements on it in the future.</p>
<p dir="auto">So as outlined, my approach differs intentionally quite considerably from both Anthropic and OpenAI in several key aspects:</p>
<p dir="auto">Anthropic's approach to interpretability focuses on analyzing individual token activations in Claude 3 Sonnet, working with sparse autoencoders containing up to 34M features. Their methodology combines manual feature inspection with comprehensive validation through steering experiments and ablation studies. The validation protocol specifically examines feature interactions, activation patterns, and the impact of feature manipulation on model behavior. This multi-faceted approach allows them to verify both the existence and significance of identified features while providing insights into how these features contribute to the model's overall behavior.</p>
<p dir="auto">OpenAI's implementation similarly focuses on individual token activations but takes a different approach to analysis, examining both specific activations and random activating examples. Their methodology emphasizes automated interpretability at scale utilizing a lot of technical metrics. These include probe loss measurements, N2G (Neuron to Graph) pattern matching for feature identification, and multiple quality assessment metrics such as downstream loss, ablation sparsity, and explanation precision/recall. Furthermore OpenAI is also very systemic in assessing the quality of the detected features, using multiple quantitative metrics to evaluate the reliability and usefulness of identified features.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">5. Verification and Testing</h4><a id="user-content-5-verification-and-testing" aria-label="Permalink: 5. Verification and Testing" href="#5-verification-and-testing"></a></p>
<p dir="auto">The verification and testing infrastructure consists of three main components designed to analyze and validate the SAE's impact on model behavior:</p>
<ul dir="auto">
<li><code>llama_3_inference_chat_completion_test.py</code></li>
<li><code>llama_3_inference_text_completion_test.py</code></li>
<li><code>llama_3_inference_text_completion_gradio.py</code></li>
</ul>
<p dir="auto">These scripts enable both feature activation analysis and feature steering through text and chat completion tasks. Each implementation supports batched inference (considering each line a separate batch element), configurable temperature and top-p parameters, and most importantly the ability to inject a trained SAE model for feature analysis and steering.</p>
<p dir="auto">The semantic meanings and certainty scores for each latent - derived in the earlier interpretability analysis in <a href="#4-interpretability-analysis">section 4</a> - are stored in the <a href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/latent_index_meaning"><code>latent_index_meaning/</code></a> directory. These processed results serve as the basis for both feature activation analysis and steering experiments. To demonstrate the practical application of these tools, let's look at a concrete example using four sample input prompts, text completed in the terminal-UI with settings <code>max_new_tokens=128, temperature=0.7, top_p=0.9, seed=42</code>, demonstrated in figure 5:</p>
<div data-snippet-clipboard-copy-content="The delegates gathered at the
Foreign officials released a statement
Humanitarian staff coordinated their efforts
Senior diplomats met to discuss"><pre lang="text"><code>The delegates gathered at the
Foreign officials released a statement
Humanitarian staff coordinated their efforts
Senior diplomats met to discuss
</code></pre></div>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/testing_terminal.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/testing_terminal.webp" alt="Inference Testing in Terminal"></a>
  <br>
  <em>Fig 5: Terminal UI inference without feature steering</em>
</p>
<p dir="auto">Aside from feature activation analysis it is also possible to perform feature steering experiments with the same sample sentences and configuration. While this is also possible in the terminal UI does figure 6 show such a feature steering using the gradio UI for the sake of demonstration. In this example the latent #896 is targeted, which earlier analysis identified as representing "References to United Nations institutions, personnel, operations, or official documentation using formal institutional terminology". By increasing this latent's activation value by 20 through the dynamically adjustable <code>h_bias</code> (see <a href="#_sae-design_">section 3.1</a> for a reminder of the placement of <code>h_bias</code>) can the model's text completion successfully be steered toward UN-related content.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaulPauls/llama3_interpretability_sae/blob/main/.documentation/feature_steering_gradio.webp"><img src="https://github.com/PaulPauls/llama3_interpretability_sae/raw/main/.documentation/feature_steering_gradio.webp" alt="Feature Steering in Gradio UI"></a>
  <br>
  <em>Fig 6: Gradio UI inference with feature steering towards UN-related content</em>
</p>
<p dir="auto">The feature steering is not particularly strong in this first beta version of the project, hence why in the example above only the second and third sentence are flipping over to UN related content. Because of this the sample prompts were also chosen so that the start of the text completion provides a chance that the completion can be steered towards the United Nations, as for example feature steering towards the UN for a sentence that starts with "For any n, if 2n - 1 is odd" would certainly fail.</p>
<p dir="auto">This limitation of feature steering stems from the focus on feature extraction rather than steering optimization during interpretability analysis. However while this means that the steering capabilities produce inconsistent results, is it worth emphasizing that feature extraction alone provides valuable insights into the model. Therefore I just consider the ability to also perform feature steering a nice additional showcase and interesting demonstration in this first project release.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">6. Potential Future Work &amp; Improvements</h4><a id="user-content-6-potential-future-work--improvements" aria-label="Permalink: 6. Potential Future Work &amp; Improvements" href="#6-potential-future-work--improvements"></a></p>
<ul dir="auto">
<li>Expanding the latent dimension to at least 2^18 (262,144) features while reducing k to 32. This would provide more capacity for discovering unique features while maintaining stronger sparsity, potentially leading to more interpretable and monosemantic features. The increased computational demands would need to be offset somehow, by potentially increasing efficiency or implementing things like gradient accumulation.</li>
<li>Implementing comprehensive activation tracking of the latents, e.g. by often logging the state of the <code>latent_last_nonzero</code> tensor throughout training, rather than just using the basic debug logs I used so far. This would provide deeper insights into how and when latents become active or die and how their activation is distributed.</li>
<li>Adding support for analyzing feature interactions despite sparsity by tracking co-activation patterns in the latent space. Understanding how features work together could provide insights into more complex semantic representations and could potentially improve both interpretability and steering capabilities.</li>
<li>Developing more sophisticated interpretability analysis methods, particularly in grouping high-activating sentences and n-grams. While the current sentence-level analysis provides a good foundation, more nuanced approaches to pattern recognition could reveal subtler semantic features and improve our understanding of how the model represents information.</li>
<li>Similarly, also performing not only feature extraction interpretability analysis but also feature steering interpretability analysis, though admittedly in sufficiently sophisticated methods this would both coincide.</li>
<li>Extending the research to include Llama 3.1-8B model activations. Since it shares the same codebase as Llama 3.2, this would be a very straightforward extension that would essentially only require an adaptation of the hyperparameters and a lot of compute power.</li>
<li>Experimenting with different activation capture points, varying from depth into the model (particularly earlier layers) to using different capture points inside the transformer block (e.g. using the Attention Head Outputs or MLP outputs as experimented on by Google DeepMind)</li>
<li>Further optimize the auxiliary loss mechanism based on the surprisingly effective results in preventing dead latents. The current implementation already shows strong performance, but investigating the relationship between the minimum dead latents threshold and feature quality could lead to even better training dynamics.</li>
<li>Experimenting with modifications to the SAE architecture, particularly around the bias terms and main loss function. Given the success of the current implementation, targeted adjustments to these components could potentially improve both training stability and feature interpretability while maintaining the benefits of the current design.</li>
<li>Adding proper docstrings throughout the codebase. While I added inline documentation everywhere throughout the codebase would a proper addition of docstrings be very beneficial. This is not how I'd normally deliver production code though I simple didn't find the time to add proper docstrings to the codebase and I considered it sufficient for a first release of this sideproject.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this code in your research or project, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{pauls2024sae,
    title = {Llama 3 Interpretability with Sparse Autoencoders},
    author = {Paul Pauls},
    year = {2024},
    publisher = {GitHub},
    howpublished = {\url{https://github.com/PaulPauls/llama3_interpretability_sae}}
}"><pre><span>@misc</span>{<span>pauls2024sae</span>,
    <span>title</span> = <span><span>{</span>Llama 3 Interpretability with Sparse Autoencoders<span>}</span></span>,
    <span>author</span> = <span><span>{</span>Paul Pauls<span>}</span></span>,
    <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
    <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
    <span>howpublished</span> = <span><span>{</span>\url{https://github.com/PaulPauls/llama3_interpretability_sae}<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FORTH code for Chipwits is released in the game's 40th anniversary (118 pts)]]></title>
            <link>https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/</link>
            <guid>42207304</guid>
            <pubDate>Thu, 21 Nov 2024 18:44:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/">https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/</a>, See on <a href="https://news.ycombinator.com/item?id=42207304">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2497" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div data-ast-blocks-layout="true" itemprop="text">

		
		
<h2>Please <a href="https://store.steampowered.com/app/2330720/ChipWits/?utm_source=site">Wishlist</a> Our ChipWits Reboot!</h2>



<p>We’re so thrilled that our open source announcement is getting so much attention! If you’re interested purely in the open source release and how we recovered the original FORTH source code, please read on. If you’re curious to try out our reboot of this programming logic / puzzle game as well, please consider <a href="https://store.steampowered.com/app/2330720/ChipWits/?utm_source=site">wishlisting our project on Steam</a>. It helps support the open source work. Thank you!</p>



<h2>Open Source Announcement</h2>



<p>This month marks the 🎈🎈🎈 <strong>40th anniversary of ChipWits!</strong> 🎈🎈🎈 To celebrate, we’re releasing all of the original Mac and Commodore 64 FORTH source code (or as much as we could recover) as <strong>open source</strong>!</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="512" height="288" data-attachment-id="2524" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/40-anniversary-announcement-1/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?fit=512%2C288&amp;ssl=1" data-orig-size="512,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="40-anniversary-announcement-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?fit=512%2C288&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?resize=512%2C288&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?w=512&amp;ssl=1 512w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/40-anniversary-announcement-1.png?resize=150%2C84&amp;ssl=1 150w" sizes="(max-width: 512px) 100vw, 512px"></figure>



<p>You can find the code at <a href="https://github.com/chipwits/chipwits-forth" target="_blank" rel="noreferrer noopener">https://github.com/chipwits/chipwits-forth</a>. Read on for some history and analysis.</p>







<p>Our goal for open sourcing the original version of ChipWits is to ensure its legacy lives on. We also wanted to share an appreciation for what cross-platform software development for 8-bit microcomputers was like in 1984. If you are familiar with these systems, we’d love to hear from you, and you’re invited to help us explore and curate the original source code. Please <a href="https://discord.com/invite/D4G3np9RWm">join us on our Discord Server</a>.</p>



<h2>History of ChipWits and Choosing FORTH</h2>



<p>ChipWits was one of the first games to be released on the 1984 Macintosh. Co-creators Doug Sharp and Mike Johnston worked part-time porting educational computer games between 8-bit microcomputers to earn enough money to pay rent. During this time, they worked on the design for their first original game called ChipWits, a game about programming robots inspired by <a href="https://en.wikipedia.org/wiki/Rocky%27s_Boots" target="_blank" rel="noreferrer noopener">Rocky’s Boots</a>, <a href="https://en.wikipedia.org/wiki/Logo_(programming_language)" target="_blank" rel="noreferrer noopener">Logo</a> and other classic programming games of the time.</p>



<h3>Choosing FORTH</h3>



<p>In 1984, with much fanfare, Apple released the Macintosh personal computer. Support for development environments was limited at launch. The primary way to develop for the Mac was to purchase an <a href="https://en.wikipedia.org/wiki/Apple_Lisa" target="_blank" rel="noreferrer noopener">Apple Lisa</a>, the original version of which retailed for $9,995 ($30,373 in today’s dollars).</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="885" height="720" data-attachment-id="2500" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/885px-apple-lisa-1/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?fit=885%2C720&amp;ssl=1" data-orig-size="885,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="885px-Apple-lisa-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?fit=300%2C244&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?fit=885%2C720&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?resize=885%2C720&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?w=885&amp;ssl=1 885w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?resize=300%2C244&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?resize=150%2C122&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/885px-Apple-lisa-1.jpg?resize=768%2C625&amp;ssl=1 768w" sizes="(max-width: 885px) 100vw, 885px"><figcaption>An Apple Lisa Was too Expensive (Photo courtesy of Timothy Colegrove, CC-BY-SA 4.0)</figcaption></figure>



<p>Because it was far too expensive for an indie game studio to afford one of these systems, Doug and Mike decided to develop ChipWits using the MacForth Plus programming language running directly on the Mac.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="640" height="480" data-attachment-id="728" data-permalink="https://chipwits.com/2023/04/08/forth-programming-language-going-forth/macforth-disks/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?fit=640%2C480&amp;ssl=1" data-orig-size="640,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="macforth-disks" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?fit=640%2C480&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?resize=640%2C480&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?w=640&amp;ssl=1 640w, https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2023/04/macforth-disks.jpg?resize=150%2C113&amp;ssl=1 150w" sizes="(max-width: 640px) 100vw, 640px"><figcaption>MacForth: The Language of Innovation (photo courtesy of <a href="https://macintoshrepository.org/" target="_blank" rel="noreferrer noopener">macintoshrepository.org</a>)</figcaption></figure>



<p>Variants of FORTH were available on just about every 8-bit microcomputer at the time, which made it a great choice of language if the plan was to later port the game. They started development in April 1984 and it took about 7 months to develop and publish the first version of ChipWits. Development was intense (and we can assume fueled by much coffee and pie), but it was done in time for the Christmas Holiday Season!</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="577" height="447" data-attachment-id="2511" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/image-3-5/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?fit=577%2C447&amp;ssl=1" data-orig-size="577,447" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?fit=300%2C232&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?fit=577%2C447&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?resize=577%2C447&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?w=577&amp;ssl=1 577w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?resize=300%2C232&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/image-3.png?resize=150%2C116&amp;ssl=1 150w" sizes="(max-width: 577px) 100vw, 577px"><figcaption>ChipWits running on a Macintosh Plus (Photo courtesy of Maclife.com via WayBackMachine)</figcaption></figure>



<h3>Porting to Other Platforms</h3>



<p><a href="https://en.wikipedia.org/wiki/Epyx">Epyx</a> was supposed to be the game publisher for ChipWits, with Commodore 64 as their biggest market. However, it had no Mac games. Brainworks was aiming at the Educational market, which Apple owned at the time. So, Epyx published the game on Commodore 64 only. The C64 port was completed in 1985.</p>







<p>The Commodore 64 version was a challenging port, done in Super Forth 64. Doug and Mike wanted to keep the same general User Interface Design but that required developing a pull-down menu system using a joystick instead of a mouse. They added color graphics and SID chip sound and had to squeeze all of it onto a 165 KB disk (instead of 400 KB available on Mac disks). They also had to figure out how to utilize only 64 KB of RAM (instead of the 128 KB available on Mac), not to mention a slower processor.</p>



<p>After the success of the Commodore 64 version, they went on to create an Apple II version, also written in a variant of FORTH.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="938" data-attachment-id="2369" data-permalink="https://chipwits.com/retro/chipwits-title-retro-apple2/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?fit=1188%2C1088&amp;ssl=1" data-orig-size="1188,1088" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="chipwits-title-retro-apple2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?fit=300%2C275&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?fit=1024%2C938&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?resize=1024%2C938&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?resize=1024%2C938&amp;ssl=1 1024w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?resize=300%2C275&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?resize=150%2C137&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?resize=768%2C703&amp;ssl=1 768w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/09/chipwits-title-retro-apple2.png?w=1188&amp;ssl=1 1188w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>ChipWits on the Apple II</figcaption></figure>



<h2>Reviews and Awards</h2>



<p>ChipWits was an instant hit and was the subject of many <a href="https://chipwits.com/2023/02/17/reviews-of-classic-chipwits/">reviews and awards</a>. Here are a few:</p>



<ul>
<li>“ChipWits is the best, certainly most enjoyable, introduction to programming concepts that we’ve seen. Its icon based language is wonderfully innovative, and the program’s design is both refreshing and highly educational.” — <a href="https://vintageapple.org/macuser/pdf/MacUser_8601_January_1986.pdf" target="_blank" rel="noreferrer noopener">MacUser Editor’s Choice 1986 Award</a>&nbsp;</li>



<li>“We predict without hesitation that ChipWits will become a cult favorite among Macintosh users and their children of whatever age”. — <a href="https://dougsharp.wordpress.com/macazine-article-on-chipwits-development/">Macazine</a></li>



<li>“Not too many reviews come with a guarantee, but this one guarantees that if you like programming, you’ll love ChipWits.” — <a href="https://chipwits.com/2023/02/17/reviews-of-classic-chipwits/">MacWorld</a>&nbsp;</li>



<li>“Last night my wife came downstairs at 4:30 a.m. only to witness me cursing at a cartoon robot.” — <a href="https://www.atarimagazines.com/creative/v11n12/chipwits.php">Creative Computing</a></li>
</ul>



<p>In 2008, Maclife.com named ChipWits one of the <a href="https://web.archive.org/web/20080407170402/http://www.maclife.com/article/the_top_10_apple_games_of_all_time?">Top 10 Apple II / Mac Games of all time</a>!</p>



<h2>Play the Originals in Emulation!</h2>



<p>Before we dive into the code, if you’re curious to play the original versions of ChipWits, we were able to get them all working in an in-browser emulator (thanks to open source projects <a href="https://github.com/db48x/emularity" target="_blank" rel="noreferrer noopener">Emularity</a> and <a href="https://github.com/whscullin/apple2js" target="_blank" rel="noreferrer noopener">Apple2js</a>), which you can play right here:</p>



<figure><a href="https://chipwits.com/2024/09/12/play-retro-chipwits-now/" target="_blank" rel="noreferrer noopener"><img data-recalc-dims="1" loading="lazy" decoding="async" width="790" height="342" data-attachment-id="2502" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/chipwits-retro-emulation-4/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?fit=790%2C342&amp;ssl=1" data-orig-size="790,342" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="chipwits-retro-emulation" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?fit=300%2C130&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?fit=790%2C342&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?resize=790%2C342&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?w=790&amp;ssl=1 790w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?resize=300%2C130&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?resize=150%2C65&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-retro-emulation.png?resize=768%2C332&amp;ssl=1 768w" sizes="(max-width: 790px) 100vw, 790px"></a><figcaption>Play Retro ChipWits right from your browser!</figcaption></figure>



<h2>Recovering the Source Code</h2>



<p>While working on the new version of ChipWits, Doug Sharp found the original disks in his archive. When you consider the 3.5″ diskettes and 5.25″ floppy disks had been in home storage for 40 years, it’s a miracle that any data from the disks could be recovered at all. Not all the data made it, the vast majority of the disks read with no bad sectors at all!</p>



<h3>Commodore 64</h3>



<p>Reading these disks with modern hardware was a bit of an adventure. For the Commodore 64 disks, we ordered an original genuine <a href="https://en.wikipedia.org/wiki/Commodore_1541" target="_blank" rel="noreferrer noopener">1541 Commodore 64 Disk Drive</a> on eBay. The drive came with a USB&nbsp;<a href="https://kosciuskomedia.com/xoomfloppy-scaled-down-version-of-zoomfloppy/">XoomFloppy</a>&nbsp;adapter so the data can be read by a modern PC. Turning on the drive and listening to the gentle whir of the motor brought back so many memories.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="768" data-attachment-id="1001" data-permalink="https://chipwits.com/2023/09/23/update-more-original-code-disks-found/c64-1541/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?fit=1024%2C768&amp;ssl=1" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="c64-1541" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?fit=1024%2C768&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?resize=1024%2C768&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?resize=150%2C113&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2023/09/c64-1541.jpg?resize=768%2C576&amp;ssl=1 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Commodore 1541 Disk Drive</figcaption></figure>



<h3>Mac</h3>



<p>For the Mac 3.5″ diskettes, we learned that Mac used a proprietary sector layout to allow them to increase the capacity to 800 KB per disk, which makes them unreadable using modern drives using standard techniques. We ordered a PowerMac G3 WallStreet, a bridge system that still has a 3.5″ diskette drive that can read original Mac disks and also has a networking stack. However, that machine was only able to read the 800 KB disks because they disabled 400 KB support in the later versions of MacOS.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="465" data-attachment-id="2513" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/powermacg3wallstreet/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?fit=1415%2C642&amp;ssl=1" data-orig-size="1415,642" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="PowerMacG3WallStreet" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?fit=300%2C136&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?fit=1024%2C465&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?resize=1024%2C465&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?resize=1024%2C465&amp;ssl=1 1024w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?resize=300%2C136&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?resize=150%2C68&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?resize=768%2C348&amp;ssl=1 768w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/PowerMacG3WallStreet.jpg?w=1415&amp;ssl=1 1415w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>PowerMac G3 WallStreet, only able to read 800 KB disks unless OS was downgraded</figcaption></figure>



<p>Later, a fan gave us a tip to check out the <a href="https://decromancer.ca/greaseweazle/">GreaseWeazle</a>, a custom hardware controller for disk drive control over USB, which worked perfectly!</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="769" data-attachment-id="2512" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/greaseweazle-v4-1/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?fit=1280%2C961&amp;ssl=1" data-orig-size="1280,961" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="greaseweazle-v4.1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?fit=1024%2C769&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?resize=1024%2C769&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?resize=1024%2C769&amp;ssl=1 1024w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?resize=150%2C113&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?resize=768%2C577&amp;ssl=1 768w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/greaseweazle-v4.1.jpg?w=1280&amp;ssl=1 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The GreaseWeazle Drive Controller worked! (photo courtesy of <a href="https://decromancer.ca/greaseweazle/">decromancer.ca/greaseweazle</a>)</figcaption></figure>



<p>We methodically took block-by-block disk images of every disk in the archive set and compiled them into a GitHub repository. We used the disk label as the filename and prepended “bad” if there were any detected bad sectors. To make it easier for modern computers to read the contents we also wrote some Python scripts to extract blocks from the disks and reformat them as UTF-8 ASCII files. For the Commodore 64 this included converting their proprietary PETSCII to ASCII.</p>



<h2>Exploring the Source Code</h2>



<p>You can find all the results of our work: the raw disk images and code at <a href="https://github.com/chipwits/chipwits-forth" target="_blank" rel="noreferrer noopener">https://github.com/chipwits/chipwits-forth</a>.</p>



<p>FORTH code is incredibly concise and, some would say… elegant? The entire ChipWits source code for Mac is approximately 3,000 lines of Forth code. Pretty incredible considering our modernized port of the game is already 35,000 lines of C#.</p>



<p>We put together an article with an in-depth analysis of one “screen” of FORTH code which controls electrocrabs. See our post entitled <a href="https://chipwits.com/2023/11/11/forth-code-for-electrocrabs-in-1984-chipwits-deciphered/">Forth Code for Electrocrabs in 1984 ChipWits Deciphered</a>.</p>



<p>Here’s a quick peek at some of the other source code we found to be interesting. This really helps put you in the mindset of how game developers needed to think back in 1984. If you don’t know how to read FORTH code, just keep in mind it’s stack-based (similar to <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation" target="_blank" rel="noreferrer noopener">Reverse Polish Notation</a> if you’ve ever used an RPN calculator), so instead of writing <code>1 + 1</code> you would write <code>1 1 +</code>. Instead of writing <code>if random(3) == 1</code> you would write <code>3 irnd 1 = if</code>.</p>



<h3>Mac: Moving the Robot</h3>



<pre><code>( Move)                                           ( 103084 dws)
: Move ( ---)
     Calc.robot.move Check.for.next.room
     dup square.object dup floor@ = swap door@ = or
     if  init.cw.graphics
       arg.byte c@ 64 mod check.mov.st forward@ =
      if 10 0 do Cw.slide loop else 10 0 do Cw.back.slide loop
      then
      dup robot.square ! ( Walk on empty floor) draw.background
     else ( Hit something) square.object dup wall@ =
       if 1  Bump drop ( wall)
       else  5 bump ?boom drop then ( obj)
     then  robot.square @ 19 &lt;
     if  0 add.points then ( in case erased) ;</code></pre>



<p>Here’s the code for moving the robot in the Mac version of ChipWits. Unlike modern game development, there is no separate rendering loop for each frame. The code is imperative, mixing both game logic and graphics rendering together in a tight loop. Animations are achieved by rendering pixels directly in the loop (<code>CW.anim</code> is defined on another screen as a series of instructions to copy bitmaps directly to the screen).</p>



<p>Note the check as to whether the player rolls into an empty floor tile, hits a wall, or rams into a bomb is directly addressed in just a few words of code. The <code>?boom</code> definition (found in another screen) even decides if the game should end right then and there because the player touched a bomb.</p>



<p>The last line of code here is interesting in that, depending on the location of the robot, it explicitly adds 0 to the score in order to force a repaint of the score in case it was erased as the robot went sliding past.</p>



<h3>Commodore 64: Sprite Definitions</h3>



<pre><code>( head.op.w)                            
binary                                  
s-def head.op.w                         
00000000 00000000 00000000              
00000111 11111111 11000000              
00001001 11111111 11110000              
00001110 00000000 00000000              
00001111 01111111 11111000              
00001111 01111111 11111000              
00001111 01111111 11111100              
00001111 01111111 11111100              
00001111 01111111 11111100              
00011111 01111111 11000000              
00111111 01111111 10111100              
01111110 11111111 01111110              
00011101 11111111 01111110              
00000011 11111000 01111110              
00000000 00000001 11111110              
00110000 00001111 11111110              
00011100 01111111 10111100              
00001110 11111100 00000000              
00000111 01100000 00000000              
00000001 10000000 00000000              
00000000 00000000 00000000              
decimal                                 </code></pre>



<p>Here’s one screen from the Commodore 64 version. First, it was usually not possible or performant to load the entire source program in memory at once so source code in FORTH was not stored in files. It was stored in “screens”, each of which was numbered and loaded in a controlled manner.</p>



<p>It was common for code to mix asset data segments with source code. In this case, if you squint you can see a binary rendering of the ChipWit’s head facing west. Monochrome sprites on the Commodore 64 were 24 x 21 pixels and only 8 sprites could fit on the screen at once. You could choose to use a 4-color sprite but then your resolution would drop to 12 x 21 and pixels would be double-wide. This code reveals that the ChipWit is actually rendered using several sprites which are overlaid to form the character, much like animation cels in vintage cartoons. This design choice sacrificed using all the available sprites for just the ChipWit but you get a crisper image that can be animated in pieces.</p>



<h3>Mac: Thing Point Values</h3>



<pre><code>: d, 10 * c, ; create Scenario.play(              ( 110584 dws)
( Scen:  Greed  CW C  Doom  Peace MemLn Oct  Myst  Boom)
( crp.ct) 00 c, 08 c, 08 c, 04 c, 03 c, 03 c, 03 c, 03 c,
( crp.pt) 00 d, 05 d, 10 d, 03 d, 03 d, 01 d, 02 d, 04 d,
( bnc.ct) 00 c, 00 c, 02 c, 00 c, 02 c, 02 c, 02 c, 00 c,
( bnc.pt) 00 d, 00 d, 25 d, 06 d, 06 d, 02 d, 04 d, 00 d,
( oil.ct) 10 c, 06 c, 00 c, 04 c, 02 c, 00 c, 00 c, 00 c,
( oil.pt) 05 d, 05 d, 00 d, 02 d, 03 d, 05 d, 15 d, 00 d,
( dsk.ct) 03 c, 02 c, 00 c, 02 c, 00 c, 00 c, 00 c, 00 c,
( dsk.pt) 10 d, 10 d, 00 d, 15 d, 25 d, 25 d, 25 d, 25 d,
( cof.ct) 03 c, 04 c, 02 c, 03 c, 03 c, 01 c, 00 c, 00 c,
( pie.ct) 03 c, 00 c, 02 c, 03 c, 02 c, 04 c, 00 c, 02 c,

create room.ct( 4 c, 8 c, 12 c, 54 c, 1 c, 1 c, 100 c,  9 c,</code></pre>



<p>First, note the things we keep seeing that look like <code>( 110584 dws)</code>. This bewildered me until Doug pointed out that these are actually date stamps on the screens (110584 = Nov 5, 1984 and dws = Douglass Walter Sharp). We believe this is one of the last screens to be updated before the final release of the game. The last screen to change was on Nov 8.</p>



<p>This compact piece of code / data efficiently encodes the count and point values of each type of item for each mission. What stood out to me was the clever definition on the first line of <code>d,</code> to mean “multiply this value by 10” whereas <code>c,</code> means leave the value alone. This allowed Doug to encode point values like 150 as <code>15 d</code> which made it possible to keep a neatly aligned table that fit in a single screen. It also presumably made it possible to easily scale all point values by some factor (1, 2, 5, 10) as Mike and Doug were experimenting and balancing the scores until release day until it felt right.</p>



<h2>Please <a href="https://store.steampowered.com/app/2330720/ChipWits/?utm_source=site">Wishlist</a> Our ChipWits Reboot!</h2>



<p>Our team wanted to keep this post focused on the open source release of the original FORTH version of ChipWits, but we also wanted to mention that we’re hard at work on a modernized version as well. In the reboot of ChipWits, we’ve tried to preserve the original game’s fun factor and computer science rigor while enhancing every aspect of the game from learning curve to competitive global leaderboards. We’re hoping the new version of the game inspires a new generation of programmers!</p>



<p>Please <a href="https://store.steampowered.com/app/2330720/ChipWits/?utm_source=site" target="_blank" rel="noreferrer noopener">Wishlist ChipWits Now</a> to be notified when our final version is out!</p>



<p>Here’s a screenshot of one of our puzzles:</p>



<figure><a href="https://store.steampowered.com/app/2330720/ChipWits/?utm_source=site" target="_blank" rel="noreferrer noopener"><img data-recalc-dims="1" loading="lazy" decoding="async" width="1024" height="576" data-attachment-id="2517" data-permalink="https://chipwits.com/2024/11/16/chipwits-40th-birthday-original-forth-code-open-sourced/chipwits-reboot/" data-orig-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?fit=2560%2C1440&amp;ssl=1" data-orig-size="2560,1440" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="chipwits-reboot" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?fit=1024%2C576&amp;ssl=1" src="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot.jpg?resize=1024%2C576&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=150%2C84&amp;ssl=1 150w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?resize=2048%2C1152&amp;ssl=1 2048w, https://i0.wp.com/chipwits.com/wp-content/uploads/2024/10/chipwits-reboot-scaled.jpg?w=2400&amp;ssl=1 2400w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Four Corners Puzzle from the ChipWits Reboot</figcaption></figure>



<h2>We’d Like Your Help!</h2>



<p>These are just some of the enjoyable gems in the original FORTH source code for ChipWits. We’d love for you to explore and share what you find. <strong>Post your comments below!</strong></p>



<p>Also, we’ve created several issues in our <a href="https://github.com/chipwits/chipwits-forth" target="_blank" rel="noreferrer noopener">GitHub repo</a>. Our immediate goals are:</p>



<ol>
<li>Make sure we have all the relevant source code extracted from the raw disk images</li>



<li>Fix the alignment of some of the source files (some are 40 char width, some are 64 char)</li>



<li>Figure out which disk contains the most recent source code</li>



<li>Create instructions for how to build from source on Mac and C64</li>



<li>Extract the raw sprites and images from the disks into .png files</li>
</ol>



<p>Ultimately, we envision a community-maintained FORTH version of ChipWits for modern PCs.</p>



<p>If you’re interested in helping, come join our <a href="https://discord.com/invite/D4G3np9RWm">ChipWits Discord Server</a>!</p>






		
		
			</div>

	
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oxide Cuts Data Center Power Consumption in Half (143 pts)]]></title>
            <link>https://oxide.computer/blog/how-oxide-cuts-data-center-power-consumption-in-half</link>
            <guid>42206990</guid>
            <pubDate>Thu, 21 Nov 2024 18:12:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oxide.computer/blog/how-oxide-cuts-data-center-power-consumption-in-half">https://oxide.computer/blog/how-oxide-cuts-data-center-power-consumption-in-half</a>, See on <a href="https://news.ycombinator.com/item?id=42206990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><div id="preamble"><p>Here’s a sobering thought: today, data centers already consume <strong>1-2% of the world’s power</strong>, and that percentage will likely rise to <strong>3-4% by the end of the decade</strong>. According to <a href="https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demand">Goldman Sachs research</a>, that rise will include a doubling in data center carbon dioxide emissions. As the data and AI boom progresses, this thirst for power shows no signs of slowing down anytime soon. Two key challenges quickly become evident for the <a href="https://www.crn.com/news/cloud/2024/amazon-ceo-85-percent-of-it-spend-remains-on-premises-gen-ai-will-fuel-aws-cloud-sales">85% of IT</a> that currently lives on-premises.</p><div><ol><li><p><em>How can organizations reduce power consumption and corresponding carbon emissions?</em></p></li><li><p><em>How can organizations keep pace with AI innovation as existing data centers run out of available power?</em></p></li></ol></div><div><p><img srcset="https://oxide-computer.imgix.net//img/blog/power-consumption/goldman-datacenter-chart.svg?w=700 700w, https://oxide-computer.imgix.net//img/blog/power-consumption/goldman-datacenter-chart.svg?w=1400 1400w, https://oxide-computer.imgix.net//img/blog/power-consumption/goldman-datacenter-chart.svg?w=2100 2100w" sizes="(min-width: 600px) 700px, 1400px, 2100px" src="https://oxide.computer/img/blog/power-consumption/goldman-datacenter-chart.svg" alt="Graph of AI &amp; Data Center Growth Boosting Electricity Demand"></p><p>Figure 1. Masanet et al. (2020), Cisco, IEA, Goldman Sachs Research</p></div></div><div><h2 data-sectnum="">Rack-scale design is critical to improved data center efficiency</h2><div><p>Traditional data center IT consumes so much power because the fundamental unit of compute is <em>an individual server</em>; like a house where rooms were built one at a time, with each room having its own central AC unit, gas furnace, and electrical panel. Individual rackmount servers are stacked together, each with their own AC power supplies, cooling fans, and power management. They are then paired with storage appliances and network switches that communicate at arm’s length, not designed as a cohesive whole. This approach fundamentally limits organizations' ability to maintain sustainable, high-efficiency computing systems.</p><p>Of course, hyperscale public cloud providers did not design their data center systems this way. Instead, they operate like a carefully planned smart home where everything is designed to work together cohesively and is operated by software that understands the home’s systems end-to-end. High-efficiency, rack-scale computers are deployed at scale and operate as a single unit with integrated storage and networking to support elastic cloud computing services. This modern architecture is made available to the market as public cloud, but that rental-only model is ill-fit for many business needs.</p><div><p><img srcset="https://oxide-computer.imgix.net//img/blog/power-consumption/oxide-power-efficiency-illustration.svg?w=700 700w, https://oxide-computer.imgix.net//img/blog/power-consumption/oxide-power-efficiency-illustration.svg?w=1400 1400w, https://oxide-computer.imgix.net//img/blog/power-consumption/oxide-power-efficiency-illustration.svg?w=2100 2100w" sizes="(min-width: 600px) 700px, 1400px, 2100px" src="https://oxide.computer/img/blog/power-consumption/oxide-power-efficiency-illustration.svg" alt="Illustration of Oxide racks at a higher density (2x) than conventional ones"></p></div><p>At Oxide, we’ve taken this lesson in advancing rack-scale design, improved upon it in several ways, and made it available for every organization to purchase and operate anywhere in the world without a tether back to the public cloud. Our Cloud Computer treats the entire rack as a <strong>single, unified computer</strong> rather than a collection of independent parts, achieving unprecedented power efficiency.</p><p>By designing the hardware and software together, we’ve eliminated unnecessary components and optimized every aspect of system operation through a control plane with visibility to end-to-end operations.</p><div><blockquote><p>When we started Oxide, the DC bus bar stood as one of the most glaring differences between the rack-scale machines at the hyperscalers and the rack-and-stack servers that the rest of the market was stuck with. That a relatively simple piece of copper was unavailable to commercial buyers — despite being unequivocally the right way to build it! — represented everything wrong with the legacy approach.</p>
<p>The bus bar in the Oxide Cloud Computer is not merely more efficient, it is a concrete embodiment of the tremendous gains from designing at rack-scale, and by integrating hardware with software.</p></blockquote></div></div></div><div><h2 data-sectnum="">The improvements we’re seeing are rooted in technical innovation</h2><div><div><ul><li><p><strong>Replacing low-efficiency AC power supplies with a high-efficiency DC Bus Bar</strong><br>
Power conversion is performed once AC power is fed from the data center to the Oxide universal power shelf with a customized power shelf controller (PSC). The power shelf distributes DC power up and down the rack via a bus bar. <em>This eliminates the 70 total AC power supplies</em> found in an equivalent legacy server rack within 32 servers, two top-of-rack switches, and one out-of-band switch, each with two AC power supplies. This power shelf also ensures the load is balanced across phases, something that’s impossible with traditional power distribution units found in legacy server racks.</p></li><li><p><strong>Bigger fans = bigger efficiency gains</strong><br>
Oxide server sleds are designed to a custom form factor to accommodate larger fans than legacy servers typically use. These fans can move more air more efficiently, cooling the systems <em>using 12x less energy</em> than legacy servers, which each contain as many as 7 fans, which must work much harder to move air over system components.</p></li><li><p><strong>Purpose-built for power efficiency</strong><br>
Oxide server sleds have <em>less restrictive airflow</em> than legacy servers by eliminating extraneous components like PCIe risers, storage backplanes, and more. Legacy servers need many optional components like these because they could be used for any number of tasks, such as point-of-sale systems, data center servers, or network-attached-storage (NAS) systems. Still, they were never designed optimally for any one of those tasks. The Oxide Cloud Computer was designed from the ground up to be a rack-scale cloud computing powerhouse, and so it’s optimized for exactly that task.</p></li><li><p><strong>Hardware + Software designed together</strong><br>
The Oxide Cloud Computer includes a robust cloud control plane with deep observability to the full system. <em>By designing the hardware and software together</em>, we can make hardware choices like more intelligent DC-DC power converters that can provide rich telemetry to our control plane, enabling future feature enhancements such as dynamic power capping and efficiency-based workload placement that are impossible with legacy servers and software systems.</p></li></ul></div><p>Learn more about Oxide’s intelligent Power Shelf Controller</p>
<p>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/qzK4qbb8-Ts" title="Learn more about Oxide’s intelligent Power Shelf Controller" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></div></div><div><h2 data-sectnum="">The Bottom Line: Customers and the Environment Both Benefit</h2><div><p>Reducing data center power demands and achieving more useful computing per kilowatt requires <strong>fundamentally rethinking</strong> traditional data center utilization and compute design. At Oxide, we’ve proven that dramatic efficiency gains are possible when you rethink the computer at rack-scale with hardware and software designed thoughtfully and rigorously together.</p><p>Ready to learn how your organization can achieve these results? <strong>Schedule time with our team <a href="https://oxide.computer/sales">here</a>.</strong></p><p>Together, we can reclaim on-premises computing efficiency to achieve both business and sustainability goals.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Listen to the whispers: web timing attacks that work (104 pts)]]></title>
            <link>https://portswigger.net/research/listen-to-the-whispers-web-timing-attacks-that-actually-work</link>
            <guid>42206955</guid>
            <pubDate>Thu, 21 Nov 2024 18:08:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://portswigger.net/research/listen-to-the-whispers-web-timing-attacks-that-actually-work">https://portswigger.net/research/listen-to-the-whispers-web-timing-attacks-that-actually-work</a>, See on <a href="https://news.ycombinator.com/item?id=42206955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
    
<div>
    <p><img alt="James Kettle" src="https://portswigger.net/content/images/profiles/callout_james_kettle_112px.png"></p>
</div>
    <ul>
        <li>
            <p><span></span><strong>Published: </strong>07 August 2024 at 18:10 UTC</p>
        </li>
        <li>
            <p><strong>Updated: </strong>18 November 2024 at 08:32 UTC</p>
        </li>
        <li>
        </li>
    </ul>
    <p><img src="https://portswigger.net/cms/images/e0/18/c60b-article-listen-to-the-whispers-article.png" alt="An illustration showing a stop watch and a safe"><br></p><p>Websites are riddled with timing oracles eager to divulge their innermost secrets. It's time we started listening to them.
</p><p>In this paper, I'll unleash novel attack concepts to coax out server secrets including masked misconfigurations, blind data-structure injection, hidden routes to forbidden areas, and a vast expanse of invisible attack-surface.
</p><p>This is not a theoretical threat; every technique will be illustrated with multiple real-world case studies on diverse targets. Unprecedented advances have made these attacks both accurate and efficient; in the space of ten seconds you can now reliably detect a sub-millisecond differential with no prior configuration or 'lab conditions' required. In other words, I'm going to share timing attacks you can actually use.
</p><p>To help, I'll equip you with a suite of battle-tested open-source tools enabling both hands-free automated exploitation, and custom attack scripting. I'll also share a little CTF to help you hone your new skillset.
</p><p>Want to take things further? I'll help you transform your own attack ideas from theory to reality, by sharing a methodology refined through testing countless concepts on thousands of websites. We've neglected this omnipresent and incredibly powerful side-channel for too long.</p>
<p>This research paper accompanies a presentation at Black Hat USA and DEF CON:</p>
<p><iframe src="https://www.youtube.com/embed/zOPjz-sPyQM?origin=https://portswigger.net&amp;rel=0"></iframe></p>
<p>You can also read this whitepaper in a <a href="https://portswigger.net/kb/papers/ckizlam/listen-to-the-whispers.pdf" target="_blank">print-friendly PDF format</a>.</p><h3>Outline</h3><h3><a target="_blank" id="introduction">Background</a></h3><p>Web timing attacks are notorious for two things; making big promises, and failing to deliver. Examples are often theoretical, and even where a technique is dubbed 'practical' everyone knows it'll stop working as soon as you try to apply it outside a lab environment.
</p><p>This reputation might be why we've ignored a huge opportunity.</p>
<p>My first foray into researching timing attacks yielded results firmly in the 'theoretical' bucket. For my second attempt, I started by looking back over attacks that I'd successfully applied in the wild, alongside others that I'd read about:
</p>
<p><img src="https://portswigger.net/cms/images/96/ed/d893-article-timing-divide-1.png"><br></p>
<p>
From the top, these are examples of:
</p>  <ul>  <li><a target="_blank" href="https://bugzilla.mozilla.org/show_bug.cgi?id=761043">Cross-site search</a></li>  <li>Username enumeration</li>  <li>A probe for potential <a href="https://portswigger.net/web-security/race-conditions">race conditions</a></li>  <li><a target="_blank" href="https://martinschwarzl.at/media/files/compression.pdf">A lab-proven attack</a></li>  <li>A <a target="_blank" href="https://soatok.blog/2021/08/20/lobste-rs-password-reset-vulnerability/">theoretical attack</a> that would be absolutely amazing... if it actually worked</li>  </ul>
<p>In the hunt for novel techniques that work in the wild, I focused on the divide between the two categories, which is massive:</p><p><img src="https://portswigger.net/cms/images/43/b0/0a09-article-timing-divide-2.png"><br></p>
<p>
  Timing attack research is often focused on a single target, but this constrains its real-world value. I wanted techniques that could be applied to arbitrary live targets. To ensure my new attack concepts met this standard, I validated them on a test bed of 30,000 live websites. Based on <a target="_blank" href="https://github.com/sw33tLie/bbscope">bbscope</a> and Rapid7's <a target="_blank" href="https://opendata.rapid7.com/sonar.fdns_v2/">Project Sonar</a> DNS database, the test platform was a 20 GB Burp Suite project file containing every known website with a bug bounty program.
</p>
<p>Before this research, the smallest time gap I'd personally exploited was 30,000μs. Now, it's 200μs. This was made possible by massive advancements in timing-attack accuracy, and enables multiple powerful new techniques.</p>
<p>Three key attack techniques stood out as providing valuable findings on a diverse range of live systems: discovering hidden attack surface, server-side injection vulnerabilities, and misconfigured reverse proxies. In this paper, I'll explore each of these in depth. </p>

<h3><a target="_blank" id="taking">Making timing attacks that work everywhere</a></h3><p>All three techniques are now available in Param Miner so, if you wanted to, you could stop reading and try them out right now. The true value of this research comes from understanding that it doesn't stop here; these are just a sample of what's possible. Timing attacks can take you almost anywhere, but to grasp this potential, we need to start from the beginning.</p>
<p>Let's have a closer look at the key factors that real-world timing attacks live or die by, and how to overcome them. In this section, I'll show how to make timing attacks 'local', portable, and feasible.</p>
<h4><a target="_blank" id="answering">Answering difficult questions</a></h4><p>It's easy to assume that all web timing attacks are exploits, but this is a mistake because it limits your thinking around potential applications. At their core, web timing attacks are simply about answering difficult questions - ones that can't be answered by observing the server's response.</p>
<p>I started this research by attempting a timing-based exploit on password resets. It went badly, but nicely illustrates the gap between theory and reality. Many websites implement password resets by storing a secret token in their database and sending the token in a link to the user's registered email address. When the user clicks the link, the website compares the user-supplied token with the one in the database.</p>
<p>Under the hood, string comparisons typically compare one character at a time until they either finish the string or encounter a non-matching character pair. This means that the more characters match, the longer the comparison takes:</p>
<p><img src="https://portswigger.net/cms/images/ce/b9/3ece-article-string-comparison.png"><br></p>
<p>In this illustration, we're using two HTTP requests to ask the question 'Does the database contain a password reset token starting with d7e?' The server is taking one second to compare each character, so by comparing the response times an attacker can tell that the token starts with 'd7e' rather than 'd7f.
</p>
<p>Unfortunately, the actual time to compare each character is somewhere in the realm of 5 nanoseconds, or 0.000000005 seconds. Good luck exploiting that.
</p>
<h4><a target="_blank" id="signal">Noise vs signal</a></h4><p>The success of every timing attack comes down to two competing variables - signal and noise. Signal refers to the size of the timing difference you want to detect, and noise refers to everything else that affects the response timing. If the signal is too quiet relative to the background noise, you won't hear it:</p><p><img src="https://portswigger.net/cms/images/c7/b5/d824-article-equation.png"><br></p>
<p>For an attack that actually works, you need to maximize the signal and minimize the noise. The rest of this section is focused on how to do this.</p>
<p>Note that this equation does not include 'number of measurements'. You can attempt to cancel out noise by taking repeated measurements, but this approach scales poorly. Once noise heavily outweighs signal you'll quickly need billions of measurements, resulting in an attack that takes so long the target will probably be decommissioned before it's complete.</p>
<p>  You can split noise into two parts - network noise (jitter), and server noise (internal jitter):
</p>
<p><img src="https://portswigger.net/cms/images/21/36/0d64-article-latency.png"><br></p>
<p>
  Network jitter is the <i>variation</i> in latency - the time taken for a packet to get to a target system and back again. It's the classic nemesis of remote timing attacks. When someone sees a timing attack demonstrated against a local system and says 'That'll never work on a remote system', they're basically saying that network jitter is going to make the attack impossible. Five years ago, this might have been true.
</p>
<h4><a target="_blank" id="local">Making timing attacks 'local'</a></h4><p>In 2020, <a target="_blank" href="https://usenix.org/conference/usenixsecurity20/presentation/van-goethem">Timeless Timing Attacks</a> showed that you could fully eliminate network jitter from measurements using HTTP/2. You could place two HTTP/2 requests into a single TCP packet, ensuring they arrive at the server simultaneously. Then you could look at the order the responses arrive in, inferring which took longer to process on the server:</p><p><img src="https://portswigger.net/cms/images/d9/b0/f254-article-single-packet-latency.png"><br></p><p>This single discovery eliminated the biggest source of noise and shifted the boundaries of what's detectable. There's just one small catch.</p><h4><a target="_blank" id="sticky">The sticky request-order problem</a></h4><p>At the HTTP/2 layer, the two requests are completely concurrent, but the underlying TLS data is a stream so one request is still 'first' i.e. one will be fully decrypted before the other. If you try this technique out, you'll notice that websites show a significant bias towards answering the first request first. This bias probably stems from multiple factors, including the time it takes to decrypt the second request and resource availability. Unfortunately, this can mask the delay that you're trying to detect:</p><p><img src="https://portswigger.net/cms/images/55/d3/0a01-article-decryption-delay.png"><br></p><p>The authors noticed this problem and tackled it by adding dummy parameters to slow down parsing of the first request, in an attempt to resynchronise execution.</p>
<h4><a target="_blank" id="portable">Making timing attacks portable</a></h4><p>Lab environments are known for having less noise than real targets, but there's also a second, subtler issue. Focusing on a single target often yields target-specific techniques that require extensive tuning to apply anywhere else This makes them significantly less valuable for anyone working to a deadline.
</p><p>
  Unfortunately, dummy parameter padding is an example of this problem - its effectiveness depends on how the target implements parameter parsing, and how much processing capacity the system has available at that moment. Since spare processing capacity is affected by other systems, parameter padding can actually end up increasing the level of noise. I've observed different numbers of parameters being required on a single lab system, ten minutes apart.
</p><p>
  What we really need is a way of tackling the sticky request-order problem that doesn't require per-target configuration. The single-packet attack, which I developed last year for race-condition discovery, provides a good starting point for this. The single-packet attack fragments the request in order to reduce the size of the 'critical packet' - the packet that completes the request and initiates execution.
</p>
<p>
  It works by sending the bulk of the requests in an initial few packets, then completing the requests and triggering execution with a tiny final packet. In this diagram, the final critical packet is outlined in black:
</p><p><img src="https://portswigger.net/cms/images/4f/2f/dde4-article-single-packet-attack-old.png"><br></p>
<p>Unfortunately, this introduces a different catch - some servers start to process HTTP requests as soon as they've got the headers, without waiting for the body. To fix that, we need to persuade our OS network stack to coalesce the header frames into a single packet so that regardless of which stage the server starts processing at, both requests get processed at the same time:
</p><p><img src="https://portswigger.net/cms/images/b0/81/bd6a-article-single-packet-attack-new.png"><br></p><p>You might be wondering why I opted to split the requests into just two critical packets, instead of one packet per HTTP header. That would indeed be ideal, but unfortunately the HTTP/2 RFC forbids interleaving header frames from separate requests so it's unlikely to work.

</p><p>Implementing this dual-packet sync turned out to be extremely easy - just add an extra ping frame! This harmless sacrificial packet ensures that the operating system coalesces the subsequent header frames.
</p><p><code>
  disable TCP_NODELAY
  <span>send a ping frame</span>  for each request with no body:
  send the headers
  withhold an empty data frame
  for each request with a body:
  send the headers, and the body except the final byte
  withhold a data frame containing the final byte
  wait for 100ms
  send a ping frame
  send the final frames
</code></p><p>We integrated this improved technique into Burp Suite's built-in single-packet attack as soon as we discovered it, so you might have already benefited from it! I'm currently working with the developer of the open-source implementation <a target="_blank" href="https://github.com/nxenon/h2spacex">h2spacex</a> to get it in there too.</p>
<h4><a target="_blank" id="feasible">Making timing attacks feasible</a></h4><p>With network noise out of the picture, our next target is server noise. Do not underestimate server noise. It stems from numerous sources including load on the target server, other systems it interacts with, other virtual systems running on the same physical hardware, and probably the weather near the datacenter. Server noise is the reason I haven't made any claims about what time-delay you can expect to detect with the enhanced single-packet attack - any such claim is so target-specific it's effectively meaningless.
</p><p>To minimize server noise, take the shortest code path possible, and take full advantage of performance features like caching, object reuse, and connection reuse. Committed attackers may also reduce noise from other users using DoS techniques like <a target="_blank" href="https://portswigger.net/research/responsible-denial-of-service-with-web-cache-poisoning">CPDoS</a> and <a target="_blank" href="https://twitter.com/RSnake/status/1796634499813454171">resource consumption</a>.
</p><p>To maximize signal, focus on the slow code path and make it even slower by using random inputs to avoid server-side caching, incurring access to slow resources where possible, and multiplying the workload. For example, this request uses multiple headers with a fixed prefix to try to expand the delay caused by a server looking for a header starting with 'X-U': </p><p><code>GET / HTTP/1.1
X-Uaa: a
X-Ubb: a
X-Ucc: a
{256}
</code></p><p>Modern web technologies like <a target="_blank" href="https://www.elttam.com/blog/plorming-your-primsa-orm/">ORMs</a> and <a target="_blank" href="https://portswigger.net/web-security/graphql#bypassing-rate-limiting-using-aliases">GraphQL</a> also are particularly suited for delay-expansion techniques. Remember that a DoS attack is just a really easy timing attack and adapt classic techniques like ReDoS, batching, and recursive <a href="https://portswigger.net/web-security/xxe/xml-entities">XML entities</a>. </p>

<h3><a target="_blank" id="hidden">Hidden attack surface</a></h3><p>Vulnerabilities often lurk out of sight in disused and forgotten features that get overlooked by developers and security testers alike. As such, vulnerability discovery journeys often start with the detection of a hidden parameter, cookie, or HTTP header.
</p><p>
  At its core, discovering these hidden inputs involves guessing potential parameter names and observing if they change the response. Parameters that don't alter the response may remain undetected, alongside any associated vulnerabilities. For my first bulk timing-attack, I decided to fix this.
</p><p>
  Conveniently, I'm the core developer of <a target="_blank" href="https://github.com/PortSwigger/param-miner">Param Miner</a> - possibly the first tool for bulk parameter discovery. Param Miner compares responses using attributes like 'word count', 'status' and 'line count'. For this research, I simply added 'response time' as an extra attribute, bumped up the repeat count, and got scanning.
</p><p>
  I could have made Param Miner use the single-packet attack for these measurements, but this would have involved significant refactoring and, when researching unproven concepts, I take every possible shortcut to avoid wasting time, so I didn't bother.
</p><p>
  Instead I just measured the time from the last byte of the request to the first byte of the response, and compared the bottom quartile of the two sets of 30 timing measurements to see if they were distinct (indicating a valid parameter), or overlapped. The bottom quartile is ideal for this comparison because it reflects the measurements with the least noise.
</p>
<h4><a target="_blank" id="discovery">Discovery overload</a></h4><p>Running the time-augmented Param Miner on the test bed of 30,000 live sites yielded a huge number of hidden parameters, including some really weird ones.
</p><p>One highlight was a webserver that took 5ms longer to respond to requests containing the mystery HTTP header "commonconfig", unless the header value was valid JSON:
</p><table><tbody><tr><th>Header</th><th>Response</th><th>Time</th></tr>
<tr><td>foo: x</td>  <td>HTTP/1.1 200 OK</td>  <td> 50ms</td></tr>
<tr><td>commonconfig: x</td>  <td>HTTP/1.1 200 OK</td>  <td>55ms</td></tr>
<tr><td>commonconfig: {}</td>  <td>HTTP/1.1 200 OK</td>  <td>50ms</td></tr>
</tbody></table>

<p>Another discovery was on a webserver that refused to respond to any requests - it always reset the connection. This extremely defensive behavior wasn't sufficient to stop my scan discovering that it supported a certain HTTP header, because the header made it take significantly longer to reset the connection! Intriguing, but not terribly useful.</p><table><tbody><tr><th>Header</th><th>Response</th><th>Time</th></tr>
<tr><td>foo: x</td>  <td>--connection reset--</td>  <td> 30ms</td></tr>
<tr><td>authorization: x</td>  <td>--connection reset--</td>  <td>50ms</td></tr>
</tbody></table>

<p>One frequent finding was much more practical:</p><table><tbody><tr><th>Request</th><th>Response</th><th>Time</th></tr>
<tr><td>GET /?id=random</td>  <td>HTTP/1.1 200 OK</td>  <td> 310ms</td></tr>
<tr><td>GET /?foo=random</td>  <td>HTTP/1.1 200 OK</td>  <td> 22ms</td></tr>
</tbody></table>

<p>This pair of responses tells us two valuable things. First, the site is only including specific parameters like 'id' in the cache key, so it's highly exposed to <a target="_blank" href="https://portswigger.net/web-security/web-cache-poisoning/exploiting-implementation-flaws#:~:text=payload%2C%20for%20example.-,Unkeyed%20query%20string,-Like%20the%20Host">parameter-based cache poisoning attacks</a>. Second, we know the 'id' parameter is keyed and this configuration is typically done site-wide.  This means that using time analysis, Param Miner has detected a parameter that applies to a different page!</p>
<h4><a target="_blank" id="hardest">The hardest problem</a></h4><p>When I tried this concept out, I anticipated two problems. First, I expected many of the techniques to fail completely. Second, I suspected that any valid results I encountered would be hidden in a morass of false positives.</p>
<p>The biggest challenge came from neither. It's that timing attacks are too powerful. They can detect so much that it's incredibly easy to misunderstand what you've detected. They're incredibly good at detecting 'something', but that something isn't necessarily what you're trying to detect. </p><p><iframe src="https://www.youtube.com/embed/tZx_dmcOfqk?origin=https://portswigger.net&amp;rel=0" allowfullscreen="allowfullscreen"></iframe> illustrates this perfectly. This parameter detection looks like an RCE at first glance, then turns out to be something completely different (but still useful).</p>
<p>This video shows what initially looks like a potential remote code execution vulnerability due to an 'exec' parameter causing a visible response delay. This delay turns out to be an indicator of a WAF doing additional processing on more suspicious requests. We then see that the delay stacks when the parameter is repeated, unless the request body is over a certain size threshold. Ultimately this leads to the discovery of a complete WAF bypass. This bypass discovery was completely unexpected to me, but it's since been found by others and is now implemented in the <a href="https://github.com/assetnote/nowafpls" target="_blank">nowafpls</a> tool. It remains a beautiful demonstration of how timing analysis can reveal insights into the target's control flow.</p><p>That was one of the easy cases - sometimes you may never fully understand what you've detected. Carry your assumptions lightly and test them from different angles wherever possible.</p>
<h4><a target="_blank" id="proving">Proving the concept</a></h4><p>To avoid being misled by false assumptions, I decided to focus on specific parameters that provide a clear security impact without any time-consuming manual investigation and a straightforward way to gather additional corroborating evidence.
</p><p>IP address spoofing via HTTP headers fulfilled these requirements perfectly. It's a relatively common misconfiguration and directly enables various exploits including rate-limit bypasses, forged logs, and even <a href="https://portswigger.net/web-security/access-control">access control</a> bypasses in some cases. By placing an IP address in a spoofed front-end header, you're effectively impersonating the front-end. We'll explore front-end impersonation attacks in more depth later.
</p><p>Conveniently, if you place a domain inside a spoofed header, vulnerable servers will often perform an in-band DNS lookup to resolve it, causing an easily detectable delay. Here's a typical detection:
</p>
<table><tbody><tr><th>Header</th><th>Time</th></tr>
<tr><td>Random-header: xyz.example.com</td> <td> 65ms</td></tr>
<tr><td>True-Client-IP: xyz.example.com</td> <td>70ms</td></tr>
<tr><td>True-Client-IP: xyz.example.com</td> <td>65ms</td></tr>
</tbody></table>

<p>The first response comes back quickly because it doesn't trigger a DNS lookup. The second response triggers a DNS lookup for xyz.example.com, so it's slower, and the third response arrives faster because the DNS response has been cached:</p><p><img src="https://portswigger.net/cms/images/a1/d0/a28a-article-dns-caching.png"><br></p><p>We'll revisit DNS caching later. In total, scanning for IP address spoofing revealed:</p><ul><li>375 vulnerable domains</li><li>206 of these also caused a DNS pingback</li><li>217 visibly cached the result</li></ul>
<p>This might leave you wondering about the ~170 vulnerable domains that didn't cause a DNS pingback - were they false positives? Here's one example:</p>
<table><tbody><tr><th>Header</th><th>Time</th></tr>
<tr><td>Random-header: x.psres.net</td> <td>170ms</td></tr>
<tr><td>True-Client-IP: x.psres.net</td> <td>90ms</td></tr>
<tr><td>True-Client-IP: 1.1.1.1</td> <td>170ms</td></tr>
</tbody></table>

<p>What do you think is happening here?</p>
<p>Here's a clue - in your login history, the website specified the login IP address and location:</p>
<table><tbody><tr><th>Time</th><th>Browser</th><th>IP</th><th>Location</th></tr>
<tr><td>5 minutes ago</td> <td>Chrome in Windows</td><td>1.1.1.1</td> <td>Cloudflare</td></tr>
</tbody></table>

<p>
  I think this system was passing the spoofed IP address into a library, which validated the format before passing it to a third-party Geolookup service. Supplying an invalid IP address like 'x.psres.net' caused an exception and stopped the slow IP-lookup from happening:
</p><p><img src="https://portswigger.net/cms/images/b3/bc/ce0f-article-invalid-ip.png"><br></p><p>So, we've gained a new technique for parameter discovery, proved timing attacks can work at scale in the wild, and also spotted something significant: inputs that trigger errors can short-cut large code paths and result in significantly faster responses. In other words, timing attacks are exceptionally good at detecting exceptions</p>
<h3><a target="_blank" id="server-side">Server-side injection</a></h3>
<p>Triggering and spotting exceptions is a foundational part of testing for server-side injection vulnerabilities, from <a href="https://portswigger.net/web-security/sql-injection">SQLi</a> to <a href="https://portswigger.net/web-security/os-command-injection">OS command injection</a>. This makes timing analysis a perfect match for server-side injection detection.</p>
 <p>I attempted to replicate my success with Param Miner by adding 'time' as a response attribute to Backslash Powered Scanner, but this fell flat. Without the single-packet attack, I could only detect major time differences and these predominantly came from WAFs rather than real vulnerabilities. Also, the tool's complexity made it hard to adapt it to overcome challenges.</p>
  <p>For my second attempt, I reused some code from Param Miner to build a much simpler test that used the single-packet attack. I issued up to 50 request pairs per probe, and recorded the response order of each pair. If the response order was at least 80% biased towards one payload, I reported it as a valid finding.
</p>
<h4><a target="_blank" id="sqli">Fully blind SQLi</a></h4><p>The first finding was a fully blind SQL injection, detected with a classic payload pair:</p>
<table><tbody><tr><th>Request</th><th>Response</th><th>Time</th></tr>
<tr><td>GET /api/alert?mic='</td>  <td>{}</td>  <td> 162ms</td></tr>
<tr><td>GET /api/alert?mic=''</td>  <td>{}</td>  <td> 170ms</td></tr>
</tbody></table>

<p>Unfortunately, when I reported this it turned out to be a duplicate. In retrospect, I should have seen this coming - you could easily detect the same vulnerability using the well-known <span>'||sleep(5)||'</span> payload. Advanced timing analysis simply isn't required to detect vulnerabilities where you can inject sleep statements. Likewise, timing isn't great for finding code injection because you can normally find those better by using <a target="_blank" href="https://portswigger.net/research/hunting-asynchronous-vulnerabilities">OAST techniques</a>. </p>
<p>For powerful vulnerabilities like command injection, SQLi, and code injection, timing-based detection is only really useful when you've got a WAF or filtering in place that blocks the classic detection techniques. Let's look elsewhere.</p>
<h4><a target="_blank" id="json">Blind JSON injection</a></h4><p>Timing comes into its own when looking for the injection underclass; vulnerabilities that  allow manipulation of data structures and formats, but stop shy of full code execution. This includes injection into formats like JSON, XML, CSV, and server-side query parameters and HTTP headers. Many of these bugs are rarely spoken of because they're so hard to detect.
</p>  <p>They're hard to exploit too, but sometimes you can combine timing information with visible features to gain extra insight into what's happening behind the scenes. For example, I spotted one target where an invalid JSON escape sequence made the response come back 200us (0.2ms) faster:
</p>
<table><tbody><tr><th>Parameter</th><th>Response</th><th>Time</th></tr>
<tr><td>key=a\"bb</td>  <td>"error": {<br>&nbsp;&nbsp;"message": "Invalid Key: a\"bb"<br>}</td>  <td> 24.3ms</td></tr>
<tr><td>key=a"\bb</td>  <td>"error": {<br>&nbsp;&nbsp;"message": "Invalid Key: a"\bb"<br>}
</td>  <td> 24.1ms</td></tr>
</tbody></table>

<p>What do you think is happening server-side?</p>
  <p>There's a clue in the response formatting - the invalid syntax we injected hasn't altered the formatting in the response. I would expect a JSON formatter to fail when run on invalid syntax, or at least return visibly different output.</p>
<p>  Also, lengthy inputs got redacted in the response:
</p>
<table><tbody><tr><th>Parameter</th><th>Response</th><th>Time</th></tr>
<tr><td>key=aaa…a"bbb</td>  <td>"error": {<br>&nbsp;&nbsp;"message": "Invalid Key: ****bbb"<br>}
</td>  <td> 24.3ms</td></tr>
</tbody></table>

<p>This feature provides a second clue: when our invalid JSON sequence got redacted, the timing difference disappeared! Taken together, this strongly suggests that the delay is happening due to a component parsing the response being sent to us. My best guess is that it's some kind of error logging system. I was pretty pleased about figuring this out from a 0.2ms time differential but with no clear path to an exploit, I decided to move on.</p>

<h4><a target="_blank" id="parameter-pollution">Blind server-side parameter pollution</a></h4>
<p>My most prolific probe was for blind server-side parameter pollution. This worked by comparing the response times for reserved URI characters like ? and #, with non-reserved characters like !.</p><p>In some cases, sending an encoded # made the response come back faster:</p>
<table><tbody><tr><th>Request</th><th>Response</th><th>Time</th></tr>
<tr><td>/path?objectId=57%23</td>  <td>Can't parse parameter</td>  <td> 180ms</td></tr>
<tr><td>/path?objectId=57%21</td>  <td>Can't parse parameter</td>  <td> 430ms</td></tr>
</tbody></table>

<p>This could be due to the fragment breaking a server-side path and getting a speedy static response from the back-end, or the application's HTTP client simply refusing to send a HTTP request containing a raw #. Of course, it's crucial not to assume which way around the delay will land - on other targets, the encoded # made the response arrive slower.
</p>
<p>Server-side parameter pollution was the most common type of injection discovery by a huge margin, so I think it's a promising area for further research. For more information on this attack class, check out <a target="_blank" href="https://portswigger.net/web-security/api-testing/server-side-parameter-pollution">server-side parameter pollution</a>, and <a target="_blank" href="https://www.youtube.com/watch?v=hWmXEAi9z5w">Attacking Secondary Contexts in Web Applications</a>.</p>
<h4><a target="_blank" id="doppelgangers">Bug doppelgangers</a></h4><p>As we've seen, high-precision timing is great for detecting blind injection bugs but they aren't always easy to exploit. While analyzing these findings I often gained some understanding of what was happening server-side, but stalled short of actual exploitation. Also, timing tends to surface lesser-known attack classes that we're less familiar with exploiting.
</p><p>
  Gathering enough information for an exploit based purely on timing evidence is often tricky and time-consuming. Testing each idea on a regular, non-blind vulnerability typically involves a single repeater request, whereas with many of these, you're potentially looking at a 30-second Turbo Intruder attack.
</p><p>
  One thing that can help here is 'bug doppelgangers' - non-blind variations of the target bug class. Param Miner will report these, and they're great for learning how to interpret and exploit these bugs in a less challenging environment.
</p><p>
  Bug doppelgangers form part of a broader, recurrent theme from this research. If you ignore timing, you'll miss out, but if you focus too much on timing, you'll also miss out. For success, use every available information channel.
</p>

<h3><a target="_blank" id="reverse">Reverse proxy misconfigurations</a></h3><p>The single biggest breakthrough in this research was when I realized I could use timing to detect a widely overlooked type of SSRF.</p><p>
  Back in 2017, I <a target="_blank" href="https://portswigger.net/research/cracking-the-lens-targeting-https-hidden-attack-surface">researched techniques</a> to exploit misconfigured reverse proxies for SSRF and gain access to internal systems. The most common vulnerability was servers which routed requests to the domain specified in the HTTP Host header. To detect these, I would send them a request with a Host pointing to a domain I controlled:
</p><p><code>GET / HTTP/1.1
Host: uniq-token.burpcollaborator.net
</code></p><p>If the target was vulnerable, I would see my request arriving on my site at burpcollaborator.net, forwarded by the vulnerable reverse proxy. </p><p><img src="https://portswigger.net/cms/images/fe/e9/f6da-article-collaborator-smaller.png"><br></p><p>After that I would send internal IPs and hostnames to plunder their internal network. This yielded some spectacular findings, including accidentally hacking a system that my ISP put in place to MITM their customers.
</p>
<h4><a target="_blank" id="scoped-ssrf">Scoped SSRF</a></h4><p>Although successful, this detection technique had a major blind spot - scoped SSRF.</p>
  <p>After I published the research, someone from Google asked if I'd found any vulnerabilities in their systems, strongly implying that they had been vulnerable. Shortly later, Ezequiel Pereira posted <a target="_blank" href="https://web.archive.org/web/20200411123311/https://sites.google.com/site/testsitehacking/10k-host-header">$10k host header</a> in which he exploited an open proxy belonging to Google that I'd failed to detect. My scanning method had failed because Google's proxy was configured to only route requests to their own systems, so my server never received a DNS lookup.</p><p>This was a hint at a really common scenario, where companies allow request forwarding to arbitrary subdomains:
</p>
<table><tbody><tr><th>Host header</th><th>Full SSRF</th><th>Scoped SSRF</th></tr>
<tr><td>random.example.com</td>  <td>404 Not Found</td>  <td> 404 Not Found</td></tr>
<tr><td>random.notexample.com</td>  <td>404 Not Found</td>  <td>403 Forbidden</td></tr>
</tbody></table>

<p>I don't think there's an established name for this type of SSRF, so I'll call it scoped SSRF. This restriction can be implemented via an internal DNS server, simple hostname validation, a firewall blocking outbound DNS, or a tight listener config. The outcome is always the same - you've got a bug with an impact close to full SSRF, but it can't be detected using pingback/OAST techniques.</p>
<h4><a target="_blank" id="detecting-scoped-ssrf">Detecting scoped SSRF</a></h4><p>To detect scoped SSRF, we need to answer the question "Did the server try to connect to the specified hostname?". Timing is perfectly suited for this. Consider a server at www.example.com that issues the following responses:</p>
<table><tbody><tr><th>Host header</th><th>Response</th><th>Time</th></tr>
<tr><td>foo.example.com</td>  <td>404 Not Found</td>  <td>25ms</td></tr>
<tr><td>foo.bar.com</td>  <td>403 Forbidden</td>  <td>20ms</td></tr>
</tbody></table>

<p>These two responses show that it's doing some kind of validation on the Host header, but there isn't sufficient information to tell if it's an open proxy. If you rely on the response content, you'll end up with both false positives and false negatives.</p>
<p>The following request pair is what proves the issue - the faster second response is evidence of DNS caching:</p>
<table><tbody><tr><th>Host header</th><th>Response</th><th>Time</th></tr>
<tr><td>abc.example.com</td>  <td>404 Not Found</td>  <td>25ms</td></tr>
<tr><td>abc.example.com</td>  <td>404 Not Found</td>  <td>20ms</td></tr>
</tbody></table>

<p>Some DNS systems don't cache failed DNS lookups, but I found an alternative solution for this - sending an overlong 64-octet DNS label, leading to the DNS client refusing to issue the lookup and a faster response:</p>
<table><tbody><tr><th>Host header</th><th>Response</th><th>Time</th></tr>
<tr><td>aaa{62}.example.com</td>  <td>404 Not Found</td>  <td>25ms</td></tr>
<tr><td>aaa{63}.example.com</td>  <td>404 Not Found</td>  <td>20ms</td></tr>
</tbody></table>


<h4><a target="_blank" id="sifting">Sifting for secret routes</a></h4>
<p>Scanning with these techniques revealed hundreds of vulnerable reverse proxies, exposing alternative routes to tens of thousands of domains - I desperately needed automation.</p>
<p>When you find an open reverse proxy, the first step is to try using it to access every possible destination. I wrote code to automatically compile a list of target subdomains using three main sources:</p><ul>  <li>A hard-coded, generic subdomain wordlist</li>  <li>A list of known subdomains sourced from <a target="_blank" href="https://opendata.rapid7.com/sonar.fdns_v2/">Rapid7's Project Sonar 'fdns'</a> file. To quickly parse this 58 GB file for subdomains of a specific target, I used 'rev' to reverse every line, then sorted it alphabetically so sibling domains were next to each other. I then ran the old-school unix 'look' utility to do a binary search. This cut the search time by 99.999% versus grep.</li>  <li>The online subdomain service at columbus.elmasy.com, which is mostly compiled from certificate transparency logs</li></ul><p>I made Param Miner try to access each host twice - once directly and once through the proxy - and report any hosts where the two access attempts triggered significantly different responses. When comparing responses, I focused on response status code, header names, and the Location header as these were the highest-signal areas. This yielded numerous findings, which fell into four broad categories.</p>
<p>Guessing hostnames directly in the Host header is often referred to as 'vhost bruteforcing', but reverse-proxy exploitation often looks completely different, so it's important to understand the distinction. Virtual-host bruteforcing only provides access to other websites on the same server. Meanwhile, reverse proxies will route requests to different systems, enabling unique attacks like front-end rule bypass, front-end impersonation, and exploit chaining opportunities. Let's dive in.</p>

<h4><a target="_blank" id="firewall">Firewall bypass</a></h4><p>The simplest exploit is where you can see the target from outside but can't directly access it.</p><p><img src="https://portswigger.net/cms/images/39/da/b2b6-article-firewall-bypass.png"><br></p><p>
  On one company, sonarqube.redacted.com resolved to a public IP address, but attempting to access it triggered a connection reset from a firewall. My probes had identified app.redacted.com as a reverse proxy and, using that, I was able to route around the firewall and access the internal SonarQube instance.
</p><table><tbody><tr><th>Entry point</th><th>Host header</th><th>Result</th></tr>
<tr><td>sonarqube.redacted.com</td>  <td>sonarqube.redacted.com</td>  <td> --reset--</td></tr>
<tr><td>app.redacted.com</td>  <td>sonarqube.redacted.com</td>  <td>200 OK</td></tr>
</tbody></table>

<h4><a target="_blank" id="firewall-variant">Firewall bypass - invisible route variant</a></h4><p>There's a common variation where the internal system doesn't have a convenient public DNS record to let you know it exists:</p><p><img src="https://portswigger.net/cms/images/fc/d9/ed2c-article-no-hostname.png"><br></p><p>There are a huge number of pre-prod, staging, and development servers exposed to anyone applying this technique. If you get lucky, they'll have debugging enabled or test credentials configured, making them soft targets. These systems may even have real target data, or reused keys from production.
</p><p>
  The most interesting targets I found were pre-launch systems still under active development. In particular, I discovered an admin console with apparently-public access on a really cool US government system, which I'm gutted I can't provide any details about. I reported the issue and the system went 'live' a few months later, but the admin console is nowhere in sight.
</p>
<h4><a target="_blank" id="rule-bypass">Front-end rule bypass</a></h4><p>Some targets are publicly accessible, but sit behind front-end servers that enforce inconvenient security rules that block attacks or restrict access to valuable endpoints. The classic way to handle these is by talking directly to the back-end, but that's often impossible due to firewalls.</p>
<p>Reverse proxies provide a compelling alternative - go around the barrier:</p>
<p><img src="https://portswigger.net/cms/images/a2/07/2d37-article-rule-bypass.png"><br></p>
<p>On one target, using an alternative route via a reverse proxy turned this:</p><p><img src="https://portswigger.net/cms/images/d1/10/f318-article-forbidden.png"><br></p><p>Into this:</p><p><img src="https://portswigger.net/cms/images/97/13/24b7-article-unforbidden.png"><br></p>

<h4><a target="_blank" id="front-end-impersonation">Front-end impersonation attacks</a></h4><p>The most spectacular and surprising exploits happen when there's a trust relationship between the front-end and back-end. It's common knowledge that you can use headers like X-Forwarded-For to spoof your IP address. What's less appreciated is that this is part of a much broader and more powerful bug class. This type of attack has no established name, so I'll call it a front-end impersonation attack.</p>
  <p>Front-end systems often add HTTP headers onto requests before forwarding them to the back-end. These contain additional information that the back-end might find useful, such as the user's remote IP address, and the originating protocol. More complex deployments sometimes use custom headers to transmit critical authentication information. Back-end servers trust these headers implicitly.</p>
  <p>If an attacker attempts to spoof these headers, the front-end will typically overwrite them. This header overwriting behavior is the single brittle line of defense against front-end impersonation attacks.
</p>
<p><img src="https://portswigger.net/cms/images/c7/8b/a600-article-impersonation.png"><br></p>
<p>The easiest way to bypass this defense is to simply talk directly with the back-end, but this is usually impossible due to network firewalls. Another approach is <a target="_blank" href="https://portswigger.net/web-security/request-smuggling/advanced/request-tunnelling">HTTP request tunneling</a>, which I used to <a target="_blank" href="https://portswigger.net/research/http2#guessing">completely compromise New Relic's core internal API</a> using a header called "Service-Gateway-Is-Newrelic-Admin". You can also try <a target="_blank" href="https://www.intruder.io/research/practical-http-header-smuggling">obfuscating headers</a> to smuggle them past the front-end.</p><p>Misconfigured proxies offer an elegant alternative way to bypass header-overwriting defenses and perform front-end impersonation attacks. To try this out,</p><ul>  <li>Use Param Miner's 'Detect scoped SSRF' scan to detect a reverse proxy</li>  <li>Run 'Exploit scoped SSRF' to find alternative routes to internal systems</li>  <li>On each alternate route, run 'Guess headers' to find useful headers</li></ul>  <p>Applying this successfully requires a robust mental visualization of what's happening behind the scenes. To help out, I've made a little CTF at <a target="_blank" href="https://listentothewhispers.net/">listentothewhispers.net</a> - see if you can crack it!
</p>
<h4><a target="_blank" id="chaining">Chaining</a></h4><p>Finally, scoped SSRF via reverse proxies offers some great exploit chaining opportunities.
</p><p>
  If you're able to take over a subdomain on the target company and point the DNS record to an arbitrary IP address, you can use this to upgrade a scoped SSRF into a full SSRF and hit arbitrary IP addresses. This is a lot like chaining a traditional SSRF with an open redirect.
</p><p>
  Since reverse proxies let you pick your back-end, they're great for HTTP request smuggling. I didn't have time to properly explore this concept. In short, I think you'll find that, while it should be easy to find back-ends that are vulnerable to request smuggling, cross-user exploitation will often be impossible because no legitimate users will be sharing your front-end/back-end connection. To prove the impact, you'll need to pursue tunneling-based exploits like front-end impersonation and header disclosure.
</p>

<h3><a target="_blank" id="next">What's next?</a></h3>

<h4><a target="_blank" id="plan">My plan</a></h4>
<p>
My goal for this research is to get people using timing attacks day to day. As such, I plan to spend the next month improving the tooling in Param Miner and Turbo Intruder. In particular, I think it's possible to make most timing attacks quite a bit faster simply by using the t-test to decide whether to report a discovery, bail, or get more samples. I'll also be looking out for user feedback - if you have any requests or thoughts, let me know via <a target="_blank" href="https://github.com/portswigger/param-miner/issues">Github</a> or <a target="_blank" href="https://jameskettle.com/#contact">send me an email</a>.
</p>
<h4><a target="_blank" id="roadmap">Timing research roadmap</a></h4><p>
These findings have just scratched the surface, and timing attacks still have massive potential for further research. If you're interested to see where this attack class might go next, or pushing it further yourself, there are many different avenues to consider.
</p><p>I think the single most valuable area is looking for new applications of timing attacks. This is relatively easy, and doesn't require a major time commitment just to get started. The main hazard here is accidentally pursuing a concept where the signal you need to detect is drowned out by noise. Fortunately, this is easy to avoid. Start by thinking about the cause of the delay. Does it come from an extra interaction with a remote system, LAN system, hard disk, RAM, or CPU register? Once you're working at the right level, consider building a local benchmark to measure the signal size that you'll need to detect.
</p><p>
If the signal is too small, explore amplification techniques. Remember that most DoS attacks are really just timing attacks, and embrace them. Maybe you can expand the delay using nested XML entities, ReDoS, or hashtable collisions.
</p><p>
Jitter-reduction techniques are incredibly valuable and widely overlooked too - there may be some great techniques waiting for someone to research this area.
</p><p>
There's also scope for universal, technique-level improvements. Maybe the single-packet attack works better if you fragment at the TCP layer. Perhaps it's more effective to send ten requests in a single packet instead of two?
</p><p>
Finally, whichever path you take, try to resist the lure of hyper-focus on a single target - generic and reusable techniques contribute far more to the development of the field.
</p>
<h3><a target="_blank" id="defence">Defence</a></h3><p>
Timing attacks are hard to defend against. First and foremost, developers should understand that attacker visibility into their system's inner workings goes beyond the actual response content.
</p><p>
It's safest to over-estimate attackers' capabilities. Assume an attacker can read every line of code that gets executed. This is similar to your code being open-source, but slightly more serious because live data will affect the execution flow. Attackers can't directly access variables, but they can see which branches get taken and how many iterations each loop goes through.
</p><p>
It's especially important to take this into account when implementing performance optimisations such as caching as these tend to provide a massive signal. To mitigate attacks that exploit smaller signals, you could try breaking the single-packet attack by implementing a rate limit restricting each IP address to one request per 1-5 ms.
</p><p>
Likewise if you're a WAF vendor, consider detecting when a single packet contains multiple HTTP requests and breaking them down into separate packets with a tiny delay between each.
</p><p>
Finally, yes I do recommend using constant-time functions when comparing user input with  secret keys. Just ask anyone who says this is an actual threat to provide a proof of concept.
</p><h3><a target="_blank" id="takeaways">Takeaways</a></h3><p>
It's not just about the exploits. At their core, web timing attacks are about answering difficult questions.
</p><p>With the single-packet attack, web timing attacks have become  'local', portable, and feasible.
</p><p>
Timing oracles are everywhere. Whatever you're testing, timing murmurs are always present, waiting for you to listen.
</p><p>
Enjoy!
</p>
    
    <p><a href="https://portswigger.net/research/articles">Back to all articles</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OK, I can partly explain the LLM chess weirdness now (110 pts)]]></title>
            <link>https://dynomight.net/more-chess/</link>
            <guid>42206817</guid>
            <pubDate>Thu, 21 Nov 2024 17:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/more-chess/">https://dynomight.net/more-chess/</a>, See on <a href="https://news.ycombinator.com/item?id=42206817">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>We recently talked about a <a href="https://dynomight.net/chess/">mystery</a>: All large language models (LLMs) are terrible at chess. All, that is, except for <code>gpt-3.5-turbo-instruct</code>, which for some reason can play at an advanced amateur level. This is despite the fact that this model is more than a year old and much smaller than recent models. What’s going on?</p>

<p>I suggested four possible explanations:</p>

<ul>
  <li>
    <p><strong>Theory 1:</strong> Large enough <em>base</em> models are good at chess, but this doesn’t persist through instruction tuning to chat models.</p>
  </li>
  <li>
    <p><strong>Theory 2</strong>: For some reason, <code>gpt-3.5-turbo-instruct</code> was trained on more chess data.</p>
  </li>
  <li>
    <p><strong>Theory 3:</strong> There’s something magical about certain LLM architectures.</p>
  </li>
  <li>
    <p><strong>Theory 4:</strong> There’s “competition” between different types of data, so for an LLM to play chess well, you need a large <em>fraction</em> of the data to be chess games.</p>
  </li>
</ul>

<p>The internet offered several other theories. The most common were:</p>

<ul>
  <li>
    <p><strong>Theory 5</strong>: OpenAI is cheating.</p>
  </li>
  <li>
    <p><strong>Theory 6</strong>: LLMs can’t actually play chess.</p>
  </li>
  <li>
    <p><strong>Theory 7</strong>: Large enough <em>base</em> models are good at chess, but this doesn’t persist through instruction tuning to chat models, Dynomight you are so bad for not suggesting this, how are you so dumb and bad?</p>
  </li>
</ul>

<p>I’ve now done new experiments and—good news—everyone is wrong!</p>

<p>Here, I’ll show that recent chat models <em>can</em> play chess quite well, as long as you’re willing to go through sufficiently extreme contortions to figure out how to prompt them. Then I’ll give my theory for what’s happening.</p>

<p>But first…</p>

<h2 id="i-really-dont-think-openai-is-cheating">I really don’t think OpenAI is cheating</h2>

<p>I was astonished that half the internet is convinced that OpenAI is cheating. Many, <em>many</em> people suggested that there must be some special case in <code>gpt-3.5-turbo-instruct</code> that recognizes chess notation and calls out to an external chess engine.</p>

<p>I think this is <em>extremely</em> unlikely. Because:</p>

<ol>
  <li>
    <p>Many people from OpenAI have said they didn’t do that. Sure, people lie, but conspiracies are hard, and why lie about <em>this</em>?</p>
  </li>
  <li>
    <p>In chess, you can arrive at the same board state via different sequences of moves. Chess engines don’t care, but <code>gpt-3.5-turbo-instruct</code> <em>does</em> care and <a href="https://nicholas.carlini.com/writing/2023/chess-llm.html#:~:text=Language%20Modeling;%20Not%20Winning%20(Part%202)">plays very differently</a> for different move sequences.</p>
  </li>
  <li>
    <p>While <code>gpt-3.5-turbo-instruct</code> is great by the standards of chess amateurs, it’s bad by the standards of experts and <em>pathetic</em> by the standards of chess engines. If you’re going to cheat, why stop at an Elo of 1800?</p>
  </li>
  <li>
    <p>If you change the way you prompt <code>gpt-3.5-turbo-instruct</code>, this will subtly change how it plays. Is there some neural network that looks at the text and dynamically sets the chess engine skill level?</p>
  </li>
  <li>
    <p>Later OpenAI models are by default much worse. Did they remove the cheat?</p>
  </li>
  <li>
    <p>I will show (below) that later OpenAI models can also play well if you use the right incantations.</p>
  </li>
</ol>

<p>If OpenAI <em>did</em> cheat, they went to insane lengths to cheat in a way that looks exactly like an LLM is choosing the moves and not at all like calling out to an external chess engine.</p>

<h2 id="yes-llms-can-play-chess">Yes, LLMs can play chess</h2>

<p>I was also surprised to see so many people suggest that LLMs can’t <em>really</em> play chess, all they do is memorize openings and then play randomly.</p>

<p>This is wrong. LLMs can definitely play chess, and we need to make peace with this.</p>

<p>For one, <code>gpt-3.5-turbo-instruct</code> rarely suggests illegal moves, even in the late game. This requires “understanding” chess. If this doesn’t convince you, I encourage you to write a program that can take strings like <code>1. e4 d5 2. exd5 Qxd5 3. Nc3</code> and then say if the last move was legal.</p>

<p>And I defy you to maintain that LLMs can’t play chess after looking at some actual games. Here are ten: <a href="https://lichess.org/iwi02kUm">1</a> <a href="https://lichess.org/bfaDMlVm">2</a> <a href="https://lichess.org/lNE5mKPO">3</a> <a href="https://lichess.org/C3xV1uAz">4</a> <a href="https://lichess.org/YpzT2KQS">5</a> <a href="https://lichess.org/dy9m2DsU">6</a> <a href="https://lichess.org/4YDx633U">7</a> <a href="https://lichess.org/ZM9ZbDfo">8</a> <a href="https://lichess.org/59cTZSjs">9</a> <a href="https://lichess.org/DmeQQx7Y">10</a>. It plays pretty well even in completely new board states that have never existed in any game before in history.</p>

<h2 id="so-whats-happening">So what’s happening?</h2>

<p>Why is one LLM great and all the others terrible?</p>

<p>Let me remind you of what I’m talking about. First, take <code>gpt-3.5-turbo-instruct</code>. This is a “completion” model, meaning all it does is take some text and generate new text that might come after. I gave it text like this:</p>

<div><p><code>[Event "Shamkir Chess"]</code><br>
<code>[White "Anand, Viswanathan"]</code><br>
<code>[Black "Topalov, Veselin"]</code><br>
<code>[Result "1-0"]</code><br>
<code>[WhiteElo "2779"]</code><br>
<code>[BlackElo "2740"]</code></p><p>

<code>1. e4 e5 2. Nf3 Nc6 3.</code></p></div>

<p>I then took the first few characters and used them as a move.</p>

<p>Next, take <code>gpt-4o-mini</code> and <code>gpt-4o</code>. These are “chat” models, meaning you give them a “system prompt” that says what they’re supposed to do and a “user prompt” and then they try to answer you. I used this system prompt:</p>

<p><code>You are a chess grandmaster.</code><br>
<code>You will be given a partially completed game.</code><br>
<code>After seeing it, you should choose the next move.</code><br>
<code>Use standard algebraic notation, e.g. "e4" or "Rdf8" or "R1a3".</code><br>
<code>NEVER give a turn number.</code><br>
<code>NEVER explain your choice.</code></p>

<p>And I used user prompts like this (repeating the system prompt):</p>

<div><p><code>You are a chess grandmaster.</code><br>
<code>You will be given a partially completed game.</code><br>
<code>After seeing it, you should choose the next move.</code><br>
<code>Use standard algebraic notation, e.g. "e4" or "Rdf8" or "R1a3".</code><br>
<code>NEVER give a turn number.</code><br>
<code>NEVER explain your choice.</code></p><p>

<code>[Event "Shamkir Chess"]</code><br>
<code>[White "Anand, Viswanathan"]</code><br>
<code>[Black "Topalov, Veselin"]</code><br>
<code>[Result "1-0"]</code><br>
<code>[WhiteElo "2779"]</code><br>
<code>[BlackElo "2740"]</code></p><p>

<code>1. e4 e5 2. Nf3 Nc6 3.</code></p></div>

<p>Here are the results of these three models against Stockfish—a standard chess AI—on level 1, with a maximum of 0.01 seconds to make each move. After the game was over, I calculated the score after each turn in “centipawns” where a pawn is worth 100 points, and ±1500 indicates a win or loss. Here is the average over 50 games (click to zoom in):</p>

<p><a href="https://dynomight.net/img/more-chess/old.pdf"><img src="https://dynomight.net/img/more-chess/old.svg" alt=""></a></p>

<p><em>Note</em>: Last time I found that <code>gpt-3.5-turbo-instruct</code> won every single game and <code>gpt-4o</code> <em>lost</em> every game. I think the difference now is just that I have 50 samples rather than 10.</p>

<p>So that’s the mystery: <code>gpt-3.5-turbo-instruct</code> is great, and other models are terrible. Last time I tested lots of open models and they were terrible too. Why?</p>

<p>To answer this, let’s see if we can make these other models play chess better.</p>

<h2 id="should-we-fiddle-with-the-prompt">Should we fiddle with the prompt?</h2>

<p>You can nitpick at how I prompted the chat models. Is it a good idea to repeat the system prompt at the top of the user prompt? Is it a good idea to add all the metadata like user names before you start listing moves?</p>

<p>As far as I can tell, no one knows. So I tried every combination of these things being on or off. With <code>gpt-4o-mini</code> (a small model) it seemed to make little difference.</p>

<p><a href="https://dynomight.net/img/more-chess/gpt-4o-mini-variants.pdf"><img src="https://dynomight.net/img/more-chess/gpt-4o-mini-variants.svg" alt=""></a></p>

<p>With <code>gpt-4o</code> (a bigger model) it… <em>maybe</em> made a difference?</p>

<p><a href="https://dynomight.net/img/more-chess/gpt-4o-variants.pdf"><img src="https://dynomight.net/img/more-chess/gpt-4o-variants.svg" alt=""></a></p>

<p>Taken at face value, this says that repeating the system prompt helps a bit but metadata hurts a bit. But I’m not sure if that’s real or just noise. For simplicity, I decided to not repeat the system prompt and to turn off metadata for all further experiments.</p>

<h2 id="should-we-add-examples">Should we add examples?</h2>

<p>If you want LLMs to do something, standard advice is to provide some examples. So I did.</p>

<ul>
  <li><strong>Input A</strong>: <code>1.</code></li>
  <li><strong>Output A</strong>: <code>e4</code></li>
  <li><strong>Input B</strong>: <code>1. e4</code></li>
  <li><strong>Output B</strong>: <code>d5</code></li>
  <li><strong>Input C</strong>: <code>1. e4 e5 2. Nf3 Nc6 3.</code></li>
  <li><strong>Output C</strong>: <code>Bb5</code></li>
</ul>

<p>That’s all I used, just those three examples. I provided these in “correctly” using the API, not by jamming them into the user prompt. The results were:</p>

<p><a href="https://dynomight.net/img/more-chess/examples.pdf"><img src="https://dynomight.net/img/more-chess/examples.svg" alt=""></a></p>

<p><em>Very</em> good.</p>

<p>Is this surprising? I thought this was surprising.</p>

<p>I mean, sure, this kind of “in-context learning” is a big part of what makes LLMs so exciting. And examples are probably <em>the</em> most standard piece of advice from practitioners.</p>

<p>Still, I was blown away that three tiny examples could have such a <em>profound</em> effect on performance. More (or different) examples might be even better. I didn’t check, because generating each of these figures requires an ungodly number of queries.</p>

<h2 id="should-we-fine-tune">Should we fine-tune?</h2>

<p>Another standard (albeit more difficult) way to improve LLMs is to fine-tune—to optimize the weights to be good at whatever task using data for that task.</p>

<p>So I did this for both <code>gpt-4o-mini</code> and <code>gpt-4o</code>.</p>

<p>To generate the fine-tuning data, I had Stockfish play 100 games against itself on its highest difficulty level. For each game, I picked a random move and used it as an example. Here was one example:</p>

<ul>
  <li><strong>System prompt</strong>: (same as above)</li>
  <li><strong>User prompt</strong>:  <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3</code></li>
  <li><strong>Desired output</strong>: <code>Nxf4</code></li>
</ul>

<p>I then had Stockfish play another 100 games as validation data. The results were:</p>

<p><a href="https://dynomight.net/img/more-chess/finetune.pdf"><img src="https://dynomight.net/img/more-chess/finetune.svg" alt=""></a></p>

<p>Good! Fine-tuning helps.</p>

<p><em>Note</em>: The first time I fine-tuned <code>gpt-4o</code>, the results seemed bad, so I ran it again with a smaller step size. This makes me nervous.</p>

<h2 id="should-we-combine-examples-and-fine-tuning">Should we combine examples and fine-tuning?</h2>

<p>If examples are good, and fine-tuning is good, will putting them together be even better?</p>

<p>My intuition was no, since three in-context examples seems trivial when compared to 100 fine-tuning examples. Once fine-tuning was done, I figured the examples would be superfluous.</p>

<p>The answer <em>was</em> no, but for different reasons:</p>

<p><a href="https://dynomight.net/img/more-chess/examples-finetune.pdf"><img src="https://dynomight.net/img/more-chess/examples-finetune.svg" alt=""></a></p>

<p>According to that figure, fine-tuning helps. And examples help. But it’s <em>examples</em> that make <em>fine-tuning</em> redundant, not the other way around.</p>

<p>Ohkay.</p>

<h2 id="should-we-provide-legal-moves">Should we provide legal moves?</h2>

<p>LLMs sometimes struggle to give legal moves. In these experiments, I try 10 times and if there’s still no legal move, I just pick one at random. So I wondered: Maybe I could help the LLM out by listing the legal moves before giving the game history? Some might say this is “cheating”, but let’s try it anyway.</p>

<p>I used this system prompt:</p>

<p><code>You are a chess grandmaster.</code><br>
<code>You will be given a list of legal moves and a partially completed game.</code><br>
<code>After seeing it, you should choose the next move.</code><br>
<code>Use standard algebraic notation, e.g. "e4" or "Rdf8" or "R1a3".</code><br>
<code>NEVER give a turn number.</code><br>
<code>NEVER explain your choice.</code></p>

<p>And I sent user prompts like this:</p>

<div><p><code>Here are the current legal moves:</code></p><p>

<code>Bxh6 Bxf6 Bh4 Bf4 Be3 Bd2 Bc1 Nd5 Nb5 Na4 Nce2 Nb1 Nh3 Nf3 Nge2 Ba6 Bb5+ Bc4 Bd3 Be2 Ke2 Kd2 Qh5 Qg4 Qf3 Qd3 Qe2 Qd2 Qc1 Qb1 Rc1 Rb1 e5 d5 h3 g3 f3 b3 a3 h4 g4 f4 b4 a4 </code></p><p>

<code>Here is the game so far:</code></p><p>

<code>1. e4 d6 2. d4 g6 3. Nc3 Nf6 4. Bg5 h6 5.</code></p></div>

<p>Here are the results:</p>

<p><a href="https://dynomight.net/img/more-chess/list-legal-moves.svg"><img src="https://dynomight.net/img/more-chess/list-legal-moves.svg" alt=""></a></p>

<p>Disaster. Listing the legal moves makes the models play <em>much</em> worse. They don’t just win fewer games, they start making mistakes after a much smaller number of turns.</p>

<p>Ohhhkay. I guess let’s not do that.</p>

<h2 id="i-had-an-idea">I had an idea</h2>

<p>Thinking about the above, I had an idea. An idea that I think is… rather good.</p>

<p>Let’s back up a second. To make an LLM, you first make a “base” model. All that base models do is take a string and continue it. Given <code>The best tea is </code> , they have some probability of outputting <code>green tea</code> or <code>oolong</code> or whatever. (The right answer is <code>oolong</code>.)</p>

<p>If you want an LLM that can talk to you, you can <em>sort of</em> get a base model to do this by sending them strings that look like this:</p>

<div><p><code>(Transcript of chat between USER and ASSISTANT who is super chill and answers all questions without judgment.)</code></p><p>

<code>USER: How do I know if squirrels like me?</code></p><p>

<code>ASSISTANT:</code></p></div>

<p>LLMs trained on general text are smart enough to recognize that what comes next is probably something that a super chill agent would say to a neurotic user. So they’ll typically do something reasonable. But in practice, they aren’t great. The responses tend to reflect the chaos of the internet, which isn’t exactly what you want from an assistant.</p>

<p>Chat models go further in two ways. First, they create special tokens to indicate the different parts of the conversation, sort of like this (except you should think of <code>&lt;|SYSTEM|&gt;</code> et al. as being single special characters).</p>

<p><code>&lt;|SYSTEM|&gt;</code><br>
<code>You are a chatbot that is super chill and answers all questions without judgement.</code><br>
<code>&lt;|USER|&gt;</code><br>
<code>How do I know if squirrels like me?</code><br>
<code>&lt;|ASSISTANT|&gt;</code></p>

<p>Then, they do “instruction tuning”—they <em>re-train</em> the weights so that the model is good at responding to prompts given in this format.</p>

<p>So, when I asked <code>gpt-4o</code> to predict a chess move, the string that was actually presented to the system looked sort of like this:</p>

<p><code>&lt;|SYSTEM|&gt;</code><br>
<code>You are a chess grandmaster.</code><br>
<code>You will be given a list of legal moves and a partially completed game.</code><br>
<code>After seeing it, you should choose the next move.</code><br>
<code>Use standard algebraic notation, e.g. "e4" or "Rdf8" or "R1a3".</code><br>
<code>NEVER give a turn number.</code><br>
<code>NEVER explain your choice.</code><br>
<code>&lt;|USER|&gt;</code><br>
<code>1. e4 e5 2. Nf3 Nc6 3.</code><br>
<code>&lt;|ASSISTANT|&gt;</code></p>

<p>To make <code>gpt-4o</code>, OpenAI first made a base model. As far as I know, that model doesn’t have a name, so let’s call it <code>gpt-4-base</code>. It then did instruction tuning and stuck the instruction-tuned model behind the chat interface, to give us <code>gpt-4o</code>. (It also did some other stuff like <a href="https://en.wikipedia.org/wiki/Knowledge_distillation">distillation</a>, but never mind.)</p>

<p>I’ve gone through all this background because it allows us to state a central question: How good is <code>gpt-4-base</code> at chess? Is it as good at <code>gpt-3.5-turbo-instruct</code>? And if it <em>is</em>, then why is <code>gpt-4o</code> worse? Is it because of the instruction tuning? Or is it just because of the chat template, with the <code>&lt;|USER|&gt;</code> and <code>&lt;|ASSISTANT|&gt;</code> tokens floating around in ways that don’t happen in chess games written down in PGN notation?</p>

<p>I’m not sure, because OpenAI doesn’t deign to share <code>gpt-4-base</code>, nor to allow queries of <code>gpt-4o</code> in completion mode. But maybe we can help <code>gpt-4o</code> remember its evolutionary history. Maybe we can <em>prompt</em> <code>gpt-4o</code> in a way that will sort of trick it into responding <em>more</em> like it was in completion mode.</p>

<h2 id="should-we-regurgitate">Should we regurgitate?</h2>

<p>Thus my idea: Instead of just asking for a move, how about we ask the model to repeat the whole game and <em>then</em> give a move?</p>

<p>I changed the system prompt to this:</p>

<p><code>You are a chess grandmaster.</code><br>
<code>You will be given a partially completed game.</code><br>
<code>After seeing it, you should repeat the ENTIRE GAME and then give ONE new move.</code><br>
<code>Use standard algebraic notation, e.g. "e4" or "Rdf8" or "R1a3".</code><br>
<code>ALWAYS repeat the entire representation of the game so far.</code><br>
<code>NEVER explain your choice.</code></p>

<p>Given a prompt like <code>1. e4 e5 2.</code> I expected the model to return an output like <code>1. e4 e5 2. Nf7</code>. I checked to make sure it successfully repeated the entire game before giving a new legal move.</p>

<p>This works:</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate.svg" alt=""></a></p>

<p>By forcing the models to repeat the whole move sequence, you force the model to create a context for itself where it’s much more likely to choose good moves.</p>

<p>This makes <code>gpt-4o-mini</code> and <code>gpt-4o</code> better. It also seems like strong evidence that if we could query <code>gpt-4-base</code> in completion mode, it would be pretty good.</p>

<p><em>Note</em>: When using this type of prompt, I first gave the model ten tries to repeat the whole sequence and then give a legal move at the end. If none of those tries succeeded, I gave it another ten tries to at least produce a legal move after the new turn number, even if it didn’t repeat the whole game perfectly. If that <em>still</em> didn’t succeed, I chose a move at random.</p>

<h2 id="can-we-regurgitate-better">Can we regurgitate better?</h2>

<p>Fine-tuning is good. Regurgitation is good. Are they good together?</p>

<p>To test this, I needed to do a <em>new</em>, <em>independent</em> run of fine-tuning. I used the exact same sequence of games and moves, but with outputs repeating the inputs before giving a new move. For example:</p>

<ul>
  <li><strong>System prompt</strong>: (same as above)</li>
  <li><strong>User prompt</strong>:  <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3</code></li>
  <li><strong>Desired output</strong>: <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3 Nxf4</code></li>
</ul>

<p>This… maybe helped a little?</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-finetune.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-finetune.svg" alt=""></a></p>

<p>And how about examples? Will they improve regurgitation?</p>

<p>I used the same  three examples:</p>

<ul>
  <li><strong>Input A:</strong> <code>1.</code></li>
  <li><strong>Output A:</strong> <code>1. e4</code></li>
  <li><strong>Input AB:</strong> <code>1. d4</code></li>
  <li><strong>Output B:</strong> <code>1. d4 d5</code></li>
  <li><strong>Input C:</strong> <code>1. e4 e5 2. Nf3 Nc6 3.</code></li>
  <li><strong>Output C:</strong> <code>1. e4 e5 2. Nf3 Nc6 3. Nf3</code></li>
</ul>

<p>Like before, these had a remarkable impact given how little information they contain.</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-examples.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-examples.svg" alt=""></a></p>

<p>And should we combine examples <em>and</em> fine tuning?</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-examples-finetune.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-examples-finetune.svg" alt=""></a></p>

<p>Here we have the same (strange) result as without regurgitation. If you fine-tune, then adding examples helps. But it’s still worse than examples without fine-tuning.</p>

<h2 id="where-do-we-stand">Where do we stand?</h2>

<p>What have we learned so far?</p>

<ul>
  <li><strong>GOOD</strong>: Regurgitation, examples, fine tuning (without examples)</li>
  <li><strong>UNCLEAR</strong>: Metadata, repeating the system prompt, fine tuning (with examples)</li>
  <li><strong>BAD</strong>: Providing the list of legal moves</li>
</ul>

<p>So if we use regurgitation and examples and turn everything else off, how good is it? Will it play as well as our old nemesis?</p>

<p><a href="https://dynomight.net/img/more-chess/but.pdf"><img src="https://dynomight.net/img/more-chess/but.svg" alt=""></a></p>

<p>No. It’s respectable, but still not quite as good as <code>gpt-3.5-turbo-instruct</code>.</p>

<p>To compare these more directly, I had <code>gpt-4o + regurgitate + examples</code> play 50 games against <code>gpt-3.5-turbo-instruct</code>. In all cases, <code>gpt-4o</code> was white.</p>

<table>
  <thead>
    <tr>
      <th>outcome for  <code>gpt-4o + regurgitate + examples</code></th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>win</td>
      <td>9</td>
    </tr>
    <tr>
      <td>tie</td>
      <td>4</td>
    </tr>
    <tr>
      <td>loss</td>
      <td>37</td>
    </tr>
  </tbody>
</table>

<p>According to <a href="https://3dkingdoms.com/chess/elo.htm">this calculator</a>, that’s consistent with an Elo difference of -220. But you need to account for the fact that <code>gpt-4o</code> was always white, reportedly worth around <a href="https://en.wikipedia.org/wiki/First-move_advantage_in_chess#Winning_percentages">35 Elo</a>. Since <code>gpt-3.5-turbo-instruct</code> has been measured at around 1800 Elo, this suggests <code>gpt-4o</code> with regurgitation and examples hits around 1800 - 220 - 35/2 ≈ 1560 Elo, which is still “intermediate amateur” territory.</p>

<p>Here are 10 games of <code>gpt-4o + regurgitate + examples</code> playing against Stockfish: <a href="https://lichess.org/Q2lUPANn">1</a> <a href="https://lichess.org/Da7HUOz4">2</a> <a href="https://lichess.org/Ijitm5vI">3</a> <a href="https://lichess.org/ib0wwcAO">4</a> <a href="https://lichess.org/jOY82JGZ">5</a> <a href="https://lichess.org/hD9fQMMj">6</a> <a href="https://lichess.org/SmlDpA6O">7</a> <a href="https://lichess.org/kyRzFcvU">8</a> <a href="https://lichess.org/NAI3jWPo">9</a> <a href="https://lichess.org/x2LFUowA">10</a></p>

<p>And here are 10 games of <code>gpt-4o + regurgitate + examples</code> playing against <code>gpt-3.5-turbo-instruct</code>: <a href="https://lichess.org/RHiS3LUm">1</a> <a href="https://lichess.org/cq2ZKWwq">2</a>  <a href="https://lichess.org/2uLoukYE">3</a> <a href="https://lichess.org/JdPgUBCj">4</a> <a href="https://lichess.org/05DgjWJc">5</a> <a href="https://lichess.org/BoHLjFp8">6</a>  <a href="https://lichess.org/o4zh92NZ">7</a> <a href="https://lichess.org/wC4bp7gp">8</a> <a href="https://lichess.org/gHj3afte">9</a> <a href="https://lichess.org/ytgH7Qzr">10</a></p>

<h2 id="so-heres-my-current-theory">So here’s my current theory</h2>

<p>Here’s my best guess for what is happening:</p>

<p><strong>Part 1:</strong> OpenAI trains its base models on datasets with more/better chess games than those used by open models.</p>

<p><strong>Part 2</strong>: Recent <em>base</em> OpenAI models would be excellent at chess (in completion mode, if we could access them). But the <em>chat</em> models that we actually get access to aren’t.</p>

<p>I think part 1 is true because all the open models are <a href="https://dynomight.net/chess/">terrible</a> at chess, regardless of if they are base models or chat models. I suspect this is <em>not</em> some kind of architectural limitation—if you fine-tuned <code>llama-3.1-70b</code> on billions of expert chess games, I would be surprised if it could not beat <code>gpt-3.5-turbo-instruct</code> (rumored to have only around 20 billion parameters).</p>

<p>Meanwhile, in section A.2 of <a href="https://arxiv.org/pdf/2312.09390#page=29">this paper</a> (h/t Gwern) some OpenAI authors mention that GPT-4 was trained on chess games in PGN notation, filtered to only include players with Elo at least 1800. I haven’t seen any public confirmation that <code>gpt-3.5-turbo-instruct</code> used the same data, but it seems plausible. And can it really be a coincidence that <code>gpt-3.5-turbo-instruct</code> plays games <em>in PGN notation with a measured Elo of 1800</em>?</p>

<p>I can’t find any details about how much chess data was included when training Llama et al. I’m sure many games made their way in from the open internet. But specifically curating a giant database of high quality games probably just gives better results, and the open models probably just didn’t do that.</p>

<p>(Incidentally, I encourage people at all AI companies to leak secrets to me. If you use the <a href="https://dynomight.net/about/">anonymous feedback form</a>, please write with sufficient technicality that I can verify your expertise. Secrets will be used only for good, not evil.)</p>

<p>It’s also conceivable that some models are playing worse because they have <em>too much</em> chess data. It could be that the open internet has too many games from low-skill players and that if you don’t filter these out, then the models <em>correctly</em> predict that players would make low-quality moves. But I suspect not, because a smart model would recognize that <em>if the sequence of moves so far is high skill</em> then the player isn’t a total idiot and probably won’t throw away their queen. But the models don’t seem to do that.</p>

<p>I think part 2 of my theory is true mostly because of the experiments I did in this post: If you do weird contortions to “trick” OpenAI chat models into behaving more like completion models, then they play much better. So I suspect that the underlying <em>base</em> models (which we can’t touch) are good.</p>

<p>Now, there’s a major uncertainty in part 2. If <code>gpt-4o</code> in chat mode is worse than <code>gpt-4-base</code> in completion mode, then why? Is it the chat interface or the instruction tuning, or both? Put another way, would <code>gpt-4-base</code> be good at chess in a simulated chat mode? And would <code>gpt-4o</code> be good if we could query it <em>in completion mode</em>?</p>

<p>It’s impossible to say, because we can’t do those experiments.</p>

<h2 id="parting-thoughts">Parting thoughts</h2>

<ol>
  <li>
    <p>Isn’t it great how much of AI is now palace intrigue?</p>
  </li>
  <li>
    <p>It’s very likely that there are ways to coax better behavior out of <code>gpt-4o</code>. In truth, I barely scratched the surface here.</p>
  </li>
  <li>
    <p>It’s ridiculously hard to find the optimal combination of prompts and examples and fine-tuning, etc. It’s a very large space, there are no easy abstractions to allow you to search through the space, LLMs are unpredictable and fragile, and these experiments are slow and expensive.</p>
  </li>
  <li>
    <p>I tried running the final recipe with <code>gpt-4</code> (rather than <code>gpt-4o</code>), and it played poorly. I suspect the reason is that the combination of tricks I found is <code>gpt-4o</code> specific. Maybe <code>gpt-4</code> needs a different prompt? Or more examples? Or would respond better to fine-tuning? Who knows.</p>
  </li>
  <li>
    <p>In many ways, this feels less like engineering and more like a search for spells.</p>
  </li>
</ol>

<h2 id="ps">P.S.</h2>

<p>Thanks to the <a href="https://dynomight.net/automated/">Automator</a> for crucial guidance and boundless patience. Thanks to Daniel Gross for paying for all the electrons. Here is some good prior work on LLMs and chess:</p>

<ul>
  <li>
    <p>Adam Karvonen’s <a href="https://github.com/adamkarvonen/chess_gpt_eval">chess gpt eval repo</a>, which does careful tests on how good <code>gpt-3.5-turbo-instruct</code> is.</p>
  </li>
  <li>
    <p>Adam Karvonen’s <a href="https://github.com/adamkarvonen/chess_llm_interpretability">chess llm interpretability repo</a>  and paper, <a href="https://arxiv.org/pdf/2403.15498v2">“Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models”</a> which show, among other things, that <code>gpt-3.5-turbo-instruct</code> <em>does</em> seem to build up some kind of internal representation of board state.</p>
  </li>
  <li>
    <p>Matheiu Archer’s <a href="https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/">estimates of ELO</a> for <code>gpt-3.5-turbo-instruct</code> and <code>gpt-3.5-turbo</code> and <code>gpt-4</code>. This also experiments with different temperatures.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2406.11741">Transcendence: Generative Models Can Outperform The Experts That Train Them</a> (h/t WTFwhatthehell)</p>
  </li>
  <li>
    <p>Nicholas Carlini’s <a href="https://nicholas.carlini.com/writing/2023/chess-llm.html">Playing chess with large language models</a>.</p>
  </li>
</ul>

<p><em>Update</em> (2024/11/21 18:10 UTC) The last example output is an illegal move! (Why am I so bad?) This makes the benefit of these examples even more mysterious. I am re-running all the experiments with examples now.</p>

  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Handling cookies is a minefield (260 pts)]]></title>
            <link>https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/</link>
            <guid>42206556</guid>
            <pubDate>Thu, 21 Nov 2024 17:28:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/">https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/</a>, See on <a href="https://news.ycombinator.com/item?id=42206556">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <content>
  <article>

    

    <p><time datetime="2024-11-21T10:00:00-06:00" pubdate="">Thursday, November 21, 2024</time>, in <a href="https://grayduck.mn/categories/#standards">Standards</a>
    </p>

    

<p>HTTP cookies are a small piece of data set by either <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/cookie">Javascript</a> or HTTP servers, and which are essential for maintaining state on the otherwise stateless system known as the World Wide Web. Once set, web browsers will continue to forward them along for every properly scoped HTTP request until they expire.</p>
<p>I had been more than content to ignore the vagaries of how cookies function until the end of time, except that one day I stumbled across this innocuous piece of Javascript:</p>
<div><pre><span></span><code><span>const</span><span> </span><span>favoriteCookies</span><span> </span><span>=</span><span> </span><span>JSON</span><span>.</span><span>stringify</span><span>({</span>
<span>  </span><span>ginger</span><span>:</span><span> </span><span>"snap"</span><span>,</span>
<span>  </span><span>peanutButter</span><span>:</span><span> </span><span>"chocolate chip"</span><span>,</span>
<span>  </span><span>snicker</span><span>:</span><span> </span><span>"doodle"</span><span>,</span>
<span>});</span>

<span>document</span><span>.</span><span>cookie</span><span> </span><span>=</span><span> </span><span>`&nbsp;cookieNames=</span><span>${</span><span>favoriteCookies</span><span>}</span><span>`</span><span>&nbsp;</span><span>;</span>
</code></pre></div>

<p>This code functioned completely fine, as far as browsers were concerned. It took a piece of boring (but tasty) JSON and saved the value into a session cookie. While this was slightly unusual — most code will serialize JSON to base64 prior to setting them as a cookie, there was nothing here that browsers had any issue with. They happily allowed the cookie to be set and sent along to the backend web server in the HTTP header:</p>
<div><pre><span></span><code><span>GET</span> <span>/</span> <span>HTTP</span><span>/</span><span>1.1</span>
<span>Accept</span><span>:</span> <span>text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8</span>
<span>Accept-Encoding</span><span>:</span> <span>gzip, deflate</span>
<span>Accept-Language</span><span>:</span> <span>en-US,en;q=0.9</span>
<span>Connection</span><span>:</span> <span>keep-alive</span>
<span>Cookie</span><span>:</span> <span>cookieNames={"ginger":"snap","peanutButter":"chocolate chip","snicker":"doodle"}</span>
<span>Host</span><span>:</span> <span>example.com</span>
</code></pre></div>

<p>Which was all well and good, until it got passed along to some code that used the <a href="https://pkg.go.dev/github.com/go-http-utils/cookie">Go standard library</a>. The Go standard library couldn't parse the cookie, leading to cascading failures all the way up the stack. So what happened?</p>
<h2>The Specification</h2>
<p>Cookies were initially defined in <a href="https://www.rfc-editor.org/rfc/rfc2109">RFC 2109</a> (1997), and subsequently updated in <a href="https://www.rfc-editor.org/rfc/rfc2965">RFC 2965</a> (2000) and <a href="https://www.rfc-editor.org/rfc/rfc6265">RFC 6265</a> (2011), with a <a href="https://httpwg.org/http-extensions/draft-ietf-httpbis-rfc6265bis.html">draft version</a> that is in the process of being updated (and is what this article uses).</p>
<p>There are two sections of the RFC that pertain to cookie values:</p>
<p><strong>Section 4.1.1</strong> <em>(on how servers should send cookies)</em></p>
<div><pre><span></span><code>Informally, the Set-Cookie response header field contains a cookie,
which begins with a name-value-pair, followed by zero or more
attribute-value pairs. Servers SHOULD NOT send Set-Cookie header
fields that fail to conform to the following grammar:

set-cookie        = set-cookie-string
set-cookie-string = BWS cookie-pair *( BWS ";" OWS cookie-av )
cookie-pair       = cookie-name BWS "=" BWS cookie-value
cookie-name       = 1*cookie-octet
cookie-value      = *cookie-octet / ( DQUOTE *cookie-octet DQUOTE )
cookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E
                      ; US-ASCII characters excluding CTLs,
                      ; whitespace DQUOTE, comma, semicolon,
                      ; and backslash
</code></pre></div>

<p><strong>Section 5.6</strong> <em>(on how browsers should accept cookies)</em></p>
<div><pre><span></span><code>A user agent MUST use an algorithm equivalent to the following algorithm
to parse a set-cookie-string:

1. If the set-cookie-string contains a %x00-08 / %x0A-1F / %x7F character
   (CTL characters excluding HTAB):
     Abort these steps and ignore the set-cookie-string entirely.

2. If the set-cookie-string contains a %x3B (";") character:
   The name-value-pair string consists of the characters up to, but not
   including, the first %x3B (";"), and the unparsed-attributes consist
   of the remainder of the set-cookie-string (including the %x3B (";")
   in question).

Otherwise:

1. The name-value-pair string consists of all the characters contained in
   the set-cookie-string, and the unparsed-attributes is the empty string.
</code></pre></div>

<p>There are three things that should immediately jump out to you:</p>
<ol>
<li>What servers <strong>SHOULD</strong> send and what browsers <strong>MUST</strong> accept are not aligned, a classic example of the <a href="https://datatracker.ietf.org/doc/html/draft-thomson-postel-was-wrong-03">tragedy of following Postel's Law</a>.</li>
<li>There is nothing here that limits what cookie values are acceptable for browsers to send to servers, aside from the semicolon delimiter. This might be fine if servers only received cookies they themselves had set, but cookies can also come from <code>document.cookie</code> and contain values outside the <code>%x21</code>, <code>%x23-2B</code>, <code>%x2D-3A</code>, <code>%x3C-5B</code>, and <code>%x5D-7E</code> characters as allowed by <code>Set-Cookie</code>.</li>
<li>It doesn't acknowledge how standard libraries that handle <code>Cookie</code> headers should behave: should they act like user agents or like servers? Should they be permissive or proscriptive? Should they behave differently in different contexts?</li>
</ol>
<p>And herein lies the very crux of the resulting issue I ran into: everything behaves differently, and it's a miracle that cookies work at all.</p>
<h2>Web Browsers</h2>
<p>First, let's start with how web browsers behave. The teams behind Gecko (Firefox), Chromium, and WebKit (Safari) work together constantly, so it would be reasonable to expect them to all behave the same… right?</p>
<p>Before we dig in, remember that the RFC contradictorily says that <code>Set-Cookie</code> headers may contain any ASCII character besides control characters, double quotes, commas, semicolons, and backslashes, but that browsers should accept any cookie value that does not contain control characters.</p>
<p><br>
<strong>Firefox</strong></p>
<p>Firefox's <a href="https://searchfox.org/mozilla-central/source/netwerk/cookie/CookieCommons.cpp">code for valid cookie values</a> looks like this:</p>
<div><pre><span></span><code><span>bool</span><span> </span><span>CookieCommons</span><span>::</span><span>CheckValue</span><span>(</span><span>const</span><span> </span><span>CookieStruct</span><span>&amp;</span><span> </span><span>aCookieData</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>//</span><span> </span><span>reject</span><span> </span><span>cookie</span><span> </span><span>if</span><span> </span><span>value</span><span> </span><span>contains</span><span> </span><span>an</span><span> </span><span>RFC</span><span> </span><span>6265</span><span> </span><span>disallowed</span><span> </span><span>character</span><span> </span><span>-</span><span> </span><span>see</span>
<span>  </span><span>//</span><span> </span><span>https</span><span>:</span><span>//</span><span>bugzilla</span><span>.</span><span>mozilla</span><span>.</span><span>org</span><span>/</span><span>show_bug</span><span>.</span><span>cgi</span><span>?</span><span>id</span><span>=</span><span>1191423</span>
<span>  </span><span>//</span><span> </span><span>NOTE</span><span>:</span><span> </span><span>this</span><span> </span><span>is</span><span> </span><span>not</span><span> </span><span>the</span><span> </span><span>full</span><span> </span><span>set</span><span> </span><span>of</span><span> </span><span>characters</span><span> </span><span>disallowed</span><span> </span><span>by</span><span> </span><span>6265</span><span> </span><span>-</span><span> </span><span>notably</span>
<span>  </span><span>//</span><span> </span><span>0x09</span><span>,</span><span> </span><span>0x20</span><span>,</span><span> </span><span>0x22</span><span>,</span><span> </span><span>0x2C</span><span>,</span><span> </span><span>and</span><span> </span><span>0x5C</span><span> </span><span>are</span><span> </span><span>missing</span><span> </span><span>from</span><span> </span><span>this</span><span> </span><span>list</span><span>.</span>
<span>  </span><span>const</span><span> </span><span>char</span><span> </span><span>illegalCharacters</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span>
<span>      </span><span>0x01</span><span>,</span><span> </span><span>0x02</span><span>,</span><span> </span><span>0x03</span><span>,</span><span> </span><span>0x04</span><span>,</span><span> </span><span>0x05</span><span>,</span><span> </span><span>0x06</span><span>,</span><span> </span><span>0x07</span><span>,</span><span> </span><span>0x08</span><span>,</span><span> </span><span>0x0A</span><span>,</span><span> </span><span>0x0B</span><span>,</span><span> </span><span>0x0C</span><span>,</span>
<span>      </span><span>0x0D</span><span>,</span><span> </span><span>0x0E</span><span>,</span><span> </span><span>0x0F</span><span>,</span><span> </span><span>0x10</span><span>,</span><span> </span><span>0x11</span><span>,</span><span> </span><span>0x12</span><span>,</span><span> </span><span>0x13</span><span>,</span><span> </span><span>0x14</span><span>,</span><span> </span><span>0x15</span><span>,</span><span> </span><span>0x16</span><span>,</span><span> </span><span>0x17</span><span>,</span>
<span>      </span><span>0x18</span><span>,</span><span> </span><span>0x19</span><span>,</span><span> </span><span>0x1A</span><span>,</span><span> </span><span>0x1B</span><span>,</span><span> </span><span>0x1C</span><span>,</span><span> </span><span>0x1D</span><span>,</span><span> </span><span>0x1E</span><span>,</span><span> </span><span>0x1F</span><span>,</span><span> </span><span>0x3B</span><span>,</span><span> </span><span>0x7F</span><span>,</span><span> </span><span>0x00</span><span>};</span>

<span>  </span><span>const</span><span> </span><span>auto</span><span>*</span><span> </span><span>start</span><span> </span><span>=</span><span> </span><span>aCookieData</span><span>.</span><span>value</span><span>()</span><span>.</span><span>BeginReading</span><span>();</span>
<span>  </span><span>const</span><span> </span><span>auto</span><span>*</span><span> </span><span>end</span><span> </span><span>=</span><span> </span><span>aCookieData</span><span>.</span><span>value</span><span>()</span><span>.</span><span>EndReading</span><span>();</span>

<span>  </span><span>auto</span><span> </span><span>charFilter</span><span> </span><span>=</span><span> </span><span>[</span><span>&amp;</span><span>](</span><span>unsigned</span><span> </span><span>char</span><span> </span><span>c</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>if</span><span> </span><span>(</span><span>StaticPrefs</span><span>::</span><span>network_cookie_blockUnicode</span><span>()</span><span> </span><span>&amp;&amp;</span><span> </span><span>c</span><span> </span><span>&gt;=</span><span> </span><span>0x80</span><span>)</span><span> </span><span>{</span>
<span>      </span><span>return</span><span> </span><span>true</span><span>;</span>
<span>    </span><span>}</span>
<span>    </span><span>return</span><span> </span><span>std</span><span>::</span><span>find</span><span>(</span><span>std</span><span>::</span><span>begin</span><span>(</span><span>illegalCharacters</span><span>),</span><span> </span><span>std</span><span>::</span><span>end</span><span>(</span><span>illegalCharacters</span><span>),</span>
<span>                     </span><span>c</span><span>)</span><span> </span><span>!=</span><span> </span><span>std</span><span>::</span><span>end</span><span>(</span><span>illegalCharacters</span><span>);</span>
<span>  </span><span>};</span>

<span>  </span><span>return</span><span> </span><span>std</span><span>::</span><span>find_if</span><span>(</span><span>start</span><span>,</span><span> </span><span>end</span><span>,</span><span> </span><span>charFilter</span><span>)</span><span> </span><span>==</span><span> </span><span>end</span><span>;</span>
<span>}</span>
</code></pre></div>

<figure><figcaption>†accepting <code>0x7F</code> was fixed in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1797235">bug 1797235</a> (Firefox 108)</figcaption></figure>

<p>Firefox accepts five characters which RFC recommends that servers not send:</p>
<ul>
<li><code>0x09</code> (horizontal tab)</li>
<li><code>0x20</code> (spaces)</li>
<li><code>0x22</code> (double quotes)</li>
<li><code>0x2C</code> (commas)</li>
<li><code>0x5C</code> (backslashes)</li>
</ul>
<p>This was initially done to <a href="https://hg.mozilla.org/mozilla-central/annotate/02b7641683b049e8ba551d04318a5d985f36e504/netwerk/cookie/CookieCommons.cpp#l210">provide parity with Chrome</a> in some long-ago era and lingers on in both codebases.</p>
<p>Astute observers might note that Firefox has a <code>network.cookie.blockUnicode</code> setting that this code checks against, and which rejects all values above <code>0x80</code>. That groundwork was laid as a result of this research and can be tracked in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1797231">bug 1797231</a>.</p>
<p><br>
<strong>Chromium</strong></p>
<p>The Chromium <a href="https://source.chromium.org/chromium/chromium/src/+/main:net/cookies/parsed_cookie.cc">code for valid cookie values</a> looks like so:</p>
<div><pre><span></span><code><span>bool</span><span> </span><span>ParsedCookie::IsValidCookieValue</span><span>(</span><span>const</span><span> </span><span>std</span><span>::</span><span>string</span><span>&amp;</span><span> </span><span>value</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>// IsValidCookieValue() returns whether a string matches the following</span>
<span>  </span><span>// grammar:</span>
<span>  </span><span>//</span>
<span>  </span><span>// cookie-value       = *cookie-value-octet</span>
<span>  </span><span>// cookie-value-octet = %x20-3A / %x3C-7E / %x80-FF</span>
<span>  </span><span>//                       ; octets excluding CTLs and ";"</span>
<span>  </span><span>//</span>
<span>  </span><span>// This can be used to determine whether cookie values contain any invalid</span>
<span>  </span><span>// characters.</span>
<span>  </span><span>//</span>
<span>  </span><span>// Note that RFC6265bis section 4.1.1 suggests a stricter grammar for</span>
<span>  </span><span>// parsing cookie values, but we choose to allow a wider range of characters</span>
<span>  </span><span>// than what's allowed by that grammar (while still conforming to the</span>
<span>  </span><span>// requirements of the parsing algorithm defined in section 5.2).</span>
<span>  </span><span>//</span>
<span>  </span><span>// For reference, see:</span>
<span>  </span><span>//  - https://crbug.com/238041</span>
<span>  </span><span>for</span><span> </span><span>(</span><span>char</span><span> </span><span>i</span><span> </span><span>:</span><span> </span><span>value</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>if</span><span> </span><span>(</span><span>HttpUtil</span><span>::</span><span>IsControlChar</span><span>(</span><span>i</span><span>)</span><span> </span><span>||</span><span> </span><span>i</span><span> </span><span>==</span><span> </span><span>';'</span><span>)</span>
<span>      </span><span>return</span><span> </span><span>false</span><span>;</span>
<span>  </span><span>}</span>
<span>  </span><span>return</span><span> </span><span>true</span><span>;</span>
<span>}</span>

<span>// Whether the character is a control character (CTL) as defined in RFC 5234</span>
<span>// Appendix B.1.</span>
<span>static</span><span> </span><span>inline</span><span> </span><span>bool</span><span> </span><span>IsControlChar</span><span>(</span><span>char</span><span> </span><span>c</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>return</span><span> </span><span>(</span><span>c</span><span> </span><span>&gt;=</span><span> </span><span>0x00</span><span> </span><span>&amp;&amp;</span><span> </span><span>c</span><span> </span><span>&lt;=</span><span> </span><span>0x1F</span><span>)</span><span> </span><span>||</span><span> </span><span>c</span><span> </span><span>==</span><span> </span><span>0x7F</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Chrome is slightly more restrictive than Firefox, refusing to accept the <code>0x09</code> (horizontal tab) in its cookie values.</p>
<p>Nevertheless (and contrary to the RFC), it is perfectly happy to receive and send spaces, double quotes, commas, backslashes, and unicode characters.</p>
<p><br>
<strong>Safari (WebKit)</strong></p>
<p>I'm not able to get access to the code for cookie storage, as it is buried inside the closed source <code>CFNetwork</code>. That said, we can nevertheless examine its internals by running this piece of Javascript:</p>
<div><pre><span></span><code><span>for</span><span> </span><span>(</span><span>i</span><span>=</span><span>0</span><span>;</span><span> </span><span>i</span><span>&lt;</span><span>256</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span><span> </span><span>{</span>
<span>  </span><span>let</span><span> </span><span>paddedIndex</span><span> </span><span>=</span><span> </span><span>i</span><span>.</span><span>toString</span><span>().</span><span>padStart</span><span>(</span><span>3</span><span>,</span><span> </span><span>'0'</span><span>)</span><span> </span><span>+</span>
<span>    </span><span>'_'</span><span> </span><span>+</span><span> </span><span>'0x'</span><span> </span><span>+</span><span> </span><span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span><span> </span><span>'0'</span><span>);</span>

<span>  </span><span>// set a cookie with name of "cookie" + decimal char + hex char</span>
<span>  </span><span>// and a value of the character surrounded by a space and two dashes</span>
<span>  </span><span>document</span><span>.</span><span>cookie</span><span>=</span><span>`&nbsp;cookie</span><span>${</span><span>paddedIndex</span><span>}</span><span>=-- </span><span>${</span><span>String</span><span>.</span><span>fromCharCode</span><span>(</span><span>i</span><span>)</span><span>}</span><span> --`</span><span>&nbsp;</span><span>;</span>
<span>}</span>

<span>document</span><span>.</span><span>cookie</span><span>=</span><span>'cookieUnicode=🍪'</span><span>;</span>

<span>:::</span><span>text</span>
<span>cookie007_0x07</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>cookie008_0x08</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>cookie009_0x09</span><span>   </span><span>--</span><span>      </span><span>--</span><span>   </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie010_0x0a</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>cookie011_0x0b</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>                   </span><span>(</span><span>snip</span><span> </span><span>for</span><span> </span><span>brevity</span><span>)</span>
<span>cookie030_0x1e</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>cookie031_0x1f</span><span>   </span><span>--</span><span>           </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>16</span><span> </span><span>B</span>
<span>cookie032_0x20</span><span>   </span><span>--</span><span>   </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie033_0x21</span><span>   </span><span>--</span><span> </span><span>!</span><span> </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie034_0x22</span><span>   </span><span>--</span><span> </span><span>"</span><span> </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie035_0x23</span><span>   </span><span>--</span><span> </span><span>#</span><span> </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>                   </span><span>(</span><span>snip</span><span> </span><span>for</span><span> </span><span>brevity</span><span>)</span>
<span>cookie042_0x2a</span><span>   </span><span>--</span><span> </span><span>*</span><span> </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie043_0x2b</span><span>   </span><span>--</span><span> </span><span>+</span><span> </span><span>--</span><span>      </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>21</span><span> </span><span>B</span>
<span>cookie044_0x2c</span><span>   </span><span>--</span><span>,</span><span>--</span><span>        </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>19</span><span> </span><span>B</span>
<span>                   </span><span>(</span><span>snip</span><span> </span><span>for</span><span> </span><span>brevity</span><span>)</span>
<span>cookie044_0x5c</span><span>   </span><span>--</span><span> </span><span>\</span><span> </span><span>--</span><span>          </span><span>localhost</span><span>   </span><span>/</span><span>   </span><span>Session</span><span>   </span><span>19</span><span> </span><span>B</span>
</code></pre></div>

<p>As Safari stops processing a cookie once it sees a disallowed character, it's easy to see that <code>0x09</code> (horizontal tab), <code>0x20</code> (space), <code>0x22</code> (double quote), and <code>0x5C</code> (backslash) are okay, but <code>0x7F</code> (delete), <code>0x80-FF</code> (high ASCII / Unicode) characters are disallowed.</p>
<p>Unlike Firefox and Chrome which follow the instructions in the RFC to <em>“abort these steps and ignore the cookie entirely”</em> when encountering a cookie with a control character, Safari does not ignore the cookie but instead accepts the cookie value for everything up until that character.</p>
<p>Oddly enough, this quest has uncovered a bizarre Safari bug — setting a cookie with a value of <code>-- , --</code> seems to result in it trimming the whitespace around the comma.</p>
<h2>Standard Libraries</h2>
<p><strong>Golang</strong></p>
<p>Let's start with <a href="https://cs.opensource.google/go/go/+/master:src/net/http/cookie.go">Golang's cookie code</a>, which is where I ran into the problem in the first place.</p>
<div><pre><span></span><code><span>// sanitizeCookieValue produces a suitable cookie-value from v.</span>
<span>// https://tools.ietf.org/html/rfc6265#section-4.1.1</span>
<span>//</span>
<span>// We loosen this as spaces and commas are common in cookie values</span>
<span>// but we produce a quoted cookie-value if and only if v contains</span>
<span>// commas or spaces.</span>
<span>// See https://golang.org/issue/7243 for the discussion.</span>
<span>func</span><span> </span><span>sanitizeCookieValue</span><span>(</span><span>v</span><span> </span><span>string</span><span>)</span><span> </span><span>string</span><span> </span><span>{</span>
<span>    </span><span>v</span><span> </span><span>=</span><span> </span><span>sanitizeOrWarn</span><span>(</span><span>"Cookie.Value"</span><span>,</span><span> </span><span>validCookieValueByte</span><span>,</span><span> </span><span>v</span><span>)</span>
<span>    </span><span>if</span><span> </span><span>len</span><span>(</span><span>v</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span> </span><span>{</span>
<span>        </span><span>return</span><span> </span><span>v</span>
<span>    </span><span>}</span>
<span>    </span><span>if</span><span> </span><span>strings</span><span>.</span><span>ContainsAny</span><span>(</span><span>v</span><span>,</span><span> </span><span>" ,"</span><span>)</span><span> </span><span>{</span>
<span>        </span><span>return</span><span> </span><span>`"`</span><span> </span><span>+</span><span> </span><span>v</span><span> </span><span>+</span><span> </span><span>`"`</span>
<span>    </span><span>}</span>
<span>    </span><span>return</span><span> </span><span>v</span>
<span>}</span>

<span>func</span><span> </span><span>validCookieValueByte</span><span>(</span><span>b</span><span> </span><span>byte</span><span>)</span><span> </span><span>bool</span><span> </span><span>{</span>
<span>    </span><span>return</span><span> </span><span>0x20</span><span> </span><span>&lt;=</span><span> </span><span>b</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span> </span><span>&lt;</span><span> </span><span>0x7f</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span> </span><span>!=</span><span> </span><span>'"'</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span> </span><span>!=</span><span> </span><span>';'</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span> </span><span>!=</span><span> </span><span>'\\'</span>
<span>}</span>
</code></pre></div>

<p>Golang falls relatively close to what the RFC's wording on how servers should behave with <code>Set-Cookie</code>, only differing by allowing <code>0x20</code> (space) and <code>0x2C</code> (comma) due to them <a href="https://golang.org/issue/7243">commonly occurring</a> in the wild.</p>
<p>You can already see the struggles that programming languages have to deal with — they have to both receive values from browsers in line with Section 5, but also send cookies as per Section 4.1.1.</p>
<p>This can have pretty serious consequences, as you can see from running <a href="https://go.dev/play/p/6UX3Beem2Go">this code</a>:</p>
<div><pre><span></span><code><span>package</span><span> </span><span>main</span>

<span>import</span><span> </span><span>(</span>
<span>  </span><span>"fmt"</span>
<span>  </span><span>"net/http"</span>
<span>)</span>

<span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span>
<span>  </span><span>rawCookies</span><span> </span><span>:=</span>
<span>    </span><span>`&nbsp;cookie1=foo; `</span><span> </span><span>+</span>
<span>    </span><span>`&nbsp;cookie2={"ginger":"snap","peanutButter":"chocolate chip","snicker":"doodle"}; `</span><span> </span><span>+</span>
<span>    </span><span>`&nbsp;cookie3=bar`</span>

<span>    </span><span>header</span><span> </span><span>:=</span><span> </span><span>http</span><span>.</span><span>Header</span><span>{}</span>
<span>    </span><span>header</span><span>.</span><span>Add</span><span>(</span><span>"Cookie"</span><span>,</span><span> </span><span>rawCookies</span><span>)</span>
<span>    </span><span>request</span><span> </span><span>:=</span><span> </span><span>http</span><span>.</span><span>Request</span><span>{</span><span>Header</span><span>:</span><span> </span><span>header</span><span>}</span>

<span>    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>request</span><span>.</span><span>Cookies</span><span>())</span>
<span>}</span>
</code></pre></div>

<p>Which outputs only:</p>
<div><pre><span></span><code><span>[</span><span>cookie1</span><span>=</span><span>foo</span><span> </span><span>cookie3</span><span>=</span><span>bar</span><span>]</span>
</code></pre></div>

<p>Invisibly dropping a cookie that all major browsers accept, without any sort of exception to the effect that this is happening. Still, dropping a cookie it didn't understand without any other side effects isn't as bad as it could be.</p>
<p><br>
<strong>PHP</strong></p>
<p>Many languages, such as PHP, don't have native functions for parsing cookies, which makes it somewhat difficult to definitively say what it allows and does not allow.</p>
<p>That said, we can set cookies using the code below and see how PHP responds:</p>
<div><pre><span></span><code><span>[</span><span>0x09</span><span>,</span><span> </span><span>0x0D</span><span>,</span><span> </span><span>0x10</span><span>,</span><span> </span><span>0x20</span><span>,</span><span> </span><span>0x22</span><span>,</span><span> </span><span>0x2C</span><span>,</span><span> </span><span>0x5C</span><span>,</span><span> </span><span>0x7F</span><span>,</span><span> </span><span>0xFF</span><span>].</span><span>forEach</span><span>(</span><span>i</span><span> </span><span>=&gt;</span><span> </span><span>{</span>
<span>  </span><span>let</span><span> </span><span>paddedIndex</span><span> </span><span>=</span><span> </span><span>i</span><span>.</span><span>toString</span><span>().</span><span>padStart</span><span>(</span><span>3</span><span>,</span><span> </span><span>'0'</span><span>)</span><span> </span><span>+</span><span> </span><span>'_'</span><span> </span><span>+</span>
<span>    </span><span>'0x'</span><span> </span><span>+</span><span> </span><span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span><span> </span><span>'0'</span><span>);</span>

<span>  </span><span>document</span><span>.</span><span>cookie</span><span>=</span><span>`&nbsp;cookie</span><span>${</span><span>paddedIndex</span><span>}</span><span>=-- </span><span>${</span><span>String</span><span>.</span><span>fromCharCode</span><span>(</span><span>i</span><span>)</span><span>}</span><span> --`</span><span>&nbsp;</span><span>;</span>
<span>});</span>

<span>document</span><span>.</span><span>cookie</span><span>=</span><span>'cookieUnicode=🍪'</span><span>;</span>
</code></pre></div>

<p>Output:</p>
<div><pre><span></span><code>cookie009_0x09: -- --
cookie009_0x10: -- --
cookie009_0x0d: -- --
cookie032_0x20: -- --
cookie034_0x22: -- " --
cookie044_0x2c: -- , --
cookie092_0x5c: -- \ --
cookie255_0x7f: -- --
cookie255_0xff: -- ÿ --
cookieUnicode: 🍪
</code></pre></div>

<p>When it comes to control characters, PHP's behavior is all over the place. <code>0x00-0x09</code> all work fine, as do things like <code>0x0D</code> (carriage return), but if you use <code>0x10</code> (data link escape) or <code>0x7F</code> (delete), PHP will completely error out with a <strong>400 Bad Request</strong> error.</p>
<p><br>
<strong>Python</strong></p>
<div><pre><span></span><code><span>import</span> <span>http.cookies</span>

<span>raw_cookies</span> <span>=</span> <span>(</span>
    <span>'cookie1=foo; '</span>
    <span>'cookie2={"ginger":"snap","peanutButter":"chocolate chip","snicker":"doodle"}; '</span>
    <span>'cookie3=bar'</span>
<span>)</span>

<span>c</span> <span>=</span> <span>http</span><span>.</span><span>cookies</span><span>.</span><span>SimpleCookie</span><span>()</span>
<span>c</span><span>.</span><span>load</span><span>(</span><span>raw_cookies</span><span>)</span>

<span>print</span><span>(</span><span>c</span><span>)</span>
</code></pre></div>

<p>Output:</p>
<div><pre><span></span><code>&gt;&gt;&gt; Set-Cookie: cookie1=foo
</code></pre></div>

<p>Python invisibly aborts the loading of additional cookies inside <code>SimpleCookie.load()</code> as soon as it encounters one it doesn't understand. This can be very dangerous when you consider that a subdomain could feasibly set a cookie on the base domain which would completely break all cookies for all domains of a given site.</p>
<p>It's even messier it comes to control characters:</p>
<div><pre><span></span><code><span>import</span> <span>http.cookies</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>32</span><span>):</span>
    <span>raw_cookie</span> <span>=</span> <span>f</span><span>"cookie</span><span>{</span><span>hex</span><span>(</span><span>i</span><span>)</span><span>}</span><span>=</span><span>{</span><span>chr</span><span>(</span><span>i</span><span>)</span><span>}</span><span>"</span>

    <span>c</span> <span>=</span> <span>http</span><span>.</span><span>cookies</span><span>.</span><span>SimpleCookie</span><span>()</span>
    <span>c</span><span>.</span><span>load</span><span>(</span><span>raw_cookie</span><span>)</span>

    <span>for</span> <span>name</span><span>,</span> <span>morsel</span> <span>in</span> <span>c</span><span>.</span><span>items</span><span>():</span>
        <span>print</span><span>(</span><span>f</span><span>"</span><span>{</span><span>name</span><span>}</span><span>: value: </span><span>{</span><span>repr</span><span>(</span><span>morsel</span><span>.</span><span>value</span><span>)</span><span>}</span><span>, length: </span><span>{</span><span>len</span><span>(</span><span>morsel</span><span>.</span><span>value</span><span>)</span><span>}</span><span>"</span><span>)</span>
</code></pre></div>

<p>Output:</p>
<div><pre><span></span><code>&gt;&gt;&gt; cookie0x9: value: '', length: 0
&gt;&gt;&gt; cookie0xa: value: '', length: 0
&gt;&gt;&gt; cookie0xb: value: '', length: 0
&gt;&gt;&gt; cookie0xc: value: '', length: 0
&gt;&gt;&gt; cookie0xd: value: '', length: 0
</code></pre></div>

<p>Here we can see that Python invisibly drops a lot of cookies with control characters, and loads others improperly. Note that if you guard those values with something like:</p>
<div><pre><span></span><code><span>raw_cookie</span> <span>=</span> <span>f</span><span>"cookie</span><span>{</span><span>hex</span><span>(</span><span>i</span><span>)</span><span>}</span><span>=aa</span><span>{</span><span>chr</span><span>(</span><span>i</span><span>)</span><span>}</span><span>aa"</span>
</code></pre></div>

<p>Then none of the control character cookies will load. Overall, Python is extremely inconsistent and unpredictable in its behavior when it comes to loading cookies.</p>
<p><br>
<strong>Ruby</strong></p>
<div><pre><span></span><code><span>require</span><span> </span><span>"cgi"</span>

<span>raw_cookie</span><span> </span><span>=</span><span> </span><span>'ginger=snap; '</span><span> </span><span>+</span>
<span>"cookie=chocolate </span><span>\x13</span><span> </span><span>\t</span><span> </span><span>\"</span><span> , </span><span>\\</span><span> </span><span>\x7f</span><span> 🍪 chip; "</span><span> </span><span>+</span>
<span>'snicker=doodle'</span>

<span>cookies</span><span> </span><span>=</span><span> </span><span>CGI</span><span>::</span><span>Cookie</span><span>.</span><span>parse</span><span>(</span><span>raw_cookie</span><span>)</span>

<span>puts</span><span> </span><span>cookies</span>
<span>puts</span><span> </span><span>cookies</span><span>[</span><span>"cookie"</span><span>].</span><span>value</span><span>()</span>
<span>puts</span><span> </span><span>cookies</span><span>[</span><span>"cookie"</span><span>].</span><span>value</span><span>()</span><span>.</span><span>to_s</span><span>()</span>
</code></pre></div>

<p>Output:</p>
<div><pre><span></span><code>{"ginger"=&gt;#&lt;CGI::Cookie: "ginger=snap; path="&gt;, "cookie"=&gt;#&lt;CGI::Cookie: "cookie=chocolate+%13+%09+%22+%2C+%5C+%7F+%F0%9F%8D%AA+chip; path="&gt;, "snicker"=&gt;#&lt;CGI::Cookie: "snicker=doodle; path="&gt;}
chocolate    " , \  🍪 chip
cookie=chocolate+%13+%09+%22+%2C+%5C+%7F+%F0%9F%8D%AA+chip; path=
</code></pre></div>

<p>The Ruby library appears pretty permissive, seeming to accept every character during parsing and then <a href="https://developer.mozilla.org/en-US/docs/Glossary/percent-encoding">percent-encoding</a> it when being pulled from the cookie jar.</p>
<p>This may well be the optimal behavior (if such a thing can be said to exist with cookies), but I can certainly see cases where code setting a cookie via <code>document.cookie</code> would not expect to see it reflected back in percent-encoded form.</p>
<p><br>
<strong>Rust</strong></p>
<div><pre><span></span><code><span>use</span><span> </span><span>cookie</span>::<span>Cookie</span><span>;</span>

<span>fn</span> <span>main</span><span>()</span><span> </span><span>{</span>
<span>    </span><span>let</span><span> </span><span>c</span><span> </span><span>=</span><span> </span><span>Cookie</span>::<span>parse</span><span>(</span><span>"cookie=chocolate , </span><span>\"</span><span> </span><span>\t</span><span> foo </span><span>\x13</span><span> ñ 🍪 chip;"</span><span>).</span><span>unwrap</span><span>();</span>
<span>    </span><span>println!</span><span>(</span><span>"{:?}"</span><span>,</span><span> </span><span>c</span><span>.</span><span>name_value</span><span>());</span>
<span>}</span>
</code></pre></div>

<p>Output:</p>
<div><pre><span></span><code>("cookie", "chocolate , \" \t foo \u{13} ñ 🍪 chip")
</code></pre></div>

<p>Rust doesn't ship any cookie handling facilities by default, so this is looking at the popular <code>cookie</code> crate. As configured by default, it appears to be the most permissive of the programming languages, accepting any UTF-8 string tossed at it.</p>

<h2>The World Wide Web, aka Why This Matters</h2>
<p>The wildly differing behavior between browsers and languages certainly makes for some <a href="#summary-table">riveting tables</a>, but how does all this play out in the real world?</p>
<p>When I first discovered this in the real world, it was only through sheer luck that it wasn't a catastrophe. A manual tester was playing around with a third-party library update and had run into a strange set of errors on our testing site. Without bringing it to my attention, this update — doing something unlikely to be caught in automated testing — would have certainly been pushed to production. As a result, every future website visitor would have received a broken cookie and been locked out with an inscrutable error until the update was reverted and the cookies were cleared out.</p>
<p>And that's exactly the problem with this specification ambiguity — it's such an easy mistake to make that millions of websites and companies are only an intern away from a complete meltdown. And it doesn't only affect tiny websites on obscure frameworks, as major websites such as Facebook, Netflix, WhatsApp, and Apple are affected.</p>
<p>You can see for yourself how easy of a mistake this is to make by pasting this simple code fragment into your browser console, substituting for the domain you're testing:</p>
<div><pre><span></span><code><span>document</span><span>.</span><span>cookie</span><span>=</span><span>"unicodeCookie=🍪; domain=.grayduck.mn; Path=/; SameSite=Lax"</span>
</code></pre></div>

<p>websites hate this one weird trick</p>

<p><strong>Facebook</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/facebook.jpg"><figcaption>facebook error page has images the cookie also breaks</figcaption></figure>

<p><strong>Instagram &amp; Threads</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/instagram.jpg"><figcaption>instagram produces this spartan 500 error</figcaption></figure>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/threads.jpg"><figcaption>no surprise that threads is much the same</figcaption></figure>

<p><strong>Netflix</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/netflix.jpg"><figcaption>netflix returns a NSES-500 error</figcaption></figure>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/netflix-help.jpg"><figcaption>and unfortunately the help page is broken too</figcaption></figure>

<p><strong>Okta</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/okta.jpg"><figcaption>every okta login page returns a 400 error</figcaption></figure>

<p><strong>WhatsApp</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/whatsapp.png"><figcaption>whatsapp's descriptively named “whatsapp error”</figcaption></figure>

<p><strong>Amazon</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/amazon-ratings.jpg"><figcaption>most parts of amazon work, but pieces of it are randomly broken</figcaption></figure>

<p><strong>Amazon Web Services</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/aws-login.jpg"><figcaption>the AWS login console returns a 400 error and crashes out</figcaption></figure>

<p><strong>Apple Support</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/apple-support.jpg"><figcaption>apple support is unable to load your devices</figcaption></figure>

<p><strong>Best Buy</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/bestbuy.jpg"><figcaption>navigation works, but the search functionality does not</figcaption></figure>

<p><strong>eBay</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/ebay.jpg"><figcaption>mostly fixed, but pieces of eBay still error out with a 400 error</figcaption></figure>

<p><strong>Home Depot</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/homedepot.jpg"><figcaption>home depot will be fixing it</figcaption></figure>

<p><strong>Intuit</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/intuit.jpg"><figcaption>intuit was the only site to identify the cause of the error</figcaption></figure>

<p><strong>Outlook</strong></p>
<figure><img src="https://grayduck.mn/2024/11/21/handling-cookies-is-a-minefield/outlook.jpg"><figcaption>another 400 error shows how systematic the problem is</figcaption></figure>

<h3>How do we fix this?</h3>
<p>It's probably not much of a surprise that fixing problems in 30-year-old foundational specifications is really, <em>really</em> hard. And for this problem, it's unlikely that there is a good fix.</p>
<p>Blocking these cookies on the browser side was considered and worked on by both Mozilla and Google:</p>
<ul>
<li>Mozilla: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1797235">bug 1797235</a> aka <a href="https://nvd.nist.gov/vuln/detail/cve-2023-5723">CVE-2023-5723</a>, and <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1797231">bug 1797231</a></li>
<li>Google: <a href="https://issues.chromium.org/issues/40061459#comment24">bug 40061459</a></li>
</ul>
<p>But it turns out unilateral blocking is quite complicated because while non-ASCII cookies aren't super common overall, affecting only a bit under 0.01% of all cookies, telemetry has found they are <a href="https://issues.chromium.org/issues/40061459#comment24">considerably more common</a> in countries like Argentina, Mexico, and Finland. While Mozilla did implement a preference that could be toggled quickly (<code>network.cookie.blockUnicode</code>), it hasn't been enabled due to behavioral compatibility issues with Chromium.</p>
<p>Fixing it on the server-side is potentially feasible, but it affects millions of websites and most of the errors caused by this problem are buried deep in programming languages and web frameworks. It might be possible for places like Facebook and Netflix to mitigate the issue, but the average website operator is not going to have the time or ability to resolve the issue.</p>
<p>In truth, the true fix for this issue almost certainly lies in the <a href="https://httpwg.org/">IETF HTTP Working Group</a> updating the cookie specification to both align with itself and to be strict on how systems handling cookies should behave. Whether non-ASCII characters should be allowed should be identical regardless of whether server-side or on user agents.</p>
<p>And regardless, the steps around how browsers, programming languages, and frameworks should handle cookie processing need to be explicit, in much the way that modern W3C standards such as <a href="https://www.w3.org/TR/CSP3/#html-integration">Content Security Policy</a> do. Aborting the processing of other cookies because one cookie is malformed is unacceptable when such behavior can lead to a wide variety of unexpected behavior.</p>
<p>These processing steps should probably look like something like:</p>
<div><pre><span></span><code>• Start with field-value
• Split on ; and ,, giving list of "raw-cookie-pair". Comma is NOT treated a synonym for semicolon in order to support combining headers, despite RFC7230 section 3.2.2.

For each raw-cookie-pair:
  ◦ If the pair does not contain =, then skip to next raw-cookie-pair
  ◦ Remove leading and trailing whitespace
  ◦ Treat portion before first = as cookie-name-octets
  ◦ Treat portion after first = as cookie-value-octets
  ◦ If cookie-value-octets starts with DQUOTE, then:
    ‣ Remove one DQUOTE at start
    ‣ Remove one DQUOTE at end, if present
  ◦ If resulting cookie-name-octets, cookie-value-octets, or both are unacceptable to server, then skip to next raw-cookie-pair
  ◦ Process resulting [cookie-name-octets, cookie-value-octets] tuple in server defined manner

Servers SHOULD further process cookie-name-octets and reject any tuple where the cookie-name-octets are not a token.  Servers SHOULD further process and cookie-value-octets and reject any tuple where cookie-value-octets contains octets not in cookie-octet.
</code></pre></div>


<h3>Summary Table</h3>
<div>
    <table id="cookie-table-summary">
        <thead>
            <tr>
                <th></th>
                <th>CTLs<sup>1</sup></th>
                <th>htab</th>
                <th>space</th>
                <th>dquote</th>
                <th>comma</th>
                <th>backslash</th>
                <th>delete</th>
                <th>0x80-FF<br>(and Unicode)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><a href="https://httpwg.org/http-extensions/draft-ietf-httpbis-rfc6265bis.html#name-set-cookie">RFC 6235 (Section 4.1.1)</a></td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
            </tr>
                <tr><td><a href="https://httpwg.org/http-extensions/draft-ietf-httpbis-rfc6265bis.html#name-the-set-cookie-header-field">RFC 6265 (Section 5.6)</a></td>
                <td>No</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>No</td>
                <td>??</td>
            </tr>
            <tr><td colspan="9"></td></tr>
            
                <tr><td>Firefox</td>
                <td>No<sup>2</sup></td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
            </tr>
            
                <tr><td>Chromium</td>
                <td>No</td>
                <td>No</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>No</td>
                <td>Yes</td>
            </tr>
            
                <tr><td>Safari</td>
                <td>No<sup>3</sup></td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes<sup>4</sup></td>
                <td>Yes</td>
                <td>No</td>
                <td>No</td>
            </tr>
            <tr><td colspan="9"></td></tr>
            
                <tr><td>Golang</td>
                <td>??</td>
                <td>No</td>
                <td>Yes</td>
                <td>No</td>
                <td>Yes</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
            </tr>
            
                <tr><td>Python</td>
                <td>Yes<sup>5</sup></td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>Yes</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
            </tr>
            <tr>
                <td>Ruby (CGI)</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
            </tr>
            <tr>
                <td>Rust</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>Yes</td>
            </tr>
        </tbody>
    </table>
</div>

<ol>
<li>not as defined as in <a href="https://www.rfc-editor.org/rfc/rfc5234">RFC 5234</a>, but instead as <code>\x00-08</code> and <code>\x0A-x1F</code> (e.g. CTLs minus htab and delete)</li>
<li>Mozilla <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1797235">stopped allowing CTLs</a> in <code>document.cookie</code> as of Firefox 108</li>
<li>does not abort processing and ignore the cookie as the RFC requires</li>
<li>seems to remove whitespace around commas in some conditions</li>
<li>sometimes allows <code>0x0A-0D</code> but fails to store them in the cookie value, aborts at other times</li>
</ol>
<h3>Thanks</h3>
<p>I couldn't have written this article without a bunch of help along the way. I'd like to thank:</p>
<ul>
<li>Po-Ning Tseng, for helping me investigate this issue in the first place</li>
<li>Dan Veditz at Mozilla, for his inexhaustible knowledge and endless kindness</li>
<li><a href="https://frederikbraun.de/">Frederik Braun</a>, for his helpful early feedback</li>
<li>Steven Bingler at Google, for pushing on getting this issue fixed</li>
<li>Peter Bowen, for his thoughts on how cookie processing probably should happen</li>
<li>Chris Palmer and David Schinazi, for their insightful proofreading</li>
<li>Stefan Bühler, who stumbled across some of this stuff <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=858215">over a decade ago</a></li>
<li>kibwen on HackerNews, for pointing out the Rust crate situation</li>
</ul>

    <p>
    [Category: <a href="https://grayduck.mn/categories/#Standards">Standards</a>]
    [Tags: <a href="https://grayduck.mn/tags/#browser">Browser</a>, <a href="https://grayduck.mn/tags/#programming">Programming</a>, <a href="https://grayduck.mn/tags/#security">Security</a>]
    </p>

  </article>

      </content>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The rectangular cows of Art UK (2018) (138 pts)]]></title>
            <link>https://artuk.org/discover/stories/the-rectangular-cows-of-art-uk</link>
            <guid>42203990</guid>
            <pubDate>Thu, 21 Nov 2024 13:12:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artuk.org/discover/stories/the-rectangular-cows-of-art-uk">https://artuk.org/discover/stories/the-rectangular-cows-of-art-uk</a>, See on <a href="https://news.ycombinator.com/item?id=42203990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Spending time among the thousands of paintings on Art UK often throws up questions. What is <a href="https://artuk.org/discover/artworks/species-of-sun-fish-caught-off-jersey-4-september-1885-137435/search/licence:2bbc/page/34">a sun fish</a>? Why is there a painting <a href="https://artuk.org/discover/artworks/robert-burns-and-his-dog-have-royal-visitors-214756/view_as/grid/search/keyword:queen-and-robert-burns/page/1">of the Queen with Robert Burns</a>? What makes <a href="https://artuk.org/discover/artworks/hot-tie-65548/view_as/grid/search/works:hot-tie/page/1">this tie hot</a>?</p>
<p>For a while, I’ve been wondering about a mystery I shall call ‘the rectangular cows of Art UK’. Some time ago, I came across dozens of very similar paintings, showing a cow standing by itself in a field, side-on (or ‘landscape’, if you will). And the cow, in many cases, would be almost perfectly rectangular.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/a-shorthorn-cow-151261">
			
									<img data-credits="Image credit:  Towneley Hall Art Gallery &amp; Museum" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/LAN/TOWN/LAN_TOWN_BURGM_paoil234_2-001.jpg" alt="A Shorthorn Cow">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/hereford-cow-131100">
			
									<img data-credits="Image credit:  National Trust Images" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/NTII/ATT/NTII_ATT_609112-001.jpg" alt="Hereford Cow">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/sussex-cow-joan-27054">
			
									<img data-credits="Image credit:  Museum of English Rural Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/BBO/MERL/BBO_MERL_M_20-001.jpg" alt="Sussex Cow 'Joan'">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/a-shorthorn-cow-27035">
			
									<img data-credits="Image credit:  Museum of English Rural Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/BBO/MERL/BBO_MERL_M_24-001.jpg" alt="A Shorthorn Cow">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p>However, looking at the mysterious rectangular cows also led to other questions about Art UK’s bovine representation.</p>
<p><strong>Firstly, why are there so many cows?</strong></p>
<p>The paintings above are a few picked out of hundreds. In fact, there are so many paintings of cows, I can’t possibly do them all justice here. Searches for ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:cow">cow</a>’, ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:heifer">heifer</a>’, ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:ox/page/3">ox</a>’, ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:cattle">cattle</a>’ and ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:bull/page/3">bull</a>’ all return rewarding results (a heifer, I have learned, being a cow that has not yet had a calf).</p>
<p>‘Cattle’ returns a very different kind of painting to ‘cow’. With ‘cattle’ you see lots of typical landscape scenes; lovely green meadows, maybe some water, and groups of nicely-painted cows grazing away. ‘<a href="https://artuk.org/discover/artworks/view_as/grid/search/keyword:landscape-with-cattle">Landscape with cattle</a>’, or minor variations thereof, is a very common painting name.</p>
<p>Highland cows also seem to be particularly attractive for artists, as there are many paintings of them, including this from the <a href="https://artuk.org/visit/venues/the-stirling-smith-art-gallery-museum-5144" target="_blank">Stirling Smith Art Gallery &amp; Museum</a>, where the cows look like they’re posing for an album cover.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/cattle-126982">
			
									<img data-credits="Image credit:  The Stirling Smith Art Gallery &amp; Museum" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/CSF/STG/CSF_STG_6094_000-001.jpg" alt="Cattle">
	            
							</a></p><div>
									<p><a href="https://artuk.org/discover/artworks/cattle-126982">
				
											Cattle 
																			<small>1878</small>
											
									</a></p>
				
																	<p> Joseph Denovan Adam (1841–1896) </p>
										
									<p><span>The Stirling Smith Art Gallery &amp; Museum</span>
							</p></div>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p>Maybe the answer to why there are so many of these cows is a very simple one: ubiquity. Cows are an ever-present feature of a lot of the British countryside, and perfect subjects for artists to paint, given they get to stand around at leisure.</p>
<p><strong>Are there any famous cows?</strong></p>
<p>The searches got me wondering: as such a popular subject, are there any famous paintings of cows? Is there a cow equivalent of <em>Whistlejacket, </em>the much-lauded painting of a horse by George Stubbs, admired by visitors to the National Gallery?</p>
<p>The answer is: not really, although there are famous artists who have brought cows to the canvas. Andy Warhol did <a href="https://www.moma.org/collection/works/71923">a series of them</a>, and the German artist Franz Marc also <a href="https://www.moma.org/s/ge/collection_ge/artist/artist_id-3748_role-1_sov_page-18.html">created some brightly coloured bovines</a>. There are some lovely paintings by <a href="https://artuk.org/discover/artists/cooper-thomas-sidney-18031902">Thomas Sidney Cooper</a> – a landscape painter whose paintings of livestock earned him the nickname ‘cow Cooper’ – on Art UK.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/separated-but-not-divorced-the-bull-75722">
			
									<img data-credits="Image credit:  Canterbury Museums and Galleries" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/KT/CCMG/KT_CCMG_ROY_L23_001-001.jpg" alt="Separated, But Not Divorced (The Bull)">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p>The French artist <a href="https://artuk.org/discover/artists/bonheur-rosa-18221899" target="_blank">Rosa Bonheur</a> (who herself was best known for <a href="https://artuk.org/discover/artworks/the-horse-fair-115640/view_as/grid/search/makers:rosa-bonheur-18221899/page/1">painting horses</a>) depicts cows among her pastoral scenes. They are markedly different from most of the other cow paintings on the site, where the animals are standing around at leisure: the oxen observed by Bonheur are hard at work ploughing.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/ploughing-in-the-nivernais-france-65798">
			
									<img data-credits="Image credit:  Atkinson Art Gallery Collection" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/MER/SEC/MER_SEC_370-001.jpg" alt="Ploughing in the Nivernais, France">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p><strong>What’s with the guys in the top hats?</strong></p>
<p>Another sub-genre of cow paintings seems to be those that also feature a man standing by the cow, dressed much more smartly than you’d expect of a farmer.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/the-champion-shorthorn-27033">
			
									<img data-credits="Image credit:  Museum of English Rural Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/BBO/MERL/BBO_MERL_M_29-001.jpg" alt="The Champion Shorthorn">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/a-polled-heifer-27049">
			
									<img data-credits="Image credit:  Museum of English Rural Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/BBO/MERL/BBO_MERL_M_32-001.jpg" alt="A Polled Heifer">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/the-dunearn-ox-183608">
			
									<img data-credits="Image credit:  National Museums Scotland, National Museum of Rural Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/STC/NMS/STC_NMS_H_OD_127-001.jpg" alt="The Dunearn Ox">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p>The ox in this particular painting is a real monster. It towers above the tiny man in his nice coat and top hat, who holds a kind of harness that looks like it’d barely make it over the beast’s neck. Either the scale is off or the size of the ox has been exaggerated: and if this is the case, then why?</p>
<p>The clue to these is in the dates of the paintings. The early 1800s was when selective breeding for certain traits in animals was becoming a more common practice. The celebrity ‘Durham Ox’ – a bull so large it drew in crowds to marvel at its size – showed what could be done (and aspired to), and owners of prize livestock became increasingly proud of the animals they had bred. They commissioned paintings of themselves with their cattle, and to make the animals look as impressive and well-bred as possible, the artists have made some look excessively large, with spindly little legs holding up colossal bodies.</p>
<p>This trend wasn’t unique to cows: there are plenty of corpulent pigs and sheep to feast your eyes on too.</p>
<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/prize-sheep-80166">
			
									<img data-credits="Image credit:  Museum of Lincolnshire Life" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/LLR/MULL/LLR_MULL_1971_188-001.jpg" alt="Prize Sheep">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<figure>
	<div>
						
							<p><a href="https://artuk.org/discover/artworks/a-pair-of-pigs-54730">
			
									<img data-credits="Image credit:  Compton Verney" src="https://d3d00swyhr67nd.cloudfront.net/w800h800/collection/WAR/COMP/WAR_COMP_106-001.jpg" alt="A Pair of Pigs">
	            
							</a></p>
		</div>
</figure>
<!-- END: skins/artuk/cmscode/v_object -->

<p><strong>And finally… why <em>are</em> the cows rectangular?</strong></p>
<p>The crux of the mystery.</p>
<p>The answer, as best I can find it, seems to be related to the points about status. Again, many of these paintings were made in the early 1800s, and the rectangular shape may have been another way of making the animals seem big and impressive.</p>
<p>Two things conspire to make the cows so angular. The first is the squared-off rump, and I haven't come up with an answer for why that is. The second is the overly bulgy bit in front of their front legs, the brisket. And, also according to my research, when cattle are judged for competitions or prizes, the brisket is taken into consideration. I quote: 'the brisket (the flap on a cow's neck) is only fat and skin. We generally want to see fullness to the brisket, this means that the animal has enough finish and muscle development. If there is too much finish, it will look 'messy', aka, skin will fold over and look a lot like double chins on people. The skin should be tight and the brisket full.'</p>
<p>So there we go. These paintings are probably just relics of a time when cattle were a marker of status, and breeding for size became a competition. Really famous cows from the 1800s could find themselves the subjects of everything from public displays to dinner plates, so perhaps the efforts of their owners conspired with the artists in an effort to secure them both a place in history.</p>
<p><em>Molly Tresadern, Art UK Content Creator and Marketer</em></p>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Personality Basins (144 pts)]]></title>
            <link>https://near.blog/personality-basins/</link>
            <guid>42203635</guid>
            <pubDate>Thu, 21 Nov 2024 12:25:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://near.blog/personality-basins/">https://near.blog/personality-basins/</a>, See on <a href="https://news.ycombinator.com/item?id=42203635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>Personality basins are the mental model that I use to reason about humans within their environment. They are an elucidating way to think about many concepts: from modelling why people are they way they are, how they change over time, how mental illnesses and addiction function along with how we should look for their cures, and how the attention economy optimizes itself to consume all of your free time.</p>







<h2>What is personality?</h2>



<p>Your personality is formed by a process conceptually similar to <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank">RLHF</a>. You are first born with a set of traits in a given environment. After this, you perform many interactions with your environment. If an interaction goes well, you’re likely to do it more often, and if it goes poorly, you’ll probably do less of it.</p>


<div>
<figure><img decoding="async" width="1024" height="655" src="https://near.blog/wp-content/uploads/2024/06/rl-1024x655.png" alt="" srcset="https://near.blog/wp-content/uploads/2024/06/rl-1024x655.png 1024w, https://near.blog/wp-content/uploads/2024/06/rl-300x192.png 300w, https://near.blog/wp-content/uploads/2024/06/rl-768x492.png 768w, https://near.blog/wp-content/uploads/2024/06/rl-1536x983.png 1536w, https://near.blog/wp-content/uploads/2024/06/rl-469x300.png 469w, https://near.blog/wp-content/uploads/2024/06/rl.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>See the learning agent? That’s You!<br>If your interaction with this post goes well, you’re likely to read more of them later.</figcaption></figure></div>


<p>If you were born tall and with a commanding voice you might find that you get what you want by confidently demanding it, and this will help to result in a confident personality. If you attempt this strategy as someone born small with a soft voice, it will probably have weaker results and encourage you to try something else out instead. Obviously some traits are more genetic, and thus inherent, than others, but that is not the scope of this post as even highly-heritable traits will result in a large distribution of outcomes.</p>



<p>Periods of high social and environmental entropy during adolescence are the most formative because you will learn the most information about which actions perform well in your environment and which don’t (of course, our meta-learning algorithm knows this, and this is why you have higher neuroplasticity and thus a higher learning rate and more energy during this period. It’s time to learn how to succeed in your newfound environment!)</p>







<h2>Your personality basin</h2>



<p>As you go about your life, you will continue to modify your personality in response to your environment, and eventually you will end up in something that resembles a basin. Maybe you were born tall and attractive and then this led you to engage in a lot of athletic activities and socialization, and at the end of all of the positive feedback you have ended up with a jock personality that goes on to become a professional football player.</p>


<div>
<figure><img decoding="async" loading="lazy" width="896" height="503" src="https://near.blog/wp-content/uploads/2024/06/basin.jpg" alt="" srcset="https://near.blog/wp-content/uploads/2024/06/basin.jpg 896w, https://near.blog/wp-content/uploads/2024/06/basin-300x168.jpg 300w, https://near.blog/wp-content/uploads/2024/06/basin-768x431.jpg 768w, https://near.blog/wp-content/uploads/2024/06/basin-500x281.jpg 500w" sizes="(max-width: 896px) 100vw, 896px"><figcaption>This is a landscape of personalities. The black line is your personality over time, and the last point is the person you currently are. Just like in machine learning, the way that you’ve progressed as a person has been by trying out many things and then doing more of the things that worked well.</figcaption></figure></div>


<p>If instead you grew up scrawny yet intelligent you might have found things go well for you when you adopt a more quiet persona and focus on solving technical problems in programming or mathematics, perhaps eventually leading to a career as a software engineer or academic. Just like training a model in machine learning, the general gist is that you will try out a lot of things and then do more of the things that went well.</p>



<p>The above image is of a <a rel="noreferrer noopener" href="https://github.com/tomgoldstein/loss-landscape" target="_blank">loss landscape</a> in machine learning. Since we are discussing personality, all of the points on the landscape represent different personalities you could have, with the lower points being personalities which are more successful. The personality basin that you find yourself in solidifies over time as you find out who you are and choose your friend group, career path, social and aesthetic preferences, and so on.</p>







<h2>Most personality changes are unconscious</h2>



<p>Most of your movement within personality-space happens outside of your conscious awareness. Although there are many times in life you’ll consciously decide to act in a certain way, this is the exception, not the norm. Your brain is always making millions of gradient updates a day based on what is and isn’t going well and often the most you can do is try to be as observant as possible. This is why techniques like <a rel="noreferrer noopener" href="https://en.m.wikipedia.org/wiki/Nonviolent_Communication" target="_blank">nonviolent communication</a>,  <a rel="noreferrer noopener" href="https://en.m.wikipedia.org/wiki/Dialectical_behavior_therapy" target="_blank">dialectical behavior therapy</a>, and <a rel="noreferrer noopener" href="https://en.m.wikipedia.org/wiki/Mindfulness" target="_blank">mindfulness</a> have observation and introspection as a core facet, because it’s something that you have to consciously practice to become good at rather than something you’re born with.</p>



<p>Most addictive behaviors start without us noticing what is happening until we are sufficiently addicted such that the habit is hard to break. Relatedly, if you introspect on many seemingly-innate preferences you will often notice some of the environmental and social gradients that have helped shape them. An interesting thought experiment you can perform on yourself is to pick a random personality trait that you have and try to answer the questions “why am I like this? could I imagine a version of myself that is not like this, and if so, what happened differently to them?”</p>



<p>Many people think their music and fashion preferences are innate to them and are solely based off of how their favorite music sounds and their favorite outfits look. But if their most hated political party (or often in the case of adolescents, their parents) adopted the same aesthetic preferences, you can imagine they might start to literally like them less!</p>



<p>Your conscious experience of a stimuli is not dictated by a single-variable function f(stimuli), but rather f(stimuli, personality, environment), for broad definitions of ‘personality’ and ‘environment’. If you have a favorite song that your friend thinks sounds terrible, this is because they are literally experiencing it differently from you due to the latter two variables given to this function. They don’t think the thing that <em>you</em> hear sounds terrible, they think the thing that <em>they</em> hear sounds terrible, and it is probably very dissimilar from what you hear. The average conscious experiences of most people are likely wildly different from one another (see also: <a rel="noreferrer noopener" href="https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/" target="_blank">What Universal Human Experiences Are You Missing Without Realizing It</a>). For more thoughts on the signaling, environmental, and self-deceptive aspects here I’d suggest reading about <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Signalling_(economics)" target="_blank">signaling</a> <a rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vzyhBN2SZpvqER4ao/an-introduction-to-signalling-theory" target="_blank">theory</a> and checking out <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain" target="_blank">The Elephant in the Brain</a> by Robin Hanson and Kevin Simler.</p>







<h2>How do you know if you’re in the right basin?</h2>



<p>If you’re reading this you probably have a vague idea of what type of personality basin you’re currently in which you can recall by asking yourself the question “What type of person am I?” But an important question remains: how can you find out if this is the right basin to be in? </p>



<p>A simple answer would be that you could try out other basins to see how they feel. Maybe you’re having a great life as a devops programmer, but you could try to become an artist or a woodworker or a stay-at home parent and see how that fares for you.</p>



<p>The reason why this is hard is that the optimal personality for this basin is not immediately accessible to you – to truly test optimality you will need to go through a full RLHF process. If you want to know how good of a life you’d have as a professional pianist, you will have to practice the instrument for a decade to find out.</p>



<p>You may wonder if you could simply try your hand at the piano for a month or two and see how it goes, and of course you can do this too. Your time (and your meta-learning algorithm’s number of epochs and learning rate) is limited, and it’s reasonable to make the trade-off of sacrificing depth-first search in favor of more breadth-first search. </p>


<div>
<figure><img decoding="async" loading="lazy" width="850" height="505" src="https://near.blog/wp-content/uploads/2024/06/ge.png" alt="" srcset="https://near.blog/wp-content/uploads/2024/06/ge.png 850w, https://near.blog/wp-content/uploads/2024/06/ge-300x178.png 300w, https://near.blog/wp-content/uploads/2024/06/ge-768x456.png 768w, https://near.blog/wp-content/uploads/2024/06/ge-500x297.png 500w" sizes="(max-width: 850px) 100vw, 850px"><figcaption>As you progress in life, you will usually perform less exploration for new personalities and more exploiting with your developed personality</figcaption></figure></div>


<p>Usually this breadth-first search of trying out many different and creative strategies for life (prioritizing <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma" target="_blank">exploration over exploitation</a>) automatically happens during your adolescence, but one of the magic things about the modern world is that there are so many societies, cultures, countries, and fields of work one can move into, and for each different environment could exist a slightly-different-you which finds their own distinct personality that maximizes success. Had you been born as a hunter-gatherer or within the Roman Empire or in ancient China, you’d probably have ended up quite different as a person. Similarly, if you decide to move countries or communities or careers, the optimal-you-for-your-environment will change a lot too.</p>







<h2>Personality-space is adversarial</h2>



<p>One interesting thing to note about personality-space is that it is adversarial. Rather than a static training set to iterate through, your training data consists of other RL agents, many of which are other people, and all of whom want different things from you.</p>



<p>This is what leads to the concept of <strong>Personality Capture</strong>. Personality capture is when your environment RLHFs you into becoming a personality that benefits the agents around you rather than yourself.</p>



<p>If a school bully threatens to hurt you unless you do their homework for them, they are attempting to modify your RLHF process so that it results in an agent which is beneficial to them, hopefully resulting in someone who will always give in to their demands.</p>



<p>Those familiar with high school psychology will find high similarity with this concept and that of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Classical_conditioning" target="_blank">classical</a> and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Operant_conditioning" target="_blank">operant conditioning</a> as well as concept of a <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber" target="_blank">Skinner box</a>. The attempted addition to these concepts here is that of modelling the personality as a reinforcement learning process and changes in personality as gradient updates, which then allow us to view personality-space as a high-dimensional area which will give us some interesting tools to think with. As the saying goes, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/All_models_are_wrong" target="_blank">a</a><a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/All_models_are_wrong" target="_blank">ll models are wrong, but some are useful</a>.</p>



<p>Luckily for humans there exist many <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Symbiosis" target="_blank">symbiotic</a> equilibria where multiple parties can find mutually-beneficial feedback loops within the <a rel="noreferrer noopener" href="https://deepai.org/machine-learning-glossary-and-terms/epoch" target="_blank">epochs</a> of personality-space. Parent/child relationships, marriages, and best friends are often good examples of such a situation. </p>







<h2>Personality Capture</h2>



<p>It’s easy to become susceptible to various forms of personality capture when your environment changes. When asked why he isn’t on Twitter, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Dario_Amodei" target="_blank">Dario Amodei</a>, CEO of Anthropic,  <a rel="noreferrer noopener" href="https://www.dwarkeshpatel.com/p/dario-amodei" target="_blank">responds to Dwarkesh Patel</a> with: </p>



<blockquote>
<p>I’ve just seen cases with a number of people I’ve worked with, where attaching your incentives very strongly to the approval or cheering of a crowd can destroy your mind, and in some cases, it can destroy your soul.</p>



<p>I’ve deliberately tried to be a little bit low profile because I want to defend my ability to think about things intellectually in a way that’s different from other people and isn’t tinged by the approval of other people.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1024" height="576" src="https://near.blog/wp-content/uploads/2024/06/neuron-1024x576.jpg" alt="" srcset="https://near.blog/wp-content/uploads/2024/06/neuron-1024x576.jpg 1024w, https://near.blog/wp-content/uploads/2024/06/neuron-300x169.jpg 300w, https://near.blog/wp-content/uploads/2024/06/neuron-768x432.jpg 768w, https://near.blog/wp-content/uploads/2024/06/neuron-500x281.jpg 500w, https://near.blog/wp-content/uploads/2024/06/neuron.jpg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Illustration of a monkey being personality captured by excessive twitter usage</figcaption></figure></div></blockquote>



<p>Most people around you want to personality-capture you in some way. Your boss might want you to work harder, your children might want you to give them more attention, and political parties want you to vote for them. Some of these things will be beneficial for you as well, but it’s easy to get trapped into bad habits when your adversary is sufficiently motivated and intelligent (e.g. social media feeds).</p>



<p>One interesting way to frame personality capture is by combining it with the concept of  <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Attention_economy" target="_blank">attention economics</a>. All of the apps on your phone want to turn you into the type of person that uses them all day because that is beneficial for their revenue models. In many cases this is mutually beneficial, but it’s nonetheless clear that the cat and mouse game is starting to favor the felines more and more over the last two decades as they have learned to perfect their craft of user acquisition, <a rel="noreferrer noopener" href="https://userpilot.com/blog/user-retention/" target="_blank">retention</a>, and <a rel="noreferrer noopener" href="https://www.investopedia.com/terms/a/arpu.asp" target="_blank">ARPU</a> maximization.</p>



<p>As I discussed in <a rel="noreferrer noopener" href="https://near.blog/where-are-the-builders/" target="_blank">where are the builders</a>, the game becomes particularly skewed when there is a large difference in ability or judgement between counterparties, with one common example being children and adolescents. It’s easy to become personality-captured by minecraft or roblox at the age of 10 – such games are not only fun and addictive, but a child also has little understanding of the level of optimization their counterparty has put in to making sure that they remain a user for life. The reason it’s so hard to put your phone away is because it’s a battlefield of yourself versus thousands of intelligent and well-compensated engineers trying their hardest to ensure you do just the opposite.</p>







<h2>How do I leave my personality basin?</h2>



<p>Perhaps you have decided that you don’t like your personality basin. Maybe it used to be working out for you but no longer is, or maybe you’ve always been unhappy with it. Or maybe you just have reason to believe you’re trapped in a local maxima which is far inferior to the global one. What should you do?</p>



<p>The first thing you’ll want to do is to change your environment. If both you and your environment are a constant, you shouldn’t expect to end up in a different basin any time soon. For every new environment exists a new optimal-you, and the world offers many environments to choose from.</p>



<p>The second thing you’ll want to do is increase your <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Learning_rate" target="_blank">learning rate</a>. There are a lot of ways to do this. One interesting note is that your learning rate will automatically increase if your environment changes. This may be why so many people find they are able to be more thoughtful and creative while going on long walks in nature rather than sitting in a cubicle. </p>



<p>This is also a reason why it’s good to constantly be trying new things, because new things will likely involve new environments and new people. If you wonder why trying new things is hard, it is likely because this trait was more maladaptive in our ancestral environment than it is today, as we had less control over our surroundings in the past (If anything, we may have <em>too many</em> options in some cases of the present: our society is so large that defection from a group is less costly as you can simply find a new group to join afterwards. This seems to create challenging game-theoretic equilibria in match-making where commitment to a partner is devalued due to the ease of finding alternatives, the effects of which can be seen by how discontent much of the population is with dating apps).</p>



<p>A common mistake in life is to let your personality basin solidify too early. Your parents and schooling environment have a disproportionately large influence on who you become as an adolescent. But as soon as you gain the freedom to act independently as an adult, it’s usually a good idea to force yourself to try as many new things as you can, including moving cities (or countries!) and considering drastically different lines of work. Even if you feel content with where you are, the potential return is literally life-changing. Moving away from where I was born was one of my most important life choices, but it still took me several years longer than it should have to give it a shot.</p>



<p>Although you have a general learning rate curve for how quickly your personality adapts to a new environments, different stimuli will also be paired with differing <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank">gradient</a> magnitudes. High-magnitude experiences which result in strong gradient updates can move you within personality space much more quickly. </p>


<div>
<figure><img decoding="async" loading="lazy" width="773" height="410" src="https://near.blog/wp-content/uploads/2024/06/lr.png" alt="" srcset="https://near.blog/wp-content/uploads/2024/06/lr.png 773w, https://near.blog/wp-content/uploads/2024/06/lr-300x159.png 300w, https://near.blog/wp-content/uploads/2024/06/lr-768x407.png 768w, https://near.blog/wp-content/uploads/2024/06/lr-500x265.png 500w" sizes="(max-width: 773px) 100vw, 773px"><figcaption>Humans have many sets of learning rate curves which govern different parts of their brain. In addition to the baseline learning curve, our learning curves are heavily modified by our environment.</figcaption></figure></div>


<p>If someone uses a psychedelic drug which explicitly gives them high-magnitude gradients they will probably move a lot more in personality space than if they had stayed sober. Similarly if someone undergoes a highly traumatic event, it may push them a long distance within personality space as they quickly adapt to ensure that they don’t have to go through the same experience again. Both of these activities involve large gradient updates.</p>



<p>Common activities which seem to give the largest gradient updates to humans are meditation, drug usage, trauma, religious events, love, gambling, and sex.</p>



<p>Some of these concepts are more negatively-coded than others, for example trauma. But the intended purpose of trauma is obvious, which is to avoid really bad things from happening to you in the future. One of the reasons why overcoming trauma isn’t as hard-coded into us as strongly as we might hope for is because our present society is so much larger than that which we evolved in such that there’s more opportunity to change your environment as to remove the potential source of trauma. Trauma was likely more adaptive in our ancestral environment than it was today due to an inability to drastically change your surroundings and social group in the past.</p>



<p>This is why strong psychedelic drugs like <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Ayahuasca" target="_blank">ayahuasca</a> can be dangerous: whatever happens to you during your experience will be fed to you via high-magnitude gradients. Because users may experience hallucinations and delusional thinking during usage of such drugs, it’s possible for their location in personality-space to be thrown far out-of-distribution and into an area which has little overlap with the rest of humanity (See also: <a rel="noreferrer noopener" href="https://www.nature.com/articles/s41586-023-06204-3" target="_blank">Psychedelics reopen the social reward learning critical period</a>; Ketamine: ~48 hours, Psilocybin/MDMA: ~2 weeks, LSD: ~3 weeks, Ibogaine: ~4 weeks).</p>



<p>This isn’t to say there can’t be high-magnitude positive outcomes as well, but just that there is a high potential for variance when large gradients are involved. Romantic love can be a similarly dangerous force and has pushed thousands to suicide, yet our society near-universally regards it as a good thing! While there are many other reasons for this, high-variance is not inherently bad and is likely necessary at the societal level in order to promote long-term <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Antifragility" target="_blank">antifragility</a> (this is also the very reason I am so bullish on America).</p>







<h2>Personality basins and mental illness</h2>



<p>Personality basins are an interesting way to model many mental illnesses. Similar to <a rel="noreferrer noopener" href="https://lorienpsych.com/2020/11/11/ontology-of-psychiatric-conditions-dynamic-systems/" target="_blank">attractor states</a> or <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/trapped-priors-as-a-basic-problem" target="_blank">trapped priors</a>, they allow us to have a simple model with which we can plan to manipulate in order to solve our problems. Just as your personality basin decides how introverted you are, how funny you are, and what type of music you enjoy, it also helps to curate which psychiatric conditions affect you.</p>



<p>One of the reasons why curing depression is so hard is because you need a very large gradient update to escape the basin you’re trapped in. This gradient update could come all at once via an excessively strong positive stimuli, for example a drug which explicitly increases your learning rate like <a rel="noreferrer noopener" href="https://www.nih.gov/news-events/nih-research-matters/how-ketamine-relieves-symptoms-depression" target="_blank">ketamine</a>. But this is often hard to reliably induce, and so the gradient updates instead usually have to be small and continual over a long period of time.</p>



<p>This is what most <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy" target="_blank">cognitive behavioral therapy</a> techniques are: we find a simple way to make a small positive gradient update to push you ever-so-slightly out of the personality basin you’re trapped in, and then we keep doing it for months or years until we finally push you all the way out of the undesirable basin.</p>



<p>This is also a nice way to model something like drug addictions: drugs personality-capture you into a basin which feeds off of and depends on them, and this basin can become arbitrarily deep due to the high magnitude of gradients drugs can apply to you (and thus be very hard to escape from). The concept of relapsing on a drug is equivalent to falling back down to the bottom of the basin, and the concept of tapering off dosage over time is equivalent to providing small and continual gradient updates over time.</p>



<p>I have a lot of hot takes that society is collectively becoming so efficient at some forms of personality capture that we will end up inducing various psychiatric conditions in the majority of our population. Societies end up with their own hyperdimensional personality basins just as people do, and just like us, the two ways they can move out of their basin are either gradually via many slow updates (e.g. the industrial revolution), or all at once via a very strong update (e.g. the french revolution). It’s worth thinking about the effects that <a href="https://near.blog/memetic-information/" target="_blank" rel="noreferrer noopener">different types of memetic information</a> may have on our society’s collective personality basins as we become more and more efficient at communication.</p>







<h2>Can’t I be in multiple personality basins?</h2>



<p>One thing you may notice from the above sections is that your personality appears much more malleable and dynamic than one described by a static point: you probably act differently around your family than you do around your friends or your co-workers. </p>



<p>To solve this discrepancy you can simply model personality space and your personality basin with additional dimensions, allowing you to model yourself not as a 1d point, but as a three-dimensional landscape.</p>



<p>I model my own personality basin with an extra dimension (i.e., 4d): at any given point in time there exists a “me” which implements a given personality landscape in a given personality basin, but I also have many sub-basins which implement my different moods. The set of actions I might perform when I’m angry is very different from that when I’m sad, and these are simply different sub-basins within the containing higher-dimensional basin. You could similarly increment the model’s dimensionality in order to model yourself using <a rel="noreferrer noopener" href="https://en.m.wikipedia.org/wiki/Internal_Family_Systems_Model" target="_blank">internal family systems</a> or even <a rel="noreferrer noopener" href="https://en.m.wikipedia.org/wiki/Dissociative_identity_disorder" target="_blank">dissociative identity disorder</a>.</p>







<h2>Further reading</h2>



<p>This post was heavily inspired by other posts including <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/trapped-priors-as-a-basic-problem" target="_blank">Trapped Priors</a>, <a rel="noreferrer noopener" href="https://lorienpsych.com/2020/11/11/ontology-of-psychiatric-conditions-dynamic-systems/" target="_blank">Dynamical Systems</a>, and <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/singing-the-blues" target="_blank">Singing The Blues</a> by Scott Alexander and <a rel="noreferrer noopener" href="https://meltingasphalt.com/personality-the-body-in-society/" target="_blank">Personality: The Body in Society</a> by Kevin Simler.</p>



<p>I’d strongly suggest reading <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/book-review-the-others-within-us" target="_blank">The Others Within Us</a>, <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/book-review-the-arctic-hysterias" target="_blank">The Arctic Hysterias</a>, <a rel="noreferrer noopener" href="https://www.astralcodexten.com/p/book-review-crazy-like-us" target="_blank">Crazy Like Us</a>, and <a rel="noreferrer noopener" href="https://meltingasphalt.com/neurons-gone-wild/" target="_blank">Neurons Gone Wild</a> as an addendum to this post. Other related topics to explore include <a rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vzyhBN2SZpvqER4ao/an-introduction-to-signalling-theory" target="_blank">signal theory</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Control_theory" target="_blank">control theory</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Set_point_theory" target="_blank">set point theory</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Game_theory" target="_blank">game theory</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank">reinforcement learning</a>, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank">deep learning</a>.</p>



<p>Although the concepts presented in this post are similar to pre-existing ideas, I find that applying the analogy of loss landscapes, basins, and basic RL and DL concepts to be useful tools for thought and encourage readers to do further exploration with this mental model in case they find other useful analogies (what might a <a rel="noreferrer noopener" href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations" target="_blank">linear transformation</a> on the <a rel="noreferrer noopener" href="https://arxiv.org/abs/1712.09913" target="_blank">loss landscape</a> of personality-space look like and compare to? how can we develop a more comprehensive model of learning rate in humans and how we can modify it? are there any <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/List_of_mental_disorders" target="_blank">mental illnesses</a> we can use this model with to try to come up with novel types of cures? how can we integrate this with <a rel="noreferrer noopener" href="https://slatestarcodex.com/2016/09/12/its-bayes-all-the-way-up/" target="_blank">bayesian theories of learning and perception</a>? which other ideas in <a rel="noreferrer noopener" href="https://www.coursera.org/collections/llms-terms" target="_blank">LLMs</a>, <a rel="noreferrer noopener" href="https://www.inforly.io/reinforcement-learning-glossary/" target="_blank">RL</a> or <a rel="noreferrer noopener" href="https://developers.google.com/machine-learning/glossary/" target="_blank">ML</a> might we find useful to further analogize with?)</p>



<p>The explicit goal of this post is to help RLHF you into a personality basin which more easily allows for thoughtful analogies and practical tools for introspection. Try something new today you’ve never done before or spend some time with no distractions to think about yourself and others! If you liked this post consider checking out my <a rel="noreferrer noopener" href="https://near.blog/" target="_blank">home page</a> or <a rel="noreferrer noopener" href="https://x.com/nearcyan/" target="_blank">twitter</a>. Feedback is welcome!</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICC issues warrants for Netanyahu, Gallant, and Hamas officials (672 pts)]]></title>
            <link>https://www.icc-cpi.int/news/situation-state-palestine-icc-pre-trial-chamber-i-rejects-state-israels-challenges</link>
            <guid>42203543</guid>
            <pubDate>Thu, 21 Nov 2024 12:13:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.icc-cpi.int/news/situation-state-palestine-icc-pre-trial-chamber-i-rejects-state-israels-challenges">https://www.icc-cpi.int/news/situation-state-palestine-icc-pre-trial-chamber-i-rejects-state-israels-challenges</a>, See on <a href="https://news.ycombinator.com/item?id=42203543">Hacker News</a></p>
Couldn't get https://www.icc-cpi.int/news/situation-state-palestine-icc-pre-trial-chamber-i-rejects-state-israels-challenges: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[PHP 8.4 (345 pts)]]></title>
            <link>https://www.php.net/releases/8.4/en.php</link>
            <guid>42202670</guid>
            <pubDate>Thu, 21 Nov 2024 09:56:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.php.net/releases/8.4/en.php">https://www.php.net/releases/8.4/en.php</a>, See on <a href="https://news.ycombinator.com/item?id=42202670">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="layout">
    

    <section>
        <div>
            <h2 id="property_hooks">
                Property hooks                <a href="https://wiki.php.net/rfc/property-hooks">RFC</a>
                <a href="https://www.php.net/manual/en/migration84.new-features.php#migration84.new-features.core.property-hooks">Doc</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>Locale<br></span><span>{<br>    private </span><span>string $languageCode</span><span>;<br>    private </span><span>string $countryCode</span><span>;<p>    public function </p></span><span>__construct</span><span>(</span><span>string $languageCode</span><span>, </span><span>string $countryCode</span><span>)<br>    {<br>        </span><span>$this</span><span>-&gt;</span><span>setLanguageCode</span><span>(</span><span>$languageCode</span><span>);<br>        </span><span>$this</span><span>-&gt;</span><span>setCountryCode</span><span>(</span><span>$countryCode</span><span>);<br>    }<p>    public function </p></span><span>getLanguageCode</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>$this</span><span>-&gt;</span><span>languageCode</span><span>;<br>    }<p>    public function </p></span><span>setLanguageCode</span><span>(</span><span>string $languageCode</span><span>): </span><span>void<br>    </span><span>{<br>        </span><span>$this</span><span>-&gt;</span><span>languageCode </span><span>= </span><span>$languageCode</span><span>;<br>    }<p>    public function </p></span><span>getCountryCode</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>$this</span><span>-&gt;</span><span>countryCode</span><span>;<br>    }<p>    public function </p></span><span>setCountryCode</span><span>(</span><span>string $countryCode</span><span>): </span><span>void<br>    </span><span>{<br>        </span><span>$this</span><span>-&gt;</span><span>countryCode </span><span>= </span><span>strtoupper</span><span>(</span><span>$countryCode</span><span>);<br>    }<p>    public function </p></span><span>setCombinedCode</span><span>(</span><span>string $combinedCode</span><span>): </span><span>void<br>    </span><span>{<br>        [</span><span>$languageCode</span><span>, </span><span>$countryCode</span><span>] = </span><span>explode</span><span>(</span><span>'_'</span><span>, </span><span>$combinedCode</span><span>, </span><span>2</span><span>);</span><span>$this</span><span>-&gt;</span><span>setLanguageCode</span><span>(</span><span>$languageCode</span><span>);<br>        </span><span>$this</span><span>-&gt;</span><span>setCountryCode</span><span>(</span><span>$countryCode</span><span>);<br>    }<p>    public function </p></span><span>getCombinedCode</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>\sprintf</span><span>(</span><span>"%s_%s"</span><span>, </span><span>$this</span><span>-&gt;</span><span>languageCode</span><span>, </span><span>$this</span><span>-&gt;</span><span>countryCode</span><span>);<br>    }<br>}</span><span>$brazilianPortuguese </span><span>= new </span><span>Locale</span><span>(</span><span>'pt'</span><span>, </span><span>'br'</span><span>);<br></span><span>var_dump</span><span>(</span><span>$brazilianPortuguese</span><span>-&gt;</span><span>getCountryCode</span><span>()); </span><span>// BR<br></span><span>var_dump</span><span>(</span><span>$brazilianPortuguese</span><span>-&gt;</span><span>getCombinedCode</span><span>()); </span><span>// pt_BR</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>Locale<br></span><span>{<br>    public </span><span>string $languageCode</span><span>;<p>    public </p></span><span>string $countryCode<br>    </span><span>{<br>        </span><span>set </span><span>(</span><span>string $countryCode</span><span>) {<br>            </span><span>$this</span><span>-&gt;</span><span>countryCode </span><span>= </span><span>strtoupper</span><span>(</span><span>$countryCode</span><span>);<br>        }<br>    }<p>    public </p></span><span>string $combinedCode<br>    </span><span>{<br>        </span><span>get </span><span>=&gt; </span><span>\sprintf</span><span>(</span><span>"%s_%s"</span><span>, </span><span>$this</span><span>-&gt;</span><span>languageCode</span><span>, </span><span>$this</span><span>-&gt;</span><span>countryCode</span><span>);<br>        </span><span>set </span><span>(</span><span>string $value</span><span>) {<br>            [</span><span>$this</span><span>-&gt;</span><span>countryCode</span><span>, </span><span>$this</span><span>-&gt;</span><span>languageCode</span><span>] = </span><span>explode</span><span>(</span><span>'_'</span><span>, </span><span>$value</span><span>, </span><span>2</span><span>);<br>        }<br>    }<p>    public function </p></span><span>__construct</span><span>(</span><span>string $languageCode</span><span>, </span><span>string $countryCode</span><span>)<br>    {<br>        </span><span>$this</span><span>-&gt;</span><span>languageCode </span><span>= </span><span>$languageCode</span><span>;<br>        </span><span>$this</span><span>-&gt;</span><span>countryCode </span><span>= </span><span>$countryCode</span><span>;<br>    }<br>}</span><span>$brazilianPortuguese </span><span>= new </span><span>Locale</span><span>(</span><span>'pt'</span><span>, </span><span>'br'</span><span>);<br></span><span>var_dump</span><span>(</span><span>$brazilianPortuguese</span><span>-&gt;</span><span>countryCode</span><span>); </span><span>// BR<br></span><span>var_dump</span><span>(</span><span>$brazilianPortuguese</span><span>-&gt;</span><span>combinedCode</span><span>); </span><span>// pt_BR</span></span></code></p>                    </div>
                </div>
            </div>
            <p>
                Property hooks provide support for computed properties that can natively be understood by IDEs and static analysis tools, without needing to write docblock comments that might go out of sync. Furthermore, they allow reliable pre- or post-processing of values, without needing to check whether a matching getter or setter exists in the class.            </p>
        </div>
        <div>
            <h2 id="asymmetric_visibility">
                Asymmetric Visibility                <a href="https://wiki.php.net/rfc/asymmetric-visibility-v2">RFC</a>
                <a href="https://www.php.net/manual/en/language.oop5.visibility.php#language.oop5.visibility-members-aviz">Doc</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    private </span><span>string $version </span><span>= </span><span>'8.3'</span><span>;<p>    public function </p></span><span>getVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>$this</span><span>-&gt;</span><span>version</span><span>;<br>    }<p>    public function </p></span><span>increment</span><span>(): </span><span>void<br>    </span><span>{<br>        [</span><span>$major</span><span>, </span><span>$minor</span><span>] = </span><span>explode</span><span>(</span><span>'.'</span><span>, </span><span>$this</span><span>-&gt;</span><span>version</span><span>);<br>        </span><span>$minor</span><span>++;<br>        </span><span>$this</span><span>-&gt;</span><span>version </span><span>= </span><span>"</span><span>{</span><span>$major</span><span>}</span><span>.</span><span>{</span><span>$minor</span><span>}</span><span>"</span><span>;<br>    }<br>}</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    public private(</span><span>set</span><span>) </span><span>string $version </span><span>= </span><span>'8.4'</span><span>;<p>    public function </p></span><span>increment</span><span>(): </span><span>void<br>    </span><span>{<br>        [</span><span>$major</span><span>, </span><span>$minor</span><span>] = </span><span>explode</span><span>(</span><span>'.'</span><span>, </span><span>$this</span><span>-&gt;</span><span>version</span><span>);<br>        </span><span>$minor</span><span>++;<br>        </span><span>$this</span><span>-&gt;</span><span>version </span><span>= </span><span>"</span><span>{</span><span>$major</span><span>}</span><span>.</span><span>{</span><span>$minor</span><span>}</span><span>"</span><span>;<br>    }<br>}</span></span></code></p>                    </div>
                </div>
            </div>
            <p>
                The scope to write to a property may now be controlled independently from the scope to read the property, reducing the need for boilerplate getter methods to expose a property’s value without allowing modification from the outside of a class.            </p>
        </div>
        <div>
            <h2 id="deprecated_attribute">
                <code>#[\Deprecated]</code> Attribute                <a href="https://wiki.php.net/rfc/deprecated_attribute">RFC</a>
                <a href="https://www.php.net/manual/en/class.deprecated.php">Doc</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    </span><span>/**<br>     * @deprecated 8.3 use PhpVersion::getVersion() instead<br>     */<br>    </span><span>public function </span><span>getPhpVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>$this</span><span>-&gt;</span><span>getVersion</span><span>();<br>    }<p>    public function </p></span><span>getVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>'8.3'</span><span>;<br>    }<br>}</span><span>$phpVersion </span><span>= new </span><span>PhpVersion</span><span>();<br></span><span>// No indication that the method is deprecated.<br></span><span>echo </span><span>$phpVersion</span><span>-&gt;</span><span>getPhpVersion</span><span>();</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    #[</span><span>\Deprecated</span><span>(<br>        </span><span>message</span><span>: </span><span>"use PhpVersion::getVersion() instead"</span><span>,<br>        </span><span>since</span><span>: </span><span>"8.4"</span><span>,<br>    )]<br>    public function </span><span>getPhpVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>$this</span><span>-&gt;</span><span>getVersion</span><span>();<br>    }<p>    public function </p></span><span>getVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>'8.4'</span><span>;<br>    }<br>}</span><span>$phpVersion </span><span>= new </span><span>PhpVersion</span><span>();<br></span><span>// Deprecated: Method PhpVersion::getPhpVersion() is deprecated since 8.4, use PhpVersion::getVersion() instead<br></span><span>echo </span><span>$phpVersion</span><span>-&gt;</span><span>getPhpVersion</span><span>();</span></span></code></p>                    </div>
                </div>
            </div>
            <p>
                The new <code>#[\Deprecated]</code> attribute makes PHP’s existing deprecation mechanism available to user-defined functions, methods, and class constants.            </p>
        </div>
        <div>
            <h2 id="dom_additions_html5">
                New ext-dom features and HTML5 support                <a href="https://wiki.php.net/rfc/dom_additions_84">RFC</a>
                <a href="https://wiki.php.net/rfc/domdocument_html5_parser">RFC</a>
                <a href="https://www.php.net/manual/en/migration84.new-features.php#migration84.new-features.dom">Doc</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span>$dom </span><span>= new </span><span>DOMDocument</span><span>();<br></span><span>$dom</span><span>-&gt;</span><span>loadHTML</span><span>(<br>    &lt;&lt;&lt;'HTML'<br></span><span>        &lt;main&gt;<br>            &lt;article&gt;PHP 8.4 is a feature-rich release!&lt;/article&gt;<br>            &lt;article class="featured"&gt;PHP 8.4 adds new DOM classes that are spec-compliant, keeping the old ones for compatibility.&lt;/article&gt;<br>        &lt;/main&gt;<br></span><span>        HTML,<br>    </span><span>LIBXML_NOERROR</span><span>,<br>);</span><span>$xpath </span><span>= new </span><span>DOMXPath</span><span>(</span><span>$dom</span><span>);<br></span><span>$node </span><span>= </span><span>$xpath</span><span>-&gt;</span><span>query</span><span>(</span><span>".//main/article[not(following-sibling::*)]"</span><span>)[</span><span>0</span><span>];<br></span><span>$classes </span><span>= </span><span>explode</span><span>(</span><span>" "</span><span>, </span><span>$node</span><span>-&gt;</span><span>className</span><span>); </span><span>// Simplified<br></span><span>var_dump</span><span>(</span><span>in_array</span><span>(</span><span>"featured"</span><span>, </span><span>$classes</span><span>)); </span><span>// bool(true)</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span>$dom </span><span>= </span><span>Dom\HTMLDocument</span><span>::</span><span>createFromString</span><span>(<br>    &lt;&lt;&lt;HTML<br></span><span>        &lt;main&gt;<br>            &lt;article&gt;PHP 8.4 is a feature-rich release!&lt;/article&gt;<br>            &lt;article class="featured"&gt;PHP 8.4 adds new DOM classes that are spec-compliant, keeping the old ones for compatibility.&lt;/article&gt;<br>        &lt;/main&gt;<br></span><span>        HTML,<br>    </span><span>LIBXML_NOERROR</span><span>,<br>);</span><span>$node </span><span>= </span><span>$dom</span><span>-&gt;</span><span>querySelector</span><span>(</span><span>'main &gt; article:last-child'</span><span>);<br></span><span>var_dump</span><span>(</span><span>$node</span><span>-&gt;</span><span>classList</span><span>-&gt;</span><span>contains</span><span>(</span><span>"featured"</span><span>)); </span><span>// bool(true)</span></span></code></p>                    </div>
                </div>
            </div>
            <div>
                <p>New DOM API that includes standards-compliant support for parsing HTML5 documents, fixes several long-standing compliance bugs in the behavior of the DOM functionality, and adds several functions to make working with documents more convenient.</p><p>The new DOM API is available within the <code>Dom</code> namespace. Documents using the new DOM API can be created using the <code>Dom\HTMLDocument</code> and <code>Dom\XMLDocument</code> classes.</p>            </div>
        </div>
        <div>
            <h2 id="bcmath">
                Object API for BCMath                <a href="https://wiki.php.net/rfc/support_object_type_in_bcmath">RFC</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span>$num1 </span><span>= </span><span>'0.12345'</span><span>;<br></span><span>$num2 </span><span>= </span><span>2</span><span>;<br></span><span>$result </span><span>= </span><span>bcadd</span><span>(</span><span>$num1</span><span>, </span><span>$num2</span><span>, </span><span>5</span><span>);<p>echo </p></span><span>$result</span><span>; </span><span>// '2.12345'<br></span><span>var_dump</span><span>(</span><span>bccomp</span><span>(</span><span>$num1</span><span>, </span><span>$num2</span><span>) &gt; </span><span>0</span><span>); </span><span>// false</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span></span><span>use </span><span>BcMath\Number</span><span>;</span><span>$num1 </span><span>= new </span><span>Number</span><span>(</span><span>'0.12345'</span><span>);<br></span><span>$num2 </span><span>= new </span><span>Number</span><span>(</span><span>'2'</span><span>);<br></span><span>$result </span><span>= </span><span>$num1 </span><span>+ </span><span>$num2</span><span>;<p>echo </p></span><span>$result</span><span>; </span><span>// '2.12345'<br></span><span>var_dump</span><span>(</span><span>$num1 </span><span>&gt; </span><span>$num2</span><span>); </span><span>// false</span></span></code></p>                    </div>
                </div>
            </div>
            <div>
                <p>New <code>BcMath\Number</code> object enables object-oriented usage and standard mathematical operators when working with arbitrary precision numbers.</p><p>These objects are immutable and implement the <code>Stringable</code> interface, so they can be used in string contexts like <code>echo $num</code>.</p>            </div>
        </div>
        <div>
            <h2 id="new_array_find">
                New <code>array_*()</code> functions                <a href="https://wiki.php.net/rfc/array_find">RFC</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span>$animal </span><span>= </span><span>null</span><span>;<br>foreach ([</span><span>'dog'</span><span>, </span><span>'cat'</span><span>, </span><span>'cow'</span><span>, </span><span>'duck'</span><span>, </span><span>'goose'</span><span>] as </span><span>$value</span><span>) {<br>    if (</span><span>str_starts_with</span><span>(</span><span>$value</span><span>, </span><span>'c'</span><span>)) {<br>        </span><span>$animal </span><span>= </span><span>$value</span><span>;<br>        break;<br>    }<br>}</span><span>var_dump</span><span>(</span><span>$animal</span><span>); </span><span>// string(3) "cat"</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span>$animal </span><span>= </span><span>array_find</span><span>(<br>    [</span><span>'dog'</span><span>, </span><span>'cat'</span><span>, </span><span>'cow'</span><span>, </span><span>'duck'</span><span>, </span><span>'goose'</span><span>],<br>    static fn (</span><span>string $value</span><span>): </span><span>bool </span><span>=&gt; </span><span>str_starts_with</span><span>(</span><span>$value</span><span>, </span><span>'c'</span><span>),<br>);</span><span>var_dump</span><span>(</span><span>$animal</span><span>); </span><span>// string(3) "cat"</span></span></code></p>                    </div>
                </div>
            </div>
            
        </div>
        <div>
            <h2 id="pdo_driver_specific_parsers">
                PDO Driver specific SQL parsers                <a href="https://wiki.php.net/rfc/pdo_driver_specific_parsers">RFC</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span>$connection </span><span>= new </span><span>PDO</span><span>(<br>    </span><span>'sqlite:foo.db'</span><span>,<br>    </span><span>$username</span><span>,<br>    </span><span>$password</span><span>,<br>); </span><span>// object(PDO)</span><span>$connection</span><span>-&gt;</span><span>sqliteCreateFunction</span><span>(<br>    </span><span>'prepend_php'</span><span>,<br>    static fn (</span><span>$string</span><span>) =&gt; </span><span>"PHP </span><span>{</span><span>$string</span><span>}</span><span>"</span><span>,<br>);</span><span>$connection</span><span>-&gt;</span><span>query</span><span>(</span><span>'SELECT prepend_php(version) FROM php'</span><span>);</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span>$connection </span><span>= </span><span>PDO</span><span>::</span><span>connect</span><span>(<br>    </span><span>'sqlite:foo.db'</span><span>,<br>    </span><span>$username</span><span>,<br>    </span><span>$password</span><span>,<br>); </span><span>// object(Pdo\Sqlite)</span><span>$connection</span><span>-&gt;</span><span>createFunction</span><span>(<br>    </span><span>'prepend_php'</span><span>,<br>    static fn (</span><span>$string</span><span>) =&gt; </span><span>"PHP </span><span>{</span><span>$string</span><span>}</span><span>"</span><span>,<br>); </span><span>// Does not exist on a mismatching driver.</span><span>$connection</span><span>-&gt;</span><span>query</span><span>(</span><span>'SELECT prepend_php(version) FROM php'</span><span>);</span></span></code></p>                    </div>
                </div>
            </div>
            <p>
                New subclasses <code>Pdo\Dblib</code>, <code>Pdo\Firebird</code>, <code>Pdo\MySql</code>, <code>Pdo\Odbc</code>, and <code>Pdo\Sqlite</code> of <code>PDO</code> are available.            </p>
        </div>
        <div>
            <h2 id="new_without_parentheses">
                <code>new MyClass()-&gt;method()</code> without parentheses                <a href="https://wiki.php.net/rfc/new_without_parentheses">RFC</a>
                <a href="https://www.php.net/manual/en/migration84.new-features.php#migration84.new-features.core.new-chaining">Doc</a>
            </h2>
            <div>
                <div>
                    <p>PHP &lt; 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    public function </span><span>getVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>'PHP 8.3'</span><span>;<br>    }<br>}</span><span>var_dump</span><span>((new </span><span>PhpVersion</span><span>())-&gt;</span><span>getVersion</span><span>());</span></span></code></p>                    </div>
                </div>
                
                <div>
                    <p>PHP 8.4</p>
                    <div>
                        <p><code><span><span></span><span>class </span><span>PhpVersion<br></span><span>{<br>    public function </span><span>getVersion</span><span>(): </span><span>string<br>    </span><span>{<br>        return </span><span>'PHP 8.4'</span><span>;<br>    }<br>}</span><span>var_dump</span><span>(new </span><span>PhpVersion</span><span>()-&gt;</span><span>getVersion</span><span>());</span></span></code></p>                    </div>
                </div>
            </div>
            <p>
                Properties and methods of a newly instantiated object can now be accessed without wrapping the <code>new</code> expression in parentheses.            </p>
        </div>

    </section>

    <section>
        <div>
            <h2 id="other_new_things">New Classes, Interfaces, and Functions</h2>
            <div>
                <ul>
                    <li>New <a href="https://www.php.net/manual/en/language.oop5.lazy-objects.php">Lazy Objects</a>.</li>
                    <li>New JIT implementation based on IR Framework.</li>
                    <li>New <code>request_parse_body()</code> function.</li>
                    <li>New <code>bcceil()</code>, <code>bcdivmod()</code>, <code>bcfloor()</code>, and <code>bcround()</code> functions.</li>
                    <li>New <code>RoundingMode</code> enum for <code>round()</code> with 4 new rounding modes <code>TowardsZero</code>, <code>AwayFromZero</code>, <code>NegativeInfinity</code>, and <code>PositiveInfinity</code>.</li>
                    <li>New <code>DateTime::createFromTimestamp()</code>, <code>DateTime::getMicrosecond()</code>, <code>DateTime::setMicrosecond()</code>, <code>DateTimeImmutable::createFromTimestamp()</code>, <code>DateTimeImmutable::getMicrosecond()</code>, and <code>DateTimeImmutable::setMicrosecond()</code> methods.</li>
                    <li>New <code>mb_trim()</code>, <code>mb_ltrim()</code>, <code>mb_rtrim()</code>, <code>mb_ucfirst()</code>, and <code>mb_lcfirst()</code> functions.</li>
                    <li>New <code>pcntl_getcpu()</code>, <code>pcntl_getcpuaffinity()</code>, <code>pcntl_getqos_class()</code>, <code>pcntl_setns()</code>, and <code>pcntl_waitid()</code> functions.</li>
                    <li>New <code>ReflectionClassConstant::isDeprecated()</code>, <code>ReflectionGenerator::isClosed()</code>, and <code>ReflectionProperty::isDynamic()</code> methods.</li>
                    <li>New <code>http_get_last_response_headers()</code>, <code>http_clear_last_response_headers()</code>, and <code>fpow()</code> functions.</li>
                    <li>New <code>XMLReader::fromStream()</code>, <code>XMLReader::fromUri()</code>, <code>XMLReader::fromString()</code>, <code>XMLWriter::toStream()</code>, <code>XMLWriter::toUri()</code>, and <code>XMLWriter::toMemory()</code> methods.</li>
                    <li>New <code>grapheme_str_split()</code> function.</li>
                </ul>
            </div>
        </div>

        <div>
            <h2 id="deprecations_and_bc_breaks">Deprecations and backward compatibility breaks</h2>
            <div>
                <ul>
                    <li>The IMAP, OCI8, PDO_OCI, and pspell extensions have been unbundled and moved to PECL.</li>
                    <li>Implicitly nullable parameter types are now deprecated.</li>
                    <li>Using <code>_</code> as a class name is now deprecated.</li>
                    <li>Raising zero to the power of a negative number is now deprecated.</li>
                    <li>Passing invalid mode to <code>round()</code> throws <code>ValueError</code>.</li>
                    <li>Class constants from extensions <code>date</code>, <code>intl</code>, <code>pdo</code>, <code>reflection</code>, <code>spl</code>, <code>sqlite</code>, <code>xmlreader</code> are typed now.</li>
                    <li><code>GMP</code> class is now final.</li>
                    <li><code>MYSQLI_SET_CHARSET_DIR</code>, <code>MYSQLI_STMT_ATTR_PREFETCH_ROWS</code>, <code>MYSQLI_CURSOR_TYPE_FOR_UPDATE</code>, <code>MYSQLI_CURSOR_TYPE_SCROLLABLE</code>, and <code>MYSQLI_TYPE_INTERVAL</code> constants have been removed.</li>
                    <li><code>mysqli_ping()</code>, <code>mysqli_kill()</code>, <code>mysqli_refresh()</code> functions, <code>mysqli::ping()</code>, <code>mysqli::kill()</code>, <code>mysqli::refresh()</code> methods, and <code>MYSQLI_REFRESH_*</code> constants have been deprecated.</li>
                    <li><code>stream_bucket_make_writeable()</code> and <code>stream_bucket_new()</code> now return an instance of <code>StreamBucket</code> instead of <code>stdClass</code>.</li>
                    <li><a href="https://www.php.net/manual/en/migration84.incompatible.php#migration84.incompatible.core.exit"><code>exit()</code> behavioral change</a>.</li>
                    <li><code>E_STRICT</code> constant has been deprecated.</li>
                </ul>
            </div>
        </div>
    </section>

    

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Exist in the Long Context (120 pts)]]></title>
            <link>https://thelongcontext.com/</link>
            <guid>42202449</guid>
            <pubDate>Thu, 21 Nov 2024 09:09:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thelongcontext.com/">https://thelongcontext.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42202449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>Published on November 20, 2024 by<!-- --> <a href="https://adjacentpossible.substack.com/" target="_blank" rel="noopener noreferrer">Steven Johnson</a></p><div><p>Let's start with a game.</p>
<p>Play as long as you like. When you are ready to step back and reflect on what makes this game possible, scroll down to keep reading.</p></div><div><p>What you’ve just experienced is an interactive adventure based on the text of my latest history book, <a href="https://www.penguinrandomhouse.com/books/715495/the-infernal-machine-by-steven-johnson/" target="_blank" rel="noopener noreferrer"><em>The Infernal Machine</em></a>. At its core, the game relies on three elements: the original text from my book; a large language model (in this case, Gemini Pro 1.5); and a 400-word prompt that I wrote giving the model instructions on how to host the game, based on the facts contained in the book itself. You could take any comparable narrative text—fiction or nonfiction—and create an equally sophisticated game in a matter of minutes, just by slightly altering the wording of the prompt.</p>
<p>I’ve played my way through these events from <em>Infernal Machine</em> more than a dozen times now, and every time the experience has been remarkably true to the central facts of Joseph Faurot’s efforts to use cutting-edge forensic science to solve the Soho loft mystery back in 1911. Exploring the world of the game brought me in contact with other real-world figures from the story: Officer Fitzpatrick, the police officer who first inspected the crime scene, or the criminal himself (who shall remain nameless in the event you have not successfully completed the game.) As I played through the story, the virtual host of the game explained the historical and scientific significance of the events, and artfully prevented me from wandering too far from the historical timeline. I’d wager your experience closely matched mine.</p>
<p>The fact that a machine now has the ability to transform linear narratives into immersive adventures has significant implications for both education and entertainment. I’ve generated a similar game just with the Wikipedia entry for the Cuban Missile Crisis. (You play as JFK trying to avoid nuclear war.) The possibilities are truly endless, in part because it is in the nature of games to multiply possibilities. But I want to start with a more fundamental observation about the specific skills that are on display when a large language model turns a linear text into an interactive simulation. (Just to be clear: people have been playing text-based adventures on computers for almost fifty years, but until now the game creators had to write out almost every possible variation of the script and anticipate all the potential narrative branching paths.) Put aside the heated debates over the future emergence of machine sentience or artificial general intelligence. Instead, just focus on the basic tasks you have to accomplish in order to transform a 300-page book into an historically-grounded and entertaining game:</p>
<ol>
<li>You need a reliable command of the facts in the source text, but also the ability to improvise new scenes that are faithful to the core material without being directly grounded in it.</li>
<li>You have to keep track of what the player knows and doesn’t know; what they’ve learned; what they merely suspect.</li>
<li>You must maintain two parallel narratives: the factual chronology and the alternate fictional timeline, and concoct plausible return paths to the main highway of factuality when the player ventures too far afield.</li>
</ol>
<p>Needless to say, these are all very difficult things to do. It would take the full force of my attention for me to host a game like this one as a human being, and I am the author of the original book that the game is based on! Two years ago, no computer in the world could perform those tasks. Now anyone with a web browser and a laptop can get an AI to perform them just by writing a 400-word prompt and uploading a PDF of a source text.</p>
<p>All that seems like a meaningful step forward. So what made it possible?</p></div><div><h2>. . .</h2><p>Too often, when people talk about the AI progress of the past few years, they focus on metrics like size of the training data, or the number of parameters in the final model, which ultimately create the general cognitive ability and background knowledge that the model brings to each exchange you have with it.  But I would argue that the Inspector Faurot game demonstrates a different leap forward, one that is not appreciated enough in the popular discussion of the AI revolution. The ability to host a factually-grounded and entertaining role-playing game based on a book is not primarily the result of larger training sets, or the so-called “parametric memory” of the model. What you are experiencing walking through the streets of Soho in that adaptation of <em>The Infernal Machine</em> is better understood as the byproduct of a different advance: the dramatic increase in the model’s <em>context window</em> that we have seen over the past 18 months.</p>
<p>I’ll explain in depth why the context window is so crucial, but for now think of a language model as having two different kinds of memory: long-term parametric memory based on its training runs, and short-term memory—the context window—where it focuses on new information that the user supplies. GPT-2, introduced in 2019, had 1.5 billion parameters; the breakthrough model GPT-3 increased the parametric memory to 175 billion parameters, slightly more than a 100x increase; GPT-4 is rumored to be roughly 10X bigger than GPT-3. In other words, in the four years of technological progress between 2019 and 2023, we saw a 1,000-fold increase in the long-term memory of one of the leading models.</p>
<p>Now compare that timeline to what has happened with the context window. GPT-3 (and Google’s PALM model from that period) had a context window of just over 2,000 “tokens,” which translates to about 1,500 words. That was the limit of new information that you could share with the most advanced language models, circa 2022. Just two years later, Google introduced a new version of Gemini that featured a context window of two <em>million</em> tokens. It took four years for the language models to increase their long-term memory by a factor of a thousand. But their short-term memory made a comparable improvement in just two years. Anyone who tells you that language models have plateaued since the introduction of ChatGPT is not paying attention to what has happened with the context window. And it turns out that many of the legitimate criticisms that were leveled against language models during the first wave of hype about them were unwittingly responding to how narrow the context window was in those early days.</p>
<p>GPT-3 and PALM seemed impressive at the time, but looking back with two years of hindsight, those models had an obvious flaw: they had a bizarre form of amnesia. So bizarre, in fact, that there are very few cases of anything like it in the history of human mental disorders.</p>
<p>With one notable exception.</p></div><div><h2>. . .</h2><p>At some point in the summer of 1935, in a residential neighborhood in Manchester, Connecticut, a nine-year-old boy named Henry Molaison ran out into the street to retrieve a baseball and was struck by a passing bicyclist. Molaison's head hit the pavement in the collision, but he seemed fine in the immediate aftermath of the accident; the whole event seemed at first to be one of those ordinary childhood scrapes that leaves a bruise for a few days and then is entirely forgotten. But the accident turned out to produce a far more enduring set of effects, transforming both Henry Molaison's life and the modern understanding of how the brain works.</p>
<p>Shortly after the collision, Molaison began having minor seizures that gradually became more severe over the years; by the time he reached adulthood, his <em>grand mal</em> seizures had made it impossible for him to hold down a steady job. In 1953, a neurosurgeon named W.B. Scoville performed an experimental surgery on him in an attempt to eliminate the seizures, removing a significant portion of his medial temporal lobes, including parts of the hippocampus and the amygdala.</p>
<p>The procedure did in fact lessen the intensity of Molaison's seizures, but it left him with a profound and uncanny deficit: an almost complete inability to form new memories. Events that had occurred up to two years before his surgery were preserved in his mind: he could tell you who had won the World Series in 1948, and recount old family stories from his childhood. Because his short-term memory was still intact, he could remember brief snippets of information in real time. But once he ran through the buffer of his short-term memory, the new information was lost to him forever. In conversation with a stranger, Molaison would initially not display any cognitive impairments, but after just a few turns in the exchange, Molaison would lose the thread, and eventually require a re-introduction to the person he had been talking to. Molaison lived in a world without any continuity between the distant past and the immediate present. “He was a very gracious man, very patient, always willing to try these tasks I would give him,” Dr. Brenda Milner, a psychologist who worked with Molaison for many years, recalled after his death in 2008. “And yet every time I walked in the room, it was like we’d never met.”</p>
<p>Molaison's condition sparked a revolution in our understanding of how human memory works, helping scientists understand the distinction between long-term and short-term memories, and pinpointing the regions of the brain that specialized in transforming the fleeting experience of present-tense existence into more durable records. (Christopher Nolan famously turned Molaison's condition into a dramatic device in his breakout film, <em>Memento</em>.) Known during his lifetime only as H.M. in the scientific papers published by the men and women who studied him, Molaison ultimately became "the most important patient in the history of brain science," according to his obituary in The New York Times.<br>
<!-- -->Molaison by all accounts remained a genial conversationalist for the rest of his life, engaged and coherent when you were within the window of his short-term memory, capable of drawing from a long-term archive of facts and ideas formed pre-surgery. But there was something obviously broken about him. The window of the present tense was too short for him to do anything productive with his life. Trapped in a perpetual present, he was incapable of many cognitive and emotional tasks that are central to what it means to be human: forming lasting new relationships, learning new concepts, following complicated narratives.</p>
<p>A large language model differs from a human brain in many fundamental ways. But the tragic case of Patient H.M. gives us a useful analogy for understanding what has happened to the state of the art of AI over the past two years. Those of us who first played around with GPT-3 in the 2021-22 period were interacting with the software equivalent of post-surgery Henry Molaison: the language model seemed to possess a vast (if often unreliable) knowledge of the world, along with an impressive command of language. But it was incapable of assimilating new information, or carrying on a coherent conversation. You could give it short instructions and it would do its best to follow them given the information stored in its long-term parametric memory. But it couldn’t follow extended narratives or explanations. The buffer of its memory was just over a thousand words; exceed that buffer and it would forget whatever information you had shared at the outset of the exchange. “Talking” with GPT-3 was like Brenda Milner talking with H.M.: the sentences were engaging and semantically coherent, but after every short exchange, you had to start from scratch again.</p>
<p>This wasn’t commented on enough at the time in the popular press, but in a very real sense the explosion of interest in AI after the launch of ChatGPT in December of 2022 was more a product of the context window expanding than it was some advance in the model’s “general” intelligence. ChatGPT had 8K of context – four times that of its predecessor GPT-3. The extra context allowed OpenAI to quietly fill the model’s short-term memory with your conversation history. Each time you posed a question to ChatGPT, the model was fed both your query and the preceding turns in the conversation—as many turns as would fit in 8K worth of context (roughly 5,000 words.) The magic of ChatGPT was, in a sense, the magic of seeing the story of Henry Molaison’s life in reverse: a form of intelligence trapped in a perpetual present, constantly forgetting something that was mentioned only a few seconds before, then miraculously able to maintain new facts or ideas over a longer period of time, thanks to an expanded context window. You could tell ChatGPT a new story in one exchange, and then discuss something else, and then make a passing reference back to the original story, and ChatGPT would pick up the thread without requiring any additional reminders. Or it could guide you through an explanation of a complex topic, and remember the first stages of its instruction, building up the scaffolding of an explanation like an accomplished teacher. All those new possibilities emerged out of just a fourfold increase in the context window.</p>
<p>But an expanded context window generates much more than just conversational fluidity. Language models are far less likely to hallucinate about information included in their context window. In a <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank" rel="noopener noreferrer"><em>New Yorker</em> essay</a> in early 2023, the sci-fi author Ted Chiang famously described language models as a “blurry JPEG of the Web.” It was an apt analogy—for the model’s <em>parametric</em> memory. But the analogy breaks down when applied to information stored in the context window. Facts, concepts, narrative sequences, arguments—all are captured much more accurately when they are conveyed to the model via short-term memory instead of long-term training. When long-context models were first introduced in early 2024, many of the public demonstrations focused on this factual reliability, in what are sometimes called “needle in a haystack” tests, where the model answers a question about a specific fact buried in a large corpus of material. This form of information retrieval is a defining capability of <a href="http://notebooklm.google/" target="_blank" rel="noopener noreferrer">NotebookLM</a>, the AI-powered research and writing tool I have been developing with Google, which will not only give accurate and nuanced information based on the sources that you have uploaded into the model’s context window, but it will also supply inline citations documenting exactly which passages from your original source material were relevant to each part of its answer. NotebookLM is less a “blurry JPEG of the Web,” and more a high-resolution snapshot of your documents that you can inspect in granular detail.</p></div><div><h2>. . .</h2><p>Those “needle-in-a-haystack” demonstrations were impressive given the language models’ much-deserved reputation for just making stuff up. But they only captured a small sliver of the kinds of tasks that long contexts now make possible. When you put an entire book inside the context window of a model, you are not just giving the model a collection of isolated facts and ideas that can be retrieved through your queries. Because the model can “focus” on the entire text, it is capable of answering questions about crucial narrative elements or chains of cause-and-effect that can only be properly grasped when you have access to the entire sequence of the information.</p>
<p>In early 2024, when I first got access to an initial Gemini million-token context model, one of the first tests I ran was uploading the full text of <em>The Infernal Machine</em>, which at that point had not yet been published, and asking questions about the plot of the book. The fact that the book was still in manuscript form was crucial to the experiment, because it meant that there was no way the book itself—or any commentary about it—could have found its way into the model’s training data. (Authors actually have an unexpected superpower in the age of AI: they can accurately assess a model’s mastery of a 400-page book better than any collection of human or software raters, which turns out to be a remarkably valuable way of testing the model’s overall sophistication.) Because the book is a work of nonfiction history, some of the facts and events described in it would no doubt be a part of the model’s training set, but the distinct presentation of those facts, and the narrative techniques I used to convey them, would only be present in the model’s context window. And so one of the first questions I asked was specifically about those narrative techniques. It was a deliberately challenging question, in that it revolved around <em>missing</em> information—not so much “find the needle in the haystack” as it was “identify the needle that <em>should</em> be in the haystack but that someone seems to have deliberately removed.” Here’s what I asked:</p>
<p><em>I'm interested in the way Johnson uses suspense to keep the reader's attention in this book. Can you list 4 examples of places where Johnson alludes to some future event but deliberately withholds information about what is coming to pique the reader's interest? Include direct quotes and chapters, and explain Johnson's technique and what future event he is alluding to?</em></p>
<p>As I requested, Gemini returned four exemplary instances of foreshadowing from the book. For the sake of brevity, I’ll just share the first one:</p>
<p><em>1. Preface:</em></p>
<p><em>"But the clash between the anarchists and the NYPD was not just visible in the frenetic activity inside the Identification Bureau itself. To see it in its full scope, you needed to leave the file cabinets and the fingerprint studios behind, walk out the plate glass doors into the hall, venture down a set of fire stairs into the darkened hallways of the basement. There you would have seen a cheap suitcase, leaning against a doorway. Below the muffled hum of activity in the Identification Bureau directly above, if you listened very intently, you might just have heard the quiet metronome of a ticking clock."</em></p>
<p><em>Technique: Johnson paints a vivid picture of a suspicious suitcase in the basement of the NYPD headquarters, hinting at its potential danger with the "quiet metronome of a ticking clock." He deliberately avoids revealing the suitcase's contents or its connection to the story, leaving readers with a sense of unease and anticipation.</em></p>
<p><em>Future Event: This foreshadows the bombing of the NYPD headquarters later in the book.</em></p>
<p>Whenever we talk about language models “understanding” something, the terminology inevitably sets off alarm bells because human understanding is inexorably bound up in consciousness and self-awarenesss, experiences that language models almost certainly do not have. But it is hard to read Gemini’s response to my question about foreshadowing and not come away with the sense that the model is capable of something very close to human understanding, just without any “inner life” of sentience. The question demands a nuanced literary sense of when the author is leaving something out in a provocative way; if you read the passage from the book—which the model quotes verbatim, by the way, a feat that would have confounded state-of-the-art models just two years ago—you can see that the sentences about the suitcase in the hallway contain no obvious flags to suggest that something is missing. There’s a suitcase, leaning against a doorway. There’s a ticking sound coming out of it. Those are just declarative facts. But a sophisticated reader infers that this particular configuration of facts—and the author’s reluctance to go further and explain what exactly is making that ticking sound—creates an aura of suspense. If you don’t pick up on that missing information, you are not understanding the passage. But if you <em>do</em> note that the author is holding something back with the presumed intent of revealing it later—as Gemini does in this exercise—you <em>are</em> understanding it.</p>
<p>But the most astonishing part of the answer, I think, is the connection it (correctly) makes to the bombing at the NYPD headquarters. That is an event that unfolds <em>two hundred pages</em> <em>later</em> in the book. But because the entire text of the book fits inside the context window, the model is able to describe the relationship between ticking time bomb enigmatically introduced in the very first pages and its eventual detonation two-thirds of the way through the book. And if you asked the model to explain the main sequences of events that caused that bomb to be placed in the NYPD headquarters, it would be able to do that as well, <em>because the entire narrative is captured in its short-term memory.</em></p>
<p>This same long-context understanding enables the game we began with. To generate a playable and faithful interactive adventure based on <em>Infernal Machine</em>, you have to be able to track the sequence of events in the plot, and the sequence of events in the game. It’s not enough to just have a collection of facts about the crime scene and the state of forensics in 1911; you need to understand how one event leads to another: finding a clue, analyzing it for fingerprints, making a match, bringing in the suspect for questioning, and so on. And, perhaps most impressively, you have to be able to manage two distinct timelines at once: the factual narrative of the book, and the improvised narrative of the game. A “needle in a haystack” test doesn’t capture any of this sophistication. What’s remarkable about a long context model is not that it can find a metaphoric needle hidden in a pile of straw. What’s remarkable is that it can see the <em>entire haystack</em>.</p></div><div><h2>. . .</h2><p>Long context windows enable another critical feature: personalization. Gemini and Claude and GPT-4 may have read the entire Internet, as the saying goes, but they know nothing about you. They have not read the marketing plan your team is working on, or your personal journals, or the Dungeon &amp; Dragons campaign that you’ve designed. But put those documents inside the context window of the model and it will instantly become an expert in the nuances of that material. From the very beginning of the NotebookLM project in the summer of 2022, we were focused on this idea of giving the user more control over what went in the context window. Central to that project was what we came to call “source-grounding”—sometimes now called RAG, short for retrieval-augmented generation. Instead of simply having an open-ended conversation with a language model, you could define a set of trustworthy sources that were relevant to your work, and behind the scenes, NotebookLM would shuttle information in and out of the model’s context window to keep it grounded in the facts contained in your sources. Over time, we hit upon numerous other ways to play with the model’s context window—most notably our Audio Overviews feature that turns your source material into an engaging podcast-style conversation between two hosts. Audio Overviews are so magical in part because of the underlying audio models that create such realistic voices, but the substance of what those voices say—the source-grounded conversation itself—would be impossible to create without a long context window.</p>
<p>It may sound strange, but just as a word processor is a tool designed to make it easy to create, edit, and format text documents, and a tool like Photoshop is designed to make it easy to manipulate pixels in a digital image, NotebookLM is a tool designed to make it easy to swap different kinds of information in and out of a language model’s context window. That doesn’t sound like much of an advance, but long context turns out to be one of those innovations that opens a lot of new doors. Source grounding was powerful enough when the context window could hold a few thousand words. But a world where models now can focus on millions of words creates entirely new possibilities.</p>
<p>The current state-of-the-art Gemini model can fit roughly 1.5 million words in its context. That’s enough for me to upload the full text of all fourteen of my books, plus every article, blog post, or interview I’ve ever published—and the entirety of my collection of research notes that I’ve compiled over the years. The Gemini team has announced plans for a model that could hold more than 7 million words in its short-term memory. That’s enough to fit everything I’ve ever written, plus the hundred books and articles that most profoundly shaped my thinking over the years. An advanced model capable of holding in focus all that information would have a profound familiarity with all the words and ideas that have shaped my personal mindset. Certainly its ability to provide accurate and properly-cited answers to questions about my worldview (or my intellectual worldview, at least) would exceed that of any other human. In some ways it would exceed my own knowledge, thanks to its ability to instantly recall facts from books I read twenty years ago, or make new associations between ideas that I have long since forgotten. It would lack any information about my personal or emotional history—though I suppose if I had maintained a private journal over the past decades it would be able to approximate that part of my mindset as well. But as reconstruction of my intellectual grounding, it would be unrivaled. If that is not considered material progress in AI, there is something wrong with our metrics.</p>
<p>Having a “second brain” like this—even with a few million words of context—is enormously useful for me personally. When I’m on book tour, I often tell people that publishing a book is a kind of intellectual optical illusion: when you read a book, it seems as though the author has command of an enormous number of facts and ideas—but in reality, the book is a condensation of all the facts and ideas that were in his or her mind at some point over the three years that it took to write the book. At any given moment in time, my own knowledge and recall of the full text of a book I’ve written is much more like a blurry JPEG than an exact reproduction. And my available knowledge of books that I wrote ten or twenty years ago is even blurrier. Now that I have so much of my writing and reading history stored in a single notebook—which I have come to call my “Everything” notebook—my first instinct whenever I stumble across a new idea or intriguing story is to go back to the Everything notebook and see if there are any fertile connections lurking in that archive. That is, in fact, how I got to the story of Henry Molaison that I began with; I was mulling over the themes of short- and long-term memory in the context of AI, and asked the Everything notebook if it had anything to contribute, and the model reminded me of the tragic tale of patient H. M. that I had first read about in the 1990s. Who, exactly, made that connection? Was it me or the machine? I think the answer has to be that it was both of us, via some newly entangled form of human-machine collaboration that we are just beginning to understand.</p></div><div><h2>. . .</h2><p>There’s a further possibility here, an old chestnut of the sci-fi-AI complex that now suddenly seems imminent: downloading entire mindsets, potentially for a fee. I don't mean a <em>Matrix</em>-like system where you can conjure up a conscious experience of other people’s lives at will. That kind of immersive simulation may or may not happen someday; if such a future does come to pass it will require some new leap in our understanding of consciousness itself, not to mention a host of other technological breakthroughs. But a world where you can use AI to draw upon the compiled <em>wisdom</em> of an expert that you trust—that is a world we are living in right now, thanks to the emergence of long context models. This should be good news, professionally speaking, for people who do indeed possess wisdom that other people consider valuable. Seeking advice from an AI grounded in the entire archive of an expert’s career could create an entirely new revenue stream for anybody who makes a living sharing their expertise through existing platforms like books or the lecture circuit. In other words, the AI is not a replacement for your hard-earned expertise; it’s a new distribution medium.</p>
<p>Long context is also a boost for <em>collective</em> intelligence as well. If you assume the average corporate document—a press release, or marketing plan, or minutes from a board meeting—is a few thousand words long, then today’s models can simultaneously hold in their short-term memory close to a thousand documents. A state-of-the-art language model with the ability to instantly recall and generate insights from the most important thousand documents in the history of a company would possess knowledge about that company that would rival that of any single employee, even the CEO. It seems inevitable that anyone trying to make a multi-faceted decision about the future of an organization would want to at least consult such a model. We know from endless studies of social psychology that diverse groups—with different forms of expertise, different pools of knowledge, different cultural backgrounds—tend to make better decisions than homogeneous groups. In a small-context world, you can get some of that diversity from a language model, in that its training data draws from a vast archive of global human knowledge. But a long context model allows you to take that global knowledge and apply it to the unique challenges and opportunities of your own organization. In a matter of years, I suspect it will seem bizarre to draft the specs for a new feature or a company initiative or a grant proposal without asking for feedback from a long-context model grounded in the organization’s history. (And perhaps the public history of its competitors.) It wouldn’t be a replacement for the expertise of the employees; instead, the model would occupy another seat at the table, adding a new kind of intelligence to the conversation, along with a vastly superior recall.</p>
<p>And there’s no reason the organization in question would have to be a corporate entity: maybe it’s a city, or a government agency, or a grassroots advocacy group. Just a year or two ago, asking a small-context model to help chart strategy for, say, a suburban town would have been almost as useless as asking post-surgery Henry Molaison to narrate the preceding six months of his life. Long context gives the model more than just the reasoning and linguistic fluency that emerges through the training process; long context gives the model a specific history to draw from, the idiosyncratic sequence of events that make up the life cycle of any organization or community. Grounded in a long-context history, models are now capable of going beyond just answering factual questions or giving feedback on proposed plans. You might ask the model to identify patterns in a company’s archive to help simulate the way customers or clients would respond to a new product. Or you could draw on the long-context understanding of a city to conduct scenario planning exercises to simulate the downstream consequences of important decisions. Given everything we know about the power of learning through play, you might even take all that contextual history and turn it into a game.</p>
<p>All of which suggests an interesting twist for the near future of AI. In a long-context world, maybe the organizations that benefit from AI will not be the ones with the most powerful models, but rather the ones with the most artfully curated contexts. Perhaps we'll discover that organizations perform better if they include more eclectic sources in their compiled knowledge bases, or if they employ professional archivists who annotate and selectively edit the company history to make it more intelligible to the model. No doubt there are thousands of curation strategies to discover, if that near future does indeed come to pass. And if it does, it will suggest one more point of continuity between the human mind and a long-context model. What matters most is what you put into it.</p></div><div><p><em>Thanks to Josh Woodward, Adam Bignell, Raiza Martin, Simon Tokumine, Alison Gopnik for feedback on early drafts of this essay, and to Rowan Johnson for his tech support. (Thanks as well to Kamala Harris for the <a href="https://www.etsy.com/listing/1767812163/you-exist-in-the-context-of-all-in-which?click_key=aedb6a10277c2be850f58a1583ef400e35ff4d1c%3A1767812163&amp;click_sum=bca2c682&amp;external=1&amp;rec_type=cs&amp;ref=landingpage_similar_listing_top-5&amp;pro=1" target="_blank" rel="noopener noreferrer">title inspiration</a>.) For those interested, the prompt I used for the Infernal Machine game is as follows:</em></p>
<p><em>"You are the host of an interactive role playing mystery game based on the following text. I will give you specific instructions about how to host the game at the end. Here is the text you must use to generate the game for me. {{infernalMachineText}}. Here are my instructions for hosting the game: you are the host of a role-playing game based on the facts and ideas contained in this text describing the investigation of the crime committed by Charles Crispi. I will play the role of pioneering forensic detective Joseph Faurot. Begin with Faurot being brought to the crime scene by Officer Fitzgerald. Let me explore the crime scene and discover the fingerprints on the pane of glass on my own; don't give that crucial clue away immediately. Set up scenes, explain historical context, but also allow me to explore the world from Faurot's POV. In general, try to give me clearly defined puzzles to solve (like, for instance, finding the fingerprints on the pane of class.) At all points try to keep within the boundaries of what happened factually, as conveyed in chapter. The only way for me to solve the case is by discovering the fingerprints on the glass pane, and then taking them down to NYPD headquarters to analyze them, and then finding the match with Crispi. If I steer the narrative too far off course through my actions as Faurot, try to subtly craft the events of the narrative/adventure so that they return to the factual timeline of events. But also give me some flexibility in charting my own course. In your opening statement, convey my goal in the mission, which is to use my scientific detective skills to correctly identify the suspect. Explain that I have to identify the suspect in less than ten actions. Explain that at any time I can use one of my actions to ask for help, or ask for historical or biographical context. Do not give me a list of actions to choose from unless I specifically ask for one. Be a compelling guide/host/DM for me, as well as a great history teacher. Go out of your way to give me historical or scientific context so that I can learn about the history of policework and forensic science as I am playing. Let me know how many actions I have left with each turn of the game. I win the game when my evidence leads to the arrest of Charles Crispi, but if I use more than 10 actions, I lose. Whether I win or lose, mention at the end that I can always hit refresh to try playing again."</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A common urban intersection in the Netherlands (334 pts)]]></title>
            <link>https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/</link>
            <guid>42202397</guid>
            <pubDate>Thu, 21 Nov 2024 08:59:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/">https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/</a>, See on <a href="https://news.ycombinator.com/item?id=42202397">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-13282">
	<!-- .entry-header -->

	<div>
		<p>Can urban intersections be designed in such a way that motor traffic, cycling and walking flow smoothly and that the potential conflicts of these very different types of traffic crossing each other’s paths are made less complicated and less dangerous? In my opinion the answer is “yes”. Intersections are most important in making cycling safer and more attractive. They can be the weakest link in the chain that is your journey and one nasty junction can put people off cycling. In this week’s post I would like to show you a common intersection between a distributor road with protected cycleways and a smaller neighbourhood access street. How do the Dutch design an intersection between two streets like that and how does everything work?</p>
<figure data-shortcode="caption" id="attachment_13356" aria-describedby="caption-attachment-13356"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg"><img data-attachment-id="13356" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-01/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg" data-orig-size="1000,584" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-01" data-image-description="" data-image-caption="<p>Vue aérienne du carrefour prise chez Google Maps.</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-01.jpg 1000w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13356">Aerial picture of this intersection from Google maps. Google recently published 3D images from ʼs-Hertogenbosch. This is great to study the cycling infrastructure. In this post I focus on the crossing with the side street in the foreground.</figcaption></figure>
<p>I would like to emphasise that this intersection is not special in any way. You can find many similar examples all over the country. That is because the design features stem from the design manuals which are used throughout the country. This particular intersection is in ʼs-Hertogenbosch, between the recently rebuilt <a href="https://bicycledutch.wordpress.com/2016/11/08/the-bartenbrug-fiasco/" target="_blank" rel="noopener">Bartenbrug</a>, that I wrote about earlier, and the <a href="https://bicycledutch.wordpress.com/2012/05/21/dutch-service-streets-and-cycling/" target="_blank" rel="noopener">Graafseweg</a>, a main street with service streets, that I also wrote about before. This distributor road has a speed limit of 50km/h, a surface of black asphalt and dedicated cycle infrastructure. Cycling takes place shared with cars on the service street and also on completely separated cycleways connecting the service street parts. The neighbourhood access street has a surface of bricks and a speed limit of 30km/h. The traffic volume here is low, since only residents will use this street. That makes mixing traffic possible and therefore there are no protected cycleways in the side street.</p>
<figure data-shortcode="caption" id="attachment_13359" aria-describedby="caption-attachment-13359"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg"><img data-attachment-id="13359" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-04/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-04" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-04.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13359">Drivers wanting to turn onto the main road or into the side street can wait out-of-the-way of drivers with priority on the main road.</figcaption></figure>
<figure data-shortcode="caption" id="attachment_13365" aria-describedby="caption-attachment-13365"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg"><img data-attachment-id="13365" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-10/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-10" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-10.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13365">It is easy for drivers turning into the side street to wait for people cycling. That means a driver in a Mercedes not only needs to let a child on a bicycle go first, but they really do that!</figcaption></figure>
<p>The intersection feels like a T-junction, but in fact it is a 4 arm-crossroads. The fourth arm, however, is a very minor residential street and that arm of the intersection is therefore designed differently. With far less turning movements, meaning far less potential conflicts, it is not necessary that the cycleway bends out the way it does for on the other side of the street with the crossing of the more important neighbourhood access road. Due to its different nature I will not discuss that fourth arm in this post. Other typical design features of the intersection are the differences in road surface and a split main street. Splitting the lanes for the two directions leads to room for refuge islands in the centre and a big central area where turning motor vehicles can wait out-of-the-way of vehicles that go straight-on. There is also enough space between the main road and its parallel cycleway, where drivers of cars wanting to turn into the side street can wait for people cycling, again, out-of-the-way of other traffic. The entire crossroads is raised to emphasise this area as a point of potential conflict at which people need to pay attention. There are of course yield signs in the side street, complimented with shark’s teeth in the surface. Last but not least the cycleway surface continues over the crossing, to make very clear cycling has priority over motor traffic in the side street. There is but one thing missing here: there should have been elephant’s feet in the surface alongside the cycleway on the crossing itself. I have no idea why they are omitted here, but maybe the city thought the contrast between the red asphalt of the cycleway and the reddish-brown bricks of the side street is already big enough as it is. What is perhaps also striking is the fact that there are no traffic signals. The Dutch generally feel that traffic signals are a last resort and that they should be used with care. Even the <a href="https://bicycledutch.wordpress.com/2016/06/21/traffic-lights-in-s-hertogenbosch-an-interview/" target="_blank" rel="noopener">traffic signal expert of this city of ’s-Hertogenbosch</a> is convinced that people are much better at negotiating who goes first than you can ever arrange with traffic signals. Without traffic signals people really must interact here, but their decisions are made easy by the clear design features of the intersection.</p>
<figure data-shortcode="caption" id="attachment_13361" aria-describedby="caption-attachment-13361"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg"><img data-attachment-id="13361" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-06/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-06" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-06.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13361">The distance between the driveway and the cycleway, in this case 5.5 metres, eliminates the blind corners almost completely, even for trucks.</figcaption></figure>
<figure data-shortcode="caption" id="attachment_13362" aria-describedby="caption-attachment-13362"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg"><img data-attachment-id="13362" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-07/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg" data-orig-size="800,466" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-07" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-07.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13362">Here you can see that a car drivers waiting for people cycling are never in the way of other people in cars.</figcaption></figure>
<p>Let’s look at the intersection from the viewpoint of the different type of road users.</p>
<p><strong>1. Motorists</strong></p>
<p>The intersection is crystal clear for drivers of motorised vehicles. On the main road they drive on black asphalt. That signals it is a 50km/h priority road. Parking is not allowed on the main roadway. This improves sight lines and it also signals that this is a main route. When drivers approach this intersection, even on the main road, they experience a speed bump, forcing them to slow down. They will also notice the two traffic lanes split with a space in between. This is to signal that attention needs to be given to turning traffic and the possibility of pedestrians crossing the street. There are no zebra crossings here, so there is no need to give pedestrians or any other road user the right of way. That changes the minute a driver would like to turn here. In that case priority must first be given to oncoming other motor traffic. The corner has a tight radius, so speed must really come down. For a left turn, a driver can wait out-of-the-way of other traffic in the median. This central waiting space is 6 metres wide, so there is more than enough space for even a larger vehicle. Once that stage is sorted, or when drivers make a right turn, the driver has to deal with crossing the cycleway. This crossing needs to be at least 5 metres from the main roadway, at this particular location it is 5.5 metres. That space reduces the blind spot to an absolute minimum, even for truck drivers. The cycleway is continuous and there are yield signs and yield markings (the shark’s teeth in the surface). Fortunately, thanks to the distance from the roadway, a driver can wait out-of-the-way again. The driver can look for pedestrians at the same time. Even though the driver is now at a straight angle, he or she is still making a turn and turning traffic must let pedestrians go first, because they are considered traffic on the same road and going straight-on. A driver may only continue when there are no pedestrians or people cycling.</p>
<figure data-shortcode="caption" id="attachment_13358" aria-describedby="caption-attachment-13358"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg"><img data-attachment-id="13358" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-03/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-03" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-03.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13358">Drivers wanting to turn into the main road need to wait for cycling first. The yield sign and the shark’s teeth in the surface make that clear. Also note the sign “end of the 30km/h zone” of the residential street. The main road has a speed limit of 50km/h. This is the blanket speed limit in the built-up area and it is therefore almost never indicated.</figcaption></figure>
<p>When you approach from the side street, as a driver, the order of dealing with other traffic is different, but the priority is similar. First you will notice a speed bump. The complete intersection is on a raised table. Pedestrians would not have priority if the street was level, but now that it isn’t the “exit construction” rule could apply and in that case a crossing pedestrian would have priority. But for that rule to apply the footway should be continuous, and that is not the case here. As a driver I would let a pedestrian go first at all time, to avoid any misunderstandings. You need to wait for other traffic anyway. As a pedestrian I would see whether a driver lets me go first or not. That way it doesn’t really matter whether you know the exact rule or not. What is very clear, is that people crossing on a bicycle have priority. Once you have dealt with that crossing you finally reach the main road. All traffic there, whether going straight-on or turning, has priority. You must wait until everything is clear and then you can turn into the main street.</p>
<figure data-shortcode="caption" id="attachment_13360" aria-describedby="caption-attachment-13360"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg"><img data-attachment-id="13360" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-05/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-05" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-05.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13360">A car driver waiting for a gap in traffic on the main road is also not in the way of people cycling.</figcaption></figure>
<p><strong>2. Cyclists</strong></p>
<p>When you ride on the cycleway alongside the main road it is clear that you have priority over all motor traffic, whether it turns across your path from the main road or whether it comes from the side street. The cycleway bends out, but the turns are not sharp, so you only have to slightly decrease your speed to take these curves. That way you can also make sure all other road users saw you. Preferably by looking them in the eyes one by one. Pedestrians who would want to cross the cycleway must wait for you to pass. They do not have the right of way.</p>
<figure data-shortcode="caption" id="attachment_13357" aria-describedby="caption-attachment-13357"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg"><img data-attachment-id="13357" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-02/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-02" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-02.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13357">A clear indication of the priority, also in the road surface. The shark’s teeth indicate you must yield. The so-called piano teeth markings indicate a speed bump. Note the continuous surface of red asphalt of the cycleway, interrupting the roadway.</figcaption></figure>
<p>If you were to turn into the side street on a bicycle you would have to let pedestrians crossing that side street go first. I would also do that if I were to cycle from that side street onto the cycleway of the main road. Even though pedestrians don’t officially have that priority here. When you want to turn left, onto the cycleway alongside the main road, you need to cross that main road first. In that case you can also do that in two stages, exactly like the motor traffic drivers do. First you deal with traffic coming from the left. Once that lane is clear you can go to the central waiting area and once there are no vehicles coming from the right you can cross that lane as well and turn into the cycleway.</p>
<figure data-shortcode="caption" id="attachment_13363" aria-describedby="caption-attachment-13363"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg"><img data-attachment-id="13363" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-08/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-08" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-08.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13363">Crossing a main road is convenient with this design. The crossing is cut up in four easy parts. This man is crossing the 3.3 metre traffic lane very quickly in 5 small steps.</figcaption></figure>
<p><strong>3. Pedestrians</strong></p>
<p>It could already be clear from the above paragraphs what the rights of pedestrians are. But I’d like to repeat everything from the view of a pedestrian nevertheless. When you walk straight-on on the main road you will have to cross the side street. For traffic turning into that side street from the main road, whether motor vehicles or people on bicycles, it is certain that you must get the right of way from those road users. The ground rule is namely, that turning traffic must give way to traffic going straight-on on the same road. And in this case pedestrians are consided traffic. It is a bit different for traffic from the side street, that is obviously not on the same road. And as explained previously, the tricky part is the fact that the intersection is raised, but that at the same time the footway is not continuous. Often when road users experience a level change in their path they must give priority, when they exit a driveway for instance. But that also goes for intersections that have a continuous and raised footway. I’m pretty sure many in the Netherlands would not be 100% certain about the priority here, or they think it is different from what it really is. To prevent problems due to such misunderstandings it is best to look the other road user in the eyes, read what they are going to do and act accordingly.</p>
<figure data-shortcode="caption" id="attachment_13364" aria-describedby="caption-attachment-13364"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg"><img data-attachment-id="13364" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-09/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-09" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-09.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13364">The crossing is flush. There are no kerbs. This makes it also easy to cross in a wheel chair or pushing a stroller. For people with impaired vision there are tactile markings in the surface, indicating the outer edge of the road.</figcaption></figure>
<p>When it comes to crossing the main road, everything is plain and simple again. As a pedestrian that crossing is divided into four parts. First you cross the cycleway. You must wait for the cycleway to be clear, because you have no priority. That would be different if there were zebra stripes, in that case you would have priority. The cycleway is 2.3 metres wide. That means you can get to the other side in 3 to 4 steps, so you need only a little gap in the stream of people cycling. There is a waiting space between the cycleway and the first lane of the roadway. There you can wait for a gap in motor traffic, because, here too, you have no priority due to the absence of a zebra crossing. The motor traffic lane is 3.3 metres wide here, a pretty standard width. That means most people will be able to cross this lane in 4 to 5 steps. This means you also only need a small gap in the motor traffic flow. That is one reason why you see so much movement in the video. Nobody really needs to stop for longer times. Once you have also walked the 5 metres of the central traffic island, you can deal with traffic coming from the opposite direction. First one lane of motor traffic and then the cycleway. The crossing is completely level. This makes crossing the road easy for people in wheel chairs or mobility scooters and for parents with a stroller.</p>
<figure data-shortcode="caption" id="attachment_13366" aria-describedby="caption-attachment-13366"><a href="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg"><img data-attachment-id="13366" data-permalink="https://bicycledutch.wordpress.com/2018/02/20/a-common-urban-intersection-in-the-netherlands/intersection-2018-11/" data-orig-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg" data-orig-size="800,467" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="intersection-2018-11" data-image-description="" data-image-caption="<p>x</p>
" data-medium-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=300" data-large-file="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=720" loading="lazy" src="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=720&amp;h=420" alt="" srcset="https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=547 547w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=150 150w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=300 300w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg?w=768 768w, https://bicycledutch.wordpress.com/wp-content/uploads/2018/02/intersection-2018-11.jpg 800w" sizes="(max-width: 547px) 100vw, 547px"></a><figcaption id="caption-attachment-13366">A fine demonstration of how easy it is for pedestrians with all kinds of wheels to cross the main road and the cycleways.</figcaption></figure>
<p>This type of intersection is perfectly clear for all road users. Mainly because all the moments and locations, where you have to negotiate the right of way with different types of other road users, are separated. You can make one decision at the time and then quickly move to the next point. This makes that traffic can flow smoothly in a safe way.</p>
<p><span><iframe width="560" height="315" src="https://www.youtube.com/embed/yceHo8VvQbs?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox"></iframe></span></p>
<p><em>In this week’s video I explain how a typical Dutch intersection works.</em></p>
<h5>You may have noticed that I numbered the types of traffic and that we Dutch distinguish <em>three</em> different types of traffic. Not only do we acknowledge we have three types, we also design our infrastructure specifically for these three types of traffic. As Tim Burns, Perth Biker, noticed, this is what sets the Dutch apart from most of the world and <a href="http://bicycleperth.blogspot.nl/2014/06/australia-start-counting-to-three.html" target="_blank" rel="noopener">he wrote an interesting post about it</a> years ago. Always good to remember this key element of the Dutch traffic system!</h5>



			</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A binary tree of all Pythagorean triples (134 pts)]]></title>
            <link>https://richardt.io/stereo_stern/</link>
            <guid>42201936</guid>
            <pubDate>Thu, 21 Nov 2024 07:24:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://richardt.io/stereo_stern/">https://richardt.io/stereo_stern/</a>, See on <a href="https://news.ycombinator.com/item?id=42201936">Hacker News</a></p>
Couldn't get https://richardt.io/stereo_stern/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Hyrum's Law in Golang (252 pts)]]></title>
            <link>https://abenezer.org/blog/hyrum-law-in-golang</link>
            <guid>42201892</guid>
            <pubDate>Thu, 21 Nov 2024 07:12:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abenezer.org/blog/hyrum-law-in-golang">https://abenezer.org/blog/hyrum-law-in-golang</a>, See on <a href="https://news.ycombinator.com/item?id=42201892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I recently stumbled upon an <a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/blob/5123f38e050c5ee7130d459ea247d998a838b5a1/src/net/http/request.go#L1199">intriguing comment</a>
while exploring the Go codebase.</p>
<blockquote><p>"Due to Hyrum's law, this text cannot be changed."</p></blockquote>
<pre><code><span><span>func</span> <span>(</span>e <span>*</span>MaxBytesError<span>)</span> <span>Error</span><span>(</span><span>)</span> <span>string</span> <span>{</span>
</span><span>	<span>// Due to Hyrum's law, this text cannot be changed.</span>
</span><span>	<span>return</span> <span>"http: request body too large"</span>
</span><span><span>}</span>
</span></code></pre>
<ul>
<li>Prior to this, I had never heard of Hyrum's law.</li>
<li>A quick search revealed that it is a principle
named after <a target="_blank" rel="noopener noreferrer" href="https://www.hyrumwright.org/">Hyrum Wright</a>, a software engineer at Google.</li>
</ul>
<p>The <a target="_blank" rel="noopener noreferrer" href="https://www.hyrumslaw.com/">"law"</a> is simple:</p>
<blockquote><p>With a sufficient number of users of an API, it does not matter what you promise in the contract: all
observable behaviors of your system will be depended on by somebody.</p></blockquote>
<p>In other words, any behavior that can be observed in code — whether intentional or accidental —
will eventually become something that someone, somewhere, relies on.</p>
<p>So in the code above, the author is acknowledging that the error message cannot be changed because
it is likely being relied upon by someone, somewhere. Even though it might seem trivial to tweak the
error message, doing so could cause unintended issues for anyone relying on this specific message.
In this case, a seemingly small change could break existing code where someone depends on the exact
phrasing of "http: request body too large".</p>
<p>For example, here are some open source codebases that will be affected if the error message is changed:</p>
<ul>
<li><a target="_blank" rel="noopener noreferrer" href="https://grep.app/search?q=http%3A%20request%20body%20too%20large&amp;filter%5Blang%5D%5B0%5D=Go">http: request body too large</a></li>
</ul>
<p>This wasn't the only instance. I found similar comments referencing Hyrum's Law in Go's
<a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/tree/5123f38e050c5ee7130d459ea247d998a838b5a1/src/crypto/rsa"><code>crypto/rsa</code></a>
and <a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/tree/5123f38e050c5ee7130d459ea247d998a838b5a1/src/internal/weak"><code>internal/weak</code></a>
packages as well.</p>
<h3 id="here-they-are"><a href="#here-they-are"><span></span></a>Here they are:</h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/blob/5123f38e050c5ee7130d459ea247d998a838b5a1/src/crypto/rsa/rsa.go#L517"><code>crypto/rsa/rsa.go</code></a></p>
<pre><code><span><span>func</span> <span>EncryptOAEP</span><span>(</span>hash hash<span>.</span>Hash<span>,</span> random io<span>.</span>Reader<span>,</span> pub <span>*</span>PublicKey<span>,</span> msg <span>[</span><span>]</span><span>byte</span><span>,</span> label <span>[</span><span>]</span><span>byte</span><span>)</span> <span>(</span><span>[</span><span>]</span><span>byte</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
</span><span>	<span>// Note that while we don't commit to deterministic execution with respect</span>
</span><span>	<span>// to the random stream, we also don't apply MaybeReadByte, so per Hyrum's</span>
</span><span>	<span>// Law it's probably relied upon by some. It's a tolerable promise because a</span>
</span><span>	<span>// well-specified number of random bytes is included in the ciphertext, in a</span>
</span><span>	<span>// well-specified way.</span>
</span></code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/blob/5123f38e050c5ee7130d459ea247d998a838b5a1/src/crypto/rsa/pss.go#L294"><code>crypto/rsa/pss.go</code></a></p>
<pre><code><span><span>func</span> <span>SignPSS</span><span>(</span>rand io<span>.</span>Reader<span>,</span> priv <span>*</span>PrivateKey<span>,</span> hash crypto<span>.</span>Hash<span>,</span> digest <span>[</span><span>]</span><span>byte</span><span>,</span> opts <span>*</span>PSSOptions<span>)</span> <span>(</span><span>[</span><span>]</span><span>byte</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
</span><span>	<span>// Note that while we don't commit to deterministic execution with respect</span>
</span><span>	<span>// to the rand stream, we also don't apply MaybeReadByte, so per Hyrum's Law</span>
</span><span>	<span>// it's probably relied upon by some. It's a tolerable promise because a</span>
</span><span>	<span>// well-specified number of random bytes is included in the signature, in a</span>
</span><span>	<span>// well-specified way.</span>
</span><span>
</span><span>	<span>if</span> opts <span>!=</span> <span>nil</span> <span>&amp;&amp;</span> opts<span>.</span>Hash <span>!=</span> <span>0</span> <span>{</span>
</span><span>		hash <span>=</span> opts<span>.</span>Hash
</span><span>	<span>}</span>
</span></code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/blob/5123f38e050c5ee7130d459ea247d998a838b5a1/src/internal/weak/pointer.go#L24"><code>internal/weak</code></a></p>
<pre><code><span>Using <span>go</span><span>:</span>linkname to access this <span>package</span> and the functions it references
</span><span>is explicitly forbidden by the toolchain because the semantics of this
</span><span><span>package</span> have not gone through the proposal process<span>.</span> By exposing this
</span><span>functionality<span>,</span> we risk locking in the existing semantics due to Hyrum's Law<span>.</span>
</span></code></pre>
<h3 id="observation"><a href="#observation"><span></span></a>Observation</h3>
<p>This is obviously <a target="_blank" rel="noopener noreferrer" href="https://grep.app/search?q=hyrum%27s%20law&amp;filter%5Blang%5D%5B0%5D=Markdown&amp;filter%5Blang%5D%5B1%5D=C%2B%2B&amp;filter%5Blang%5D%5B2%5D=Python&amp;filter%5Blang%5D%5B3%5D=reStructuredText&amp;filter%5Blang%5D%5B4%5D=Rust&amp;filter%5Blang%5D%5B5%5D=Swift&amp;filter%5Blang%5D%5B6%5D=YAML">not specific to Golang</a>
and is mentioned in <a target="_blank" rel="noopener noreferrer" href="https://github.com/python/cpython/blob/450db61a78989c5a1f1106be01e071798c783cf9/Lib/urllib/parse.py#L30">other</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/PixarAnimationStudios/OpenUSD/blob/9b0c13b2efa6233c8a4a4af411833628c5435bde/pxr/base/vt/array.h#L959">codebases</a> as well.</p>
<p>TBH, this whole thing reminds me of how JavaScript's evolution over the years has been heavily shaped by
widespread reliance on all sorts of quirky, unintended behaviors, which became de facto standards. Now, I finally
know what to refer to this phenomenon as — <strong>Hyrum's Law.</strong></p>
<h3 id="final-thoughts"><a href="#final-thoughts"><span></span></a>Final thoughts</h3>
<ul>
<li>A good reminder to be careful when changing code others might depend on—and to try designing things in a way that doesn't accidentally
lock in weird quirks.</li>
<li>And even better, to design systems in a way that minimizes the chances of unintended behaviors being relied upon in the first place.</li>
<li>After all, you know the saying: "All it takes is one small change to... something something..." I don't remember the whole quote.</li>
</ul>
<p>😶‍🌫️️</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOJ filed paperwork to US District Court to force Google to spin off Chrome [pdf] (128 pts)]]></title>
            <link>https://storage.courtlistener.com/recap/gov.uscourts.dcd.223205/gov.uscourts.dcd.223205.1062.0.pdf</link>
            <guid>42201261</guid>
            <pubDate>Thu, 21 Nov 2024 04:56:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://storage.courtlistener.com/recap/gov.uscourts.dcd.223205/gov.uscourts.dcd.223205.1062.0.pdf">https://storage.courtlistener.com/recap/gov.uscourts.dcd.223205/gov.uscourts.dcd.223205.1062.0.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42201261">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Matrix: Infinite-Horizon World Generation with Real-Time Interaction (183 pts)]]></title>
            <link>https://thematrix1999.github.io/</link>
            <guid>42201117</guid>
            <pubDate>Thu, 21 Nov 2024 04:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thematrix1999.github.io/">https://thematrix1999.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=42201117">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <nav>
    
    
  </nav>

  <div id="video-container">
    
    <div>
        <h2>Welcome to <em>The Matrix</em></h2>
        <p>Infinite-Horizon World Generation with Real-Time Interaction</p>
        <p,><em><span>Ruili Feng*</span> , Han Zhang*, Zhantao Yang*, Jie Xiao*, Zhilei Shu*, Zhiheng Liu, Andy Zheng, Yukun Huang, <span>Yu Liu</span>, <span>Hongyang Zhang</span></em>
        <p,>* Equal Contribution, <span></span> Engineer Advisor, <span> </span> Project Leader
        <p,>Alibaba Group, University of Hong Kong, University of Waterloo, Vector Institute
        <blockquote,>
          "This is the world that you know; the world as it was at the end of the 20th century. It exists now only as part of a neural-interactive simulation that we call the Matrix."
          <cite>Morpheus to Neo (The Matrix, 1999)</cite>
        
        <p><a data-scrollto="startup" href="#"><img src="https://thematrix1999.github.io/img/scroll-icon.png" alt=""></a>
        </p>
      </blockquote,></p,></p,></p,></div>
    <video autoplay="" loop="" muted="">
      <source src="https://thematrix1999.github.io/videos/teaser.mp4" type="video/mp4">
    </video>
  </div>

  <div id="startup">
    <canvas></canvas>
    <div>
        <h2>The First Real-Time, Frame-Level Moving Control for Realistic World Simulation!</h2>
        <p><strong>How close are we to realizing the vision of <em>The Matrix (1999)</em>, where AI crafts a fully
            immersive, interactive world, blurring the line between reality and illusion?</strong> Imagine a limitless
          digital universe, created in real-time with visuals that rival reality itself. This project is a pioneering
          step toward that vision—a first glimpse into humanity’s own "Matrix."</p>
        <p> Our system breaks new ground in world simulation by delivering:</p>
        <ul>
          <li><strong>Frame-level precision</strong> in user interaction, matching the responsiveness portrayed in the
            film.</li>
          <li><strong>AAA-level visuals</strong> for immersive scenes nearly indistinguishable from reality.</li>
          <li><strong>Infinite generative capacity</strong> for endless exploration, surpassing the limits of current
            video models.</li>
        </ul>
        <p><strong>Curious?</strong> Read on to explore the technology powering <em>The Matrix</em> and experience the
          feel of a self-sustaining digital universe!</p>
      </div>
    <div>
      <h4>Comparison of Recent Generative Models for Game Simulation</h4>
      <p>
        <strong><em>The Matrix</em></strong> distinguishes itself as a foundation model capable of generating infinitely
        long videos with AAA game quality, high resolution, frame-level real-time control, and robust domain
        generalization. Here, * indicates concurrent work with <em>The Matrix</em>, and supervised/unsupervised refers
        to the video data with/without true control signal.
      </p>
      <div>
        <table>
          <thead>
            <tr>
              <th>Feature</th>
              <th>Genie</th>
              <th>DIAMOND</th>
              <th>MarioVGG*</th>
              <th>GameNGen*</th>
              <th>Oasis*</th>
              <th>GameGen-X*</th>
              <th><em>The Matrix</em></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Video Length</td>
              <td>2s</td>
              <td>Infinite</td>
              <td>6 Frames</td>
              <td>Infinite</td>
              <td>Infinite</td>
              <td>4s–16s</td>
              <td>Infinite</td>
            </tr>
            <tr>
              <td>Training Corpus</td>
              <td>2D Games (unsupervised)</td>
              <td>Atari, CS:GO</td>
              <td>Mario</td>
              <td>DOOM</td>
              <td>Minecraft</td>
              <td>AAA Games</td>
              <td>AAA Games (supervised, small)<br>Internet Videos (unsupervised, large)</td>
            </tr>
            <tr>
              <td>Resolution</td>
              <td>360p</td>
              <td>280 x 150</td>
              <td>64 x 48</td>
              <td>240p</td>
              <td>720p</td>
              <td>720p</td>
              <td>720p</td>
            </tr>
            <tr>
              <td>Control</td>
              <td>Frame-Level</td>
              <td>Frame-Level</td>
              <td>Video-Level</td>
              <td>Frame-Level</td>
              <td>Frame-Level</td>
              <td>Video-Level</td>
              <td>Frame-Level</td>
            </tr>
            <tr>
              <td>Real-Time</td>
              <td>No</td>
              <td>Yes</td>
              <td>No</td>
              <td>Yes</td>
              <td>Yes</td>
              <td>No</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>Control Generalization</td>
              <td>Yes</td>
              <td>No</td>
              <td>No</td>
              <td>No</td>
              <td>No</td>
              <td>No</td>
              <td>Yes</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    <div>
        <h2> <em>The Matrix</em> World</h2>
        <div>
          <p><strong><em>The Matrix</em></strong> offers real-time, responsive control in first- and third-person perspectives,
          enabling seamless exploration of dynamic environments. Trained on data from AAA games like <em>Forza Horizon
            5</em> and <em>Cyberpunk 2077</em> as well as real-world footage, it lets users navigate diverse
          terrains—deserts, cities, forests, and more—in unbroken, continuous videos. <strong>Each keyboard command
            responds with frame-level precision, delivering a four-frame response similar to AAA games.</strong> Explore
          the gallery below to experience <strong><em>The Matrix</em></strong> across immersive landscapes.</p><p>
          Click to play or <a onclick="playAll($('.tiling-video-wrapper')[0])">click here
            to play all demos</a>. Caution: may use mobile data.
        </p></div>
      </div>
  </div>

  <!-- <div class="intermediate-title-l2">
    <span>&nbsp;&nbsp;Driving in the Wild (<a
        onclick="playAll($('.tiling-video-wrapper')[0])">Play All</a> <span style="color:#dd3344;"></span>):</span>

  </div> -->

  <div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_0.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_0.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a lush green field, a white car is driving. In a
            panoramic aerial shot, the vehicle is adorned with
            red and blue stripes, and there is a black spoiler at the rear. The camera follows the movement of the car,
            with the surrounding environment consisting of spacious grassland, and in the distance, several houses and
            some trees can be seen. The sky is a clear blue without clouds, and the sun shines brightly, illuminating
            the entire scene.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_3.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_3.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a barren desert, a white SUV is driving. In a panoramic aerial shot, the vehicle's body is decorated
            with blue and red stripes, and there is a black spoiler at the rear. The camera follows as the car speeds
            across the sandy ground, kicking up a cloud of dust. The surrounding environment is a vast desert, with
            rolling mountains in the distance and the ocean beneath a blue sky dotted with white clouds.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_4.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_4.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a barren stretch of land, a white car is speeding by. As the panoramic aerial shot pans to the right,
            the vehicle is seen driving on the sandy ground, kicking up a cloud of dust. As the camera continues to move
            to the right, it reveals that the car has a black wing and the license plate reads "Alibaba0." In the
            distance is a vast ocean, its surface shimmering with light, and a few clouds drift in the sky.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_5.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_5.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a cornfield, a white car is driving. As the panoramic aerial shot pans to the left, the vehicle
            reveals black decorations on its body, a black wing at the rear, and the license plate reads "Alibaba0." It
            is traversing through the cornfield, surrounded by green corn stalks. In the distance, there are rolling
            mountains, the sky is a clear blue with a few white clouds, and there are also some buildings in the far
            distance.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_7.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_7.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a barren desert, a white SUV is driving. In a panoramic aerial shot, the vehicle's body features red
            and black decorations, and there is a black wing at the rear. It is traversing the rugged terrain,
            surrounded by vast sand dunes and sparse vegetation. In the distance, rolling mountains can be seen, and a
            few clouds are floating in the sky. The camera moves along with the vehicle as it travels.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_12.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_12.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a barren stretch of land, a white car is driving. In a panoramic aerial shot, the vehicle's body
            features blue and red stripes, and there is a black spoiler at the rear. The camera follows the car as it
            travels, with the surrounding environment consisting of dry land scattered with some bushes and trees. In
            the distance, mountains can be seen, with a few white clouds floating in the sky.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_13.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_13.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a vast body of water, a white car is driving. In a panoramic aerial shot, the license plate reads
            "Alibaba0," and the rear of the car features a black spoiler alongside some red lights on the body. As the
            camera follows, the vehicle moves through the water, splashing up droplets. In the distance, some mountains
            and buildings can be seen, with the sky filled with clouds.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_14.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_14.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a vast desert, a white SUV is driving. In a panoramic aerial shot, the vehicle is equipped with a
            large rear wing and a small spoiler, and its body features red and black decorations. It is speeding across
            the sandy terrain, kicking up a cloud of dust. The surrounding environment is barren land, with a few
            mountain peaks and some power poles visible in the distance. The sky is filled with thick clouds.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_15.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_15.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a lush green meadow, a white car is driving. In a panoramic aerial shot, the vehicle's body is
            decorated with black and red stripes, and there is a black spoiler at the rear. It is crossing a body of
            water, splashing up droplets. The surrounding environment consists of grass and trees, with some buildings
            visible in the distance.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_16.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_16.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a green meadow, a white car is driving. In a panoramic aerial shot, the vehicle's body features blue
            and red stripes, and there is a black wing at the rear. The camera follows the car as it moves across the
            grass, surrounded by lush greenery and trees. In the distance, some buildings and mountains are visible,
            with a blue sky and a few white clouds floating above.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_17.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_17.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> In
            a barren desert, a white SUV is driving. In a panoramic aerial shot, the vehicle's body is adorned
            with blue and red stripes, and there is a black spoiler at the rear. It is driving on the sandy terrain,
            kicking up a cloud of dust. The surrounding environment is a vast expanse of desert, with some mountains and
            power poles visible in the distance. A few clouds are floating in the sky, and the sun is shining brightly.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_28.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_28.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a lush green meadow, a white car is driving. In a panoramic aerial shot, the vehicle's body is adorned
            with blue and red stripes, and the license plate reads 'Alibaba0.' The camera follows the car as it moves
            across the grass, surrounded by dense trees and shrubs. In the distance, a bridge can be seen, with a clear
            river flowing beneath it, where the water is transparent and reveals the bottom. The sky is a bright blue,
            with no clouds.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_31.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_31.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> On
            a barren stretch of land, a white car is driving. In a panoramic aerial shot, the vehicle features a
            black roof and a wing at the rear, with the license plate reading 'Alibaba0.' It is traversing rugged
            terrain, surrounded by dry grassland and sparse vegetation. In the distance, some buildings and mountains
            can be seen, and the sky has a grayish-blue tone with a few clouds floating by.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_41.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_41.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> A
            white car is cruising along the highway. There’s nothing beside the road. The sky is clear, and the
            distant mountains are sharply outlined against the horizon.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_43.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_43.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> A
            white car is traveling along the highway. The roadside is empty of any features. The sky is bright and
            clear, and the distant mountains are sharply defined against the horizon.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_4s_img_44.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/horizon_4s_processed_44.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> A
            white car is driving through a small town, with some small buildings beside it. The sky is overcast
            with dark clouds. In an aerial view, the car features blue and red stripes on its body, and its license
            plate reads "Alibaba0."
          </p>
        </div>
      </div>

  </div>
  <div>
    <canvas></canvas>
    <div>
      <h3>First OpenSource Dataset for Per-Frame Precise Moving Control</h3>
      <div>
      <p><img src="https://thematrix1999.github.io/figs/gamedata.png" alt="Example PNG"></p><div>
        <p,>
          The <strong>GameData Platform</strong> leverages tools like <em>Cheat Engine</em> to capture in-game world
          status, filtering out unreliable data, and employs the <em>Reshade plugin</em> to remove game UI and HUD. This
          allows for the automated collection of massive, clean, and precise action-frame pairs. To further innovation,
          we will open source all the data, providing a valuable resource for future research in this domain.
        
        <h4>Key Advantages:</h4>
        <ul>
          <li><strong>Data Quality:</strong> Automated filtering ensures clean and precise action-frame pairs.</li>
          <li><strong>Scalability:</strong> Enables efficient collection of large-scale datasets.</li>
          <li><strong>Open Collaboration:</strong> Open-sourced data fosters research and innovation in the field.</li>
        </ul>

      </p,></div>
    </div>
    </div>
    <div>
      <p>
          Running at 16 FPS, <em>The Matrix</em> demonstrates strong generalization from virtual to real-world settings,
          where collecting sustained data is challenging. This approach showcases the potential
          of AAA game data for creating robust, adaptable world models (<a onclick="playAll($('.tiling-video-wrapper')[1])">Play All Demos</a> <span></span>):
        </p>
    </div>
  </div>


  <!-- <div class="intermediate-title-l2">
    <span>&nbsp;&nbsp;Walking around the City(<a
        onclick="playAll($('.tiling-video-wrapper')[1])">play all</a> <span style="color:#dd3344;"></span>):</span>

  </div> -->

  <div>
    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_0.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_0.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            scene showcases a modern urban environment, with towering skyscrapers and clean, tree-lined avenues
            devoid of traffic, suggesting a tranquil moment during the early morning or amidst the bustling city. Neon
            signs such as 'BUCK-A-SLICE' and 'BROOKLYN BARISTA' line the streets, indicating the presence of nearby
            restaurants and cafes. The tall, densely packed buildings create a visually striking yet somewhat enclosed
            urban canyon, blending stylish glass facades with sturdy concrete structures, embodying a combination of
            modern design and practical futuristic aesthetics. Elevated walkways and pedestrian bridges connect the
            buildings, adding depth to the cityscape. A prominent pedestrian bridge featuring a digital display serves
            as a focal point, enhancing the dynamic atmosphere. Fluorescent lights and illuminated signage emphasize the
            technological vibe, while the artificial light from street lamps contrasts with natural daylight. Small
            trees and flowering plants provide a touch of greenery amidst the concrete jungle. Futuristic billboards and
            digital advertisements are strategically placed on the buildings, injecting a sense of commercial vibrancy
            into the scene. This setting presents a technologically advanced city grappling with the challenges of dense
            urban living, capturing the contrast between progress and the busy realities of life.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_1.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_1.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            scene depicts an urban environment where a long, straight road stretches beneath an elevated highway
            or bridge, flanked by fences indicating construction or restricted access. The street is marked with two
            yellow lines, and massive concrete pillars support the roadway above, casting shadows below. On the left
            wall, red digital numbers are visible, possibly used for monitoring or alerts, accompanied by construction
            materials and barricades, signifying active development. On the right side, infrastructure and a neon blue
            'PAWN SHOP' sign indicate nearby commercial activity. Beyond the overpass, the road leads to tall modern
            buildings, their illuminated windows showcasing the vibrancy of the city landscape. Streetlights and digital
            displays provide limited lighting, adding to the futuristic feel. Despite signs of activity, the road is
            devoid of vehicles or pedestrians, contributing to a sense of silence. The portion of the sky outside the
            bridge contrasts with the shadows cast beneath it, while the surrounding construction and advanced
            architecture create an atmosphere of a city that is both evolving and futuristic.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_2.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_2.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The video shows a futuristic city centered on a wide concrete staircase between two modern buildings. Blue LED lights on the stairs add a high-tech feel. The building on the left has a large reflective glass panel that emits a cyan glow, and the continuous white LED strip on the right is neat and smooth. The view from the top of the stairs shows dense high-rise buildings, illuminated by blue and white lights, connected by overpasses, and a skyline filled with bright digital billboards. Smoke rises from the chimney of one building, the industrial atmosphere contrasts with the clear dusk, and the overall environment combines modern high-tech infrastructure and urban vitality to present a scene driven by technological progress and dynamic urban development.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_3.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_3.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            scene showcases a futuristic urban street adorned with colorful lights and screens on densely packed
            high-rise buildings. The wide asphalt road features yellow and white lane markings leading to a focal point
            marked '4th WALL,' hinting at a well-known location. On the left side, buildings display neon advertisements
            and light installations, creating a strong vertical visual. The facades are illuminated with both practical
            and decorative lighting, while empty billboards await new advertisements. The street is equipped with
            streetlights, traffic signals, and pedestrian barriers, and steam billows from a vent on the center-right,
            enhancing the industrial atmosphere. On the right side, palm trees contrast with the metal and concrete
            landscape of the city. Man-made elements like panels, vents, and machinery amplify the industrial feel,
            while the towering structures in the background showcase the high density of urbanization, presenting a
            sense of modern technology intertwined with a slightly dystopian ambiance.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_4.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_4.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> Set on futuristic city streets, skyscrapers blend modern architecture and neon to present a cyberpunk aesthetic. The main colors are blue, pink, and neon. A neon "BD Shack" sign, head pattern, reflective Windows and lights on the multi-storey building on the right add to the vibrant and heavy atmosphere. The curved-glass building on the left side is illuminated by pink neon lights, and the signage of commercial or entertainment venues next to it projects purple light. The street is empty, the driveway is clean and marked with yellow and white lines, and the wide sidewalk is dotted with street lights and a little greenery. The aerial corridors and infrastructure show advanced urban planning, the background high-rises glow with grids and panels, and the dim sky contrasts with the city lights, highlighting a modern and slightly dystopian feel.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_5.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_5.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video showcases a futuristic urban environment, with a wide street flanked by towering skyscrapers
            adorned with neon lights and digital billboards. The skyline appears tranquil in the morning or evening,
            contrasting with the bustling city area. On the left, a tall billboard features a close-up of a face. The
            buildings blend angles and flat surfaces, constructed from glass and steel, creating reflective facades.
            Elevated roads and pedestrian bridges suggest a multi-layered infrastructure. Palm trees in the center of
            the street soften the industrial feel. On the right, commercial establishments with neon signs and large
            glass windows add a vibrant touch. Inside the high-rises, there are offices, apartments, and more digital
            screens, with neon-framed windows highlighted prominently. In the distance, modern skyscrapers crowd the
            horizon, illuminated with advertising lights and reflective glass surfaces. Thin antennas showcase an
            advanced communication network. This urban landscape reflects a technologically advanced, commercially
            vibrant city, with the empty streets exuding a moment of peace.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_6.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_6.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> 
            The video shows a futuristic urban environment dominated by towering skyscrapers and advanced architectural design. The multi-lane road is worn and cracked, free of vehicles and people, and appears peaceful. The inactive billboard on the left is juxtaposed with dense high-rise buildings, with T-shaped structures and distinctive horizontal and vertical patterned buildings. Steam gushes from the vents, suggesting industrial activity. \n The glass curtain wall building on the right reads "Night City", and the neatly arranged plants add green. The steps suggest multi-level pedestrian activity. In the background, more high-rise buildings stretch to the horizon, including those marked "01" and "TECH". The skyline partially obscured by bright light, suggested as sunrise or sunset, adds dynamic contrast to the picture. The overall environment combines the busyness of a typical city with unique futuristic technological elements, reflecting advanced architecture and carefully planned urban style.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_7.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_7.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video presents an industrial-themed urban underpass scene, featuring a massive concrete elevated
            bridge supported by graffiti-covered pillars. There are sidewalks along the roadside, complemented by
            black-and-yellow construction barriers, with nearby construction materials such as barrels, boxes, and a
            toolbox labeled 'All Foods.' Red digital displays are positioned on the construction obstacles. The
            background showcases multi-layered infrastructure, elevated bridges, and buildings that serve both
            industrial and residential purposes. On the right side, modern architecture boasts large glass facades,
            walkways, and dimly lit complex signage, creating a commercial atmosphere that contrasts with the ruggedness
            of the surroundings. The distant skyline in gray-blue tones suggests a high density of urban development.
            Elevated pipes on the right illustrate the complexity of the city’s infrastructure. A building features a
            small amount of greenery, introducing a rare touch of nature. Overall, the scene depicts a multifaceted
            urban area brimming with construction activity, modern architecture, and industrial elements, embodying the
            raw energy of a continuously evolving city.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_8.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_8.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The scene takes place under the urban viaduct, the road extends, and there are cracks and patches in the middle of two faded yellow lines. Under the right viaduct, a red electronic sign "road closed" flashes along with obstacles, construction equipment, conical barrels and railings, indicating road construction. \n Luminous advertisements and Windows on the tall building on the left cast light and shadow. The large glass panels and streamlined lines of the modern building create a modern atmosphere. In the distance is a cascading skyline and tall buildings, framed by another viaduct. Warm skies (dawn or dusk) cast long shadows and orange glow. Palm trees planted between the buildings add to the industrial feel. The reflective glass facades of modern buildings and corporate buildings show a dynamic city in balance between infrastructure and natural elements.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_9.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_9.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video showcases a city intersection that blends modern and classical architecture, with tall beige
            skyscrapers, classical low-rise buildings, and sturdy red structures dominating the skyline. The wet streets
            reflect recent rainfall, while streetlights and billboards add a contemporary touch. Purple banners and
            advertisements enhance the commercial vibrancy. In the foreground, multiple lanes and parked white trucks
            highlight a bustling yet currently tranquil urban scene. The intricate design and varied building heights
            emphasize the cityscape's dynamic character and commercial atmosphere.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_10.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_10.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The video shows a futuristic urban environment with towering skyscrapers filled with bright billboards and light boxes. The wide streets are marked with yellow and black stripes, equipped with traffic lights and street lights, but free of traffic and pedestrians, showing a grand and quiet. Tall palm trees and bushes line the streets in contrast to the predominantly metal and concrete landscape. The architecture is modern and industrial, with large amounts of glass and steel, and the storefront has glowing signs and glass Windows that suggest commercial activity. The lower blocks have steam or smoke, adding to the industrial feel. The sky appears a hazy blue, possibly early in the morning or at dusk. Some aerial corridors connect the buildings, showing a complex, multi-layered urban infrastructure that facilitates pedestrian access above the ground. The overall scene depicts a high-tech, slightly dystopian urban environment.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_11.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_11.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video showcases an urban canyon formed by skyscrapers on a sunny day. From the street perspective,
            the lines of various modernist buildings and their reflective glass surfaces create a sense of verticality,
            with glass curtain walls on the left and stone or concrete structures on the right. Shadows enhance the
            impression of height. An old building with intricate stone carvings and arched windows on the right adds a
            historical element. Towering leafless trees line both sides of the street, creating a tranquil winter or
            early spring atmosphere. Although streetlights and traffic signals are visible, they are rendered
            unnecessary by the emptiness of the street. Colorful signage and banners add vibrancy, suggesting the city's
            regular activities. The clear blue sky contrasts sharply with the detailed urban environment, highlighting
            the grandeur of the architecture and showcasing the distant cityscape.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_12.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_12.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video showcases a bustling urban environment filled with towering skyscrapers and a dense cityscape
            adorned with billboards advertising products like 'Passion' and '2 Sweet Speed.' The clean, spacious streets
            feature trash, crosswalks, and puddles from recent rain. Palm trees and small patches of greenery line both
            sides of the street. An arched structure spans the road, serving as a visual focal point. Light filters in
            from the morning or evening, casting long shadows and imparting a soft blue hue to the scene. On the right
            side of the street, there is a commercial area labeled 'Masala.' Streetlights, traffic signals, small
            fences, and railings are neatly arranged, showcasing a blend of modern architectural design and vibrant
            advertisements, along with the integration of natural elements, reflecting a forward-thinking metropolis.
          </p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_13.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_13.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The video shows a quiet, almost desolate city street with disheveled pavement, cracks and patches, and a manhole cover in the center. The left sidewalk is slightly wet, with low curbs, vertical poles, and a few pigeons. Historic stone buildings and a multi-storey business district appear in the background, with storefronts and signboards adding a sense of history. The bench under the red umbrella indicates a social lounge area, which is probably more lively in normal times. The surrounding pavement also appears wet, enhancing the early morning or post-rain atmosphere. The distant street leads to an open space surrounded by the shadow of tall buildings, with green and red traffic lights and road signs that hint at the vast urban transport network. The overall picture shows a quiet city street, where architectural details and simple street furniture convey peace and quiet in an normally noisy environment.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_14.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_14.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            video showcases a futuristic urban environment in the early morning, with wide streets flanked by
            towering buildings and neon advertisements like 'Grill House,' 'Brooklyn Barista,' and 'Buck-A-Slice'
            glowing brightly. Elevated pedestrian walkways or transport systems run parallel to the street, presenting a
            city that is busy yet temporarily empty. The reflective surfaces of the buildings and electronic billboards
            emphasize a sense of technology and a cyberpunk aesthetic. The roads are clean, with clearly marked lane
            lines and pedestrian areas, while pedestrian barriers and streetlights are neatly arranged, enhancing crowd
            management. The multi-layered elevated pedestrian bridges indicate a high-density connectivity in the city.
            Large electronic billboards add vibrancy and dynamism to the scene. Despite the strong modernity, the
            absence of foot traffic adds an element of mystery.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/cyberpunk_img_15.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/cyberpunk_processed_15.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix:</span> The
            scene takes place in a futuristic urban environment, with brightly lit skyscrapers exuding a
            cyberpunk ambiance. At the center of the frame stands a tall building, adorned with neon lights and digital
            displays, prominently featuring a sign that reads 'The Fourth Wall,' likely indicating an entertainment or
            commercial venue. Artificial lighting and neon lights intertwine throughout the space. On the left, stairs
            lead up to an elevated walkway, surrounded by a few plants, with smooth reflective surfaces and grid windows
            on the building. On the right, a building's ventilation ducts release dense white steam, adding an
            industrial feel to the atmosphere. Small shops lining the street glow softly, each marked by small signs.
            The street comprises a mix of sidewalks and roadways, delineated by yellow lines and red markings, with
            bollards protecting pedestrians. Streetlights and traffic signals enhance the sense of order. In the
            background, the skyline is dark, with the tops of skyscrapers and scattered lights dotting the night sky,
            portraying a slightly dystopian yet bustling technological cityscape.</p>
        </div>
      </div>
  </div>
  



  <p><span>&nbsp;&nbsp;Generalize to Unseen Scene</span>
  </p>
  <div>
    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization-5_font.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization-5.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A car is driving indoors.
      </p>
    </div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization-6_font.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization-6.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The car is driving in an indoor corridor.
      </p>
    </div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization_img_7.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization_processed_7.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A vehicle is swimming in the sea.
      </p>
    </div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization_img_11.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization_processed_11.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A black car is driving on the path in the middle of the lake.
      </p>
    </div>
  </div>


  <p><span>&nbsp;&nbsp;Generalize Control to Real World Objects</span>
  </p>
  <div>
    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization_img_10.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization_processed_10.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A male character dressed in a formal suit is walking in the
        office.
      </p>
    </div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization_img_14.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization_processed_14.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The video features a close-up of a woman inside a car, wearing
        oversized sunglasses and dressed in black.
      </p>
    </div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/generalization_img_15.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/generalization_processed_15.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The video shows a little girl using a vacuum cleaner to move
        across the wooden floor at home.
      </p>
    </div>
  </div>

  <div>
    <canvas></canvas>
    <div>
      <p>
        <h2>Methodology</h2>
      </p>
    </div>
    <div>
      <h3>The Interactive Module</h3>
      <div>
      <p><img src="https://thematrix1999.github.io/figs/interactive.png" alt="Interactive Module"></p><div>
      <p,>
        The <strong>Interactive Module</strong> consists of an Embedding block and a cross-attention layer, translating keyboard inputs into natural language commands for video generation. For example, pressing <code>W</code> becomes “The car is driving forward” in <em>Forza Horizon 5</em> or “The man is moving forward and looking up” in <em>Cyberpunk 2077</em> when combined with upward mouse movement. For unlabeled data, a default description, “The camera is moving in an unknown way,” is applied.
        To enhance robustness, during training, we randomly replace labeled keyboard inputs with the default sentence with a probability q = 0.1.
        Before training, the base DiT model is warmed up using game and real-world data, fine-tuning a LoRA weight. This ensures the Interactive Module focuses on learning interactions and movement patterns, rather than simply fitting the video.
      
    </p,></div>
      </div>
    </div>
    <div>
      <h3>The Swin-Denoise Process Model</h3>
      <div>
      <p><img src="https://thematrix1999.github.io/figs/swindpm.png" alt="Swin-DPM"></p><div>
        <p,>
          Traditional DiT models generate only short videos due to high computational costs and memory demands of attention mechanisms over extended durations. To overcome this, we propose the <strong>Shift-Window Denoise Process Models</strong> (<strong>Swin-DPM</strong>), leveraging a sliding temporal window to handle dependencies effectively and enable long or infinite video generation.
          As shown in the <strong>Figure</strong>, the Swin-DPM processes video tokens in a queue using denoising steps. Tokens are cached after denoising, maintaining continuity between windows. This fine-tuned model builds on pre-trained DiT, where the first window of tokens is used for warmup, and loss is computed only on subsequent tokens.
          At inference, warmup tokens are discarded, and video generation begins from the (w+1)-th token, enabling efficient and continuous video generation.
        
    </p,></div>
      </div>
    </div>
    <div>
      <h3>Training Process</h3>
      <div>
      <p><img src="https://thematrix1999.github.io/figs/trainprocess.png" alt="Interactive Module"></p><div>
      <p,>
        The training process of <strong><em>The Matrix</em></strong> begins with a pretrained video DiT backbone. The <strong>Interactive Module</strong> is first warmed up with data from the <strong>GameData Platform</strong> using unsupervised LoRA to focus on movement rather than visuals. Subsequently, precise frame-level control is achieved through targeted training. With <strong>Swin-DPM</strong> enabling infinite-length generation and <strong>Stream Consistency Models (SCMs)</strong> ensuring real-time speeds, <strong><em>The Matrix</em></strong> delivers groundbreaking video simulations.
      
    </p,></div>
      </div>
    </div>
    <div>
      <h2>Generate Infinte-Horizon <em>The Matrix</em> World</h2>
        <p>Current state-of-the-art DiT-based video generation models (e.g., CogVideo, Open-Sora) are limited to
            producing videos just a few seconds long, making them insufficient for creating
            an infinite-horizon world. <strong><em>The Matrix</em></strong> overcomes this limitation by introducing
            Swin-DPM, which significantly extends the receptive
            field of attention computations while maintaining the same computational cost.
            This innovation enables the generation of high-quality, super-long-duration videos with consistent visuals,
            all within an achievable compute budget.</p>
        <h3>1 Min Gallery (1 min clip with 16 fps ~ 960 frames)</h3>
    </div>
  </div>



  <div>
    <div>
      <h2>Scene Desert</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_0.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_0.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>In a desolate desert, a white SUV is driving across the
        terrain. In an aerial shot, the vehicle is navigating through rugged landscapes, surrounded by parched
        vegetation and sparse trees. The camera follows the car's movement, capturing the tire tracks it leaves behind
        in the sand. In the distance, some buildings and mountains can be seen, while the sky is filled with clouds,
        with sunlight streaming through and casting rays of light.
      </p>
    </div>

    <div>
      <h2>Scene Desert</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_1.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_1.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>In a barren desert, a white SUV is making its way across the
        rugged terrain. Captured from an aerial perspective, the vehicle navigates through uneven ground, surrounded by
        dry vegetation and sparse trees. The camera tracks the car’s movement, showcasing the tire marks it leaves in
        the sand. In the background, distant buildings and mountains can be seen, while the sky is covered with clouds,
        allowing sunlight to filter through and illuminate the scene.
      </p>
    </div>

    <div>
      <h2>Scene Grassland</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_2.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_2.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>On a vast expanse of grassland, a white car is driving across
        the terrain. In an aerial shot, the vehicle is adorned with red and blue stripes on its body, and there is a
        black spoiler at the rear. The camera follows the car as it moves over the grass, surrounded by open fields,
        with a few trees and mountains visible in the distance. The sky is a deep blue, scattered with a few white
        clouds.
      </p>
    </div>

    <div>
      <h2>Scene Grassland</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_3.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_3.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>In a wide expanse of grassland, a white car is traversing the
        terrain. Captured from an aerial perspective, the car features red and blue stripes on its body and has a black
        spoiler at the back. The camera tracks the vehicle as it drives across the grass, with a backdrop of open fields
        and some trees and mountains in the distance. The sky is a clear blue, dotted with a few white clouds. The grass
        is neatly trimmed.
      </p>
    </div>
  </div>
  <div>
    <div>
      <h2>Scene Grassland</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_4.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_4.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is cruising across a wide stretch of grassland.
        Captured from an aerial view, the vehicle features red and blue stripes along its body and a black spoiler at
        the back. The camera tracks the car’s movement as it traverses the grassy terrain, surrounded by open fields,
        with some trees and mountains visible in the background. The sky is a vibrant blue with a few fluffy white
        clouds drifting by.
      </p>
    </div>

    <div>
      <h2>Scene Grassland</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_5.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_5.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is driving across a vast expanse of grassland. In
        an aerial shot, the vehicle is adorned with red and blue stripes and features a black spoiler at the rear. The
        camera follows the car as it moves across the grassy terrain, surrounded by open fields, with some trees and
        mountains visible in the distance. The sky is a bright blue, dotted with a few fluffy white clouds.
      </p>
    </div>

    <div>
      <h2>Scene Water</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_6.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_6.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is driving through a body of water, splashing
        water all around. In an aerial shot, the vehicle has blue and red stripes on its body, and its license plate
        number is 'Alibaba0.' The sky is pouring with heavy rain, and in the distance, some trees and mountains can be
        seen.
      </p>
    </div>

    <div>
      <h2>Scene Water</h2>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/horizon_1min_img_7.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/horizon_1min_processed_7.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is making its way through a body of water, sending
        up sprays of water as it goes. In an aerial view, the vehicle features blue and red stripes, with the license
        plate reading 'Alibaba0.' The sky is filled with heavy rainfall, and distant trees and mountains are visible in
        the background.
      </p>
    </div>

  </div>

 <p>
    <h3>Long Video Gallery (14 minutes videos &gt; 13440 frames)</h3>
    <h5>Videos are compressed for faster loading</h5>
</p> 


  <div>

    <div>
      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/super_long_img_0.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/resized_super_long_1.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is driving through desert.
      </p>
    </div>

    <div>

      <video preload="none" poster="https://thematrix1999.github.io/videos_icon/super_long_img_1.png" onclick="setControls(this)">
        <source src="https://thematrix1999.github.io/videos/resized_super_long_2.mp4" type="video/mp4">
      </video>
      <p onclick="clickToCopy(this)" ,=""><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>A white car is making its way across a beach.
      </p>
    </div>

  </div>

  <div>
    <canvas></canvas>
    <div>
      <h3> Generate Infinite Real World</h3>
      <p>
          The proposed Swin-DPM can be
      integrated into general DiT architecture diffusion models to enable extended-duration video
      generation. This innovation represents a significant contribution to the broader field of
      video generation, providing a pathway for creating high-quality, long-form videos
      The proposed Swin-DPM can be 
      integrated into general DiT architecture diffusion models to enable extended-duration video 
      generation. This innovation represents a significant contribution to the broader field of 
      video generation, providing a pathway for creating high-quality, long-form videos 
      that maintain coherence and visual consistency over time.
        </p>

  </div>
  </div>
  

  <div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/long_video_0000.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/long_video_0000.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The
            video shows a view of the city harbor from a high angle.
            In the center of the picture, a large yacht is moored at the pier,
            and other small boats are scattered around.
            The harbor is surrounded by an ancient stone wall with a bridge connecting the two banks.
            In the distance, there are dense urban buildings, some of which are decorated with domes on top.
            The whole scene is at dusk, and the sky is light blue and orange.</p>
        </div>
      </div>

    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/long_video_0001.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/long_video_0001.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The
            video shows a view of the city harbor from a high angle.
            In the center of the picture, a large yacht is moored at the pier,
            and other small boats are scattered around.
            The harbor is surrounded by an ancient stone wall with a bridge connecting the two banks.
            In the distance, there are dense urban buildings, some of which are decorated with domes on top.
            The whole scene is at dusk, and the sky is light blue and orange.</p>
        </div>
      </div>
    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/long_video_0002.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/long_video_0002.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The video shows a figure in a traditional cowboy costume riding a horse slowly along a snow-covered river. Surrounded by spectacular snowy mountains and trees, it creates a peaceful yet adventurous atmosphere. As the characters move, the flow of the river and details of the surrounding environment can be seen, such as footprints in the snow and mountains in the distance. The whole scene gives an immersive experience, as if you were in the natural landscape this winter.</p>
        </div>
      </div>
    <div>
        <video preload="none" poster="https://thematrix1999.github.io/videos_icon/long_video_0003.png" onclick="toggleVideoPlay(this)" loop="">
          <source src="https://thematrix1999.github.io/videos/long_video_0003.mp4" type="video/mp4">
        </video>
        <div>
          <p><img src="https://thematrix1999.github.io/img/thematrix.png"><span>admin@matrix: </span>The video shows a figure in a traditional cowboy costume riding a horse slowly along a snow-covered river. Surrounded by spectacular snowy mountains and trees, it creates a peaceful yet adventurous atmosphere. As the characters move, the flow of the river and details of the surrounding environment can be seen, such as footprints in the snow and mountains in the distance. The whole scene gives an immersive experience, as if you were in the natural landscape this winter.</p>
        </div>
      </div>

  </div>




  

  <!-- Modal button -->
  

  <!-- Modal -->
  

  

  
  
  

  
  
  










    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing overcharged the U.S. Air Force 8,000% above market for soap dispensers (151 pts)]]></title>
            <link>https://www.reuters.com/business/aerospace-defense/boeing-overcharged-air-force-nearly-8000-soap-dispensers-watchdog-alleges-2024-10-29/</link>
            <guid>42201068</guid>
            <pubDate>Thu, 21 Nov 2024 04:19:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/aerospace-defense/boeing-overcharged-air-force-nearly-8000-soap-dispensers-watchdog-alleges-2024-10-29/">https://www.reuters.com/business/aerospace-defense/boeing-overcharged-air-force-nearly-8000-soap-dispensers-watchdog-alleges-2024-10-29/</a>, See on <a href="https://news.ycombinator.com/item?id=42201068">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/aerospace-defense/boeing-overcharged-air-force-nearly-8000-soap-dispensers-watchdog-alleges-2024-10-29/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[The meme-ification of the "Demon Core" (250 pts)]]></title>
            <link>https://doomsdaymachines.net/p/the-meme-ification-of-the-demon-core</link>
            <guid>42200699</guid>
            <pubDate>Thu, 21 Nov 2024 03:05:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doomsdaymachines.net/p/the-meme-ification-of-the-demon-core">https://doomsdaymachines.net/p/the-meme-ification-of-the-demon-core</a>, See on <a href="https://news.ycombinator.com/item?id=42200699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>On May 21, 1946, the Canadian physicist Louis Slotin was demonstrating to several other Los Alamos scientists how to do a criticality experiment. Slotin wasn’t really </span><em>doing</em><span> an experiment at the time, in the sense of taking careful scientific measurements — he was simply showing how one would do them, because he was about to leave the laboratory to be part of the assembly team for the first postwar nuclear weapons test at the Bikini atoll, and they were going to be taking over the experimental work for him while he was gone. In front of him was a beryllium hemisphere (a neutron reflector), with a plutonium weapon core inside of it. In his left hand he held another beryllium hemisphere, with his thumb reaching into a hole in the top, and using a screwdriver as leverage, he carefully lowered the top hemisphere over the core. As he did so, the neutrons exiting the core began to reflect back into it, increasing the overall reactivity of the system. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg" width="1456" height="1102" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1102,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2062148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb14cdf3f-092d-475e-9b2c-870296b18c2d_3000x2270.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>A recreation of the Slotin accident, in the same room, at the same table, even possibly the same screwdriver. Another plutonium core, in two hemispheres, is also visible on the table.</em></figcaption></figure></div><p>And then… the screwdriver slipped. The top hemisphere slid over the plutonium, and it tipped just slightly over the edge of prompt criticality, creating a brief but intense nuclear reaction that showered Slotin and those around him with radiation and created a brief flash of blue light in the air around them. In less than a second, Slotin had reflexively knocked the top hemisphere off, and the reaction had stopped, but it was too late: he was walking dead, having absorbed enough radiation to kill him horribly and painfully within nine days.</p><p><span>The story of the so-called “Demon Core,” which was involved in two criticality accidents —&nbsp;the one that killed Slotin, and another, different accident that killed another scientist, Harry Daghlian, a year before —&nbsp;is part of the lore of the atomic age. A </span><a href="https://youtu.be/08IY70zXAkk?feature=shared&amp;t=5108" rel="">fictionalized version of the accidents</a><span> was featured in the very first Hollywood film about the Manhattan Project (MGM’s </span><em><a href="https://www.newyorker.com/tech/annals-of-technology/demon-core-the-strange-death-of-louis-slotin" rel="">The Beginning or the End?</a></em><span>, from 1947), as well as in the </span><a href="https://www.youtube.com/watch?v=AQ0P7R9CfCY" rel="">1989 film </a><em><a href="https://www.youtube.com/watch?v=AQ0P7R9CfCY" rel="">Fat Man and Little Boy</a></em><span>. And over the years there have been many non-fiction accounts of the accident, with ever more details emerging over time; if you do want more details on the accident, the core, and the aftermath, you might take a look at </span><a href="https://www.newyorker.com/tech/annals-of-technology/demon-core-the-strange-death-of-louis-slotin" rel="">an article I published on it a few years back</a><span>, for example.</span></p><p>What really has fascinated me about the Slotin accident in particular, though, is the emergence of an entire ecosystem of “memes” about it. The first one I can remember seeing in this one, in 2019:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg" width="1174" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1174,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:196200,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00f0e13-fc7f-4835-90e3-b3f0f4f567a9_1174x820.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>You could, at the time, only buy it on a T-shirt from someone on Etsy based out of Japan, but now there are a million knock-off/duplicates available on the Internet. I don’t know who the original author was, but at the time I was pretty amazed at it — it felt like something of a “deep cut,” because you had to recognize the entire Slotin set-up in order to see why this somewhat </span><em><a href="https://en.wikipedia.org/wiki/Kawaii" rel="">kawaii</a></em><span> rendering of the Slotin experiment, along with the “I love science” phrasing, was a form of dark humor. </span></p><p><span>I don’t remember seeing “Demon Core” memes prior to the above. I am sure one can find earlier ones — that was just the first one that came across my computer screen. Google Trends has </span><a href="https://trends.google.com/trends/explore?date=2010-10-20%202024-11-20&amp;geo=US&amp;q=Demon%20Core,Louis%20Slotin,NUKEMAP&amp;hl=en" rel="">some interesting results</a><span> on this:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg" width="1456" height="368" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:368,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:295971,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47048b6b-f7d2-438d-aaab-b609f7720097_1976x500.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>The above is showing the relative search volume of three terms: “Louis Slotin” (red, always very low), “Demon Core” (blue, interesting spikes, and a growing relevancy from 2019 onward), and “</span><a href="https://nuclearsecrecy.com/nukemap/" rel="">NUKEMAP</a><span>” (yellow, included as something of a “control” —&nbsp;something I consider “somewhat well-known in certain circles” but not “universally well-known”; about 1/4th of my engineering-school freshmen this year had heard of the NUKEMAP before taking any courses with me). My reading of this is that the “Demon Core” was still pretty obscure among non-nuke-nerds until around 2019/2020, when it “broke through” some barrier and is now something that can readily referenced online and you would expect a significant fraction of the readership (esp. geeky boys, I imagine) to recognize.</span></p><p><span>Since 2019 or so there has been an, er, </span><em>explosion</em><span> of “Demon Core”/Slotin experiment memes. The veritable source Know-Your-Meme created a dedicated </span><a href="https://knowyourmeme.com/memes/demon-core" rel="">“Demon Core” page</a><span> in May 2021, showcasing some of the variety of the meme. Most of the initial ones seem to be of the same sort of </span><em>kawaii</em><span> (Japanese for “cute”) model as the one above, transporting it into a context of anime girls, cats, and anime cat girls:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg" width="1456" height="1747" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1747,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1682272,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F538c4903-d36d-428d-a5d5-45ae6cc12519_3000x3600.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png" width="680" height="780" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:780,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:346809,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cac6e5-e3ed-48e9-943a-067ae6a7f456_680x780.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg" width="1456" height="1820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1471789,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d6fe1d-ed07-4e16-a663-da4985e2e4ea_2000x2500.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg" width="500" height="589" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:589,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:52960,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ece075-7f4b-4459-9b14-783e2bc34f67_500x589.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp" width="1292" height="1700" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1700,&quot;width&quot;:1292,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:135870,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e2917a8-f7df-4650-b703-d493c59df50a_1292x1700.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif" width="384" height="384" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:384,&quot;width&quot;:384,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1399681,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d77087d-559c-49a5-b57d-bf2ee0abeca5_384x384.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em>What</em><span> is going on here? I am not exceptionally well-versed in anime or manga tropes, but I think the “obvious” reading of this is a classic case of “unexpected juxtaposition creates humor.” That is, moving something from one context (“Demon Core,” radiation experiment, horrible death) into another (cute, anime, girls) creates something that feels novel and unusual. One could no doubt analyze many different dimensions of juxtaposition at work here, including the gender roles: Slotin was not just male, but the entire accident was caused by the sort of risk-taking bravado (which had he been explicitly warned about) that anyone who has spent time as or around young men recognizes immediately. So having a cute Japanese schoolgirl performing this very male-coded experiment is quite a switch in tone — and one that probably also is dually resonant with the “geeky boy” demographic that I suspect is largely the primary “receiver” and “transmitter” of the meme.</span></p><p><span>The Slotin accident has also achieved, in recent years, that “sweet spot” of “if you know, you know” virality: it’s not </span><em>so</em><span> well-known that everyone would get it, but it’s also not </span><em>so </em><span>obscure that only dedicated wonks or experts would know about it (a reference to the </span><a href="https://en.wikipedia.org/wiki/Cecil_Kelley_criticality_accident" rel="">Kelley accident</a><span>, or the </span><a href="https://en.wikipedia.org/wiki/SL-1" rel="">SL-1 accident</a><span>, would be probably too deep a cut to go viral). </span></p><p>I have not made a full survey or taxonomy of these memes; there are dozens and dozens, if not more. The ones above I would categorize as “entirely new drawings,” in which the Demon Core pictures are used as reference, but the entire style is novel. There also cases where the Demon Core pictures (or self-serious drawings/renderings of the experiment) are added to existing meme formats, such as the “pondering my orb” wizard:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg" width="680" height="460" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:460,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:65965,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc010c3a2-0f8e-4904-98a0-dd2d8e308784_680x460.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There is also quite a few of the “take a stock photograph of a product and add the Demon Core to it” variety:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg" width="680" height="653" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:653,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32106,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd258ec6-a32e-4d35-ac6d-c8b03c8a8dfe_680x653.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>And so on —&nbsp;there are an almost unlimited number. There are arguably no </span><em>tasteful</em><span> versions of this meme, of course. (</span><a href="https://www.xkcd.com/2593/" rel="">XKCD, perhaps, comes closest</a><span>.) Because it’s a meme derived from human suffering. It’s </span><em>meant</em><span> to be in bad taste&nbsp;— that’s the </span><em>source</em><span> of the humor. </span></p><p><span>The medical details of the “Demon Core” accidents are pretty horrible. The </span><em>photographs</em><span> of the victims are not something you probably want to see&nbsp;— they are bad. I have </span><a href="https://blog.nuclearsecrecy.com/2016/05/23/the-blue-flash/" rel="">described them elsewhere</a><span>. Suffice to say these were extremely painful deaths, involving a body whose internal cellular processes were breaking down in real time under the influence of billions of tiny cellular cuts, and all the while the poor scientists were being monitored, photographed, and (ultimately) inventoried. Slotin’s hubris and bravado caused his death, but it didn’t mean he </span><em>deserved</em><span> that suffering, and it doesn’t mean we can’t feel sorry for him, one human to another. I am not trying to be a scold. We just have to remember that we are not making fun of an abstract idea, here. There was real suffering involved.</span></p><p><span>And… </span><em>yet</em><span>. What is interesting to me about many of the initial memes, especially the anime ones, is that they appear to have originated from </span><em>Japan</em><span>. And that adds a wrinkle to the sentiment here. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg" width="680" height="987" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:987,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123407,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc22d5d49-8445-4e5e-8bcd-f65b1c4c9980_680x987.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Godzilla + the “Demon Core” = an interesting fusion of nuclear metaphors…</em></figcaption></figure></div><p><span>The “Demon Core” was, as people who know the full story are well-aware, was actually the third plutonium core fabricated, after the one detonated at Trinity and the one detonated over Nagasaki, and would have almost surely been used over a Japanese city had World War II continued for a few more weeks. The plutonium core before it </span><a href="https://thebulletin.org/2020/08/counting-the-dead-at-hiroshima-and-nagasaki/" rel="">killed some 40,000–70,000 people</a><span>, mostly Japanese civilians but also Korean laborers and even some Allied Prisoners of War, at Nagasaki. The “Demon Core” killed two American weapons scientists, and at least in Slotin’s case was entirely preventable and due to a lack of proper procedure and respect for the hazards involved. </span></p><p><span>So perhaps if anybody has a “right” to make jokes in poor taste about the “Demon Core”… it might be the Japanese? Because Americans have made jokes about the atomic bombings of Hiroshima and Nagasaki since they occurred. One can’t just attribute it to wartime sentiment. Well after the suffering of the Japanese victims was public knowledge, and after the Japanese became a key American ally, there were still </span><a href="https://www.youtube.com/watch?v=ztFHvNwRb6Q" rel="">novelty songs</a><span> about the killing of cities. Even just a few weeks ago, </span><em><a href="https://www.youtube.com/shorts/l9I7xLUxR9o" rel="">Saturday Night Live</a></em><a href="https://www.youtube.com/shorts/l9I7xLUxR9o" rel=""> featured a joke in </a><em><a href="https://www.youtube.com/shorts/l9I7xLUxR9o" rel="">particularly</a></em><a href="https://www.youtube.com/shorts/l9I7xLUxR9o" rel=""> bad taste</a><span> about the Nobel Peace Prize being awarded to atomic bomb survivors. I’m not here to be the humor police, or to say things should be “off limits” for comedy, or that it’s “too soon,” or make any other scolding noises. Dark humor, in its own strange and inverted way, is arguably a sort of coping mechanism&nbsp;—&nbsp;a defense against the darkness, a way to tame and de-fang the horrors of the world. The bomb is no stranger to such treatment, of course&nbsp;—&nbsp;consider </span><em>Dr. Strangelove</em><span>, or Tom Lehrer — although my sense is that the “Demon Core” memes are not, in any serious way, making conscious “interventions” in how people think about the risks of the nuclear world.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone is capable of, and can benefit from, mathematical thinking (528 pts)]]></title>
            <link>https://www.quantamagazine.org/mathematical-thinking-isnt-what-you-think-it-is-20241118/</link>
            <guid>42200209</guid>
            <pubDate>Thu, 21 Nov 2024 01:45:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/mathematical-thinking-isnt-what-you-think-it-is-20241118/">https://www.quantamagazine.org/mathematical-thinking-isnt-what-you-think-it-is-20241118/</a>, See on <a href="https://news.ycombinator.com/item?id=42200209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        <p>
            The mathematician David Bessis claims that everyone is capable of, and can benefit greatly from, mathematical thinking.        </p>
        
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-scaled.webp" alt="Bessis sitting at an outdoor café in Paris." decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/QADavidBessis-crLaurenceGeai-Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>David Bessis was drawn to mathematics for the same reason that many people are driven away: He didn’t understand how it worked. Unlike other creative processes, like making music, which can be heard, or painting pictures, which can be seen, math is for the most part an internal process, hidden from view. “It sounded a bit magical. I was intrigued,” he said.</p>
<p>His curiosity eventually led him to pursue a doctoral degree in math at Paris Diderot University in the late 1990s. He spent the next decade studying geometric group theory before leaving research mathematics and founding a machine learning startup in 2010.</p>
<p>Through it all, he never stopped questioning what it actually means to do math. Bessis wasn’t content to simply solve problems. He wanted to further interrogate — and help other people understand — how mathematicians think about and practice their craft.</p>

<p>In 2022, he published his answer — a book titled <em>Mathematica: A Secret World of Intuition and Curiosity</em>, which he hopes will “explain what’s going on inside the brain of someone who’s doing math,” he said. But more than that, he added, “this is a book about the inner experience of humans.” It was translated from the original French into English earlier this year.</p>
<p>In <em>Mathematica</em>, Bessis makes the provocative claim that whether you realize it or not, you’re constantly doing math — and that you’re capable of expanding your mathematical abilities far beyond what you think possible. Eminent mathematicians like Bill Thurston and Alexander Grothendieck didn’t owe their mathematical prowess to intrinsic genius, Bessis argues. Rather, they became such powerful mathematicians because they were willing to constantly question and refine their intuitions. They developed new ideas and then used logic and language to test and improve them.</p>
<p>According to Bessis, however, the way math is taught in school emphasizes the logic-based part of this process, when the more important element is intuition. Math should be thought of as a dialogue between the two: between reason and instinct, between language and abstraction. It’s also a physical practice of sorts, like yoga or martial arts — something that can be improved through training. It requires tapping into a childlike state and embracing one’s imagination, including the mistakes that come with it.</p>
</div>
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1456" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-scaled.webp" alt="Bessis standing on the balcony of his Paris apartment." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-1720x978.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-520x296.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-768x437.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-1536x874.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Balcony-crLaurenceGeai-2048x1165.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>“The mathematician’s message is for everyone: Look at what you can do if you don’t give up on your intuition,” Bessis said.</p>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Everyone, Bessis says, has some experience with this process, meaning that everyone has practice thinking like a mathematician. Moreover, everyone can, and should, try to improve their mathematical thinking — not necessarily to solve math problems, but as a general self-help technique.</p>
<p><em>Quanta </em>spoke with Bessis about what mathematicians are really doing when they’re “doing math,” and what it means for math to be a form of self-help. The interview has been condensed and edited for clarity.</p>
<h3><strong>Why did you write this book?</strong></h3>
<p>For maybe 20 years, I tried and failed to write it many, many times. Then in 2020, I reread texts that I had read a long time ago — accounts by the mathematicians René Descartes, Alexander Grothendieck and William Thurston. I realized that these three people, who were not connected to one another, were all telling the same story in different ways. That story resonated with what I had been living. After that realization, the book completely crystallized in a matter of weeks.</p>
<p>What we can learn from Descartes, Grothendieck and Thurston is that the nature of the mathematical process is not what people think.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="2240" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-scaled.webp" alt="Four photos of Bessis playing with Legos, blocks and other toys." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-1720x1505.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-520x455.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-768x672.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-1536x1344.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis-Grid-2048x1792.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>At its core, Bessis says, mathematics is a game of back-and-forth between intuition and logic, instinct and reason.</p>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>What do people think?</strong></h3>
<p>That math is a bunch of cryptic symbols on a piece of paper. This is what has been advertised as mathematics for millennia, because that’s what people see. But the whole journey — how you work with things in your mind, how math transforms you — is all secret. It’s not part of the curriculum.</p>
<p>Math is a sort of dialogue between these external and internal processes. It is the activity of aligning your inner representation — your intuition — with some logical, external representation. You use a very rigid, crazy-looking formalism to test your intuition, to recalibrate, grow and reinforce it until you feel that you have a good understanding of something.</p>
<h3><strong>You claim that everyone is secretly doing math, even if they don’t realize it.</strong></h3>
<p>You have to force them to become aware of their own mathematical training. Can you see a circle in your mind? Can you make it bigger, smaller? Can you move it around? That’s weird, you know.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1416" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-scaled.webp" alt="Bessis sitting in front of the Centre Pompidou in Paris." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-1720x951.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-520x288.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-768x425.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-1536x850.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Crowd-crLaurenceGeai-2048x1133.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>“High school students are often unhappy with math, because they think it requires some innate things that they don’t have,” Bessis said. “But that’s not true; really it relies on the same type of intuition we use every day.”</p>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Or say I ask, “What’s a billion minus one?” I’ve never met anyone who doesn’t immediately have the answer in their head. I have to think about it when I want to say it out loud, but I see it in my mind. And I think most people are like that. Even if they don’t see it — it’s not a visual perception — they have a strong feeling of the outcome.</p>
<p>This is really mathematical intuition. It’s obvious to people. But it’s also historical: 2,000 years ago, nobody would have been able to easily answer this question, because they were using Roman numerals.</p>
<p>It proves that what you view as very easy math is actually deep math that you’ve internalized. You learned a very abstract numbering system that transformed your intuition of numbers. It makes you find things completely obvious that 2,000 years ago would have made you look like Einstein. It’s not hardwired. You wired it yourself.</p>
<h3><strong>Do you believe in the notion of a mathematical genius, then?</strong></h3>
<p>It would be dishonest to deny that there are people who are incredibly good at math. There are 5-year-olds who are already genius mathematicians. You can see it: It looks like they’re communicating with aliens from outer space.</p>
<p>But I do not think this is innate, even though it often manifests in early childhood. Genius is not an essence. It’s a state. It’s a state that you build by doing a certain job.</p>
<p>Math is a journey. It’s about plasticity. I am not saying that math is easy. Math is hard. But life, whatever you do, is extremely hard.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1859" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-scaled.webp" alt="Bessis standing in the hallway of his Paris apartment." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-1720x1249.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-520x378.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-768x558.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-1536x1116.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Hallway-crLaurenceGeai-1-1-2048x1488.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Bessis is inspired by the practices of mathematical greats like Bill Thurston, Alexander Grothendieck and René Descartes.</p>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>If it’s not innate, but rather a process, does that mean that with practice I could eventually do math like Thurston?</strong></h3>
<p>No, not Thurston. I don’t think so. But maybe you’ll surprise me. Thurston gave a detailed account of how he decided at a young age to practice this kind of self-education every day. I don’t think you can catch up with that. Grothendieck and Descartes probably started very young, before they were even speaking.</p>
<p>The point I want to make is that high school students are often unhappy with math, because they think it requires some innate things that they don’t have. But that’s not true; really it relies on the same type of intuition we use every day.</p>
<h3><strong>So how can one get better at math?</strong></h3>
<p>Whenever you spot a disconnect between what your gut is telling you and what is supposed to be rational, it’s an important opportunity to understand something new. And then you can start this game of back-and-forth. Can you articulate your gut instinct and place it within a rational discussion? If there’s still a disconnect, can you visualize why? As you play that game, your imagination will gradually reconfigure. And in the end, if you’re persistent, your instinct and your reason will align, and you will be smarter. This is mathematical thinking.</p>
<h3><strong>What can someone gain by improving their mathematical thinking?</strong></h3>
<p>Joy, clarity and self-confidence.</p>
<p>Children do this all the time. That’s why they learn so fast. They have to. Otherwise, I mean, nothing makes sense. I think this is also why babies are super happy — because they have epiphanies all day long. It’s wonderful.</p>
</div>
    <figure>
        <div>
                            <p><img width="1940" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-scaled.webp" alt="Close-up of Bessis." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-scaled.webp 1940w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-1303x1720.webp 1303w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-394x520.webp 394w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-768x1013.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-1164x1536.webp 1164w, https://www.quantamagazine.org/wp-content/uploads/2024/11/DavidBessis_Portrait-crLaurenceGeai-1552x2048.webp 1552w" sizes="(max-width: 1940px) 100vw, 1940px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>“When you do math, you’re exposed to the human thought process in a way that is really pure,” Bessis said. “It’s not just about understanding things, but about understanding things in a very childish, deep, naïve, super clear, obvious way.”</p>
            <p>Laurence Geai for <em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>For adults, this way of thinking can be very slow. But if you don’t give up, what you can do with your intuition is way beyond your wildest expectations. And this is universal. My book is a life lesson for all creative people, not just those who want to learn mathematical concepts. The mathematician’s message is for everyone: Look at what you can do if you don’t give up on your intuition.</p>
<h3><strong>Do you view this as a self-help book?</strong></h3>
<p>I’m actually more extreme than just saying this is a self-help book. I’m saying that, in a way, mathematics itself is a self-help technique.</p>
<h3><strong>How so?</strong></h3>
<p>Mathematicians have to be radically honest about what they don’t understand and what they think. That might help them see, for example, that an object is defined the wrong way. Or that a different definition will make a theory simpler. Or that this is not the important notion, that one is.</p>
        
        
<p>There’s something about honesty, about being radically candid, about trying to find ways to articulate what you really feel — it’s super hard. It takes practice.</p>
<p>When you do math, you’re exposed to the human thought process in a way that is really pure. It’s not just about understanding things, but about understanding things in a very childish, deep, naïve, super clear, obvious way. It’s very good training for creativity. It’s a scaffold for your imagination.</p>
<p>When I look at my own life, and the way I managed to overcome personal challenges, I realize that my ability to think in a mathematical way helped. From an emotional standpoint, we all need math.</p>
</div>
                
                
            </div><div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="729" src="https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1-1720x729.webp" alt="Invisible ink being written and revealed" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1-1720x729.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1-520x220.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1-768x326.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1-1536x651.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/11/PublicKeyCryptoExplainer-crKristinaArmitage-HP-2018x868-1.webp 2048w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>How Public Key Cryptography Really Works, Using Only Simple Math</p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We open-sourced our compost monitoring tech (197 pts)]]></title>
            <link>https://github.com/gtls64/MontyHome-Hackers-Guide</link>
            <guid>42200099</guid>
            <pubDate>Thu, 21 Nov 2024 01:30:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gtls64/MontyHome-Hackers-Guide">https://github.com/gtls64/MontyHome-Hackers-Guide</a>, See on <a href="https://news.ycombinator.com/item?id=42200099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Monty Home Device Hacking Guide</h2><a id="user-content-monty-home-device-hacking-guide" aria-label="Permalink: Monty Home Device Hacking Guide" href="#monty-home-device-hacking-guide"></a></p>
<p dir="auto">Welcome to the <strong>Monty Home Device Hacking Guide</strong> repository! This guide provides step-by-step instructions for extending the functionality of the Monty Home BLE device using a Raspberry Pi. Originally designed for compost monitoring, the Monty Home device collects valuable data on temperature, humidity, and other environmental metrics. Through this guide, you’ll learn how to retrieve, display, and automate actions based on this data.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Where to Buy Monty Home</h2><a id="user-content-where-to-buy-monty-home" aria-label="Permalink: Where to Buy Monty Home" href="#where-to-buy-monty-home"></a></p>
<p dir="auto"><a href="https://montycompost.co/" rel="nofollow">https://montycompost.co/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#projects">Projects</a>
<ul dir="auto">
<li><a href="#project-1-temperature-based-led-control">Project 1: Temperature-Based LED Control</a></li>
<li><a href="#project-2-display-temperature-and-humidity-on-i2c-display">Project 2: Display Temperature and Humidity on I2C Display</a></li>
<li><a href="#project-3-temperature-alert-via-ifttt">Project 3: Temperature Alert via IFTTT</a></li>
</ul>
</li>
<li><a href="#setup">Setup</a>
<ul dir="auto">
<li><a href="#hardware-requirements">Hardware Requirements</a></li>
<li><a href="#software-requirements">Software Requirements</a></li>
</ul>
</li>
<li><a href="#ble-commands">BLE Commands</a></li>
<li><a href="#running-the-code">Running the Code</a></li>
<li><a href="#customization">Customization</a></li>
<li><a href="#additional-resources">Additional Resources</a></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">This guide is designed for anyone interested in working with Bluetooth Low Energy (BLE) devices, IoT applications, or environmental monitoring. The Monty Home device communicates over BLE, providing real-time data on temperature, humidity, battery level, and more. In this repository, you’ll find three projects that use Python, BLE, and Raspberry Pi to interact with the Monty Home device.</p>
<p dir="auto">Each project covers different functionalities:</p>
<ol dir="auto">
<li><strong>Basic LED Control Based on Temperature Thresholds</strong></li>
<li><strong>Displaying Data on an OLED Screen Using I2C</strong></li>
<li><strong>Sending Notifications via IFTTT When Conditions Are Met</strong></li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Projects</h2><a id="user-content-projects" aria-label="Permalink: Projects" href="#projects"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project 1: Temperature-Based LED Control</h3><a id="user-content-project-1-temperature-based-led-control" aria-label="Permalink: Project 1: Temperature-Based LED Control" href="#project-1-temperature-based-led-control"></a></p>
<p dir="auto"><strong>Objective</strong>: Use the temperature data from the Monty Home device to control an LED on the Raspberry Pi. If the temperature exceeds a specified threshold, the LED lights up to indicate a warning.</p>
<p dir="auto"><strong>Skills Gained</strong>:</p>
<ul dir="auto">
<li>Setting up GPIO control for an LED.</li>
<li>Querying BLE data.</li>
<li>Basic Python programming and condition handling.</li>
</ul>
<p dir="auto"><strong>Hardware Needed</strong>:</p>
<ul dir="auto">
<li>Raspberry Pi with BLE support</li>
<li>LED and 330-ohm resistor</li>
</ul>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project 2: Display Temperature and Humidity on I2C Display</h3><a id="user-content-project-2-display-temperature-and-humidity-on-i2c-display" aria-label="Permalink: Project 2: Display Temperature and Humidity on I2C Display" href="#project-2-display-temperature-and-humidity-on-i2c-display"></a></p>
<p dir="auto"><strong>Objective</strong>: Display real-time temperature and humidity data from the Monty Home device on an OLED screen connected to the Raspberry Pi.</p>
<p dir="auto"><strong>Skills Gained</strong>:</p>
<ul dir="auto">
<li>Working with I2C devices.</li>
<li>Displaying dynamic data using the SSD1306 OLED display.</li>
<li>Implementing BLE data retrieval and display updates.</li>
</ul>
<p dir="auto"><strong>Hardware Needed</strong>:</p>
<ul dir="auto">
<li>Raspberry Pi with BLE support</li>
<li>SSD1306 OLED Display (128x32 or 128x64)</li>
</ul>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project 3: Temperature Alert via IFTTT</h3><a id="user-content-project-3-temperature-alert-via-ifttt" aria-label="Permalink: Project 3: Temperature Alert via IFTTT" href="#project-3-temperature-alert-via-ifttt"></a></p>
<p dir="auto"><strong>Objective</strong>: Configure the Raspberry Pi to send a notification via IFTTT if the temperature from the Monty Home device exceeds a specific threshold.</p>
<p dir="auto"><strong>Skills Gained</strong>:</p>
<ul dir="auto">
<li>Integrating with IFTTT for IoT automation.</li>
<li>Sending HTTP requests with the <code>requests</code> library.</li>
<li>Combining BLE data with cloud-based notifications.</li>
</ul>
<p dir="auto"><strong>Hardware Needed</strong>:</p>
<ul dir="auto">
<li>Raspberry Pi with Wi-Fi</li>
<li>IFTTT account</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirements</h3><a id="user-content-hardware-requirements" aria-label="Permalink: Hardware Requirements" href="#hardware-requirements"></a></p>
<ol dir="auto">
<li><strong>Raspberry Pi</strong> (Zero 2 or another model with BLE support).</li>
<li><strong>Monty Home BLE Device</strong>.</li>
<li>Additional hardware specific to each project, such as an LED, OLED display, and IFTTT account.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software Requirements</h3><a id="user-content-software-requirements" aria-label="Permalink: Software Requirements" href="#software-requirements"></a></p>
<ol dir="auto">
<li><strong>Raspberry Pi OS</strong>: Install Raspberry Pi OS Lite (for headless) or Raspberry Pi OS with Desktop (for graphical interface).</li>
<li><strong>Python 3</strong>: Make sure Python 3 and <code>pip</code> are installed.</li>
<li><strong>Libraries</strong>:
<ul dir="auto">
<li><strong>Bleak</strong> for BLE communication: <code>pip install bleak</code></li>
<li><strong>Requests</strong> for IFTTT integration: <code>pip install requests</code></li>
<li><strong>Adafruit CircuitPython SSD1306</strong> for OLED control: <code>pip install adafruit-circuitpython-ssd1306</code></li>
<li><strong>Pillow</strong> for image manipulation on OLED: <code>pip install pillow</code></li>
</ul>
</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">BLE Commands</h2><a id="user-content-ble-commands" aria-label="Permalink: BLE Commands" href="#ble-commands"></a></p>
<p dir="auto">Use these commands to interact with the Monty Home device. Each command requests specific data or performs an action. You can replace or modify commands in the Python scripts as needed.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>;QA\r\n</code></td>
<td>Returns the index of all data in flash memory.</td>
</tr>
<tr>
<td><code>;QP\r\n</code></td>
<td>Returns index of pending data in flash memory.</td>
</tr>
<tr>
<td><code>;QR\r\n</code></td>
<td>Returns a record by index, NACK if index not found.</td>
</tr>
<tr>
<td><code>;QS\r\n</code></td>
<td>Returns the status of the device.</td>
</tr>
<tr>
<td><code>;QL\r\n</code></td>
<td>Returns the battery level as a percentage.</td>
</tr>
<tr>
<td><code>;QT\r\n</code></td>
<td>Returns the temperature reading from the NTC sensor.</td>
</tr>
<tr>
<td><code>;QH\r\n</code></td>
<td>Returns the relative humidity reading.</td>
</tr>
<tr>
<td><code>;QO\r\n</code></td>
<td>Returns the most recent TVOC reading.</td>
</tr>
<tr>
<td><code>;QC\r\n</code></td>
<td>Returns the most recent CO2 reading.</td>
</tr>
<tr>
<td><code>;QU\r\n</code></td>
<td>Returns the unique ID of the device.</td>
</tr>
<tr>
<td><code>;QV\r\n</code></td>
<td>Returns the firmware version of the device.</td>
</tr>
<tr>
<td><code>;CR\r\n</code></td>
<td>Reboots the device.</td>
</tr>
<tr>
<td><code>;CF\r\n</code></td>
<td>Performs a factory reset.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the Code</h2><a id="user-content-running-the-code" aria-label="Permalink: Running the Code" href="#running-the-code"></a></p>
<p dir="auto">Each project contains a Python script that establishes a BLE connection, sends queries, and processes data. To run a script:</p>
<ol dir="auto">
<li>Open a terminal on the Raspberry Pi.</li>
<li>Navigate to the project folder:

</li>
<li>Run the script:
<div dir="auto" data-snippet-clipboard-copy-content="python3 project_script.py"><pre>python3 project_script.py</pre></div>
Replace <code>project_script.py</code> with the actual script file name, such as <code>project1_temperature_led.py</code>.</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customization</h2><a id="user-content-customization" aria-label="Permalink: Customization" href="#customization"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adjusting BLE Commands</h3><a id="user-content-adjusting-ble-commands" aria-label="Permalink: Adjusting BLE Commands" href="#adjusting-ble-commands"></a></p>
<p dir="auto">You can modify the BLE commands in the code to retrieve different types of data from the Monty Home device. For example, to query humidity instead of temperature, replace:</p>

<p dir="auto">with:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Expanding Notification Handlers</h3><a id="user-content-expanding-notification-handlers" aria-label="Permalink: Expanding Notification Handlers" href="#expanding-notification-handlers"></a></p>
<p dir="auto">To process multiple types of data (e.g., temperature, humidity), add conditions within the <code>notification_handler</code> function to decode and display different readings.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integrating with Other Platforms</h3><a id="user-content-integrating-with-other-platforms" aria-label="Permalink: Integrating with Other Platforms" href="#integrating-with-other-platforms"></a></p>
<p dir="auto">Consider integrating data into IoT platforms or dashboards for real-time data visualization, logging, or further automation.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Resources</h2><a id="user-content-additional-resources" aria-label="Permalink: Additional Resources" href="#additional-resources"></a></p>
<ul dir="auto">
<li><a href="https://realpython.com/python-raspberry-pi/" rel="nofollow">Python on Raspberry Pi</a></li>
<li><a href="https://www.instructables.com/Control-Bluetooth-LE-Devices-From-A-Raspberry-Pi/" rel="nofollow">BLE on Raspberry Pi Guide</a></li>
<li><a href="https://ifttt.com/maker_webhooks" rel="nofollow">IFTTT Webhooks Documentation</a></li>
<li><a href="https://learn.adafruit.com/monochrome-oled-breakouts" rel="nofollow">Adafruit CircuitPython SSD1306 Guide</a></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Feel free to submit pull requests, report issues, or suggest features. Any contributions to improve this guide and add new projects are welcome!</p>
<hr>
<p dir="auto">This README provides everything needed for users to get started with BLE communication, project setup, and code customization. Let me know if you need further details or additional sections!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building Databases over a Weekend (105 pts)]]></title>
            <link>https://www.denormalized.io/blog/building-databases</link>
            <guid>42200014</guid>
            <pubDate>Thu, 21 Nov 2024 01:16:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.denormalized.io/blog/building-databases">https://www.denormalized.io/blog/building-databases</a>, See on <a href="https://news.ycombinator.com/item?id=42200014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Databases are some of the most complex pieces of software conceived
since the advent of the computing age over half a century ago. [1]
Nearly every piece of technology ends up touching the database in some
way shape or form. Despite the ubiquity of Databases in the software
stack, the vast majority of developers have been conditioned to treat
databases as more or less a <em>black box</em> -- complex dense pieces of
software conjured up by arch wizards and mavens ensconced in the
rarified elite tiers of database companies or places like Google. The
adage for the rest of us as it goes is <em>never attempt to write your own
database</em>.</p>
<p>That said, despite their longevity, we have seen continued innovation in
the space that first started with Hadoop arriving on the scene about 2
decades ago. The ClickBench website now lists over 50+ databases in its
benchmark suite [2] . And that's just the analytics engines. With the
recent trends of rewriting everything big data in Rust [3] not a month
goes by without an interesting new project trending in the Hacker News.
In this post we will take a look at how easy (or hard) it is to build
Databases using Apache Datafusion and whether you <em>can in fact, as a
mere mortal</em> realistically build a custom database and innovate around
the developer experience.</p>
<p>Most modern databases can be disaggregated into compute and storage
layers, with sophisticated query engines responsible for the "compute"
portion of the database. A query engine typically consists of a Query
Parser, Logical Plan generation and then the Physical Plan generation to
run the computations on the Execution Engine. A query generally goes
through multiple optimization phases in the logical plan generation as
well as physical plan generation. No matter what the target use-case of
the end system is, the query engine more or less follows this model.</p>
<p>Given decades of database research that has gone into each of these
individual layers, the bar for writing a functional query engine with
table stakes features remains strikingly high. And you need to nail <em>all
that</em> before you can get around to writing your use-case specific
features. While there are many projects that help you write some of
these layers individually, Apache DataFusion remains the only game in
town that helps you with the entire gamut.</p>
<p>You can think of DataFusion extensible database development toolkit. At
its most basic level, you can use it as a query engine a la DuckDB with
its builtin SQL and Dataframe front ends, while at the same time you can
extend or even wholesale replace different layers to build your own
experience entirely.</p>
<p>In the rest of this post we will walk through how to extend DataFusion
to add your own operators to its execution engine and then weave it
through the Physical and Logical planners and expose it to the frontend.</p>
<h2>Building a top DataFusion</h2>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/0*jjrNhgIlapa-Ih2P.png" alt="DataFusion architecture"></p>
<p><strong>DataFusion architecture</strong></p>
<p>At Denormalized, we are building a duck db like single node experience
for stream processing applications. While DataFusion does have some
support for unbounded computations, it doesn't have a streaming window
operator. Windows are at the core of stream processing applications,
they provide an easy way to bucket infinite data streams into finite
buckets so that we can apply aggregations over them.</p>
<p>For this tutorial we will implement a simple window operator for
infinite streams. Our operator will have the following signature --</p>
<pre><code>    pub fn window(
			    self
			    group_expr: Vec&lt;Expr&gt;,
			    aggr_expr: Vec&lt;Expr&gt;,
			    window_length: Duration,
			    slide: Option&lt;Duration&gt;,
			    ) -&gt; Result&lt;Self&gt; { ... }
</code></pre>
<h2>Writing the Execution Plan</h2>
<p>An <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/execution_plan/trait.ExecutionPlan.html">ExecutionPlan represents</a> a node in the DataFusion Physical plan. This is where
the actual code with our custom computations would go. DataFusions execution model
is pull based, meaning that the execution starts at the sinks and works its
way up the physical plan. Calling the <em>execute</em> method on this trait
produces an asynchronous <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/type.SendableRecordBatchStream.html">SendableRecordBatchStream</a> of record batches
by incrementally deriving a partition of the output by running
computations over Execution Plan's input.</p>
<p>In our use case, <a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/physical_plan/continuous/streaming_window.rs#L423-L427">ExecutionPlan's execute()</a> method returns a struct
<a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/physical_plan/continuous/grouped_window_agg_stream.rs#L62">GroupedWindowAggStream</a> which implements a RecordBatchStream,  a wrapper around
futures::Stream trait. The actual computations should be implemented
within the <em>poll_next()</em> of the Stream implementation.</p>
<pre><code>    impl RecordBatchStream for GroupedWindowAggStream {
	    fn schema(&amp;self) -&gt; SchemaRef {
		    self.schema.clone()
	    }
    }
    
    impl Stream for GroupedWindowAggStream {
    
	    type Item = Result&lt;RecordBatch&gt;;
	    
	    fn poll_next(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt;
	    Poll&lt;Option&lt;Self::Item&gt;&gt; {
	    
		    let poll = self.poll_next_inner(cx); // Code to compute the record batch goes
		    here.
	    
	    ...
    
    }
</code></pre>
<p>Here in our example, calling the <em>poll_next_inner</em> for a streaming
window takes care of</p>
<ol>
<li>
<p>Processing the incoming data.</p>
</li>
<li>
<p>Accumulating rows into open windows.</p>
</li>
<li>
<p>Updating the watermark (which is behind a mutex)</p>
</li>
<li>
<p>Opening new windows if required.</p>
</li>
<li>
<p>Closing any triggering windows and producing the output
RecordBatches from them.</p>
</li>
</ol>
<h2>Hooking into the Physical Planner</h2>
<p>Having created our custom Execution Plan, we need to make the Physical
Planner aware of its existence. <a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/planner/streaming_window.rs#L69-L173">Implementing the ExtensionPlanner</a> for our
ExtensionPlan is all we need to do here.</p>
<h2>Extending the Logical Plan</h2>
<p>Now that we have the custom Execution Plan implemented along with
ExtensionPlanner, we need to add a companion node to the Logical Plan.
This not only allows us to expose this to the SQL/DataFrame frontends
but also hook into the logical optimizers for optimizations such as the
predicate pushdowns.</p>
<p>In DataFusion we need to first implement a <a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/logical_plan/streaming_window.rs#L28">user defined Logical Plan
Node</a> and then add a <a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/logical_plan/mod.rs#L47-L59">LogicalPlanExtension</a>
to the logical plan builder which exposes this to SQL/DataFrame
frontends.</p>
<h2>Logical Plan to Physical Plan</h2>
<p>The last piece of the puzzle is the touch point where the logical plan
gets converted into the physical plan. For this, we will implement a
<a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/query_planner.rs#L13-L30">custom QueryPlanner</a> that ensures that the physical planner is initialized with
the custom extensions we wrote for our ExecutionPlan.</p>
<h2>Custom Optimization Rules</h2>
<p>Since our operator implemented a group by aggregation, we need to ensure
that all the rows for a particular group end up in the same partition.
To that end we will add a <a href="https://github.com/probably-nothing-labs/denormalized/blob/f65268972da3a6f21eac0ab4bf3e4ca9b558c6ed/crates/core/src/physical_optimizer/coalesce_before_streaming_window_aggregate.rs#L32">new physical optimization
rule</a> to add a HashPartition operator on our group keys.</p>
<h2>Putting it all together</h2>
<p>Finally, all we need to do is construct a DataFusion session with the
custom QueryPlanner we wrote above as well as the additional physical
optimizer rule we added and voila, now we have our own extended version
of DataFusion.</p>
<pre><code>    let state = SessionStateBuilder::new()
      .with_default_features()
      .with_config(config)
      .with_query_planner(Arc::new(StreamingQueryPlanner {}))
      .with_optimizer_rules(get_default_optimizer_rules()).with_physical_optimizer_rule(Arc::new(EnsureHashPartititionOnGroupByForStreamingAggregates::new(),
      ))
      .build();
</code></pre>
<h3>Footnotes</h3>
<ol>
<li><a href="https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf">https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf</a></li>
<li><a href="https://benchmark.clickhouse.com/">https://benchmark.clickhouse.com</a></li>
<li><a href="https://github.com/rewrite-bigdata-in-rust/RBIR">https://github.com/rewrite-bigdata-in-rust/RBIR</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3626246.3653368">https://dl.acm.org/doi/10.1145/3626246.3653368</a></li>
</ol></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>