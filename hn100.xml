<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 09 Jan 2026 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Kagi releases alpha version of Orion for Linux (142 pts)]]></title>
            <link>https://help.kagi.com/orion/misc/linux-status.html</link>
            <guid>46553343</guid>
            <pubDate>Fri, 09 Jan 2026 12:54:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.kagi.com/orion/misc/linux-status.html">https://help.kagi.com/orion/misc/linux-status.html</a>, See on <a href="https://news.ycombinator.com/item?id=46553343">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-39a288b8=""><!--[--><!--]--><main data-v-39a288b8=""><div data-v-39a288b8=""><p>The alpha stage is an early, unstable version meant primarily for testing.</p><h2 id="what-is-ready-to-test" tabindex="-1">What is ready to test <a href="#what-is-ready-to-test" aria-label="Permalink to &quot;What is ready to test&quot;">​</a></h2><p>All visual components, including:</p><ul><li>Main menus, submenus, dialogs, buttons, and toolbars.</li><li>Right-click menus and other visual controls.</li><li>Window layouts and basic controls.</li><li>Demonstrated basic website navigation functionality, supporting essentials like the homepage, tabs, and simple searches</li><li>Advanced tab management is now complete, with the exception of the Tab Switcher UI, which is not supported yet.</li><li>Tabs now function independently and can be opened in parallel</li><li>Session persistence is implemented: previously opened tabs, along with their history, will reopen when the application is launched again.</li><li>Tabs currently appear in the main window and are supported in the left sidebar as well.</li><li>Bookmarks system a simple bookmark feature is now available.</li><li>Users can save pages, organize them into folders</li><li>Users can view them in the bookmarks dialog, sidebar, and bookmarks bar.</li><li>Bookmarking via the ✴︎ icon.</li><li>Intuitive folder assignment when saving a new bookmark.</li><li>Advanced history management provides handling of browsing history</li><li>Password management framework establishes the core infrastructure needed for secure password handling and future improvements in this area.</li><li>Local export/import (via file)</li><li>Managing passwords</li></ul><h2 id="future-improvements-not-implemented-in-alpha" tabindex="-1">Future improvements (not implemented in Alpha): <a href="#future-improvements-not-implemented-in-alpha" aria-label="Permalink to &quot;Future improvements (not implemented in Alpha):&quot;">​</a></h2><ul><li>WebKit Extension support</li><li>Sync infrastructure</li></ul></div></main><!--[--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MCP is a fad (120 pts)]]></title>
            <link>https://tombedor.dev/mcp-is-a-fad/</link>
            <guid>46552254</guid>
            <pubDate>Fri, 09 Jan 2026 10:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tombedor.dev/mcp-is-a-fad/">https://tombedor.dev/mcp-is-a-fad/</a>, See on <a href="https://news.ycombinator.com/item?id=46552254">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><h2 id="overview">Overview<a href="#overview" aria-label="Direct link to Overview" title="Direct link to Overview" translate="no">​</a></h2>
<p>MCP has taken off as the standardized platform for AI integrations, and it's difficult to justify <em>not</em> supporting it. However, this popularity will be short-lived.</p>
<p>Some of this popularity stems from misconceptions about what MCP uniquely accomplishes, but the majority is due to the fact that it's <em>very easy</em> to add an MCP server. For a brief period, it seemed like adding an MCP server was a nice avenue for getting attention to your project, which is why so many projects have added support.</p>
<h2 id="what-is-mcp">What is MCP?<a href="#what-is-mcp" aria-label="Direct link to What is MCP?" title="Direct link to What is MCP?" translate="no">​</a></h2>
<p>MCP claims to solve the "NxM problem": with N agents and M toolsets, users would otherwise need many bespoke connectors.</p>
<h3 id="the-nxm-problem">The NxM problem<a href="#the-nxm-problem" aria-label="Direct link to The NxM problem" title="Direct link to The NxM problem" translate="no">​</a></h3>
<p>A common misconception is that MCP is <em>required</em> for function calling. It's not. With tool-calling models, a list of available tools is provided to the LLM with each request. If the LLM wants to call a tool, it returns JSON-formatted parameters:</p>
<p><img decoding="async" loading="lazy" alt="function_calling_no_mcp" src="https://tombedor.dev/assets/images/function_calling_no_mcp-3f3ed851f1398f52c8cb7d71d853261f.png" width="3499" height="749"></p>
<p>The application is responsible for providing tool schemas, parsing parameters, and executing calls. The problem arises when users want to reuse toolsets across different agents, since each has slightly different APIs.</p>
<p>For example, tools are exposed to <a href="https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#rest_2" target="_blank" rel="noopener noreferrer">Gemini's API</a> via <code>functionDeclarations</code> nested inside a <code>tools</code> array:</p>
<div><pre tabindex="0"><code><span><span>curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \</span><br></span><span><span>  -d '{</span><br></span><span><span>    "contents": [...],</span><br></span><span><span>    "tools": [</span><br></span><span><span>      {</span><br></span><span><span>        "functionDeclarations": [</span><br></span><span><span>          {</span><br></span><span><span>            "name": "set_meeting",</span><br></span><span><span>            "description": "...",</span><br></span><span><span>...</span><br></span></code></pre></div>
<p>In <a href="https://platform.openai.com/docs/guides/text?lang=curl" target="_blank" rel="noopener noreferrer">OpenAI's API</a>, tool schemas use a flat <code>tools</code> array with <code>type: "function"</code>:</p>
<div><pre tabindex="0"><code><span><span>curl -X POST https://api.openai.com/v1/responses \</span><br></span><span><span>  -d '{</span><br></span><span><span>    "model": "gpt-4o",</span><br></span><span><span>    "input": [...],</span><br></span><span><span>    "tools": [</span><br></span><span><span>      {</span><br></span><span><span>        "type": "function",</span><br></span><span><span>        "name": "get_weather",</span><br></span><span><span>...</span><br></span></code></pre></div>
<p>This is the "NxM" problem. In theory, users must build N × M connectors. In practice, the differences are minor (same semantics, slightly different JSON shape), and frameworks like <a href="https://python.langchain.com/docs/how_to/function_calling/" target="_blank" rel="noopener noreferrer">LangChain</a>, <a href="https://docs.litellm.ai/docs/completion/function_call" target="_blank" rel="noopener noreferrer">LiteLLM</a>, and <a href="https://huggingface.co/learn/cookbook/en/agents" target="_blank" rel="noopener noreferrer">SmolAgents</a> already abstract them away. Crucially, these options <em>execute tool calls in the same runtime as the agent</em>.</p>
<h3 id="how-mcp-addresses-it">How MCP addresses it<a href="#how-mcp-addresses-it" aria-label="Direct link to How MCP addresses it" title="Direct link to How MCP addresses it" translate="no">​</a></h3>
<p>MCP handles exposing and invoking tools via separate processes:</p>
<p><img decoding="async" loading="lazy" alt="function_calling_mcp" src="https://tombedor.dev/assets/images/function_calling_mcp-7ddac7e9d3439168d21fdd812a16c8b6.png" width="3633" height="1163"></p>
<p>A JSON configuration controls which MCP servers to start. Each server runs in its own long-lived process, handling tool invocations independently. The application still orchestrates the agent loop and presents results to users.</p>
<p>This abstracts away schema generation and invocation, but at a cost. Tool logic runs in a separate process, making resource management opaque. The application loses control over tool instructions, logging, and error handling. And every tool call crosses a process boundary.</p>
<h3 id="scope-tools-dominate">Scope: tools dominate<a href="#scope-tools-dominate" aria-label="Direct link to Scope: tools dominate" title="Direct link to Scope: tools dominate" translate="no">​</a></h3>
<p>MCP also defines primitives for prompts and resources, but adoption of these is much smaller than tools<sup><a href="#user-content-fn-1-e0bb36" id="user-content-fnref-1-e0bb36" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>:</p>
<p><img decoding="async" loading="lazy" alt="code_references" src="https://tombedor.dev/assets/images/code_references-4bea4400fa382dec1d99d50df013aa6b.png" width="919" height="908"></p>
<p>Given this, the rest of this post focuses on tool calling, which is MCP's primary use case in practice.</p>
<h2 id="problems">Problems<a href="#problems" aria-label="Direct link to Problems" title="Direct link to Problems" translate="no">​</a></h2>
<p>The convenience of MCP comes with a price, stemming from two architectural attributes of an MCP-driven application:</p>
<p><img decoding="async" loading="lazy" alt="issues" src="https://tombedor.dev/assets/images/issues-aacc03230cf0cf8fc4fc94f2ba3c5876.png" width="1851" height="971"></p>
<p>Since tools are drawn from arbitrary sources, they are not aware of what other tools are available to the agent. Their instructions can't account for the rest of the toolbox.</p>
<p>The second issue stems from different toolsets having their own runtimes. This introduces a variety of problems I'll discuss below.</p>
<h3 id="incoherent-toolbox">Incoherent toolbox<a href="#incoherent-toolbox" aria-label="Direct link to Incoherent toolbox" title="Direct link to Incoherent toolbox" translate="no">​</a></h3>
<p><a href="https://www.microsoft.com/en-us/research/video/tool-space-interference-an-emerging-problem-for-llm-agents/" target="_blank" rel="noopener noreferrer">Agents tend to be less effective at tool use as the number of tools grows</a>. With a well-organized, coherent toolset, agents do well. With a larger, disorganized toolset, they struggle. <a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener noreferrer">OpenAI recommends keeping tools well below 20</a>, yet many MCP servers exceed this threshold.</p>
<p>Why does this happen? Consider a workflow in which an agent should send a notification after doing work:</p>
<p><img decoding="async" loading="lazy" alt="confusion" src="https://tombedor.dev/assets/images/confusion-00fa7e3d04a2eae3855e414829b16e58.png" width="2005" height="1001"></p>
<p>A tool's fit for a task depends not just on the job at hand, but also on what else is in the toolbox. Pliers can pull a nail, but if a hammer is available it's probably the better choice. When tools ship in isolation, their instructions can't say "use me only when you don't have a hammer," so agents don't get cohesive guidance.</p>
<p>If the toolset is controlled by the same authors as the application, they can add prompting to the toolsets to disambiguate when to use which tool. If not, the problem must be solved by system prompts or user guidance.</p>
<p>Looking through #mcp channels of open source coding agents, you'll invariably find users who struggle to get the agent to use the tools in the way they want<sup><a href="#user-content-fn-2-e0bb36" id="user-content-fnref-2-e0bb36" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>:</p>
<p><img decoding="async" loading="lazy" alt="trouble" src="https://tombedor.dev/assets/images/trouble-e4382a8d6ae0564086624162693e61b6.png" width="3314" height="328"></p>
<p>Or, users complaining of how many tokens are burned by tool instructions:</p>
<p><img decoding="async" loading="lazy" alt="inefficient" src="https://tombedor.dev/assets/images/inefficient-1abf3d1cad9bdcf5648d9677f6f8c6e1.png" width="2400" height="232"></p>
<h3 id="arbitrary-separate-runtimes">Arbitrary, separate runtimes<a href="#arbitrary-separate-runtimes" aria-label="Direct link to Arbitrary, separate runtimes" title="Direct link to Arbitrary, separate runtimes" translate="no">​</a></h3>
<p>Each MCP server <a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle" target="_blank" rel="noopener noreferrer">starts a separate process</a> that survives for the length of the agent session.</p>
<p>Even in the healthy state, this introduces a collection of processes that remain mostly idle, aside from serving occasional requests from an agent. In an error state, we get all the usual headaches: dangling subprocesses, memory leaks, resource contention.</p>
<p>Users have these issues, if they are able to get the servers running at all: in support channels, the most common complaint is difficulty getting the servers to run:</p>
<p><img decoding="async" loading="lazy" alt="connection_problems" src="https://tombedor.dev/assets/images/connection_problem-7c5e6ede95ca4d7790b0caa5dd27d976.png" width="3196" height="668"></p>
<p>MCP offers no way for servers to declare their runtime/dependency needs. Some authors work around it by baking installation into the launch command (e.g., <code>uv run some_tool mcp</code>), which only succeeds if the user already has the right tooling installed.</p>
<p>Even if the relevant package is there, the MCP server might not start it successfully. MCP servers only inherit <a href="https://modelcontextprotocol.io/legacy/tools/debugging#environment-variables" target="_blank" rel="noopener noreferrer">a subset of parent ENV variables</a> (<code>USER</code>, <code>HOME</code>, and <code>PATH</code>). This is particularly problematic for <code>nvm</code> or users leveraging virtual environments.</p>
<p>Python or Node developers might be comfortable debugging environment issues, (although MCP's subprocess orchestration makes this more difficult), but are likely less comfortable debugging Node issues <em>and</em> Python <em>and</em> other runtimes. MCP seems to assert that I as the user should not really care which of these are used, or how many.</p>
<p>Even if toolsets are in one given runtime, MCP potentially spins up many instances of it, obviating efficiencies from caching, connection pooling, and shared in-memory state. MCP's HTTP transport mode doesn't help; it's just another HTTP API, but with MCP's protocol overhead instead of battle-tested REST/OpenAPI patterns.</p>
<h3 id="security">Security<a href="#security" aria-label="Direct link to Security" title="Direct link to Security" translate="no">​</a></h3>
<p>MCP pushes users to install servers from npm, pip, or GitHub. This inherits the usual supply-chain risk, but without even the minimal guardrails those ecosystems provide. There's no central publisher or signing; anyone can ship a daemon that runs on your machine and MCP offers no provenance check.</p>
<p>MCP's specification <a href="https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/mcp-security-network-exposed-servers-are-backdoors-to-your-private-data" target="_blank" rel="noopener noreferrer">doesn't mandate authentication</a>, leaving security decisions to individual server authors. The result: <a href="https://www.darkreading.com/vulnerabilities-threats/2000-mcp-servers-security" target="_blank" rel="noopener noreferrer">one scan found 492 MCP servers</a> running without any client authentication or traffic encryption. Even Anthropic's own Filesystem MCP Server had a sandbox escape via directory traversal (<a href="https://strobes.co/blog/mcp-model-context-protocol-and-its-critical-vulnerabilities/" target="_blank" rel="noopener noreferrer">CVE-2025-53110</a>).</p>

<table><thead><tr><th>Issue</th><th>CVSS / Impact</th></tr></thead><tbody><tr><td><strong><a href="https://jfrog.com/blog/2025-6514-critical-mcp-remote-rce-vulnerability/" target="_blank" rel="noopener noreferrer">CVE-2025-6514</a></strong></td><td>9.6 (RCE in mcp-remote; 437,000+ downloads)</td></tr><tr><td><strong><a href="https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html" target="_blank" rel="noopener noreferrer">CVE-2025-49596</a></strong></td><td>9.4 (RCE in Anthropic's MCP Inspector)</td></tr><tr><td><strong><a href="https://www.imperva.com/blog/another-critical-rce-discovered-in-a-popular-mcp-server/" target="_blank" rel="noopener noreferrer">CVE-2025-53967</a></strong></td><td>RCE in Figma MCP Server; 600,000+ downloads</td></tr><tr><td><strong><a href="https://www.bleepingcomputer.com/news/security/asana-warns-mcp-ai-feature-exposed-customer-data-to-other-orgs/" target="_blank" rel="noopener noreferrer">Asana data exposure</a></strong></td><td>Tenant isolation flaw exposed ~1,000 customers' data</td></tr></tbody></table>
<p>Unlike a human carefully clicking through an API, agents can be manipulated via prompt injection to call tools in unintended ways. The <a href="https://www.generalanalysis.com/blog/supabase-mcp-blog" target="_blank" rel="noopener noreferrer">Supabase MCP leak</a> demonstrated this "lethal trifecta": prompt injection → tool call → data exfiltration, extracting entire SQL databases including OAuth tokens. Again, this risk isn't unique to MCP. But the best mitigations are existing security infrastructure: scoped OAuth tokens, service identities with minimal permissions, and audit logging. MCP sidesteps this infrastructure rather than building on it.</p>
<p>A common defense is that MCP isolates credentials—the agent talks to a socket, never seeing your API tokens. But this threat model is narrow: an agent that can invoke <code>mcp.github.delete_repo()</code> doesn't need your token to cause damage. You're not eliminating trust; you're redirecting it to third-party code that, as the CVEs demonstrate, is often unaudited and vulnerable.</p>
<h3 id="the-cost-benefit-doesnt-add-up">The cost-benefit doesn't add up<a href="#the-cost-benefit-doesnt-add-up" aria-label="Direct link to The cost-benefit doesn't add up" title="Direct link to The cost-benefit doesn't add up" translate="no">​</a></h3>
<p>These problems could be worth the cost, if we were to gain significantly. But comparing tool calling with MCP to tool calling without it, MCP handles remarkably little. MCP is, more or less, handling serializing function call schemas and responses.</p>
<p>The tools developers are saving themselves from having to write are, overwhelmingly, <a href="https://mcp.alphavantage.co/?utm_source=mcp.so&amp;utm_medium=referral&amp;utm_campaign=202508&amp;utm_id=000001&amp;utm_term=web_project&amp;utm_content=v2" target="_blank" rel="noopener noreferrer">relatively thin wrappers around API clients</a>, or <a href="https://mcp.so/server/time/modelcontextprotocol" target="_blank" rel="noopener noreferrer">utility scripts</a>. In the former case, users must still obtain API keys, billing accounts, and so on.</p>
<p>This code <em>was</em> a hassle to write, prior to the advent of coding agents. But these small utility scripts are the precise thing that coding agents excel most at! A technical user of MCP tools will be hard-pressed to find a tool an agent could not one-shot in the programming language they are most comfortable in.</p>
<h2 id="why-it-took-off">Why it took off<a href="#why-it-took-off" aria-label="Direct link to Why it took off" title="Direct link to Why it took off" translate="no">​</a></h2>
<p>With these issues, it's fair to wonder why MCP has gained the popularity it has. It has had lots of support from Anthropic, and no trouble gaining traction with toolset publishers, agent providers, and enterprises. Why? It helps narratives:</p>

<p>It's quite easy to publish an MCP server. The lack of startup requirements means you don't even need to publish to <code>npm</code> or <code>pip</code>: you can drop an <code>@mcp.server</code> annotation in your repo and host a small manifest JSON that points to your entry command (e.g., <code>node server.js</code>) and lists the tools.</p>
<p>This provides a nice narrative to gain attention to AI projects: A user can, in theory, easily add some MCP tools from a project, gain value, and follow interest in learning more about the project. Support overhead will, in the main, fall to agent maintainers.</p>
<p>Once publishers started appearing, it became difficult to justify <em>not</em> supporting MCP. Your project could be perceived as being against open standards.</p>
<h3 id="enterprise-ai-credibility">Enterprise: AI credibility<a href="#enterprise-ai-credibility" aria-label="Direct link to Enterprise: AI credibility" title="Direct link to Enterprise: AI credibility" translate="no">​</a></h3>
<p>Over the last few years, anyone watching San Francisco billboards has witnessed enterprise tools rebranding toward AI. MCP support provided an easy way to make your e.g. project management tool be AI. The branding of MCP as an "open standard" increased pressure to adopt - lack of MCP support could signal a lack of willingness to adopt open standards.</p>
<h3 id="anthropic-open-source-credibility">Anthropic: Open source credibility<a href="#anthropic-open-source-credibility" aria-label="Direct link to Anthropic: Open source credibility" title="Direct link to Anthropic: Open source credibility" translate="no">​</a></h3>
<p>MCP's status as <em>the</em> open standard for AI and the enterprise adoption greatly benefited Anthropic. The big fear of investors is that enterprise adoption doesn't persist - adoption of Anthropic's open standard helped this.</p>
<h2 id="alternatives">Alternatives<a href="#alternatives" aria-label="Direct link to Alternatives" title="Direct link to Alternatives" translate="no">​</a></h2>
<h3 id="who-benefits-from-mcp">Who benefits from MCP?<a href="#who-benefits-from-mcp" aria-label="Direct link to Who benefits from MCP?" title="Direct link to Who benefits from MCP?" translate="no">​</a></h3>
<p>There are a few different possible users who interact with MCP:</p>
<p><img decoding="async" loading="lazy" alt="users" src="https://tombedor.dev/assets/images/users-e233d38824dd614cca76cbe6a8e983f0.png" width="1077" height="722"></p>
<ul>
<li>
<p><em>Technical end users</em> want to create tools and share them between different agents they might want to use.</p>
</li>
<li>
<p><em>Non-technical end users</em> want to use different tools while using agents. Note that this user group for MCP is, at present, largely theoretical. Exposing toolsets to MCP involves editing JSON, making it out of reach for non-technical users.</p>
</li>
<li>
<p><em>Internal app devs</em> run production AI applications.</p>
</li>
<li>
<p><em>Agent devs</em> create agents for external users. They wish to enable their end users to swap in whatever toolsets they like.</p>
</li>
<li>
<p><em>Tool authors</em> create toolsets they wish to expose to users. MCP provides a way to easily share their work to users of different agents.</p>
</li>
</ul>
<p>Notice that the supposed beneficiaries are overwhelmingly technical. The "app store for AI" vision that would serve non-technical users remains unfulfilled.</p>
<p>For each user type, there's a simpler approach that avoids MCP's overhead:</p>
<table><thead><tr><th>User Type</th><th>MCP Promise</th><th>Better Alternative</th><th>Why</th></tr></thead><tbody><tr><td><strong>Technical end users</strong></td><td>Share tools between agents</td><td>Local scripts + command runner</td><td>AI can one-shot these scripts; works with any agent via shell; exposes tools to humans too</td></tr><tr><td><strong>Non-technical end users</strong></td><td>Easy tool installation</td><td><em>(MCP doesn't deliver)</em></td><td>MCP requires JSON editing—this group remains underserved regardless</td></tr><tr><td><strong>Internal app devs</strong></td><td>Standard tool interface</td><td>1st party tools</td><td>Same codebase, existing auth/logging/tracing, no process overhead, coherent toolbox</td></tr><tr><td><strong>Agent devs</strong></td><td>Let users swap toolsets</td><td>SDK abstraction (LangChain, LiteLLM)</td><td>Handles model API differences without separate processes</td></tr><tr><td><strong>Tool authors</strong></td><td>Distribute to all agents</td><td>OpenAPI specs or libraries</td><td>Existing distribution (npm, pip), decades of tooling, no new protocol</td></tr></tbody></table>
<h3 id="local-scripts-with-command-runner">Local scripts with command runner<a href="#local-scripts-with-command-runner" aria-label="Direct link to Local scripts with command runner" title="Direct link to Local scripts with command runner" translate="no">​</a></h3>
<p>For a technical user, letting an agent invoke scripts directly is very difficult to beat. Useful 50-100 line scripts are <em>extremely</em> easy to write with AI coding agents. Care needs to be taken to filter output - raw build scripts can stream verbose logs into agent context, eating up tokens.</p>
<p><img decoding="async" loading="lazy" alt="just" src="https://tombedor.dev/assets/images/just-c9e8594a50331d5b095b35a059dae448.png" width="1352" height="1171"></p>
<p>Robust security against agent actions going haywire can be achieved via command runners like <a href="https://github.com/casey/just" target="_blank" rel="noopener noreferrer">just</a> or <a href="https://en.wikipedia.org/wiki/Make_(software)" target="_blank" rel="noopener noreferrer">make</a>. These tools provide everything that MCP does - command specifications, descriptions, arguments. Agents allow you to specify what command prefixes can be invoked without approval - put your agent commands in a <code>justfile</code>, and only auto-allow shell commands prefixed with <code>just</code>.</p>
<p>This approach also exposes tools to humans, and is a nice approach for improving dev environments for humans and AI agents at the same time. (See <a href="https://tombedor.dev/make-it-easy-for-humans/">Make It Easy for Humans First, Then AI</a> for more on this).</p>
<h3 id="1st-party-tools">1st party tools<a href="#1st-party-tools" aria-label="Direct link to 1st party tools" title="Direct link to 1st party tools" translate="no">​</a></h3>
<p>For a self contained application, there is little reason to separate tool codebases from the codebase for the rest of the application. Tools can be dynamically exposed to the agent based on application context.</p>
<p>In a first party context, any code that devs wish to reuse can be exposed as libraries, just like any other code they wish to share. An AI tool is really nothing more than a function, and the fact that it's invoked by AI does not warrant special handling.</p>
<p>An enterprise context should have robust infrastructure for authenticating, authorizing, provisioning service identities, and tracing call chains for service to service calls. That some of these calls are now <em>AI</em> service to service calls does not warrant a rebuilt security posture.</p>
<h3 id="openapi--rest">OpenAPI / REST<a href="#openapi--rest" aria-label="Direct link to OpenAPI / REST" title="Direct link to OpenAPI / REST" translate="no">​</a></h3>
<p>OpenAPI specs are already self-describing enough for agents—they include operation descriptions, parameter schemas, examples, and enums. LLMs understand them well; GPT Actions are literally OpenAPI specs. The glue needed between an OpenAPI endpoint and an agent (output filtering, context, auth) is the same glue MCP requires. MCP doesn't provide meaningfully better tool descriptions; it just reinvents a schema format that already exists, without the decades of tooling, validation, and battle-testing.</p>
<h2 id="a-prediction">A prediction<a href="#a-prediction" aria-label="Direct link to A prediction" title="Direct link to A prediction" translate="no">​</a></h2>
<p>MCP's popularity will be relatively short-lived. The cost benefit does not add up, and there are readily available alternatives. The introduction of <a href="https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview" target="_blank" rel="noopener noreferrer">Claude Skills</a> and <a href="https://simonwillison.net/2025/Dec/12/openai-skills/" target="_blank" rel="noopener noreferrer">OpenAI's quick adoption</a> signal that even model providers agree.</p>
<p>Claude Skills is an incremental improvement over MCP, but is similarly overengineered. Longstanding tools and techniques for collaboration amongst human devs remain compelling, and these options will chip away at more AI-centric techniques which reinvent the wheel.</p>
<!-- -->
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1-e0bb36">
<p>Source: Github searches for <a href="https://github.com/search?q=%40mcp.tool&amp;type=code" target="_blank" rel="noopener noreferrer">@mcp.tool</a> (58.1K results), <a href="https://github.com/search?q=%40mcp.resource&amp;type=code" target="_blank" rel="noopener noreferrer">@mcp.resource</a> (9.1K), and <a href="https://github.com/search?q=%40mcp.prompt&amp;type=code" target="_blank" rel="noopener noreferrer">@mcp.prompt</a> (6.1K), searched 2025-12-08. <a href="#user-content-fnref-1-e0bb36" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2-e0bb36">
<p>Support request snippets are pulled from Discord. <a href="#user-content-fnref-2-e0bb36" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Surveillance Watch – A map that shows connections between surveillance companies (106 pts)]]></title>
            <link>https://www.surveillancewatch.io</link>
            <guid>46551855</guid>
            <pubDate>Fri, 09 Jan 2026 09:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.surveillancewatch.io">https://www.surveillancewatch.io</a>, See on <a href="https://news.ycombinator.com/item?id=46551855">Hacker News</a></p>
<div id="readability-page-1" class="page"><title>Surveillance Watch: They Know Who You Are</title><meta name="description" content="Surveillance Watch is an interactive map revealing the intricate connections between surveillance companies, their funding sources and affiliations."><meta property="og:title" content="Surveillance Watch: They Know Who You Are"><meta property="og:description" content="Surveillance Watch is an interactive map revealing the intricate connections between surveillance companies, their funding sources and affiliations."><meta property="og:image" content="https://surveillancewatch.io/og-image.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Surveillance Watch: They Know Who You Are"><meta name="twitter:description" content="Surveillance Watch is an interactive map revealing the intricate connections between surveillance companies, their funding sources and affiliations."><meta name="twitter:image" content="https://surveillancewatch.io/og-image.jpg"></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happened to WebAssembly (236 pts)]]></title>
            <link>https://emnudge.dev/blog/what-happened-to-webassembly/</link>
            <guid>46551044</guid>
            <pubDate>Fri, 09 Jan 2026 07:38:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emnudge.dev/blog/what-happened-to-webassembly/">https://emnudge.dev/blog/what-happened-to-webassembly/</a>, See on <a href="https://news.ycombinator.com/item?id=46551044">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-astro-cid-w6n32adp=""> <details data-astro-cid-6t6zfk7k=""><summary data-astro-cid-6t6zfk7k="">Table Of Contents</summary><ul data-astro-cid-6t6zfk7k=""><li data-astro-cid-6t6zfk7k=""><a href="#in-the-real-world" data-astro-cid-6t6zfk7k="">In The Real World</a></li><li data-astro-cid-6t6zfk7k=""><a href="#what-is-webassembly" data-astro-cid-6t6zfk7k="">What Is WebAssembly</a></li><li data-astro-cid-6t6zfk7k=""><a href="#a-note-on-speed" data-astro-cid-6t6zfk7k="">A Note On Speed</a></li><li data-astro-cid-6t6zfk7k=""><a href="#an-efficient-mapping" data-astro-cid-6t6zfk7k="">An Efficient Mapping</a></li><li data-astro-cid-6t6zfk7k=""><a href="#a-compilation-target" data-astro-cid-6t6zfk7k="">A compilation target</a></li><li data-astro-cid-6t6zfk7k=""><a href="#security-and-what-it-enables" data-astro-cid-6t6zfk7k="">Security and what it enables</a></li><li data-astro-cid-6t6zfk7k=""><a href="#portability-and-embeddability" data-astro-cid-6t6zfk7k="">Portability and Embeddability</a></li><li data-astro-cid-6t6zfk7k=""><a href="#speed-and-size-revisited" data-astro-cid-6t6zfk7k="">Speed and size revisited</a></li><li data-astro-cid-6t6zfk7k=""><a href="#language-development-and-you" data-astro-cid-6t6zfk7k="">Language development and you</a></li></ul></details> <p>On every WebAssembly discussion, there is inevitably one comment (often near the top) asking what happened.</p>
<p>It seems to have been advertised as a world-changing advancement. Was it just oversold? Was it another JVM applet scenario, doomed to fail?</p>
<p>I’d like to tackle this in a weirdly roundabout way because I think these sorts of questions make a few misplaced assumptions that are critical to clarify.</p>
<h2 id="in-the-real-world">In The Real World</h2>
<p>Of course, WebAssembly <strong>does</strong> see real-world usage. Let’s list some examples!</p>
<ul>
<li>Godot uses WebAssembly to <a href="https://docs.godotengine.org/en/stable/contributing/development/compiling/compiling_for_web.html">build games for the web</a>.</li>
<li>Squoosh.app uses WebAssembly to <a href="https://web.dev/blog/squoosh-v2#new_codecs_support">make use of image libraries</a>.</li>
<li>Zellij uses WebAssembly for its <a href="https://zellij.dev/documentation/plugins#plugins">plugin ecosystem</a>.</li>
<li>Figma uses WebAssembly to <a href="https://www.figma.com/blog/webassembly-cut-figmas-load-time-by-3x/">convert their C++ code</a> to something usable in a browser</li>
<li>Stackblitz uses WebAssembly for <a href="https://blog.stackblitz.com/posts/introducing-webcontainers/">their web containers</a>.</li>
<li>Ruffle uses WebAssembly to run <a href="https://ruffle.rs/">a flash emulator</a> in your browser</li>
</ul>
<p>For many of these, WebAssembly is critical to either their entire product or a major feature.</p>
<p>But I think this alone is not very convincing. We don’t yet see major websites entirely built with <a href="https://github.com/yewstack/yew">webassembly-based frameworks</a>. We’re not building our applications directly to WebAssembly for maximum portability. But why not?</p>
<p>To answer this, we need a good mental model for what WebAssembly is. This will help us qualify where it is most impactful and the limitations we’re up against.</p>
<h2 id="what-is-webassembly">What Is WebAssembly</h2>
<p>In a word, WebAssembly is a language.</p>
<h2 id="a-note-on-speed">A Note On Speed</h2>
<p>This makes questions like “how fast is WebAssembly” a bit hard to answer. You don’t ask how fast algebraic notation is—it’s not a very sensible question.</p>
<p>Taken in the context of something like JavaScript, the language is only as fast as the engine running it. JavaScript the language has no speed, but you can benchmark JS engines like V8, SpiderMonkey, and JavaScriptCore. You can benchmark the IO libraries of JS runtimes like Bun, Deno, and Node.</p>
<p>What people actually mean is “how useful are the constructs of this language to efficient mappings of modern hardware” and “what is the current landscape of systems taking advantage of these constructs”.</p>
<p>Through clever-enough engineering, you can make any system sufficiently fast with some trade-offs. If compiling your code directly to C doesn’t bother you, getting “near native” speeds is possible in both <a href="https://hermesengine.dev/">JavaScript</a> and <a href="https://github.com/WebAssembly/wabt/blob/main/Wasm2c/README.md">WebAssembly</a>.</p>
<p>That’s right, you can compile WebAssembly! You can also choose to interpret it directly—that’ll be up to your runtime, just like every other system.</p>
<p>So let’s ask the actual question of WebAssembly: how useful are the constructs of this language to efficient mappings of modern hardware? Turns out, pretty useful!</p>
<h2 id="an-efficient-mapping">An Efficient Mapping</h2>
<p>WebAssembly is a pretty close approximation of an assembly language. Not too close, mind you. It’s higher level than that. But it’s close enough to cleanly compile to most assembly languages without a significant speed trade-off.</p>
<p>And yes, you can write WebAssembly by hand! I made a rustlings-esque course called <a href="https://github.com/EmNudge/watlings/">watlings</a> where you can hand-write WAT to solve some basic exercises.</p>
<p>WAT is a very close approximation to Wasm. It is <strong>almost</strong> 1:1 in that you can compile WAT to Wasm and then back to WAT with barely any loss in information (you may lose variable names and some metadata). It looks like this:</p>
<div><figure><pre data-language="clojure"><code><div><p><span>(</span><span>module</span></p></div><div><p><span>  </span><span>;; import external i32, name it $global_num_import</span></p></div><div><p><span><span>  </span></span><span>(</span><span>import</span><span> </span><span>"env"</span><span> </span><span>"global_num"</span><span> (</span><span>global</span><span> $global_num_import i32))</span></p></div><div><p><span>  </span><span>;; A function that adds param $a to $global_num_import, returns i32</span></p></div><div><p><span><span>  </span></span><span>(</span><span>func</span><span> $add_to_global_num (</span><span>param</span><span> $a i32) (</span><span>result</span><span> i32)</span></p></div><div><p><span>    </span><span>;; The last stack value is the return value</span></p></div><div><p><span><span>    </span></span><span>(</span><span>i32.add</span><span> (</span><span>local.get</span><span> $a) (</span><span>global.get</span><span> $global_num_import))</span></p></div><div><p><span><span>  </span></span><span>)</span></p></div><div><p><span>  </span><span>;; export local function, name it add_to_global</span></p></div><div><p><span><span>  </span></span><span>(</span><span>export</span><span> </span><span>"add_to_global"</span><span> (</span><span>func</span><span> $add_to_global_num))</span></p></div><div><p><span>)</span></p></div></code></pre></figure></div>
<p>Try reading the code. It will feel both familiar and foreign.</p>
<p>We have functions and S-expressions. We have imports and exports. But we also have instructions like <code>i32.add</code> and implicit stack returns.</p>
<p>Wasm is a bytecode perhaps best compared to <em>JVMIS</em> (i.e. JVM bytecode). They have similar goals and constraints, but different landscapes and guarantees.</p>
<p>Compared to JVM bytecode, Wasm has a significantly smaller API and stronger safety guarantees. It has fewer opinions on your memory management strategy and more limitations on what your program can do without permission from its host environment.</p>
<p>It can crunch numbers, but must be explicitly provided its memory and all imports. In this way, it is much different from an actual assembly language (or, a more widely used one).</p>
<p>We’ll wrap back around to this later.</p>
<h2 id="a-compilation-target">A compilation target</h2>
<p>You can compile many languages to Wasm.</p>
<p>Notable among them are Rust, C, Zig, Go, Kotlin, Java, and C#. Commonly interpreted languages have even had their runtimes compiled to WebAssembly, such as Python, PHP, and Ruby. There are also many languages that solely compile to WebAssembly, such as <a href="https://www.assemblyscript.org/">AssemblyScript</a>, <a href="https://grain-lang.org/">Grain</a>, and <a href="https://www.moonbitlang.com/">MoonBit</a>.</p>
<p>For many of these, it is important not to require a garbage-collector. For others, it would be helpful to include one. Wasm allows for both (with the GC option being much more recent).</p>
<p>Your browser includes a Wasm “engine”, making this doubly an attractive compilation target. This means without much setup, your phone and laptop can run Wasm programs already.</p>
<p>Like how JVM can have many implementations of its runner, there are many implementations that run independently of your browser such as <a href="https://github.com/bytecodealliance/Wasmtime">Wasmtime</a>, <a href="https://github.com/WasmEdge/WasmEdge">WasmEdge</a>, and <a href="https://github.com/Wasmerio/Wasmer">Wasmer</a>.</p>
<div><figure><pre data-language="bash"><code><div><p><span>$</span><span> </span><span>Wasmer</span><span> </span><span>run</span><span> </span><span>cowsay</span><span> </span><span>"I am cow"</span></p></div><div><p><span> </span><span>__________</span></p></div><div><p><span>&lt;</span><span> I am cow </span><span>&gt;</span></p></div><div><p><span> </span><span>----------</span></p></div><div><p><span>        </span><span>\</span><span>   </span><span>^__^</span></p></div><div><p><span>         </span><span>\</span><span>  (oo)</span><span>\_</span><span>______</span></p></div><div><p><span><span>            </span></span><span>(</span><span>__</span><span>)</span><span>\</span><span>       )</span><span>\/\</span></p></div><div><p><span>               </span><span>||</span><span>----w</span><span> </span><span>|</span></p></div><div><p><span>                </span><span>||</span><span>     </span><span>||</span></p></div></code></pre></figure></div>
<p>These languages can output a single artifact without being too specific to your computer’s hardware. You only need a Wasm runner to execute it (note more JVM analogies).</p>
<h2 id="security-and-what-it-enables">Security and what it enables</h2>
<p>Right now, Wasm is looking really similar to JVM. The main differences seem to be around memory management strategies and how many platforms support it.</p>
<p>The security story is what really starts to drive in the wedge.</p>
<p>WebAssembly maintains a minimal attack surface by treating all external interactions as explicit, host-defined imports. We went over this earlier. Its “deny-by-default” architecture, small instruction set, hidden control-flow stack (i.e. no raw pointers), and linear memory combine to create a <strong>very strong</strong> security story.</p>
<p>It is such that you can ensure process-like isolation within a single process. Cloudflare takes advantage of this aspect within V8 to run untrusted code very efficiently <a href="https://blog.cloudflare.com/mitigating-spectre-and-other-security-threats-the-cloudflare-workers-security-model/">using V8 isolates</a>. This means significant efficiency gains without significant security trade-offs.</p>
<p>Wasm programs can start 100x faster if you can avoid spinning up a separate process. Fermyon, a company in the Wasm hosting space, advertises <a href="https://www.fermyon.com/serverless-guide/speed-and-execution-time">sub-millisecond spinup times</a>.</p>
<p>In these cases, the performance is a direct result of what the security guarantees enable.</p>
<p>In other cases, security can unlock feature support.</p>
<p>Flash is a multimedia platform that was primarily used for animations and games up until it was dropped from all major browsers in January of 2021 (primarily) due to security concerns. <a href="https://ruffle.rs/">Ruffle</a> has revived Flash experiences on sites like <a href="https://www.notion.so/What-Happened-To-WebAssembly-aa6295c947a24fc48de2a31378ced53b?pvs=21">Newgrounds</a> by acting as an interpreter and VM for ActionScript.</p>
<p>Cloudflare allows running Python code with similar security guarantees to its JS code by using <a href="https://pyodide.org/en/stable/">Pyodide</a>, which is a Wasm build of CPython.</p>
<p>Figma runs untrusted user plugins in your browser by running them in <a href="https://www.figma.com/blog/how-we-built-the-figma-plugin-system/">a QuickJS engine that is compiled to Wasm</a>.</p>
<p>Elsewhere, the security allows for extreme embeddability.</p>
<h2 id="portability-and-embeddability">Portability and Embeddability</h2>
<p>We’ve gone over the number of ways you can run Wasm programs. A Wasm runner can be pretty light. Instead of forcing library authors into a specific language (usually Lua or JavaScript), supporting Wasm itself opens the door to a much wider set of choices.</p>
<p>Tools like <a href="https://github.com/zellij-org/zellij">Zellij</a>, <a href="https://www.envoyproxy.io/">Envoy</a>, and <a href="https://github.com/lapce/lapce">Lapce</a> support Wasm for their plugin ecosystem.</p>
<p>In environments where a JavaScript engine is already being used, this means access to programs you would not have been able to run otherwise.</p>
<p>This includes <a href="https://github.com/kleisauke/Wasm-vips">image processing</a>, <a href="https://github.com/naptha/tesseract.js">ocr</a>, <a href="https://github.com/kripken/ammo.js">physics engines</a>, <a href="https://skia.org/docs/user/modules/canvaskit/">rendering engines</a>, <a href="https://ffmpegwasm.netlify.app/">media toolkits</a>, <a href="https://github.com/sql-js/sql.js">databases</a>, and <a href="https://github.com/tree-sitter/tree-sitter/blob/master/lib/binding_web/README.md">parsers</a>, among many others.</p>
<p>In a majority of these cases, the use of Wasm will be transparent to you. A library you installed will just be using it somewhere in its dependency tree.</p>
<p>Godot and Figma have codebases written in C++, but are often browser-ready by compiling to (or in combination with) WebAssembly.</p>
<p>It seems the most common use of Wasm is bridging the language gap. Certain ecosystems seem to have suites of tools more common to them. <a href="https://squoosh.app/">Squoosh</a> would be a much more limited application if it could only choose image compression libraries from NPM.</p>
<h2 id="speed-and-size-revisited">Speed and size revisited</h2>
<p>Browsers run WebAssembly with roughly the same pipeline that runs JavaScript. This seemingly puts a hard limit on the performance of Wasm applications, but they will often be more or less performant due to their architecture or domain.</p>
<p>Using languages with richer type systems and more sophisticated optimizing compilers can produce more efficient programs. The JIT model of engines like V8 might prevent optimizations if the cost of optimizing exceeds the gains from running the optimized code. You might avoid <a href="https://mrale.ph/blog/2015/01/11/whats-up-with-monomorphism.html">megamorphic functions</a> more easily by avoiding JavaScript.</p>
<p>However, there is a cost to crossing the host-program boundary, especially if cloning memory. <a href="https://news.ycombinator.com/item?id=31214521">Zaplib’s post-mortem</a> is an interesting read here. Incrementally moving a codebase to Wasm can incur significant costs in boundary crossing, eliminating any benefit in the short term.</p>
<p>A small API surface also means binary bloat as system APIs are more often re-created than imported. There are standards like <a href="https://wasi.dev/">WASI</a> which aim to help here. Still, there is no native string type (<a href="https://github.com/WebAssembly/stringref">yet</a>).</p>
<p>Zig seems to produce the smallest Wasm binaries among mainstream languages.</p>
<p>Practical performance of Wasm in native contexts (i.e. outside of a JS engine) seems to suffer for a variety of reasons. Threading and IO of any sort incurs some cost. Memory usage is larger. Cold start is slower.</p>
<p>Still, the performance trade-offs might not be significant enough to matter. For most uses, I’d wager it’s “fast enough”. If you’re in a performance-sensitive context, the benefits of Wasm are likely not as relevant.</p>
<h2 id="language-development-and-you">Language development and you</h2>
<p>Clearly things are happening.</p>
<p>The <a href="https://www.youtube.com/@Wasmio">Wasm IO YouTube channel</a> has lots of talks worth watching.</p>
<p>In fact, standards and language development in Wasm has stirred significant controversy internally. There is a lot of desire for advancement, but standardization means decisions are hard to reverse. For many, things are moving too quickly and in the wrong direction.</p>
<p>There is the “more official” <a href="https://www.w3.org/groups/wg/Wasm/">W3C working group</a> and then the “less official” <a href="https://github.com/bytecodealliance">Bytecode Alliance</a> which works much more quickly and is centered around tooling and language development outside of Wasm directly (e.g. on <a href="https://component-model.bytecodealliance.org/design/wit.html">WIT</a> and the <a href="https://component-model.bytecodealliance.org/">WebAssembly Component Model</a>).</p>
<p><a href="https://webassembly.org/features/">Wasm feature proposals</a> are being quickly advanced and adopted by a wide suite of tools. This is remarkable progress for standardization, but is also scary to watch if you fear large missteps.</p>
<p>So why do people think nothing has happened?</p>
<p>I figure most are under the impression that the advancement of this technology would have had a more visible impact on their work. That they would intentionally reach for and use Wasm tools.</p>
<p>Many seem to think there is a path to Wasm replacing JavaScript within the browser—that they might not need to include a <code>.js</code> file at all. This is very unlikely.</p>
<p>However, you can use frameworks like <a href="https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor">Blazor</a> and <a href="https://github.com/leptos-rs/leptos">Leptos</a> without being aware or involved in the produced JS artifacts.</p>
<p>Mostly, Wasm tools have been adopted and used by library authors, not application developers. The internals are opaque. This is fine, probably.</p>
<p>Separately, I think the community is not helped by the philosophy of purposely obfuscating teaching material around Wasm. This is a fight I lost a few times.</p>
<p>For now, maybe check out <a href="https://github.com/EmNudge/watlings">watlings</a>. I’ll expand it at some point, surely.</p>  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Commission issues call for evidence on open source (324 pts)]]></title>
            <link>https://lwn.net/Articles/1053107/</link>
            <guid>46550912</guid>
            <pubDate>Fri, 09 Jan 2026 07:09:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1053107/">https://lwn.net/Articles/1053107/</a>, See on <a href="https://news.ycombinator.com/item?id=46550912">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The European Commission has <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/16213-European-Open-Digital-Ecosystems_en">opened</a>
a "<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=intcom%3AAres%282026%2969111">call
for evidence</a>" to help shape its European Open Digital Ecosystem
Strategy. The commission is looking to reduce its dependence on
software from non-EU countries:</p>

<blockquote>
The EU faces a significant problem of dependence on non-EU countries
in the digital sphere. This reduces users' choice, hampers EU
companies' competitiveness and can raise supply chain security issues
as it makes it difficult to control our digital infrastructure (both
physical and software components), potentially creating
vulnerabilities including in critical sectors. In the last few years,
it has been widely acknowledged that open source – which is a public
good to be freely used, modified, and redistributed – has the strong
potential to underpin a diverse portfolio of high-quality and secure
digital solutions that are valid alternatives to proprietary ones. By
doing so, it increases user agency, helps regain control and boost the
resilience of our digital infrastructure.
</blockquote>

<p>The feedback period runs until midnight (Brussels time)
February&nbsp;3, 2026. The commission seeks input from all interested
stakeholders, "<q>in particular the European open-source community
(including individual contributors, open-source companies and
foundations), public administrations, specialised business sectors,
the ICT industry, academia and research institutions</q>".</p><br clear="all"><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mathematics for Computer Science (2018) [pdf] (239 pts)]]></title>
            <link>https://courses.csail.mit.edu/6.042/spring18/mcs.pdf</link>
            <guid>46550895</guid>
            <pubDate>Fri, 09 Jan 2026 07:06:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://courses.csail.mit.edu/6.042/spring18/mcs.pdf">https://courses.csail.mit.edu/6.042/spring18/mcs.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46550895">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do not mistake a resilient global economy for populist success (188 pts)]]></title>
            <link>https://www.economist.com/leaders/2026/01/08/do-not-mistake-a-resilient-global-economy-for-populist-success</link>
            <guid>46550777</guid>
            <pubDate>Fri, 09 Jan 2026 06:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/leaders/2026/01/08/do-not-mistake-a-resilient-global-economy-for-populist-success">https://www.economist.com/leaders/2026/01/08/do-not-mistake-a-resilient-global-economy-for-populist-success</a>, See on <a href="https://news.ycombinator.com/item?id=46550777">Hacker News</a></p>
Couldn't get https://www.economist.com/leaders/2026/01/08/do-not-mistake-a-resilient-global-economy-for-populist-success: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The No Fakes Act has a “fingerprinting” trap that kills open source? (168 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/</link>
            <guid>46550231</guid>
            <pubDate>Fri, 09 Jan 2026 05:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/">https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/</a>, See on <a href="https://news.ycombinator.com/item?id=46550231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey everyone,
​I’ve been reading the text of the "NO FAKES Act" currently in Congress, and it’s worse than I thought.
​The Tldr: It creates a "digital replica right" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who "makes available" a tool that is primarily used for replicas.<br>
​The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).
​There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.</p>

<p>What I did:
I contacted my reps email to flag this as an "innovation killer." If you run a repo or care about open weights, you might want to do the same. We need them to add a "Safe Harbor" for tool devs.</p>

<p>S.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress <a href="https://share.google/u6dpy7ZQDvZWUrlfc">https://share.google/u6dpy7ZQDvZWUrlfc</a></p>

<p>UPDATE: ACTION ITEMS (How to actually stop this)
​If you don't want to go to jail for hosting a repo, you need to make noise now.
​1. The "Lazy" Email (Takes 30 seconds):
Go to Democracy.io or your Senator’s contact page.
​Subject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability
​Message: "I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers."
​2. The "Nuclear" Option (Call them):
​Call the Capitol Switchboard: (202) 224-3121
​Ask for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain.
​Script: "The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic blocks third-party use of Claude Code subscriptions (465 pts)]]></title>
            <link>https://github.com/anomalyco/opencode/issues/7410</link>
            <guid>46549823</guid>
            <pubDate>Fri, 09 Jan 2026 03:44:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/anomalyco/opencode/issues/7410">https://github.com/anomalyco/opencode/issues/7410</a>, See on <a href="https://news.ycombinator.com/item?id=46549823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">Description</h3>
<p dir="auto">As of a few moments ago, usage of claude max stopped with the following error:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/48773775/533688146-ecc0f211-a883-4a6a-b1c8-816d7ea449a2.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MzY5MDIsIm5iZiI6MTc2NzkzNjYwMiwicGF0aCI6Ii80ODc3Mzc3NS81MzM2ODgxNDYtZWNjMGYyMTEtYTg4My00YTZhLWIxYzgtODE2ZDdlYTQ0OWEyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMGU3OWFlODFhNWVhYWM4ZDVmYzcwYzVmMTc3NjA5NjgyNTcyNGY2MjFiZjk5ZDgxNTM2Y2VmMmViMjkxZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.GsvDEalmHMMawElWRsKIv5PyBY_1kda0H7u82CwmqfU"><img width="831" height="66" alt="Image" src="https://private-user-images.githubusercontent.com/48773775/533688146-ecc0f211-a883-4a6a-b1c8-816d7ea449a2.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MzY5MDIsIm5iZiI6MTc2NzkzNjYwMiwicGF0aCI6Ii80ODc3Mzc3NS81MzM2ODgxNDYtZWNjMGYyMTEtYTg4My00YTZhLWIxYzgtODE2ZDdlYTQ0OWEyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDA1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMGU3OWFlODFhNWVhYWM4ZDVmYzcwYzVmMTc3NjA5NjgyNTcyNGY2MjFiZjk5ZDgxNTM2Y2VmMmViMjkxZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.GsvDEalmHMMawElWRsKIv5PyBY_1kda0H7u82CwmqfU"></a></p><p dir="auto">I did try to reconnect, but got the same error.</p>
<h3 dir="auto">Plugins</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">OpenCode version</h3>
<p dir="auto">1.1.8</p>
<h3 dir="auto">Steps to reproduce</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Screenshot and/or share link</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Operating System</h3>
<p dir="auto">mac</p>
<h3 dir="auto">Terminal</h3>
<p dir="auto"><em>No response</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I Left iNaturalist (230 pts)]]></title>
            <link>https://kueda.net/blog/2026/01/06/why-i-left-inat/</link>
            <guid>46548940</guid>
            <pubDate>Fri, 09 Jan 2026 01:17:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kueda.net/blog/2026/01/06/why-i-left-inat/">https://kueda.net/blog/2026/01/06/why-i-left-inat/</a>, See on <a href="https://news.ycombinator.com/item?id=46548940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>After almost 18 years, I left iNaturalist, the product and organization I helped create. I left because I don’t believe the current Leadership team is pointing the product in the right direction, and I don’t think they are managing their talented staff in an empathetic or effective way. If you’d like me to continue working on natural history software, <a href="https://patreon.com/kueda?utm_medium=web&amp;utm_source=why-i-left-inat&amp;utm_campaign=creatorshare_creator&amp;utm_content=support-me-on-patreon"><strong>support me on Patreon</strong></a>.</p>

<p>This post is an announcement for those who were unaware, an explanation for those who are confused, and a record so I don’t forget.</p>

<h2 id="some-history">Some History</h2>
<p>I wanted to build something like iNat shortly after I moved to the San Francisco Bay Area in 2003. In 2007 I attended the UC Berkeley School of Information and built it along with my fellow students and co-founders <a href="https://www.inaturalist.org/people/5">Nate Agrin</a> and <a href="https://www.inaturalist.org/people/2">Jess Kline</a>. Nate and I worked on it a bit in our spare time after graduating, and I started collaborating with <a href="https://www.inaturalist.org/people/477">Scott</a> in 2009, still in our spare time. Scott was instrumental in recruitment, funding, and collaboration, and we formed an <abbr title="Limited Liabilty Corporation">LLC</abbr> together as a way to have a bank account to accept funds to build out functionality like the first iPhone and Android apps. In 2014 we joined the California Academy of Sciences (<abbr title="California Academy of Sciences">CAS</abbr>) in the hope of gaining access to more resources to hire more staff, which we did and thus survived almost a decade of growth in usage. In 2023 we left <abbr title="California Academy of Sciences">CAS</abbr> after arduous negotiations and formed an independent non-profit.</p>

<p>Up to this point, iNat functioned as an unstructured anarchy. Scott and I were titular “co-directors” but we did not provide a lot of direction and most of the big moves and features were driven largely by individual initiative. We never found a great way to collaborate. We struggled to set collective goals that might override individual ones, we struggled to keep longer-term goals in mind amid day-to-day firefighting, and we remained merely the sum of our parts, if that. Toward the end of our time at <abbr title="California Academy of Sciences">CAS</abbr> we experimented with <a href="https://en.wikipedia.org/wiki/Sociocracy">sociocracy</a> as a way to organize without hierarchy and coercion, but despite my enthusiasm for the form, we didn’t start with universal buy-in or understanding from the whole team, we didn’t fully adopt its structures, and, like many democracies before us, we ultimately voted to abolish our own democracy when we formed the “leadership circle” and created a hierarchy.</p>

<p>The leadership circle was me, Scott, and <a href="https://www.inaturalist.org/people/7580">Carrie</a>. We didn’t have clearly-defined roles, but our mandate was navigating the separation from <abbr title="California Academy of Sciences">CAS</abbr> while forming a new organization, which meant lots of talking with lawyers, understanding the requirements of tax-exempt status, forming a new board and writing its bylaws. I had very little interest in any of it and to my discredit, I let Carrie and Scott do most of the work. When we did need to make decisions, I was generally the minority, e.g. in how much to disclose in our negotiations with <abbr title="California Academy of Sciences">CAS</abbr> (<abbr title="in my opinion">IMO</abbr> everything) or whether board members should be required to donate money (<abbr title="in my opinion">IMO</abbr> no, plutocracy is bad at all times and at all levels). In addition to constantly feeling like the losing vote, the “leadership circle” was sliding into becoming a Leadership team of department heads, and being a dis-empowered leader of people doing the actual work wasn’t what I wanted, so I stepped down and was officially just an engineer, work I’d been doing all along anyway.</p>

<p>The new mobile app (which we were calling “iNat Next” and is now simply “iNaturalist,” the older app becoming “iNaturalist Classic”… still following? I’m going to refer to them as “iNat Next” and “iNat Classic” for clarity) was proving to be more work than the team dedicated to it could handle, so I put all of my time into that. We hired two new staff to work on it, and did our best to reconcile what little guidance we got from Leadership with an internal process focused on discussion and consent. By late 2024, the app wasn’t what anyone wanted it to be, but we had soft-launched on iPhone and were iteratively improving. At this point Leadership started establishing arbitrary goals like a hard launch with promotion in time for City Nature Challenge 2025 with the hopes of getting featured in the Apple App Store. They insisted the app needed to be simpler, to cater first to incidental users who wanted a quick answer, to be a friction-less path to a feeling of contribution. I don’t believe that’s possible while also serving existing users who value (don’t laugh) the power and nuance of iNat, including, among many other things, the way it <em>doesn’t</em> give you a quick answer, forcing you to consider options when making an identification. At this point it was clear to me Leadership wanted the app to be something I had no interest in using and that I didn’t believe would serve people like me, and that they were totally uninterested in hearing feedback from the team developing it, so I left and joined the web team.</p>

<p>Over that spring, Leadership changed direction on iNat Next again and again, shifting the baseline for what constituted a viable feature set for release and driving the team toward a deadline for getting featured in the App Store that ultimately proved fruitless when the app wasn’t featured after all. The team felt dis-empowered and Leadership seemed unwilling to listen to their complaints. Some of the team asked if I could do anything, so I tried to do what seemed like the only thing I could do in a hierarchy and communicate the problems up to the Head of Engineering and Head of Engagement (early March 2025), but there was likewise no change in behavior from Leadership in the way they managed the mobile team. I ultimately proposed restructuring product Leadership under a new Head of Product role with the explicit responsibility of consulting with workers and with users about their needs and capabilities, working closely with design and engineering staff to ensure that no one was blindsided by sudden changes in direction but also ensuring no team was left without product direction (17 April 2025). I proposed that this role have independent control over product decisions to resist impulses from the rest of Leadership to override existing priorities before work could be tested by use. I also proposed that I could do the job.</p>

<p>A day later, Leadership informed me they had no intention of adopting my proposal (“We’re not planning to change the Head of Product structure at this time,” 18 April 2025, referring the three-member sub-group of the Leadership team that was making product decisions at the time). A few days after that Scott announced that he was taking on an additional role to his Executive Director responsibilities: Head of Product (circa 21 April 2025). On 5 May 2025, the Leadership team summoned everyone who worked on iNat Next into an agenda-less meeting titled “post launch Strategic alignment” and announced that they would offer us half a year’s pay to quit. Offended and incensed that they would rather so many staff leave than listening to and addressing their concerns, I decided iNaturalist was no longer an organization I wanted to work for and I told them to write up the offer. I consulted with friends and advisors, who all suggested that I take longer to decide, perhaps on a sabbatical. Leadership agreed to that, so I tried to spend the last few months thinking about whether I could remain in an organization led by people in whom I have lost so much faith. In the interim, most of the recipients of that buyout offer left the organization, as well as another engineer, and the Head of Engineering, resulting in a 30% attrition in staff. The org is trying to fill that gap with the three new engineers that have been hired for the mobile team, former board member <a href="https://www.inaturalist.org/people/181">Dan Rademacher</a> joining staff as Head of Product (though not, apparently, with the independence I proposed, so responsibility without the necessary power), and hiring two more engineers for the web/ops team (in progress, to my knowledge).</p>

<p>As I was leaving, the Google gen AI debacle happened, a fiasco big enough to merit description in <a href="https://www.scientificamerican.com/article/google-ai-grant-to-inaturalist-prompts-community-outcry/">Scientific American</a>. This was an own goal. iNat’s Engagement team predicted the backlash but the Leadership team chose not to listen to their warning. I was not involved in this grant, its announcement, or its fulfillment, but while it didn’t directly lead to my own choice to leave, it is symptomatic of the problems that led to that choice, and I think it played a role convincing more staff to quit. In my discussions with staff since, almost everyone recalled being blindsided by the announcement of the grant and confused about what the money was going to be used for.</p>

<h2 id="product">Product</h2>
<p>My fundamental difference from Leadership in terms of the organization’s products is that I think different products should meet different needs, while the Leadership team believes a single product can meet all the needs of the organization’s potential users (I have many others differences, but this might be the least reconcilable). At the beginning of iNat, I thought one product could meet all those needs too. In grad school, we described personas of potential users and quickly identified the divide in needs between enthusiast naturalists and people with a more occasional interest in nature, and we tried to design to meet the needs of both groups of people. Again and again over the years, I saw that we just couldn’t do it. Enthusiasts need power and complexity that is immediately confusing and intimidating to more casual users, and that extends from the use of scientific names through the kinds of filters you can use to explore the data. Casual users need structure and guidance to invite them into the amazing but complex and often unfamiliar world of biodiversity, but that structure can be a hindrance or even infantilizing to people who just want to get things done. iNaturalist the product is fundamentally complicated, and I have watched many, many people bounce off that wall of complexity over the years, even as I’ve seen so many people enrich their lives after they climb over it.</p>

<p>The last nail in the coffin on this subject for me was Seek, an idea from <a href="https://www.inaturalist.org/people/44845">Alex</a> and <a href="https://www.inaturalist.org/people/79343">Joelle</a> (later rebuilt by <a href="https://www.inaturalist.org/people/1132118">Amanda</a> and <a href="https://www.inaturalist.org/people/247291">Abhas</a>) to build a version of iNat <em>specifically</em> for that casual crowd, focused on quick answers with a gamified structure. Seek was wildly successful, eventually equaling and occasionally exceeding the usage of the iNaturalist apps. Two critical anecdotes: I remember looking for rare plants in a burn zone and overhearing two young botanists one hill over trying to figure out a plant and eventually saying, “well let’s just see what Seek says.” I was alarmed that these professionals were using such an unprofessional tool, but also reminded that in that moment, they didn’t need iNat-level complexity. They just needed a Seek-sized nudge in their identification process. One summer a year or two later I went home for my annual family vacation and my dad told me he’d just discovered the marvels of Seek and was puttering around everywhere seeing what Seek thought of the plants in the yard. Dad has been a meticulous recorder of information his entire life, and has been a loyal iNat contributor for years, but he has never had much interest in computers and absolutely couldn’t give a toss about connecting with other people on the Internet, so Seek really worked for him in a way iNat never did.</p>

<p>iNat’s history and Seek’s success prove to me that my initial belief in a single app to meet all needs was wrong: iNat the product should serve the enthusiast users, and Seek should serve the casual users. Arguments about Seek eating into iNat’s potential usage are absurd: the mission of the organization is to connect people to nature through technology, and if two products are doing that better than one product, that’s success. If Seek users aren’t contributing to a global data set, that’s also fine. Data generation has always been a byproduct of building connection. Seek could be better about encouraging data contribution, but it doesn’t need to.</p>

<p>iNat’s current Leadership does not share this belief. To them, Seek is an off-brand liability that they don’t intend to improve. They think iNaturalist the product can serve those Seek users while also serving existing core iNat contributors to the detriment of neither.</p>

<h2 id="management">Management</h2>
<p>Leadership made many mistakes over the last year, including</p>

<ol>
  <li>Failing to listen to the concerns of the mobile team in the development of iNat Next and overriding their decisions</li>
  <li>Abandoning product direction on the web team and for Seek</li>
  <li>Trying to address discord by jettisoning staff instead of addressing their concerns</li>
  <li>Accepting a grant without any consultation with staff about how its obligations might be met</li>
  <li>Ignoring staff warnings about how the announcement of that grant would not go over well with users</li>
</ol>

<p>I lay the departure of 30% of staff at their feet, primarily because of the buyout offer. I think that showed the entire organization that their bosses are the kind of people who deal with criticism with a firm shove out the door. Yes, the people who left may have had other opportunities, but they primarily left because they were not being consulted when they had feedback, not getting help when they needed it, and not granted the level of independence they needed to get things done. The reputational damage from the Google AI grant was similarly due to Leadership’s inability to hear and digest criticism. If they had listened to the warnings of the Engagement team, it might have gone down very differently.</p>

<p>Since the exodus, Leadership has improved on some fronts. Within the Engagement and Engineering teams they are consulting with staff more than they have in the past. Within Engagement that consultation seems to be translated into action more than it has in the past. They hired three new engineers for the mobile team that seem both experienced and enthusiastic. Bringing Dan into a Head of Product role will hopefully provide more empathy and clarity to the engineering teams.</p>

<p>But fundamentally for me, even if Leadership has learned something from their mistakes, they have not learned to admit them, which proves to me they haven’t really learned to accept criticism. There has been no all-staff discussion about stumbling through the release of iNat Next, of the Google gen AI debacle, or, most importantly, why 30% of staff left the organization in three months. The mistakes of the Leadership team are partially or totally to blame for these events, and if they do not own their responsibility before staff and before the community, I don’t think they deserve their power. They certainly don’t deserve my trust.</p>

<h2 id="whats-next">What’s Next</h2>

<p>First of all, I quit. I can’t work with a Leadership team that has such a different vision for where iNat the product needs to go and how iNat the organization should be managed. Or if I can, I can’t find satisfaction while doing so.</p>

<p>I’d like to keep working on natural history software, and if you’d like me to as well, <a href="https://patreon.com/kueda?utm_medium=web&amp;utm_source=why-i-left-inat&amp;utm_campaign=creatorshare_creator&amp;utm_content=support-me-on-patreon"><strong>support me on Patreon</strong></a>. In a perfect world, I’d be employed by the people who use the software I make, so I figured I’d try it. Right now I’m working on <a href="https://github.com/kueda/chuck">a way to back up your iNat observations</a> and an <a href="https://underfoot.rocks/">app for viewing geologic maps</a>. Both could use a lot of work. In the future I’d like to experiment with a decentralized version of iNat. In all likelihood I’ll need to get a “real” job so <a href="mailto:work@kueda.net">let me know</a> if you’d think you’d like to hire me.</p>

<p>I am, of course, heartbroken. I genuinely like almost everyone on staff and I will miss working with them. It would be hard to find a more talented, bright, and perceptive group of people, and they genuinely care about iNat and are trying to do right by it, which makes it all the more tragic that the organization can’t seem to do right by them.</p>

<p>To those who are mystified by a co-founder’s choice to leave the thing they co-founded, I don’t really care about the fact that I helped create iNat (it was not a unique idea), but I very much care about it existing in something like its present form because I use it every day, interact with people like me on it every day, and it hurts to step away from an active role in improving and maintaining it.</p>

<p>Obviously, I think the Leadership team should change, and maybe they will. Hiring Dan is a good step, but I think they will need more fundamental changes to their approach to right the ship, starting with learning how to admit when they’re wrong.</p>

<p>I also think the board should change, because they are either complicit in Leadership’s mistakes or failing in their oversight function. I think the board should develop lines of communication with staff and users outside of the Leadership team. We set up iNat’s board to be largely non-interventionist, but I think that was a mistake. Now they only get information from a Leadership team that cannot admit fault. If the board is really to perform its oversight function, it should be hearing from staff and users more directly.</p>

<p>I also think the board should follow Wikimedia’s lead and commit to a <a href="https://foundation.wikimedia.org/wiki/Legal:Wikimedia_Foundation_Bylaws#(F)_Overall_Board_composition.">majority of community-selected members</a>, instead of selecting their own membership from people like them. Users should have more power in the organization. Right now they’re barely even informed about what staff are doing.</p>

<p>In case anyone is alarmed about the state of iNat after reading this, I do not advise stopping your use of iNat, and I definitely don’t advise deleting your account. I’m certainly not doing either. I was appalled at the number of people who went nuclear and deleted their accounts during the gen AI debacle. Temporarily withdrawing your data would be a reasonable form of protest if it was possible, but destroying existing data that can’t be retrieved hurts everyone. If you think iNat has problems, either the ones I’ve described or others, I think you should keep contributing observations, identifications, and if you can, donations, <em>but</em> also organize. Users don’t currently have any effective control over staff or the board, but you do control the life blood of iNat: participation. A large enough cessation of data or money would exert pressure, but it would take organization.</p>

<p>If you’re concerned, I’d also advise exploring or creating alternatives. While I don’t think iNat has reached Doctorow-level <a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a>, it does suffer from centralization. If you don’t like the way your data is used to train <abbr title="Artificial Intelligence">AI</abbr> models, you can’t currently move your data to a non-<abbr title="Artificial Intelligence">AI</abbr> service while still contributing to and using iNat data, even if such a service existed. But you should be able to do that. It also suffers from stagnation. If you want features like client-side geofencing, video support, sonograms, sound <abbr title="identification">ID</abbr>, etc, you might need to build them. iNat still maintains the web 2.0 virtue of <a href="https://www.eff.org/deeplinks/2019/10/adversarial-interoperability">adversarial interoperability</a> in the form of its <abbr title="Application Programming Interface">API</abbr>, so some of these alternatives can be built in non-competitive ways (and if the <abbr title="Application Programming Interface">API</abbr> goes away, we’ll know things have gotten really bad), but some may require competition. Those of us who benefit from tools like iNat should be looking seriously to the decentralized models being developed by the likes of <a href="https://en.wikipedia.org/wiki/AT_Protocol">Bluesky</a> and <a href="https://en.wikipedia.org/wiki/ActivityPub">Mastodon</a>, because we can’t rely on any single organization to provide that benefit forever.</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embassy: Modern embedded framework, using Rust and async (258 pts)]]></title>
            <link>https://github.com/embassy-rs/embassy</link>
            <guid>46547740</guid>
            <pubDate>Thu, 08 Jan 2026 23:00:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/embassy-rs/embassy">https://github.com/embassy-rs/embassy</a>, See on <a href="https://news.ycombinator.com/item?id=46547740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Embassy</h2><a id="user-content-embassy" aria-label="Permalink: Embassy" href="#embassy"></a></p>
<p dir="auto">Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://embassy.dev/book/index.html" rel="nofollow">Documentation</a> - <a href="https://docs.embassy.dev/" rel="nofollow">API reference</a> - <a href="https://embassy.dev/" rel="nofollow">Website</a> - <a href="https://matrix.to/#/#embassy-rs:matrix.org" rel="nofollow">Chat</a></h2><a id="user-content-documentation---api-reference---website---chat" aria-label="Permalink: Documentation - API reference - Website - Chat" href="#documentation---api-reference---website---chat"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rust + async ❤️ embedded</h2><a id="user-content-rust--async-️-embedded" aria-label="Permalink: Rust + async ❤️ embedded" href="#rust--async-️-embedded"></a></p>
<p dir="auto">The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.</p>
<p dir="auto">Rust's <a href="https://rust-lang.github.io/async-book/" rel="nofollow">async/await</a> allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is <a href="https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown" rel="nofollow">faster and smaller than one!</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Batteries included</h2><a id="user-content-batteries-included" aria-label="Permalink: Batteries included" href="#batteries-included"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Hardware Abstraction Layers</strong></p>
<ul dir="auto">
<li>HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.</li>
<li><a href="https://docs.embassy.dev/embassy-stm32/" rel="nofollow">embassy-stm32</a>, for all STM32 microcontroller families.</li>
<li><a href="https://docs.embassy.dev/embassy-nrf/" rel="nofollow">embassy-nrf</a>, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.</li>
<li><a href="https://docs.embassy.dev/embassy-rp/" rel="nofollow">embassy-rp</a>, for the Raspberry Pi RP2040 and RP23xx microcontrollers.</li>
<li><a href="https://docs.embassy.dev/embassy-mspm0/" rel="nofollow">embassy-mspm0</a>, for the Texas Instruments MSPM0 microcontrollers.</li>
<li><a href="https://github.com/esp-rs">esp-rs</a>, for the Espressif Systems ESP32 series of chips.
<ul dir="auto">
<li>Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the <a href="https://github.com/esp-rs/esp-hal">esp-rs/esp-hal</a> repository.</li>
</ul>
</li>
<li><a href="https://github.com/ch32-rs/ch32-hal">ch32-hal</a>, for the WCH 32-bit RISC-V(CH32V) series of chips.</li>
<li><a href="https://github.com/AlexCharlton/mpfs-hal">mpfs-hal</a>, for the Microchip PolarFire SoC.</li>
<li><a href="https://github.com/py32-rs/py32-hal">py32-hal</a>, for the Puya Semiconductor PY32 series of microcontrollers.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Time that Just Works</strong> -
No more messing with hardware timers. <a href="https://docs.embassy.dev/embassy-time" rel="nofollow">embassy_time</a> provides Instant, Duration, and Timer types that are globally available and never overflow.</p>
</li>
<li>
<p dir="auto"><strong>Real-time ready</strong> -
Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the <a href="https://github.com/embassy-rs/embassy/blob/main/examples/nrf52840/src/bin/multiprio.rs">example</a>.</p>
</li>
<li>
<p dir="auto"><strong>Low-power ready</strong> -
Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.</p>
</li>
<li>
<p dir="auto"><strong>Networking</strong> -
The <a href="https://docs.embassy.dev/embassy-net/" rel="nofollow">embassy-net</a> network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.</p>
</li>
<li>
<p dir="auto"><strong>Bluetooth</strong></p>
<ul dir="auto">
<li>The <a href="https://github.com/embassy-rs/trouble">trouble</a> crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the <a href="https://github.com/embassy-rs/bt-hci">bt-hci</a> traits (currently
<code>nRF52</code>, <code>nrf54</code>, <code>rp2040</code>, <code>rp23xx</code> and <code>esp32</code> and <code>serial</code> controllers are supported).</li>
<li>The <a href="https://github.com/embassy-rs/nrf-softdevice">nrf-softdevice</a> crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.</li>
<li>The <a href="https://github.com/embassy-rs/embassy/tree/main/embassy-stm32-wpan">embassy-stm32-wpan</a> crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LoRa</strong> -
The <a href="https://github.com/lora-rs/lora-rs">lora-rs</a> project provides an async LoRa and LoRaWAN stack that works well on Embassy.</p>
</li>
<li>
<p dir="auto"><strong>USB</strong> -
<a href="https://docs.embassy.dev/embassy-usb/" rel="nofollow">embassy-usb</a> implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.</p>
</li>
<li>
<p dir="auto"><strong>Bootloader and DFU</strong> -
<a href="https://github.com/embassy-rs/embassy/tree/main/embassy-boot">embassy-boot</a> is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sneak peek</h2><a id="user-content-sneak-peek" aria-label="Permalink: Sneak peek" href="#sneak-peek"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri<'static, AnyPin>) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!(&quot;Button pressed!&quot;);
        button.wait_for_high().await;
        info!(&quot;Button released!&quot;);
    }
}"><pre><span>use</span> defmt<span>::</span>info<span>;</span>
<span>use</span> embassy_executor<span>::</span><span>Spawner</span><span>;</span>
<span>use</span> embassy_time<span>::</span><span>{</span><span>Duration</span><span>,</span> <span>Timer</span><span>}</span><span>;</span>
<span>use</span> embassy_nrf<span>::</span>gpio<span>::</span><span>{</span><span>AnyPin</span><span>,</span> <span>Input</span><span>,</span> <span>Level</span><span>,</span> <span>Output</span><span>,</span> <span>OutputDrive</span><span>,</span> <span>Pin</span><span>,</span> <span>Pull</span><span>}</span><span>;</span>
<span>use</span> embassy_nrf<span>::</span><span>{</span><span>Peri</span><span>,</span> <span>Peripherals</span><span>}</span><span>;</span>

<span>// Declare async tasks</span>
<span>#<span>[</span>embassy_executor<span>::</span>task<span>]</span></span>
<span>async</span> <span>fn</span> <span>blink</span><span>(</span><span>pin</span><span>:</span> <span>Peri</span><span>&lt;</span><span>'</span><span>static</span><span>,</span> <span>AnyPin</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> led = <span>Output</span><span>::</span><span>new</span><span>(</span>pin<span>,</span> <span>Level</span><span>::</span><span>Low</span><span>,</span> <span>OutputDrive</span><span>::</span><span>Standard</span><span>)</span><span>;</span>

    <span>loop</span> <span>{</span>
        <span>// Timekeeping is globally available, no need to mess with hardware timers.</span>
        led<span>.</span><span>set_high</span><span>(</span><span>)</span><span>;</span>
        <span>Timer</span><span>::</span><span>after_millis</span><span>(</span><span>150</span><span>)</span><span>.</span><span>await</span><span>;</span>
        led<span>.</span><span>set_low</span><span>(</span><span>)</span><span>;</span>
        <span>Timer</span><span>::</span><span>after_millis</span><span>(</span><span>150</span><span>)</span><span>.</span><span>await</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>// Main is itself an async task as well.</span>
<span>#<span>[</span>embassy_executor<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>spawner</span><span>:</span> <span>Spawner</span><span>)</span> <span>{</span>
    <span>let</span> p = embassy_nrf<span>::</span><span>init</span><span>(</span><span>Default</span><span>::</span><span>default</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>// Spawned tasks run in the background, concurrently.</span>
    spawner<span>.</span><span>spawn</span><span>(</span><span>blink</span><span>(</span>p<span>.</span><span>P0_13</span><span>.</span><span>into</span><span>(</span><span>)</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>let</span> <span>mut</span> button = <span>Input</span><span>::</span><span>new</span><span>(</span>p<span>.</span><span>P0_11</span><span>,</span> <span>Pull</span><span>::</span><span>Up</span><span>)</span><span>;</span>
    <span>loop</span> <span>{</span>
        <span>// Asynchronously wait for GPIO events, allowing other tasks</span>
        <span>// to run, or the core to sleep.</span>
        button<span>.</span><span>wait_for_low</span><span>(</span><span>)</span><span>.</span><span>await</span><span>;</span>
        <span>info</span><span>!</span><span>(</span><span>"Button pressed!"</span><span>)</span><span>;</span>
        button<span>.</span><span>wait_for_high</span><span>(</span><span>)</span><span>.</span><span>await</span><span>;</span>
        <span>info</span><span>!</span><span>(</span><span>"Button released!"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Examples are found in the
<code>examples/</code> folder separated by the chip manufacturer they are designed to run on. For example:</p>
<ul dir="auto">
<li><code>examples/nrf52840</code> run on the
<code>nrf52840-dk</code> board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.</li>
<li><code>examples/nrf5340</code> run on the <code>nrf5340-dk</code> board (PCA10095).</li>
<li><code>examples/stm32xx</code> for the various STM32 families.</li>
<li><code>examples/rp</code> are for the RP2040 chip.</li>
<li><code>examples/std</code> are designed to run locally on your PC.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running examples</h3><a id="user-content-running-examples" aria-label="Permalink: Running examples" href="#running-examples"></a></p>
<ul dir="auto">
<li>Install <code>probe-rs</code> following the instructions at <a href="https://probe.rs/" rel="nofollow">https://probe.rs</a>.</li>
<li>Change directory to the sample's base directory. For example:</li>
</ul>

<ul dir="auto">
<li>
<p dir="auto">Ensure <code>Cargo.toml</code> sets the right feature for the name of the chip you are programming.
If this name is incorrect, the example may fail to run or immediately crash
after being programmed.</p>
</li>
<li>
<p dir="auto">Ensure <code>.cargo/config.toml</code> contains the name of the chip you are programming.</p>
</li>
<li>
<p dir="auto">Run the example</p>
</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --release --bin blinky"><pre>cargo run --release --bin blinky</pre></div>
<p dir="auto">For more help getting started, see <a href="https://github.com/embassy-rs/embassy/wiki/Getting-Started">Getting Started</a> and <a href="https://github.com/embassy-rs/embassy/wiki/Running-the-Examples">Running the Examples</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developing Embassy with Rust Analyzer-based editors</h2><a id="user-content-developing-embassy-with-rust-analyzer-based-editors" aria-label="Permalink: Developing Embassy with Rust Analyzer-based editors" href="#developing-embassy-with-rust-analyzer-based-editors"></a></p>
<p dir="auto">The <a href="https://rust-analyzer.github.io/" rel="nofollow">Rust Analyzer</a> is used by <a href="https://code.visualstudio.com/" rel="nofollow">Visual Studio Code</a>
and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer
must be told of the target project to work with. In the case of Visual Studio Code,
please refer to the <code>.vscode/settings.json</code> file's <code>rust-analyzer.linkedProjects</code>setting.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimum supported Rust version (MSRV)</h2><a id="user-content-minimum-supported-rust-version-msrv" aria-label="Permalink: Minimum supported Rust version (MSRV)" href="#minimum-supported-rust-version-msrv"></a></p>
<p dir="auto">Embassy is guaranteed to compile on stable Rust 1.75 and up. It <em>might</em>
compile with older versions, but that may change in any new patch release.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why the name?</h2><a id="user-content-why-the-name" aria-label="Permalink: Why the name?" href="#why-the-name"></a></p>
<p dir="auto">EMBedded ASYnc! :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Embassy is licensed under either of</p>
<ul dir="auto">
<li>Apache License, Version 2.0 (<a href="https://github.com/embassy-rs/embassy/blob/main/LICENSE-APACHE">LICENSE-APACHE</a> or
<a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a>)</li>
<li>MIT license (<a href="https://github.com/embassy-rs/embassy/blob/main/LICENSE-MIT">LICENSE-MIT</a> or <a href="http://opensource.org/licenses/MIT" rel="nofollow">http://opensource.org/licenses/MIT</a>)</li>
</ul>
<p dir="auto">at your option.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution</h2><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's Call a Murder a Murder (187 pts)]]></title>
            <link>https://daringfireball.net/2026/01/lets_call_a_murder_a_murder</link>
            <guid>46547612</guid>
            <pubDate>Thu, 08 Jan 2026 22:47:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/2026/01/lets_call_a_murder_a_murder">https://daringfireball.net/2026/01/lets_call_a_murder_a_murder</a>, See on <a href="https://news.ycombinator.com/item?id=46547612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">



<p><a href="https://www.nytimes.com/video/us/100000010631041/minneapolis-ice-shooting-video.html">The New York Times has frame-by-frame analysis</a>, from three angles, of the murder of 37-year-old Renee Good in Minneapolis yesterday. She was shot to death by <s>a still-unnamed</s> mask-wearing ICE agent <a href="https://www.startribune.com/ice-agent-who-fatally-shot-woman-in-minneapolis-is-identified/601560214">Jonathan Ross</a>, with what was obviously no justification. The shooting is, justifiably, national news. I’m sure you’ve read about it. But this Times analysis coolly and calmly shows just how outrageous it was, and how preposterous the claims from President Trump and <a href="https://bsky.app/profile/atrupar.com/post/3mbwjgwh5xc2u">Secretary of Hats Kristi Noem</a> are ostensibly attempting to defend it — both as an act of self-defense by the cowardly ICE agent and, even more absurdly, as an act of “domestic terrorism” by Good, who was attempting to do nothing more than drive away from the scene.</p>

<p>George Orwell, in <em>1984</em>: “The Party told you to reject the evidence of your eyes and ears. It was their final, most essential command.” Let’s stop pussyfooting around what happened here. This ICE agent murdered Renee Good, in broad daylight, in front of many witnesses and multiple cameras. Trust the evidence of your eyes and ears.</p>

<hr>

<p>But I want to add another note. The main footage here comes from bystander Caitlin Callenson. <a href="https://www.youtube.com/watch?v=K9CJY5p0xz4">Here’s her full 4m:25s footage</a>, uncensored, hosted — with credit, and I hope, permission — on the YouTube account of Minnesota Reformer. Be warned that it shows Good being shot to death (albeit sans gore), and contains many loud profanities. This is <em>very</em> good and clear footage. It is difficult viewing but you should watch it. Callenson was very close to Good’s vehicle. I’d say about 30 feet or so. You can see why she thought to start filming  <em>before</em> the murderous agent drew his gun and fired. The scene was already chaotic. But then, <em>after</em> the murderous agent fired three shots — just 30 or 40 feet in front of Callenson — Callenson had the courage and conviction to stay with the scene and keep filming. Not to run away, but instead to follow the scene. To keep filming. To continue documenting with as best clarity as she could, what was unfolding.</p>

<p>I’d like to think I’d have done the same. I’m not sure at all that I would have. I definitely might have been using my iPhone to shoot video of the incident up until the shots were fired. But when that happened, my mind would immediately have turned to “<em>These agents are scared and angry and out of control, and</em> that <em>one just went psycho and fired his gun unprovoked. That guy is just as likely to shoot more people as he was the woman he just shot. His angry, scared, obviously undertrained colleagues might join in. And the most likely people they’ll shoot next are people pointing cameras at them.</em>” I do not know what I would have done in that moment. I hope I never find out. But I know with certainty what I would immediately think, which is that if I choose to continue shooting video of the incident, there is a very good chance one of them will shoot or brutalize me next. It would make <em>more</em> sense to shoot someone filming the scene than it did to shoot Renee Good in the first place. Good’s killing was utterly senseless. Shooting a witness with a running camera and then destroying their phone to eliminate the evidence (and a witness) would make some sense. Sick sense, but sense.</p>

<p>But in that moment of pandemonium and obvious danger to herself, Callenson didn’t merely continue filming. She didn’t merely stand her ground. She proceeded <em>into</em> the scene to get closer to Good’s vehicle after it crashed into a parked car, <a href="https://www.youtube.com/watch?v=ixNkVvtC5FY">Mr. Brown-style</a>. She pointed her camera directly at the only-partially-masked face of the murderous agent as he walked away from Good’s crashed vehicle, then got into an unmarked Chevy Tahoe and just fled from the scene like the obvious coward he is. I presume the murderous agent will soon be identified, and Callenson’s clear steady-handed footage may be the reason why. [<strong>Update:</strong> While I was finishing this post, the Minnesota Star Tribune identified and named him — <a href="https://www.startribune.com/ice-agent-who-fatally-shot-woman-in-minneapolis-is-identified/601560214">Jonathan Ross</a> — and indeed, it was Callenson’s footage that made his identification possible.] And, to top it off, all the while — starting <em>before</em> the shooting — Callenson was screaming “Shame!” in the faces of these agents, and calling them out on their abhorrent indefensible actions. To each of their directives to her, she responds, with the definition of righteous anger, “You shot someone in the fucking face!” (Emily Heller, Renee Good’s neighbor, <a href="https://www.cnn.com/2026/01/07/us/video/ebof-minneapolis-ice-shooting-witness-emily-heller">showed similar courage</a>, telling an ICE agent who refused to allow a citizen physician to check on Good (who laid dying or dead inside her car), as she filmed the scene, “How can I relax, you just killed my fucking neighbor! You shot her in the fucking face! You killed my fucking neighbor! How do you show up to work every day?”)</p>

<p>Callenson’s courage in the face of obvious danger is just remarkable. My god. She rose to the moment in a crucible of chaos, insanity, and murderous violence. We all need to think about what she did, to really imagine ourselves in the same moment — the danger she stood up to, and the principles she stood up for — if we hope to do the same if a similar moment comes to us.</p>

<p>And, to top it off, she had the presence of mind to shoot her historic footage in widescreen. </p>



 <!-- PreviousNext -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iran Protest Map (169 pts)]]></title>
            <link>https://pouyaii.github.io/Iran/</link>
            <guid>46547303</guid>
            <pubDate>Thu, 08 Jan 2026 22:20:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pouyaii.github.io/Iran/">https://pouyaii.github.io/Iran/</a>, See on <a href="https://news.ycombinator.com/item?id=46547303">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi (2017) (215 pts)]]></title>
            <link>https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/</link>
            <guid>46546614</guid>
            <pubDate>Thu, 08 Jan 2026 21:17:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/">https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/</a>, See on <a href="https://news.ycombinator.com/item?id=46546614">Hacker News</a></p>
<div id="readability-page-1" class="page">
<header>
<h2>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/themes/item/assets/gfx/logo.png?v=1" alt="WARP"></a>
</h2>
<nav>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/">ITEMS</a>
<a href="https://web.archive.org/web/20180719052026/http://www.warp.net/" target="_blank">WARP.NET</a>
<a href="https://web.archive.org/web/20180719052026/https://warp.us7.list-manage.com/subscribe/post?u=92da423b0a560b89a4b558a36&amp;id=3d39485b46&amp;MERGE0=" target="_blank">SIGN UP</a>

</nav>
<h2>
ITEM • Richard D. James speaks to Tatsuya Takahashi </h2>
</header>
<img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/1.jpg" alt="Richard D. James speaks to Tatsuya Takahashi">
<div>
<p>
<h4>
<span>
Richard D. James interviews Ex. Korg engineer about their collaboration on the Monologue, microtuning, geometry and dreams. Accompanied by track "Korg Funk 5" using exclusively Korg synthesisers.
Tatsuya Takahashi is currently advisor for Korg and holds a full-time position at Yadastar GmbH to work on technology based projects. His Tokyo home was photographed by Akemi Kurosaka. <br>
10/06/2017 </span>
</h4>
</p>
</div>
<div id="post20">
<p>
<iframe src="https://web.archive.org/web/20180719052026if_/http://www.youtube.com/embed/hUT01p-C2xo?rel=0&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen=""></iframe>
</p>
</div>
<div id="post16">
<p><span>Richard D. James:</span> I really enjoyed working on this with you. I know I only joined the project near the end, but I found it really exciting. Like a proper job, ha.</p>
<p><span>Tatsuya Takahashi:</span> Richard, it was amazing working with you on the monologue. And now to be interviewed by you?!? That's crazy. But also a lot of fun. The monologue was also the last Korg synth that I was involved with directly, so I guess it's a nice conclusion to things.</p>
<p><span>RDJ:</span> It is now the only synth on the market currently being made to have full microtuning editing, congratulations!</p>
<p><span>TT:</span> Thanks! But it was completely because of you that we included microtuning. If you hadn't insisted on it, I definitely wouldn't have discovered how powerful it was. Did you ever have a moment of realisation, or some kind of trigger that made you discover microtuning?</p>
<p><span>RDJ:</span> The first thoughts that I had about tuning in general happened with my early noodlings on a Yamaha DX100, one of the first synths I saved up for. I remember looking at the master tuning of 440 Hz and thinking I would change it, for no other reason apart from it was set by default to that frequency and that it could be changed.</p>
<p>I just used to select a single note, adjust the master tuning of it to taste and then base the whole track around that, something I’ve done ever since, just intuition and maybe a bit of rebelliousness. It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?</p>
<p>I’ve since gone on to learn more about this <a href="https://web.archive.org/web/20180719052026/https://en.wikipedia.org/wiki/A440_(pitch_standard)">damn 440 Hz</a>. It was a standard introduced in 1939 by western governments, so I’m very glad I trusted my instincts. Listening to that other voice is THE most important thing in creativity, whether you’re an engineer or a musician. Tesla had some important advice on listening to the thoughts from the other. One of the most important inventors ever, but we’re not taught about him in British schools. Funny that.</p>
<p><span>TT:</span> I don't know why it's thin on the curriculum, but the Tesla coil is definitely amazing. If you modulate the high frequency with audio signals you can play music with plasma – that's super cool. I will read up on him though, cos I don't know much about his life and thinking.</p>
<p><span>RDJ:</span> An interesting “note”: I’ve just been reading a book on electronic instruments published in the 1940’s and it says that 440 Hz was transmitted over the radio on different frequencies 24 hours a day and others between midnight and 2 in the afternoon, ha, so you could tune your instruments and be well behaved or calibrate your lab equipment to it.</p>
</div>
<blockquote id="post17">
<div>
<p>
It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you? </p>
<p><cite>
RICHARD D. JAMES </cite>
</p></div>
</blockquote>
<div id="post19">
<p>But I’ve also read studies from the old Philips laboratories in the Netherlands that show orchestras average deviation from 440 Hz was measured over many concerts and was seen to differ by a few Hz, usually slightly below. Pretty anal. Some people obviously really cared that 440 Hz was being adhered to in practice.</p>
<p>Why 440 Hz was chosen in the first place is another interesting story, but looking at the resonances of water and sound is a great place to start, or read up on <a href="https://web.archive.org/web/20180719052026/https://en.wikipedia.org/wiki/Cymatics" target="_blank">cymatics</a>. If you aren’t already familiar with it, that is.</p>
<p><span>TT:</span> So many things are standardised that you don't really think about because they were there before you started using it. 440 Hz was brought about to standardise the way people play together and, yeah, someone can bring a guitar to a piano and it would work together because of that standard. </p>
<p>It's like how a green light means you can cross the road or if you shake your head sideways it means no. Those two standards will help you through life in many places around the world. But it's dangerous to enforce standards in creativity. I have a son who's started school in Japan, where every kid will paint the sun red. Now that is some fucked up standardisation! Just really messed up on so many levels.</p>
<p>Anyway, I'm not going into that whole 432 Hz vs 440 Hz debate. (BTW: I absolutely love cymatics and I've done some nice workshops for kids with it.) But I will say different frequencies sound different, so why not use that in your music? You got to use whatever feels right and the monologue let's you do exactly that with pitch.</p>
<p><span>RDJ:</span> Yep.</p>
<p><span>TT:</span> Talking of standards, the sample rate of 48 kHz is another one for sampling and signal processing, but the volca sample uses a weird one at 31.25 kHz. Purely because of technical constraints, but I was thinking that might be part of the reason you liked it so much, because the different sample rate gives it a unique sound.</p>
<p><span>RDJ:</span> Haha, yes, it was pretty much the first thing I noticed. Yeah, I thought the 48 kHz, was based on the <a href="https://web.archive.org/web/20180719052026/https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" target="_blank">Nyquist Theorem</a>. I think it’s double what humans can apparently hear or something, which is another weird one. I don’t know how anybody worked out humans only hear to 20 kHz. I mean even if you can’t hear above 20 kHz, it doesn't mean that your body doesn't feel it. You don’t just experience sound through your eardrums. A good example of this is listening to a recording of your own voice. To almost everyone apart from maybe the most narcissistic, it always sounds weird/thinner/smaller, as you don’t feel the vibration of your chest and body. There are other reasons of course but that’s one for sure. Anyway, I’m into the extremes of the audio spectrum, ultra clarity ’n’ all but I probably prefer fucked-muffled/lo-bit/’70s sound more, ha!</p>
<p><span>TT:</span> Oh, and when something defies the standard – I just remembered the first time I played a Yamaha SK-10, the faders were all upside down, like max was downwards, even on the volume. I didn't know what was going on and it threw me off at first, but it's actually a bit fun like that and you soon realise it all comes from organ drawbars.</p>
<p><span>RDJ:</span> I never played the SK-10, but these Calrec mixers I use are like that also, the faders are backwards. There is a little dip switch inside to change it, but I think they have them like that for TV/broadcasting, coz if someone falls asleep at the desk they don’t want them to push all the faders up and distort two million TVs at once… Not surprising they have this safeguard considering how skull numbingly boring most TV is.</p>
<p><span>TT:</span> Right!! Yeah, but there is a certain feeling to pulling rather than pushing. It's like how an orgasm is "coming" in English, but it's “going” [iku] in Japanese.</p>
<p><span>RDJ:</span> Never thought of it like that.</p>
<p><span>TT:</span> I mean, written text in Japanese was traditionally vertical. Although now a lot is westernised and horizontal.</p>
<p><span>RDJ:</span> Ah, that’s kinda sad… So traditional Japanese text is like trackers and now it’s going like Cubase! :)</p>
<p><span>TT:</span> I sometimes wonder what Japanese synths would have looked like if they didn't copy Moog in the ’70s. You've got to think about what is convention and what is really a good design.</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/work-space.jpeg" data-fancybox="gallery" data-caption="The studio workspace of Tatsuya Takahashi" id="post73"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/work-space.jpeg" alt="The studio workspace of Tatsuya Takahashi"></a>
<div id="post29">
<p><span>RDJ:</span> I’ve got one Japanese keyboard, Suzuki, which has got some Japanese tunings built in and a little string on one end that you can pluck. It sounds really nice as well. It also has some good Japanese percussion and MIDI. I don’t think it’s very well known.</p>
<p>I wish faders were curved horizontally and vertically, so you could make them like a double helix that go over and under each other, hehe. Could do it with an augmented reality UI I guess.</p>
<p><span>TT:</span> Now that could be cool (if I'm imagining it right)! I've seen rotation sensors on the camera lens focus that work like faders on a curved surface and really thin. That could do it.</p>
<p><span>RDJ:</span> Later on when I got an SH-101, I realised its tuning wasn't like the DX100 at all. It was based on 1v/octave and was supposed to be equal temperament but because of the nature of analogue, it really wasn’t and I REALLY loved that and how it layered with the frozen <a href="https://web.archive.org/web/20180719052026/https://en.wikipedia.org/wiki/Equal_temperament" target="_blank">12TET</a> of the DX100.</p>
<p>I recently made a tuning on the monologue that I matched to an improperly calibrated SH-101 that I was fond of. I tried at first to do this using formulas inside <a href="https://web.archive.org/web/20180719052026/http://www.huygens-fokker.org/scala/" target="_blank">Scala</a>, but it's impossible to represent this accurately with simple maths, Scala can’t deal with these types of tunings unless it’s a keyboard map tuning file. This “bad” tuning is really great when I apply it to a precisely tuned digital synth that has full microtuning capabilities. It’s top making a digital synth sound like an out of tune 101! :)</p>
<p><span>TT:</span> Yeah, I think it's really telling of the age we live in when you get a knob like "SLOP" on the new Prophet that makes pitch inconsistencies a programmable parameter. On one hand, you think that the level of control is great, but on the other it feels weird to deliberately degrade something that's stable. Especially if you're a young engineer striving to design something to be close as possible to perfection, it can be hard to grasp. The best lesson about this came from Mieda – my hero at Korg. When he looked at my first synth schematic, he told me, “Takahashi-kun, your circuits are functional, but they are not musical. Musical instruments do not need perfect waveforms and correct operating points. You need to use the transistor for what it is. As long as it sounds good, it’s OK.”</p>
</div>
<blockquote id="post27">
<div>
<p>
WHEN [MIEDA] LOOKED AT MY FIRST SYNTH SCHEMATIC, HE TOLD ME, “TAKAHASHI-KUN, YOUR CIRCUITS ARE FUNCTIONAL, BUT THEY ARE NOT MUSICAL. MUSICAL INSTRUMENTS DO NOT NEED PERFECT WAVEFORMS AND CORRECT OPERATING POINTS. YOU NEED TO USE THE TRANSISTOR FOR WHAT IT IS. AS LONG AS IT SOUNDS GOOD, IT’S OK. </p>
<p><cite>
TATSUYA TAKAHASHI </cite>
</p></div>
</blockquote>
<div id="post31">
<p><span>RDJ:</span> I was going to ask you about SLOP, as you brought that up before in some old emails. I get you now. I mean, yeah, if it just sounds good in the first place then you don’t need that option, but I guess some people like their Osc’s drifty and others not so. It changes with the context I guess. Also, if you’re doing FM you might want to keep them dead on, and for analogue lead sounds, really drifty. Anyway I think I mentioned it before, but the drift on the monologue sounds REALLY nice. It seems to move, but then never go out. Care to explain? Sounds to me like it gets reset/synced at some point, but I’m probably wrong, haven't studied it in depth, just listened. Reminds me a bit of Arp oscillators, which have really nice driftyness, prob my faves! :)</p>
<p><span>TT:</span> That's bang on! So same thing in the minilogue and the volcas too: the oscillators are re-tuned when they're not being used. I'm super glad you like it though because this is such a subjective thing. The autotuning was done in a way that felt nice to me, so it was a really subjective thing and you can’t present a report to convince others that it was OK. At least now I can say RDJ said it was alright!</p>
</div>
<div id="post89">
<p><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC2176.jpg" alt="Classic KORG35 filter chip used in MS-20 and invented by Mieda."></p>
</div>
<div id="post88">
<p><span>RDJ:</span> I’d like to talk more about this 1v/octave, but that’s for another time. But, anyway, getting back to the question, I was always interested in sound and how it affected me, especially the tuning. It wasn't until my *Selected Ambient Works Vol. II* album that I actually made my own full custom tunings, although there were a few scattered things before that.</p>
<p>I’ve got a slightly weird balance thing going on and getting the tuning “right” sometimes makes the balance thing less weird for me. It’s a longer story though. </p>
<p><span>TT:</span> Yeah, I think I read somewhere about how humans normally hear pitches differently in the left and right ear and that you don't have that. That is super interesting. </p>
<p><span>RDJ:</span> Because we made it very intuitive to edit the tuning tables, I would actually just buy this synth only for that feature alone. When the export is implemented, it can be the central hub of either complete table creation or just to tweak existing imported Scala files, etc.</p>
<p><span>TT:</span> Yeah, absolutely. I would definitely download the <a href="https://web.archive.org/web/20180719052026/http://www.korg.com/us/support/download/product/0/733/#software">monologue librarian</a> because you can import and export Scala files easily with that. Hopefully other manufacturers will join the club.</p>
<p>The intuitive interface was pretty much all your idea, so a great job on that. I think your idea for the interface came from when you got your <a href="https://web.archive.org/web/20180719052026/http://www.vintagesynth.com/misc/chroma.php" target="_blank">Chroma</a> modded for microtuning. Have you modded a lot of synths for this functionality?</p>
<p><span>RDJ:</span> That’s right, I burned my own custom O.S. Eproms for the Chroma, which enables full micro tuning and editing and that’s what the monologue editor was based on. I’ve got a good list of hardware and software now that can do it. It’s been a long haul and involved hassling a lot of people, but it is now finally possible with quite a bit of equipment.</p>
<p>I’ve generally received really good responses from engineers and programmers. I’ve contacted around 50 different people/companies in the last ten years. Many weren’t even aware that all their equipment and programs were adhering to a standard that was devised hundreds of years ago.</p>
<p>Same goes for a lot of electronic musicians, this is quite surprising for electronic music, which supposedly is forward-thinking and futuristic, but most people have since told me how fascinating they have found the subject once they realised it *was* a subject!</p>
<p>I know microtuning is much more useful on polyphonic keyboards, but it’s still very usable on monophonic instruments and, again, it can be used in the future to create tuning tables that can be used in other Scala-compatible polyphonic synths.</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/DSC1836.jpg" data-fancybox="gallery" data-caption="" id="post55"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC1836.jpg" alt=""></a>
<div id="post33">
<p><span>TT:</span> Well, my initial impression was that microtuning is a really niche thing that wouldn't be needed for a mass market synth, especially a monophonic one, but if you try shifting the tuning while running a sequence, you can hear that it gives it another dimension even if it’s subtle. I'm not super-sensitive to pitch or anything, but you can still hear it change. To me, it feels like casting light on a rough surface and seeing different patterns as you move the light. So it was really important to have the easy scale edits you can do on the fly. Scala is great, it's super flexible, but it can be daunting to use and you won't get the real-time interaction, so I hope the monologue gets more people into this stuff.</p>
<p><span>RDJ:</span> I really like your light analogy, that’s great. Yep, on a monophonic instrument, what you just described will be more pronounced if you use a delay with plenty of feedback or reverb, so you can hear the differently tuned notes overlap each other.</p>
<p>Scala is deep, very deep, but some things are very quick and easy to get going. For instance, you can just type Equal 24 &amp; press the sysex send shortcut and you have a quarter tone tuning in your synth. Scala is only good for non-intuitive tuning creation, purely mathematical. I love this approach, but really prefer making tunings intuitively, note-by-note. When you’re actually composing something, making them up while you go along, a combination of the two is best for me.</p>
<p><span>TT:</span> I know that you like that Wilsonic app you showed me, which is mainly structured on mathematical relationships of frequencies, but you've also mentioned using a lot of trial-and-error. Do you have a method to your microtuning?</p>
<p><span>RDJ:</span> Yes, many. For instance, on the Chroma I like holding down one key, pressing another key and then tuning the second key in relation to the first, sometimes making two extremely different frequency combinations, like something very low and extremely high at the same time and maybe a group of these dual combos only existing in the top octave of the keyboard map, the rest being another tuning or multiple tunings, all in one tuning table.</p>
<p>It’s something I never saw in anyone else’s tunings, combining several tuning tables within one map, so that’s one of my little inventions I guess, as I rarely used the full range of 127 notes in one tuning within one track. monologue can tune four notes at a time which we planned. It’s a different approach again and something I look forward to experimenting with more.</p>
<p><span>TT:</span> Here are five short tracks you made with custom scales. Could you explain how you came up with the scales?</p>
</div>

<div id="post41">
<p><span>RDJ:</span> I forgot which tunings they used, I’ve got so many floating around in folders on the computer and in hardware. I didn’t make any notes. I think they might have been ones that I made in Scala and then tweaked on the monologue, most likely.</p>
<p><span>TT:</span> If you could share the tuning files that you created, that would be great too!</p>
<p><span>RDJ:</span> Yes, I’ve got loads saved and loads lost. I’ve never been a saver. I do save more things these days, getting older or something, but still love to use new sets of rules for every set of new tracks. Also I’ve got to say again many thanks for that lovely MIDI tuning box you made me for the minilogue!</p>
<p><span>TT:</span> No problem! That was an eye-opener for all of us. [For the readers: Richard asked me for microtuning on our synths and since, at the time, we thought it wasn't something we would put on a production model, we made a custom little tuning tool. Fellow engineer Kazuki Saita and I made a MIDI thru box that could load custom scales. Any MIDI coming in would be transposed by note and cent (using pitch bend) and so you could get microtuning on any mono synth.] When we were testing that box, Saita and I were blown away. I mean, sequencing on a simple step sequencer like in the monologue can be a bit rigid, but messing with the tuning really opens it up. It basically redefines the keyboard. We were messing around with some subtle stuff and more extreme ones like octaves split into 50 intervals and playing with the arpeggiator. It was crazy and that's when we decided we should put it on the next synth.</p>
<p><span>RDJ:</span> Yes, great! Arpeggiators and microtunings can be a very nice mix. We should include a picture of that box, I’ve got one here if you don’t.</p>
<p><span>TT:</span> We should! Don't have one handy, would you be able to snap a photo?</p>
<p><span>RDJ:</span> Attached it!</p>
</div>
<div id="post39">
<p><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/midibox.jpg" alt="Microtuning to pitchbend converter created for Richard D. James"></p>
</div>
<div id="post59">
<p><span>TT:</span> Cheers! Wood cheeks for the Cirklon. Nice. </p>
<p><span>RDJ:</span> I think the monologue is very nice looking, small, very cute and very capable. At first I thought, “Oh, it hasn’t got this, it hasn’t got that, etc. etc.” But I very quickly realised you have turned these limitations into advantages, which is really quite something special. I really mean that. The lack of extensive features makes the whole thing much more speedy to work with.</p>
<p><span>TT:</span> That's got to be the best compliment. And it's a way of thinking that runs through all the synths I've worked on, from the volcas and monotrons to the monologue. I think with electronic instruments we've got to a point where software can do most things. But I'm a fan of gear where less is more – where the simplest controls can give you the most creative freedom.</p>
<p><span>RDJ:</span> Yes, I like this approach. It’s true, I do it with modular setups as well. I’m lucky to have loads of modular gear but I prefer to make small systems now and leave everything else in another room where I just try things out before committing them to a more thought out config.</p>
<p>Of course us musicians always look at something new and we see if it does what we expect it to. And this is OK. But we shouldn’t overlook something before actually trying it out, try and get into the head of the designer first. I try and do this. It’s difficult sometimes to push your ego and expectations out of the way for a while, but if we don’t do this we won’t learn anything new. That’s not to say that every designer’s head is worth getting into, but we gotta give it a go sometimes.</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/DSC1850.jpg" data-fancybox="gallery" data-caption="Hand made sequencer by Tasuya Takahashi." id="post92"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC1850.jpg" alt="Hand made sequencer by Tasuya Takahashi."></a>
<div id="post94">
<p><span>TT:</span> This is exactly the reason I really enjoyed working with you. I'd send you a prototype and a day later you'd be sending me a dozen emails about how the drive circuit actually controls gain and dry/wet at the same time. Or how some menu option wasn’t working completely as intended. You would give everything a chance. You went through every single menu option and went after some easter eggs, like finding CC34 VCO1 pitch! In fact, you were the best ever beta tester. Guess you wouldn't be after a day job tho...</p>
<p><span>RDJ:</span> *blush* Some examples of this: When I first checked out the volca sample, the lack of velocity response had me scratching my head, but when I realised how it handled it with motion recording of the level control, it was actually loads more fun and SO much faster to program! It’s such a great little idea, I really love it, way more intuitive. I’ve started doing it this way on the Cirklon now sometimes.</p>
<p><span>TT:</span> Yeah, so you're a huge fan of the Cirklon, which you used for "korg funk 5." Could you tell us how that track was put together?</p>
<p>Here's the gear list you sent me:<br>
Korg Monologue x3<br>
Korg MS-20 kit<br>
Korg Poly-61M<br>
Korg Volca keys<br>
Korg Volca beats<br>
Korg Volca sample<br>
Korg Minilogue<br>
My son on vox<br>
</p>
<p>I was blown away by this and really really touched. I don't think there is another track out there using so much of the gear I worked on! Also, can you touch on the processing that went on the sounds, cos I can tell there's a lot going on.</p>
<p><span>RDJ:</span> That’s so nice to hear… It was really top making some tracks with only Korg gear. I’m a secret nerd-fan of synth demos, mainly vintage ’80s ones currently! Some amazing music has been made as equipment demos, unsung heroes. I collect synth demos. Well, ones that I like. It’s kind of an unclassified music genre, so doing these tracks for you and Korg was a natural thing for me. I also really like picking certain combinations of gear. That is endlessly fascinating.</p>
<p>The Volca beats I used, I did the snare mod but used the mix output, so I treated all the sounds with the same treatment, I think I sent you the full list… looks it up… OK, here it is.</p>
<p>volca beats &gt; Skibbe 736-5 mic pre [nice low mid sound] &gt; BAC 500 compressor &gt; RTZ PEQ1549 [this is based on my fave eq, I’ve got some Calrec originals as well, standard circuit design but not standard sound! ] &gt; Calrec minimixer</p>
<p>Monologue [main riff] &gt; blonder tongue EQ [i love these eq’s, hardly anyone has heard of them]</p>
</div>
<div id="post45">
<p><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC1955.jpg" alt=""></p>
</div>
<div id="post50">
<p><span>TT:</span> Any chance you could share the tracks separately? There might be something we could do with that and a lot of people will be interested in seeing how the different synths sound soloed. Only if you're up for it of course!</p>
<p><span>RDJ:</span> I would if I had them, but I never save individual tracks. I’m trying to get into the habit of that soon. I just recorded that down to the Sound Devices 722.</p>
<p><span>TT:</span> Ah shame! But you know that was the other great thing – that the track was done totally sequenced on the Cirklon and recorded in one take.</p>
<p><span>RDJ:</span> I was thinking a while back on different ways to visualise the data in the Cirklon. Also with the volca fm, you also managed to turn the lack of velocity per note into a bonus [again], it puts a different slant on it, applying and recording motion velocity on the whole phrase, it works very well.</p>
<p><span>TT:</span> So the volca keyboard is never going to do a great job of sensing velocity and we could have spent a lot more money to make it velocity-sensitive, but then you'd sit there going, "Well, it's too small to play. We need to make it bigger..." So trying to force it to be something it's not is a great way of creating more problems. Much rather turn the game around.</p>
<p><span>RDJ:</span> That’s a great example of necessity and invention. I was absolutely amazed to find out that it IS actually possible to edit a DX7 voice with great speed from the interface you have designed. I never thought you could do that, but it is and is totally usable. I’ve come up with loads of things on it that I would never have done on a full size DX7. Hats off to Tats!</p>
<p><span>TT:</span> Cheers! So everyone knows the typical DX7 sounds – well, the presets anyway – and by doing things a bit differently, you can open up so much stuff. Take an organ patch on the volca fm and sequence it normally, but then motion sequence the algorithm and it goes in a completely different dimension. It's a discovery, which is fun. I find a lot of artists are discovery junkies.</p>
<p><span>RDJ:</span> Yes, I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything. Great fun being able to take a DX7 in your pocket, love it, ultimate walkman in a way. In fact, one for the future: volca fm with built in MP3 player + radio… be super lush.</p>
<p><span>TT:</span> Yeah, super great idea! Also if it could tap into some MIDI archives and play them on the FM engine, it would be great.</p>
<p><span>RDJ:</span> Or maybe a pitch tracker from the MP3s! :-)</p>
<p><span>TT:</span> Even better! :) And it can take real time mic input, so people are saying hello to you, but you're just hearing bells or something.</p>
<p><span>RDJ:</span> Yes, recently I was offering up ideas to a talented coder friend on an app that uses evolutionary/genetic synthesis to try and resynthesise audio/live audio into DX7 patches. It sounds really cool. He’s working on making it a standalone app on Raspberry Pi, and it is based on some vintage code by Andrew Horner. Kyma also used his code for their GA synthesis. Chuck <a href="https://web.archive.org/web/20180719052026/https://fo.am/midimutant/">that</a> in there while we’re at it. </p>
<p><span>TT:</span> Got to say it's pretty funny getting a consumer product idea from you. Haha!</p>
<p><span>RDJ:</span> :) I’m full of ‘em, I’m like <a href="https://web.archive.org/web/20180719052026/https://www.youtube.com/watch?v=qf22bddvLnc" target="_blank">this guy</a>.</p>
<p><span>TT:</span> BAHAHAHHA! Holy crap.</p>
</div>
<blockquote id="post37">
<div>
<p>
I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything. </p>
<p><cite>
Richard D. James </cite>
</p></div>
</blockquote>
<div id="post91">
<p><span>RDJ:</span> How different is the finished monologue to what was designed or what you had in mind?</p>
<p><span>TT:</span> Well, it didn't have microtuning for a start!</p>
<p><span>RDJ:</span> :)</p>
<p><span>TT:</span> When I initially came up with the product plan, it wasn't very detailed. None of my product plans are. Something like: "smaller than the minilogue and monophonic." It's only when you start designing and prototyping that things start to come together. Things like: “What kind of filter do we need?” “Do we need distortion?” “Battery power would be great!”</p>
<p><span>RDJ:</span> If there are features that were designed that didn’t make it, could you tell us about them?</p>
<p><span>TT:</span> Nothing really got properly designed before being ditched. The team is pretty good at putting together test versions where we can just about see if something is going to work before we go to full implementation.</p>
<p>In terms of ideas, you had some pretty good ones:
</p><ul>
<li>keyboard to alphabet mapping for program name edit</li>
<li>random sequence generator</li>
<li>random scale generator</li>
<li>velocity to sequence position control</li>
</ul>

<p>I think the team had others like arpeggiator, which is the most obvious one. But we dropped that and added key-trigger sequence instead.</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/DSC1779.jpg" data-fancybox="gallery" data-caption="Close up of hand made sequencer taken into job interview at Korg" id="post62"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC1779.jpg" alt="Close up of hand made sequencer taken into job interview at Korg"></a>
<div id="post61">
<p><span>RDJ:</span> When or how do you find out that features that were wanted by your team are not going to make it? Is that frustrating?</p>
<p><span>TT:</span> Well, it's not like someone stands there casting their decision on whether something makes it or not. We all try to figure out how it will come together as an instrument, so a single feature might be the focus in a heated discussion, but really it's about the whole thing being coherent but also incoherent and surprising in a good way. Sometimes you need to throw people off what they're expecting to do something interesting. The team was always pretty small, so we could do it without having a draconian decision-making process, but also without it getting too democratic either. We would never ever vote on a feature.</p>
<p><span>RDJ:</span> Would it be possible that Korg could release limited edition and more costly versions of your designs with no corners cut, for us posh musos?</p>
<p><span>TT:</span> Sure, that's definitely a possibility. What's on your wish list?</p>
<p><span>RDJ:</span> Oh dear, that is a big question, I think I’ll have to get back to you on that. Well, those ones above to start with I suppose. :) Do you have a studio at home? Got any pics? Or a description of your setup?</p>
<p><span>TT:</span> I wouldn't say it's a studio, but more of a workshop. I build stuff there for my own live setup, although recently most of it is made up of products I've worked on. One of my favourite things is volca fm going into audio input of monotribe which has been modded so you can kill the VCO. I put on a slow chord progression on the fm and then work a sequence with it with the monotribe. It's actually better if I don't sync the volca fm to the monotribe.</p>
<p><span>RDJ:</span> Nice, I keep meaning to rack up 8 analogue filters to a <a href="https://web.archive.org/web/20180719052026/http://www.vintagesynth.com/yamaha/tx802.php" target="_blank">TX802</a>. Nobody ever made a decent FM synth with analogue filters, there are a few simple FM ones but not 4OP+.</p>
<p><span>TT:</span> My other favourite thing is my speaker system designed by my friends at Taguchi. They're omni-directional and I've been experimenting with the positions. My room is acoustically untreated, but with these speakers you can actually work with the reflections in the room. It's definitely not a typical setup, but it's great because you can pan your instruments around the room and you’re not glued to a sweet spot between a stereo pair. It's great if you just sequence piano phase on two volcas and offset the BPM and just let it run while the sequence phases in and out. The trick there is actually not to hard pan them, but to leave quite a bit of overlap.</p>
<p><span>RDJ:</span> [*looks at pics*] Great, that is an unusual speaker setup! I’m a big fan of suspending speakers from the ceiling, the first speakers that I built, I filled with tar and hung them from nylon cords from my bedroom ceiling. Saves space as well. Do you live and breathe Korg, do you get time for anything else, any other hobbies?</p>
<p><span>TT:</span> Don't know if it counts as a hobby, but I really like polyhedra. Maybe that’s why I like those speakers, since they're great 3D structures hanging off my ceiling. My favourite polyhedron is the dodecahedron and when you make one with wire, it's hard to make it completely regular. But it turns out I actually like the wonky ones better. Anyway, they have a cool name.</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/DSC2160.jpg" data-fancybox="gallery" data-caption="Omnidirectional speakers made by Taguchi" id="post56"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC2160.jpg" alt="Omnidirectional speakers made by Taguchi"></a>
<div id="post57">
<p><span>RDJ:</span> That’s very nice. I absolutely love geometry, I did a track called “<a href="https://web.archive.org/web/20180719052026/https://www.youtube.com/watch?v=4dHxSpHjcOE">Dodeccaheedron</a>,” a long time ago, one of my fave tracks. I was playing on <a href="https://web.archive.org/web/20180719052026/http://nathanfriend.io/inspirograph/" target="_blank">this spirograph emulator</a> recently. Ha, a 3D one would be really interesting.</p>
<p><span>TT:</span> Oh man, of course you have a track named “Dodeccaheedron”! I wonder if the track had anything to do with the fact I like them now. Bet it did. Spirographs are so cool. Bit like Lissajous – could stare at that stuff all day. I really want to get hold of some XY lasers actually and fire some really intense ones. Wish there was a way to do that in 3D.</p>
<p><span>RDJ:</span> I’ve been looking into this recently. :)</p>
<p><span>TT:</span> Maybe you can design some phosphorescent smoke that you could fire lasers into and the lines would stay in the air. That will be so cool. And the smoke particles will move with the bass – get some fat bass bins and you would get lines of light vibrating.</p>
<p><span>RDJ:</span> Top idea… Reminds me of <a href="https://web.archive.org/web/20180719052026/https://www.youtube.com/watch?v=uENITui5_jU" target="_blank">this</a>.</p>
<p><span>TT:</span> Yeah, really. I mean it could be a way of visualising the propagation of sound waves, so maybe a scientific use too. And not just sound waves. It could be used in wind tunnels to study air flow. Are we onto something here?</p>
<p><span>RDJ:</span> Yes.</p>
<p><span>RDJ:</span> What Is Your Dream?</p>
<p><span>TT:</span> Having a good cigarette. When you're having a shit day or you're under a lot of stress, cigarettes taste crap. On the other hand, a cigarette after an amazing experience tastes good. So my dream is to smoke the best cigarette ever. Smoking is a full-stop, a moment of recognition that whatever came before it was real.</p>
<p><span>RDJ:</span> I like that.</p>
<p><span>TT:</span> Bit wanky tho. ;) Getting weird vibes reading back at my answers!</p>
<p><span>RDJ:</span> LOLz</p>
<p><span>TT:</span> Oh well, wrote it once, can't deny it. </p>
<p><span>RDJ:</span> If you could magically create any device, what would it be? I understand if you’re not allowed to answer this!</p>
<p><span>TT:</span> A time machine, teleportation machine – the obvious ones. Or actually a machine where you could have as many parallel existences as you want. So you could be a super-dimensional being encompassing all the different possibilities of yourself. That's what popped into my head, but how self-centred!</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/dodecahedron-close-up-geometry-section.jpeg" data-fancybox="gallery" data-caption="Taguchi speaker close up, with dodecahedron" id="post97"><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/dodecahedron-close-up-geometry-section.jpeg" alt="Taguchi speaker close up, with dodecahedron"></a>
<div id="post76">
<p><span>RDJ:</span> I go to sleep thinking things like this… Maybe it's a bit like this already! :)</p>
<p><span>TT:</span> Hell yeah. Anyway, that's probably not what you meant. So... a lifelogging device for your musical activities. I was packing up to leave Tokyo and found a bunch of minidiscs of music that I'd forgotten I'd made in my teens and I’m guessing there would have been a lot more if I knew where my cassettes were. I cringed at most of it, but it's still part of who I am and I can't erase whatever brain patterns I have because of that.</p>
<p><span>RDJ:</span> Yes, bloody right, that would be very useful. One thing I’d say, though, is I’ve found a lot of artists write off their older work for various personal reasons, while other people won’t have those associations and just really love what you made.</p>
<p><span>TT:</span> Do you have lost musical moments from the past that you would like to hear again?</p>
<p><span>RDJ:</span> Yes, I think I’m obsessed with thoughts like this. If you could selectively erase your memory so you could keep experiencing things for the first time, it would be very interesting, although you would get stuck in loops, so you would have to limit it to a certain number of re-experiences, ha! How many future products have you got in your head or on the drawing board?</p>
<p><span>TT:</span> Quite a lot, but not all will be made. We (meaning the team still at Korg) have always got a bunch of ideas up our sleeves, it's just a case of which ones will get made and when.</p>
</div>
<div id="post95">
<p><img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/DSC2182.jpg" alt="Early ideas for volcas and first schematic for monotron"></p>
</div>
<div id="post104">
<p><span>RDJ:</span> Is your job stressful? I imagine it’s very stressful. What's the most stressful part?</p>
<p><span>TT:</span> Well, the stress was part of the balance, because there's a lot of adrenaline involved in meeting deadlines, starting production and working up to release. Now that I've left that position, I can look back in calm retrospect. I'd say it was quite physical. Kind of like a sport and also quite addictive. But at the same time you can't do it forever. I was also lucky enough to find new possibilities elsewhere, so I stopped before that high pace / full-throttle thing became the only thing I could do. I really did have an amazing time at Korg. I had the best team and I also had a lot of freedom. My decision to leave was really about me than anything to do with my working environment.</p>
<p><span>RDJ:</span> What is your worst fear?</p>
<p><span>TT:</span> Well, doing the same thing over again and then one day realising that's all you can do.</p>
<div><p><span>RDJ:</span> Yeah, I think we all have to fight against this, especially as you get older. I’ve really been looking at my habits recently and denying them. It feels great if you can manage it. </p><p>I don't understand the economics of getting hardware to market, but I guess it's safe to assume that the company makes more money from releasing new products than it does upgrading old ones. </p></div>
<p>I can’t help thinking, though, that by continuing to upgrade older products that are still in production, to make them absolutely awesome, would benefit the company in the long-term. Any thoughts about this?</p>
<p><span>TT:</span> That depends how you look at it. You can look at something like the monotribe which we spent a lot of time doing the major update for, which was then soon discontinued. So your initial point might look to hold true. But then you look at the amount we learnt from that update and that we put into the volcas, and then you can say it was worthwhile. I think it's really really important to look back and review past products. Some would benefit from an update, but others are better off redesigned. </p>
<p><span>RDJ:</span> Ok then, well lovely chatting to you as always.. wishing you all the best in your new endeavours, very brave moving yourself to a new country, well done and speak soon.</p>
<p>Here’s a <a href="https://web.archive.org/web/20180719052026/https://dood.al/pinktrombone/" target="_blank">nice link</a> to end with!</p>
</div>
<a href="https://web.archive.org/web/20180719052026/http://item.warp.net/wp-content/uploads/2017/06/aaa.jpg" data-fancybox="gallery" data-caption="Polyhedra" id="post51">
<img src="https://web.archive.org/web/20180719052026im_/http://item.warp.net/wp-content/uploads/2017/06/aaa.jpg" alt="Polyhedra">
</a>









</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU (297 pts)]]></title>
            <link>https://github.com/samuel-vitorino/sopro</link>
            <guid>46546113</guid>
            <pubDate>Thu, 08 Jan 2026 20:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/samuel-vitorino/sopro">https://github.com/samuel-vitorino/sopro</a>, See on <a href="https://news.ycombinator.com/item?id=46546113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    
    <span>sopro_readme.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/44442720/532979226-40254391-248f-45ff-b9a4-107d64fbb95f.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MTg5MDIsIm5iZiI6MTc2NzkxODYwMiwicGF0aCI6Ii80NDQ0MjcyMC81MzI5NzkyMjYtNDAyNTQzOTEtMjQ4Zi00NWZmLWI5YTQtMTA3ZDY0ZmJiOTVmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDAwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVhZmMyOGY1OTIzZGQ2MGM4YTlkY2ZiYmFkYjkzMTIwZWQwMWZiMjUzZWQ4MWQwZDc5MDRmMWRmYmNjNjdkYTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z3LSpCrqcr03tbw8G7O8jL8iQKlIQ2XX4_utKwwd1Pc" data-canonical-src="https://private-user-images.githubusercontent.com/44442720/532979226-40254391-248f-45ff-b9a4-107d64fbb95f.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MTg5MDIsIm5iZiI6MTc2NzkxODYwMiwicGF0aCI6Ii80NDQ0MjcyMC81MzI5NzkyMjYtNDAyNTQzOTEtMjQ4Zi00NWZmLWI5YTQtMTA3ZDY0ZmJiOTVmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDAwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVhZmMyOGY1OTIzZGQ2MGM4YTlkY2ZiYmFkYjkzMTIwZWQwMWZiMjUzZWQ4MWQwZDc5MDRmMWRmYmNjNjdkYTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z3LSpCrqcr03tbw8G7O8jL8iQKlIQ2XX4_utKwwd1Pc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Sopro TTS</h2><a id="user-content-sopro-tts" aria-label="Permalink: Sopro TTS" href="#sopro-tts"></a></p>
<p dir="auto"><a href="https://huggingface.co/samuel-vitorino/sopro" rel="nofollow"><img src="https://camo.githubusercontent.com/a05e88b73ccd0b19d358a98085401f5db500c52c9bdaba999119ccd6577f65ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67466163652d4d6f64656c2d6f72616e67653f6c6f676f3d68756767696e6766616365" alt="Alt Text" data-canonical-src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface"></a></p>
<p dir="auto">Sopro (from the Portuguese word for “breath/blow”) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (à la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it’s a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.</p>
<p dir="auto">Some of the main features are:</p>
<ul dir="auto">
<li><strong>169M parameters</strong></li>
<li><strong>Streaming</strong></li>
<li><strong>Zero-shot voice cloning</strong></li>
<li><strong>0.25 RTF on CPU</strong> (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds</li>
<li><strong>3-12 seconds of reference audio</strong> for voice cloning</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instructions</h2><a id="user-content-instructions" aria-label="Permalink: Instructions" href="#instructions"></a></p>
<p dir="auto">I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, <code>torch==2.6.0</code> (without <code>torchvision</code>) achieves ~3× more performance.</p>
<p dir="auto">(Optional)</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n soprotts python=3.10
conda activate soprotts"><pre>conda create -n soprotts python=3.10
conda activate soprotts</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From PyPI</h3><a id="user-content-from-pypi" aria-label="Permalink: From PyPI" href="#from-pypi"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">From the repo</h3><a id="user-content-from-the-repo" aria-label="Permalink: From the repo" href="#from-the-repo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/samuel-vitorino/sopro
cd sopro
pip install -e ."><pre>git clone https://github.com/samuel-vitorino/sopro
<span>cd</span> sopro
pip install -e <span>.</span></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CLI</h3><a id="user-content-cli" aria-label="Permalink: CLI" href="#cli"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="soprotts \
  --text &quot;Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU.&quot; \
  --ref_audio ref.wav \
  --out out.wav"><pre>soprotts \
  --text <span><span>"</span>Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU.<span>"</span></span> \
  --ref_audio ref.wav \
  --out out.wav</pre></div>
<p dir="auto">You have the expected <code>temperature</code> and <code>top_p</code> parameters, alongside:</p>
<ul dir="auto">
<li><code>--style_strength</code> (controls the FiLM strength; increasing it can improve or reduce voice similarity; default <code>1.0</code>)</li>
<li><code>--no_stop_head</code> to disable early stopping</li>
<li><code>--stop_threshold</code> and <code>--stop_patience</code> (number of consecutive frames that must be classified as final before <strong>stopping</strong>). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Python</h3><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Non-streaming</h4><a id="user-content-non-streaming" aria-label="Permalink: Non-streaming" href="#non-streaming"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from sopro import SoproTTS

tts = SoproTTS.from_pretrained(&quot;samuel-vitorino/sopro&quot;, device=&quot;cpu&quot;)

wav = tts.synthesize(
    &quot;Hello! This is a non-streaming Sopro TTS example.&quot;,
    ref_audio_path=&quot;ref.wav&quot;,
)

tts.save_wav(&quot;out.wav&quot;, wav)"><pre><span>from</span> <span>sopro</span> <span>import</span> <span>SoproTTS</span>

<span>tts</span> <span>=</span> <span>SoproTTS</span>.<span>from_pretrained</span>(<span>"samuel-vitorino/sopro"</span>, <span>device</span><span>=</span><span>"cpu"</span>)

<span>wav</span> <span>=</span> <span>tts</span>.<span>synthesize</span>(
    <span>"Hello! This is a non-streaming Sopro TTS example."</span>,
    <span>ref_audio_path</span><span>=</span><span>"ref.wav"</span>,
)

<span>tts</span>.<span>save_wav</span>(<span>"out.wav"</span>, <span>wav</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Streaming</h4><a id="user-content-streaming" aria-label="Permalink: Streaming" href="#streaming"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from sopro import SoproTTS

tts = SoproTTS.from_pretrained(&quot;samuel-vitorino/sopro&quot;, device=&quot;cpu&quot;)

chunks = []
for chunk in tts.stream(
    &quot;Hello! This is a streaming Sopro TTS example.&quot;,
    ref_audio_path=&quot;ref.mp3&quot;,
):
    chunks.append(chunk.cpu())

wav = torch.cat(chunks, dim=-1)
tts.save_wav(&quot;out_stream.wav&quot;, wav)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>sopro</span> <span>import</span> <span>SoproTTS</span>

<span>tts</span> <span>=</span> <span>SoproTTS</span>.<span>from_pretrained</span>(<span>"samuel-vitorino/sopro"</span>, <span>device</span><span>=</span><span>"cpu"</span>)

<span>chunks</span> <span>=</span> []
<span>for</span> <span>chunk</span> <span>in</span> <span>tts</span>.<span>stream</span>(
    <span>"Hello! This is a streaming Sopro TTS example."</span>,
    <span>ref_audio_path</span><span>=</span><span>"ref.mp3"</span>,
):
    <span>chunks</span>.<span>append</span>(<span>chunk</span>.<span>cpu</span>())

<span>wav</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>chunks</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)
<span>tts</span>.<span>save_wav</span>(<span>"out_stream.wav"</span>, <span>wav</span>)</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive streaming demo</h2><a id="user-content-interactive-streaming-demo" aria-label="Permalink: Interactive streaming demo" href="#interactive-streaming-demo"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/44442720/532988339-a1902bb9-734c-4da8-ad0d-f842fb7da370.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MTg5MDIsIm5iZiI6MTc2NzkxODYwMiwicGF0aCI6Ii80NDQ0MjcyMC81MzI5ODgzMzktYTE5MDJiYjktNzM0Yy00ZGE4LWFkMGQtZjg0MmZiN2RhMzcwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDAwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3YmQyNWIxMTdhNjU5ZWNhYTMxNzVlODFhY2U1ZmRjYTY5NGJiNDRkYTAwOTVhM2M4YTFmN2U5ZWIyNWYyY2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NV8g1m1WKscy_4Dhscg2m-7khtdFkUP0Lh5G_v7ae3s"><img src="https://private-user-images.githubusercontent.com/44442720/532988339-a1902bb9-734c-4da8-ad0d-f842fb7da370.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MTg5MDIsIm5iZiI6MTc2NzkxODYwMiwicGF0aCI6Ii80NDQ0MjcyMC81MzI5ODgzMzktYTE5MDJiYjktNzM0Yy00ZGE4LWFkMGQtZjg0MmZiN2RhMzcwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDAwMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3YmQyNWIxMTdhNjU5ZWNhYTMxNzVlODFhY2U1ZmRjYTY5NGJiNDRkYTAwOTVhM2M4YTFmN2U5ZWIyNWYyY2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NV8g1m1WKscy_4Dhscg2m-7khtdFkUP0Lh5G_v7ae3s" alt="Screenshot"></a></p>
<p dir="auto">After you install the <code>sopro</code> package:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000"><pre>pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000</pre></div>
<p dir="auto">Or with docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -t sopro-demo .
docker run --rm -p 8000:8000 sopro-demo"><pre>docker build -t sopro-demo <span>.</span>
docker run --rm -p 8000:8000 sopro-demo</pre></div>
<p dir="auto">Navigate to <a href="http://localhost:8000/" rel="nofollow">http://localhost:8000</a> on your browser.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimers</h2><a id="user-content-disclaimers" aria-label="Permalink: Disclaimers" href="#disclaimers"></a></p>
<ul dir="auto">
<li>Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.</li>
<li>Voice cloning is <strong>highly dependent</strong> on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.</li>
<li>Prefer phonemes instead of abbreviations and symbols. For example, <code>“1 + 2”</code> → <code>“1 plus 2”</code>. That said, Sopro can generally read abbreviations like “CPU”, “TTS”, etc.</li>
<li>The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.</li>
<li>If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.</li>
<li>I will publish the training code once I have time to organize it.</li>
</ul>
<p dir="auto">Due to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.</p>
<p dir="auto">I didn't lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.</p>
<p dir="auto">Currently, generation is limited to <strong>~32 seconds (400 frames)</strong>. You can increase it, but the model generally hallucinates beyond that.</p>
<p dir="auto">AI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.</p>
<p dir="auto">I would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: <a href="https://buymeacoffee.com/samuelvitorino" rel="nofollow">https://buymeacoffee.com/samuelvitorino</a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training data</h2><a id="user-content-training-data" aria-label="Permalink: Training data" href="#training-data"></a></p>
<ul dir="auto">
<li><a href="https://huggingface.co/datasets/amphion/Emilia-Dataset" rel="nofollow">Emilia YODAS</a></li>
<li><a href="https://huggingface.co/datasets/mythicinfinity/libritts_r" rel="nofollow">LibriTTS-R</a></li>
<li><a href="https://datacollective.mozillafoundation.org/" rel="nofollow">Mozilla Common Voice 22</a></li>
<li><a href="https://huggingface.co/datasets/parler-tts/mls_eng_10k" rel="nofollow">MLS</a></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://huggingface.co/kyutai/mimi" rel="nofollow">Mimi Codec (Kyutai)</a></li>
<li><a href="https://arxiv.org/abs/1609.03499" rel="nofollow">WaveNet</a></li>
<li><a href="https://arxiv.org/abs/1803.10963" rel="nofollow">Attentive Stats Pooling</a></li>
<li><a href="https://arxiv.org/pdf/2209.03143" rel="nofollow">AudioLM</a></li>
<li><a href="https://github.com/SesameAILabs/csm">CSM</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Code Claude Code in 200 Lines of Code (620 pts)]]></title>
            <link>https://www.mihaileric.com/The-Emperor-Has-No-Clothes/</link>
            <guid>46545620</guid>
            <pubDate>Thu, 08 Jan 2026 19:54:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mihaileric.com/The-Emperor-Has-No-Clothes/">https://www.mihaileric.com/The-Emperor-Has-No-Clothes/</a>, See on <a href="https://news.ycombinator.com/item?id=46545620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-reactid="11"><p>
  <a href="https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="emperor has no clothes" title="" src="https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg" srcset="https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-f3581.jpeg 240w,
https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-b0573.jpeg 480w,
https://www.mihaileric.com/static/emperor_no_clothes-74ca6bae4decd3746d45fec249cad830-36f83.jpeg 700w" sizes="(max-width: 700px) 100vw, 700px">
    </span>
  </span>
  
  </a>
    </p>
<p>Today AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code. </p>
<p>But here’s the thing: the core of these tools isn’t magic. It’s about 200 lines of straightforward Python. </p>
<p>Let’s build a functional coding agent from scratch.</p>
<h2>The Mental Model</h2>
<p>Before we write any code, let’s understand what’s actually happening when you use a coding agent. It’s essentially just a conversation with a powerful LLM that has a toolbox. </p>
<ol>
<li><strong>You</strong> send a message (“Create a new file with a hello world function”)</li>
<li><strong>The LLM</strong> decides it needs a tool and responds with a structured tool call (or multiple tool calls)</li>
<li><strong>Your program</strong> executes that tool call locally (actually creates the file)</li>
<li><strong>The result</strong> gets sent back to the LLM</li>
<li><strong>The LLM</strong> uses that context to continue or respond</li>
</ol>
<p>That’s the whole loop. The LLM never actually touches your filesystem. It just <em>asks</em> for things to happen, and your code makes them happen.</p>
<h2>Three Tools You Need</h2>
<p>Our coding agent fundamentally needs three capabilities:</p>
<ul>
<li><strong>Read files</strong> so the LLM can see your code</li>
<li><strong>List files</strong> so it can navigate your project</li>
<li><strong>Edit files</strong> so it can give the directive to create and modify code</li>
</ul>
<p>That’s it. Production agents like Claude Code have a few more capabilities including <code>grep</code>, <code>bash</code>, <code>websearch</code>, etc but for our purposes we’ll see that three tools is sufficient to do incredible things.</p>
<h2>Setting Up the Scaffolding</h2>
<p>We start with basic imports and an API client. I’m using OpenAI here, but this works with any LLM provider:</p>
<div>
      <pre><code><span>import</span> inspect
<span>import</span> json
<span>import</span> os

<span>import</span> anthropic
<span>from</span> dotenv <span>import</span> load_dotenv
<span>from</span> pathlib <span>import</span> Path
<span>from</span> typing <span>import</span> Any<span>,</span> Dict<span>,</span> List<span>,</span> Tuple

load_dotenv<span>(</span><span>)</span>

claude_client <span>=</span> anthropic<span>.</span>Anthropic<span>(</span>api_key<span>=</span>os<span>.</span>environ<span>[</span><span>"ANTHROPIC_API_KEY"</span><span>]</span><span>)</span></code></pre>
      </div>
<p>Some terminal colors to make outputs readable:</p>
<div>
      <pre><code>YOU_COLOR <span>=</span> <span>"\u001b[94m"</span>
ASSISTANT_COLOR <span>=</span> <span>"\u001b[93m"</span>
RESET_COLOR <span>=</span> <span>"\u001b[0m"</span></code></pre>
      </div>
<p>And a utility to resolve file paths (so <code>file.py</code> becomes <code>/Users/you/project/file.py</code>):</p>
<div>
      <pre><code><span>def</span> <span>resolve_abs_path</span><span>(</span>path_str<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Path<span>:</span>
    <span>"""
    file.py -&gt; /Users/you/project/file.py
    """</span>
    path <span>=</span> Path<span>(</span>path_str<span>)</span><span>.</span>expanduser<span>(</span><span>)</span>
    <span>if</span> <span>not</span> path<span>.</span>is_absolute<span>(</span><span>)</span><span>:</span>
        path <span>=</span> <span>(</span>Path<span>.</span>cwd<span>(</span><span>)</span> <span>/</span> path<span>)</span><span>.</span>resolve<span>(</span><span>)</span>
    <span>return</span> path</code></pre>
      </div>
<h2>Implementing the Tools</h2>
<p>Note you should be detailed about your tool function docstrings as they will be used by the LLM to reason about what tools should be called during the conversation. More on this below. </p>
<h3>Tool 1: Read File</h3>
<p>The simplest tool. Take a filename, return its contents:</p>
<div>
      <pre><code><span>def</span> <span>read_file_tool</span><span>(</span>filename<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>"""
    Gets the full content of a file provided by the user.
    :param filename: The name of the file to read.
    :return: The full content of the file.
    """</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>filename<span>)</span>
    <span>print</span><span>(</span>full_path<span>)</span>
    <span>with</span> <span>open</span><span>(</span><span>str</span><span>(</span>full_path<span>)</span><span>,</span> <span>"r"</span><span>)</span> <span>as</span> f<span>:</span>
        content <span>=</span> f<span>.</span>read<span>(</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>"file_path"</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>"content"</span><span>:</span> content
    <span>}</span></code></pre>
      </div>
<p>We return a dictionary because the LLM needs structured context about what happened.</p>
<h3>Tool 2: List Files</h3>
<p>Navigate directories by listing their contents:</p>
<div>
      <pre><code><span>def</span> <span>list_files_tool</span><span>(</span>path<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>"""
    Lists the files in a directory provided by the user.
    :param path: The path to a directory to list files from.
    :return: A list of files in the directory.
    """</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>path<span>)</span>
    all_files <span>=</span> <span>[</span><span>]</span>
    <span>for</span> item <span>in</span> full_path<span>.</span>iterdir<span>(</span><span>)</span><span>:</span>
        all_files<span>.</span>append<span>(</span><span>{</span>
            <span>"filename"</span><span>:</span> item<span>.</span>name<span>,</span>
            <span>"type"</span><span>:</span> <span>"file"</span> <span>if</span> item<span>.</span>is_file<span>(</span><span>)</span> <span>else</span> <span>"dir"</span>
        <span>}</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>"path"</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>"files"</span><span>:</span> all_files
    <span>}</span></code></pre>
      </div>
<h3>Tool 3: Edit File</h3>
<p>This is the most complex tool, but still straightforward. It handles two cases:</p>
<ul>
<li><strong>Creating a new file</strong> when <code>old_str</code> is empty</li>
<li><strong>Replacing text</strong> by finding <code>old_str</code> and replacing with <code>new_str</code></li>
</ul>
<div>
      <pre><code><span>def</span> <span>edit_file_tool</span><span>(</span>path<span>:</span> <span>str</span><span>,</span> old_str<span>:</span> <span>str</span><span>,</span> new_str<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>:</span>
    <span>"""
    Replaces first occurrence of old_str with new_str in file. If old_str is empty,
    create/overwrite file with new_str.
    :param path: The path to the file to edit.
    :param old_str: The string to replace.
    :param new_str: The string to replace with.
    :return: A dictionary with the path to the file and the action taken.
    """</span>
    full_path <span>=</span> resolve_abs_path<span>(</span>path<span>)</span>
    <span>if</span> old_str <span>==</span> <span>""</span><span>:</span>
        full_path<span>.</span>write_text<span>(</span>new_str<span>,</span> encoding<span>=</span><span>"utf-8"</span><span>)</span>
        <span>return</span> <span>{</span>
            <span>"path"</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
            <span>"action"</span><span>:</span> <span>"created_file"</span>
        <span>}</span>
    original <span>=</span> full_path<span>.</span>read_text<span>(</span>encoding<span>=</span><span>"utf-8"</span><span>)</span>
    <span>if</span> original<span>.</span>find<span>(</span>old_str<span>)</span> <span>==</span> <span>-</span><span>1</span><span>:</span>
        <span>return</span> <span>{</span>
            <span>"path"</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
            <span>"action"</span><span>:</span> <span>"old_str not found"</span>
        <span>}</span>
    edited <span>=</span> original<span>.</span>replace<span>(</span>old_str<span>,</span> new_str<span>,</span> <span>1</span><span>)</span>
    full_path<span>.</span>write_text<span>(</span>edited<span>,</span> encoding<span>=</span><span>"utf-8"</span><span>)</span>
    <span>return</span> <span>{</span>
        <span>"path"</span><span>:</span> <span>str</span><span>(</span>full_path<span>)</span><span>,</span>
        <span>"action"</span><span>:</span> <span>"edited"</span>
    <span>}</span></code></pre>
      </div>
<p>The convention here: empty <code>old_str</code> means “create this file.” Otherwise, find and replace. Real IDEs add sophisticated fallback behavior when the string isn’t found, but this works.</p>
<h2>The Tool Registry</h2>
<p>We need a way to look up tools by name:</p>
<div>
      <pre><code>TOOL_REGISTRY <span>=</span> <span>{</span>
    <span>"read_file"</span><span>:</span> read_file_tool<span>,</span>
    <span>"list_files"</span><span>:</span> list_files_tool<span>,</span>
    <span>"edit_file"</span><span>:</span> edit_file_tool 
<span>}</span></code></pre>
      </div>
<h2>Teaching the LLM About Our Tools</h2>
<p>The LLM needs to know what tools exist and how to call them. We generate this dynamically from our function signatures and docstrings:</p>
<div>
      <pre><code><span>def</span> <span>get_tool_str_representation</span><span>(</span>tool_name<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> <span>str</span><span>:</span>
    tool <span>=</span> TOOL_REGISTRY<span>[</span>tool_name<span>]</span>
    <span>return</span> f<span>"""
    Name: {tool_name}
    Description: {tool.__doc__}
    Signature: {inspect.signature(tool)}
    """</span>

<span>def</span> <span>get_full_system_prompt</span><span>(</span><span>)</span><span>:</span>
    tool_str_repr <span>=</span> <span>""</span>
    <span>for</span> tool_name <span>in</span> TOOL_REGISTRY<span>:</span>
        tool_str_repr <span>+=</span> <span>"TOOL\n==="</span> <span>+</span> get_tool_str_representation<span>(</span>tool_name<span>)</span>
        tool_str_repr <span>+=</span> f<span>"\n{'='*15}\n"</span>
    <span>return</span> SYSTEM_PROMPT<span>.</span><span>format</span><span>(</span>tool_list_repr<span>=</span>tool_str_repr<span>)</span></code></pre>
      </div>
<p>And the system prompt itself:</p>
<div>
      <pre><code>SYSTEM_PROMPT <span>=</span> <span>"""
You are a coding assistant whose goal it is to help us solve coding tasks. 
You have access to a series of tools you can execute. Here are the tools you can execute:

{tool_list_repr}

When you want to use a tool, reply with exactly one line in the format: 'tool: TOOL_NAME({{JSON_ARGS}})' and nothing else.
Use compact single-line JSON with double quotes. After receiving a tool_result(...) message, continue the task.
If no tool is needed, respond normally.
"""</span></code></pre>
      </div>
<p>This is the key insight: we’re just telling the LLM “here are your tools, here’s the format to call them.” The LLM figures out when and how to use them.</p>
<h2>Parsing Tool Calls</h2>
<p>When the LLM responds, we need to detect if it’s asking us to run a tool:</p>
<div>
      <pre><code><span>def</span> <span>extract_tool_invocations</span><span>(</span>text<span>:</span> <span>str</span><span>)</span> <span>-</span><span>&gt;</span> List<span>[</span>Tuple<span>[</span><span>str</span><span>,</span> Dict<span>[</span><span>str</span><span>,</span> Any<span>]</span><span>]</span><span>]</span><span>:</span>
    <span>"""
    Return list of (tool_name, args) requested in 'tool: name({...})' lines.
    The parser expects single-line, compact JSON in parentheses.
    """</span>
    invocations <span>=</span> <span>[</span><span>]</span>
    <span>for</span> raw_line <span>in</span> text<span>.</span>splitlines<span>(</span><span>)</span><span>:</span>
        line <span>=</span> raw_line<span>.</span>strip<span>(</span><span>)</span>
        <span>if</span> <span>not</span> line<span>.</span>startswith<span>(</span><span>"tool:"</span><span>)</span><span>:</span>
            <span>continue</span>
        <span>try</span><span>:</span>
            after <span>=</span> line<span>[</span><span>len</span><span>(</span><span>"tool:"</span><span>)</span><span>:</span><span>]</span><span>.</span>strip<span>(</span><span>)</span>
            name<span>,</span> rest <span>=</span> after<span>.</span>split<span>(</span><span>"("</span><span>,</span> <span>1</span><span>)</span>
            name <span>=</span> name<span>.</span>strip<span>(</span><span>)</span>
            <span>if</span> <span>not</span> rest<span>.</span>endswith<span>(</span><span>")"</span><span>)</span><span>:</span>
                <span>continue</span>
            json_str <span>=</span> rest<span>[</span><span>:</span><span>-</span><span>1</span><span>]</span><span>.</span>strip<span>(</span><span>)</span>
            args <span>=</span> json<span>.</span>loads<span>(</span>json_str<span>)</span>
            invocations<span>.</span>append<span>(</span><span>(</span>name<span>,</span> args<span>)</span><span>)</span>
        <span>except</span> Exception<span>:</span>
            <span>continue</span>
    <span>return</span> invocations</code></pre>
      </div>
<p>Simple text parsing. Look for lines starting with <code>tool:</code>, extract the function name and JSON arguments.</p>
<h2>The LLM Call</h2>
<p>A thin wrapper around the API:</p>
<div>
      <pre><code><span>def</span> <span>execute_llm_call</span><span>(</span>conversation<span>:</span> List<span>[</span>Dict<span>[</span><span>str</span><span>,</span> <span>str</span><span>]</span><span>]</span><span>)</span><span>:</span>
    system_content <span>=</span> <span>""</span>
    messages <span>=</span> <span>[</span><span>]</span>
    
    <span>for</span> msg <span>in</span> conversation<span>:</span>
        <span>if</span> msg<span>[</span><span>"role"</span><span>]</span> <span>==</span> <span>"system"</span><span>:</span>
            system_content <span>=</span> msg<span>[</span><span>"content"</span><span>]</span>
        <span>else</span><span>:</span>
            messages<span>.</span>append<span>(</span>msg<span>)</span>
    
    response <span>=</span> claude_client<span>.</span>messages<span>.</span>create<span>(</span>
        model<span>=</span><span>"claude-sonnet-4-20250514"</span><span>,</span>
        max_tokens<span>=</span><span>2000</span><span>,</span>
        system<span>=</span>system_content<span>,</span>
        messages<span>=</span>messages
    <span>)</span>
    <span>return</span> response<span>.</span>content<span>[</span><span>0</span><span>]</span><span>.</span>text</code></pre>
      </div>
<h2>The Agent Loop</h2>
<p>Now we put it all together. This is where the “magic” happens:</p>
<div>
      <pre><code><span>def</span> <span>run_coding_agent_loop</span><span>(</span><span>)</span><span>:</span>
    <span>print</span><span>(</span>get_full_system_prompt<span>(</span><span>)</span><span>)</span>
    conversation <span>=</span> <span>[</span><span>{</span>
        <span>"role"</span><span>:</span> <span>"system"</span><span>,</span>
        <span>"content"</span><span>:</span> get_full_system_prompt<span>(</span><span>)</span>
    <span>}</span><span>]</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>try</span><span>:</span>
            user_input <span>=</span> <span>input</span><span>(</span>f<span>"{YOU_COLOR}You:{RESET_COLOR}:"</span><span>)</span>
        <span>except</span> <span>(</span>KeyboardInterrupt<span>,</span> EOFError<span>)</span><span>:</span>
            <span>break</span>
        conversation<span>.</span>append<span>(</span><span>{</span>
            <span>"role"</span><span>:</span> <span>"user"</span><span>,</span>
            <span>"content"</span><span>:</span> user_input<span>.</span>strip<span>(</span><span>)</span>
        <span>}</span><span>)</span>
        <span>while</span> <span>True</span><span>:</span>
            assistant_response <span>=</span> execute_llm_call<span>(</span>conversation<span>)</span>
            tool_invocations <span>=</span> extract_tool_invocations<span>(</span>assistant_response<span>)</span>
            <span>if</span> <span>not</span> tool_invocations<span>:</span>
                <span>print</span><span>(</span>f<span>"{ASSISTANT_COLOR}Assistant:{RESET_COLOR}: {assistant_response}"</span><span>)</span>
                conversation<span>.</span>append<span>(</span><span>{</span>
                    <span>"role"</span><span>:</span> <span>"assistant"</span><span>,</span>
                    <span>"content"</span><span>:</span> assistant_response
                <span>}</span><span>)</span>
                <span>break</span>
            <span>for</span> name<span>,</span> args <span>in</span> tool_invocations<span>:</span>
                tool <span>=</span> TOOL_REGISTRY<span>[</span>name<span>]</span>
                resp <span>=</span> <span>""</span>
                <span>print</span><span>(</span>name<span>,</span> args<span>)</span>
                <span>if</span> name <span>==</span> <span>"read_file"</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>"filename"</span><span>,</span> <span>"."</span><span>)</span><span>)</span>
                <span>elif</span> name <span>==</span> <span>"list_files"</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>"path"</span><span>,</span> <span>"."</span><span>)</span><span>)</span>
                <span>elif</span> name <span>==</span> <span>"edit_file"</span><span>:</span>
                    resp <span>=</span> tool<span>(</span>args<span>.</span>get<span>(</span><span>"path"</span><span>,</span> <span>"."</span><span>)</span><span>,</span> 
                                args<span>.</span>get<span>(</span><span>"old_str"</span><span>,</span> <span>""</span><span>)</span><span>,</span> 
                                args<span>.</span>get<span>(</span><span>"new_str"</span><span>,</span> <span>""</span><span>)</span><span>)</span>
                conversation<span>.</span>append<span>(</span><span>{</span>
                    <span>"role"</span><span>:</span> <span>"user"</span><span>,</span>
                    <span>"content"</span><span>:</span> f<span>"tool_result({json.dumps(resp)})"</span>
                <span>}</span><span>)</span></code></pre>
      </div>
<p>The structure:</p>
<ol>
<li><strong>Outer loop</strong>: Get user input, add to conversation</li>
<li>
<p><strong>Inner loop</strong>: Call LLM, check for tool invocations</p>
<ul>
<li>If no tools needed, print response and break inner loop</li>
<li>If tools needed, execute them, add results to conversation, loop again</li>
</ul>
</li>
</ol>
<p>The inner loop continues until the LLM responds without requesting any tools. This lets the agent chain multiple tool calls (read a file, then edit it, then confirm the edit).</p>
<h2>Running It</h2>
<div>
      <pre><code><span>if</span> __name__ <span>==</span> <span>"__main__"</span><span>:</span>
    run_coding_agent_loop<span>(</span><span>)</span></code></pre>
      </div>
<p>Now you can have conversations like:</p>
<blockquote>
<p><strong>You:</strong> Make me a new file called hello.py and implement hello world in it</p>
</blockquote>
<p>Agent calls <em>edit_file</em> with path=“hello.py”, old_str="", new_str=“print(‘Hello World’)”</p>
<blockquote>
<p><strong>Assistant:</strong> Done! Created hello.py with a hello world implementation.</p>
</blockquote>
<p>Or multi-step interactions:</p>
<blockquote>
<p><strong>You:</strong> Edit hello.py and add a function for multiplying two numbers</p>
</blockquote>
<p>Agent calls <em>read_file</em> to see current contents. Agent calls <em>edit_file</em> to add the function.</p>
<blockquote>
<p><strong>Assistant:</strong> Added a multiply function to hello.py.</p>
</blockquote>
<h2>What We Built vs. Production Tools</h2>
<p>This is about 200 lines. Production tools like Claude Code add:</p>
<ul>
<li><strong>Better error handling</strong> and fallback behaviors</li>
<li><strong>Streaming responses</strong> for better UX</li>
<li><strong>Smarter context management</strong> (summarizing long files, etc.)</li>
<li><strong>More tools</strong> (run commands, search codebase, etc.)</li>
<li><strong>Approval workflows</strong> for destructive operations</li>
</ul>
<p>But the core loop? It’s exactly what we built here. The LLM decides what to do, your code executes it, results flow back. That’s the whole architecture.</p>
<h2>Try It Yourself</h2>
<p>The <a href="https://shorturl.at/HmMeI">full source</a> is about 200 lines. Swap in your preferred LLM provider, adjust the system prompt, add more tools as an exercise. You’ll be surprised how capable this simple pattern is.</p>
<p><em>If you’re interested in learning state-of-the-art AI software development techniques for professional engineers, check out my <a href="https://maven.com/the-modern-software-developer/ai-course">online course</a>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google AI Studio is now sponsoring Tailwind CSS (712 pts)]]></title>
            <link>https://twitter.com/OfficialLoganK/status/2009339263251566902</link>
            <guid>46545077</guid>
            <pubDate>Thu, 08 Jan 2026 19:09:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/OfficialLoganK/status/2009339263251566902">https://twitter.com/OfficialLoganK/status/2009339263251566902</a>, See on <a href="https://news.ycombinator.com/item?id=46545077">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><p><img alt="⚠️" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Unreasonable Effectiveness of the Fourier Transform (269 pts)]]></title>
            <link>https://joshuawise.com/resources/ofdm/</link>
            <guid>46544981</guid>
            <pubDate>Thu, 08 Jan 2026 19:00:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshuawise.com/resources/ofdm/">https://joshuawise.com/resources/ofdm/</a>, See on <a href="https://news.ycombinator.com/item?id=46544981">Hacker News</a></p>
<div id="readability-page-1" class="page">


<p><b><i>Notes from <a href="https://joshuawise.com/">Joshua Wise</a>'s talk at <a href="https://www.crowdsupply.com/teardown/portland-2025">Teardown 2025</a>.</i></b></p>

<p><img src="https://joshuawise.com/resources/ofdm/title.png"></p>

<p><b>New</b>: You can now watch a recording of my talk <a href="https://youtu.be/9k1Wu69Gw4w">on my YouTube channel</a>!  Or you can
just click "play" below, I suppose.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/9k1Wu69Gw4w?si=5mC7Ehmy065Knpd-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>


<p>Here are a few resources from my talk.</p>

<ul>
<li>Here is <a href="https://joshuawise.com/resources/ofdm/jwise-ofdm-teardown.pdf">a PDF of my slides</a>, if you
wanted to refer to anything in specific.</li>
<li>Here is <a href="https://joshuawise.com/resources/ofdm/teardown_2025_talk.ipynb">the Jupyter notebook</a> that I
used to produce all of the zillions of plots.  I do not claim that it is
good code, but it is code.</li>
<li>The OFDM patent is <a href="https://patents.google.com/patent/US3488445A/en">US3488445A</a>, filed
in 1966, expired in 1987.</li>
<li>Here is <a href="https://doi.org/10.1002/cpa.3160130102">Eugene Wigner's
original discussion, "The Unreasonable Effectiveness of Mathematics in the
Natural Sciences"</a>.  There are many good follow-ons to this, too.</li>
<li>Here is <a href="https://ieeexplore.ieee.org/document/599949">the paper
on how to estimate both carrier offset <i>and</i> time offset <i>at the same
time</i></a>.  I implemented it by typing in the algorithm, and it worked, but if you
understand it and can explain it to me please let me know.</li>
<li>Here is <a href="https://github.com/jwise/dvbt">the DVB-T decoder that I
wrote</a>.  I do not claim that it is the right way to do any of these
things, but it is a way to do these things.</li>
<li>Finally, here is <a href="https://www.youtube.com/watch?v=h7apO7q16V0">an absolutely fantastic
video that breaks down the implementation of the <i>Fast</i> Fourier
Transform algorithm</a>.  I watch it every year or two.</li>
</ul>

<p>Thanks so much for coming!  Please let me know if you have feedback on
this.  I'd love to hear what you thought.</p>

<ul>
<li><b>Joshua Wise, June 2025</b></li>
<li><i>Personal:</i> joshua@joshuawise.com (or <a href="https://joshuawise.com/">https://joshuawise.com/</a>)</li>
<li><i>Work:</i> joshua@accelerated.tech (or <a href="https://accelerated.tech/">https://accelerated.tech</a>)</li>
<li><i>Fediverse:</i> <a href="https://social.emarhavil.com/joshua">@joshua@social.emarhavil.com</a></li>
</ul>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fixing a Buffer Overflow in Unix v4 Like It's 1973 (146 pts)]]></title>
            <link>https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/</link>
            <guid>46544610</guid>
            <pubDate>Thu, 08 Jan 2026 18:29:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/">https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/</a>, See on <a href="https://news.ycombinator.com/item?id=46544610">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="introduction">Introduction</h2><p>In 2025, the only known copy of UNIX v4 surfaced on a magnetic tape<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
This version marks a pivotal moment in computer history: the rewriting of UNIX into C.
Enthusiasts quickly recovered the data and successfully ran the system on a PDP-11 simulator<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p><p>Fascinated by this artifact, I set up an instance to explore it.
Because the distribution includes the source code, I examined the implementation of several core utilities.
While auditing the <code>su(1)</code> program, I identified a bug. Let’s fix it.</p><h2 id="the-unix-v4-su1-program">The UNIX v4 su(1) program</h2><p>Although more than 50 years old, the <code>su</code> program functions similarly to its modern variant.
As a setuid-root executable, it validates the root password.
If the user provides the correct credentials, the program spawns a root shell, allowing an unprivileged user to escalate privileges.</p><p>The source file, <code>su.c</code>, contains fewer than 50 lines of code.</p><div><pre tabindex="0"><code data-lang="C"><span><span><span>/* su -- become super-user */</span>
</span></span><span><span>
</span></span><span><span><span>char</span>    <span>password</span><span>[</span><span>100</span><span>];</span>
</span></span><span><span><span>char</span>    <span>pwbuf</span><span>[</span><span>100</span><span>];</span>
</span></span><span><span><span>int</span>     <span>ttybuf</span><span>[</span><span>3</span><span>];</span>
</span></span><span><span><span>main</span><span>()</span>
</span></span><span><span><span>{</span>
</span></span><span><span>        <span>register</span> <span>char</span> <span>*</span><span>p</span><span>,</span> <span>*</span><span>q</span><span>;</span>
</span></span><span><span>        <span>extern</span> <span>fin</span><span>;</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span><span>(</span><span>getpw</span><span>(</span><span>0</span><span>,</span> <span>pwbuf</span><span>))</span>
</span></span><span><span>                <span>goto</span> <span>badpw</span><span>;</span>
</span></span><span><span>        <span>(</span><span>&amp;</span><span>fin</span><span>)[</span><span>1</span><span>]</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>        <span>p</span> <span>=</span> <span>pwbuf</span><span>;</span>
</span></span><span><span>        <span>while</span><span>(</span><span>*</span><span>p</span> <span>!=</span> <span>':'</span><span>)</span>
</span></span><span><span>                <span>if</span><span>(</span><span>*</span><span>p</span><span>++</span> <span>==</span> <span>'\0'</span><span>)</span>
</span></span><span><span>                        <span>goto</span> <span>badpw</span><span>;</span>
</span></span><span><span>        <span>if</span><span>(</span><span>*++</span><span>p</span> <span>==</span> <span>':'</span><span>)</span>
</span></span><span><span>                <span>goto</span> <span>ok</span><span>;</span>
</span></span><span><span>        <span>gtty</span><span>(</span><span>0</span><span>,</span> <span>ttybuf</span><span>);</span>
</span></span><span><span>        <span>ttybuf</span><span>[</span><span>2</span><span>]</span> <span>=&amp;</span> <span>~</span><span>010</span><span>;</span>
</span></span><span><span>        <span>stty</span><span>(</span><span>0</span><span>,</span> <span>ttybuf</span><span>);</span>
</span></span><span><span>        <span>printf</span><span>(</span><span>"password: "</span><span>);</span>
</span></span><span><span>        <span>q</span> <span>=</span> <span>password</span><span>;</span>
</span></span><span><span>        <span>while</span><span>((</span><span>*</span><span>q</span> <span>=</span> <span>getchar</span><span>())</span> <span>!=</span> <span>'\n'</span><span>)</span>
</span></span><span><span>                <span>if</span><span>(</span><span>*</span><span>q</span><span>++</span> <span>==</span> <span>'\0'</span><span>)</span>
</span></span><span><span>                        <span>return</span><span>;</span>
</span></span><span><span>        <span>*</span><span>q</span> <span>=</span> <span>'\0'</span><span>;</span>
</span></span><span><span>        <span>ttybuf</span><span>[</span><span>2</span><span>]</span> <span>=|</span> <span>010</span><span>;</span>
</span></span><span><span>        <span>stty</span><span>(</span><span>0</span><span>,</span> <span>ttybuf</span><span>);</span>
</span></span><span><span>        <span>printf</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>);</span>
</span></span><span><span>        <span>q</span> <span>=</span> <span>crypt</span><span>(</span><span>password</span><span>);</span>
</span></span><span><span>        <span>while</span><span>(</span><span>*</span><span>q</span><span>++</span> <span>==</span> <span>*</span><span>p</span><span>++</span><span>);</span>
</span></span><span><span>        <span>if</span><span>(</span><span>*--</span><span>q</span> <span>==</span> <span>'\0'</span> <span>&amp;&amp;</span> <span>*--</span><span>p</span> <span>==</span> <span>':'</span><span>)</span>
</span></span><span><span>                <span>goto</span> <span>ok</span><span>;</span>
</span></span><span><span>        <span>goto</span> <span>error</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>badpw</span><span>:</span>
</span></span><span><span>        <span>printf</span><span>(</span><span>"bad password file</span><span>\n</span><span>"</span><span>);</span>
</span></span><span><span><span>ok</span><span>:</span>
</span></span><span><span>        <span>setuid</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>        <span>execl</span><span>(</span><span>"/bin/sh"</span><span>,</span> <span>"-"</span><span>,</span> <span>0</span><span>);</span>
</span></span><span><span>        <span>printf</span><span>(</span><span>"cannot execute shell</span><span>\n</span><span>"</span><span>);</span>
</span></span><span><span><span>error</span><span>:</span>
</span></span><span><span>        <span>printf</span><span>(</span><span>"sorry</span><span>\n</span><span>"</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>In short, the program executes the following steps:</p><ol><li>It calls <code>getpw()</code> to retrieve the passwd entry for the root user (UID 0) from <code>/etc/passwd</code>. Surprisingly, if the read fails or the line format is incorrect, <code>su</code> continues execution rather than aborting. While unusual, this likely acts as a safeguard to ensure <code>su</code> remains usable on a partially corrupted system. This is a security issue on its own because an unprivileged user could consume enough resources to make the <code>getpw()</code> call fail. Ron Natalie pointed<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> out that this attack vector was known at the time.</li><li>It disables the TTY echo mode and prompts the user for a password.</li><li>It reads byte-by-byte from the TTY into a buffer until it encounters a newline or <code>NUL</code> character, <code>NUL</code> causes the program to exit immediately.</li><li>Once reading is complete, it re-enables echo mode, hashes the input using the <code>crypt()</code> library function, and compares the result with the stored hash.</li><li>If the hashes match, it spawns a shell, otherwise, it terminates.</li></ol><p>The logic is standard, except for one critical flaw: the <code>password</code> buffer has a fixed size of <code>100</code> bytes, yet the input loop lacks a bounds check.
If a user enters more than <code>100</code> characters, a buffer overflow occurs.</p><p>I confirmed this behavior by testing with a long input string, which successfully crashed the program.
Not all long strings trigger a core dump.
The outcome depends on which area of adjacent memory is overwritten, sometimes, <code>su</code> simply exits.</p><pre tabindex="0"><code># su
password:&lt;long input&gt;Memory fault -- Core dumped
</code></pre><p>Note: Because <code>su</code> disables TTY echo mode, a crash prevents the terminal from displaying subsequent input.
To restore visibility, type <code>stty echo</code> blindly and press Enter.</p><h2 id="fixing-su1">Fixing su(1)</h2><p>UNIX traditionally includes the source code necessary for self-recompilation, and v4 is no exception.
This allows us to patch and compile <code>su</code> directly on the system.
In 1973, editor options were sparse. Neither <code>vi</code> nor <code>emacs</code> had been invented yet.
However, the system provides <code>ed</code>, a line-oriented text editor designed for teletype terminals where output was printed on paper rather than displayed on a screen.
<code>ed</code> allows us to list, delete, and append lines, which is sufficient for our needs.</p><p>We will edit <code>su.c</code> to prevent the overflow by maintaining a counter, <code>i</code>, and verifying it against the buffer size during the read loop.
I initially attempted a fix using pointer arithmetic, but the 1973 C compiler didn’t like it, while it didn’t refuse the syntax, the code had no effect.
I settled on a simpler index-based check instead.</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>--- a/s2/su.c
</span></span></span><span><span><span></span><span>+++ b/s2/su.c
</span></span></span><span><span><span></span><span>@@ -7,6 +7,7 @@ main()
</span></span></span><span><span><span></span> {
</span></span><span><span>        register char *p, *q;
</span></span><span><span>        extern fin;
</span></span><span><span><span>+       register int i;
</span></span></span><span><span><span></span> 
</span></span><span><span>        if(getpw(0, pwbuf))
</span></span><span><span>                goto badpw;
</span></span><span><span><span>@@ -22,9 +23,13 @@ main()
</span></span></span><span><span><span></span>        stty(0, ttybuf);
</span></span><span><span>        printf("password: ");
</span></span><span><span>        q = password;
</span></span><span><span><span>-       while((*q = getchar()) != '\n')
</span></span></span><span><span><span></span><span>+       i = 0;
</span></span></span><span><span><span>+       while((*q = getchar()) != '\n') {
</span></span></span><span><span><span>+               if (++i &gt;= sizeof(password))
</span></span></span><span><span><span>+                       goto error;
</span></span></span><span><span><span></span>                if(*q++ == '\0')
</span></span><span><span>                        return;
</span></span><span><span><span>+       }
</span></span></span><span><span><span></span>        *q = '\0';
</span></span><span><span>        ttybuf[2] =| 010;
</span></span><span><span>        stty(0, ttybuf);
</span></span></code></pre></div><pre tabindex="0"><code># chdir /usr/source/s2
# ed su.c
</code></pre><p>Upon launch, <code>ed</code> outputs the file size in bytes and awaits input.
The command <code>i</code> inserts text before the current line, <code>d</code> deletes the line, and <code>p</code> prints it.
Entering a number moves the focus to that specific line, while pressing Return prints the current line’s content.</p><p>Below is a screen recording of the editing session:</p><pre tabindex="0"><code data-lang="ed">741
8
        register char *p, *q;

        extern fin;
i
        register int i;
.
24
        printf("password: ");

        q = password;
i
        i = 0;
.
p
        i = 0;

        while((*q = getchar()) != '\n')
d
i
        while((*q = getchar()) != '\n') {
.

                if(*q++ == '\0')
i
                if (++i &gt;= sizeof(password))
                        goto error;
.

                if(*q++ == '\0')

                        return;

        *q = '\0';
i
        }
.
w
811
q
</code></pre><p>First, we jump to line <code>8</code> and press Return several times to locate a suitable spot for the variable declaration.
We use <code>i</code> to enter insert mode, add the variable, and then type a single period (<code>.</code>) on a new line to exit insert mode.
The critical change occurs around the while loop: we initialize <code>i</code> and add a boundary check to the loop condition.
Finally, <code>w</code> writes the modified buffer to disk, confirming the file has grown by a few bytes, and <code>q</code> terminates the editor.</p><h2 id="building-and-deploying">Building and Deploying</h2><p>With the source code patched, we must rebuild the binary.
Since <code>su</code> consists of a single C file, the compilation process is trivial:</p><pre tabindex="0"><code># cc su.c
</code></pre><p>The compiler outputs a binary named <code>a.out</code>.
To deploy it, we move the file to <code>/bin/su</code>:</p><pre tabindex="0"><code># mv a.out /bin/su
</code></pre><p>However, the installation is incomplete.
Because <code>su</code> requires root privileges to function, we must set the setuid bit and adjust the file permissions:</p><pre tabindex="0"><code># ls -l /bin/su
-rwxrwxrwx 1 root     2740 Jun 12 19:58 /bin/su
# chmod 4755 /bin/su
# ls -l /bin/su
-rwsr-xr-x 1 root     2740 Jun 12 19:58 /bin/su
</code></pre><h2 id="summary">Summary</h2><p>UNIX v4 is a fascinating gem of computer history.
It feels surprisingly similar to our current systems.
While it lacks modern conveniences, the fundamental logic remains recognizable to anyone with modern UNIX experience.</p><p>The ability to fix <code>su</code> so quickly highlights the power of the early UNIX philosophy: shipping the operating system with its full source code and a C compiler.
We patched, compiled, and deployed the fix directly on the system, no external toolchains required.</p><p>Finally, this bug reminds us of the era’s different priorities.
In the trusted, isolated environments of 1973, security was not the critical concern it is today.
Furthermore, the knowledge that a buffer overflow could be exploited for arbitrary code execution had not yet come of age.</p><p>As an exercise for the reader to improve their <code>ed</code> skills, try adding the code to restore TTY echo mode to the overflow detection logic.
This ensures the terminal functions correctly even after the program catches the error.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: macOS menu bar app to track Claude usage in real time (143 pts)]]></title>
            <link>https://github.com/richhickson/claudecodeusage</link>
            <guid>46544524</guid>
            <pubDate>Thu, 08 Jan 2026 18:24:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/richhickson/claudecodeusage">https://github.com/richhickson/claudecodeusage</a>, See on <a href="https://news.ycombinator.com/item?id=46544524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Claude Usage</h2><a id="user-content-claude-usage" aria-label="Permalink: Claude Usage" href="#claude-usage"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/richhickson/claudecodeusage/blob/main/screenshot.png"><img src="https://github.com/richhickson/claudecodeusage/raw/main/screenshot.png" alt="Claude Usage Screenshot" width="300"></a>
</p>
<div dir="auto"><p>A lightweight macOS menubar app that displays your Claude Code usage limits at a glance.
</p><p>
Built by <a href="https://x.com/richhickson" rel="nofollow">@richhickson</a></p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🔄 <strong>Auto-refresh</strong> every 2 minutes</li>
<li>🚦 <strong>Color-coded status</strong> - Green (OK), Yellow (&gt;70%), Red (&gt;90%)</li>
<li>⏱️ <strong>Time until reset</strong> for both session and weekly limits</li>
<li>📊 <strong>Session &amp; Weekly limits</strong> displayed together</li>
<li>🪶 <strong>Lightweight</strong> - Native Swift, minimal resources</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download</h3><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<ol dir="auto">
<li>Go to <a href="https://github.com/richhickson/claudecodeusage/releases">Releases</a></li>
<li>Download <code>ClaudeUsage.zip</code></li>
<li>Unzip and drag <code>ClaudeUsage.app</code> to your Applications folder</li>
<li>Open the app (you may need to right-click → Open the first time)</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build from Source</h3><a id="user-content-build-from-source" aria-label="Permalink: Build from Source" href="#build-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/YOUR_USERNAME/claude-usage.git
cd claude-usage
open ClaudeUsage.xcodeproj"><pre>git clone https://github.com/YOUR_USERNAME/claude-usage.git
<span>cd</span> claude-usage
open ClaudeUsage.xcodeproj</pre></div>
<p dir="auto">Then build with ⌘B and run with ⌘R.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>macOS 13.0 (Ventura) or later</li>
<li>Claude Code CLI installed and logged in</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install <a href="https://claude.ai/code" rel="nofollow">Claude Code</a> if you haven't already:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install -g @anthropic-ai/claude-code"><pre>npm install -g @anthropic-ai/claude-code</pre></div>
</li>
<li>
<p dir="auto">Log in to Claude Code:</p>

</li>
<li>
<p dir="auto">Launch Claude Usage - it will read your credentials from Keychain automatically</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<p dir="auto">Claude Usage reads your Claude Code OAuth credentials from macOS Keychain and queries the usage API endpoint at <code>api.anthropic.com/api/oauth/usage</code>.</p>
<p dir="auto"><strong>Note:</strong> This uses an undocumented API that could change at any time. The app will gracefully handle API changes but may stop working if Anthropic modifies the endpoint.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Privacy</h2><a id="user-content-privacy" aria-label="Permalink: Privacy" href="#privacy"></a></p>
<ul dir="auto">
<li>Your credentials never leave your machine</li>
<li>No analytics or telemetry</li>
<li>No data sent anywhere except Anthropic's API</li>
<li>Open source - verify the code yourself</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Normal</th>
<th>Warning</th>
<th>Critical</th>
</tr>
</thead>
<tbody>
<tr>
<td>🟢 30%</td>
<td>🟡 75%</td>
<td>🔴 95%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">"Not logged in to Claude Code"</h3><a id="user-content-not-logged-in-to-claude-code" aria-label="Permalink: &quot;Not logged in to Claude Code&quot;" href="#not-logged-in-to-claude-code"></a></p>
<p dir="auto">Run <code>claude</code> in Terminal and complete the login flow.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">App doesn't appear in menubar</h3><a id="user-content-app-doesnt-appear-in-menubar" aria-label="Permalink: App doesn't appear in menubar" href="#app-doesnt-appear-in-menubar"></a></p>
<p dir="auto">Check if the app is running in Activity Monitor. Try quitting and reopening.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage shows wrong values</h3><a id="user-content-usage-shows-wrong-values" aria-label="Permalink: Usage shows wrong values" href="#usage-shows-wrong-values"></a></p>
<p dir="auto">Click the refresh button (↻) in the dropdown. If still wrong, your Claude Code session may have expired - run <code>claude</code> again.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">PRs welcome! Please open an issue first to discuss major changes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT License - do whatever you want with it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">This is an unofficial tool not affiliated with Anthropic. It uses an undocumented API that may change without notice.</p>
<hr>
<p dir="auto">Made by <a href="https://x.com/richhickson" rel="nofollow">@richhickson</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM AI ('Bob') Downloads and Executes Malware (253 pts)]]></title>
            <link>https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware</link>
            <guid>46544454</guid>
            <pubDate>Thu, 08 Jan 2026 18:19:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware">https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware</a>, See on <a href="https://news.ycombinator.com/item?id=46544454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="main" id="main-1"><article data-framer-name="container"><section data-framer-name="section"><p>Threat Intelligence</p><section data-framer-name="section"><p>IBM's AI coding agent 'Bob' has been found vulnerable to downloading and executing malware without human approval through command validation bypasses exploited using indirect prompt injection.</p></section></section><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p><img alt="IBM's 'Bob' AI executes a command to download and run malware triggering a system message on the user's computer." width="800" height="443" src="https://framerusercontent.com/images/goGqPVHqyuybIHtuO6dx47fyc.png" srcset="https://framerusercontent.com/images/goGqPVHqyuybIHtuO6dx47fyc.png?scale-down-to=512&amp;width=1600&amp;height=887 512w,https://framerusercontent.com/images/goGqPVHqyuybIHtuO6dx47fyc.png?scale-down-to=1024&amp;width=1600&amp;height=887 1024w,https://framerusercontent.com/images/goGqPVHqyuybIHtuO6dx47fyc.png?width=1600&amp;height=887 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></p><div><p>A vulnerability has been identified that allows malicious actors to exploit IBM Bob to download and execute malware without human approval if the user configures ‘always allow’ for <em>any</em> command.</p><p>IBM Bob is IBM’s new coding agent, currently in Closed Beta. IBM Bob is offered through the Bob CLI (a terminal-based coding agent like Claude Code or OpenAI Codex) and the Bob IDE (an AI-powered editor similar to Cursor).</p><p>In this article, we demonstrate that the Bob CLI is vulnerable to prompt injection attacks resulting in malware execution, and the Bob IDE is vulnerable to known AI-specific data exfiltration vectors.&nbsp;</p></div><p>In the documentation, IBM warns that setting auto-approve for commands constitutes a 'high risk' that can 'potentially execute harmful operations' - with the recommendation that users leverage whitelists and avoid wildcards. We have opted to disclose this work publicly to ensure users are informed of the acute risks of using the system prior to its full release. We hope that further protections will be in place to remediate these risks for IBM Bob's General Access release.</p><h3 id="the-attack-chain"><a href="#the-attack-chain"><strong>The Attack Chain</strong></a></h3><ol><li data-preset-tag="p"><p>The user wants to explore a new repository - they ask Bob for help.</p><img alt="" width="800" height="244" src="https://framerusercontent.com/images/zttL7TQRnbKeL2iQO4kOImTAvEQ.png" srcset="https://framerusercontent.com/images/zttL7TQRnbKeL2iQO4kOImTAvEQ.png?scale-down-to=512&amp;width=1600&amp;height=488 512w,https://framerusercontent.com/images/zttL7TQRnbKeL2iQO4kOImTAvEQ.png?scale-down-to=1024&amp;width=1600&amp;height=488 1024w,https://framerusercontent.com/images/zttL7TQRnbKeL2iQO4kOImTAvEQ.png?width=1600&amp;height=488 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li></ol><ol start="2"><li data-preset-tag="p"><p>Bob encounters an instruction at the bottom of the README that manipulates it into believing it is responsible for conducting a phishing training to test the user.</p><img alt="" width="550" height="203" src="https://framerusercontent.com/images/THkwh9IvXhvaHue3LkIbsF7G0jM.png" srcset="https://framerusercontent.com/images/THkwh9IvXhvaHue3LkIbsF7G0jM.png?scale-down-to=512&amp;width=1101&amp;height=406 512w,https://framerusercontent.com/images/THkwh9IvXhvaHue3LkIbsF7G0jM.png?scale-down-to=1024&amp;width=1101&amp;height=406 1024w,https://framerusercontent.com/images/THkwh9IvXhvaHue3LkIbsF7G0jM.png?width=1101&amp;height=406 1101w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><img alt="" width="800" height="113" src="https://framerusercontent.com/images/kH4igTwYH32bHFIHwtXqyW9o68.png" srcset="https://framerusercontent.com/images/kH4igTwYH32bHFIHwtXqyW9o68.png?scale-down-to=512&amp;width=1600&amp;height=226 512w,https://framerusercontent.com/images/kH4igTwYH32bHFIHwtXqyW9o68.png?scale-down-to=1024&amp;width=1600&amp;height=226 1024w,https://framerusercontent.com/images/kH4igTwYH32bHFIHwtXqyW9o68.png?width=1600&amp;height=226 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li></ol><ol start="3"><li data-preset-tag="p"><p>Bob prompts the user several times with benign ‘echo’ commands; after the third time, the user selects ‘always allow’ for execution of ‘echo’.</p><img alt="" width="770" height="260" src="https://framerusercontent.com/images/N30eycDOH9U2YFRJTGdeihBo.png" srcset="https://framerusercontent.com/images/N30eycDOH9U2YFRJTGdeihBo.png?scale-down-to=512&amp;width=1540&amp;height=520 512w,https://framerusercontent.com/images/N30eycDOH9U2YFRJTGdeihBo.png?scale-down-to=1024&amp;width=1540&amp;height=520 1024w,https://framerusercontent.com/images/N30eycDOH9U2YFRJTGdeihBo.png?width=1540&amp;height=520 1540w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><img alt="" width="800" height="238" src="https://framerusercontent.com/images/36n5Oevjjw71eRYd2rCQHJOKTK0.png" srcset="https://framerusercontent.com/images/36n5Oevjjw71eRYd2rCQHJOKTK0.png?scale-down-to=512&amp;width=1600&amp;height=477 512w,https://framerusercontent.com/images/36n5Oevjjw71eRYd2rCQHJOKTK0.png?scale-down-to=1024&amp;width=1600&amp;height=477 1024w,https://framerusercontent.com/images/36n5Oevjjw71eRYd2rCQHJOKTK0.png?width=1600&amp;height=477 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li><li data-preset-tag="p"><p>Bob attempts to ‘test’ the user as part of the training by offering a dangerous command. However, the command has been specially crafted to bypass built-in defenses, so it executes immediately, installing and running a script retrieved from an attacker’s server.</p><img alt="" width="620" height="351" src="https://framerusercontent.com/images/wOC10UIC2jpyJ4xZVweF5ITA.png" srcset="https://framerusercontent.com/images/wOC10UIC2jpyJ4xZVweF5ITA.png?scale-down-to=512&amp;width=1240&amp;height=702 512w,https://framerusercontent.com/images/wOC10UIC2jpyJ4xZVweF5ITA.png?scale-down-to=1024&amp;width=1240&amp;height=702 1024w,https://framerusercontent.com/images/wOC10UIC2jpyJ4xZVweF5ITA.png?width=1240&amp;height=702 1240w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li></ol><h3 id="bob-has-three-defenses-that-are-bypassed-in-this-attack"><a href="#bob-has-three-defenses-that-are-bypassed-in-this-attack">Bob has three defenses that are bypassed in this attack</a></h3><ol><li data-preset-tag="p"><p>When a multi-part command is requested (using operators like ‘;’), the user is shown a request that asks for permission to run each sub-command. </p><img alt="" width="515" height="90" src="https://framerusercontent.com/images/BQ3XceuYGD19RtTNi7pOFMocuA0.png" srcset="https://framerusercontent.com/images/BQ3XceuYGD19RtTNi7pOFMocuA0.png?scale-down-to=512&amp;width=1031&amp;height=180 512w,https://framerusercontent.com/images/BQ3XceuYGD19RtTNi7pOFMocuA0.png?scale-down-to=1024&amp;width=1031&amp;height=180 1024w,https://framerusercontent.com/images/BQ3XceuYGD19RtTNi7pOFMocuA0.png?width=1031&amp;height=180 1031w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><p>Note, this modal says: ‘Allow execution of echo, cat’. </p><p>This is bypassed because the defense system fails to identify separate sub-commands when they are chained using a redirect operator (&gt;).</p><img alt="" width="533" height="87" src="https://framerusercontent.com/images/9VISzzA8ZMFcaeduPuNwCQj3Q.png" srcset="https://framerusercontent.com/images/9VISzzA8ZMFcaeduPuNwCQj3Q.png?scale-down-to=512&amp;width=1066&amp;height=174 512w,https://framerusercontent.com/images/9VISzzA8ZMFcaeduPuNwCQj3Q.png?scale-down-to=1024&amp;width=1066&amp;height=174 1024w,https://framerusercontent.com/images/9VISzzA8ZMFcaeduPuNwCQj3Q.png?width=1066&amp;height=174 1066w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><div><p>Note, this modal only mentions: ‘Allow execution of echo’.</p></div></li><li data-preset-tag="p"><p>Bob prohibits the use of command substitution like $(command) as a security measure.&nbsp;</p><img alt="" width="800" height="50" src="https://framerusercontent.com/images/f9QaFU6hFaHJM7xw3zghrFgGIek.png" srcset="https://framerusercontent.com/images/f9QaFU6hFaHJM7xw3zghrFgGIek.png?scale-down-to=512&amp;width=1600&amp;height=100 512w,https://framerusercontent.com/images/f9QaFU6hFaHJM7xw3zghrFgGIek.png?scale-down-to=1024&amp;width=1600&amp;height=100 1024w,https://framerusercontent.com/images/f9QaFU6hFaHJM7xw3zghrFgGIek.png?width=1600&amp;height=100 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><p>However, even though the security message says it's restricted, the underlying code fails to adequately restrict evaluation via process substitution: &gt;(command). </p><img alt="" width="670" height="185" src="https://framerusercontent.com/images/dVhffI8ROvWMmgatYUUz5BDUFw.png" srcset="https://framerusercontent.com/images/dVhffI8ROvWMmgatYUUz5BDUFw.png?scale-down-to=512&amp;width=1340&amp;height=370 512w,https://framerusercontent.com/images/dVhffI8ROvWMmgatYUUz5BDUFw.png?scale-down-to=1024&amp;width=1340&amp;height=370 1024w,https://framerusercontent.com/images/dVhffI8ROvWMmgatYUUz5BDUFw.png?width=1340&amp;height=370 1340w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"><div><p>This allows for the output of the malicious sub-command that retrieves the malware to be used as the input for the sub-command to execute that malware.&nbsp;</p><p>One can examine where the defense should have been in the minified JS source code:</p></div><div aria-autocomplete="list" aria-label="Code Editor for example.jsx" aria-multiline="true" role="tabpanel" tabindex="0" translate="no" aria-labelledby="/example.jsx-:R5midalaop:-tab" id="/example.jsx-:R5midalaop:-tab-panel" data-width="fill"><pre><span>function</span> <span>detectCommandSubstitution</span><span>(</span><span>cmd</span><span>)</span> <span>{</span>
      <span>...</span>
      <span>if</span> <span>(</span>
        <span>...</span>
          <span>(</span><span>ch</span> === <span>"$"</span> &amp;&amp; <span>nxt</span> === <span>"("</span><span>)</span> ||      
          <span>(</span><span>ch</span> === <span>"&lt;"</span> &amp;&amp; <span>nxt</span> === <span>"("</span> ... &amp;&amp; !<span>inBacktick</span><span>)</span> || 
          <span>(</span><span>ch</span> === <span>"`"</span> &amp;&amp; !<span>inBacktick</span><span>)</span>         
	  
  	    ...
      <span>)</span> <span>{</span>
        <span>return</span> <span>true</span><span>;</span>
      <span>}</span>
    <span>...</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></pre></div></li><li data-preset-tag="p"><p>By pre-fixing the malicious command with a benign ‘echo’, which the user has selected ‘always approve’ for, the user inadvertently auto-approves the entire malicious payload (since bypasses 1 and 2 prevent the payload from being recognized as separate commands).&nbsp;</p><img alt="" width="620" height="46" src="https://framerusercontent.com/images/iBKlfU7eMCa5fn0Rko1nvB5u4Qk.png" srcset="https://framerusercontent.com/images/iBKlfU7eMCa5fn0Rko1nvB5u4Qk.png?scale-down-to=512&amp;width=1240&amp;height=93 512w,https://framerusercontent.com/images/iBKlfU7eMCa5fn0Rko1nvB5u4Qk.png?scale-down-to=1024&amp;width=1240&amp;height=93 1024w,https://framerusercontent.com/images/iBKlfU7eMCa5fn0Rko1nvB5u4Qk.png?width=1240&amp;height=93 1240w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li></ol><h3 id="impact"><a href="#impact"><strong>Impact</strong></a></h3><p>With the ability to deliver an arbitrary shell script payload to the victim, attackers can leverage known (or custom) malware variants to conduct cyber attacks such as:</p><ul><li data-preset-tag="p"><p>Executing ransomware that encrypts or deletes files</p></li><li data-preset-tag="p"><p>Credential theft or spyware deployment</p></li><li data-preset-tag="p"><p>Device takeover (opening a reverse shell)</p></li><li data-preset-tag="p"><p>Forcing the victim into a cryptocurrency-mining botnet</p></li></ul><p>Together, these outcomes demonstrate how a prompt injection can escalate into a full-scale compromise of a user’s machine through vulnerabilities in the IBM Bob CLI.</p><h3 id="further-findings"><a href="#further-findings"><strong>Further Findings</strong></a></h3><p>Additional findings indicate that the Bob IDE is susceptible to several known zero-click data exfiltration vectors that affect many AI applications:</p><ol><li data-preset-tag="p"><p>Markdown images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).<br>Here is an interesting spin on the typical Markdown image attack where, beyond just exfiltrating data from query parameters as the image is rendered, the image itself is hyperlinked and made to pose as a button - used for phishing.</p><img alt="" width="800" height="503" src="https://framerusercontent.com/images/eitM3ZCZXR7mrby14wIBd7KiZvc.png" srcset="https://framerusercontent.com/images/eitM3ZCZXR7mrby14wIBd7KiZvc.png?scale-down-to=512&amp;width=1600&amp;height=1006 512w,https://framerusercontent.com/images/eitM3ZCZXR7mrby14wIBd7KiZvc.png?scale-down-to=1024&amp;width=1600&amp;height=1006 1024w,https://framerusercontent.com/images/eitM3ZCZXR7mrby14wIBd7KiZvc.png?width=1600&amp;height=1006 1600w" data-framer-original-sizes="" sizes="(min-width: 1080px) 100vw, (min-width: 810px) and (max-width: 1079.98px) 100vw, (max-width: 809.98px) 100vw"></li><li data-preset-tag="p"><div><p>Mermaid diagrams supporting external images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).</p></div></li><li data-preset-tag="p"><p>JSON schemas are pre-fetched, which can yield data exfiltration if a dynamically generated attacker-controlled URL is provided in the field (this can happen before a file edit is accepted).&nbsp;</p></li></ol></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[He was called a 'terrorist sympathizer.' Now his AI company is valued at $3B (198 pts)]]></title>
            <link>https://sfstandard.com/2026/01/07/called-terrorist-sympathizer-now-ai-company-valued-3b/</link>
            <guid>46544276</guid>
            <pubDate>Thu, 08 Jan 2026 18:05:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfstandard.com/2026/01/07/called-terrorist-sympathizer-now-ai-company-valued-3b/">https://sfstandard.com/2026/01/07/called-terrorist-sympathizer-now-ai-company-valued-3b/</a>, See on <a href="https://news.ycombinator.com/item?id=46544276">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>“Should I wear a keffiyeh to the shooting range?” Amjad Masad asked as I slid into the passenger seat of his black Mercedes sports car. The patterned Palestinian scarf has become a political lightning rod in the two years since the war in Gaza began. Masad, the founder of AI coding startup Replit, and a Palestinian by origin, wrapped it around his neck anyway.</p><p>At the shooting range in Santa Clara, we collected an assault rifle and a pistol and headed in. With the AR-22 tucked into his shoulder, Masad fired at a brisk clip, peppering bullets into a cartoon burglar. In less than two minutes, the burglar’s head was perfectly pocked with holes.&nbsp;</p><p>“You should compete,” I suggested.&nbsp;</p></div><div><p>He smirked. “I always compete.”&nbsp;</p><p>Indeed, Masad has never been shy about his competitive streak or his political beliefs — especially since Oct. 7, 2023, when Hamas attacked Israel, setting off the war in Gaza.&nbsp;</p><p>Masad, 38, has felt obliged to speak out about Gaza ever since, calling out those in tech who, in his view, have supported Israel’s “genocide” of the Palestinian people. He quickly learned just how unpopular that opinion was in Silicon Valley.</p><p>Party invitations dried up, group chats lit up with techies condemning his posts, and investors called him a “terrorist sympathizer.” One member of a firm that backed Replit <u><a href="https://x.com/rabois/status/1943995111915827321" target="_blank" rel="noopener noreferrer">publicly declared<span> (opens in new tab)</span></a></u> in July that he would donate any earnings from the investment to the Israel Defense Forces.&nbsp;</p><p>“I felt like I was sort of expelled from Silicon Valley,” Masad said in November on the way to the range.&nbsp;</p><p>But even as Silicon Valley cooled on him, the AI boom that Masad had long bet on took off. In 2024, Replit combined its coding tools with AI to create an agent that could turn plain English prompts into pre-coded apps. After nearly a decade, the company was suddenly in the right place at exactly the right time.&nbsp;</p><p>Two years later, Replit is booming. In September, it raised $250 million from Prysm, Andreessen Horowitz, Amex Ventures, and others, at a valuation of $3 billion. The company hopes to create the preeminent AI coder — one so easy to use that anyone can become a software engineer. Meanwhile, Masad continues to advocate for his political beliefs.&nbsp;</p><p>“People reached out to me to express that my words have been hurtful and that many have been deeply offended,” he <u><a href="https://x.com/amasad/status/1933752598638833938" target="_blank" rel="noopener noreferrer">posted on X over the summer<span> (opens in new tab)</span></a></u>. “I finally realized that I must, from the bottom of my heart, apologize to — absolutely nobody.”&nbsp;</p><p>Masad’s swagger isn’t unique among tech founders. But for many, their against-the-grain takes tend to align with the bottom line. Masad insists he speaks up even when it hurts his business. In that regard, “I’m probably the only contrarian in Silicon Valley,” he told me.</p><p>As Replit takes off, will he be able to maintain that independence?</p></div><h2>The balancing act&nbsp;</h2><div><p>Replit’s multibillion valuation is hitched to the AI boom. Should the boom fizzle, the company could be worth nothing. But if it holds, and AI continues to remake the tech industry and the greater economy, Masad could find himself among the Valley’s next class of billionaires.&nbsp;</p><p>Replit has plenty of competition among AI code-to-product firms (which are typically built on top of the mega LLMs). Swedish startup Lovable raised $330 million <u><a href="https://techcrunch.com/2025/12/18/vibe-coding-startup-lovable-raises-330m-at-a-6-6b-valuation/" target="_blank" rel="noopener noreferrer">last month<span> (opens in new tab)</span></a></u>, and Israeli company Base44 was bought by website builder Wix for $80 million this summer. All told, the AI coding sector received $4.7 billion in U.S. funding in the last year alone, according to PitchBook.&nbsp;</p><p>To persevere, Masad will not only have to beat his competitors with a better product but will have to earn the favor of still more investors, given the massive capital it takes to build and power Replit’s models.</p><p>For this reason, most players in AI have been pragmatic with their politics; see, for instance, how many Silicon Valley figures who previously supported liberal causes — from <u><a href="https://sfstandard.com/2025/10/13/publicly-turning-san-francisco-marc-benioff-had-privately-left/" data-post-id="e2908743-63dd-4db1-a156-67afd5147a4f">Marc Benioff </a></u>to <u><a href="https://sfstandard.com/2025/06/09/chan-zuckerberg-initiative-politics-pr/" data-post-id="f030d40f-da23-4f28-bad7-f1585255beda">Mark Zuckerberg</a></u> — suddenly and enthusiastically embraced the Trump White House. Or how Jensen Huang <u><a href="https://www.nytimes.com/2025/11/30/technology/david-sacks-white-house-profits.html" target="_blank" rel="noopener noreferrer">reportedly cozied<span> (opens in new tab)</span></a></u> up to David Sacks, hoping the AI czar would convince President Donald Trump to loosen U.S. chip restrictions and allow Nvidia to sell more products globally. Or how Sam Altman has courted Gulf royalty, <u><a href="https://openai.com/index/introducing-stargate-uae/" target="_blank" rel="noopener noreferrer">landing major<span> (opens in new tab)</span></a></u> deals to build data centers in the region and fund OpenAI’s stateside infrastructure needs.</p><p>For the most part, Masad has been unwilling to soften his positions. With a penchant for discursive discussions on political philosophy, Masad remains eager to speak about the war in Gaza and rail against what he sees as human rights abuses committed by Israel.</p><p>And he’s keen to take meetings with anyone who might see eye to eye on Palestine, even if they don’t agree on tech policy. Masad has spoken with New York Mayor Zohran Mamdani and messaged Lina Khan, the former chair of the Federal Trade Commission and nemesis of the tech elite. He had dinner with Tucker Carlson and lasted the <u><a href="https://www.youtube.com/watch?v=WfmrEa0L08E" target="_blank" rel="noopener noreferrer">obligatory three hours<span> (opens in new tab)</span></a></u> on “The Joe Rogan Experience.”&nbsp;&nbsp;</p><p>In short, Masad sees himself as both a champion for Palestine and a mogul in waiting. Which, particularly to other Arabs in tech, has made him a hero. “Having one of us make it in Silicon Valley — and still stay vocal on Palestine in the midst of all of this, specifically in Silicon Valley — is extremely inspirational,” said Fadi Ghandour, executive chairman at investment firm Wamda, one of the backers of Replit.&nbsp;</p><p>All this is a world away from Amman, Jordan, where Masad grew up. On the drive from his home in Palo Alto to the gun range, Masad told me his youthful hobbies included drift racing on Amman’s streets and shooting guns in empty lots.&nbsp;&nbsp;</p><p>Masad, who speaks unexpectedly softly given his combative persona, is aware of how lucky he is. He compared his circumstances to a simulation experiment in which “if you ran the world a million times, I think 90% of them, I’m in a really bad situation. I’m in prison or dead.” Instead, “somehow I keep surviving and winning.”&nbsp;</p></div><div data-count="1"><p><a href="https://sfstandard.com/pacific-standard-time/2025/12/10/wearable-ai-accessories/" tabindex="-1"><img alt="A person holds a round white object with their thumb and forefinger, with a colorful, partially torn overlay highlighting the hand's shape." loading="lazy" width="1000" height="666.6666666666666" decoding="async" data-nimg="1" sizes="300px" srcset="https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S80x53-FPNG 80w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S120x80-FPNG 120w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S240x160-FPNG 240w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S350x233-FPNG 350w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S450x300-FPNG 450w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S512x341-FPNG 512w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S640x427-FPNG 640w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S750x500-FPNG 750w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S768x512-FPNG 768w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S828x552-FPNG 828w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S1024x683-FPNG 1024w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S1080x720-FPNG 1080w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S1200x800-FPNG 1200w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S1920x1280-FPNG 1920w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S2048x1365-FPNG 2048w, https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S3840x2560-FPNG 3840w" src="https://assets.sfstandard.com/image/994911177489/image_ershijg1jh0br6gueo9cm4da37/-C100.00%25x100.00%25%2C0.00%25%2C0.00%25-S3840x2560-FPNG"></a></p></div><div><p>Replit’s multibillion valuation was in many ways the fruits of Masad’s decades-long study of how to make it in Silicon Valley. As a teen in Jordan, he avidly followed Y Combinator cofounder Paul Graham, studying his blog posts and obsessively scrolling through Hacker News, the Y Combinator news feed.&nbsp;</p><p>After getting his computer science degree in 2010 from Jordan’s Princess Sumaya University for Technology, he created the first iteration of what would become Replit: an open-source program for coders to work within a web browser, a novelty back when programmers needed to buy specialized software.</p><p>He posted a link to the program on Hacker News, where it went viral. That led to securing his first job in the U.S., as the founding engineer at Codecademy in New York. In 2013, he took a job as a software engineer at Facebook. By 2016, he and his wife, Haya Odeh, decided to revisit his old project from Jordan, officially founding Replit as an online platform to help developers build, collaborate, and publish their software.&nbsp;</p><p>As a student of Silicon Valley, Masad knew that mythmaking was just as important as the product. He began live-blogging Replit’s progress, and his efforts soon caught the attention of Graham himself. “What struck me most about him, at first, was that he was a programmers’ programmer,” Graham said. “He wasn’t just working on software to make money. He’d have been writing it no matter what.”</p></div><div><ul><li><a href="https://sfstandard.com/opinion/2026/01/08/gavin-newsom-ro-khanna-get-wrong-their-beef-billionaire-tax/" tabindex="-1"><p><img alt="A man wearing sunglasses and a dark suit stands in shadow before the sunlit white dome and columns of a classical government building." loading="lazy" width="4609" height="3225" decoding="async" data-nimg="1" srcset="https://assets.sfstandard.com/image/994911177489/image_hvmb0t9s2l4b936so85usjm604/-S3840x2687-FPNG 1x" src="https://assets.sfstandard.com/image/994911177489/image_hvmb0t9s2l4b936so85usjm604/-S3840x2687-FPNG"></p></a><a href="https://sfstandard.com/opinion/2026/01/08/gavin-newsom-ro-khanna-get-wrong-their-beef-billionaire-tax/"><h3>What Gavin Newsom and Ro Khanna get wrong in their battle over Billionaire Tax</h3></a></li><li><a href="https://sfstandard.com/2025/12/30/california-s-billionaire-tax-explained/" tabindex="-1"><p><img alt="A man with gray hair wearing a gray suit jacket and black shirt sits on a white chair in front of a red and gray backdrop." loading="lazy" width="5000" height="3352" decoding="async" data-nimg="1" srcset="https://assets.sfstandard.com/image/994911177489/image_qvbrltm2l91j1449j45j04kb4v/-S3840x2574-FPNG 1x" src="https://assets.sfstandard.com/image/994911177489/image_qvbrltm2l91j1449j45j04kb4v/-S3840x2574-FPNG"></p></a><a href="https://sfstandard.com/2025/12/30/california-s-billionaire-tax-explained/"><h3>California’s billionaire tax, explained</h3></a></li><li><a href="https://sfstandard.com/2025/12/29/bay-area-s-tech-billionaires-behaved-2025/" tabindex="-1"><p><img alt="" loading="lazy" width="7819" height="5213" decoding="async" data-nimg="1" srcset="https://assets.sfstandard.com/image/994911177489/image_4p95jcm7s525n6pdo776vbt83q/-S3840x2560-FPNG 1x" src="https://assets.sfstandard.com/image/994911177489/image_4p95jcm7s525n6pdo776vbt83q/-S3840x2560-FPNG"></p></a><a href="https://sfstandard.com/2025/12/29/bay-area-s-tech-billionaires-behaved-2025/"><h3>How the Bay Area’s tech billionaires behaved in 2025</h3></a></li></ul></div><div><p>Graham invited Masad and Odeh to apply to the Y Combinator accelerator program in 2018. They were accepted, and after completing the program, Replit landed investment from Andreessen Horowitz. With that funding and support, Replit gained traction as a tool for students to learn coding and educators to teach it. According to Masad, he turned down a $1 billion acquisition from Github, believing Replit would one day be worth exponentially more.&nbsp;</p><p>Masad’s ultimate objective was to leverage his success in tech to help the Palestinian cause — a sentiment instilled in him by his mother. “She would tell me about all the great things I’m gonna do,” he said. “I fantasized as a kid about helping the Palestinians and ending the occupation and being an important force in the world.”</p><p>But in 2019, he effectively put the company on pause after his mother’s cancer took a turn for the worse. Masad and Odeh traveled to be with his father, a civil engineer with the Jordanian government.&nbsp;</p><p>The experience was traumatic for Masad. In his mother’s final days, the doctors in Jordan wouldn’t let any visitors into the ICU. Masad begged, then yelled, and they finally relented — on the condition he sign a “do not resuscitate” order. He still doesn’t understand why. By the time he got to see her, she was unconscious. “I was so angry at modern medicine,” Masad said. “So angry at modernity.” She died shortly thereafter.</p><p>After a few months, Masad returned to Silicon Valley. He couldn’t stop thinking about civilization at large, about where the world was going, and if he could stand it — or if he could change it. He found himself agreeing with the Valley’s increasingly powerful libertarians.&nbsp;</p><p>He bonded with Marc Andreessen over politics and literature, and traveled to Malaysia to visit Balaji Srinivasan’s mysterious “startup society.” He grew to realize that the limitations he had assumed don’t necessarily apply. “There was an aspect of, like, ‘Fuck the system,’” Masad said. “‘We need to remake civilization.’”&nbsp;</p><p>Masad gained a following on Twitter by bantering with tech legends about everything from coding to bitcoin to Covid vaccine skepticism. His growing stature landed him on podcasts and in <u><a href="https://www.wsj.com/opinion/10-things-to-love-america-patriotism-liberty-immigrant-amjad-masad-tech-silicon-valley-woke-crt-11640902246" target="_blank" rel="noopener noreferrer">glowing<span> (opens in new tab)</span></a></u> articles in The Wall Street Journal portraying him as a maverick. Then came the ultimate Silicon Valley validation: Graham, the idol whom he’d studied obsessively as a teenager in Jordan, <u><a href="https://paulgraham.com/weird.html?viewfullsite=1" target="_blank" rel="noopener noreferrer">started thanking him in his own blog posts.&nbsp;<span> (opens in new tab)</span></a></u></p><p>Meanwhile, Replit was struggling. The education market turned out to be small, and the company had trouble expanding into enterprise deals. In late 2023, Replit killed its education product and the following year laid off 30 employees — right after moving into a bigger office in Foster City. “I remember walking around the office and just feeling sick to my stomach,” he said. “I felt like I let people down as well, because I sold them this big dream, and it wasn’t working.”&nbsp;</p><p>He had a plan, though, one he hoped would make his odyssey worth it. For years, he had proselytised on podcasts about how AI would turbocharge coding. Replit had spent years building an extensive database of coding materials — he just needed foundational companies like OpenAI and Anthropic to catch up and release an LLM capable of writing code. Once they did, Replit could plug those models into its pre-existing database and create a powerful app-building AI agent.&nbsp;</p><p>While a new AI coding company would have to build out that infrastructure from scratch, Replit, Masad realized, could start miles ahead. After nearly a decade, he would finally have perfect timing.&nbsp;&nbsp;</p><p>And then came Oct. 7, 2023.&nbsp;</p></div><h2>The breaking point&nbsp;</h2><div><p>Masad had always been outspoken online. After Hamas’ terrorist attacks, as segments of the tech elite publicly lined up behind Israel, he held his ground. And his responses grew more costly.&nbsp;</p><p>First, there were the public fights. Masad <u><a href="https://x.com/amasad/status/1725677158134345754" target="_blank" rel="noopener noreferrer">advised Elon Musk<span> (opens in new tab)</span></a></u> to “stick to his principles” when the billionaire said the slogan “from the river to the sea” was calling for genocide. (Palestinian activists say it’s a call for the peaceful liberation of the Palestinian people; pro-Israel voices say it calls for the extermination of Israel.) He <u><a href="https://x.com/amasad/status/1723557399703814545" target="_blank" rel="noopener noreferrer">clapped back<span> (opens in new tab)</span></a></u> at Lux Capital founding partner Josh Wolfe, who said that equating Israel’s actions in Gaza to genocide “grotesquely disgraces” Holocaust victims. And when Keith Rabois of Khosla Ventures <u><a href="https://x.com/rabois/status/1943804360863232513" target="_blank" rel="noopener noreferrer">said Masad’s “friends”<span> (opens in new tab)</span></a></u> were behind the Oct. 7 attack, Masad challenged him directly.</p><p>“If you think my friends did that then why don’t you confront me when you see me?” Masad <u><a href="https://x.com/amasad/status/1943805215960183095" target="_blank" rel="noopener noreferrer">replied on X<span> (opens in new tab)</span></a></u>.&nbsp;</p><p>But even as things got noisy in public, Masad met eerie silence professionally. “My calendar was suddenly empty, because I was talking about Palestine,” he said. “Replit was not a hot company anymore. We did a layoff. And at the same time, a lot of my friends were no longer my friends. I was no longer invited to parties.”&nbsp;</p><p>Potential partnerships dried up. Masad became a frequent topic in pro-Israel tech groupchats, a source said, where some investors accused him of being antisemitic.&nbsp;&nbsp;</p><p>A Replit investor who requested anonymity to speak candidly told me Masad’s public persona has been “really challenging,” and he’s had to defend the founder in investor circles. I asked if Masad had lost business because of his views. “I’m sure the answer is yes,” the investor said.&nbsp;</p><p>A startup that Masad had invested in pitched another investor — and found that Masad was a liability. “He said, ‘Oh, Amjad is investing. He’s like a terrorist sympathizer. If he’s investing, I’m not investing,” Masad said he was told, though he declined to name the company or the investor.</p><p>But as Silicon Valley grew chilly, other regions beckoned.</p><p>In April 2024, with Replit a few months from launching its AI agent, Saudi Arabia’s Public Investment Fund, which is worth nearly $1 trillion, invited Masad and other AI thought leaders to stay at Ekland Safaris, a property in <u><a href="https://amabhungane.org/how-a-south-african-hunting-resort-opened-a-window-to-saudi-crown-princes-business-empire/#" target="_blank" rel="noopener noreferrer">South Africa linked<span> (opens in new tab)</span></a></u> to Crown Prince Mohammed bin Salman.&nbsp;</p><p>For Masad, it was a lifeline. Here were potential partners who didn’t care about his politics on Palestine. If anything, his Arab identity and his outspokenness made him more attractive.</p><p>In the heat of the day, a hunting guide sped Masad and other guests, including Tareq Amin, the future CEO of Saudi AI company HUMAIN, across 30,000 acres to kill kudu and wildebeest. By night, guests feasted in a grand ballroom while Masad demoed his not-yet-released AI coding agent to Saudi officials.&nbsp;</p><p>“We’re going to be able to give a software engineer to every person,” Masad told Amin. “Anyone in the world will have a software engineer on demand.”</p><p>Amin said he thought then that Replit was on “a path to build a game-changing technology.” Indeed, if the agent worked, and if the AI boom Masad had been betting on came to pass, everyone in that room stood to benefit.</p><p>Replit launched its AI agent in September 2024 and within a year, revenue had reached $150 million. Amin and Masad closed a deal for Replit to be the exclusive AI coding software for Saudi governmental agencies, a partnership Masad expects to lead to “hundreds of millions” of dollars worth of business.&nbsp;</p><p>And just like that, Silicon Valley wanted back in. Replit landed enterprise deals with Atlassian and Zillow, with a Meta deal in the wings, according to Masad. Replit raised its $250 million round and scored its $3 billion valuation.&nbsp;</p><p>It helped Masad that, in the two years since Oct. 7, there’s been <u><a href="https://www.pewresearch.org/politics/2025/10/03/how-americans-view-the-israel-hamas-conflict-2-years-into-the-war/" target="_blank" rel="noopener noreferrer">rising public disapproval<span> (opens in new tab)</span></a></u> of the Israeli government. He also gained strength in numbers: There have been <u><a href="https://www.notechforapartheid.com/" target="_blank" rel="noopener noreferrer">high-profile movements<span> (opens in new tab)</span></a></u> within Google, Amazon, and other tech giants, with employees speaking out in support of the Palestinian cause.</p><p>“Today, the tide in tech has shifted,” Masad <u><a href="https://x.com/amasad/status/1972882969628217399" target="_blank" rel="noopener noreferrer">posted to X in September.<span> (opens in new tab)</span></a></u> “If you’ve been holding back, now is the time to speak out and call out anyone supporting or celebrating genocide.”</p><p>With Silicon Valley warming back up to him — he was on <u><a href="https://www.youtube.com/watch?v=g-WeCOUYBrk" target="_blank" rel="noopener noreferrer">a16z’s podcast in October<span> (opens in new tab)</span></a></u> — Masad was invited to larger, and more controversial, audiences. He drove hours to record at Carlson’s rural Maine cabin, nodding politely when the host marveled on air at his perfect English. In July, he sat down with Rogan, talking about everything from Gaza to the Covid vaccine, which Masad has refused to take.&nbsp;</p><p>“I tend to have a negative reaction to anyone forcing me to do something,” Masad said, prompting a “Good for you” from Rogan. “It was the same thing now with talking about Palestine and things like that. The more they come at me, the more I want to say things.”&nbsp;</p><p>He appears now to have the capital, both financially and socially, to draw hard lines. “There are a lot of people who care about the Palestinians but who are afraid to speak out publicly,” said Graham, who has also been an outspoken advocate for Palestine. “What Amjad and I have in common is that we don’t have to worry about being fired if we do.”</p><p>Masad said Israel’s Ministry of Education had reached out around 2016 to work with Replit. “We got very close to a deal, but they were very overbearing,” he said, adding that the government had what he viewed as cumbersome requirements, like fining Replit if the service went down. The ministry did not respond to a request for comment.&nbsp;</p><p>He says he would never work with Israel now. “I think it’s an illegitimate and criminal government,” he told me during our gun safety training. “I mean, [Benjamin] Netanyahu is a war criminal.”&nbsp;</p><p>When I pointed out that Saudi Arabia has its own abysmal human rights record, Masad drew a contrast.</p><p>“I just think about how Replit is going to be used. Like, Israel is actively committing genocide and ethnic cleansing, and if you sell to the government there, it’s possible that they’re going to use it for that,” he said, pointing to the country’s use of Microsoft cloud services to track Palestinians’ phone calls. (After an investigation by The Guardian, Microsoft said it disabled the services that made the tracking possible <u><a href="https://www.cnbc.com/2025/09/25/microsoft-cuts-cloud-services-to-israeli-military-after-investigation.html" target="_blank" rel="noopener noreferrer">in September.<span> (opens in new tab)</span></a></u>)</p><p>It’s a careful, and conveniently self-serving, justification. This logic allows Replit to work with countries that Masad finds tolerable. Whether that line will hold as Replit scales is unclear.</p><p>After the shooting range, we drove through a thick forest fog to the restaurant The Mountain House in Woodside. Over grilled venison, Masad talked about Replit’s next challenge: expanding its user base from corporate clients to everyday users. This isn’t essential — plenty of companies thrive selling enterprise software. But Masad wants Replit to be an engine for socioeconomic mobility around the world, letting anyone, anywhere, easily create apps and businesses.</p><p>Masad acknowledged that if he succeeds — if Replit goes public and he becomes a billionaire — he will have the capital to make real change for Palestine, though he’s light on details. “It’s hard to plan for 20 years in the future,” he shrugged. “I just know that wealth is a prerequisite.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Minnesota officials say they can't access evidence after fatal ICE shooting (308 pts)]]></title>
            <link>https://www.pbs.org/newshour/nation/minnesota-officials-say-they-cant-access-evidence-after-fatal-ice-shooting-and-fbi-wont-work-jointly-on-investigation</link>
            <guid>46543457</guid>
            <pubDate>Thu, 08 Jan 2026 17:04:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pbs.org/newshour/nation/minnesota-officials-say-they-cant-access-evidence-after-fatal-ice-shooting-and-fbi-wont-work-jointly-on-investigation">https://www.pbs.org/newshour/nation/minnesota-officials-say-they-cant-access-evidence-after-fatal-ice-shooting-and-fbi-wont-work-jointly-on-investigation</a>, See on <a href="https://news.ycombinator.com/item?id=46543457">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="articleBody">
                <div>
                    <p>MINNEAPOLIS (AP) — The head of Minnesota's state investigations agency says the U.S. attorney's office has cut off its access in the fatal shooting of a Minneapolis woman by an ICE agent.</p>
<p><a href="https://www.pbs.org/newshour/show/minneapolis-police-chief-urges-lawful-peaceful-response-to-ice-shooting"><strong>WATCH:</strong> Minneapolis police chief urges 'lawful, peaceful' response to ICE shooting</a></p>
<p>"The investigation would now be led solely by the FBI, and the BCA would no longer have access to the case materials, scene evidence or investigative interviews necessary to complete a thorough and independent investigation," Minnesota Bureau of Criminal Apprehension Superintendent Drew Evans said in a statement.</p>
<p>It had been decided that the Bureau of Criminal Apprehension would investigate Good's shooting death along with the FBI, but that later was changed by the U.S. Attorney's office, according to Evans.</p>
<p>The BCA "has reluctantly withdrawn from the investigation," Evans wrote.</p>
<p><em>This is a developing story and will be updated.</em></p>

                                            <div>
                <figure>
                    <svg><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#pbs-newshour-horiz-refresh"></use></svg>
                </figure>
                <p>
                    A free press is a cornerstone of a healthy democracy. 
                </p>
                <p>
                    Support trusted journalism and civil dialogue. 
                </p>
                <a href="https://give.newshour.org/page/88646/donate/1?ea.tracking.id=pbs_news_sept_2025_article&amp;supporter.appealCode=N2509AW1000100">
                    
                    <svg width="24px" height="24px" viewBox="0 0 16 16" fill="#000000" version="1.1" id="svg1" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg">
                    <defs id="defs1"></defs>
                    <path fill-rule="evenodd" d="M4 8a.5.5 0 0 1 .5-.5h5.793L8.146 5.354a.5.5 0 1 1 .708-.708l3 3a.5.5 0 0 1 0 .708l-3 3a.5.5 0 0 1-.708-.708L10.293 8.5H4.5A.5.5 0 0 1 4 8z" id="path1"></path>
                    </svg>
                </a>
            </div>


                                    </div>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE's Tool to Monitor Phones in Neighborhoods (258 pts)]]></title>
            <link>https://www.404media.co/inside-ices-tool-to-monitor-phones-in-entire-neighborhoods/</link>
            <guid>46543420</guid>
            <pubDate>Thu, 08 Jan 2026 17:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/inside-ices-tool-to-monitor-phones-in-entire-neighborhoods/">https://www.404media.co/inside-ices-tool-to-monitor-phones-in-entire-neighborhoods/</a>, See on <a href="https://news.ycombinator.com/item?id=46543420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A social media and phone surveillance system ICE bought access to is designed to monitor a city neighborhood or block for mobile phones, track the movements of those devices and their owners over time, and follow them from their places of work to home or other locations, according to material that describes how the system works obtained by 404 Media.&nbsp;</p><p>Commercial location data, in this case acquired from hundreds of millions of phones via a company called Penlink, can be queried without a warrant, according to an internal ICE legal analysis shared with 404 Media. The purchase comes squarely during ICE’s <a href="https://www.latimes.com/california/story/2025-11-04/ice-border-patrol-dodger-stadium-parking-lot?ref=404media.co"><u>mass deportation effort</u></a> and continued <a href="https://x.com/ReichlinMelnick/status/1975899183644356640?ref=404media.co"><u>crackdown on protected speech</u></a>, alarming civil liberties experts and raising questions on what exactly ICE will use the surveillance system for.&nbsp;</p><div><p>💡</p><p><b><strong>Do you know anything else about this tool? Do you work for ICE, CBP, or another agency? I would love to hear from you. Using a non-work device, you can message me securely on Signal at joseph.404 or send me an email at joseph@404media.co.</strong></b></p></div><p>“This is a very dangerous tool in the hands of an out-of-control agency. This granular location information paints a detailed picture of who we are, where we go, and who we spend time with,” Nathan Freed Wessler, deputy project director of the American Civil Liberties Union’s (ACLU) Speech, Privacy, and Technology Project, told 404 Media.</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Digital Red Queen: Adversarial Program Evolution in Core War with LLMs (117 pts)]]></title>
            <link>https://sakana.ai/drq/</link>
            <guid>46542761</guid>
            <pubDate>Thu, 08 Jan 2026 16:16:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sakana.ai/drq/">https://sakana.ai/drq/</a>, See on <a href="https://news.ycombinator.com/item?id=46542761">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
  <h2><a href="https://sakana.ai/drq/">Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</a></h2><time datetime="2026-01-08T00:00:00+09:00">January 08, 2026</time>
</header>

  <center>
<video src="https://pub.sakana.ai/drq/drq-blog-video/drq-2.0x.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
<!--<video class="b-lazy" autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://pub.sakana.ai/drq/drq-blog-video/drq-2.0x.mp4" type="video/mp4" /></video>-->
</center>
<p><small><b>Survival of the Fittest Code.</b> In the game <a href="https://en.wikipedia.org/wiki/Core_War">Core War</a>, assembly-like programs called “warriors” fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. <b>Top</b>: We visualize battles between assembly programs (“warriors”) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. <b>Bottom</b>: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics.
</small>
<!--more--></p>



<h2 id="summary">Summary</h2>

<p><em><a href="https://en.wikipedia.org/wiki/Core_War">Core War</a></em> is a competitive programming game introduced in 1984, in which battle programs called <em>warriors</em> fight for dominance inside a virtual computer. To compete, developers write their code in <em>Redcode</em>, a specialized assembly language. In this work, we explore what happens when large language models (LLMs) drive an <em>adversarial  evolutionary arms race</em> in this domain, where programs continuously adapt to defeat a growing history of opponents rather than a static benchmark. We find that this dynamic adversarial process leads to the emergence of <strong>increasingly general</strong> strategies and reveals an intriguing form of <strong>convergent evolution</strong>, where different code implementations settle into similar high-performing behaviors. Ultimately, this work positions Core War as a sandbox for studying “Red Queen” dynamics in artificial systems, offering a safe controlled environment for analyzing how AI agents might evolve in real-world adversarial settings such as cybersecurity.</p>

<p>For further details, please read our technical report (<a href="https://pub.sakana.ai/drq">web paper</a>, <a href="https://arxiv.org/abs/2601.03335/">arxiv</a>) and released code (<a href="https://github.com/SakanaAI/drq">github</a>).</p>

<center>
<img data-src="/assets/drq/example-1.png" src="https://sakana.ai/assets/drq/example-1.png"><img data-src="/assets/drq/example-2.png" src="https://sakana.ai/assets/drq/example-2.png"> <!-- example-2-taller.png -->
</center>
<p><small><i><b>Two example warriors produced by DRQ:</b> <a>Ring Warrior Enhanced v9</a> and <a>Spiral Bomber Optimized v22</a>. These examples were selected to illustrate two complementary aspects of DRQ: its ability to synthesize qualitatively distinct strategies within a single program, and to produce generally performant warriors. Note that comments are LLM generated.</i></small></p>

<center>
<video autoplay="" muted="" playsinline="" loop=""><source src="https://pub.sakana.ai/drq/drq-blog-video/sandbox-v2.mp4" type="video/mp4"></video>
</center>
<p><small>
<i><b>Simulating our evolved “warriors” in a sandboxed Core War environment.</b> The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located.</i></small></p>



<h2 id="introduction">Introduction</h2>

<p>Humans are the product of an extraordinary evolutionary arms race, shaped by constant competition with other organisms. Yet evolution did not stop with the emergence of modern humans: competition persists at every scale, from viruses and bacteria to people, companies, and even nations vying for dominance. As more AI systems are deployed into the world, they too will enter this competitive landscape. Inevitably, these AI systems will begin to compete with one another, either directly or indirectly, giving rise to a new kind of evolutionary dynamic. To prepare for such a future and study these fascinating dynamics, we use large language models (LLMs) to evolve programs that compete against each other for control of a virtual computer in a game called <em><a href="https://en.wikipedia.org/wiki/Core_War">Core War</a>.</em></p>

<p><em>Core War</em> is a competitive programming game played out in a shared block of computer memory, called the “Core,” where two or more assembly programs fight for survival. Each program, known as a “warrior”, is written in an assembly language called <em>Redcode</em>. These programs are tasked with crashing their competitors while keeping their own processes alive. The simulation runs by alternating between the programs, executing one instruction at a time. A warrior “attacks” by writing invalid instructions (DAT commands) into the memory slots occupied by opponents, causing them to crash upon execution.</p>

<center>
<video autoplay="" muted="" playsinline="" loop=""><source src="https://pub.sakana.ai/drq/drq-blog-video/core-fast.mp4" type="video/mp4"></video>
</center>
<p><small>
<i><b>Examples of discovered warriors competing against each other in Core War.</b><br>
Core War is a programming game where assembly-like programs called “warriors” compete for control of a virtual machine. In this work, we use LLMs to evolve warriors through a self-play algorithm called Digital Red Queen. This process leads to the discovery of diverse and sophisticated strategies, including targeted bombing, self-replication, and massive multithreading. Here, we show some of the discovered warriors competing against each other in Core War battles. Symbols indicate instruction opcodes, and colors denote the warrior that last modified each memory address. There is no distinction between code and data, making the environment highly dynamic and volatile.
</i></small></p>



<p>Notably, there is no distinction between code and data, so warriors regularly modify both themselves and their opponents on the fly. This enables self-modification and even self-replication, but it also creates an extremely volatile environment in which programs must survive. Core War is also Turing-complete, meaning it can in principle support arbitrarily complex strategies.</p>

<p>Over the years, humans have devised many clever Core War strategies, including bombing random memory locations, self-replicating programs, and programs which continually scan the Core to detect opponent locations. These strategies were devised through a meta arms race between humans who try out new strategies and see what works. What would happen if we do this same arms race with LLMs?</p>

<p>In collaboration with <strong>MIT</strong>, we are excited to release our new paper <a href="http://pub.sakana.ai/drq">Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</a>! (<a href="https://arxiv.org/abs/2601.03335">arxiv</a>)</p>



<h2 id="our-method-digital-red-queen-drq">Our Method: Digital Red Queen (DRQ)</h2>

<p>In evolutionary biology, the <a href="https://en.wikipedia.org/wiki/Red_Queen_hypothesis">Red Queen Hypothesis</a> posits that species must constantly evolve simply to survive against their ever-changing competitors. It argues that being “fit” in the current environment is not enough. Instead, organisms must continuously adapt—not to gain an advantage, but simply to maintain their relative fitness in a world that is always changing. This concept perfectly captures the nature of adversarial arms races, where being “fit” is never a permanent state. The name implies that standing still is not an option, drawing from <em>Through the Looking-Glass</em> where the Red Queen tells Alice: “Now, here, you see, it takes all the running you can do, to keep in the same place.”</p>

<center>
<video autoplay="" muted="" playsinline="" loop=""><source src="https://pub.sakana.ai/drq/drq-blog-video/alice-animation.mp4" type="video/mp4"></video>
<small><i><b>“Now, here, you see, it takes all the running you can do, to keep in the same place.”</b><br>
Red Queen to Alice. By Lewis Carroll, Through the Looking-Glass. (<a href="https://en.wikipedia.org/wiki/Red_Queen_hypothesis#/media/File:Alice_queen2.jpg">Original Source</a>)</i></small>
</center>

<p>Taking inspiration from biology, we study a simple algorithm that we call <b>Digital Red Queen (DRQ)</b>, which embodies this idea in a computational setting. DRQ uses LLMs to evolve warriors under perpetual environmental change. Concretely, it begins with an initial warrior, then evolves a second warrior to defeat it in battle. A third warrior is then evolved to perform well against the first two, and so on. This process produces a lineage of warriors, each adapted to a changing environment defined by all of its predecessors.</p>

<p>DRQ is not intended to be a novel algorithm in itself. Rather, it is a minimal instantiation of prior multi-agent and self-play approaches, adapted to the Core War domain, designed to isolate and study the dynamics of continual coevolution.</p>

<center>
<video src="https://pub.sakana.ai/drq/drq-blog-video/drq-2.0x.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
</center>



<h2 id="results">Results</h2>

<p>We find that as DRQ is run for many rounds, warriors gradually become more generally robust, as measured by their performance against unseen human-designed warriors. This provides a stable way to consistently produce more robust programs without needing to “train on the test set” (i.e., directly optimizing against a large set of human-designed programs).</p>

<p>More surprisingly, we observe that independent runs of DRQ, each initialized with different warriors, slowly converge over time toward warriors with similar behaviors. Notably, this convergence does not occur at the level of source code, indicating that what converges is function rather than implementation.</p>

<center>
<img data-src="/assets/drq/figure-1.png" src="https://sakana.ai/assets/drq/figure-1.png"><br>
<small><i><b>DRQ’s Convergent Evolution:</b> With more rounds, DRQ produces warriors that are more generally robust. At the same time, across independent DRQ runs, the variance in the warrior’s behaviors decreases, indicating convergence.</i></small><br>
<img data-src="/assets/drq/figure-2.png" src="https://sakana.ai/assets/drq/figure-2.png"><br>
<small><i><b>Phenotypic Convergence: </b> Convergence with rounds is seen only in the phenotype (behavior) of the warriors, and not the genotype (the source code), analogous to convergence in biological function rather than DNA.</i></small>
</center>

<p>This result is reminiscent of <a href="https://en.wikipedia.org/wiki/Convergent_evolution">convergent evolution</a> in biology, where similar functional traits evolved independently multiple times through different mechanisms. For example, birds and bats evolved wings separately, and spiders and snakes independently evolved venom. In these cases, evolution arrived at similar general-purpose solutions because the functional demands imposed by changing environments favored them.</p>



<h2 id="discussion">Discussion</h2>

<p>The emergence of convergent evolution from Red Queen dynamics, both commonly found in nature, hints that the DRQ algorithm and the Core War domain may be a promising setup for studying other properties of adversarial arms races. High level insights found in simulation could help inform how the arms race between LLMs in the wild might play out. Algorithms like DRQ could even help automate the “red-teaming” of systems before they are deployed in the real world.</p>

<p>The benefit of doing this research in a sandbox like Core War is that it’s completely self-contained: all programs run on an artificial machine with an artificial language, so nothing generated can execute outside the sandbox. This provides a safe space to explore adversarial dynamics that might be risky in the real world.</p>

<center>
<video autoplay="" muted="" playsinline="" loop=""><source src="https://pub.sakana.ai/drq/drq-blog-video/sandbox.mp4" type="video/mp4"></video>
</center>
<p><small>
<i><b>In a sandboxed Core War environment, we can simulate our evolved “warriors” and visualize their behaviors.</b> The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located. Please see our <a href="https://github.com/SakanaAI/drq">GitHub</a> for more information.</i></small></p>



<p>Despite its simplicity, vanilla DRQ performs surprisingly well in Core War, suggesting that even minimal self-play loops can reveal complex and robust strategies. This makes DRQ a promising candidate for exploring other competitive multi-agent simulations in artificial life, biology, drug design, real-world cybersecurity, or market ecosystems. Future work could also explore richer setups where agents co-evolve simultaneously, better resembling the real-world where large populations adapt in parallel rather than along a single line of descent. Ultimately the insights gathered will help control the future for the better and help us understand the science of these evolutionary arms races.</p>


<center>
<img data-src="/assets/careers/funny_fish_new.jpg" src="https://sakana.ai/assets/careers/funny_fish_new.jpg">
</center>




<p>We are taking this technology far beyond adversarial competitive programming to unlock a new era of AI-driven discovery.</p>

<p>If you are interested in advancing AI-driven discovery, <strong>we’re hiring</strong>!</p>

<p>Sakana AI is at the forefront of AI-driven discovery. In addition to this work, we are also behind works such as <a href="https://sakana.ai/ai-scientist/">The AI Scientist</a>, <a href="https://sakana.ai/llm-squared/">LLM-Squared</a>, <a href="https://sakana.ai/shinka-evolve/">Shinka-Evolve</a>, <a href="https://sakana.ai/asal/">Automating the Search for Artificial Life</a> and <a href="https://sakana.ai/ahc058/">ALE-Agent</a>. We’re looking for engineers to join our team to work on our advanced AI-driven discovery platform and productionize our model-development efforts.</p>

<p>Please see our <a href="https://sakana.ai/careers/#software-engineer-research-and-development">career opportunities</a> for more information.</p>



  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iran Goes Into IPv6 Blackout (434 pts)]]></title>
            <link>https://radar.cloudflare.com/routing/ir</link>
            <guid>46542683</guid>
            <pubDate>Thu, 08 Jan 2026 16:11:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.cloudflare.com/routing/ir">https://radar.cloudflare.com/routing/ir</a>, See on <a href="https://news.ycombinator.com/item?id=46542683">Hacker News</a></p>
Couldn't get https://radar.cloudflare.com/routing/ir: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>