<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 01 Oct 2025 08:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[An informational website about why I went to prison (160 pts)]]></title>
            <link>https://prison.josh.mn/</link>
            <guid>45434062</guid>
            <pubDate>Wed, 01 Oct 2025 03:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prison.josh.mn/">https://prison.josh.mn/</a>, See on <a href="https://news.ycombinator.com/item?id=45434062">Hacker News</a></p>
Couldn't get https://prison.josh.mn/: Error: getaddrinfo ENOTFOUND prison.josh.mn]]></description>
        </item>
        <item>
            <title><![CDATA[The gaslit asset class (124 pts)]]></title>
            <link>https://blog.dshr.org/2025/09/the-gaslit-asset-class.html</link>
            <guid>45433866</guid>
            <pubDate>Wed, 01 Oct 2025 02:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.dshr.org/2025/09/the-gaslit-asset-class.html">https://blog.dshr.org/2025/09/the-gaslit-asset-class.html</a>, See on <a href="https://news.ycombinator.com/item?id=45433866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6556465880306785878" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZX0fqPMMdG3KevN5anuO0O4Vzb2gPpCV0ipAxDl6iQODtpgJd_HhpF6M4tb9Oojlb1IkT-FtLa9nhxLbY7TcXMNJnYuLCS6yvxYcDiolSXwKHPTZ11bB-hr55RJYBwR9NM3q-7rdr8z_jgxed293EUpeU89cbjBJpPfy9NTbAlJgsOJg-_hxSrqFEs2bV/s2048/Grants.png"><img data-original-height="593" data-original-width="2048" height="58" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZX0fqPMMdG3KevN5anuO0O4Vzb2gPpCV0ipAxDl6iQODtpgJd_HhpF6M4tb9Oojlb1IkT-FtLa9nhxLbY7TcXMNJnYuLCS6yvxYcDiolSXwKHPTZ11bB-hr55RJYBwR9NM3q-7rdr8z_jgxed293EUpeU89cbjBJpPfy9NTbAlJgsOJg-_hxSrqFEs2bV/w200-h58/Grants.png" width="200"></a>
James Grant invited me to address the annual conference of <a href="https://www.grantspub.com/"><i>Grant's Interest Rate Observer</i></a>. This was an intimidating prospect, the previous year's conference featured billionaires <a href="https://en.wikipedia.org/wiki/Scott_Bessent">Scott Bessent</a> and <a href="https://en.wikipedia.org/wiki/Bill_Ackman">Bill Ackman</a>. As usual, below the fold is the text of my talk, with the slides, links to the sources, and additional material in footnotes. Yellow background indicates textual slides.<br>
<span><a name="more"></a></span></p><h3>The Gaslit Asset Class</h3><p>
Before I explain that much of what you have been told about cryptocurrency technology is gaslighting, I should stress that I hold no long or short positions in cryptocurrencies, their derivatives or related companies. Unlike most people discussing them, I am not "<a href="https://blog.dshr.org/2022/02/talking-their-book.html">talking my book</a>".</p><p>

To fit in the allotted time, this talk focuses mainly on Bitcoin and omits many of the finer points. My text, with links to the sources and additional material in footnotes, will go up on my blog later today.</p><h3>Why Am I Here?</h3><p>
I imagine few of you would understand why a retired software engineer with more than forty years in Silicon Valley  was asked to address you on cryptocurrencies<sup><a href="#Footnote1">[1]</a></sup>.</p><p>
I was an early employee at <a href="https://en.wikipedia.org/wiki/Sun_Microsystems">Sun Microsystems</a> then <a href="https://blog.dshr.org/2025/05/the-dawn-of-nvidias-technology.html">employee #4 at Nvidia</a>, so I have been long Nvidia for more than 30 years. It has been a <a href="https://blog.dshr.org/2024/07/accelerated-computing.html">wild ride</a>. I quit after 3 years as part of fixing Nvidia's first near-death experience and immediately did 3 years as employee #12 at another startup, which also IPO-ed. If you do two in six years in your late 40s you get seriously burnt out.</p><p>

So my wife and I started a program at Stanford that is still running 27 years later. She was a career librarian at the Library of Congress and the Stanford Library. She was part of the team that, 30 years ago, pioneered the transition of academic publishing to the Web. She was also the person who <a href="https://blog.dshr.org/2024/02/the-stanford-digital-library-project.html">explained citation indices to Larry and Sergey</a>, which led to Page Rank.</p><p>

The academic literature has archival value. Multiple libraries hold complete runs on paper of the <a href="https://royalsocietypublishing.org/journal/rstl"><i>Philosophical Transactions of the Royal Society</i></a> starting 360 years ago<sup><a href="#Footnote1">[2]</a></sup>.
The interesting engineering problem we faced was how to enable libraries to deliver comparable longevity to Web-published journals.</p><h3>Five Years Before Satoshi Nakamoto</h3>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAPQ7O6lmB4paIa0yCsHim_SMJlYGWMWTdq1uYsdAJE7jubDbPJBWg-7jOC-mBYLX_G4NUSJKhJy3CW4bGU2rNoY3y2PsrU6_Icb5ELT5-ZfX8jR7jef16hRSYAfrRMJ_DqLZL5lIdfKS2-O0USJoK2lSnhaesAytJk_BesD_Z6ggub-rHsprp2zHuP5SB/s151/LOCKSS.logo.png"><img data-original-height="151" data-original-width="151" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAPQ7O6lmB4paIa0yCsHim_SMJlYGWMWTdq1uYsdAJE7jubDbPJBWg-7jOC-mBYLX_G4NUSJKhJy3CW4bGU2rNoY3y2PsrU6_Icb5ELT5-ZfX8jR7jef16hRSYAfrRMJ_DqLZL5lIdfKS2-O0USJoK2lSnhaesAytJk_BesD_Z6ggub-rHsprp2zHuP5SB/s16000/LOCKSS.logo.png"></a>
I worked with a group of outstanding Stanford CS Ph.D. students to design and implement <a href="http://dx.doi.org/10.1145/945445.945451">a system for stewardship of Web content</a> modeled on the paper library system. The goal was to make it extremely difficult for even a powerful adversary to delete or modify content without detection. It is called <a href="https://lockss.org/">LOCKSS</a>, for Lots Of Copies Keep Stuff Safe; a decentralized peer-to-peer system secured by Proof-of-Work. We won a <a href="http://dx.doi.org/10.1145/945445.945451">"Best Paper" award</a> for it five years before Satoshi Nakamoto published his <a href="https://bitcoin.org/bitcoin.pdf">decentralized peer-to-peer system</a> secured by Proof-of-Work.  When he did, LOCKSS had been in production for a few years and we had learnt a lot about how difficult decentralization is in the online world.</p><p>

Bitcoin built on more than <a href="https://queue.acm.org/detail.cfm?id=3136559">two decades of research</a>. Neither we nor Nakamoto invented Proof-of-Work, <a href="https://doi.org/10.1007/3-540-48071-4_10">Cynthia Dwork and Moni Naor</a> published it in 1992. Nakamoto didn't invent blockchains, <a href="https://doi.org/10.1007/3-540-38424-3_32">Stuart Haber and W. Scott Stornetta</a> patented them in 1991. He was extremely clever in assembling well-known techniques into a cryptocurrency, but his only major innovation was the <a href="https://doi.org/10.1007/978-3-642-27739-9_1804-1">Longest Chain Rule</a>.</p><h3>Digital cash</h3><p>
The fundamental problem of representing cash in digital form is that a digital coin can be endlessly copied, thus you need some means to prevent each of the copies being spent. When you withdraw cash from an ATM, turning digital cash in your account into physical cash in your hand, the bank performs an atomic transaction against the database mapping account numbers to balances. The bank is trusted to prevent multiple spending.</p><p>

There had been several attempts at a cryptocurrency before Bitcoin.  The primary goals of the <a href="https://search.worldcat.org/title/1099341012">libertarians and cypherpunks</a> were that a cryptocurrency be as anonymous as physical cash, and that it not have a central point of failure that had to be trusted.  The only one to get any traction was David Chaum's <a href="https://en.wikipedia.org/wiki/DigiCash">DigiCash</a>; it was anonymous but it was centralized to prevent multiple spending and it involved banks.</p><h3>Nakamoto's <i>magnum opus</i></h3>
<div><p>
Bitcoin claims:
</p><ul>
<li>The system was <i>trustless</i> because it was <i>decentralized</i>.</li>
<li>It was a <i>medium of exchange</i> for buying and selling in the real world.</li>
<li>Transactions were <i>faster</i> and <i>cheaper</i> than in the existing financial system.</li>
<li>It was secured by <i>Proof-of-Work</i> and <i>cryptography</i>.</li>
<li>It was <i>privacy-preserving</i>.</li>
</ul>
</div><p>
When in November 2008 Nakamoto published <a href="https://bitcoin.org/bitcoin.pdf"><i>Bitcoin: A Peer-to-Peer Electronic Cash System</i></a> it was the peak of the <a href="https://en.wikipedia.org/wiki/2008_financial_crisis">Global Financial Crisis</a> and people were very aware that the financial system was broken (and it still is). Because it solved many of the problems that had dogged <a href="https://davidgerard.co.uk/blockchain/book/">earlier attempts at electronic cash</a>, it rapidly attracted a clique of enthusiasts. When Nakamoto went silent in 2010 they took over proseltyzing the system. The main claims they made were:</p><ul>
<li>The system was <i>trustless</i> because it was <i>decentralized</i>.</li>
<li>It was a <i>medium of exchange</i> for buying and selling in the real world.</li>
<li>Transactions were <i>faster</i> and <i>cheaper</i> than in the existing financial system.</li>
<li>It was secured by <i>Proof-of-Work</i> and <i>cryptography</i>.</li>
<li>It was <i>privacy-preserving</i>.</li>
</ul><p>
They are all either false or misleading. In most cases Nakamoto's own writings show he knew this. His acolytes were gaslighting.</p><h3>Trustless because decentralized (1)</h3><p>
Assuming that the Bitcoin network consists of a large number of roughly equal nodes, it randomly selects a node to determine the transactions that will form the next block. There is no need to trust any particular node because the chance that they will be selected is small.<sup><a href="#Footnote3">[3]</a></sup></p><div><p>
At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware. A server farm would only need to have one node on the network and the rest of the LAN connects with that one node.
</p><blockquote>
<small>
Satoshi Nakamoto 2<sup>nd</sup> November 2008
</small>
</blockquote><p>
The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms. The rest will be client nodes that only do transactions and don’t generate.
</p><blockquote>
<small>
Satoshi Nakamoto: 29<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
But only three days after publishing his white paper, Nakamoto understood that this assumption would become false:</p><blockquote>
At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware.
</blockquote><p>
He didn't change his mind. On 29<sup>th</sup> July 2010, less than five months before he went silent, he made the same point:</p><blockquote>
The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms.
</blockquote><p>
"Letting users be users" necessarily means that the "users" have to trust the "few nodes" to include their transactions in blocks.  The very strong economies of scale of technology in general and "big server farms" in particular meant that the centralizing force described in W. Brian Arthur's 1994 book <a href="http://www.amazon.com/Increasing-Returns-Dependence-Economics-Cognition/dp/0472064967"><i>Increasing Returns and Path Dependence in the Economy</i></a> resulted in there being "fewer nodes". Indeed, on 13<sup>th</sup> June 2014 a single node controlled 51% of Bitcoin's mining, the <a href="https://hackingdistributed.com/p/2014/06/13/in-ghash-bitcoin-trusts/">GHash pool</a>.<sup><a href="#Footnote4">[4]</a></sup></p><h3>Trustless because decentralized (2)</h3><p>
In June 2022 <a href="https://aidenlab.org/bitcoin.pdf"><i>Cooperation among an anonymous group protected Bitcoin during failures of decentralization</i></a> by Alyssa Blackburn <i>et al</i> showed that it had not been decentralized from the very start. The same month a DARPA-sponsored report entitled <a href="https://assets-global.website-files.com/5fd11235b3950c2c1a3b6df4/62af6c641a672b3329b9a480_Unintended_Centralities_in_Distributed_Ledgers.pdf"><i>Are Blockchains Decentralized?</i></a> by a large team from the <a href="https://www.trailofbits.com/">Trail of Bits</a> security company examined the economic and many other centralizing forces affecting a wide range of blockchain implementations and concluded that the answer to their question is "No".<sup><a href="#Footnote5">[5]</a></sup></p><p>

The same centralizing economic forces apply to Proof-of-Stake blockchains such as Ethereum. Grant's <i>Memo to the bitcoiners</i> explained the process last February.</p><h3>Trustless because decentralized (3)</h3><p>
Another centralizing force drives pools like GHash. The network creates a new block and rewards the selected node about every ten minutes.  Assuming they're all state-of-the-art, there are currently about 15M rigs mining Bitcoin<sup><a href="#Footnote6">[6]</a></sup>. Their <a href="https://doi.org/10.1016/j.resconrec.2021.105901">economic life is around 18 months</a>, so only 0.5%% of them will ever earn a reward.  The owners of mining rigs pool their efforts, converting a small chance of a huge reward into a steady flow of smaller rewards. On average GHash was getting three rewards an hour.</p><h3>A medium of exchange (1)</h3>
<div><p>
Quote from: Insti, July 17, 2010, 02:33:41 AM</p><blockquote>
How would a Bitcoin snack machine work?<br>
<ol>
<li>You want to walk up to the machine. Send it a bitcoin.</li>
<li>?</li>
<li>Walk away eating your nice sugary snack. (Profit!)</li>
</ol>
You don’t want to have to wait an hour for you transaction to be confirmed.<p>

The vending machine company doesn’t want to give away lots of free candy.</p><p>

How does step 2 work?
</p></blockquote><p>
I believe it’ll be possible for a payment processing company to provide as a service the rapid distribution of transactions with good-enough checking in something like 10 seconds or less.
</p><blockquote>
<small>
Satoshi Nakamoto: 17<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
Bitcoin's ten-minute block time is a problem for real-world buying and selling<sup><a href="#Footnote7">[7]</a></sup>, but the problem is even worse. Network delays mean a transaction isn't final when you see it in a block. Assuming no-one controlled more than 10% of the hashing power, Nakamoto required another 5 blocks to have been added to the chain, so 99.9% finality would take an hour.  With a more realistic 30%, the rule should have been 23 blocks, with finality taking 4 hours<sup><a href="#Footnote8">[8]</a></sup>.</p><p>

Nakamoto's 17<sup>th</sup> July 2010 exchange with Insti shows he understood that the Bitcoin network couldn't be used for ATMs, vending machines, buying drugs or other face-to-face transactions because he went on to describe how a payment processing service layered on top of it would work.</p><h3>A medium of exchange (2)</h3>
<div><p>
assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.</p><p>

two-party escrow smart contracts are ...  simply a game of who gets to declare their choice ﬁrst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract ﬁrst and extort the other party.
</p><blockquote>
<small>
Amir Kafshdar Goharshady, <a href="https://arxiv.org/abs/2110.09857"><i>Irrationality, Extortion, or Trusted Third-parties: Why it is Impossible to Buy and Sell Physical Goods Securely on the Blockchain</i></a>
</small>
</blockquote>
</div><p>
The situation is even worse when it comes to buying and selling real-world objects via programmable blockchains such as Ethereum<sup><a href="#Footnote9">[9]</a></sup>. In 2021 <a href="https://arxiv.org/abs/2110.09857">Amir Kafshdar Goharshady showed that</a><sup><a href="#Footnote10">[10]</a></sup>:</p><blockquote>
assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.<br>
</blockquote><p>
Goharshady <a href="https://arxiv.org/abs/2110.09857">noted that</a>:</p><blockquote>
on the Ethereum blockchain escrows with trusted third-parties are used more often than two-party escrows, presumably because they allow dispute resolution by a human.
</blockquote><p>
And goes on to show that in practice trusted third-party escrow services are essential because two-party escrow smart contracts are:</p><blockquote>
simply a game of who gets to declare their choice ﬁrst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract ﬁrst and extort the other party.
</blockquote><p>
The choice being whether or not the good had been delivered. Given the current enthusiasm for <i>tokenization</i> of physical goods the market for trusted escrow services looks bright.</p><h3>Fast transactions</h3><p>
Actually the delay between submitting a transaction and finality is unpredictable and can be much longer than an hour. Transactions are validated by miners then added to the <a href="https://wiki.bitcoinsv.io/index.php/Transaction_Pools">mempool</a> of pending transactions where they wait until either:</p><ul>
<li>The selected network node chooses it as one of the most profitable to include in its block.</li>
<li>It reaches either its specified timeout or the default of 2 weeks.</li>
</ul>
<p>
This year the demand for transactions has been low, typically under 4 per second, so the backlog has been low, around 40K or under three hours. Last October it peaked at around 14 hours worth.</p><p>

The distribution of transaction wait times is highly skewed.  The median wait is typically around a block time. The proportion of low-fee transactions means the average wait is normally around 10 times that. But when everyone wants to transact the <a href="https://blog.dshr.org/2025/03/bitcoins-fee-spikes.html">ratio spikes</a> to over 40 times.</p><h3>Cheap transactions</h3>
<p>
There are two ways miners can profit from including a transaction in a block:</p><ul>
<li>The fee to be paid to the miner which the user chose to include in the transaction. In effect, transaction slots are auctioned off.</li>
<li>The transactions the miner included in the block to front- and back-run the user's transaction, called <a href="https://ethereum.org/en/developers/docs/mev/">Maximal Extractable Value</a><sup><a href="#Footnote11">[11]</a></sup>:<br>
<blockquote>
Maximal extractable value (MEV) refers to the maximum value that can be extracted from block production in excess of the standard block reward and gas fees by including, excluding, and changing the order of transactions in a block.
</blockquote></li>
</ul><p>
The block size limit means there is a fixed supply of transaction slots, about 7 per second, but the demand for them varies, and thus so does the price.  In normal times the auction for transaction fees means they are much smaller than the block reward. But when everyone wants to transact they suffer <a href="https://blog.dshr.org/2025/03/bitcoins-fee-spikes.html">massive spikes</a>.</p><h3>Secured by Proof-of-Work (1)</h3><p>
In cryptocurrencies "secured" means that the cost of an attack exceeds the potential loot.  The security provided by Proof-of-Work is linear in its cost, unlike techniques such as encryption, whose security is exponential in cost.  It is generally believed that it is impractical to reverse a Bitcoin transaction after about an hour because the miners are wasting such immense sums on Proof-of-Work.  Bitcoin pays these immense sums, but it doesn't get the decentralization they ostensibly pay for.</p><div><p>
Monero, a privacy-focused blockchain network, has been undergoing an attempted 51% attack — an existential threat to any blockchain. In the case of a successful 51% attack, where a single entity becomes responsible for 51% or more of a blockchain's mining power, the controlling entity could reorganize blocks, attempt to double-spend, or censor transactions.</p><p>

A company called Qubic has been waging the 51% attack by offering economic rewards for miners who join the Qubic mining pool. They claim to be "stress testing" Monero, though many in the Monero community have condemned Qubic for what they see as a malicious attack on the network or a marketing stunt.
</p><blockquote>
<small>
Molly White: <a href="https://www.web3isgoinggreat.com/?id=monero-51-attack"><i>Monero faces 51% attack</i></a>
</small>
</blockquote>
</div><p>
The advent of "mining as a service" about 7 years ago made 51% attacks against smaller Proof-of-Work alt-coin such as <a href="https://qz.com/1287701/bitcoin-golds-51-attack-is-every-cryptocurrencys-nightmare-scenario/">Bitcoin Gold</a> endemic. In August <a href="https://www.niemanlab.org/2025/08/independent-journalist-molly-white-knows-how-to-follow-the-memecoin/">Molly White</a> reported that <a href="https://www.web3isgoinggreat.com/?id=monero-51-attack"><i>Monero faces 51% attack</i></a>:</p><p>

In 2018's <a href="http://www.nber.org/papers/w24717"><i>The Economic Limits Of Bitcoin And The Blockchain</i></a> Eric Budish of the Booth School analyzed two versions of the 51% attack. I summarized his analysis of the classic multiple spend attack <a href="https://blog.dshr.org/2018/06/cryptocurrencies-have-limits.html">thus</a>:</p><blockquote>
Note that only Bitcoin and Ethereum among cryptocurrencies with "market cap" over $100M would cost more than $100K to attack. The total "market cap" of these 8 currencies is $271.71B and the total cost to 51% attack them is $1.277M or 4.7E-6 of their market cap.
</blockquote><p>
His key insight was that to ensure that 51% attacks were uneconomic, the reward for a block, implicitly the transaction tax, plus the fees had to be greater than the maximum value of the transactions in it. The total transaction cost (reward + fee) typically peaks around 1.8% but is normally between 0.6% and 0.8%, or around 150 times less than Budish's safety criterion.  The result is that a conspiracy between a few large pools could find it economic to mount a 51% attack.</p><h3>Secured by Proof-of-Work (2)</h3>
<div><p>
However, ∆<sub>attack</sub> is something of a “pick your poison” parameter. If ∆<sub>attack</sub> is small, then the system is vulnerable to the double-spending attack ... and the implicit transactions tax on economic activity using the blockchain has to be high. If ∆<sub>attack</sub> is large, then a short time period of access to a large amount of computing power can sabotage the blockchain.
</p><blockquote>
<small>
Eric Budish: <a href="http://www.nber.org/papers/w24717"><i>The Economic Limits Of Bitcoin And The Blockchain</i></a>
</small>
</blockquote>
</div><p>
But everyone assumes the pools won't do that. Budish further analyzed the effects of a multiple spend attack. It would be public, so it would in effect be sabotage, decreasing the Bitcoin price by a factor ∆<sub>attack</sub>. He <a href="http://www.nber.org/papers/w24717">concludes</a> that if the decrease is small, then double-spending attacks are feasible and the per-block reward plus fee must be large, whereas if it is large then access to the hash power of a few large pools can quickly sabotage the currency.</p><p>

The implication is that miners, motivated to keep fees manageable, believe ∆<sub>attack</sub> is large. Thus Bitcoin is secure because those who could kill the golden goose don't want to.</p><h3>Secured by Proof-of-Work (3)</h3>
<p>
The following year, in <a href="https://www.bis.org/publ/work765.pdf"><i>Beyond the doomsday economics of “proof-of-work” in cryptocurrencies</i></a>, Raphael Auer of the Bank for International Settlements showed that the problem Budish identified was inevitable<sup><a href="#Footnote12">[12]</a></sup>:</p><blockquote>
proof-of-work can only achieve payment security if mining income is high, but the transaction market cannot generate an adequate level of income. ... the economic design of the transaction market fails to generate high enough fees.
</blockquote><p>
In other words, the security of Bitcoin's blockchain depends upon inflating the currency with block rewards.  This problem is excerbated by Bitcoin's regular "halvenings" reducing the block reward. To maintain miner's current income after the next halvening in less than three years the "price" would need to be over $200K; security depends upon the "price" appreciating faster than 20%/year.</p><p>

Once the block reward gets small, safety requires the fees in a block to be worth more than the value of the transactions in it.  But everybody has decided to ignore Budish and Auer.</p><h3>Secured by Proof-of-Work (4)</h3>
<p>
In 2024 Soroush Farokhnia &amp; Amir Kafshdar Goharshady's <a href="https://hal.science/hal-04616643/"><i>Options and Futures Imperil Bitcoin's Security</i></a>:</p><blockquote>
showed that (i) a successful block-reverting attack does not necessarily require ... a majority of the hash power; (ii) obtaining a majority of the hash power ... costs roughly 6.77 billion ...  and (iii) Bitcoin derivatives, i.e. options and futures, imperil Bitcoin’s security by creating an incentive for a block-reverting/majority attack.
</blockquote><p>
They assume that an attacker would purchase enough state-of-the-art hardware for the attack. Given Bitmain's dominance in mining ASICs, such a purchase is unlikely to be feasible.</p><h3>Secured by Proof-of-Work (5)</h3>
<p>
But it would not be necessary. Mining is a very competitive business, and power is the major cost<sup><a href="#Footnote13">[13]</a></sup>. Making a profit requires both cheap power and early access to the latest, most efficient chips. So it wasn't a surprise that Ferreira <i>et al</i>'s <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3320437"><i>Corporate capture of blockchain governance</i></a> showed that:</p><blockquote>
As of March 2021, the pools in Table 1 collectively accounted for 86% of the total hash rate employed. All but one pool (Binance) have known links to Bitmain Technologies, the largest mining ASIC producer.
<sup><a href="#Footnote14">[14]</a></sup>
</blockquote>
<h3>Secured by Proof-of-Work (6)</h3>
<p>
Bitmain, a Chinese company, exerts significant control of Bitcoin. China has firmly suppressed domestic use of cryptocurrencies, whereas the current administration seems intent on integrating them (and their inevitable grifts) into the US financial system. Except for Bitmain, no-one in China gets eggs from the golden goose.  This asymmetry provides China with an way to disrupt the US financial system.</p><p>
It would be important to prevent the disruption being attributed to China. A necessary precursor would therefore be to obscure the extent of Bitmain-affiliated pools' mining power. This has been a significant trend in the past year, note the change in the "unknown" in the graphs from 38 to 305. There could be other explanations, but whether or not intentionally this is creating a weapon.<sup><a href="#Footnote15">[15]</a></sup></p><h3>Secured by cryptography (1)</h3><p>
The dollars in your bank account are simply an entry in the bank's private ledger tagged with your name. You control this entry, but what you own is a claim on the bank<sup><a href="#Footnote16">[16]</a></sup>.  Similarly, your cryptocurrency coins are effectively an entry in a public ledger tagged with the public half of a key pair. The two differences are that:</p><ul>
<li>No ownership is involved, so you have no recourse if something goes wrong.</li>
<li>Anyone who knows the secret half of the key pair controls the entry. Since it is extremely difficult to stop online secrets leaking, something is likely to go wrong<sup><a href="#Footnote17">[17]</a></sup>.</li>
</ul>
<p>
The secret half of your key can leak via what Randall Munro depicted as a "<a href="https://xkcd.com/538/">wrench attack</a>", via phishing, social engineering, <a href="https://blog.dshr.org/2025/03/software-supply-chain-attack.html">software supply chain attacks</a><sup><a href="#Footnote18">[18]</a></sup>, and other forms of malware.  Preventing these risks requires you to maintain an <a href="https://blog.dshr.org/2022/02/inadequate-opsec.html">extraordinary level of operational security</a>.</p><h3>Secured by cryptography (2)</h3><p>
Even <i>perfect</i> opsec may not be enough. Bitcoin and most cryptocurrencies use two cryptographic algorithms, <a href="https://www.kraken.com/learn/how-do-cryptocurrencies-use-cryptography">SHA256 for hashing and ECDSA for signatures</a>.</p><div><p>
Quote from: llama on July 01, 2010, 10:21:47 PM</p><blockquote>
Satoshi, That would indeed be a solution if SHA was broken (certainly the more likely meltdown), because we could still recognize valid money owners by their signature (their private key would still be secure).<p>

However, if something happened and the signatures were compromised (perhaps integer factorization is solved, quantum computers?), then even agreeing upon the last valid block would be worthless.
</p></blockquote><p>
True, if it happened suddenly. If it happens gradually, we can still transition to something stronger. When you run the upgraded software for the first time, it would re-sign all your money with the new stronger signature algorithm. (by creating a transaction sending the money to yourself with the stronger sig)
</p><blockquote>
<small>
Satoshi Nakamoto: 10<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
On 10<sup>th</sup> July 2010 Nakamoto addressed the issue of what would happen if either of these algorithms were compromised.  There are three problems with his response; that compromise is likely in the near future, when it does Nakamoto's fix is inadequate, and there is a huge incentive for it to happen suddenly:</p><h3>Secured by cryptography (3)</h3><p>
Divesh Aggarwal <i>et al</i>'s 2019 paper <a href="https://arxiv.org/abs/1710.10377"><i>Quantum attacks on Bitcoin, and how to protect against them</i></a> noted that:
</p><blockquote>
the elliptic curve signature scheme used by Bitcoin is much more at risk, and could be completely broken by a quantum computer as early as 2027, by the most optimistic estimates.
</blockquote><p>
Their "most optimistic estimates" are likely to be correct; <a href="https://www.psiquantum.com/blueprint">PsiQuantum</a> expects to have two 1M qubit computers operational in 2027<sup><a href="#Footnote19">[19]</a></sup>. Each should be capable of breaking an ECDSA key in under a week.</p><p>

Bitcoin's transition to post-quantum cryptography faces a major problem because, to transfer coins from an ECDSA wallet to a post-quantum wallet, you need the key for the ECDSA wallet. Chainalysis <a href="https://blog.chainalysis.com/reports/money-supply">estimates that</a>:
</p><blockquote>
about 20% of all Bitcoins have been "lost", or in other words are sitting in wallets whose keys are inaccessible
</blockquote><p>
An example is the notorious <a href="https://en.wikipedia.org/wiki/Bitcoin_buried_in_Newport_landfill">hard disk in the garbage dump</a>. A sufficiently powerful quantum computer could recover the lost keys.</p><p>

The incentive for it to happen suddenly is that, even if Nakamoto's fix were in place, someone with access to the first sufficiently powerful quantum computer could transfer 20% of all Bitcoin, currently worth $460B, to <a href="https://blog.dshr.org/2025/05/the-740b-prize.html">post-quantum wallets they controlled</a>. This would be a 230x return on the investment in PsiQuantum.</p><h3>Privacy-preserving</h3>
<div><p>
privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.</p><p>

As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.</p><p>

Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.</p><blockquote>
<small>
Satoshi Nakamoto: <a href="https://bitcoin.org/bitcoin.pdf"><i>Bitcoin: A Peer-to-Peer Electronic Cash System</i></a>
</small>
</blockquote>
</div><p>
Nakamoto addressed the concern that, unlike DigiCash, because Bitcoin's blockchain was public it wasn't <a href="https://bitcoin.org/bitcoin.pdf">anonymous</a>:</p><blockquote>
privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.
</blockquote><p>
This is true but misleading. In practice, users need to use exchanges and other services that can tie them to a public key.
There is a flourishing ecosystem of companies that deanonymize wallets by <a href="https://search.worldcat.org/title/1298713583">tracing the web of transactions</a>. Nakamoto <a href="https://bitcoin.org/bitcoin.pdf">added</a>:</p><blockquote>
As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.
</blockquote><p>
This advice is just unrealistic. As <a href="https://blog.mollywhite.net/abuse-and-harassment-on-the-blockchain/">Molly White wrote</a><sup><a href="#Footnote20">[20]</a></sup>:</p><blockquote>
funds in a wallet have to come from somewhere, and it’s not difficult to infer what might be happening when your known wallet address suddenly transfers money off to a new, empty wallet. 
</blockquote><p>
Nakamoto <a href="https://bitcoin.org/bitcoin.pdf">acknowledged</a>:</p><blockquote>
Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.
</blockquote><p>
For more than a decade <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">Jamison Lopp</a> has been tracking what happens when a wallet with significant value is deanonymized, and it is a <a href="https://blog.dshr.org/2025/05/the-risks-of-hodl-ing.html">serious risk to life and limbs</a><sup><a href="#Footnote21">[21]</a></sup>.</p><h3>One more risk</h3><p>
I have steered clear of the financial risks of cryptocurrencies. It  may appear that the endorsement of the current administration has effectively removed their financial risk. But the technical and operational risks remain, and I should note another technology-related risk.</p><p>
Equities are currently being <a href="https://www.bloodinthemachine.com/p/the-ai-bubble-is-so-big-its-propping">inflated by the AI bubble</a>. The AI platforms are <a href="https://blog.dshr.org/2025/08/the-drugs-are-taking-hold.html">running the drug-dealer's algorithm</a>, "the first one's free", burning cash by offering their product free or massively under-priced. This cannot last; only <a href="https://www.zdnet.com/article/only-8-of-americans-would-pay-extra-for-ai-according-to-zdnet-aberdeen-research/">8% of their users would pay</a> even the current price. <a href="https://garymarcus.substack.com/p/openais-waterloo">OpenAI's August launch of GPT-5</a>, which was about <a href="https://www.theregister.com/2025/08/13/gpt_5_cost_cutting/">cost-cutting not better functionality</a>, and <a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed">Anthropic's cost increases</a> were both panned by the customers who do pay.  AI may deliver some value, but it doesn't come close to the cost of delivering it<sup><a href="#Footnote22">[22]</a></sup>.</p><p>

There is likely to be an epic AI equity bust. <a href="https://www.ft.com/content/7052c560-4f31-4f45-bed0-cbc84453b3ce">Analogies</a> are being drawn to the <a href="https://www.noahpinion.blog/p/will-data-centers-crash-the-economy">telecom boom</a>, but <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up"><i>The Economist</i> reckons</a><sup><a href="#Footnote23">[23]</a></sup>:</p><blockquote>
the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.
</blockquote>
<p>
History shows a fairly strong and increasing correlation between equities and cryptocurrencies, so they will get dragged down too. The automatic liquidation of leveraged long positions in DeFi will start, causing a self-reinforcing downturn. Periods of heavy load such as this tend to reveal bugs in IT systems, and especially in "smart contracts", as their assumptions of adequate resources and timely responses are violated.</p><p>
Experience shows that Bitcoin's limited transaction rate and the fact that the Ethereum computer that runs all the "smart contracts" is 1000 times slower than a $50 Raspberry Pi 4<sup><a href="#Footnote24">[24]</a></sup> lead to major slow-downs and fee spikes during panic selling, exacerbated by the fact that the panic sales are public<sup><a href="#Footnote25">[25]</a></sup>.</p><h3>Conclusion</h3><p>
The fascinating thing about cryptocurrency technology is the number of ways people have developed and how much they are willing to pay to avoid actually using it.  What other transformative technology has had people desperate not to use it?</p><p>

The whole of TradFi has been erected on this much worse infrastructure, including exchanges, <a href="https://blog.dshr.org/2022/04/grayscale-bitcoin-trust.html">closed-end funds</a>, ETFs, <a href="https://en.wikipedia.org/wiki/Hypothec#Hypothecation_and_rehypothecation">rehypothecation</a>, and derivatives.  Clearly, the only reason for doing so is to escape regulation and extract excess profits from what would otherwise be crimes.</p><h3>Footnotes</h3>
<ol start="1">
<li id="Footnote1">
The cause was the <a href="https://www.youtube.com/watch?v=twrduL8aNGE">video</a> of a talk I gave at Stanford in 2022 entitled <a href="https://blog.dshr.org/2022/02/ee380-talk.html"><i>Can We Mitigate The Externalities Of Cryptocurrencies?</i></a>. It was an updated version of a talk at the 2021 <a href="https://blog.dshr.org/2021/12/talk-at-ttivanguard-conference.html">TTI/Vanguard conference</a>. The talk conformed to <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">Betteridge's Law of Headlines</a> in that the answer was "no".<br>
</li>
<li id="Footnote2">
Paper libraries form a model fault-tolerant system. It is highly replicated and decentralized. Libraries cooperate via inter-library loan and copy to deliver a service that is far more reliable than any individual library. 
</li>
<li id="Footnote3">
The importance Satoshi Nakamoto attached to trustlessness can be seen from his <a href="https://web.archive.org/web/20110822150926/https://p2pfoundation.ning.com/forum/topics/bitcoin-open-source">release note for Bitcoin 0.1</a>:
<blockquote>
The root problem with conventional currency is all the trust that's required to make it work. The central bank must be trusted not to debase the currency, but the history of fiat currencies is full of breaches of that trust. Banks must be trusted to hold our money and transfer it electronically, but they lend it out in waves of credit bubbles with barely a fraction in reserve. We have to trust them with our privacy, trust them not to let identity thieves drain our accounts. Their massive overhead costs make micropayments impossible.
</blockquote>
The problem with this ideology is that trust (but verify) is an incredibly effective optimization in almost any system. For example, Robert Putnam <i>et al</i>'s <a href="https://search.worldcat.org/title/52234023"><i>Making Democracy Work: Civic Traditions in Modern Italy</i></a> shows that the difference between the economies of Northern and Southern Italy is driven by the much higher level of trust in the North.<p>

Bitcoin's massive cost is a result of its lack of trust. Users pay this massive cost but they don't get a trustless system, they just get a system that makes the trust a bit harder to see.</p><p>

In response to Nakamoto's diatribe, note that:</p><ul>
<li>"trusted not to debase the currency", but Bitcoin's security depends upon debasing the currency.</li>
<li>"waves of credit bubbles", is a pretty good description of the cryptocurrency market.</li>
<li>"not to let identity thieves drain our accounts", see Molly White's <a href="https://www.web3isgoinggreat.com/"><i>Web3 is Going Just Great</i></a>.</li>
<li>"massive overhead costs". The current cost per transaction is <a href="https://www.blockchain.com/explorer/charts/cost-per-transaction">around $100</a>.</li>
</ul>
I rest my case.
</li>
<li id="Footnote4">
The problem of trusting mining pools is actually much worse. There is nothing to stop pools <strike>conspiring</strike> coordinating. In 2017 <a href="https://en.wikipedia.org/wiki/Vitalik_Buterin">Vitalik Buterin</a>, co-founder of Ethereum, published <a href="https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274"><i>The Meaning of Decentralization</i></a>:<br>
<blockquote>
In the case of blockchain protocols, the mathematical and economic reasoning behind the safety of the consensus often relies crucially on the uncoordinated choice model, or the assumption that the game consists of many small actors that make decisions independently. If any one actor gets more than 1/3 of the mining power in a proof of work system, they can gain outsized profits by selfish-mining. However, can we really say that the uncoordinated choice model is realistic when 90% of the Bitcoin network’s mining power is well-coordinated enough to show up together at the same conference? 
</blockquote>
See <a href="https://blog.dshr.org/2024/05/sufficiently-decentralized.html"><i>"Sufficiently Decentralized"</i></a> for a review of evidence from a Protos article entitled <a href="https://protos.com/new-research-suggests-bitcoin-mining-centralized-around-bitmain/"><i>New research suggests Bitcoin mining centralized around Bitmain</i></a> that concludes:<br>
<blockquote>
In all, it seems unlikely that up to nine major bitcoin mining pools use a shared custodian for coinbase rewards unless a single entity is behind all of their operations. 
</blockquote>
The "single entity" is clearly Bitmain.
</li>
<li id="Footnote5">
Peter Ryan, a reformed Bitcoin enthusiast, noted another form of centralization in <a href="https://www.compactmag.com/article/money-by-vile-means/"><i>Money by Vile Means</i></a>:<br>
<blockquote>
Bitcoin is anything but decentralized: Its functionality is maintained by a small and privileged clique of software developers who are funded by a centralized cadre of institutions. If they wanted to change Bitcoin’s 21 million coin finite supply, they could do it with the click of a keyboard.
</blockquote>
His account of the politics behind the argument over raising the Bitcoin block size should dispel any idea of Bitcoin's decentralized nature.  He also <a href="https://www.compactmag.com/article/money-by-vile-means/">notes</a>:<br>
<blockquote>
By one estimate from Hashrate Index, Foundry USA and Singapore-based AntPool control more than 50 percent of computing power, and the top ten mining pools control over 90 percent. Bitcoin blogger 0xB10C, who <a href="https://b10c.me/blog/015-bitcoin-mining-centralization/">analyzed</a> mining data as of April 15, 2025, found that centralization has gone even further than this, “with only six pools mining more than 95 percent of the blocks.”
</blockquote>
</li>
<li id="Footnote6">
The <a href="https://perfecthashrate.com/asics/antminer-s17-mining-hashrate/">Bitmain S17</a> comes in 4 versions with hash rates from 67 to 76 TH/s. Lets assume 70TH/s. As I write the Bitcoin hash rate is about 1 billion TH/s. So if they were all mid-range S17s there would be around 15M mining. If their economic life were 18 months, there would be 77,760 rewards. Thus only 0.5% of them would earn a reward.<p>

In December 2021 Alex de Vries and Christian Stoll <a href="https://doi.org/10.1016/j.resconrec.2021.105901">estimated that</a>:</p><blockquote>
The average time to become unprofitable sums up to less than 1.29 years. 
</blockquote>
It has been obvious since mining ASICs first hit the market that, apart from access to cheap or free electricity, there were two keys to profitable mining:<br>
<ol>
<li>Having close enough ties to Bitmain to get the latest chips early in their 18-month economic life.</li>
<li>Having the scale to buy Bitmain chips in the large quantities that get you early access.</li>
</ol>
</li>
<li id="Footnote7">
See David Gerard's account of Steve Early's experiences accepting Bitcoin in his chain of pubs in <a href="https://davidgerard.co.uk/blockchain/book/"><i>Attack of the 50 Foot Blockchain</i></a> Page 94.
<a href="https://www.kansascityfed.org/research/payments-system-research-briefings/us-consumers-use-of-cryptocurrency-for-payments/"><i>U.S. Consumers’ Use of Cryptocurrency for Payments</i></a> by Fumiko Hayashi and Aditi Routh of the Kansas City Fed reports that:<br>
<blockquote>
The share of U.S. consumers who report using cryptocurrency for payments—purchases, money transfers, or both—has been very small and has declined slightly in recent years. The light blue line in Chart 1 shows that this share declined from nearly 3 percent in 2021 and 2022 to less than 2 percent in 2023 and 2024.
</blockquote>
</li>
<li id="Footnote8">
User DeathAndTaxes on Stack Exchange explains the <a href="https://bitcoin.stackexchange.com/questions/1170/why-is-6-the-number-of-confirms-that-is-considered-secure">6 block rule</a>:<br>
<blockquote>
<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Rdlmwa4wUnJuwPrbjcs9qy3a6vTGf343LjXoHcyTkQBs3zce4FdNsSC__zKbwanXirtqHq4QW2CI_urbBybfLi3LunH-ol3nPsIky49xE1UI6-vMCEFTS7C-DvBe3_l4_I-lZ46rO54i4a-B4nKrwSzNcM18CmvRMZOZY025NJNtyo5pAC_fondpV7wb/s416/SatoshiTable.png"><img data-original-height="416" data-original-width="380" height="200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Rdlmwa4wUnJuwPrbjcs9qy3a6vTGf343LjXoHcyTkQBs3zce4FdNsSC__zKbwanXirtqHq4QW2CI_urbBybfLi3LunH-ol3nPsIky49xE1UI6-vMCEFTS7C-DvBe3_l4_I-lZ46rO54i4a-B4nKrwSzNcM18CmvRMZOZY025NJNtyo5pAC_fondpV7wb/w183-h200/SatoshiTable.png" width="183"></a>
p is the chance of attacker eventually getting longer chain and reversing a transaction (0.1% in this case). q is the % of the hashing power the attacker controls. z is the number of blocks to put the risk of a reversal below p (0.1%).<p>

So you can see if the attacker has a small % of the hashing power 6 blocks is sufficient. Remember 10% of the network at the time of writing is ~100GH/s. However if the attacker had greater % of hashing power it would take increasingly longer to be sure a transaction can't be reversed.</p><p>

If the attacker had significantly more hashpower say 25% of the network it would require 15 confirmation to be sure (99.9% probability) that an attacker can't reverse it.
</p></blockquote>
For example, last May Foundry USA had more than 30% of the hash power, so the rule should have been 24 not 6, and finality should have taken 4 hours.
</li>
<li id="Footnote9">
To be fair, Ethereum has introduced at least one genuine innovation, <a href="https://blog.dshr.org/2023/06/flash-loans.html"><i>Flash Loans</i></a>. In <a href="https://bankunderground.co.uk/2023/05/25/flash-loans-flash-attacks-and-the-future-of-defi/"><i>Flash loans, flash attacks, and the future of DeFi</i></a> Aidan Saggers, Lukas Alemu and Irina Mnohoghitnei of the Bank of England provide an excellent overview of them.
Back in 2021 Kaihua Qin, Liyi Zhou, Benjamin Livshits, and Arthur Gervais from Imperial College posted <a href="https://arxiv.org/pdf/2003.03810.pdf"><i>Attacking the defi ecosystem with flash loans for fun and profit</i></a>, analyzing and optimizing two early flash loan attacks:<br>
<blockquote>
We show quantitatively how transaction atomicity increases the arbitrage revenue. We moreover analyze two existing attacks with ROIs beyond 500k%. We formulate finding the attack parameters as an optimization problem over the state of the underlying Ethereum blockchain and the state of the DeFi ecosystem. We show how malicious adversaries can efficiently maximize an attack profit and hence damage the DeFi ecosystem further. Specifically, we present how two previously executed attacks can be “boosted” to result in a profit of 829.5k USD and 1.1M USD, respectively, which is a boost of 2.37× and 1.73×, respectively. 
</blockquote>
They predicted an upsurge in attacks since "flash loans democratize the attack, opening this strategy to the masses". They were right, as you can see from Molly White's <a href="https://www.web3isgoinggreat.com/?collection=flash-loan-attack">list of flash loan attacks</a>.
</li>
<li id="Footnote10">
This is one of a whole series of <a href="https://blog.dshr.org/2022/09/impossibilities.html"><i>Impossibilities</i></a>, many imposed on Ethereum by fundamental results in computer science because it is a Turing-complete programming environment.
</li>
<li id="Footnote11">
For details of the story behind Miners' Extractable Value (MEV), see these posts:<br>
<ol>
<li><a href="https://blog.dshr.org/2020/11/the-order-flow.html"><i>The Order Flow</i></a> from November 2020.</li>
<li><a href="https://blog.dshr.org/2022/04/ethereum-has-issues.html"><i>Ethereum Has Issues</i></a> from April 2022.</li>
<li><a href="https://blog.dshr.org/2022/09/miners-extractable-value.html"><i>Miners' Extractable Value</i></a> From September 2022.</li>
</ol>

The first links to two must-read posts. The first is from Dan Robinson and Georgios Konstantopoulos, <a href="https://medium.com/@danrobinson/ethereum-is-a-dark-forest-ecc5f0505dff"><i>Ethereum is a Dark Forest</i></a>:<br>
<blockquote>
It’s no secret that the Ethereum blockchain is a highly adversarial environment. If a smart contract can be exploited for profit, it eventually will be. The frequency of new hacks indicates that some very smart people spend a lot of time examining contracts for vulnerabilities.<p>

But this unforgiving environment pales in comparison to the mempool (the set of pending, unconfirmed transactions). If the chain itself is a battleground, the mempool is something worse: a dark forest. 
</p></blockquote>
The second is from Samczsun, <a href="https://samczsun.com/escaping-the-dark-forest/"><i>Escaping the Dark Forest</i></a>. It is an account of how:<br>
<blockquote>
On September 15, 2020, a small group of people worked through the night to rescue over 9.6MM USD from a vulnerable smart contract.
</blockquote>
Note in particular that MEV poses a risk to the integrity of blockchains. In <a href="https://doi.org/10.48550/arXiv.2203.15930"><i>Extracting Godl [sic] from the Salt Mines: Ethereum Miners Extracting Value</i></a> Julien Piet, Jaiden Fairoze and Nicholas Weaver examine the use of transactions that avoid the mempool, finding that:<br>
<blockquote>
(i) 73% of private transactions hide trading activity or re-distribute miner rewards, and 87.6% of MEV collection is accomplished with privately submitted transactions, (ii) our algorithm finds more than $6M worth of MEV profit in a period of 12 days, two thirds of which go directly to miners, and (iii) MEV represents 9.2% of miners' profit from transaction fees.<p>

Furthermore, in those 12 days, we also identify four blocks that contain enough MEV profits to make time-bandit forking attacks economically viable for large miners, undermining the security and stability of Ethereum as a whole. 
</p></blockquote>
When they say "large miners" they mean more than 10% of the power.
</li>
<li id="Footnote12">
Back in 2016 Arvind Narayanan's group at Princeton had published a related instability in Carlsten <i>et al</i>'s <a href="http://randomwalker.info/publications/mining_CCS.pdf"><i>On the instability of bitcoin without the block reward</i></a>. Narayanan summarized the paper in a <a href="https://freedom-to-tinker.com/2016/10/21/bitcoin-is-unstable-without-the-block-reward/">blog post</a>:<br>
<blockquote>
Our key insight is that with only transaction fees, the variance of the miner reward is very high due to the randomness of the block arrival time, and it becomes attractive to fork a “wealthy” block to “steal” the rewards therein. 
</blockquote>
</li>
<li id="Footnote13">
The leading source of data on which to base Bitcoin's carbon footprint is the <a href="https://cbeci.org/"><i>Cambridge Bitcoin Energy Consumption Index</i></a>. As I write their central estimate is that Bitcoin consumes 205TWh/year, or between Thailand and Vietnam.
</li>
<li id="Footnote14">
Ferreira <i>et al</i> <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3320437">write</a>:<br>
<blockquote>
AntPool and BTC.com are fully-owned subsidiaries of Bitmain. Bitmain is the largest investor in ViaBTC. Both F2Pool and BTC.TOP are partners of BitDeer, which is a Bitmain-sponsored cloud-mining service. The parent companies of Huobi.pool and OkExPool are strategic partners of Bitmain. Jihan Wu, Bitmain’s founder and chairman, is also an adviser of Huobi (one of the largest cryptocurrency exchanges in the world and the owner of Huobi.pool).
</blockquote>
This makes economic sense. Because mining rigs depreciate quickly, profit depends upon <a href="https://blog.dshr.org/2018/05/asics-and-mining-centralization.html">early access to the latest chips</a>.
</li>
<li id="Footnote15">
See <a href="https://blog.dshr.org/2025/05/who-is-mining-bitcoin.html"><i>Who Is Mining Bitcoin?</i></a> for more detail on the state of mining and its gradual obfuscation.
</li>
<li id="Footnote16">
In this context to say you "control" your entry in the bank's ledger is an oversimplification. You can instruct the bank to perform transactions against your entry (and no-one else's) but the bank can reject your instructions. For example if they would overdraw your account, or send money to a sanctioned account. The key point is that your ownership relationship with the bank comes with a dispute resolution system and the ability to reverse transactions. Your cryptocurrency wallet has neither.
</li><li id="Footnote17">
<a href="https://www.web3isgoinggreat.com/"><i>Web3 is Going Just Great</i></a> is Molly White's list of things that went wrong. The cumulative losses she tracks currently stand at over $79B.
</li>
<li id="Footnote18">
Your secrets are especially at risk if anyone in your software supply chain use a build system implemented using AI "vibe coding". David Gerard's <a href="https://pivot-to-ai.com/2025/08/29/vibe-coded-build-system-nx-gets-hacked-steals-vibe-coders-crypto/"><i>Vibe-coded build system NX gets hacked, steals vibe-coders’ crypto</i></a> details a truly beautiful example of the extraordinary level of incompetence this reveals.
</li>
<li id="Footnote19">
<a href="https://en.wikipedia.org/wiki/IBM_Heron">IBM's Heron</a>, which HSBC recently used to <a href="https://www.bloomberg.com/news/articles/2025-09-24/hsbc-says-it-s-beaten-wall-street-rivals-with-new-quantum-trial">grab headlines</a>, has 156 qubits.
</li>
<li id="Footnote20">
Molly White's <a href="https://blog.mollywhite.net/abuse-and-harassment-on-the-blockchain/"><i>Abuse and harassment on the blockchain</i></a> is an excellent overview of the privacy risks inherent to real-world transactions on public blockchain ledgers:<br>
<blockquote>
Imagine if, when you Venmo-ed your Tinder date for your half of the meal, they could now see every other transaction you’d ever made—and not just on Venmo, but the ones you made with your credit card, bank transfer, or other apps, and with no option to set the visibility of the transfer to “private”. The split checks with all of your previous Tinder dates? That monthly transfer to your therapist? The debts you’re paying off (or not), the charities to which you’re donating (or not), the amount you’re putting in a retirement account (or not)? The location of that corner store right by your apartment where you so frequently go to grab a pint of ice cream at 10pm? Not only would this all be visible to that one-off Tinder date, but also to your ex-partners, your estranged family members, your prospective employers. An abusive partner could trivially see you siphoning funds to an account they can’t control as you prepare to leave them.
</blockquote>
</li>
<li id="Footnote21">
In <a href="https://blog.dshr.org/2025/05/the-risks-of-hodl-ing.html"><i>The Risks Of HODL-ing</i></a> I go into the details of the attack on the <a href="https://www.nytimes.com/2025/04/24/magazine/crybercrime-crypto-minecraft.html">parents of Veer Chetal</a>, who had unwisely live-streamed the social engineering that stole $243M from a resident of DC.<p>

Anyone with significant cryptocurrency wallets needs to follow Jamison Lopp's <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">Known Physical Bitcoin Attacks</a>.
</p></li>
<li id="Footnote22">

Torsten Sløk's <a href="https://www.apolloacademy.com/ai-has-moved-from-a-niche-sector-to-the-primary-driver-of-all-vc-investment/"><i>AI Has Moved From a Niche Sector to the Primary Driver of All VC Investment</i></a> leads with this graph, one of the clearest signs that we're in a bubble.<p>

Whether AI delivers net value in most cases is debatable. "Vibe coding" is touted as the example of increasing productivity, but the <a href="https://pivot-to-ai.com/2025/07/11/ai-coders-think-theyre-20-faster-but-theyre-actually-19-slower/">experimental</a> <a href="https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding">evidence</a> is that it decreases productivity. Kate Niederhoffer <i>et al</i>'s <i>Harvard Business Review</i> article <a href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity"><i>AI-Generated "Workslop” Is Destroying Productivity</i></a> explains one effect:</p><blockquote>
Employees are using AI tools to create low-effort, passable looking work that ends up creating more work for their coworkers. On social media, which is increasingly clogged with low-quality AI-generated posts, this content is often referred to as “AI slop.” In the context of work, we refer to this phenomenon as “<a href="https://www.betterup.com/workslop">workslop</a>.” We define workslop as <i>AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task</i>.<p>

Here’s how this happens. As AI tools become more accessible, workers are increasingly able to quickly produce polished output: well-formatted slides, long, structured reports, seemingly articulate summaries of academic papers by non-experts, and usable code. But while some employees are using this ability to polish good work, others use it to create content that is actually unhelpful, incomplete, or missing crucial context about the project at hand. The insidious effect of workslop is that it shifts the burden of the work downstream, requiring the receiver to interpret, correct, or redo the work. In other words, it transfers the effort from creator to receiver.
</p></blockquote>
David Gerard's <a href="https://pivot-to-ai.com/2025/09/23/workslop-bad-study-but-an-excellent-word/"><i>Workslop: bad ‘study’, but an excellent word</i></a> points out that:<br>
<blockquote>
<i>Unfortunately</i>, this article pretends to be a writeup of a study — but it’s actually a promotional brochure for enterprise AI products. It’s an unlabeled advertising feature.
</blockquote>
And goes on to explain where the workslop comes from:<br>
<blockquote>
Well, you know how you get workslop — it’s when your boss mandates you use AI. He can’t say what he wants you to use it for. But you’ve been told. You’ve got metrics on how much AI you use. They’re watching and they’re measuring.
</blockquote>
Belle Lin and Steven Rosenbush's <a href="https://www.wsj.com/articles/stop-worrying-about-ais-return-on-investment-d5cbc822"><i>Stop Worrying About AI’s Return on Investment</i></a> describes goalposts being moved:<br>
<blockquote>
Return on investment has evaded chief information officers since AI started <a href="https://www.wsj.com/articles/companies-had-fun-experimenting-with-ai-now-they-have-to-show-the-returns-2a683592">moving from early experimentation to more mature implementations</a> last year. But while AI is still rapidly evolving, CIOs are recognizing that traditional ways of recognizing gains from the technology aren’t cutting it.<p>

Tech leaders at the WSJ Leadership Institute’s Technology Council Summit on Tuesday said racking up a few minutes of efficiency here and there don’t add up to a meaningful way of measuring ROI.
</p></blockquote>
Given the hype and the massive sunk costs, admitting that there is no there there would be a career-limiting move.<p>

None of this takes account of the productivity externalities of AI, such as <a href="https://www.404media.co/librarians-are-being-asked-to-find-ai-hallucinated-books/"><i>Librarians Are Being Asked to Find AI-Hallucinated Books</i></a>, <a href="https://pivot-to-ai.com/2025/02/15/how-ai-slop-generators-started-talking-about-vegetative-electron-microscopy/">academic journals' reviewers' time wasted by AI slop papers</a>, <a href="https://pivot-to-ai.com/2025/06/07/uk-high-court-to-lawyers-cut-the-chatgpt-or-else/">judges' time wasted with hallucinated citations</a>, a flood of generated <a href="https://www.theguardian.com/technology/2025/jul/10/ai-generated-child-sexual-abuse-videos-surging-online-iwf">child sex abuse videos</a>, <a href="https://www.noemamag.com/the-last-days-of-social-media/">the death of social media</a> and a <a href="https://www.washingtonpost.com/technology/2025/09/20/ai-hacking-cybersecurity-cyberthreats/">vast new cyberthreat landscape</a>.
</p></li>
<li id="Footnote23">
<i>The Economist</i> writes in <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up"><i>What if the AI stockmarket blows up?</i></a>:<br>
<blockquote>
we picked ten historical bubbles and assessed them on factors including spark, cumulative capex, capex durability and investor group. By our admittedly rough-and-ready reckoning, the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.
</blockquote>
They <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up">note that</a>:<br>
<blockquote>
For now, the splurge looks fairly modest by historical standards. According to our most generous estimate, American AI firms have invested 3-4% of current American GDP over the past four years. British railway investment during the 1840s was around 15-20% of GDP. But if forecasts for data-centre construction are correct, that will change. What is more, an unusually large share of capital investment is being devoted to assets that depreciate quickly. Nvidia’s cutting-edge chips will look clunky in a few years’ time. We estimate that the average American tech firm’s assets have a shelf-life of just nine years, compared with 15 for telecoms assets in the 1990s.
</blockquote>
I think they are over-estimating the shelf-life. Like Bitcoin mining, power is a major part of AI opex. Thus the incentive to (a) retire older, less power-efficient hardware, and (b) adopt the latest data-center power technology, is overwhelming. Note that Nvidia is moving to a one-year product cadence, and even when they were on a two-year cadence Jensen claimed it wasn't worth running chips from the previous cycle. Note also that the current generation of AI systems is incompatible with the power infrastructure of older data centers, and this may well happen again in a future product generation.  For example, Caiwei Chen reports in <a href="https://www.technologyreview.com/2025/03/26/1113802/china-ai-data-centers-unused/"><i>China built hundreds of AI data centers to catch the AI boom. Now many stand unused</i></a>:<br>
<blockquote>
The local Chinese outlets <i>Jiazi Guangnian</i> and <i>36Kr</i> report that up to 80% of China’s newly built computing resources remain unused. 
</blockquote>
Rogé Karma makes the same point as <i>The Economist</i> in <a href="https://www.theatlantic.com/economy/archive/2025/09/ai-bubble-us-economy/684128/"><i>Just How Bad Would an AI Bubble Be?</i></a>:<br>
<blockquote>
An AI-bubble crash could be different. AI-related investments have already <a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/">surpassed</a> the level that telecom hit at the peak of the dot-com boom as a share of the economy. In the first half of this year, business spending on AI added more to GDP growth than all consumer spending <i>combined</i>. Many experts believe that a major reason the U.S. economy has been able to weather tariffs and mass deportations without a recession is because all of this AI spending is acting, in the <a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/">words</a> of one economist, as a “massive private sector stimulus program.” An AI crash could lead broadly to less spending, fewer jobs, and slower growth, potentially dragging the economy into a recession.
</blockquote>
</li>
<li id="Footnote24">
In 2021 Nicholas Weaver estimated that the Ethereum computer was 5000 times slower than a <a href="https://www.usenix.org/publications/loginonline/web3-fraud">Raspberry Pi 4</a>. Since then the gas limit has been raised making his current estimate only 1000 times slower.
</li>
<li id="Footnote25">
Prof. Hilary Allen writes in <a href="https://fintechdystopia.com/chapters/chapter4.html"><i>Fintech Dystopia</i></a> that:<br>
<blockquote>
if people do start dumping blockchain-based assets in fire sales, everyone will know immediately because the blockchain is publicly visible. This level of transparency will only add to the panic (at least, that’s what happened during the <a href="https://corpgov.law.harvard.edu/2023/05/22/anatomy-of-a-run-the-terra-luna-crash/">run on the Terra stablecoin</a> in 2022).<br>
...<br>
We also saw ... that assets on a blockchain can be pre-programmed to execute transactions without the intervention of any human being. In good times, this makes things more efficient – but the code will execute just as quickly in bad situations, even if everyone would be better off if it didn’t. 
</blockquote>
She <a href="https://fintechdystopia.com/chapters/chapter4.html">adds</a>:<br>
<blockquote>
When things are spiraling out of control like this, sometimes the best medicine is a pause. Lots of traditional financial markets close at the end of the day and on weekends, which provides a natural opportunity for a break (and if things are really bad, for emergency government intervention). But one of blockchain-based finance’s claims to greater efficiency is that operations continue 24/7. We may end up missing the pauses once they’re gone.
</blockquote>
In the 26<sup>th</sup> September <i>Grant's</i>, Joel Wallenberg notes that:<br>
<blockquote>
Lucrative though they may be, the problem with stablecoin deposits is that exposure to the crypto-trading ecosystem makes them inherently correlated to it and subject to runs in a new “crypto winter,” like that of 2022–23.  Indeed, since as much as 70% of gross stablecoin-transaction volume derives from automated arbitrage bots and high-speed trading algorithms, runs may be rapid and without human over-sight. What may be worse, the insured banks that could feed a stablecoin boom are the very ones that are likely to require taxpayer support if liquidity dries up, and Trump-style regulation is likely to be light.
</blockquote>
So the loophole in the GENIUS act for banks is likely to cause contagion from cryptocurrencies via stablecoins to the US banking system.
</li>
</ol>
<h3>Acknowledgments</h3><p>
This talk benefited greatly from critiques of drafts by Hilary Allen, David Gerard, Jon Reiter, Joel Wallenberg, and Nicholas Weaver.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CDC File Transfer (200 pts)]]></title>
            <link>https://github.com/google/cdc-file-transfer</link>
            <guid>45433768</guid>
            <pubDate>Wed, 01 Oct 2025 02:38:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/cdc-file-transfer">https://github.com/google/cdc-file-transfer</a>, See on <a href="https://news.ycombinator.com/item?id=45433768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CDC File Transfer</h2><a id="user-content-cdc-file-transfer" aria-label="Permalink: CDC File Transfer" href="#cdc-file-transfer"></a></p>
<p dir="auto">Born from the ashes of Stadia, this repository contains tools for syncing and
streaming files from Windows to Windows or Linux. The tools are based on Content
Defined Chunking (CDC), in particular
<a href="https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia" rel="nofollow">FastCDC</a>,
to split up files into chunks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">At Stadia, game developers had access to Linux cloud instances to run games.
Most developers wrote their games on Windows, though. Therefore, they needed a
way to make them available on the remote Linux instance.</p>
<p dir="auto">As developers had SSH access to those instances, they could use <code>scp</code> to copy
the game content. However, this was impractical, especially with the shift to
working from home during the pandemic with sub-par internet connections. <code>scp</code>
always copies full files, there is no "delta mode" to copy only the things that
changed, it is slow for many small files, and there is no fast compression.</p>
<p dir="auto">To help this situation, we developed two tools, <code>cdc_rsync</code> and <code>cdc_stream</code>,
which enable developers to quickly iterate on their games without repeatedly
incurring the cost of transmitting dozens of GBs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CDC RSync</h2><a id="user-content-cdc-rsync" aria-label="Permalink: CDC RSync" href="#cdc-rsync"></a></p>
<p dir="auto"><code>cdc_rsync</code> is a tool to sync files from a Windows machine to a Linux device,
similar to the standard Linux <a href="https://linux.die.net/man/1/rsync" rel="nofollow">rsync</a>. It is
basically a copy tool, but optimized for the case where there is already an old
version of the files available in the target directory.</p>
<ul dir="auto">
<li>It quickly skips files if timestamp and file size match.</li>
<li>It uses fast compression for all data transfer.</li>
<li>If a file changed, it determines which parts changed and only transfers the
differences.</li>
</ul>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_recursive_upload_demo.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_recursive_upload_demo.gif" alt="cdc_rsync demo" width="688" data-animated-image=""></a>
</p>
<p dir="auto">The remote diffing algorithm is based on CDC. In our tests, it is up to 30x
faster than the one used in <code>rsync</code> (1500 MB/s vs 50 MB/s).</p>
<p dir="auto">The following chart shows a comparison of <code>cdc_rsync</code> and Linux <code>rsync</code> running
under Cygwin on Windows. The test data consists of 58 development builds
of some game provided to us for evaluation purposes. The builds are 40-45 GB
large. For this experiment, we uploaded the first build, then synced the second
build with each of the two tools and measured the time. For example, syncing
from build 1 to build 2 took 210 seconds with the Cygwin <code>rsync</code>, but only 75
seconds with <code>cdc_rsync</code>. The three outliers are probably feature drops from
another development branch, where the delta was much higher. Overall,
<code>cdc_rsync</code> syncs files about <strong>3 times faster</strong> than Cygwin <code>rsync</code>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_vs_cygwin_rsync.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_vs_cygwin_rsync.png" alt="Comparison of cdc_rsync and Linux rsync running in Cygwin" width="753"></a>
</p>
<p dir="auto">We also ran the experiment with the native Linux <code>rsync</code>, i.e syncing Linux to
Linux, to rule out issues with Cygwin. Linux <code>rsync</code> performed on average 35%
worse than Cygwin <code>rsync</code>, which can be attributed to CPU differences. We did
not include it in the figure because of this, but you can find it
<a href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_vs_cygwin_rsync_vs_linux_rsync.png">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work and why is it faster?</h3><a id="user-content-how-does-it-work-and-why-is-it-faster" aria-label="Permalink: How does it work and why is it faster?" href="#how-does-it-work-and-why-is-it-faster"></a></p>
<p dir="auto">The standard Linux <code>rsync</code> splits a file into fixed-size chunks of typically
several KB.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/fixed_size_chunks.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/fixed_size_chunks.png" alt="Linux rsync uses fixed size chunks" width="258"></a>
</p>
<p dir="auto">If the file is modified in the middle, e.g. by inserting <code>xxxx</code> after <code>567</code>,
this usually means that <span>the modified chunks as well as
all subsequent chunks</span> change.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/fixed_size_chunks_inserted.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/fixed_size_chunks_inserted.png" alt="Fixed size chunks after inserting data" width="301"></a>
</p>
<p dir="auto">The standard <code>rsync</code> algorithm hashes the chunks of the remote "old" file
and sends the hashes to the local device. The local device then figures out
which parts of the "new" file matches known chunks.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/linux_rsync_animation.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/linux_rsync_animation.gif" alt="Syncing a file with the standard Linux rsync" width="855" data-animated-image=""></a>
  <br>
  Standard rsync algorithm
</p>
<p dir="auto">This is a simplification. The actual algorithm is more complicated and uses
two hashes, a weak rolling hash and a strong hash, see
<a href="https://rsync.samba.org/tech_report/" rel="nofollow">here</a> for a great overview. What makes
<code>rsync</code> relatively slow is the "no match" situation where the rolling hash does
not match any remote hash, and the algorithm has to roll the hash forward and
perform a hash map lookup for each byte. <code>rsync</code> goes to
<a href="https://github.com/librsync/librsync/blob/master/src/hashtable.h">great lengths</a>
optimizing lookups.</p>
<p dir="auto"><code>cdc_rsync</code> does not use fixed-size chunks, but instead variable-size,
content-defined chunks. That means, chunk boundaries are determined by the
<em>local content</em> of the file, in practice a 64 byte sliding window. For more
details, see
<a href="https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia" rel="nofollow">the FastCDC paper</a>
or take a look at <a href="https://github.com/google/cdc-file-transfer/blob/main/fastcdc/fastcdc.h">our implementation</a>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/variable_size_chunks.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/variable_size_chunks.png" alt="cdc_rsync uses variable, content-defined size chunks" width="260"></a>
</p>
<p dir="auto">If the file is modified in the middle, only <span>the modified
chunks</span>, but not <span>subsequent chunks</span>
change (unless they are less than 64 bytes away from the modifications).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/variable_size_chunks_inserted.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/variable_size_chunks_inserted.png" alt="Content-defined chunks after inserting data" width="314"></a>
</p>
<p dir="auto">Computing the chunk boundaries is cheap and involves only a left-shift, a memory
lookup, an <code>add</code> and an <code>and</code> operation for each input byte. This is cheaper
than the hash map lookup for the standard <code>rsync</code> algorithm.</p>
<p dir="auto">Because of this, the <code>cdc_rsync</code> algorithm is faster than the standard
<code>rsync</code>. It is also simpler. Since chunk boundaries move along with insertions
or deletions, the task to match local and remote hashes is a trivial set
difference operation. It does not involve a per-byte hash map lookup.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_animation.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_animation.gif" alt="Syncing a file with cdc_rsync" width="857" data-animated-image=""></a>
  <br>
  cdc_rsync algorithm
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CDC Stream</h2><a id="user-content-cdc-stream" aria-label="Permalink: CDC Stream" href="#cdc-stream"></a></p>
<p dir="auto"><code>cdc_stream</code> is a tool to stream files and directories from a Windows machine to
a Linux device. Conceptually, it is similar to
<a href="https://github.com/libfuse/sshfs">sshfs</a>, but it is optimized for read speed.</p>
<ul dir="auto">
<li>It caches streamed data on the Linux device.</li>
<li>If a file is re-read on Linux after it changed on Windows, only the
differences are streamed again. The rest is read from the cache.</li>
<li>Stat operations are very fast since the directory metadata (filenames,
permissions etc.) is provided in a streaming-friendly way.</li>
</ul>
<p dir="auto">To efficiently determine which parts of a file changed, the tool uses the same
CDC-based diffing algorithm as <code>cdc_rsync</code>. Changes to Windows files are almost
immediately reflected on Linux, with a delay of roughly (0.5s + 0.7s x total
size of changed files in GB).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_stream_demo.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_stream_demo.gif" alt="cdc_stream demo" width="688" data-animated-image=""></a>
</p>
<p dir="auto">The tool does not support writing files back from Linux to Windows; the Linux
directory is readonly.</p>
<p dir="auto">The following chart compares times from starting a game to reaching the menu.
In one case, the game is streamed via <code>sshfs</code>, in the other case we use
<code>cdc_stream</code>. Overall, we see a <strong>2x to 5x speedup</strong>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_stream_vs_sshfs.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_stream_vs_sshfs.png" alt="Comparison of cdc_stream and sshfs" width="752"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Platforms</h2><a id="user-content-supported-platforms" aria-label="Permalink: Supported Platforms" href="#supported-platforms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><code>cdc_rsync</code></th>
<th>From</th>
<th>To</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows x86_64</td>
<td>✓</td>
<td>✓ <sup>1</sup></td>
</tr>
<tr>
<td>Ubuntu 22.04 x86_64</td>
<td>✗ <sup>2</sup></td>
<td>✓</td>
</tr>
<tr>
<td>Ubuntu 22.04 aarch64</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 x86_64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 aarch64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><code>cdc_stream</code></th>
<th>From</th>
<th>To</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows x86_64</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr>
<td>Ubuntu 22.04 x86_64</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td>Ubuntu 22.04 aarch64</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 x86_64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 aarch64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<span>
<p dir="auto"><sup>1</sup> Only local syncs, e.g. <code>cdc_rsync C:\src\* C:\dst</code>. Support for
remote syncs is being added, see
<a href="https://github.com/google/cdc-file-transfer/issues/61" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/61/hovercard">#61</a>.<br>
<sup>2</sup> See <a href="https://github.com/google/cdc-file-transfer/issues/56" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/56/hovercard">#56</a>.<br>
<sup>3</sup> See <a href="https://github.com/google/cdc-file-transfer/issues/62" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/62/hovercard">#62</a>.</p>
</span>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Download the precompiled binaries from the
<a href="https://github.com/google/cdc-file-transfer/releases">latest release</a> to a
Windows device and unzip them. The Linux binaries are automatically deployed
to <code>~/.cache/cdc-file-transfer</code> by the Windows tools. There is no need to manually
deploy them. We currently provide Linux binaries compiled on
<a href="https://github.com/actions/runner-images">Github's latest Ubuntu</a> version.
If the binaries work for you, you can skip the following two sections.</p>
<p dir="auto">Alternatively, the project can be built from source. Some binaries have to be
built on Windows, some on Linux.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites for Building</h2><a id="user-content-prerequisites-for-building" aria-label="Permalink: Prerequisites for Building" href="#prerequisites-for-building"></a></p>
<p dir="auto">To build the tools from source, the following steps have to be executed on
<strong>both Windows and Linux</strong>.</p>
<ul dir="auto">
<li>Download and install Bazel from <a href="https://bazel.build/install" rel="nofollow">here</a>. See
<a href="https://github.com/google/cdc-file-transfer/actions">workflow logs</a> for the
currently used version.</li>
<li>Clone the repository.
<div data-snippet-clipboard-copy-content="git clone https://github.com/google/cdc-file-transfer"><pre><code>git clone https://github.com/google/cdc-file-transfer
</code></pre></div>
</li>
<li>Initialize submodules.
<div data-snippet-clipboard-copy-content="cd cdc-file-transfer
git submodule update --init --recursive"><pre><code>cd cdc-file-transfer
git submodule update --init --recursive
</code></pre></div>
</li>
</ul>
<p dir="auto">Finally, install an SSH client on the Windows machine if not present.
The file transfer tools require <code>ssh.exe</code> and <code>sftp.exe</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">The two tools CDC RSync and CDC Stream can be built and used independently.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC RSync</h3><a id="user-content-cdc-rsync-1" aria-label="Permalink: CDC RSync" href="#cdc-rsync-1"></a></p>
<ul dir="auto">
<li>On a Linux device, build the Linux components
<div data-snippet-clipboard-copy-content="bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server"><pre><code>bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server
</code></pre></div>
</li>
<li>On a Windows device, build the Windows components
<div data-snippet-clipboard-copy-content="bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync"><pre><code>bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync
</code></pre></div>
</li>
<li>Copy the Linux build output file <code>cdc_rsync_server</code> from
<code>bazel-bin/cdc_rsync_server</code> to <code>bazel-bin\cdc_rsync</code> on the Windows machine.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC Stream</h3><a id="user-content-cdc-stream-1" aria-label="Permalink: CDC Stream" href="#cdc-stream-1"></a></p>
<ul dir="auto">
<li>On a Linux device, build the Linux components
<div data-snippet-clipboard-copy-content="bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs"><pre><code>bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs
</code></pre></div>
</li>
<li>On a Windows device, build the Windows components
<div data-snippet-clipboard-copy-content="bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream"><pre><code>bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream
</code></pre></div>
</li>
<li>Copy the Linux build output files <code>cdc_fuse_fs</code> and <code>libfuse.so</code> from
<code>bazel-bin/cdc_fuse_fs</code> to <code>bazel-bin\cdc_stream</code> on the Windows machine.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">The tools require a setup where you can use SSH and SFTP from the Windows
machine to the Linux device without entering a password, e.g. by using key-based
authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuring SSH and SFTP</h3><a id="user-content-configuring-ssh-and-sftp" aria-label="Permalink: Configuring SSH and SFTP" href="#configuring-ssh-and-sftp"></a></p>
<p dir="auto">By default, the tools search <code>ssh.exe</code> and <code>sftp.exe</code> from the path environment
variable. If you can run the following commands in a Windows cmd without
entering your password, you are all set:</p>
<div data-snippet-clipboard-copy-content="ssh user@linux.device.com
sftp user@linux.device.com"><pre><code>ssh user@linux.device.com
sftp user@linux.device.com
</code></pre></div>
<p dir="auto">Here, <code>user</code> is the Linux user and <code>linux.device.com</code> is the Linux host to
SSH into or copy the file to.</p>
<p dir="auto">If additional arguments are required, it is recommended to provide an SSH config
file. By default, both <code>ssh.exe</code> and <code>sftp.exe</code> use the file at
<code>%USERPROFILE%\.ssh\config</code> on Windows, if it exists. A possible config file
that sets a username, a port, an identity file and a known host file could look
as follows:</p>
<div data-snippet-clipboard-copy-content="Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts"><pre><code>Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts
</code></pre></div>
<p dir="auto">If <code>ssh.exe</code> or <code>sftp.exe</code> cannot be found, you can specify the full paths via
the command line arguments <code>--ssh-command</code> and <code>--sftp-command</code> for <code>cdc_rsync</code>
and <code>cdc_stream start</code> (see below), or set the environment variables
<code>CDC_SSH_COMMAND</code> and <code>CDC_SFTP_COMMAND</code>, e.g.</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=&quot;C:\path with space\to\ssh.exe&quot;
set CDC_SFTP_COMMAND=&quot;C:\path with space\to\sftp.exe&quot;"><pre><code>set CDC_SSH_COMMAND="C:\path with space\to\ssh.exe"
set CDC_SFTP_COMMAND="C:\path with space\to\sftp.exe"
</code></pre></div>
<p dir="auto">Note that you can also specify SSH configuration via the environment variables
instead of using a config file:</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts"><pre><code>set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
</code></pre></div>
<p dir="auto">Note the small <code>-p</code> for <code>ssh.exe</code> and the capital <code>-P</code> for <code>sftp.exe</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Google Specific</h4><a id="user-content-google-specific" aria-label="Permalink: Google Specific" href="#google-specific"></a></p>
<p dir="auto">For Google internal usage, set the following environment variables to enable SSH
authentication using a Google security key:</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe"><pre><code>set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe
</code></pre></div>
<p dir="auto">Note that you will have to touch the security key multiple times during the
first run. Subsequent runs only require a single touch.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC RSync</h3><a id="user-content-cdc-rsync-2" aria-label="Permalink: CDC RSync" href="#cdc-rsync-2"></a></p>
<p dir="auto"><code>cdc_rsync</code> is used similar to <code>scp</code> or the Linux <code>rsync</code> command. To sync a
single Windows file <code>C:\path\to\file.txt</code> to the home directory <code>~</code> on the Linux
device <code>linux.device.com</code>, run</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\file.txt user@linux.device.com:~"><pre><code>cdc_rsync C:\path\to\file.txt user@linux.device.com:~
</code></pre></div>
<p dir="auto"><code>cdc_rsync</code> understands the usual Windows wildcards <code>*</code> and <code>?</code>.</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\*.txt user@linux.device.com:~"><pre><code>cdc_rsync C:\path\to\*.txt user@linux.device.com:~
</code></pre></div>
<p dir="auto">To sync the contents of the Windows directory <code>C:\path\to\assets</code> recursively to
<code>~/assets</code> on the Linux device, run</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r"><pre><code>cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r
</code></pre></div>
<p dir="auto">To get per file progress, add <code>-v</code>:</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr"><pre><code>cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr
</code></pre></div>
<p dir="auto">The tool also supports local syncs:</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr"><pre><code>cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC Stream</h3><a id="user-content-cdc-stream-2" aria-label="Permalink: CDC Stream" href="#cdc-stream-2"></a></p>
<p dir="auto">To stream the Windows directory <code>C:\path\to\assets</code> to <code>~/assets</code> on the Linux
device, run</p>
<div data-snippet-clipboard-copy-content="cdc_stream start C:\path\to\assets user@linux.device.com:~/assets"><pre><code>cdc_stream start C:\path\to\assets user@linux.device.com:~/assets
</code></pre></div>
<p dir="auto">This makes all files and directories in <code>C:\path\to\assets</code> available on
<code>~/assets</code> immediately, as if it were a local copy. However, data is streamed
from Windows to Linux as files are accessed.</p>
<p dir="auto">To stop the streaming session, enter</p>
<div data-snippet-clipboard-copy-content="cdc_stream stop user@linux.device.com:~/assets"><pre><code>cdc_stream stop user@linux.device.com:~/assets
</code></pre></div>
<p dir="auto">The command also accepts wildcards. For instance,</p>

<p dir="auto">stops all existing streaming sessions for the given user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">On first run, <code>cdc_stream</code> starts a background service, which does all the work.
The <code>cdc_stream start</code> and <code>cdc_stream stop</code> commands are just RPC clients that
talk to the service.</p>
<p dir="auto">The service logs to <code>%APPDATA%\cdc-file-transfer\logs</code> by default. The logs are
useful to investigate issues with asset streaming. To pass custom arguments, or
to debug the service, create a JSON config file at
<code>%APPDATA%\cdc-file-transfer\cdc_stream.json</code> with command line flags.
For instance,</p>

<p dir="auto">instructs the service to log debug messages. Try <code>cdc_stream start-service -h</code>
for a list of available flags. Alternatively, run the service manually with</p>

<p dir="auto">and pass the flags as command line arguments. When you run the service manually,
the flag <code>--log-to-stdout</code> is particularly useful as it logs to the console
instead of to the file.</p>
<p dir="auto"><code>cdc_rsync</code> always logs to the console. To increase log verbosity, pass <code>-vvv</code>
for debug logs or <code>-vvvv</code> for verbose logs.</p>
<p dir="auto">For both sync and stream, the debug logs contain all SSH and SFTP commands that
are attempted to run, which is very useful for troubleshooting. If a command
fails unexpectedly, copy it and run it in isolation. Pass <code>-vv</code> or <code>-vvv</code> for
additional debug output.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Multi-Armed Bandits (2019) (106 pts)]]></title>
            <link>https://arxiv.org/abs/1904.07272</link>
            <guid>45431271</guid>
            <pubDate>Tue, 30 Sep 2025 21:08:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/1904.07272">https://arxiv.org/abs/1904.07272</a>, See on <a href="https://news.ycombinator.com/item?id=45431271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/1904.07272">View PDF</a>
    <a href="https://arxiv.org/html/1904.07272v8">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.
<br>The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.
<br>The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Aleksandrs Slivkins [<a href="https://arxiv.org/show-email/16469ce6/1904.07272" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/1904.07272v1" rel="nofollow">[v1]</a></strong>
        Mon, 15 Apr 2019 18:17:01 UTC (510 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v2" rel="nofollow">[v2]</a></strong>
        Mon, 29 Apr 2019 20:45:01 UTC (510 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v3" rel="nofollow">[v3]</a></strong>
        Tue, 25 Jun 2019 14:39:03 UTC (536 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v4" rel="nofollow">[v4]</a></strong>
        Sun, 15 Sep 2019 02:06:22 UTC (557 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v5" rel="nofollow">[v5]</a></strong>
        Mon, 30 Sep 2019 00:15:42 UTC (543 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v6" rel="nofollow">[v6]</a></strong>
        Sat, 26 Jun 2021 20:15:32 UTC (639 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v7" rel="nofollow">[v7]</a></strong>
        Sat, 8 Jan 2022 20:05:40 UTC (627 KB)<br>
    <strong>[v8]</strong>
        Wed, 3 Apr 2024 21:32:42 UTC (629 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mind the encryptionroot: How to save your data when ZFS loses its mind (108 pts)]]></title>
            <link>https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</link>
            <guid>45431167</guid>
            <pubDate>Tue, 30 Sep 2025 20:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/">https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</a>, See on <a href="https://news.ycombinator.com/item?id=45431167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
While <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a> has a well-earned reputation for data integrity and reliability, OpenZFS native encryption has some <em>incredibly</em> <a href="https://github.com/openzfs/openzfs-docs/issues/494">sharp edges</a> that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious data. This is the postmortem of that terrible, horrible, no good, very bad week…</p>
<hr>
<div>
  <h2>
    Table of Contents
  </h2>
  <nav id="TableOfContents">
<ul>
<li><a href="#headline-1">Part 1: An unfortunate series of events</a>
<ul>
<li><a href="#headline-2">The status quo</a>
</li>
<li><a href="#headline-3">Encrypting the old pool</a>
</li>
<li><a href="#headline-4">Decommissioning the old pool</a>
</li>
<li><a href="#headline-5">The realization</a>
</li>
</ul>
</li>
<li><a href="#headline-6">Part 2: Debugging the issue</a>
<ul>
<li><a href="#headline-7">Learning how ZFS actually works</a>
</li>
<li><a href="#headline-8">Learning how ZFS native encryption actually works</a>
</li>
<li><a href="#headline-9">Forming a hypothesis</a>
</li>
<li><a href="#headline-10">Creating a test environment</a>
</li>
<li><a href="#headline-11">Reproducing the issue</a>
</li>
</ul>
</li>
<li><a href="#headline-12">Part 3: Recovering our data</a>
<ul>
<li><a href="#headline-13">Theoretically easy to fix</a>
</li>
<li><a href="#headline-14">Not so easy in practice</a>
</li>
<li><a href="#headline-15">Idea for a hack</a>
</li>
<li><a href="#headline-16">A brief detour into pool histories</a>
</li>
<li><a href="#headline-17">Hacking ZFS to manually create a bookmark</a>
</li>
<li><a href="#headline-18">The final obstacle</a>
</li>
<li><a href="#headline-19">The moment of truth</a>
</li>
</ul>
</li>
<li><a href="#headline-20">Conclusion</a>
</li>
</ul>
</nav>
</div>

<hr>
<p>
<strong>Note:</strong> The issue covered in this postmortem only applies to OpenZFS native encryption. Oracle ZFS has its own encryption scheme which is different and, as far as I can tell, should not be vulnerable to this particular failure mode, though I have not personally tested it. Thank you to <a href="https://old.reddit.com/r/zfs/comments/1ntwrjx/mind_the_encryptionroot_how_to_save_your_data/ngyp0wx/">u/HobartTasmania</a> for pointing this out!</p>
<div id="outline-container-headline-1">
<h2 id="headline-1">
Part 1: An unfortunate series of events
</h2>
<div id="outline-text-headline-1">
<div id="outline-container-headline-2">
<h3 id="headline-2">
The status quo
</h3>
<div id="outline-text-headline-2">
<p>
In the beginning, there were two ZFS pools: <code>old</code> and <code>new</code> (names changed for clarity). Each pool was hosted on an instance of TrueNAS CORE 13.0-U5.1 located at two different sites about an hour's drive apart with poor Internet connectivity between them. For this reason, a third pool <a href="https://en.wikipedia.org/wiki/Sneakernet"><code>sneakernet</code></a> was periodically moved between the two sites and used to exchange snapshots of <code>old</code> and <code>new</code> datasets for backup purposes. ZFS dataset <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-snapshot.8.html">snapshots</a> would be indirectly relayed from <code>old</code> to <code>new</code> (and vice versa) using <code>sneakernet</code> as an intermediate ZFS <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-send.8.html">send</a>/<a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-recv.8.html">recv</a> source/destination (e.g. <code>old/foo@2023-06-01</code> -&gt; <code>sneakernet/old/foo@2023-06-01</code> -&gt; <code>new/old/foo@2023-06-01</code>).</p>
<p>
The <code>new</code> pool was natively encrypted from the very beginning. When ZFS snapshots were sent from <code>new</code> to <code>sneakernet/new</code> to <code>old/new</code>, they were sent <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-send.8.html#w">raw</a>, meaning that blocks were copied unmodified in their encrypted form. To decrypt and mount them on <code>sneakernet</code> or <code>old</code>, you would need to first load <code>new</code>'s <a href="https://openzfs.github.io/openzfs-docs/man/master/7/zfsprops.7.html#keyformat">hex</a> encryption key, which is stored in TrueNAS's SQLite database.</p>
<p>
The <code>old</code> pool, on the other hand, was created before the advent of native encryption and was unencrypted for the first part of its life. Because it's desirable to encrypt data at rest, an encrypted dataset <code>sneakernet/old</code> was created for <code>old</code> using a <a href="https://openzfs.github.io/openzfs-docs/man/master/7/zfsprops.7.html#keyformat">passphrase</a> encryption key when <code>sneakernet</code> was set up. Unencrypted snapshots were sent non-raw from <code>old</code> to <code>sneakernet/old</code>, where they were encrypted, and then sent raw from <code>sneakernet/old</code> to <code>new/old</code>. To decrypt and mount them on <code>sneakernet</code> or <code>new</code>, you would need to first load <code>sneakernet</code>'s passphrase encryption key.</p>
<p>
This was all tested thoroughly and snapshots were proven to be readable at each point on every pool.</p>
</div>
</div>
<div id="outline-container-headline-3">
<h3 id="headline-3">
Encrypting the old pool
</h3>
<div id="outline-text-headline-3">
<p>
Now that we had encrypted snapshots of <code>old</code> on <code>sneakernet/old</code>, we wanted to encrypt <code>old</code> itself. To do this, I simply took <code>old</code> offline during a maintenance window to prevent new writes, took snapshots of all datasets, sent them to <code>sneakernet/old</code>, and then sent the raw encrypted snapshots from <code>sneakernet/old</code> back to <code>old/encrypted</code>. Once I verified each dataset had been encrypted successfully, I destroyed the unencrypted dataset, updated the mount point of the encrypted dataset to that of the late unencrypted dataset, and then moved on to the next dataset. After all datasets were migrated, I used <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-change-key.8.html"><code>zfs change-key -i</code></a> to make all child datasets inherit from the new <code>old/encrypted</code> encryption root, and then changed the key of the encryption root from a passphrase to a hex key, since TrueNAS only supported automatically unlocking datasets with hex encryption keys. Finally, I issued a <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zpool-initialize.8.html"><code>zpool initialize</code></a> to overwrite all the unencrypted blocks which were now in unallocated space.</p>
<p>
<strong>Spoiler Alert:</strong> It may not be immediately obvious why, but changing the encryption key on <code>old/encryption</code> silently broke backups of <code>old</code> datasets. Snapshots would still send and recv successfully, but were no longer decryptable or mountable. Since the encryption key is not normally loaded, and we only load it when periodically testing the backups, we would not realize until it was too late.</p>
<p>
<strong>Lesson: Test backups continuously so you get immediate feedback when they break.</strong></p>
</div>
</div>
<div id="outline-container-headline-4">
<h3 id="headline-4">
Decommissioning the old pool
</h3>
<div id="outline-text-headline-4">
<p>
Later, the <code>old</code> pool was moved to the same site as the <code>new</code> pool, so we wanted to fully decommission <code>old</code> and migrate all its datasets to <code>new</code>. I began going about this in a similar way. I took <code>old</code> offline to prevent new writes, sent snapshots to <code>sneakernet/old</code>, and then to <code>new/old</code>. It was at this point that I made a very unfortunate mistake: I accidentally destroyed one dataset <code>old/encrypted/foo</code> <em>before</em> verifying the files were readable on <code>new/old/foo</code>, and I would soon realize that they were not.</p>
<p>
<strong>Lesson: Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.</strong></p>
</div>
</div>
<div id="outline-container-headline-5">
<h3 id="headline-5">
The realization
</h3>
<div id="outline-text-headline-5">
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span>sam@newnas ~<span>]</span>$ <span>DATASET</span><span>=</span>foo<span>;</span> <span>[[</span> <span>$(</span>ssh sam@oldnas zfs list -H -o guid old/encrypted/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>=</span> <span>$(</span>zfs list -H -o guid sneakernet/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>"GUIDs match"</span> <span>||</span> <span>echo</span> <span>"GUIDs DO NOT MATCH"</span>
</span></span><span><span>GUIDs match
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ <span>DATASET</span><span>=</span>foo<span>;</span> <span>[[</span> <span>$(</span>zfs list -H -o guid sneakernet/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>=</span> <span>$(</span>zfs list -H -o guid new/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>"GUIDs match"</span> <span>||</span> <span>echo</span> <span>"GUIDs DO NOT MATCH"</span>
</span></span><span><span>GUIDs match
</span></span><span><span>
</span></span><span><span><span>[</span>sam@oldnas ~<span>]</span>$ sudo zfs destroy -r old/encrypted/foo
</span></span><span><span>
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ ls /mnt/new/old/foo
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ ls -a /mnt/new/old/foo
</span></span><span><span>. ..
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ zfs list -o name,mounted new/old/foo
</span></span><span><span>NAME         MOUNTED
</span></span><span><span>new/old/foo  no
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs mount new/old/foo
</span></span><span><span>cannot mount <span>'new/old/foo'</span>: Permission denied</span></span></code></pre></div>
<p>
What do you mean, permission denied? I am root!</p>
<p>
Crap, I already destroyed <code>old/encrypted/foo</code>. This is not good, but I can still restore it from the remaining copy on <code>sneakernet/old/foo</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs load-key sneakernet/old
</span></span><span><span>Enter passphrase <span>for</span> <span>'sneakernet/old'</span>:
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs mount sneakernet/old/foo
</span></span><span><span>cannot mount <span>'sneakernet/old/foo'</span>: Permission denied</span></span></code></pre></div>
<p>
Oh no, <code>sneakernet/old</code> is broken too. This is <em>very not good</em>!</p>
<p>
In an act of desperation, I tried rebooting the machine, but it didn't change a thing.</p>
<p>
It is at this point that I realized:</p>
<ol>
<li>Something has gone terribly wrong to prevent datasets on both <code>sneakernet/old</code> and <code>new/old</code> from mounting.</li>
<li>Whatever it is, it's not likely going to be easy to diagnose or fix.</li>
<li>There's a very real possibility the data might be gone forever.</li>
</ol>
<p>I found myself in a hole and I wanted to stop digging. Fortunately, uptime was no longer critical for the <code>old</code> datasets after the relocation, so I could afford to step away from the keyboard, collect my thoughts, and avoid making the situation any worse that it already was.</p>
<hr>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6">
<h2 id="headline-6">
Part 2: Debugging the issue
</h2>
<div id="outline-text-headline-6">
<p>
Once the worst of the overwhelming, visceral feelings that come with the realization that you may have just caused permanent data loss had subsided, I started to work the incident and try to figure out why the backups aren't mounting.</p>
<p>
As a precaution, I first <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zpool-export.8.html">exported</a> the <code>old</code> pool and took a forensic image of every disk in the pool. ZFS is a copy-on-write filesystem, so even though the dataset had been destroyed, most of the data was probably still on disk, just completely inaccessible with the normal ZFS tooling. In the worst case scenario, I may have had to try to forensically reconstruct the dataset from what was left on disk, and I didn't want to risk causing any more damage than I already had. Fortunately, I never had to use the disk images, but they still served as a valuable safety net while debugging and repairing.</p>
<p>
Next, I realized that if we are to have any chance of debugging and fixing this issue, I need to learn how ZFS actually works.</p>
<div id="outline-container-headline-7">
<h3 id="headline-7">
Learning how ZFS actually works
</h3>
<div id="outline-text-headline-7">
<p>
I unfortunately did not keep track of every resource I consumed, but in addition to reading the <a href="https://github.com/openzfs/zfs">source</a> and <a href="https://openzfs.github.io/openzfs-docs/">docs</a>, I found these talks by Jeff Bonwick, Bill Moore, and Matt Ahrens (the original creators of ZFS) to be particularly helpful in understanding the design and implementation of ZFS:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=NRoUC9P1PmA">ZFS: The Last Word in File Systems Part 1</a></li>
<li><a href="https://www.youtube.com/watch?v=TwCXVp_u86o">ZFS: The Last Word in File Systems Part 2</a></li>
<li><a href="https://www.youtube.com/watch?v=ybjdAEUfXzw">ZFS: The Last Word in File Systems Part 3</a></li>
<li><a href="https://www.youtube.com/watch?v=NXg86uBDSqI">How ZFS Snapshots Really Work</a></li>
</ul>
<p>I <em>highly</em> recommend watching them all despite their age and somewhat poor recording quality, but will summarize the relevant information for those who don't have 3 hours to spare.</p>
<p>
ZFS is a <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage">copy-on-write</a> filesystem, which means that it does not overwrite blocks in place when a write is requested. Instead, the updated contents are written to a newly allocated block, and the old block is freed, which keeps the filesystem consistent if a write is interrupted. All blocks of both data and metadata are arranged in a <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> structure where each block pointer contains a checksum of the child block, which allows ZFS to detect both block corruption and misdirected/phantom reads/writes. This means that any write will cause the block's checksum to change, which will then cause the parent block's checksum to change (since the parent block includes the block pointer which includes checksum of the child block that changed), and so on, all the way up to the root of the tree which ZFS calls an uberblock.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_1.png" alt="zfs_1.png" title="zfs_1.png"></p>
<p>
Uberblocks are written atomically, and because of the Merkle tree structure, they always represent a consistent snapshot of the entire filesystem at a point in time. Writes are batched together into <a href="https://ahl.dtrace.org/2012/12/13/zfs-fundamentals-transaction-groups/">transaction groups</a> identified by a monotonically increasing counter, and each transaction group when synced to disk produces a new uberblock and associated filesystem tree. Taking a snapshot is then as simple as saving an uberblock and not freeing any of the blocks it points to.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_2.png" alt="zfs_2.png" title="zfs_2.png"></p>
<p>
In addition to the checksum, each block pointer also contains the transaction group id in which the child block was written, which is called the block's birth time or creation time. ZFS uses birth times to determine which blocks have been written before or after a snapshot. Any blocks with a birth time less than or equal to the snapshot's birth time, must have been written before the snapshot was taken, and conversely, any blocks with a birth time greater than the snapshot's birth time must have been written after the snapshot was taken.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_3.png" alt="zfs_3.png" title="zfs_3.png"></p>
<p>
One application of birth times is to generate incremental send streams between two snapshots. ZFS walks the tree but only needs to include blocks where the birth time is both greater than the first snapshot and less than or equal to the second snapshot. In fact, you don't even need to keep the data of the first snapshot around—you can create a <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-bookmark.8.html">bookmark</a> which saves the snapshot's transaction id (but none of the data blocks), delete the snapshot to free its data, and then use the bookmark as the source to generate the same incremental send stream.</p>
<p>
<strong>Spoiler Alert:</strong> <a href="https://en.wikipedia.org/wiki/Chekhov%27s_gun">Chekhov's</a> bookmark will become relevant later.</p>
</div>
</div>
<div id="outline-container-headline-8">
<h3 id="headline-8">
Learning how ZFS native encryption actually works
</h3>
<div id="outline-text-headline-8">
<p>
ZFS native encryption is a relatively new feature, which was first released in <a href="https://github.com/openzfs/zfs/releases/tag/zfs-0.8.0">OpenZFS 0.8.0</a> (2019) and subsequently made it into <a href="https://www.freebsd.org/releases/13.0R/relnotes/">FreeBSD 13.0</a> (2021) when OpenZFS was adopted.</p>
<p>
In addition to the <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-load-key.8.html#Encryption">docs</a>, I found this 2016 talk on <a href="https://www.youtube.com/watch?v=frnLiXclAMo">ZFS Native Encryption</a> by Tom Caputi (the original author of native encryption) to be helpful in understanding its design and implementation. Again, I will summarize the relevant information.</p>
<p>
ZFS native encryption works by encrypting dataset blocks with an symmetric authenticated encryption cipher suite (AES-256-GCM by default). To use native encryption, you must create a new dataset with <code>-o encryption=on</code> which generates a unique master key for the dataset. The dataset's master key is then used to derive block data encryption keys with a salted HKDF.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_4.png" alt="zfs_4.png" title="zfs_4.png"></p>
<p>
The master key can't be changed, so it is encrypted with a wrapping key which can be changed. The wrapping key is provided by the user with <code>zfs load-key</code> and can be changed with <code>zfs change-key</code> which re-encrypts the same master key with a new wrapping key.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_5.png" alt="zfs_5.png" title="zfs_5.png"></p>
<p>
The encrypted master keys are stored in each dataset since each dataset has its own master key, but the wrapping key parameters are stored on what is called the encryption root dataset. The encryption root may be the same encrypted dataset, or it may be a parent of the encrypted dataset. When a child encrypted dataset inherits from a parent encryption root, the encryption root's wrapping key is used to decrypt the child dataset's master key. This is how one key can be used to unlock a parent encryption root dataset and all child encrypted datasets that inherit from it at the same time instead of having to load a key for every single encrypted dataset.</p>
<p>
In our case, <code>new</code>, <code>sneakernet/new</code>, <code>sneakernet/old</code>, and <code>old/encrypted</code> are the encryption roots, and all child encrypted datasets inherit from them.</p>
</div>
</div>
<div id="outline-container-headline-9">
<h3 id="headline-9">
Forming a hypothesis
</h3>
<div id="outline-text-headline-9">
<p>
At this point, we now know enough to form a hypothesis as to what may have happened. Feel free to pause here and try to figure it out on your own.</p>
<p>
Recall that <code>sneakernet/old</code> was created using a passphrase encryption key, and <code>old/encrypted</code> was created by raw sending <code>sneakernet/old</code>, so it initially used the same passphrase derived wrapping encryption key. When the <code>old/encrypted</code> encryption key was changed from a passphrase to a hex key, ZFS must have changed the wrapping key parameters on the <code>old/encrypted</code> encryption root and re-encrypted all child encrypted dataset master keys with the new hex wrapping key. Crucially, a new snapshot of <code>old/encrypted</code> was never taken and sent to <code>sneakernet/old</code> because it ostensibly didn't contain any data and was just a container for the child datasets.</p>
<p>
<strong>Hypothesis:</strong> When subsequent snapshots were sent from <code>old</code> to <code>sneakernet</code>, the master keys of the child encrypted datasets were updated to be encrypted with the new hex wrapping key, but the <code>sneakernet/old</code> encryption root was never updated with the new hex wrapping key parameters because a new snapshot was never sent. Therefore, when we load the key for <code>sneakernet/old</code>, ZFS asks for the old passphrase, not a hex key, and when we try to mount <code>sneakernet/old/foo</code>, it tries and fails to decrypt its master key with the old passphrase wrapping key instead of the new hex wrapping key.</p>
<p>
If correct, this would explain the behavior we're seeing. To test this hypothesis, let's try to reproduce the issue in a test environment.</p>
</div>
</div>
<div id="outline-container-headline-10">
<h3 id="headline-10">
Creating a test environment
</h3>
<div id="outline-text-headline-10">
<p>
TrueNAS CORE 13.0-U5.1 is based on FreeBSD 13.1, despite the different minor version numbers, so we'll create a FreeBSD 13.1 VM to test in. Make sure to include the system source tree and install on UFS so that we can build OpenZFS and reload the ZFS kernel module without rebooting.</p>
<p>
TrueNAS CORE 13.0-U5.1 uses ZFS 2.1.11, so we'll want to build the same version from source for consistency. I started by reading the <a href="https://openzfs.github.io/openzfs-docs/Developer%20Resources/Building%20ZFS.html">Building ZFS</a> guide and following the steps documented there with some small modifications for FreeBSD since the page was clearly written with Linux in mind.</p>
<p>
First, install the dependencies we'll need.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo pkg install autoconf automake autotools git gmake python devel/py-sysctl sudo</span></span></code></pre></div>
<p>
Then, clone ZFS and check out tag zfs-2.1.11.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ git clone https://github.com/openzfs/zfs
</span></span><span><span>sam@zfshax:~ $ <span>cd</span> zfs
</span></span><span><span>sam@zfshax:~/zfs $ git checkout zfs-2.1.11
</span></span><span><span>sam@zfshax:~/zfs $ git show --summary
</span></span><span><span>commit e25f9131d679692704c11dc0c1df6d4585b70c35 <span>(</span>HEAD, tag: zfs-2.1.11<span>)</span>
</span></span><span><span>Author: Tony Hutter &lt;hutter2@llnl.gov&gt;
</span></span><span><span>Date:   Tue Apr <span>18</span> 11:44:34 <span>2023</span> -0700
</span></span><span><span>
</span></span><span><span>    Tag zfs-2.1.11
</span></span><span><span>
</span></span><span><span>    META file and changelog updated.
</span></span><span><span>
</span></span><span><span>    Signed-off-by: Tony Hutter &lt;hutter2@llnl.gov&gt;</span></span></code></pre></div>
<p>
Now, configure, build, and install ZFS.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sh autogen.sh
</span></span><span><span>sam@zfshax:~/zfs $ ./configure
</span></span><span><span>sam@zfshax:~/zfs $ gmake -s -j<span>$(</span>sysctl -n hw.ncpu<span>)</span>    <span># &lt;-- modified for FreeBSD</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo gmake install<span>;</span> sudo ldconfig  <span># &lt;-- modified for FreeBSD</span></span></span></code></pre></div>
<p>
Then, replace the FreeBSD's ZFS kernel module with the one we just built.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo kldunload zfs.ko  <span># Needed because zfs.sh only unloads openzfs.ko</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh</span></span></code></pre></div>
<p>
Finally, verify we're running version 2.1.11 as desired.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs version
</span></span><span><span>zfs-2.1.11-1
</span></span><span><span>zfs-kmod-2.1.11-1</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-11">
<h3 id="headline-11">
Reproducing the issue
</h3>
<div id="outline-text-headline-11">
<p>
Now we're ready to try reproducing the issue. This took some iteration to get right, so I wrote a bash script that starts from scratch on each invocation and then runs the commands needed to reproduce the corrupt state. After quite a bit of trial and error, I eventually produced a <a href="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/reproduce.sh">reproducer script</a> which does the following:</p>
<ol>
<li>Create 2 pools: <code>src</code> and <code>dst</code>.</li>
<li>Create <code>src/encryptionroot</code> using a passphrase encryption key.</li>
<li>Create <code>src/encryptionroot/child</code> which inherits <code>src/encryptionroot</code> as its encryption root.</li>
<li>Create files and take snapshots <code>src/encryptionroot@111</code> and <code>src/encryptionroot/child@111</code>.</li>
<li>Send raw snapshots <code>src/encryptionroot@111</code> and <code>src/encryptionroot/child@111</code> to <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code> respectively.</li>
<li>Load encryption key for <code>dst/encryptionroot</code> using passphrase and mount encrypted datasets <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code>. At this point, <code>src</code> and <code>dst</code> pools are in sync.</li>
<li>Change the <code>src/encryptionroot</code> encryption key from passphrase to hex.</li>
<li>Update files and take snapshots <code>src/encryptionroot@222</code> and <code>src/encryptionroot/child@222</code>.</li>
<li>Send a raw incremental snapshot of <code>src/encryptionroot/child@222</code> to <code>dst/encryptionroot/child</code>, but do <em>not</em> send <code>src/encryptionroot@222</code> which contains the key change!</li>
<li>Unmount <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code> and unload the cached encryption key for <code>dst/encryptionroot</code>.</li>
<li>Load the encryption key for <code>dst/encryptionroot</code> using the passphrase since we didn't send the updated encryption root after changing the key.</li>
<li>Try to remount <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code>.</li>
</ol>
<p>When we run the reproducer, the root encrypted dataset <code>dst/encryptionroot</code> mounts successfully and we can read the old file from the first snapshot, but the child encrypted dataset <code>dst/encryptionroot/child</code> fails to mount with <code>cannot mount 'dst/encryptionroot/child: Permission denied</code> just as we expected.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs mount dst/encryptionroot/child
</span></span><span><span>cannot mount <span>'dst/encryptionroot/child'</span>: Permission denied</span></span></code></pre></div>



<details>
    <summary title="Click to interact"><span>Full reproducer script output (long!)</span></summary>
    <div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce
</span></span><span><span>
</span></span><span><span>Destroy pools and backing files <span>if</span> they exist.
</span></span><span><span>+ zpool destroy src
</span></span><span><span>+ zpool destroy dst
</span></span><span><span>+ rm -f /src.img
</span></span><span><span>+ rm -f /dst.img
</span></span><span><span>
</span></span><span><span>Create pools using sparse files.
</span></span><span><span>+ truncate -s 100M /src.img
</span></span><span><span>+ truncate -s 100M /dst.img
</span></span><span><span>+ zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /src src /src.img
</span></span><span><span>+ zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /dst dst /dst.img
</span></span><span><span>
</span></span><span><span>Create root encrypted dataset using a passphrase encryption key.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs create -o <span>encryption</span><span>=</span>on -o <span>keyformat</span><span>=</span>passphrase -o <span>keylocation</span><span>=</span>prompt src/encryptionroot
</span></span><span><span>
</span></span><span><span>Create child encrypted dataset which inherits src/encryptionroot as its encryption root.
</span></span><span><span>+ zfs create src/encryptionroot/child
</span></span><span><span>
</span></span><span><span>Create files in the root and child encrypted datasets and snapshot both.
</span></span><span><span>+ touch /src/encryptionroot/111
</span></span><span><span>+ touch /src/encryptionroot/child/111
</span></span><span><span>+ zfs snapshot -r src/encryptionroot@111
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>1</span> <span>]</span> Files and snapshots are on the src pool but not the dst pool yet.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  passphrase  available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>NAME  ENCROOT  KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst   -        none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>/dst
</span></span><span><span>
</span></span><span><span>Send a raw replication stream of the src snapshots to the dst pool.
</span></span><span><span>+ zfs send --replicate --raw src/encryptionroot@111
</span></span><span><span>+ zfs recv dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load encryption key <span>for</span> the dst encryption root using passphrase and mount the encrypted datasets.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>2</span> <span>]</span> Files and snapshots are on both pools and in sync.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  passphrase  available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>
</span></span><span><span>Change the src encryption root key from passphrase to hex.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs change-key -o <span>keyformat</span><span>=</span>hex src/encryptionroot
</span></span><span><span>
</span></span><span><span>Update the files in the root and child encrypted datasets and snapshot both.
</span></span><span><span>+ mv /src/encryptionroot/111 /src/encryptionroot/222
</span></span><span><span>+ mv /src/encryptionroot/child/111 /src/encryptionroot/child/222
</span></span><span><span>+ zfs snapshot -r src/encryptionroot@222
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>3</span> <span>]</span> Updated files and snapshots are on the src pool but not the dst pool yet.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the child encrypted dataset to the dst pool.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot/child@111 src/encryptionroot/child@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>NOTE: The encryption key change on the src encryption root has not been sent to dst!
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>4</span> <span>]</span> File is updated in the dst child encrypted dataset but not the dst root encrypted dataset.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>
</span></span><span><span>NOTE: The updated file in the dst child encrypted dataset is only still readable because the encryption key is still loaded from before sending the snapshot taken after the key change.
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the passphrase since we did not send the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>cannot mount <span>'dst/encryptionroot/child'</span>: Permission denied
</span></span><span><span>+ <span>true</span>
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>5</span> <span>]</span> Mounting dst child encrypted dataset failed even though encryption key is ostensibly available. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    no       <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child</span></span></code></pre></div></details>

<p>
Now that we understand and can reliably reproduce the issue, we're a big step closer to fixing it!</p>
<hr>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-12">
<h2 id="headline-12">
Part 3: Recovering our data
</h2>
<div id="outline-text-headline-12">
<div id="outline-container-headline-13">
<h3 id="headline-13">
Theoretically easy to fix
</h3>
<div id="outline-text-headline-13">
<p>
We know now that a child encrypted dataset will become unmountable if the following conditions are met:</p>
<ol>
<li>The wrapping encryption key on the encryption root is changed.</li>
<li>A snapshot of the child encrypted dataset that was taken after the key change is sent.</li>
<li>A snapshot of the encryption root that was taken after the key change is <em>not</em> sent.</li>
</ol>
<p><strong>Lesson: Always send a snapshot of the encryption root after changing the encryption key.</strong></p>
<p>
In theory, all we should have to do to fix it is send the latest snapshot of the encryption root.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo ./repair_snapshot
</span></span><span><span>
</span></span><span><span>HYPOTHESIS: The child encrypted dataset should become decryptable again <span>if</span> a snapshot containing the key change on the root encrypted dataset is sent.
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the root encrypted dataset to the dst pool.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot@111 src/encryptionroot@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>3822096046979704342</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>10687499872806328230</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>16650389156603898046</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>157927145464667221</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>15788284772663365294</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>8879828033920251704</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>6286619359795670820</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>1835983340793043086</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>6911130245015256647</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>16650389156603898046</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>157927145464667221</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>15804809318195285947</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>8879828033920251704</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>6286619359795670820</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-14">
<h3 id="headline-14">
Not so easy in practice
</h3>
<div id="outline-text-headline-14">
<p>
Unfortunately, this isn't enough to fix <code>new</code> and <code>sneakernet</code>; there are no remaining snapshots or bookmarks left on the <code>old</code> encryption root from before the key change, and we can't generate an incremental send stream without one. Mapped to our reproduced example, this means that <code>src/encryptionroot@111</code> does not exist.</p>
<p>
You might think we could forcibly send the entire encryption root, but <code>zfs recv</code> will reject it no matter what you do.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: destination <span>'dst/encryptionroot'</span> exists
</span></span><span><span>must specify -F to overwrite it
</span></span><span><span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: destination has snapshots <span>(</span>eg. dst/encryptionroot@111<span>)</span>
</span></span><span><span>must destroy them to overwrite it
</span></span><span><span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs destroy dst/encryptionroot@111
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem or overwrite an unencrypted one with an encrypted one</span></span></code></pre></div>
<p>
<strong>Lesson: Create bookmarks before destroying snapshots.</strong></p>
<p>
We need to find a way to create an incremental send stream that contains the key change, but how?. We could try to manually craft a send stream containing the new key, but that sounds tricky. There's got to be a better way!</p>
</div>
</div>
<div id="outline-container-headline-15">
<h3 id="headline-15">
Idea for a hack
</h3>
<div id="outline-text-headline-15">
<p>
Recall that a snapshot is not the only valid source for generating an incremental send stream. What if we had a bookmark?</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo ./repair_bookmark
</span></span><span><span>
</span></span><span><span>Replace the initial parent encrypted dataset snapshot with a bookmark.
</span></span><span><span>+ zfs bookmark src/encryptionroot@111 src/encryptionroot#111
</span></span><span><span>+ zfs destroy src/encryptionroot@111
</span></span><span><span>
</span></span><span><span>HYPOTHESIS: The child encrypted dataset should become decryptable again <span>if</span> a snapshot containing the key change on the root encrypted dataset is sent.
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the root encrypted dataset to the dst pool using the bookmark.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1018261135296547862</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>1985286651877572312</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>src/encryptionroot#111        -                   -           -            -        <span>4964628655505655411</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>12927592016081051429</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>15258247229701443799</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>17755083343181277380</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>4964628655505655411</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>364333975888407846</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
<p>
A bookmark works just as well as a snapshot for generating an incremental send stream, but we don't have a bookmark on <code>old</code> either. How is this any better?</p>
<p>
Unlike a snapshot, which is effectively an entire dataset tree frozen in time (very complex), a bookmark is a very simple object on disk which consists of:</p>
<ol>
<li>The GUID of the snapshot.</li>
<li>The transaction group the snapshot was created in.</li>
<li>The Unix timestamp when the snapshot was created.</li>
</ol>
<p>For example, this is what our bookmark looks like in <code>zdb</code>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zdb src/encryptionroot#111
</span></span><span><span>	<span>#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}</span></span></span></code></pre></div>
<p>
Note that <code>zdb</code> shows the GUID in hexadecimal versus <code>zfs get guid</code> which shows it in decimal, consistency be damned. The <code>redaction_obj</code> is optional and only used for <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-redact.8.html#redaction">redaction bookmarks</a>, so we can ignore it.</p>
<p>
A bookmark is simple enough that we could feasibly hack ZFS into manually writing one for us, provided that we can figure out the right values to use. The GUID and Unix timestamp don't really matter for generating an incremental send stream, so we could choose them arbitrarily if we had to, but the transaction group id really matters because that is what ZFS uses to determine which blocks to include.</p>
<p>
But how can we figure out what transaction group the snapshot was created in if neither the snapshot nor a bookmark of the snapshot still exist? I initially considered walking the dataset trees on each pool, diffing them to find the newest block present on both datasets, and using its transaction group id, but I found a much easier way with one of ZFS's lesser known features.</p>
</div>
</div>
<div id="outline-container-headline-16">
<h3 id="headline-16">
A brief detour into pool histories
</h3>
<div id="outline-text-headline-16">
<p>
I didn't know about <a href="https://docs.freebsd.org/en/books/handbook/zfs/#zfs-zpool-history">pool histories</a> before embarking on this unplanned journey, but they are now yet another thing I love about ZFS. Every pool allocates <a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/spa_history.c#L108-L111">0.1% of its space (128 KiB minimum, 1 GiB maximum)</a> to a ring buffer which is used to log every command that is executed on the pool. This can be used to forensically reconstruct the state of the pool over time.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zpool <span>history</span>
</span></span><span><span>History <span>for</span> <span>'dst'</span>:
</span></span><span><span>2025-09-01.00:00:00 zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /dst dst /dst.img
</span></span><span><span>2025-09-01.00:00:00 zfs recv dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs load-key dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs recv -F dst/encryptionroot/child
</span></span><span><span>2025-09-01.00:00:00 zfs unload-key dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>History <span>for</span> <span>'src'</span>:
</span></span><span><span>2025-09-01.00:00:00 zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /src src /src.img
</span></span><span><span>2025-09-01.00:00:00 zfs create -o <span>encryption</span><span>=</span>on -o <span>keyformat</span><span>=</span>passphrase -o <span>keylocation</span><span>=</span>prompt src/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs create src/encryptionroot/child
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
</span></span><span><span>2025-09-01.00:00:00 zfs send --replicate --raw src/encryptionroot@111
</span></span><span><span>2025-09-01.00:00:00 zfs change-key -o <span>keyformat</span><span>=</span>hex src/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@222</span></span></code></pre></div>
<p>
ZFS also logs many internal operations in the pool history (search for <code>spa_history_log</code> in the source code) which can be viewed with the <code>-i</code> flag. For snapshots, this includes the transaction group (txg) id when the snapshot was created, which is exactly what we're looking for!</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zpool <span>history</span> -i src
</span></span><span><span>History <span>for</span> <span>'src'</span>:
</span></span><span><span>...
</span></span><span><span>2025-09-01.00:00:00 <span>[</span>txg:12<span>]</span> snapshot src/encryptionroot@111 <span>(</span>768<span>)</span>
</span></span><span><span>2025-09-01.00:00:00 <span>[</span>txg:12<span>]</span> snapshot src/encryptionroot/child@111 <span>(</span>770<span>)</span>
</span></span><span><span>2025-09-01.00:00:00 <span>(</span>3ms<span>)</span> ioctl snapshot
</span></span><span><span>    input:
</span></span><span><span>        snaps:
</span></span><span><span>            src/encryptionroot@111
</span></span><span><span>            src/encryptionroot/child@111
</span></span><span><span>        props:
</span></span><span><span>
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
</span></span><span><span>...</span></span></code></pre></div>
<p>
The GUID and creation timestamp we can easily get from the snapshot on <code>dst</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs list -o name,guid,creation -p dst/encryptionroot@111
</span></span><span><span>NAME                                   GUID  CREATION
</span></span><span><span>dst/encryptionroot@111  <span>4964628655505655411</span>  <span>1756699200</span></span></span></code></pre></div>
<p>
Now that we know everything we need to create the bookmark, we just need to figure out a way to manually create a bookmark with arbitrary data.</p>
</div>
</div>
<div id="outline-container-headline-17">
<h3 id="headline-17">
Hacking ZFS to manually create a bookmark
</h3>
<div id="outline-text-headline-17">
<p>
To understand how ZFS creates a bookmark, we can trace the code path from <code>zfs bookmark</code> all the way down to <code>dsl_bookmark_add</code> which actually adds the bookmark node to the tree.</p>
<ul>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/cmd/zfs/zfs_main.c#L215">command_table</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/cmd/zfs/zfs_main.c#L7806">zfs_do_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzfs_core/libzfs_core.c#L1128">lzc_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzfs_core/libzfs_core.c#L172">lzc_ioctl</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzutil/os/freebsd/zutil_compat.c#L95">zfs_ioctl_fd</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzutil/os/freebsd/zutil_compat.c#L48">zcmd_ioctl_compat</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/zfs_ioctl.c#L7028">zfs_ioctl_register bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/zfs_ioctl.c#L3721">zfs_ioc_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L613">dsl_bookmark_create</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L584">dsl_bookmark_create_sync</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L516">dsl_bookmark_create_sync_impl_book</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L393">dsl_bookmark_node_add</a></li>
</ul>
<p>This is the bookmark structure physically written to disk:</p>
<p>
<a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/include/sys/dsl_bookmark.h#L31-L57">zfs/include/sys/dsl_bookmark.h</a></p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>/*
</span></span></span><span><span><span> * On disk zap object.
</span></span></span><span><span><span> */</span>
</span></span><span><span><span>typedef</span> <span>struct</span> <span>zfs_bookmark_phys</span> <span>{</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_guid</span><span>;</span>		<span>/* guid of bookmarked dataset */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_creation_txg</span><span>;</span>	<span>/* birth transaction group */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_creation_time</span><span>;</span>	<span>/* bookmark creation time */</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for redacted send / recv */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_redaction_obj</span><span>;</span>	<span>/* redaction list object */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_flags</span><span>;</span>		<span>/* ZBM_FLAG_* */</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for bookmark written size */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_referenced_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_compressed_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_uncompressed_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_referenced_freed_before_next_snap</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_compressed_freed_before_next_snap</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_uncompressed_freed_before_next_snap</span><span>;</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for raw sends */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_ivset_guid</span><span>;</span>
</span></span><span><span><span>}</span> <span>zfs_bookmark_phys_t</span><span>;</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>#define	BOOKMARK_PHYS_SIZE_V1	(3 * sizeof (uint64_t))
</span></span></span><span><span><span>#define	BOOKMARK_PHYS_SIZE_V2	(12 * sizeof (uint64_t))</span></span></span></code></pre></div>
<p>
Only the first 3 fields are required for v1 bookmarks, while v2 bookmarks contain all 12 fields. <code>dsl_bookmark_node_add</code> only writes a v2 bookmark if one of the 9 v2 fields are non-zero, so we can leave them all zero to write a v1 bookmark.</p>
<p>
After a few iterations, I had a patch which hijacks the normal <code>zfs bookmark pool/dataset#src pool/dataset#dst</code> code path to create a bookmark with arbitrary data when the source bookmark name is <code>missing</code>.</p>
<div><pre tabindex="0"><code data-lang="diff"><span><span>sam@zfshax:~/zfs $ git --no-pager diff
</span></span><span><span>sam@zfshax:~/zfs $ git --no-pager diff
</span></span><span><span><span>diff --git a/cmd/zfs/zfs_main.c b/cmd/zfs/zfs_main.c
</span></span></span><span><span><span>index 2d81ef31c..73b5d7e70 100644
</span></span></span><span><span><span></span><span>--- a/cmd/zfs/zfs_main.c
</span></span></span><span><span><span></span><span>+++ b/cmd/zfs/zfs_main.c
</span></span></span><span><span><span></span><span>@@ -7892,12 +7892,15 @@ zfs_do_bookmark(int argc, char **argv)
</span></span></span><span><span><span></span> 		default: abort();
</span></span><span><span> 	}
</span></span><span><span>
</span></span><span><span><span>+// Skip testing for #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	/* test the source exists */
</span></span><span><span> 	zfs_handle_t *zhp;
</span></span><span><span> 	zhp = zfs_open(g_zfs, source, source_type);
</span></span><span><span> 	if (zhp == NULL)
</span></span><span><span> 		goto usage;
</span></span><span><span> 	zfs_close(zhp);
</span></span><span><span><span>+}
</span></span></span><span><span><span></span>
</span></span><span><span> 	nvl = fnvlist_alloc();
</span></span><span><span> 	fnvlist_add_string(nvl, bookname, source);
</span></span><span><span><span>diff --git a/module/zfs/dsl_bookmark.c b/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span>index 861dd9239..fae882f45 100644
</span></span></span><span><span><span></span><span>--- a/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span></span><span>+++ b/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span></span><span>@@ -263,7 +263,12 @@ dsl_bookmark_create_check_impl(dsl_pool_t *dp,
</span></span></span><span><span><span></span> 		 * Source must exists and be an earlier point in newbm_ds's
</span></span><span><span> 		 * timeline (newbm_ds's origin may be a snap of source's ds)
</span></span><span><span> 		 */
</span></span><span><span><span>+// Skip looking up #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source, "#missing") == NULL) {
</span></span></span><span><span><span></span> 		error = dsl_bookmark_lookup(dp, source, newbm_ds, &amp;source_phys);
</span></span><span><span><span>+} else {
</span></span></span><span><span><span>+		error = 0;
</span></span></span><span><span><span>+}
</span></span></span><span><span><span></span> 		switch (error) {
</span></span><span><span> 		case 0:
</span></span><span><span> 			break; /* happy path */
</span></span><span><span><span>@@ -545,12 +550,34 @@ dsl_bookmark_create_sync_impl_book(
</span></span></span><span><span><span></span> 	 *   because the redaction object might be too large
</span></span><span><span> 	 */
</span></span><span><span>
</span></span><span><span><span>+// Skip looking up #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source_name, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	VERIFY0(dsl_bookmark_lookup_impl(bmark_fs_source, source_shortname,
</span></span><span><span> 	    &amp;source_phys));
</span></span><span><span><span>+}
</span></span></span><span><span><span></span> 	dsl_bookmark_node_t *new_dbn = dsl_bookmark_node_alloc(new_shortname);
</span></span><span><span>
</span></span><span><span><span>+// Skip copying from #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source_name, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	memcpy(&amp;new_dbn-&gt;dbn_phys, &amp;source_phys, sizeof (source_phys));
</span></span><span><span> 	new_dbn-&gt;dbn_phys.zbm_redaction_obj = 0;
</span></span><span><span><span>+} else {
</span></span></span><span><span><span>+	// Manually set the bookmark parameters.
</span></span></span><span><span><span>+	new_dbn-&gt;dbn_phys = (zfs_bookmark_phys_t){
</span></span></span><span><span><span>+		.zbm_guid = 4964628655505655411,
</span></span></span><span><span><span>+		.zbm_creation_txg = 12,
</span></span></span><span><span><span>+		.zbm_creation_time = 1756699200,
</span></span></span><span><span><span>+		.zbm_redaction_obj = 0,
</span></span></span><span><span><span>+		.zbm_flags = 0,
</span></span></span><span><span><span>+		.zbm_referenced_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_compressed_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_uncompressed_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_referenced_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_compressed_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_uncompressed_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_ivset_guid = 0,
</span></span></span><span><span><span>+	};
</span></span></span><span><span><span>+}
</span></span></span><span><span><span></span>
</span></span><span><span> 	/* update feature counters */
</span></span><span><span> 	if (new_dbn-&gt;dbn_phys.zbm_flags &amp; ZBM_FLAG_HAS_FBN) {
</span></span></code></pre></div>
<p>
To test, we recompile ZFS, reload the kernel module, and reimport the pools.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ gmake -s -j<span>$(</span>sysctl -n hw.ncpu<span>)</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo gmake install <span>&amp;&amp;</span> sudo ldconfig
</span></span><span><span>sam@zfshax:~/zfs $ sudo zpool <span>export</span> src <span>&amp;&amp;</span> sudo zpool <span>export</span> dst
</span></span><span><span>sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh -r
</span></span><span><span>sam@zfshax:~/zfs $ sudo zpool import src -d / <span>&amp;&amp;</span> sudo zpool import dst -d /</span></span></code></pre></div>
<p>
Then, we create the bookmark ex nihilo using the magic bookmark name <code>missing</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs bookmark src/encryptionroot#missing src/encryptionroot#111
</span></span><span><span>sam@zfshax:~/zfs $ sudo zdb src/encryptionroot#111
</span></span><span><span>	<span>#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}</span></span></span></code></pre></div>
<p>
Success! We can now use the bookmark to generate an incremental send stream containing the new hex wrapping key parameters.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> zstreamdump
</span></span><span><span>BEGIN record
</span></span><span><span>	<span>hdrtype</span> <span>=</span> <span>1</span>
</span></span><span><span>	<span>features</span> <span>=</span> <span>1420004</span>
</span></span><span><span>	<span>magic</span> <span>=</span> 2f5bacbac
</span></span><span><span>	<span>creation_time</span> <span>=</span> 68d3d93f
</span></span><span><span>	<span>type</span> <span>=</span> <span>2</span>
</span></span><span><span>	<span>flags</span> <span>=</span> 0xc
</span></span><span><span>	<span>toguid</span> <span>=</span> 3f99b9e92cc0aca7
</span></span><span><span>	<span>fromguid</span> <span>=</span> 44e5e7755d23c673
</span></span><span><span>	<span>toname</span> <span>=</span> src/encryptionroot@222
</span></span><span><span>	<span>payloadlen</span> <span>=</span> <span>1028</span>
</span></span><span><span>nvlist version: <span>0</span>
</span></span><span><span>	<span>crypt_keydata</span> <span>=</span> <span>(</span>embedded nvlist<span>)</span>
</span></span><span><span>	nvlist version: <span>0</span>
</span></span><span><span>		<span>DSL_CRYPTO_SUITE</span> <span>=</span> 0x8
</span></span><span><span>		<span>DSL_CRYPTO_GUID</span> <span>=</span> 0x6196311f2622e30
</span></span><span><span>		<span>DSL_CRYPTO_VERSION</span> <span>=</span> 0x1
</span></span><span><span>		<span>DSL_CRYPTO_MASTER_KEY_1</span> <span>=</span> 0x6c 0x55 0x13 0x78 0x8c 0x2d 0x42 0xb5 0x9e 0x33 0x2 0x7e 0x73 0x3a 0x46 0x20 0xd2 0xf7 0x23 0x7d 0x7c 0x5d 0x5f 0x76 0x63 0x90 0xd2 0x43 0x6a 0xdd 0x63 0x2b
</span></span><span><span>		<span>DSL_CRYPTO_HMAC_KEY_1</span> <span>=</span> 0x85 0xd1 0xf3 0xba 0xed 0xec 0x6 0x28 0x36 0xd6 0x60 0x28 0x8d 0x2f 0x6f 0x14 0xc9 0x2b 0x6f 0xf4 0x19 0x23 0x2d 0xf 0x3d 0xe 0xc4 0x88 0x4 0x6d 0xca 0xb5 0x2d 0x4d 0x8 0x75 0x17 0x1c 0xe3 0xe7 0xe6 0x23 0x7 0x53 0x94 0xba 0xc7 0x4b 0xf5 0xde 0x8c 0x29 0xa3 0x27 0xdf 0x82 0x64 0x9d 0x92 0xb4 0xc1 0x26 0x5b 0x32
</span></span><span><span>		<span>DSL_CRYPTO_IV</span> <span>=</span> 0xdf 0x52 0x77 0xe8 0xf 0xfd 0xc2 0x42 0x66 0x88 0xb9 0xf0
</span></span><span><span>		<span>DSL_CRYPTO_MAC</span> <span>=</span> 0x54 0x54 0x15 0xa4 0x21 0x55 0x6b 0x4e 0x99 0xe7 0xf 0xef 0x9f 0x90 0x42 0x54
</span></span><span><span>		<span>portable_mac</span> <span>=</span> 0x3a 0xd6 0x30 0xc4 0x6a 0x2d 0x60 0x24 0x95 0xfc 0x99 0xbb 0xfa 0x10 0xa0 0x6b 0xc6 0x1 0xdd 0x1d 0x9 0xcd 0xa8 0x19 0xdf 0x57 0xb9 0x90 0x4f 0x2e 0x33 0xc1
</span></span><span><span>		<span>keyformat</span> <span>=</span> 0x2
</span></span><span><span>		<span>pbkdf2iters</span> <span>=</span> 0x0
</span></span><span><span>		<span>pbkdf2salt</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_checksum</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_compress</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_nlevels</span> <span>=</span> 0x6
</span></span><span><span>		<span>mdn_blksz</span> <span>=</span> 0x4000
</span></span><span><span>		<span>mdn_indblkshift</span> <span>=</span> 0x11
</span></span><span><span>		<span>mdn_nblkptr</span> <span>=</span> 0x3
</span></span><span><span>		<span>mdn_maxblkid</span> <span>=</span> 0x4
</span></span><span><span>		<span>to_ivset_guid</span> <span>=</span> 0x957edeaa7123a7
</span></span><span><span>		<span>from_ivset_guid</span> <span>=</span> 0x0
</span></span><span><span>	<span>(</span>end crypt_keydata<span>)</span>
</span></span><span><span>
</span></span><span><span>END <span>checksum</span> <span>=</span> 14046201258/62f53166ccc36/14023a70758c3195/1e906f4670783cd
</span></span><span><span>SUMMARY:
</span></span><span><span>	Total DRR_BEGIN <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>1028</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_END <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_OBJECT <span>records</span> <span>=</span> <span>7</span> <span>(</span><span>960</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_FREEOBJECTS <span>records</span> <span>=</span> <span>2</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>512</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE_BYREF <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE_EMBEDDED <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_FREE <span>records</span> <span>=</span> <span>12</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_SPILL <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total <span>records</span> <span>=</span> <span>26</span>
</span></span><span><span>	Total payload <span>size</span> <span>=</span> <span>2500</span> <span>(</span>0x9c4<span>)</span>
</span></span><span><span>	Total header <span>overhead</span> <span>=</span> <span>8112</span> <span>(</span>0x1fb0<span>)</span>
</span></span><span><span>	Total stream <span>length</span> <span>=</span> <span>10612</span> <span>(</span>0x2974<span>)</span></span></span></code></pre></div>
<p>
But we can't receive the send stream.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive incremental stream: IV <span>set</span> guid missing. See errata <span>4</span> at https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-18">
<h3 id="headline-18">
The final obstacle
</h3>
<div id="outline-text-headline-18">
<p>
ZFS refuses the stream because it is missing a source IV set GUID (see <code>from_ivset_guid = 0x0</code> in the <code>zstreamdump</code> above). This is because we created a v1 bookmark which does not contain the IV set GUID like a v2 bookmark would.</p>
<p>
Since we know that the send stream is created using the right snapshots, we can temporarily disable checking IV set GUIDs to allow the snapshot to be received as described in <a href="https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER/index.html#zfs-errata-4">errata 4</a>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check<span>=</span><span>1</span>
</span></span><span><span>vfs.zfs.disable_ivset_guid_check: <span>0</span> -&gt; <span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check<span>=</span><span>0</span>
</span></span><span><span>vfs.zfs.disable_ivset_guid_check: <span>1</span> -&gt; <span>0</span>
</span></span><span><span>sam@zfshax:~ $ sudo zpool <span>export</span> dst
</span></span><span><span>sam@zfshax:~ $ sudo zpool import dst -d /
</span></span><span><span>sam@zfshax:~ $ sudo zpool scrub dst
</span></span><span><span>sam@zfshax:~ $ sudo zpool status -x
</span></span><span><span>all pools are healthy</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-19">
<h3 id="headline-19">
The moment of truth
</h3>
<div id="outline-text-headline-19">
<p>
And now for the moment of truth…</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ <span>echo</span> <span>"0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef"</span> <span>|</span> sudo zfs load-key dst/encryptionroot
</span></span><span><span>sam@zfshax:~ $ sudo zfs mount -a
</span></span><span><span>sam@zfshax:~ $ sudo zfs list -t all -o name,encryptionroot,keyformat,keystatus,mounted,guid -r dst
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>15258247229701443799</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>17755083343181277380</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>4964628655505655411</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>364333975888407846</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>sam@zfshax:~ $ tree --noreport --noreport /dst
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
<p>
<a href="https://www.youtube.com/watch?v=30jNsCVLpAE&amp;t=931s">WE'RE GONNA LIVE!!!</a></p>
<p>
At this point, we can now reliably fix the issue in our test environment. All we need to do now is use our hacked ZFS build to create the bookmark on <code>old</code>, send an incremental snapshot of the encryption root with the new key to <code>sneakernet</code>, and then send that snapshot from <code>sneakernet</code> to <code>new</code>. I rebuilt ZFS again with the correct transaction group, GUID, and creation timestamp for <code>old</code>, repeated the same steps with the names changed, and thanks to our thorough testing, it worked on the first try!</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-20">
<h2 id="headline-20">
Conclusion
</h2>
<div id="outline-text-headline-20">
<p>
After a week of intense research and debugging, I had rescued our data back from the brink and could again sleep soundly at night. While I appreciated the opportunity to learn more about ZFS, I can't help but think about how this entire incident <a href="https://how.complexsystems.fail/#3">could have been avoided at several key points</a> which translate directly into lessons learned:</p>
<ol>
<li>Test backups continuously so you get immediate feedback when they break.</li>
<li>Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.</li>
<li>Always send a snapshot of the encryption root after changing the encryption key.</li>
<li>Create bookmarks before destroying snapshots.</li>
</ol>
<p>I hope that you may learn from my mistakes and avoid a similar incident. If you do happen to find yourself in a similar predicament, I'd love to hear from you regardless of whether this postmortem was helpful or not. My contact details can be found <a href="https://sambowman.tech/">here</a>.</p>
<p>
Knowing what I now know about ZFS native encryption, I find it difficult to recommend until the <a href="https://github.com/openzfs/openzfs-docs/issues/494">sharp edges</a> have all been filed down. In most cases, I'd prefer to encrypt the entire pool at the block device level and encrypt send streams with <a href="https://age-encryption.org/">age</a>. But if you really do need the flexibility offered by native encryption, always remember to mind the encryptionroot!</p>
</div>
</div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atuin Desktop: Runbooks That Run – Now Open Source (219 pts)]]></title>
            <link>https://blog.atuin.sh/atuin-desktop-open-source/</link>
            <guid>45431001</guid>
            <pubDate>Tue, 30 Sep 2025 20:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.atuin.sh/atuin-desktop-open-source/">https://blog.atuin.sh/atuin-desktop-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=45431001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://blog.atuin.sh/tag/news/">News</a>
            
                <p>Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.</p>

            <div>
                <p><a href="https://blog.atuin.sh/author/ellie/">
                                <img src="https://blog.atuin.sh/content/images/size/w160/2024/01/me2.jpg" alt="Ellie Huxtable">
                            </a>
                </p>
                
            </div>

            
        </header>

        <section>
            <figure data-kg-thumbnail="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final.mp4" poster="https://img.spacergif.org/v1/1852x1600/0a/spacer.png" width="1852" height="1600" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:27</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg"></figure><p>Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.</p><p><a href="https://atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">Atuin CLI</a>&nbsp;fixed part of this, with synced, searchable shell history. But history isn’t enough. Teams need workflows they can&nbsp;<strong>repeat, share, and trust</strong>.</p><p>That’s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully&nbsp;<a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">open source</a>.</p><h2 id="what-is-atuin-desktop">What is Atuin Desktop?</h2><p>Atuin Desktop looks like a doc, but runs like your terminal.&nbsp;Built to make local developer workflows repeatable, shareable, and reliable.</p><figure><div><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png" width="2000" height="1584" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png" width="2000" height="1654" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.</p><ul><li><strong>Kill context switching:</strong> Chain shell <a href="https://man.atuin.sh/blocks/executable/script/?ref=blog.atuin.sh" rel="noreferrer">scripts</a>, <a href="https://man.atuin.sh/blocks/databases/?ref=blog.atuin.sh" rel="noreferrer">database</a> queries, and <a href="https://man.atuin.sh/blocks/network/http/?ref=blog.atuin.sh" rel="noreferrer">HTTP</a> requests</li><li><strong>Docs that don't rot: </strong>execute directly + stay relevant</li><li><strong>Reusable automation: </strong>dynamic runbooks with <a href="https://man.atuin.sh/templating/?ref=blog.atuin.sh" rel="noreferrer">Jinja-style templating</a></li><li><strong>Local knowledge</strong>: Build runbooks from your real shell history</li><li><strong>Collaborative</strong>: Sync and share via <a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noreferrer">Git</a>, or in <a href="https://man.atuin.sh/hub/getting-started/?ref=blog.atuin.sh" rel="noreferrer">real-time</a> via our Hub</li></ul><div data-layout="immersive">
                    
                        <div>
                            <p dir="ltr"><span>Back in April we&nbsp;</span><a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/" target="_blank" rel="noopener noreferrer nofollow"><span>launched the closed beta</span></a><span>.&nbsp;</span></p><p dir="ltr"><span>Thousands of you signed up, used it at your day jobs, and told us exactly what broke. We’ve listened, rebuilt, and now it’s ready for everyone.</span></p>
                        </div>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="what%E2%80%99s-new-since-april">What’s new since April?</h2><p>Our early users gave us a lot of feedback, which we've used to build something much better. </p><ul><li>Offline, file based, Git/VCS-compatible&nbsp;<a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">workspaces</a></li><li><a href="https://man.atuin.sh/hub/collaborative-editing/?ref=blog.atuin.sh#team-based-collaboration" rel="noopener noreferrer nofollow">Team</a>&nbsp;accounts with shared, realtime workspaces</li><li><a href="https://man.atuin.sh/blocks/executable/kubernetes?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Kubernetes</a>&nbsp;integration for live state and monitoring</li><li><a href="https://man.atuin.sh/blocks/databases/mysql?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">MySQL</a>&nbsp;query blocks</li><li><a href="https://man.atuin.sh/blocks/executable/dropdown?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Dropdown</a>&nbsp;and more contextual blocks</li><li>A huge number of bug fixes, performance improvements, and UI upgrades</li></ul><h2 id="how-it%E2%80%99s-being-used">How it’s being used</h2><p>Atuin Desktop is already being used across engineering teams for serious, day-to-day work.</p><ul><li><strong>Automation and debugging:</strong> linking commands, monitoring systems, and tracking results</li><li><strong>Database operations:</strong> managing migrations, access control, and production queries</li><li><strong>Onboarding:</strong> getting started workflows new engineers can actually run</li><li><strong>Deploying and managing clusters:</strong> repeatable, documented automation for real environments</li><li><strong>Incident response:</strong> runbooks that execute instead of rotting in some internal wiki</li></ul><p>It’s become a shared system of record for the commands and processes that keep production alive.</p><h2 id="what%E2%80%99s-next">What’s next</h2><p>We’re just getting started! We've got a lot in the pipeline, including:</p><ul><li>Block dependencies and advanced execution flow</li><li>Run runbooks remotely and on CI</li><li>Audit logs and enhanced permissions</li><li>Comments and deeper collaboration</li><li>More block types<ul><li>Specify local networks, containers, and more</li><li>Tighter integration with authentication and cloud providers</li></ul></li><li>More polish, more speed, fewer bugs</li></ul><div data-layout="immersive">
                    
                        <p dir="ltr"><span>Stop copy-pasting from outdated wiki pages, and get started with Atuin Desktop</span></p>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="getting-involved">Getting involved</h2><p>Atuin Desktop is now in open beta and open source under the Apache 2.0 license. Star it, fork it, break it: <a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener">github.com/atuinsh/desktop</a></p><p>Infrastructure deserves better than rotting docs and tribal knowledge. Atuin Desktop is our attempt to fix that for everyone who’s ever said “I swear I’ve done this before.”</p><p><strong>Discord: </strong><a href="https://discord.gg/Fq8bJSKPHh?ref=blog.atuin.sh">discord.gg/Fq8bJSKPHh</a></p><p><strong>Forum: </strong><a href="https://forum.atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">forum.atuin.sh</a></p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diff Algorithms (204 pts)]]></title>
            <link>https://flo.znkr.io/diff/</link>
            <guid>45430604</guid>
            <pubDate>Tue, 30 Sep 2025 20:09:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flo.znkr.io/diff/">https://flo.znkr.io/diff/</a>, See on <a href="https://news.ycombinator.com/item?id=45430604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to
compare different versions of the same file (e.g., during code review or when trying to understand
the history of a file), to visualize the difference of a failing test compared with its
expectation, or to apply changes to source files automatically.</p>
<p>Every project I worked on professionally or privately eventually needed a diff to visualize a change
or to apply a patch. However, I have never been satisfied with any of the freely available diff
libraries. This was never really a problem professionally, but for private projects, I have copied
and modified my own library from project to project until I mentioned this to a colleague who set me
on the path to publish my Go library (a port of a previous C++ library I used to copy and modify).
<em>Boy, did I underestimate how close my library was to publishability!</em></p>
<p>Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at
<a href="https://znkr.io/diff">znkr.io/diff</a> and what I learned in this article. I am not finished learning
yet, so I plan to update this article as my understanding continues to evolve.</p>
<h2 id="existing-diff-libraries">Existing Diff Libraries<a href="#existing-diff-libraries"></a></h2>
<p>Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of
attributes that are important to me. Not all of these attributes are important for every use case,
but a diff library that I can use for all of my use cases needs to fulfill all of them.</p>
<p>Usually, the input to a diff algorithm is text, and most diff libraries only support that. However,
I occasionally have use cases where I need to compare things that are not text. So any diff library
that only supports text doesn't meet my needs; instead, I need support for <strong>arbitrary sequences</strong>.</p>
<p>The resulting diff output is intended to be readable by humans. Quite often, especially for text, a
good way to present a diff is in the <strong>unified format</strong>. However, it's not always the best
presentation. A diff library should make it easy to output a diff in unified format, but it should
also provide a way to customize the presentation by providing a <strong>structured result</strong>.</p>
<p>Besides the presentation, the content of a diff should make it easy for humans to understand the
diff. This is a somewhat subjective criterion, but there are a number of failure cases that are
easily avoided, and there's some research into <strong>diff readability</strong> to set a benchmark. On the other
hand, diffs should be <strong>minimal</strong> in that they should be as small as possible.</p>
<p>Last but not least, it's important that a diff library has a <strong>simple API</strong> and provides good
<strong>performance</strong> in both runtime and memory usage, even in worst-case
scenarios<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries
and summarized them.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/sergi/go-diff">diffmatchpatch</a></td>
<td>❌<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>🤔<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></td>
<td>➖➖</td>
<td>➖</td>
<td>➖</td>
</tr>
<tr>
<td><a href="https://github.com/rogpeppe/go-internal/tree/master/diff">go-internal</a></td>
<td>❌<sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td><a href="https://github.com/kylelemons/godebug/tree/master/diff">godebug</a></td>
<td>❌<sup id="fnref2:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➖➖➖ /🧨<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/mb0/diff">mb0</a></td>
<td>✅</td>
<td>❌<sup id="fnref1:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>😐<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></td>
<td>➖➖</td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/aymanbagabas/go-udiff">udiff</a></td>
<td>❌<sup id="fnref3:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➕<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
<td>➖</td>
<td>➖➖<sup id="fnref1:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
</tr>
</tbody>
</table>
<div><p>Beware</p><p>The way I assigned ➕ and ➖ in this table doesn't follow any scientific methodology
it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking
for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons.
You can find the code I used for these comparisons in <a href="https://github.com/znkr/diff/tree/main/internal/benchmarks">on
github</a>.</p>
</div>
<h2 id="challenges">Challenges<a href="#challenges"></a></h2>
<p>The results suggest that it's far from trivial to implement a good diff library, and the one I had
started out with wasn't much better. To understand why the existing libraries are as they are,
we need to take a peek into the implementation.</p>
<h3 id="complexity">Complexity<a href="#complexity"></a></h3>
<p>With the exception of go-internal, all libraries use <a href="http://www.xmailserver.org/diff2.pdf">Myers'
Algorithm</a> to compute the diff. This is a standard algorithm
that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a
runtime complexity of 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mi>D</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(ND)</annotation>
  </semantics>
</math>
 where 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>N</mi>
    </mrow>
    <annotation encoding="application/x-tex">N</annotation>
  </semantics>
</math>
 is the number of input elements and 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>D</mi>
    </mrow>
    <annotation encoding="application/x-tex">D</annotation>
  </semantics>
</math>
 is the
edit distance between the two inputs. This means that the algorithm is very fast for inputs that are
similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for
inputs that are very different, the complexity approaches 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
. Furthermore, the
algorithm comes in two variants with a space complexity of either 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 or

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N)</annotation>
  </semantics>
</math>
. Only godebug uses the variant with quadratic memory growth.</p>
<p>This means that <strong>it's relatively easy to write a well-performing diffing algorithm for small or
similar inputs, but it takes a very long time to complete for larger, less similar inputs</strong>. A
consequence of this is that we can't trust simple benchmarks; instead, we need to test the
worst-case scenario<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>As always in cases like this, we can improve the performance by approximating an optimal solution.
There are a number of heuristics that reduce the time complexity by trading off diff minimality. For
example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a
an extremely aggressive heuristic.</p>
<p>Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using
only heuristics. go-internal uses <a href="https://bramcohen.livejournal.com/73318.html">patience diff</a>. The
heuristic is good enough that it alone almost always results in a good diff with a runtime
complexity of 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo stretchy="false" form="prefix">(</mo>
      <mi>N</mi>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N \, \log \, N)</annotation>
  </semantics>
</math>
<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>. An additional advantage of
this algorithm is that it produces more readable diffs. However, patience diff can fail with very
large diffs, and it can only be implemented efficiently using a hash table, which restricts the
possible applications.</p>
<div><p>Histogram Diff</p><p>Besides patience diff, there's another interesting heuristic called histogram
diff. I still have to implement it and understand it better before writing about it here, though.</p>
</div>
<h3 id="readability">Readability<a href="#readability"></a></h3>
<p>Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial
cases, there are always multiple minimal diffs. For example, this simple diff</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_01.diff">example_01.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>is as minimal as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_02.diff">example_02.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>Not all of the minimal or near-minimal diffs have the same readability for humans. For
example<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03.diff">example_03.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="14">
                <td>7</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="14">
                <td>14</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>is much more readable than the equally minimal and correct</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_04.diff">example_04.diff</a>
</caption>
<tbody><tr data-op="delete" data-x-lineno="1">
                <td>1</td>
                <td></td>
                <td>-</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="3">
                <td>3</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="4">
                <td>4</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="4">
                <td>5</td>
                <td>4</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="6">
                <td>6</td>
                <td></td>
                <td>-</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="6">
                <td>7</td>
                <td>6</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="7">
                <td>8</td>
                <td>7</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="8">
                <td></td>
                <td>8</td>
                <td>+</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="9">
                <td>10</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="10">
                <td></td>
                <td>10</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="12">
                <td>12</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="15">
                <td>15</td>
                <td>15</td>
                <td> </td>
                <td><code></code></td>
            </tr></tbody>
</table>
<p>Furthermore, if we relax minimality to accept approximations, the number of possible results
increases significantly.</p>
<p>For good diff readability, we have to select one solution from the many possible ones that is
readable for humans. Many people believe that the diff readability is determined by the algorithm.
However, that's only partially correct, because <strong>different <em>implementations</em> of the same algorithm
can produce vastly different results</strong>.</p>
<p>There's also been a lot of progress in the past years to improve diff readability. Perhaps the best
work about diff readability is <a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a> by
<a href="https://github.com/mhagger">Michael Haggerty</a>. He implemented a heuristic that's applied in a
post-processing step to improve the readability.</p>
<p>In fact, <code>example_03.diff</code> above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03_no_indent_heuristic.diff">example_03_no_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="delete" data-x-lineno="7">
                <td>7</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>Notice that the deletion starts at the end of the preceding function and leaves a small
remainder of the function being deleted? Michael's heuristic fixes this problem and results in the
very readable <code>example_03.diff</code>.</p>
<div><p>It's not the algorithm</p><p><code>example_04.diff</code> was found using a different implementation of Myers'
linear-space variant. That is, both <code>example_03.diff</code> and <code>example_04.diff</code> used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.</p>
</div>
<h2 id="a-new-diffing-library-for-go">A New Diffing Library for Go<a href="#a-new-diffing-library-for-go"></a></h2>
<p>I created <a href="https://znkr.io/diff">znkr.io/diff</a> to address these challenges in a way that works for
all my use cases. Let's reiterate what I want from a diffing library:</p>
<ul>
<li>The input can be text and arbitrary slices</li>
<li>The output should be possible in unified format and as a structured result</li>
<li>The API should be simple</li>
<li>The diffs should be minimal or near-minimal</li>
<li>The runtime and memory performance should be excellent</li>
</ul>
<p>This is a lot more than what any of the existing libraries provide. When I copied and modified my
old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing
library needs to be general enough to cover the vast majority of use cases. At the same time, it
needs to be extensible to make sure new features can be implemented without cluttering the API over
time.</p>
<p>Unfortunately, excellent performance and minimal results are somewhat in opposition to one another
and I ended up providing three different modes of operation: Default (balanced between performance
and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever
the cost).</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref2:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref3:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Default</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
<tr>
<td>Fast</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td>Optimal</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
</tbody>
</table>
<div><p>Text Only</p><p>This table only applies to text (same as the table above), non-text inputs can have
a different performance (if they are not <code>comparable</code> or readability).</p>
</div>
<h3 id="api">API<a href="#api"></a></h3>
<p>To design this API, I started with the data structures that I wanted to use as a user of the API and
worked backwards from there. At a very high level, there are two structured representations of a
diff that have been useful to me: a flat sequence of all deletions, insertions, and matching
elements (called <em>edits</em>) and a nested sequence of consecutive changes (called <em>hunks</em>).</p>
<ul>
<li>Edits are what I use to represent edits in this article; they contain the full content of both
inputs and how one is transformed into the other.</li>
<li>Hunks are a great representation for unit tests, because they are empty if both inputs are
identical and they make it possible to visualize just the changes even if the inputs are large.</li>
</ul>
<h4 id="arbitrary-slices">Arbitrary Slices<a href="#arbitrary-slices"></a></h4>
<p>I started with the design for the most general case, arbitrary slices. The Go representation for
diffing slices I liked the most is this one (see also
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a>):</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>5</td>
        <td><code><span>// Op describes an edit operation.</span>
</code></td>
    </tr>
    <tr>
        <td>6</td>
        <td><code><span>type</span> Op int
</code></td>
    </tr>
    <tr>
        <td>7</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>const</span> (
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code>	Match  Op = <span>iota</span> <span>// Two slice elements match</span>
</code></td>
    </tr>
    <tr>
        <td>10</td>
        <td><code>	Delete           <span>// A deletion from an element on the left slice</span>
</code></td>
    </tr>
    <tr>
        <td>11</td>
        <td><code>	Insert           <span>// An insertion of an element from the right side</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code>)
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code><span>// Edit describes a single edit of a diff.</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code><span>// - For Match, both X and Y contain the matching element.</span>
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code><span>// - For Delete, X contains the deleted element and Y is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// - For Insert, Y contains the inserted element and X is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Edit[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	Op   Op
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	X, Y T
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>type</span> Hunk[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code>	PosX, EndX int       <span>// Start and end position in x.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code>	PosY, EndY int       <span>// Start and end position in y.</span>
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x[PosX:EndX] to y[PosY:EndY]</span>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code>}
</code></td>
    </tr></tbody>
</table>
<p>The alternatives I have seen are variations and combinations of two themes. Either using slices to
represent edit operations in <code>Hunk</code></p>
<pre><code>type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
</code></pre>
<p>Or using indices instead of elements</p>
<pre><code>type Edit struct {
	Op         Op
	PosX, PosY []int
}
</code></pre>
<p>All of these representations work, but I found that the representations above served my use cases
best. One little quirk is that <code>Edit</code> always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).</p>
<p>Once the data structures were established, it was quite obvious that the simplest way to fill them
with diff data was to write two functions <a href="https://pkg.go.dev/znkr.io/diff#Edits"><code>diff.Edits</code></a> and
<a href="https://pkg.go.dev/znkr.io/diff#Hunks"><code>diff.Hunks</code></a> to return the diffs. I made them extensible by
using <a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">functional options</a>.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>30</td>
        <td><code><span>// Edits compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>31</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>32</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>33</td>
        <td><code><span>// Edits returns one edit for every element in the input slices. If x and y are identical, the</span>
</code></td>
    </tr>
    <tr>
        <td>34</td>
        <td><code><span>// output will consist of a match edit for every input element.</span>
</code></td>
    </tr>
    <tr>
        <td>35</td>
        <td><code><span>func</span> Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>36</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>37</td>
        <td><code><span>// Hunks compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>38</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>39</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>40</td>
        <td><code><span>// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions</span>
</code></td>
    </tr>
    <tr>
        <td>41</td>
        <td><code><span>// and deletions) along with some surrounding context.</span>
</code></td>
    </tr>
    <tr>
        <td>42</td>
        <td><code><span>func</span> Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<p>The options allow for future extensibility and allow changing the behavior of these functions. For
example, the option <a href="https://pkg.go.dev/znkr.io/diff#Context"><code>diff.Context(5)</code></a> configures <code>Hunks</code>
to provide 5 elements of surrounding context.</p>
<p>However, the current API still doesn't allow <em>arbitrary slices</em>; it only allows slices of
<code>comparable</code> types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the <code>Func</code> suffix for functions like this, so I followed
the lead:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>44</td>
        <td><code><span>// EditsFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>45</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>46</td>
        <td><code><span>func</span> EditsFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>47</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>48</td>
        <td><code><span>// HunksFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>49</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>50</td>
        <td><code><span>func</span> HunksFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="text">Text<a href="#text"></a></h4>
<p>While this API works well to produce a structured result for arbitrary slices, it doesn't provide
output in unified format for text inputs. My first approach was to provide a helper function that
returns a diff in unified format: <code>diff.ToUnified(hunks []Hunk[string]) string</code>. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>7</td>
        <td><code><span>// Unified compares the lines in x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>// the other in unified format.</span>
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code><span>func</span> Unified[T string | []byte](x, y T, opts ...diff.Option) T
</code></td>
    </tr></tbody>
</table>
<p>I also moved this function to the <a href="https://pkg.go.dev/znkr.io/diff/textdiff"><code>textdiff</code></a> package to
highlight the difference in expected input.</p>
<p>Now, I also happen to have use cases where I need structured results for text diffs. It would be
very annoying if I had to split those into lines manually. Besides, I can make a few more
assumptions about text that allow for a slight simplification of the data structures:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>11</td>
        <td><code><span>// Edit describes a single edit of a line-by-line diff.</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code><span>type</span> Edit[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>	Op   diff.Op <span>// Edit operation</span>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code>	Line T       <span>// Line, including newline character (if any)</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Hunk[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	PosX, EndX int       <span>// Start and end line in x (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	PosY, EndY int       <span>// Start and end line in y (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x lines PosX..EndX to y lines PosY..EndY</span>
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>// Edits compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code><span>func</span> Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code><span>// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>29</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>30</td>
        <td><code><span>func</span> Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="conclusion">Conclusion<a href="#conclusion"></a></h4>
<p>For the full API and examples for how to use it, please see the package documentation for
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a> and
<a href="https://pkg.go.dev/znkr.io/diff/textdiff">znkr.io/diff/textdiff</a>. I am certain that there are use
cases not covered by this API, but I feel confident that it can evolve to cover these use cases in
the future. For now, all my needs are fulfilled, but if you run into a situation that can't be
solved by this API or requires some contortions, please <a href="https://github.com/znkr/diff/issues/new">tell me about
it</a>.</p>
<h3 id="implementation">Implementation<a href="#implementation"></a></h3>
<p>To implement this API, we need to implement a diff algorithm. There are a couple of standard diff
algorithms that we can choose from. The choice of the algorithm as well as how it's implemented
matters for the readability of the result as well as the performance.</p>
<p>A good starting point for this project was Myers' algorithm, simply because it's the fastest
algorithm that can cover the whole API. In particular, the <code>...Func</code> variants for <code>any</code> types
instead of <code>comparable</code> can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.</p>
<p>On the flip side, in the <a href="#existing-diff-libraries">comparison above</a>, it came out as relatively
slow compared to the patience diff algorithm and didn't produce the most readable results. It turns
out, however, that this can be mitigated and almost completely overcome for <code>comparable</code> types using
a combination of preprocessing, heuristics, and post-processing.</p>
<p>I am not going to cover the diff algorithm in detail here. There are a number of excellent articles
on the web that describe it<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>, but I recommend reading the
paper<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup>: All articles I have seen try to keep a distance from the theory that
makes this algorithm work, but that's not really helpful if you want to understand how and why this
algorithm works.</p>
<h4 id="preprocessing">Preprocessing<a href="#preprocessing"></a></h4>
<p>The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size.
The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps
a little. However, it can also reduce diff readability, because it will consume matching elements
eagerly.</p>
<p>For example, let's say we have this change:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05.diff">example_05.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="15">
                <td>11</td>
                <td>15</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are
all identical and the so are the last 4. This in turn would result in a different diff:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05_strip_common_prefix_and_suffix.diff">example_05_strip_common_prefix_and_suffix.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="11">
                <td>11</td>
                <td>11</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="15">
                <td></td>
                <td>15</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>Fortunately, this is easy to fix in post processing.</p>
<p>Much more impactful, but only efficiently possible for <code>comparable</code> types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-<code>comparable</code> types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step <a href="https://github.com/znkr/diff/commit/37b4470eeb45867adcae1581907770041326e1b5">reduced the runtime by up to
99%</a> for a few
real-world worst-case diffs.</p>
<p>In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability
impact.</p>
<h4 id="heuristics">Heuristics<a href="#heuristics"></a></h4>
<p>Another very impactful way to improve the performance is <em>Anchoring</em>. It is based on <a href="https://bramcohen.livejournal.com/73318.html">patience
diff</a>. The word patience is a bit misleading, because
it's too easily associated with having to wait and it doesn't describe the heuristic very well
either. It works by finding elements that are occur exactly once on both the left and the right
side. When we matching up these unique pairs we create a segmentation of the input into smaller
parts that can be analyzed individually. Even better, we're very likely to find matching lines atop
and below such a pair of unique elements. This allows us to shrink the segments by stripping common
prefixes and suffixes. This heuristic <a href="https://github.com/znkr/diff/commit/feb7bda337f269935d80ee18e703e0940f406873">reduced the runtime by up to
95%</a>. Unfortunately,
finding unique elements and matching them up requires a hash map again which means that it can only
be used for <code>comparable</code> types.</p>
<p>There are two more heuristics that are I implemented. They help for non-<code>comparable</code> types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The <em>Good Diagonal</em> heuristic stops searching for a better solution if we found a solution
that's good enough and the <em>Too Expensive</em> heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 to

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>1.5</mn>
      </msup>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^1.5 \, \log \, N)</annotation>
  </semantics>
</math>
.</p>
<p>However, heuristics like this trade diff minimality for performance, this is not always desirable.
Sometimes, a minimal diff is exactly what's required.
<a href="https://pkg.go.dev/znkr.io/diff#Optimal"><code>diff.Optimal</code></a> disables these heuristics to always find a
minimal diff irrespective of the costs.</p>
<h4 id="post-processing">Post-processing<a href="#post-processing"></a></h4>
<p>We established before that a diff algorithm finds one of many possible solutions. Given such a
solution we can discover more solutions by it locally and then selecting the best solution according
to some metric. This is exactly how <a href="https://github.com/mhagger">Michael Haggerty's</a> indentation
heuristic works for text.</p>
<p>For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning
of a diff. For example,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06.diff">example_06.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end
</code></td>
            </tr></tbody>
</table>
<p>has the same meaning as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06_indent_heuristic.diff">example_06_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="5">
                <td>1</td>
                <td>5</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end</code></td>
            </tr></tbody>
</table>
<p>We call edits that can be slid up or down <em>sliders</em>. The question is, how do we select the best
slide? Michael collected human ratings for different sliders of the same diff and used them to
develop a heuristic to match these ratings:
<a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a>.</p>
<p>However, this heuristic only works for text and is tuned towards code instead of prose. I decided to
make it optional. It can be enabled with the
<a href="https://pkg.go.dev/znkr.io/diff/textdiff#IndentHeuristic"><code>textdiff.IndentHeuristic</code></a> option.</p>
<h4 id="diff-representation">Diff Representation<a href="#diff-representation"></a></h4>
<p>The representation used during the execution of the diff algorithm has a surprising impact on the
algorithm performance and result readability. This is not at all obvious, and so it took me a while
to figure out that the best approach is akin to a side-by-side view of a diff: You use two <code>[]bool</code>
slices to represent the left side and the right side respectively: <code>true</code> in the left side slice
represents a deletion and on the right side an insertion. <code>false</code> is a matching element.</p>
<p>This representation has four big advantages: It can be preallocated, the order in which edits are
discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate
other representations from it.</p>
<h2 id="open-questions">Open Questions<a href="#open-questions"></a></h2>
<ul>
<li>What exactly is the reason that two different algorithms produce different results? - I looked
into this question a little, but I haven't found a conclusive answer yet.</li>
</ul>
<h2 id="conclusion-1">Conclusion<a href="#conclusion-1"></a></h2>
<p>Diff algorithms are relatively complicated by themselves, but they pale in comparison  to what's
necessary to provide a high-quality diff library. This article tries to explain what went into
my new diff library, but there's still more that I haven't implemented yet.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inflammation now predicts heart disease more strongly than cholesterol (498 pts)]]></title>
            <link>https://www.empirical.health/blog/inflammation-and-heart-health/</link>
            <guid>45430498</guid>
            <pubDate>Tue, 30 Sep 2025 20:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empirical.health/blog/inflammation-and-heart-health/">https://www.empirical.health/blog/inflammation-and-heart-health/</a>, See on <a href="https://news.ycombinator.com/item?id=45430498">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <div>   <p>Chronic inflammation has long been known to double your risk of heart disease, but prior to now,
inflammation has never been a SMuRF: <strong>s</strong>tandard <strong>m</strong>odifiable <strong>r</strong>isk <strong>f</strong>actor for heart disease.</p>
<p>The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone
measure inflammation (specifically, hs-CRP) via a blood test:</p>
<blockquote>
<p>Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">American College of Cardiology</a></p>
</blockquote>
<p>There were a many interesting bits of evidence that led to this recommendation. The whole <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">article, published in JACC</a>, is worth a read, but this blog post extracts a few of the most interesting parts — or at
least, the parts I thought were most interesting.</p>
<p>
Want to skip ahead and measure your inflammation? Empirical Health's <a href="https://www.empirical.health/product/comprehensive-health-panel?utm_source=blog">advanced heart health panel</a> includes hs-CRP, ApoB, Lp(a), and other critical heart health biomarkers.
</p>
<hr>
<h2 id="inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol"><a href="#inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol">Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol</a></h2>
<p>For decades, LDL cholesterol (or <a href="https://www.empirical.health/blog/apob-blood-test/">ApoB</a>) has been the main focus of cardiovascular risk assessment. But
this chart shows hs-CRP is actually a <em>stronger</em> predictor of heart disease than LDL.</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="407" height="620" src="https://www.empirical.health/_astro/ldl_vs_inflammation.B5MkNi52_Z2655L8.webp"></p>
<p>Why? In some ways, <strong>cholesterol has become a victim of its own success.</strong> We now screen the whole population
for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who
end up having heart attacks have lower cholesterol than they would naturally have. This means most of
the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.</p>
<p>Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true
in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients).
In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.</p>
<p>Of course, other traditional risk factors matter <em>in addition</em> to inflammation: blood pressure, HbA1c or
insulin resistance, eGFR (kidney function), and so on.</p>
<h2 id="what-can-you-actually-do-to-lower-inflammation"><a href="#what-can-you-actually-do-to-lower-inflammation">What can you actually do to lower inflammation?</a></h2>
<p>The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:</p>



































































































































<table><thead><tr><th>Trial Name</th><th>Drug (Class)</th><th>Sample Size (n)</th><th>Population/NYHA Functional Class</th><th>Follow-Up</th><th>Primary Endpoint</th><th>Treatment Outcome</th></tr></thead><tbody><tr><td>ATTACH</td><td>Infliximab (TNF inhibitor)</td><td>150</td><td>NYHA III/IV HF</td><td>7 mo</td><td>Clinical status (composite score)</td><td>No improvement or worsening; deaths highest in high-dose infliximab</td></tr><tr><td>ACCLAIM</td><td>IVIG</td><td>2314</td><td>NYHA II-IV HF</td><td>10.2 mo</td><td>Composite all-cause mortality and CV hospitalization</td><td>No reduction in events; trend toward benefit in NYHA III and IV</td></tr><tr><td>CANTOS</td><td>Canakinumab (anti–IL-1β)</td><td>10,061</td><td>Prior MI; hsCRP ≥2 mg/L</td><td>3.7 y (median)</td><td>Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality</td><td>Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%</td></tr><tr><td>CIRT</td><td>Methotrexate</td><td>4,786</td><td>Stable MI plus CAD</td><td>2.3 y (median)</td><td>CV event rates</td><td>No effect on CV events, inflammation, or lipids</td></tr><tr><td>CLEAR SYNERGY</td><td>Colchicine</td><td>3,056</td><td>Acute MI plus PCI</td><td>22.6 mo</td><td>Death from CV causes, recurrent MI, ischemic stroke</td><td>No significant difference in primary endpoint</td></tr><tr><td>COLCOT</td><td>Colchicine</td><td>4,745</td><td>Acute MI patients</td><td>22.6 mo</td><td>CV event rates</td><td>CV events lower than placebo</td></tr><tr><td>LoDoCo2</td><td>Colchicine</td><td>5,522</td><td>Stable CAD</td><td>28.6 mo</td><td>Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.</td><td>CV events lower than placebo</td></tr><tr><td>GISSI-HF</td><td>Rosuvastatin (statin)</td><td>4,574</td><td>NYHA II-IV HF</td><td>3.9 y</td><td>All-cause mortality and CV hospitalization</td><td>No effect on primary endpoints</td></tr><tr><td>JUPITER</td><td>Rosuvastatin (statin)</td><td>17,802</td><td>No CVD / LDL &lt;130 mg/dL; hsCRP ≥2 mg/L</td><td>1.9 y (median)</td><td>MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death</td><td>Reduced events (HR 0.56–0.69)</td></tr><tr><td>CORONA</td><td>Rosuvastatin (statin)</td><td>5,011</td><td>NYHA II-IV HF; ischemic etiology</td><td>32.8 mo</td><td>CV death, nonfatal MI, nonfatal stroke</td><td>No effect on primary endpoint</td></tr><tr><td>OPT-CHF</td><td>Etanercept (TNF inhibitor)</td><td>1,500</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Death, hospitalization, or worsening HF</td><td>No effect on primary endpoint</td></tr><tr><td>DCMP</td><td>Prednisone (corticosteroid)</td><td>84</td><td>NYHA II-IV HF; biopsy-proven myocarditis</td><td>5.7 and 12.3 mo</td><td>Improvement in LVEF, survival, or combined outcome of death or transplantation</td><td>No significant benefit</td></tr><tr><td>RENEWAL</td><td>Etanercept (TNF inhibitor)</td><td>2,048</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Composite outcome of death or hospitalization</td><td>No effect on primary endpoint</td></tr></tbody></table>
<p><strong>What works</strong> to lower inflammation?</p>
<ul>
<li><strong>Statins</strong> (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).</li>
<li><strong>Colchicine</strong>: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).</li>
<li><strong>Canakinumab</strong>: Reduces events but is expensive and increases infection risk (CANTOS).</li>
<li><strong>Lifestyle</strong>: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.</li>
</ul>
<p><strong>What doesn’t work?</strong></p>
<ul>
<li>Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.</li>
</ul>
<hr>
<h2 id="whats-a-normal-good-or-bad-hs-crp"><a href="#whats-a-normal-good-or-bad-hs-crp">What’s a normal, good, or bad hs-CRP?</a></h2>
<p>If you’ve already measured your hs-CRP (great!), then it’s ideally below &lt;1 mg/L. hs-CRP above 3 mg/L is
high risk:</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="638" height="456" src="https://www.empirical.health/_astro/normal-inflammation-levels.BflUT68q_2dks4G.webp"></p>
<p>(If you’re in moderate or high ranges, see the section above for what to do.)</p>
<h2 id="are-other-biomarkers-of-inflammation-relevant"><a href="#are-other-biomarkers-of-inflammation-relevant">Are other biomarkers of inflammation relevant?</a></h2>
<p>The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A.
These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.</p>
<p>In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.</p>
<h2 id="other-interesting-bits"><a href="#other-interesting-bits">Other interesting bits</a></h2>
<p>The JACC article is packed with other interesting insights. These ones were interesting:</p>
<ul>
<li><strong>Imaging biomarkers</strong> (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.</li>
<li><strong>Bempedoic acid</strong> is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.</li>
<li><strong>Residual inflammatory risk</strong>: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.</li>
<li><strong>Universal hs-CRP screening</strong> is now recommended by the ACC for both people with and without established heart disease.</li>
<li><strong>Colchicine (0.5 mg/d)</strong> is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.</li>
<li><strong>Novel IL-6 inhibitors</strong> are being studied as future anti-inflammatory therapies for heart disease.</li>
</ul>
<h2 id="how-to-measure-your-inflammation"><a href="#how-to-measure-your-inflammation">How to measure your inflammation</a></h2>
<p>A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).</p>
  </div> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making sure AI serves people and knowledge stays human (112 pts)]]></title>
            <link>https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</link>
            <guid>45430048</guid>
            <pubDate>Tue, 30 Sep 2025 19:23:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/">https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</a>, See on <a href="https://news.ycombinator.com/item?id=45430048">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-174601">
	<div>
			


<div><p>At the Wikimedia Foundation, <a href="https://diff.wikimedia.org/2021/12/09/what-the-wikimedia-foundations-new-human-rights-policy-means-for-our-movement/">we believe</a> that access to knowledge is a human right. Our mission is to ensure everyone, everywhere can access and share reliable information freely and openly on Wikipedia and other Wikimedia projects. Access to free and open knowledge, supported by the fundamental right to freedom of expression, empowers people to exercise many other rights enshrined in the <a href="https://www.un.org/en/about-us/universal-declaration-of-human-rights">Universal Declaration of Human Rights</a>, including the rights to education, artistic expression, economic advancement, and political participation. Today, we are sharing <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Artificial_Intelligence_and_Machine_Learning_Human_Rights_Impact_Assessment">a human rights impact assessment (HRIA) on artificial intelligence (AI) and machine learning (ML)</a> that was carried out in 2024 to help the Foundation and Wikimedia volunteer communities better understand how these technologies may affect the exercise of human rights in our ecosystem.</p><p>Wikimedia projects, and Wikimedia volunteers everywhere, occupy a unique space in today’s online information ecosystem. This ecosystem is, however, rapidly evolving. The introduction and rapid advancement of emerging technologies such as large language models (LLMs) and other kinds of generative AI introduce opportunities as well as challenges related to the creation, access, and distribution of information. Generative AI is fundamentally changing how the public seeks, receives, and imparts information and ideas online, raising novel questions about the role and responsibility of the Foundation and Wikimedia volunteer communities in this ecosystem.&nbsp;</p><p>AI and ML are neither new to Wikimedia projects nor to the Wikimedia volunteers who make these projects possible. Both the Foundation and volunteer communities have developed numerous ML tools to support volunteers in contributing, editing, and curating the ever-growing volume of knowledge across the projects as far back as 2010. Several of these tools have harnessed ML and AI to assist volunteers with frequently recurring tasks such as identifying vandalism or flagging when citations are needed. Most tools currently used were developed before the introduction of generative AI. In the age of these emerging technologies, Wikimedia volunteers are contending with new questions:</p></div>



<ul>
<li>What, if any, role should AI play in terms of the knowledge shared on Wikimedia projects?</li>



<li>Given the widespread use of generative AI on the internet, how can we protect and strengthen the accuracy and integrity of knowledge on the Wikimedia projects?&nbsp;</li>



<li>How can ML and AI tools help strengthen, not replace, what humans do best: creating, cultivating, and sharing free knowledge?</li>



<li>How can LLMs and AI tools be used to translate content into new languages, while preserving reliability and cultural nuance and context?</li>



<li>How should the volunteer communities’ policies evolve to account for such uses of these new technologies?</li>
</ul>



<h2><strong><strong><strong>About the AI/ML Human Rights Impact Assessment (HRIA)</strong></strong></strong></h2>



<p>This HRIA is the latest outcome of the Foundation’s ongoing efforts to meet our <a href="https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Human_Rights_Policy">commitment</a> to protect and uphold the human rights of all those who interact with Wikimedia projects. The Foundation commissioned it to identify and analyze the impacts, opportunities, and risks emanating from the use of AI and ML technologies in the Wikimedia ecosystem. The report was written and compiled by <a href="https://taraazresearch.org/">Taraaz Research</a>, a specialized research and advocacy organization working at the intersection of technology and human rights. In developing the report, Taraaz consulted Foundation staff, individual volunteers, volunteer affiliates, civil society organizations, and external subject matter experts, though the report does not represent the views or shared consensus of any of these groups. Instead, the report offers suggestions for further inquiry, policy, and technology investment based on the state of the Wikimedia projects and technology from October 2023 to August 2024 when the research was conducted. Furthermore, the findings in the report represent potential areas of risk and opportunity. The report does not identify any actual observed risks, harms, opportunities, or benefits that have resulted from the use of ML or AI technologies on Wikimedia projects.</p>



<h2><strong><strong><strong>What are the findings of this report?</strong></strong></strong></h2>



<div><p>This report considered risks emanating from three different categories of issues relating to AI/ML on Wikimedia projects: tools developed in-house by Foundation staff to support the work of volunteer editors; Generative AI (GenAI) and its potential for marginal human rights risks in the Wikimedia context; and content on Wikimedia projects that may be used for external machine learning (ML) development.</p><p>It is important to note that the findings contained in this report reflect potential harms that could occur in the future. The report does not find that any such harms have occurred. Rather, it explains that these harms could occur if AI is employed or leveraged at scale in certain ways on Wikimedia projects without proper mitigations in place.</p><p>The report found that AI/ML tools developed by the Foundation to support volunteer editors have the potential to contribute positively to several human rights, such as freedom of expression and the right to education, among others. Nonetheless, certain risks exist that stem from known limitations of AI/ML-enabled tools: for example, the possibilities of perpetuating or amplifying existing gaps and biases in knowledge representation or incorrectly flagging or marking content for deletion. Such risks, if they were to materialize at scale, could have negative impacts on the human rights of Wikimedia volunteers.</p><p>Furthermore, the report considered in broad terms what new risks external GenAI tools could introduce to Wikimedia projects. The researchers determined that GenAI could increase the scale, speed, and sophistication of harmful content generation, including for disinformation campaigns and to attack individual Wikimedia volunteers or their communities. These tools could also automate the creation of misleading content across multiple languages simultaneously, making its detection and moderation more challenging, and play a role in generating large volumes of personalized, abusive content targeting specific individuals or communities. These risks, among others identified, could negatively affect the human rights of Wikimedia volunteers and, even, the general public if not properly mitigated.</p><p>Finally, the report considered the downstream risks of how content from Wikimedia projects are used in the training of large language models (LLMs). While the Wikimedia Foundation cannot control how freely and openly licensed content from the Wikimedia projects is used by the general public, we do have a duty to safeguard risks to human rights that could result from downstream impacts. The researchers identified concerns about how the outputs of LLMs partially trained on Wikimedia content could represent risk in terms of bias and representation, data quality and accuracy, privacy risks, and issues related to cultural sensitivity. As such, they recommended monitoring for these potential risks, although they also found that ongoing data-quality initiatives and equity-focused programs already mitigate the risks in question, since these programs address content and representation gaps across language communities.</p><p>Within each of these focus areas, the report notes that the Foundation and Wikimedia volunteer communities have also already implemented many strategies and processes to mitigate the identified risks while providing recommendations for additional mitigation measures as well. Given the prominence of Wikimedia projects in the online information ecosystem, it is critical that we consider new risks emerging from technologies as rapidly evolving and growing as AI and ML. Importantly, the discussions and conclusions in this report allow us to contemplate such potential harms early and to plan how we can best mitigate them proactively.</p></div>



<h2><strong><strong><strong>What does this HRIA report mean for the Wikimedia projects and volunteer communities?</strong></strong></strong></h2>



<div><p>Since we <a href="https://diff.wikimedia.org/2022/07/12/what-does-the-wikimedia-foundations-human-rights-impact-assessment-mean-for-the-wikimedia-movement/">published</a> our first HRIA in July 2022, the Foundation has been clear that implementing many of these reports’ recommendations requires the buy-in and collaboration of the global volunteer communities. It will take time to discuss this HRIA’s findings and recommendations with the volunteer communities in order to decide how best to work together on their implementation, but our actions will be more effective for having done so.</p><p>We are publishing this HRIA report to help the Foundation and volunteer communities explore and address the profound societal impacts that might come from the interaction of AI technologies and the Wikimedia projects in the coming years. Wikimedia communities around the world are already grappling with important decisions about how to establish clear policies for appropriate use of generative AI on the projects, or whether any such uses even exist. We hope that considering the risks and opportunities identified in this report will help guide community discussions and decisions to make sure that the projects can continue to contribute positively to the online information ecosystem and our global society.&nbsp;</p></div>



<h2><strong><strong><strong>How can Wikimedians learn more and give feedback?</strong></strong></strong></h2>



<div><p>We want to hear from you! What questions do you have? What are your thoughts on the risks and recommendations discussed in the report? What is your community already doing, or what would you like to do, to responsibly harness the benefits of AI and ML on Wikimedia projects?</p><p>Over the coming months, we will create opportunities to hear directly from your communities and you about the findings and recommendations of this report as well as your perspectives on the opportunities and risks associated with AI and ML in the Wikimedia ecosystem. You can already leave your thoughts and comments <a href="https://meta.wikimedia.org/w/index.php?title=Talk:Wikimedia_Foundation_Artificial_Intelligence_and_Machine_Learning_Human_Rights_Impact_Assessment">on the HRIA’s Talk page</a> or join us at one of the following conversations on this topic:</p></div>



<ul>
<li><a href="https://wikimedia.zoom.us/webinar/register/WN_9qwyMXETTwugi0R9uhqvEw">21 November (12:00 UTC): Global Advocacy Community Conversation Hour</a></li>



<li><a href="https://wikimedia.zoom.us/webinar/register/WN_vmFR3DM4QqqMF6I6fhLHxA">21 November (17:00 UTC): Global Advocacy Community Conversation Hour </a></li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2025%2F09%2F30%2Fmaking-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing has started working on a 737 MAX replacement (241 pts)]]></title>
            <link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link>
            <guid>45428482</guid>
            <pubDate>Tue, 30 Sep 2025 17:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df">https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</a>, See on <a href="https://news.ycombinator.com/item?id=45428482">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (270 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45428122</guid>
            <pubDate>Tue, 30 Sep 2025 17:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45428122">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (724 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45427982</guid>
            <pubDate>Tue, 30 Sep 2025 16:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45427982">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Sculptor, the Missing UI for Claude Code (151 pts)]]></title>
            <link>https://imbue.com/sculptor/</link>
            <guid>45427697</guid>
            <pubDate>Tue, 30 Sep 2025 16:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imbue.com/sculptor/">https://imbue.com/sculptor/</a>, See on <a href="https://news.ycombinator.com/item?id=45427697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><header><a href="https://imbue.com/"><img src="data:image/svg+xml;base64,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" alt="Imbue logo"></a><nav><a aria-current="page" href="https://imbue.com/sculptor/"><p>Sculptor</p></a><a href="https://imbue.com/our-work/"><p>Our Work</p></a><a href="https://imbue.com/company/"><p>Company</p></a><a href="https://imbue.com/careers/"><p>Careers</p></a><a href="https://imbue.com/blog/"><p>Blog</p></a></nav></header><div><hr color="pride"><div><div><p><img src="https://imbue.com/static/parallel-agents-ac8d6f0192da6b575005401e71bbe66a.png" alt="Spin up parallel agents"></p><h3>Spin up parallel agents</h3><p>Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.</p></div><div><p><img src="https://imbue.com/static/instantly-pair-d0b4bc629d9146f1be7d0930e8200813.png" alt="Instantly test agent changes"></p><h3>Instantly test agent changes</h3><p>Switch seamlessly between agents with Pairing Mode.</p></div><div><p><img src="https://imbue.com/static/merge-f3c7f16c8450713cf46eed9700091655.png" alt="Merge without conflicts"></p><h3>Merge without conflicts</h3><p>Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.</p></div></div></div><div><div><div><hr color="envy"><div><p>I've been moving more and more of my coding off of Cursor and on to Sculptor btw.</p>
<p>The vibes are good, and the experience has been pretty nice.</p></div></div><div><hr color="peace"><p>Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.</p></div></div><div><div><hr color="pride"><p>Wow, this is slick!!</p><div><p>Dale Hamilton</p><p>Staff Engineer, Universe</p></div></div><div><hr color="excitement"><div><p>At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.</p>
<p>It's like—oh wow I don't have to manage that mess anymore.</p></div><div><p>Patrick Sulin</p><p>Technical Lead, InterSystems</p></div></div></div><div><div><hr color="joy"><p>The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.</p><div><p>Robert Starmer</p><p>CEO, Kumulus Technologies</p></div></div><div><hr color="sadness"><p>I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.</p><div><p>Andrew Gabriel</p><p>Founder, Monostate.ai</p></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction (190 pts)]]></title>
            <link>https://arxiv.org/abs/2509.22906</link>
            <guid>45427634</guid>
            <pubDate>Tue, 30 Sep 2025 16:31:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.22906">https://arxiv.org/abs/2509.22906</a>, See on <a href="https://news.ycombinator.com/item?id=45427634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.22906">View PDF</a>
    <a href="https://arxiv.org/html/2509.22906v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Henrique Godoy [<a href="https://arxiv.org/show-email/90d16046/2509.22906" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 26 Sep 2025 20:34:43 UTC (1,102 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Airweave (YC X25) – Let agents search any app (151 pts)]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>45427482</guid>
            <pubDate>Tue, 30 Sep 2025 16:21:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/airweave-ai/airweave">https://github.com/airweave-ai/airweave</a>, See on <a href="https://news.ycombinator.com/item?id=45427482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-darkbg.svg">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-lightbg.svg">
  <img width="1673" alt="airweave-lettermark" src="https://github.com/airweave-ai/airweave/raw/main/frontend/public/logo-airweave-darkbg.svg">
</picture></themed-picture>

<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>Airweave is a tool that lets agents search any app.</strong> It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.</p>
<p dir="auto">The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#airweave">Airweave</a>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#-quick-start">🚀 Quick Start</a></li>
<li><a href="#-supported-integrations">🔌 Supported Integrations</a></li>
<li><a href="#-usage">💻 Usage</a>
<ul dir="auto">
<li><a href="#frontend">Frontend</a></li>
<li><a href="#api">API</a></li>
</ul>
</li>
<li><a href="#-sdks">📦 SDKs</a>
<ul dir="auto">
<li><a href="#python">Python</a></li>
<li><a href="#typescriptjavascript">TypeScript/JavaScript</a></li>
</ul>
</li>
<li><a href="#-key-features">🔑 Key Features</a></li>
<li><a href="#-technology-stack">🔧 Technology Stack</a></li>
<li><a href="#-contributing">👥 Contributing</a></li>
<li><a href="#-license">📄 License</a></li>
<li><a href="#-connect">🔗 Connect</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Managed Service: <a href="https://app.airweave.ai/" rel="nofollow">Airweave Cloud</a></h3><a id="user-content-managed-service-airweave-cloud" aria-label="Permalink: Managed Service: Airweave Cloud" href="#managed-service-airweave-cloud"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-hosted:</h3><a id="user-content-self-hosted" aria-label="Permalink: Self-hosted:" href="#self-hosted"></a></p>
<p dir="auto">Make sure docker and docker-compose are installed, then...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh"><pre><span><span>#</span> 1. Clone the repository</span>
git clone https://github.com/airweave-ai/airweave.git
<span>cd</span> airweave

<span><span>#</span> 2. Build and run</span>
chmod +x start.sh
./start.sh</pre></div>
<p dir="auto">That's it! Access the dashboard at <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Supported Integrations</h2><a id="user-content--supported-integrations" aria-label="Permalink: 🔌 Supported Integrations" href="#-supported-integrations"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/asana.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/asana.svg" alt="Asana" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/bitbucket.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/bitbucket.svg" alt="Bitbucket" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/confluence.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/confluence.svg" alt="Confluence" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/dropbox.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/dropbox.svg" alt="Dropbox" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/github.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/github.svg" alt="Github" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/gmail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/gmail.svg" alt="Gmail" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_calendar.svg" alt="Google Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/google_drive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/google_drive.svg" alt="Google Drive" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/hubspot.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/hubspot.svg" alt="Hubspot" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/jira.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/jira.svg" alt="Jira" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/linear.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/linear.svg" alt="Linear" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/monday.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/monday.svg" alt="Monday" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/notion.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/notion.svg" alt="Notion" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/onedrive.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/onedrive.svg" alt="Onedrive" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_calendar.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_calendar.svg" alt="Outlook Calendar" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/outlook_mail.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/outlook_mail.svg" alt="Outlook Mail" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/postgresql.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/postgresql.svg" alt="Postgresql" width="40" height="40"></a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/slack.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/slack.svg" alt="Slack" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/stripe.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/stripe.svg" alt="Stripe" width="40" height="40"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/airweave-ai/airweave/blob/main/frontend/src/components/icons/apps/todoist.svg"><img src="https://github.com/airweave-ai/airweave/raw/main/frontend/src/components/icons/apps/todoist.svg" alt="Todoist" width="40" height="40"></a>
  </p>


<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Usage</h2><a id="user-content--usage" aria-label="Permalink: 💻 Usage" href="#-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Frontend</h3><a id="user-content-frontend" aria-label="Permalink: Frontend" href="#frontend"></a></p>
<ul dir="auto">
<li>Access the UI at <code>http://localhost:8080</code></li>
<li>Connect sources, configure syncs, and query data</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">API</h3><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<ul dir="auto">
<li>Swagger docs: <code>http://localhost:8001/docs</code></li>
<li>Create connections, trigger syncs, and search data</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 SDKs</h2><a id="user-content--sdks" aria-label="Permalink: 📦 SDKs" href="#-sdks"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Python</h3><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)
client.collections.create(
    name=&quot;name&quot;,
)"><pre><span>from</span> <span>airweave</span> <span>import</span> <span>AirweaveSDK</span>

<span>client</span> <span>=</span> <span>AirweaveSDK</span>(
    <span>api_key</span><span>=</span><span>"YOUR_API_KEY"</span>,
    <span>base_url</span><span>=</span><span>"http://localhost:8001"</span>
)
<span>client</span>.<span>collections</span>.<span>create</span>(
    <span>name</span><span>=</span><span>"name"</span>,
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">TypeScript/JavaScript</h3><a id="user-content-typescriptjavascript" aria-label="Permalink: TypeScript/JavaScript" href="#typescriptjavascript"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install @airweave/sdk
# or
yarn add @airweave/sdk"><pre>npm install @airweave/sdk
<span><span>#</span> or</span>
yarn add @airweave/sdk</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: &quot;name&quot;,
});"><pre><span>import</span> <span>{</span> <span>AirweaveSDKClient</span><span>,</span> <span>AirweaveSDKEnvironment</span> <span>}</span> <span>from</span> <span>"@airweave/sdk"</span><span>;</span>

<span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>AirweaveSDKClient</span><span>(</span><span>{</span>
    <span>apiKey</span>: <span>"YOUR_API_KEY"</span><span>,</span>
    <span>environment</span>: <span>AirweaveSDKEnvironment</span><span>.</span><span>Local</span>
<span>}</span><span>)</span><span>;</span>
<span>await</span> <span>client</span><span>.</span><span>collections</span><span>.</span><span>create</span><span>(</span><span>{</span>
    <span>name</span>: <span>"name"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔑 Key Features</h2><a id="user-content--key-features" aria-label="Permalink: 🔑 Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Data synchronization</strong> from 25+ sources with minimal config</li>
<li><strong>Entity extraction</strong> and transformation pipeline</li>
<li><strong>Multi-tenant</strong> architecture with OAuth2</li>
<li><strong>Incremental updates</strong> using content hashing</li>
<li><strong>Semantic search</strong> for agent queries</li>
<li><strong>Versioning</strong> for data changes</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Tech Stack</h2><a id="user-content--tech-stack" aria-label="Permalink: 🔧 Tech Stack" href="#-tech-stack"></a></p>
<ul dir="auto">
<li><strong>Frontend</strong>: React/TypeScript with ShadCN</li>
<li><strong>Backend</strong>: FastAPI (Python)</li>
<li><strong>Databases</strong>: PostgreSQL (metadata), Qdrant (vectors)</li>
<li><strong>Deployment</strong>: Docker Compose (dev), Kubernetes (prod)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👥 Contributing" href="#-contributing"></a></p>
<p dir="auto">We welcome contributions! Please check <a href="https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">Airweave is released under the <a href="https://github.com/airweave-ai/airweave/blob/main/LICENSE">MIT</a> license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 Connect</h2><a id="user-content--connect" aria-label="Permalink: 🔗 Connect" href="#-connect"></a></p>
<ul dir="auto">
<li><strong><a href="https://discord.com/invite/484HY9Ehxt" rel="nofollow">Discord</a></strong> - Get help and discuss features</li>
<li><strong><a href="https://github.com/airweave-ai/airweave/issues">GitHub Issues</a></strong> - Report bugs or request features</li>
<li><strong><a href="https://x.com/airweave_ai" rel="nofollow">Twitter</a></strong> - Follow for updates</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked Apple M5 9 core Geekbench scores (257 pts)]]></title>
            <link>https://browser.geekbench.com/v6/cpu/14173685</link>
            <guid>45427197</guid>
            <pubDate>Tue, 30 Sep 2025 16:00:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.geekbench.com/v6/cpu/14173685">https://browser.geekbench.com/v6/cpu/14173685</a>, See on <a href="https://news.ycombinator.com/item?id=45427197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p>4133</p>
<p>Single-Core Score</p>
</div>
<div>
<p>15437</p>
<p>Multi-Core Score</p>
</div>
<p>
Geekbench 6.5.0 for iOS AArch64
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cerebras systems raises $1.1B Series G (121 pts)]]></title>
            <link>https://www.cerebras.ai/press-release/series-g</link>
            <guid>45427111</guid>
            <pubDate>Tue, 30 Sep 2025 15:54:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cerebras.ai/press-release/series-g">https://www.cerebras.ai/press-release/series-g</a>, See on <a href="https://news.ycombinator.com/item?id=45427111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slicetype="articleText" data-sanity="id=cbc365b9-f2ce-421e-971f-addf7e5f717b;type=pressRelease;path=slices;base=https%3A%2F%2Fwww.cerebras.ai"><p><em>Fidelity Management &amp; Research Company Anchors Investment with an All-Star Consortium of Investors</em></p><p>Sunnyvale, CA – September 30, 2025 – Cerebras Systems, makers of the fastest AI infrastructure in the industry, today announced the completion of an over subscribed$1.1 billion Series G funding round at $8.1 billion post-money valuation. The round was led by Fidelity Management &amp; Research Company and Atreides Management. The round included significant participation from Tiger Global, Valor Equity Partners, and 1789 Capital, as well as existing investors Altimeter, Alpha Wave, and Benchmark.</p><p>As the fastest inference provider in the world, Cerebras will use these funds to expand its pioneering technology portfolio with continued inventions in AI processor design, packaging, system design and AI supercomputers. In addition, it will expand its U.S. manufacturing capacity and its U.S. data center capacity to keep pace with the explosive demand for Cerebras products and services.</p><p>"From our inception we have been backed by the most knowledgeable investors in the industry. They have seen the historic opportunity that is AI and have chosen to invest in Cerebras,” said Andrew Feldman, co-founder and CEO, Cerebras. "We are proud to expand our consortium of best-in-world investors.”</p><h3>Inference Momentum as AI Industry Leaders Choose Cerebras</h3><p>Cerebras has experienced extraordinary growth since launching its inference service in late 2024. Over the past year, Cerebras has held the performance crown every single day, routinely demonstrating speeds more than 20X faster than Nvidia GPUs on open-source and closed source models.</p><p>"Since our founding, we have tested every AI inference provider across hundreds of models. Cerebras is consistently the fastest,” said Micah Hill-Smith, CEO of leading benchmarking firm Artificial Analysis.</p><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><p>Cerebras’ performance advantage has led to massive demand. New real-time use cases – including code generation, reasoning, and agentic work – have increased the benefits from speed and the increased the cost of being slow, driving customers to Cerebras. Today, Cerebras is serving trillions of tokens per month, in its own cloud, on its customers premises, and across leading partner platforms.</p><p>In 2025, AI leaders including AWS, Meta, IBM, Mistral, Cognition, AlphaSense, Notion and hundreds more have chosen Cerebras, joining enterprises and governments, including GlaxoSmithKline, Mayo Clinic, the US Department of Energy, the US Department of Defense. Individual developers have also chosen Cerebras for their AI work. On Hugging Face, the leading AI hub for developers, Cerebras is the #1 inference provider with over 5 million monthly requests.</p><p>Citigroup and Barclays Capital acted as joint placement agents for the transaction.</p><h3>About Cerebras Systems</h3><p>Cerebras Systems builds the fastest AI infrastructure in the world. We are a team of pioneering computer architects, computer scientists, AI researchers, and engineers of all types. We have come together to make AI blisteringly fast through innovation and invention because we believe that when AI is fast it will change the world. Our flagship technology, the Wafer Scale Engine 3 (WSE-3) is the world’s largest and fastest AI processor.56 times larger than the largest GPU, the WSE uses a fraction of the power per unit compute while delivering inference and training more than 20 times faster than the competition.Leading corporations, research institutes and governments on four continents chose Cerebras to run their AI workloads. Cerebras solutions are available on premise and in the cloud, for further information, visit cerebras.ai or follow us on LinkedIn, X and/or Threads.</p><p>Media Contact</p><p><a href="mailto:PR@zmcommunications.com">PR@zmcommunications.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizations of Random Attractors Found Using Lyapunov Exponents (119 pts)]]></title>
            <link>https://paulbourke.net/fractals/lyapunov/</link>
            <guid>45427059</guid>
            <pubDate>Tue, 30 Sep 2025 15:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulbourke.net/fractals/lyapunov/">https://paulbourke.net/fractals/lyapunov/</a>, See on <a href="https://news.ycombinator.com/item?id=45427059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<center>

Written by <a href="https://paulbourke.net/fractals/">Paul Bourke</a><br>
October 2001<p>
Contribution by Philip Ham: <a href="https://paulbourke.net/fractals/lyapunov/attractor.basic">attractor.basic</a><br>
and <a href="https://paulbourke.net/fractals/lyapunov/fractalgan.py">Python implementation</a> by Johan Bichel Lindegaard.</p><p>
This document is "littered" with a
selection of attractors found using the techniques described.
</p></center>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/1.jpg" width="800" height="800"></center>
<p>
In order for a system to exhibit chaotic behaviour it must be non linear.
Representing chaotic systems on a screen or on paper leads one to 
considering a two dimensional system, an equation in two variables. 
One possible two dimensional non-linear system, the one used
here, is the quadratic map defined as follows.
</p>

<center>
x<sub>n+1</sub> = 
a<sub>0</sub> + 
a<sub>1</sub> x<sub>n</sub> +
a<sub>2</sub> x<sub>n</sub><sup>2</sup> +
a<sub>3</sub> x<sub>n</sub> y<sub>n</sub> +
a<sub>4</sub> y<sub>n</sub> +
a<sub>5</sub> y<sub>n</sub><sup>2</sup>
<br>
y<sub>n+1</sub> =
b<sub>0</sub> + 
b<sub>1</sub> x<sub>n</sub> +
b<sub>2</sub> x<sub>n</sub><sup>2</sup> +
b<sub>3</sub> x<sub>n</sub> y<sub>n</sub> +
b<sub>4</sub> y<sub>n</sub> +
b<sub>5</sub> y<sub>n</sub><sup>2</sup>
</center>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/2.jpg" width="800" height="800"></center>

<p>
The standard measure for determining whether or not a system is chaotic
is the Lyapunov exponent, normally represented by the lambda symbol.
Consider two close points at step n, x<sub>n</sub> and
x<sub>n</sub>+dx<sub>n</sub>. At the next time step they
will have diverged, namely to x<sub>n+1</sub> and
x<sub>n+1</sub>+dx<sub>n+1</sub>. It is this average
rate of divergence (or convergence) that the Lyapunov exponent captures.
Another way to think about the Lyapunov exponent is as the rate at
which information about the initial conditions is lost.
</p>

<center>
<img src="https://paulbourke.net/fractals/lyapunov/diagram.gif" width="400" height="456"></center>

<p>
There are as many Lyapunov exponents as dimensions of the phase space.
Considering a region (circle, sphere, hypersphere, etc) in phase space
then at a later time all trajectories in this region form an
n-dimensional elliptical region. The Lyapunov exponent can be calculated for
each dimension.
When talking about a single exponent one is normally referring to the
largest, this convention will be assumed from now onwards.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/3.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is positive then the system is chaotic and unstable.
Nearby points will diverge irrespective of how close they are. Although there
is no order the system is still deterministic!
The magnitude of the Lyapunov exponent
is a measure of the sensitivity to initial conditions, the primary
characteristic of a chaotic system.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/4.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is less than zero then the system attracts to
a fixed point or stable periodic orbit. These systems are non conservative
(dissipative). The absolute value of the exponent indicates the degree
of stability.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/5.jpg" width="800" height="800"></center>

<p>
If the Lyapunov exponent is zero then the system is neutrally stable, such
systems are conservative and in a steady state mode.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/6.jpg" width="800" height="800"></center>
<p>
To create the chaotic attractors shown on this page
each parameter a<sub>n</sub> and b<sub>n</sub> in the quadratic
equation above is chosen at random between some bounds (+- 2 say). 
The system so specified is generated
by iterating for some suitably large number of time steps
(eg; 100000) steps during which time the image is created
and the Lyapunov exponent computed. Note that the first few (1000) timesteps
are ignored to allow the system to settle into its "natural" behaviour.
If the Lyapunov exponent indicates
chaos then the image is saved and the program moves on to the
next random parameter set. 
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/7.jpg" width="800" height="800"></center>

<p>
There are a number of ways the series may behave.
</p>
<ul>
<li><p>
It may converge to a single point, called a fixed point. These
can be detected by comparing the distances between successive
points. For numerical reasons this is safer than relying on the
Lyapunov exponent which may be infinite (logarithm of 0)</p>
</li><li><p>
It may diverge to infinity, for the range (+- 2) used here for
each parameter this is the most likely event. These are also easy
to detect and discard, indeed they need to be in order to avoid
numerical errors.</p>
</li><li><p>
It will form a periodic orbit, these are identified by their 
negative Lyapunov exponent.
</p>
</li><li><p>
It will exhibit chaos, filling in some region of the plane.
These are the solutions that "look good" and the ones we wish to
identify with the Lyapunov exponent.
</p>
</li></ul>
<center>
	<img src="https://paulbourke.net/fractals/lyapunov/8.jpg" width="800" height="800"></center>

<p>
It should be noted that there may be visually appealing structures
that are not chaotic attractors. That is, the resulting image is different
for different initial conditions and there is no single basin of attraction.
It's interesting how we "see" 3 dimensional structures in these essentially
2 dimensional systems.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/9.jpg" width="800" height="800"></center>
<p>
The software used to create these images is given here:
<a href="https://paulbourke.net/fractals/lyapunov/gen.c">gen.c</a>. On average 98% of the random
selections of (a<sub>n</sub>, b<sub>n</sub>) result
in an infinite series. This is so common because of
the range (-2 &lt;= a,b &lt;= 2) for each of the parameters
a and b, the number of infinite cases will reduce greatly with
a smaller range.
About 1% were point attractors, and
about 0.5% were periodic basins of attraction.
</p>

<center>
	<img src="https://paulbourke.net/fractals/lyapunov/10.jpg" width="800" height="800"></center><center>
<p>
<a href="https://paulbourke.net/fractals/lyapunov/shuttle.jpg"><img src="https://paulbourke.net/fractals/lyapunov/shuttle.jpg" width="800" height="600"></a><br>
<span size="-1">
<p>
Image courtesy of Robert McGregor, Space Coast of Florida.
Launch trail perhaps 30 minutes after the shuttle launch (June 2007)
dispersing from a column into a smoke ring 
due to some unusual air currents in the upper atmosphere.
</p>
</span>
</p></center>

<center>
   <img src="https://paulbourke.net/fractals/lyapunov/11.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/12.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/13.jpg" width="800" height="800"></center><center>
   <img src="https://paulbourke.net/fractals/lyapunov/14.jpg" width="800" height="800"></center><p>

<b>References</b></p><p>

Berge, P., Pomeau, Y., Vidal, C.,<br>
Order Within Chaos, Wiley, New York, 1984.
</p><p>

Crutchfield, J., Farmer, J., Packard, N.<br>
Chaos, Scientific American, 1986, 255, 46-47
</p><p>

Das, A., Das, Pritha, Roy, A<br>
Applicability of Lyapunov Exponent in EEG data analysis. 
Complexity International, draft manuscript.
</p><p>

Devaney, R.<br>
An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989
</p><p>

Feigenbaum, M.,<br>
Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981
</p><p>

Peitgen, H., Jurgens, H., Saupe, D<br>
Lyapunov exponents and chaotic attractors in Chaos and fractals
- new frontiers of science. Springer, new York, 1992.
</p><p>


<b>Contributions by Dmytry Lavrov</b>
</p><center>
<img src="https://paulbourke.net/fractals/lyapunov/explore1.jpg" width="800" height="600"><p>
<img src="https://paulbourke.net/fractals/lyapunov/explore2.jpg" width="800" height="600"></p></center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Correctness and composability bugs in the Julia ecosystem (2022) (103 pts)]]></title>
            <link>https://yuri.is/not-julia/</link>
            <guid>45427021</guid>
            <pubDate>Tue, 30 Sep 2025 15:46:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yuri.is/not-julia/">https://yuri.is/not-julia/</a>, See on <a href="https://news.ycombinator.com/item?id=45427021">Hacker News</a></p>
<div id="readability-page-1" class="page">








<p>For many years I used the <a href="https://julialang.org/">Julia</a> programming language for transforming, cleaning, analyzing, and visualizing data, doing statistics, and performing simulations.</p>

<p>I published a handful of open-source packages for things like <a href="https://github.com/JuliaGraphics/SignedDistanceFields.jl">signed distance fields</a>, <a href="https://github.com/yurivish/LowDimNearestNeighbors.jl">nearest-neighbor search</a>, and <a href="https://github.com/yurivish/TuringPatterns.jl">Turing patterns</a> (<a href="https://github.com/JuliaWeb/Hyperscript.jl">among</a> <a href="https://github.com/yurivish/Treaps.jl">others</a>), made visual explanations of Julia concepts like <a href="https://julia-guide.netlify.app/broadcasting">broadcasting</a> and <a href="https://observablehq.com/@yurivish/julia-array-notation">arrays</a>, and used Julia to make the generative art on my <a href="https://yuri.is/cardcrafting">business cards</a>.</p>

<p>I stopped using Julia a while ago, but it still sometimes comes up. When people ask, I tell them that I can no longer recommend it. I thought I’d write up my reasons why.</p>

<p>My conclusion after using Julia for many years is that there are too many correctness and composability bugs throughout the ecosystem to justify using it in just about any context where correctness matters. </p>

<p>In my experience, Julia and its packages have the highest rate of serious correctness bugs of any programming system I’ve used, and I started programming with Visual Basic 6 in the mid-2000s. </p>

<p>It might be useful to give some concrete examples.</p>

<p>Here are some correctness issues I filed:</p>

<ul>
<li><a href="https://github.com/JuliaStats/Distributions.jl/issues/1241">Sampling a probability density produces an incorrect result</a></li>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/issues/642">Sampling an array can produce biased results</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39183">The product function can produce incorrect results for 8-bit, 16-bit, and 32-bit integers</a></li>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/issues/616">Fitting a histogram to a Float64 array can produce incorrect results</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39385">Base functions sum!, prod!, any!, and all! may silently return incorrect results</a></li>
</ul>

<p>Here are comparable issues filed by others:</p>

<ul>
<li><a href="https://github.com/JuliaStats/StatsBase.jl/pull/632">Summarystats returns NaN quantiles for arrays with mean 0</a></li>
<li><a href="https://github.com/JuliaCollections/OrderedCollections.jl/issues/71">OrderedDict can corrupt keys</a></li>
<li><a href="https://github.com/JuliaLang/julia/pull/36543">Off-by-one error in dayofquarter() in leap years</a></li>
<li><a href="https://github.com/JuliaPhysics/Measurements.jl/issues/64">Incorrect simulation results when using a number type with error bars</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/36069">Pipeline with stdout=IOStream writes out of order</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/39460">Wrong results since some <code>copyto!</code> methods don’t check for aliasing</a></li>
<li><a href="https://github.com/JuliaLang/julia/issues/41096">Wrong if-else control flow</a></li>
</ul>

<p>I would hit bugs of this severity often enough to make me question the correctness of any moderately complex computation in Julia. </p>

<p>This was particularly true when trying a novel combination of packages or functions — composing together functionality from multiple sources was a significant source of bugs.</p>

<p>Sometimes the problems would be with packages that don’t compose together, and other times an unexpected combination of Julia’s features within a single package would unexpectedly fail.</p>

<p>For example, I found that the Euclidean distance from the Distances package <a href="https://github.com/JuliaStats/Distances.jl/issues/201">does not work with Unitful vectors</a>. Others discovered that Julia’s function to run external commands <a href="https://github.com/JuliaLang/julia/issues/36406">doesn’t work with substrings</a>. Still others found that Julia’s support for missing values <a href="https://github.com/JuliaLang/julia/issues/39362">breaks matrix multiplication in some cases</a>. And that the standard library’s <code>@distributed</code> macro <a href="https://github.com/JuliaLang/julia/issues/34870">didn’t work with OffsetArrays</a>.</p>

<p><a href="https://github.com/JuliaArrays/OffsetArrays.jl">OffsetArrays</a> in particular proved to be a strong source of correctness bugs. The package provides an array type that leverages Julia’s flexible <a href="https://julialang.org/blog/2017/04/offset-arrays/">custom indices feature</a> to create arrays whose indices don’t have to start at zero or one. </p>

<p>Using them would often result in out-of-bounds memory accesses, just like those one might encounter in C or C++. This would lead to segfaults if you were lucky, or, if you weren’t, to results that were quietly wrong. I once found <a href="https://github.com/JuliaLang/julia/issues/39379">a bug in core Julia</a> that could lead to out-of-bounds memory accesses even when both the user and library authors wrote correct code.</p>

<p>I filed a number of indexing-related issues with the JuliaStats organization, which stewards statistics packages like <a href="https://juliahub.com/ui/Packages/Distributions/xILW0/0.25.58">Distributions</a>, which 945 packages depend on, and <a href="https://juliahub.com/ui/Packages/StatsBase/EZjIG/0.33.16">StatsBase</a>, which 1,660 packages depend on. Here are some of them:</p>

<ul>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/646">The majority of sampling methods are unsafe and incorrect in the presence of offset axes</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distributions.jl/issues/1253">Fitting a DiscreteUniform distribution can silently return an incorrect answer</a></p></li>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/638">counteq, countne, sqL2dist, L2dist, L1dist, L1infdist, gkldiv, meanad, maxad, msd, rmsd, and psnr may return incorrect results with offset indices</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distributions.jl/issues/1265">Incorrect uses of @inbounds cause miscalculation of statistics</a></p></li>
<li><p><a href="https://github.com/JuliaStats/Distances.jl/issues/206">Colwise and pairwise can return incorrect distances</a></p></li>
<li><p><a href="https://github.com/JuliaStats/StatsBase.jl/issues/643">Showing a Weights vector wrapping an offset array accesses out-of-bounds memory</a></p></li>
</ul>

<p>The root cause behind these issues was not the indexing alone but its use together with another Julia feature, <code>@inbounds</code>, which permits Julia to remove bounds checks from array accesses.</p>

<p>For example:</p>

<pre><code>function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i] # ← 🌶
    end
    return r
end
</code></pre>

<p>The code above iterates <code>i</code> from 1 to the length of the array. If you pass it an array with an unusual index range, it will access out-of-bounds memory: the array access was annotated with <code>@inbounds</code>, which removed the bounds check.</p>

<p>The example above shows how to use <code>@inbounds</code> <strong>incorrectly</strong>. However, for years it was also the official example of how to use <code>@inbounds</code> <strong>correctly</strong>. The example was situated directly above a warning explaining why it was incorrect:</p>

<figure>
<img src="https://yuri.is/not-julia/issue.png" alt="">
</figure>

<p><a href="https://github.com/JuliaLang/julia/issues/39367">That issue</a> is now fixed, but it is worrying that <code>@inbounds</code> can be so easily misused, causing silent data corruption and incorrect mathematical results.</p>

<p>In my experience, issues like these were not confined to the mathematical parts of the Julia ecosystem.</p>

<p>I encountered library bugs while trying to accomplish mundane tasks like <a href="https://github.com/quinnj/JSON3.jl/issues/63">encoding</a> <a href="https://github.com/JuliaLang/julia/issues/34249">JSON</a>, issuing <a href="https://github.com/JuliaWeb/HTTP.jl/issues/626">HTTP requests</a>, using <a href="https://github.com/apache/arrow-julia/issues/101">Arrow files</a> <a href="https://github.com/apache/arrow-julia/issues/102">together</a> with DataFrames, and <a href="https://github.com/fonsp/Pluto.jl/issues/826">editing</a> <a href="https://github.com/fonsp/Pluto.jl/issues/751">Julia</a> <a href="https://github.com/fonsp/Pluto.jl/issues/836">code</a> with <a href="https://github.com/fonsp/Pluto.jl">Pluto</a>, Julia’s reactive notebook environment.</p>

<p>When I became curious if my experience was representative, a number of Julia users privately shared similar stories. Recently, public accounts of comparable experiences have begun to surface.</p>

<p>For example, in <a href="https://kidger.site/thoughts/jax-vs-julia/">this post</a> Patrick Kidger describes his attempt to use Julia for machine learning research:</p>

<blockquote>
<p>It’s pretty common to see posts on the Julia Discourse saying “XYZ library doesn’t work”, followed by a reply from one of the library maintainers stating something like “This is an upstream bug in the new version a.b.c of the ABC library, which XYZ depends upon. We’ll get a fix pushed ASAP.”</p>
</blockquote>

<p>Here’s Patrick’s experience tracking down a correctness bug (emphasis mine):</p>

<blockquote>
<p>I remember all too un-fondly a time in which one of my Julia models was failing to train. I spent multiple months on-and-off trying to get it working, trying every trick I could think of.</p>

<p>Eventually – eventually! – I found the error: <strong>Julia/Flux/Zygote was returning incorrect gradients.</strong> After having spent so much energy wrestling with points 1 and 2 above, this was the point where I simply gave up. Two hours of development work later, I had the model successfully training… in PyTorch.</p>
</blockquote>

<p>In a <a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/4">discussion</a> about the post others responded that they, too, had similar experiences.</p>

<p><a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/13">@Samuel_Ainsworth</a>:</p>

<blockquote>
<p>Like @patrick-kidger, I have been bit by incorrect gradient bugs in Zygote/ReverseDiff.jl. This cost me weeks of my life and has thoroughly shaken my confidence in the entire Julia AD landscape. [...] In all my years of working with PyTorch/TF/JAX I have not once encountered an incorrect gradient bug.</p>
</blockquote>

<p><a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/21">@JordiBolibar</a>:</p>

<blockquote>
<p>Since I started working with Julia, I’ve had two bugs with Zygote which have slowed my work by several months. On a positive note, this has forced me to plunge into the code and learn a lot about the libraries I’m using. But I’m finding myself in a situation where this is becoming too much, and I need to spend a lot of time debugging code instead of doing climate research. </p>
</blockquote>

<p>Given Julia’s extreme generality it is not obvious to me that the correctness problems <em>can</em> be solved. Julia has no formal notion of interfaces, generic functions tend to leave their semantics unspecified in edge cases, and the nature of many common implicit interfaces has not been made precise (for example, there is no agreement in the Julia community on what a number is).</p>

<p>The Julia community is full of capable and talented people who are generous with their time, work, and expertise. But systemic problems like this can rarely be solved from the bottom up, and my sense is that the project leadership does not agree that there is a serious correctness problem. They accept the existence of individual isolated issues, but not the pattern that those issues imply. </p>

<p>At a time when Julia’s machine learning ecosystem was even less mature, for example, a co-founder of the language spoke enthusiastically about using Julia in production for self-driving cars:</p>

<figure>
<img src="https://yuri.is/not-julia/picture.png" alt="">
</figure>

<p>And while it’s possible that attitudes have shifted since I was an active member, the following quote from another co-founder, also made around the same time, serves as a good illustration of the perception gap (emphasis mine):</p>

<blockquote>
<p>I think the top-level take away here is not that Julia is a great language (although it is) and that they should use it for all the things (although that’s not the worst idea), but that its design has hit on something that has made a major step forwards in terms of our ability to achieve code reuse. <strong>It is actually the case in Julia that you can take generic algorithms that were written by one person and custom types that were written by other people and just use them together efficiently and effectively.</strong> This majorly raises the table stakes for code reuse in programming languages. Language designers should not copy all the features of Julia, but they should at the very least understand why this works so well, and be able to accomplish this level of code reuse in future designs.</p>
</blockquote>

<p>Whenever a post critiquing Julia makes the rounds, people from the community are often quick to respond that, while there have historically been some legitimate issues, things have improved substantially and most of the issues are now fixed.</p>

<p>For example:</p>

<ul>
<li>In 2016: <a href="https://news.ycombinator.com/item?id=11073642">“The legitimate issues raised in that blog post are fixed.”</a></li>
<li>In 2018: <a href="https://news.ycombinator.com/item?id=17726336">“I complained also about the ‘cowboy’ culture I saw among the Julia developers when I first started with it ... but those days are gone.”</a></li>
<li>In 2020: <a href="https://news.ycombinator.com/item?id=22138071">“In 2016 yes. But this has been very much addressed.”</a></li>
<li>In 2021: <a href="https://news.ycombinator.com/item?id=27921467">“In Julia there is no technical enforcement of consistency, but semantic meaning of generic functions is broadly respected and generic code works.”</a></li>
<li>In 2022: <a href="https://news.ycombinator.com/item?id=30939963">“Of course there are bugs, but none of them are serious.”</a></li>
<li>In 2024: <a href="https://news.ycombinator.com/item?id=41104531">“My general sense of this article is that all of these have been fixed.”</a></li>
<li>In 2025: <a href="https://news.ycombinator.com/item?id=45428134">“This is a reasonable article, but way out of date now. Almost all issues raised were solved a while ago.”</a></li>
</ul>

<p>These responses often look reasonable in their narrow contexts, but the net effect is that people’s legitimate experiences feel diminished or downplayed, and the deeper issues go unacknowledged and unaddressed.</p>

<p>My experience with the language and community over the past ten years strongly suggests that, at least in terms of basic correctness, Julia is not currently reliable or on the path to becoming reliable. For the majority of use cases the Julia team wants to service, the risks are simply not worth the rewards.</p>

<p>Ten years ago, Julia was introduced to the world with an <a href="https://julialang.org/blog/2012/02/why-we-created-julia/">inspiring and ambitious set of goals</a>. I still believe that they can, one day, be achieved—but not without revisiting and revising the patterns that brought the project to the state it is in today.</p>



<p><small>Thanks to Mitha Nandagopalan, Ben Cartwright-Cox, Imran Qureshi, Dan Luu, Elad Bogomolny, Zora Killpack, Ben Kuhn, and Yuriy Rusko for discussions and comments on earlier drafts of this post.</small></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designing agentic loops (203 pts)]]></title>
            <link>https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</link>
            <guid>45426680</guid>
            <pubDate>Tue, 30 Sep 2025 15:21:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">https://simonwillison.net/2025/Sep/30/designing-agentic-loops/</a>, See on <a href="https://news.ycombinator.com/item?id=45426680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Sep/30/designing-agentic-loops/">

<p>30th September 2025</p>



<p>Coding agents like Anthropic’s <a href="https://claude.com/product/claude-code">Claude Code</a> and OpenAI’s <a href="https://github.com/openai/codex">Codex CLI</a> represent a genuine step change in how useful LLMs can be for producing working code. These agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems.</p>
<p>As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.</p>
<p>A critical new skill to develop is <strong>designing agentic loops</strong>.</p>
<p>One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.</p>
<p>My preferred definition of an LLM agent is something that <a href="https://simonwillison.net/2025/Sep/18/agents/">runs tools in a loop to achieve a goal</a>. The art of using them well is to carefully design the tools and loop for them to use.</p>
<ul>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#the-joy-of-yolo-mode">The joy of YOLO mode</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#picking-the-right-tools-for-the-loop">Picking the right tools for the loop</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#issuing-tightly-scoped-credentials">Issuing tightly scoped credentials</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#when-to-design-an-agentic-loop">When to design an agentic loop</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/#this-is-still-a-very-fresh-area">This is still a very fresh area</a></li>
</ul>
<h4 id="the-joy-of-yolo-mode">The joy of YOLO mode</h4>
<p>Agents are inherently dangerous—they can make poor decisions or fall victim to malicious <a href="https://simonwillison.net/tags/prompt-injection/">prompt injection attacks</a>, either of which can result in harmful results from tool calls. Since the most powerful coding agent tool is “run this command in the shell” a rogue agent can do anything that you could do by running a command yourself.</p>
<p>To <a href="https://simonwillison.net/2025/Jun/5/wrecking-its-environment-in-a-loop/">quote Solomon Hykes</a>:</p>
<blockquote>
<p><strong>An AI agent is an LLM wrecking its environment in a loop.</strong></p>
</blockquote>
<p>Coding agents like Claude Code counter this by defaulting to asking you for approval of almost every command that they run.</p>
<p>This is kind of tedious, but more importantly, it dramatically reduces their effectiveness at solving problems through brute force.</p>
<p>Each of these tools provides its own version of what I like to call YOLO mode, where everything gets approved by default.</p>
<p>This is <em>so dangerous</em>, but it’s also key to getting the most productive results!</p>
<p>Here are three key risks to consider from unattended YOLO mode.</p>
<ol>
<li>Bad shell commands deleting or mangling things you care about.</li>
<li>Exfiltration attacks where something steals files or data visible to the agent—source code or secrets held in environment variables are particularly vulnerable here.</li>
<li>Attacks that use your machine as a proxy to attack another target—for DDoS or to disguise the source of other hacking attacks.</li>
</ol>
<p>If you want to run YOLO mode anyway, you have a few options:</p>
<ol>
<li>Run your agent in a secure sandbox that restricts the files and secrets it can access and the network connections it can make.</li>
<li>Use someone else’s computer. That way if your agent goes rogue, there’s only so much damage they can do, including wasting someone else’s CPU cycles.</li>
<li>Take a risk! Try to avoid exposing it to potential sources of malicious instructions and hope you catch any mistakes before they cause any damage.</li>
</ol>
<p>Most people choose option 3.</p>
<p>Despite the existence of <a href="https://attack.mitre.org/techniques/T1611/">container escapes</a> I think option 1 using Docker or the new Apple <a href="https://github.com/apple/container">container tool</a> is a reasonable risk to accept for most people.</p>
<p>Option 2 is my favorite. I like to use <a href="https://github.com/features/codespaces">GitHub Codespaces</a> for this—it provides a full container environment on-demand that’s accessible through your browser and has a generous free tier too. If anything goes wrong it’s a Microsoft Azure machine somewhere that’s burning CPU and the worst that can happen is code you checked out into the environment might be exfiltrated by an attacker, or bad code might be pushed to the attached GitHub repository.</p>
<p>There are plenty of other agent-like tools that run code on other people’s computers. <a href="https://simonwillison.net/tags/code-interpreter/">Code Interpreter</a> mode in both ChatGPT and <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">Claude</a> can go a surprisingly long way here. I’ve also had a lot of success (ab)using OpenAI’s <a href="https://chatgpt.com/features/codex">Codex Cloud</a>.</p>
<p>Coding agents themselves implement various levels of sandboxing, but so far I’ve not seen convincing enough documentation of these to trust them.</p>
<p><strong>Update</strong>: It turns out Anthropic have their own documentation on <a href="https://www.anthropic.com/engineering/claude-code-best-practices#d-safe-yolo-mode">Safe YOLO mode</a> for Claude Code which says:</p>
<blockquote>
<p>Letting Claude run arbitrary commands is risky and can result in data loss, system corruption, or even data exfiltration (e.g., via prompt injection attacks). To minimize these risks, use <code>--dangerously-skip-permissions</code> in a container without internet access. You can follow this <a href="https://github.com/anthropics/claude-code/tree/main/.devcontainer">reference implementation</a> using Docker Dev Containers.</p>
</blockquote>
<p>Locking internet access down to a <a href="https://github.com/anthropics/claude-code/blob/5062ed93fc67f9322f807ecbf391ae4376cf8e83/.devcontainer/init-firewall.sh#L66-L75">list of trusted hosts</a> is a great way to prevent exfiltration attacks from stealing your private source code.</p>
<h4 id="picking-the-right-tools-for-the-loop">Picking the right tools for the loop</h4>
<p>Now that we’ve found a safe (enough) way to run in YOLO mode, the next step is to decide which tools we need to make available to the coding agent.</p>
<p>You can bring <a href="https://modelcontextprotocol.io/">MCP</a> into the mix at this point, but I find it’s usually more productive to think in terms of shell commands instead. Coding agents are <em>really good</em> at running shell commands!</p>
<p>If your environment allows them the necessary network access, they can also pull down additional packages from NPM and PyPI and similar. Ensuring your agent runs in an environment where random package installs don’t break things on your main computer is an important consideration as well!</p>
<p>Rather than leaning on MCP, I like to create an <a href="https://agents.md/">AGENTS.md</a> (or equivalent) file with details of packages I think they may need to use.</p>
<p>For a project that involved taking screenshots of various websites I installed my own <a href="https://shot-scraper.datasette.io/">shot-scraper</a> CLI tool and dropped the following in <code>AGENTS.md</code>:</p>
<pre><code>To take a screenshot, run:

shot-scraper http://www.example.com/ -w 800 -o example.jpg
</code></pre>
<p>Just that one example is enough for the agent to guess how to swap out the URL and filename for other screenshots.</p>
<p>Good LLMs already know how to use a bewildering array of existing tools. If you say "use <a href="https://playwright.dev/python/">playwright python</a>" or "use ffmpeg" most models will use those effectively—and since they’re running in a loop they can usually recover from mistakes they make at first and figure out the right incantations without extra guidance.</p>
<h4 id="issuing-tightly-scoped-credentials">Issuing tightly scoped credentials</h4>
<p>In addition to exposing the right commands, we also need to consider what credentials we should expose to those commands.</p>
<p>Ideally we wouldn’t need any credentials at all—plenty of work can be done without signing into anything or providing an API key—but certain problems will require authenticated access.</p>
<p>This is a deep topic in itself, but I have two key recommendations here:</p>
<ol>
<li>Try to provide credentials to test or staging environments where any damage can be well contained.</li>
<li>If a credential can spend money, set a tight budget limit.</li>
</ol>
<p>I’ll use an example to illustrate. A while ago I was investigating slow cold start times for a scale-to-zero application I was running on <a href="https://fly.io/">Fly.io</a>.</p>
<p>I realized I could work a lot faster if I gave Claude Code the ability to directly edit Dockerfiles, deploy them to a Fly account and measure how long they took to launch.</p>
<p>Fly allows you to create organizations, and you can set a budget limit for those organizations and issue a Fly API key that can only create or modify apps within that organization...</p>
<p>So I created a dedicated organization for just this one investigation, set a $5 budget, issued an API key and set Claude Code loose on it!</p>
<p>In that particular case the results weren’t useful enough to describe in more detail, but this was the project where I first realized that “designing an agentic loop” was an important skill to develop.</p>
<h4 id="when-to-design-an-agentic-loop">When to design an agentic loop</h4>
<p>Not every problem responds well to this pattern of working. The thing to look out for here are problems with <strong>clear success criteria</strong> where finding a good solution is likely to involve (potentially slightly tedious) <strong>trial and error</strong>.</p>
<p>Any time you find yourself thinking “ugh, I’m going to have to try a lot of variations here” is a strong signal that an agentic loop might be worth trying!</p>
<p>A few examples:</p>
<ul>
<li>
<strong>Debugging</strong>: a test is failing and you need to investigate the root cause. Coding agents that can already run your tests can likely do this without any extra setup.</li>
<li>
<strong>Performance optimization</strong>: this SQL query is too slow, would adding an index help? Have your agent benchmark the query and then add and drop indexes (in an isolated development environment!) to measure their impact.</li>
<li>
<strong>Upgrading dependencies</strong>: you’ve fallen behind on a bunch of dependency upgrades? If your test suite is solid an agentic loop can upgrade them all for you and make any minor updates needed to reflect breaking changes. Make sure a copy of the relevant  release notes is available, or that the agent knows where to find them itself.</li>
<li>
<strong>Optimizing container sizes</strong>: Docker container feeling uncomfortably large? Have your agent try different base images and iterate on the Dockerfile to try to shrink it, while keeping the tests passing.</li>
</ul>
<p>A common theme in all of these is <strong>automated tests</strong>. The value you can get from coding agents and other LLM coding tools is massively amplified by a good, cleanly passing test suite. Thankfully LLMs are great for accelerating the process of putting one of those together, if you don’t have one yet.</p>
<h4 id="this-is-still-a-very-fresh-area">This is still a very fresh area</h4>
<p><strong>Designing agentic loops</strong> is a very new skill—Claude Code was <a href="https://www.anthropic.com/news/claude-3-7-sonnet">first released</a> in just February 2025!</p>
<p>I’m hoping that giving it a clear name can help us have productive conversations about it. There’s <em>so much more</em> to figure out about how to use these tools as effectively as possible.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi News (824 pts)]]></title>
            <link>https://blog.kagi.com/kagi-news</link>
            <guid>45426490</guid>
            <pubDate>Tue, 30 Sep 2025 15:09:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/kagi-news">https://blog.kagi.com/kagi-news</a>, See on <a href="https://news.ycombinator.com/item?id=45426490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759164655-533625-news.png" alt="Cartoon illustration showing multiple smartphone screens showing the Kagi News app with news in different languages."></p>

<p><strong>A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.</strong></p>

<p>News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be.
We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.</p>

<h2>Our approach: Signal over noise</h2>

<p><a href="https://kite.kagi.com/">Kagi News</a> operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.</p>

<p>We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.</p>

<h2>Design principles that put readers first</h2>

<p><img src="https://kagifeedback.org/assets/files/2025-09-30/1759238916-838501-kaginews.gif" alt="Gif showing a demo of using the Kagi News app, scrolling through the daily press review for world news, clicking on a headline and showing the detailed summary that Kagi News creates. On the left is the Kagi News logo and download icons for Google Play and App Store."></p>

<p><strong>One daily update:</strong> We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.</p>

<p><strong>Five-minute complete understanding:</strong> Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.</p>

<p><strong>Diversity over echo chambers:</strong> Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.</p>

<p><strong>Privacy by design:</strong> Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.</p>

<p><strong>Community-driven sources:</strong> Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.</p>

<p><strong>Customizable:</strong> In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.</p>

<p><strong>News in your language:</strong> You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.</p>

<h2>Technical implementation that respects publishers</h2>

<p>We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.</p>

<p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759171753-315733-review.png" alt="Review from apple store that says “I’ve avoided news feeds all of my adult life. The onslaught of information and opinions always felt like a waste of my precious time. This app is on a great track to cut through the noise.”"></p>

<h2>Ready to experience news differently?</h2>

<p>If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:</p>

<ul>
<li><a href="https://kite.kagi.com/">Web</a></li>
<li><a href="https://apps.apple.com/us/app/kagi-news/id6748314243">iOS</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.kagi.news">Android</a></li>
</ul>

<p><img src="https://kagifeedback.org/assets/files/2025-09-29/1759164726-927335-kagi-news.jpg" alt="Photo showing hands holding an iPad, with Kagi News open on the Sports news section."></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the AI bubble ate Y Combinator (192 pts)]]></title>
            <link>https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632</link>
            <guid>45426205</guid>
            <pubDate>Tue, 30 Sep 2025 14:52:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632">https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632</a>, See on <a href="https://news.ycombinator.com/item?id=45426205">Hacker News</a></p>
Couldn't get https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Largest Mass Resignation in US History as 100k Federal Workers Quit (107 pts)]]></title>
            <link>https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162</link>
            <guid>45425892</guid>
            <pubDate>Tue, 30 Sep 2025 14:27:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162">https://www.newsweek.com/largest-mass-resignation-in-us-history-as-100000-federal-workers-quit-10802162</a>, See on <a href="https://news.ycombinator.com/item?id=45425892">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Some 100,000 federal workers are set to formally quit the U.S. <a href="https://www.newsweek.com/topic/government">government</a> in what will constitute the largest mass <a href="https://www.newsweek.com/topic/resignation">resignation</a> of government workers in U.S. history.</p><p>The resignations—which come as part of a program drawn up by President Donald Trump at the start of his second administration—will happen on Tuesday as Congress is facing a deadline on the same day to authorize more funding or <a href="https://www.newsweek.com/vance-shutdown-warning-meeting-trump-schumer-jeffries-10801122">risk a government shutdown</a>. </p><p>If there is no deal, the White House has ordered federal agencies to make plans for the large-scale redundancies.</p><p><em>Newsweek </em>contacted the Office of Personnel Management by email outside of normal business hours to comment on this story.</p><h2><strong>Why It Matters</strong></h2><p>The loss of federal workers will have huge impacts on operations across different parts of government and could disrupt services including in the Department of Veteran Affairs and the Social Security Administration. </p><p>An increasing number of unemployed adults will also impact the economy, with a large number of people clamoring to secure jobs in the private sector at once.</p><div><p><img id="10802438" alt="" caption="A view of the U.S. Capitol on September 29, 2025 in Washington, DC." credit="(Photo by Anna Moneymaker/Getty Images)" sourcealt="" sources="[]" fetchpriority="auto" loading="lazy" width="6000" height="4000" decoding="async" data-nimg="1" srcset="https://assets.newsweek.com/wp-content/uploads/2025/09/GettyImages-2238123307.jpg?w=1600&amp;quality=75&amp;webp=1 1x" src="https://assets.newsweek.com/wp-content/uploads/2025/09/GettyImages-2238123307.jpg?w=1600&amp;quality=75&amp;webp=1"></p></div><h2><strong>What To Know</strong></h2><p>The resignations come as, since assuming office, the <a href="https://www.newsweek.com/topic/trump-administration">Trump administration</a> has made trimming down federal bureaucracy and streamlining services one of its key priorities. To that end, Trump created the Department of Government Efficiency (<a href="https://www.newsweek.com/topic/doge">DOGE</a>), an agency spearheaded by billionaire and Tesla CEO Elon Musk, until <a href="https://www.newsweek.com/elon-musk-statement-white-house-exit-doge-2078270">he left the White House </a>in May following disagreements with the President.</p><p>The administration also implemented&nbsp;<a href="https://www.newsweek.com/donald-trumps-purge-federal-workers-spark-mass-confusion-anger-2031729" target="_blank" rel="noreferrer noopener">hiring freezes and mass layoffs</a> and in February issued a directive giving federal employees the choice of accepting a resignation package which included salary payments until the end of September or risk future layoffs. </p><p>The planned 100,000 resignations come as a part of this program, known as the Deferred Resignation Program (DRP).</p><p>According to a Senate Democrat's report in July, DRP will cost $14.8 billion as 200,000 workers will be paid their full salary and benefits while on administrative leave for up to eight months.</p><p>There are 2.4 million federal workers in the U.S., according to the Bureau of Labor Statistics (BLS). Aside from resignations, while there is no official data revealing how many people have been fired,&nbsp;but <em>The New York Times</em>&nbsp;estimates the figure at 135,000.</p><p>Meanwhile,<em> Newsweek</em> reported that fired federal workers have suffered from poor mental health, <a href="https://www.newsweek.com/fired-federal-workers-employees-jobs-market-2087217">while others have struggled to find jobs in the private sector.</a></p><h2><strong>What People Are Saying</strong></h2><p><strong>Speaking to <em>Newsweek, </em>Scott Lucas, who teaches international politics at University College Dublin</strong> said the resignations were "technically a redundancy." He added that they "will have an affect on the unemployment rate in the United States so it's going to have a political effect because that spike will look bad for the Trump administration."</p><p>He added: "The Trump administration will affect government services, it will affect basic government functions."</p><p><strong>Writing in a post on LinkedIn, Next Interior, a group affiliated with government department the Department of the Interior, </strong>said the mass resignation: "represents a massive loss in institutional knowledge and capacity across the <a href="https://www.newsweek.com/topic/federal-government">federal government</a> - the expertise, relationships, and dedicated people that make it work for taxpayers."</p><p><strong>An August <a href="https://www.opm.gov/news/secrets-of-opm/what-they-got-wrong-about-the-deferred-resignation-program-1/">memo</a> by Scott Kupor, Director at the U.S. Office of Personnel management said</strong>: "We designed the DRP as a practical, humane, and voluntary option to accelerate workforce transitions in a system that desperately needed movement. Employees were given the option to retire early and receive eight months of paid leave; in return, the government will save $20+ billion in costs,&nbsp;annually."</p><p>It added: "At OPM we’re here to fix the decades of broken systems that have put us in this position. The Deferred Resignation Program was a necessary step toward a smarter, leaner, more effective government. If that ruffles a few feathers in Washington, so be it. The American people deserve a workforce built for performance, not permanence."</p><h2><strong>What Happens Next</strong></h2><p>It remains to be seen whether the government shutdown can be avoided.</p><p>Meanwhile, while 100,000 workers are set to leave the government, a Washington D.C. federal judge has temporarily blocked the Trump administration's plans to <a href="https://www.newsweek.com/republican-nominated-judge-blocks-trump-admin-kari-lake-in-new-order-10801633">cut over 500 jobs</a> at the U.S. Agency for Global Media, a government-funded news network.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Frank Chimero: I think we're in the lemon stage of the internet (194 pts)]]></title>
            <link>https://frankchimero.com/blog/2025/selling-lemons/</link>
            <guid>45425746</guid>
            <pubDate>Tue, 30 Sep 2025 14:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frankchimero.com/blog/2025/selling-lemons/">https://frankchimero.com/blog/2025/selling-lemons/</a>, See on <a href="https://news.ycombinator.com/item?id=45425746">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
			<div>
				<!-- <figure class="full" style="margin-top: calc(-1 * var(--unit-xxl))">
					<img src="/images/borderlands.png" style="object-fit: cover; max-height: 50dvh">
				</figure> -->
				<header>
					<div>
						
						<h2>The hidden costs of the meta game</h2>
						<p><time>September 30, 2025</time>
					</p></div>
				</header>
				<p>I’ve been researching a new talk the last few weeks and along the way stumbled across a concept that’s been rattling around in my head. I am writing to share, because I find it a satisfying description for the tech flop era.</p>

<p>The idea is called “a market for lemons.” The phrase comes from a <a href="https://en.wikipedia.org/wiki/The_Market_for_Lemons">1970 paper by George Akerlof</a> that explains how information asymmetry between buyers and sellers can undermine a marketplace. Akerlof asks us to imagine ourselves buying a used car. Some cars on the lot are reliable, well-maintained gems. Others cars are lemons, the kinds of cars that can make it off the lot but are disasters waiting to happen. The sellers know which cars are which, but you, as a buyer, can’t tell the difference. That information asymmetry affects the average price in the market and eventually impacts the overall market dynamics.</p>

<p>The thinking goes like this: if a buyer can’t distinguish between good and bad, everything gets priced somewhere in the middle. If you’re selling junk, this is fantastic news—you’ll probably get paid more than your lemon is worth. If you’re selling a quality used car, this price is insultingly low. As a result, people with good cars leave the market to sell their stuff elsewhere, which pushes the overall quality and price down even further, until eventually all that’s left on the market are lemons.</p>

<p>I think we’re in the lemon stage of the internet.</p>

<hr>

<p>I thought about this last week while shopping online for a sleep mask. Brands like MZOO, YFONG, WAOAW popped up, and these seemed less like companies and more like vowel smoke ejected from a factory flue hole, then slotted into a distribution platform. The long tail of generic brands on e-commerce platforms is a textbook lemons market: good products get drowned out by these alphabet soup products, who use their higher margins to buy sponsored placement in search results. Both buyers and sellers eventually lose (and perhaps the platforms win, as long as they don’t wear out their reputation).</p>

<p>For shoppers, buying online now feels like rolling the dice on the quality of the product. For sellers, the gamble is that their survival relies more on gaming the system than actually improving the product.</p>

<p>I think the post-pandemic experience has been a collective realization that the value that drew us to certain digital products and marketplaces is gone. Much of this reduction in value gets pinned to ZIRP, but there’s another critical factor—the natural flight of value creators. As platforms matured, the users and sellers who generated real value were squeezed out by players focused on capturing value rather than creating it.</p>

<p>Once you identify a lemon market, you start to see it all over the place.</p>

<p><em>Online dating.</em> A lemon market where participants have no familiarity with one another participate in strategic self-presentation. High-quality partners (emotionally available, looking for genuine connection) can’t effectively distinguish themselves from those just seeking validation and eventually leave.</p>

<p><em>Search results.</em> A lemon market where platforms profit from sponsored placement, misaligning incentives with user needs. The first page is a minefield: sponsored listings posing as organic results, SEO content farms, affiliate aggregators. You add “reddit” to work around this, but even that has less success these days.</p>

<p><em>Social media.</em> Your feed is now professional content creators, low-effort podcast video clips, algorithmic filler reaction videos, stand-up chaff, and animals. Good ideas don’t happen frequently enough to satisfy the pace of the algorithm, so many have pivoted to newsletters or stopped posting.</p>

<p>What makes the Market for Lemons concept so appealing (and what differentiates it in my mind from <a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> is that everyone can be acting reasonably, pursuing their own interests, and things still get worse for everyone. No one has to be evil or stupid: the platform does what’s profitable, sellers do what works, buyers try to make smart decisions, and yet the whole system degrades into something nobody actually wants.</p>

<hr>

<p>I was first introduced to the Market of Lemons by Dan Luu in an essay titled, <a href="https://danluu.com/nothing-works/">Why is it so hard to buy things that work well?</a>. Luu applies the market of lemons as a metaphor, and specifically identifies hiring as a market of lemons, because of the <a href="https://danluu.com/hiring-lemons/">information asymmetry for both companies and individuals</a>.</p>

<p>Companies have always struggled to tell the difference between great individual contributors and mediocre ones. Lacking a clear way to separate the two, they lump everyone together and rely on proxy games to evaluate skill. Candidates, for their part, walk into interviews without crucial information: whether the company is quietly dysfunctional, whether the manager they liked during interviews is about to quit, or whether the open role itself is little more than a vestige of an abandoned strategy that’s likely to be cut once the other foot drops. The usual signals of strength or weakness don’t signify much at all when it comes to hiring. Layer on the automated scale of the application process—candidates firing off applications by the hundreds, companies screening by the thousands—and the result is a highly inefficient market that wastes everyone’s time. Meaningful signals get drowned out, everyone gets lumped together, rational players opt out to the extent they are able, and the market slides steadily downward.</p>

<p>There have been countless attempts to make hiring more rational and efficient—the stuff of startup pitch deck lore. But I’m not sure hiring can ever be much more efficient, because neither side has reason to show themselves as they really are, warts and all. Idealistically, both would come straight; pragmatically, it is a game of chicken. Candidates polish résumés and present curated versions of their abilities, listing outcomes and impact statistics with dubious accuracy and provenance. Companies do the same, putting culture and mission front and center while hiding systematic dysfunctions and looming existential risks. When neither side is forthcoming, you’re left with proxies: a famous logo on a resume, a polished culture deck. Gaming the meta of the system supersedes the actual development or evaluation of skill. And, much to my disappointment, gaming the meta may, in fact, be an essential aspect of most jobs.</p>

<hr>

<p>At this point, it should be obvious how the market for lemons applies to ill-considered AI-generated content. I’ll let you sketch out that argument yourself since it’s fairly straightforward, and this thing is already long enough.</p>

<p>Instead, let’s zag and revisit my point earlier about system-gaming becoming the most viable playbook instead of focusing on the product. As a consumer and as a designer, I hope this is a temporary state before a massive recalibration. The primacy of meta-activities—optimizing for algorithms, visibility theater, consumer entrapment, externalization of costs, performative internal alignment, horse-trading amongst a set of DOA ideas—is poison. It is a road to nowhere worth going.</p>

<p>This reflects a business culture obsessed with outcomes while treating outputs as speed bumps. But outputs (code, design, the products themselves) are the load-bearing work—the actual prerequisites for the outcomes desired. Focusing on outcomes while ignoring outputs means hiding in abstractions and absolving oneself of accountability. If any output is acceptable to hit your targets, what awful things emerge at scale? What horrors happen when success detaches completely from the necessity of being good—having both skill and ethics?</p>

<p>The safest, smartest path is also the most mundane: keep the main thing the main thing. Outcomes matter, but output literally comes first. Outputs are the business to everyone outside it—what customers see, buy, and use. You can’t stay safe in abstractions forever. Eventually, you must attend to the reality of what’s in front of you, because that’s where work gets done and where assumptions get validated or falsified (because <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail">reality has a surprising amount of detail</a>).</p>

<p>In other words, the meta ruins things for everyone. To hide in abstractions is to dodge the reality of your choices. These tactics may get you profit, but you sacrifice benefit. The climb may feel like progress, but at the end you’ll find yourself at the top of a mountain of lemons, perhaps not of your own making, but almost certainly of your own doing.</p>

			</div>
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software essays that shaped me (131 pts)]]></title>
            <link>https://refactoringenglish.com/blog/software-essays-that-shaped-me/</link>
            <guid>45425568</guid>
            <pubDate>Tue, 30 Sep 2025 14:01:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refactoringenglish.com/blog/software-essays-that-shaped-me/">https://refactoringenglish.com/blog/software-essays-that-shaped-me/</a>, See on <a href="https://news.ycombinator.com/item?id=45425568">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><article><header></header><p>I started reading software blogs before I got my first programming job 20 years ago. At this point, I’ve read thousands of blog posts and essays about software, but only a small handful stuck in my mind and changed the way I think.</p><ol><li><a href="#-by-joel-spolsky-2000">“The Joel Test: 12 Steps to Better Code” by Joel Spolsky (2000)</a></li><li><a href="#-by-alexis-king-2019">“Parse, don’t validate” by Alexis King (2019)</a></li><li><a href="#-by-fred-brooks-1986">“No Silver Bullet - Essence and Accident in Software Engineering” by Fred Brooks (1986)</a></li><li><a href="#-by-joel-spolsky-2000-1">“Choices” by Joel Spolsky (2000)</a></li><li><a href="#-by-raymond-chen-2010">“Application compatibility layers are there for the customer, not for the program” by Raymond Chen (2010)</a></li><li><a href="#-by-erik-kuefler-2014">“Don’t Put Logic in Tests” by Erik Kuefler (2014)</a></li><li><a href="#-by-julia-evans-2020">“A little bit of plain Javascript can do a lot” by Julia Evans (2020)</a></li><li><a href="#-by-dan-mckinley-2015">“Choose Boring Technology” by Dan McKinley (2015)</a></li><li><a href="#-by-terence-eden-2022">“I’ve locked myself out of my digital life” by Terence Eden (2022)</a></li><li><a href="#bonus-brad-fitzpatrick-on-parsing-user-input-2009">Bonus: Brad Fitzpatrick on parsing user input (2009)</a></li></ol><h2 id="-by-joel-spolsky-2000"><a href="https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/">“The Joel Test: 12 Steps to Better Code”</a> by Joel Spolsky (2000)<a href="#-by-joel-spolsky-2000">🔗</a></h2><p>Joel Spolsky is the greatest software blogger of all time. His essays have informed so much of my approach to software that it was hard to pick out just one, but “The Joel Test” is my favorite.</p><p>The Joel Test is a set of 12 questions that employers can ask themselves to see how well they’re investing in their software team:</p><blockquote><ol><li>Do you use source control?</li><li>Can you make a build in one step?</li><li>Do you make daily builds?</li><li>Do you have a bug database?</li><li>Do you fix bugs before writing new code?</li><li>Do you have an up-to-date schedule?</li><li>Do you have a spec?</li><li>Do programmers have quiet working conditions?</li><li>Do you use the best tools money can buy?</li><li>Do you have testers?</li><li>Do new candidates write code during their interview?</li><li>Do you do hallway usability testing?</li></ol></blockquote><p>Some of the questions are dated, but the point was never the questions themselves but rather the meta-point of the questions.</p><p>Joel was really asking employers: <strong>do you respect developers?</strong></p><p>The questions all assess whether an employer prioritizes their developers’ time and focus over things like cheap office space and short-term deadlines.</p><p>Joel published this article at the height of the dot-com boom, when skilled developers were a precious resource, but not everyone realized it, including developers themselves.</p><p>Joel’s blog always presented programmers as rare, delicate geniuses that employers needed to pursue and pamper. I liked that.</p><p>Throughout my career, I sought out employers that scored well on the Joel test, and I’m grateful to Joel for giving me the map to find them.</p><h2 id="-by-alexis-king-2019"><a href="https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">“Parse, don’t validate”</a> by Alexis King (2019)<a href="#-by-alexis-king-2019">🔗</a></h2><p>This essay is about leveraging the type system in Haskell to — wait, wait! Don’t go to sleep.</p><p>If you don’t care about type systems or Haskell, I get it. I don’t either. But this essay radically changed the way I think about software. You can use Alexis’ technique outside of Haskell in any language that supports static types (e.g., Go, C++, Rust).</p><p>The highly abridged version of the essay is that whenever you validate any data, you should convert it to a new type.</p><p>Suppose that your app has a rule limiting usernames to a maximum of 20 alphanumeric characters. The naïve solution would be to define a function that looks like this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>validateUsername</span>(username <span>string</span>) <span>error</span> { ... }
</span></span></code></pre></div><p>With the above function, you run <code>validateUsername</code> anytime you receive a username from a user.</p><p>The problem with this approach is that your code is unsafe by default. You have to remember to validate every username you receive, so it’s easy to create a code path that accidentally processes a username without validating it. And if a nefarious user notices the mistake, they can do tricky things like embed malicious code in the username field or stuff it with a billion characters to fill up your database.</p><p>Alexis’ solution is to instead use a function like this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>parseUsername</span>(raw <span>string</span>) (Username, <span>error</span>) { ... }
</span></span></code></pre></div><p>In the rest of your codebase, instead of passing around a <code>string</code> called “username,” you use a custom type: <code>Username</code>. The only function that can create a <code>Username</code> is <code>parseUsername</code>, and it applies validation rules before returning a <code>Username</code> instance.</p><p>Therefore, if you have a <code>Username</code> instance, it must contain a valid username. Otherwise, it couldn’t exist.</p><p>You can’t forget to validate a username because untrusted input will always be a <code>string</code>, and you can’t pass a <code>string</code> to a function that expects a <code>Username</code>.</p><p>Before Alexis’ essay, I thought type systems were just a fun way to distract language nerds. “Parse, don’t validate” opened my eyes to how valuable compiler features can be in improving an application’s security and reliability.</p><h2 id="-by-fred-brooks-1986"><a href="https://www.cs.unc.edu/techreports/86-020.pdf">“No Silver Bullet - Essence and Accident in Software Engineering”</a> by Fred Brooks (1986)<a href="#-by-fred-brooks-1986">🔗</a></h2><p>In college, I read <em>The Mythical Man-Month</em>, a collection of essays about software engineering by Fred Brooks, drawing on his experience directing <a href="https://en.wikipedia.org/wiki/OS/360_and_successors">IBM’s OS/360 project</a>.</p><p>The essays were hit or miss. Some felt too old to be relevant, even in 2002, but the one that stuck with me was, “No Silver Bullet.”</p><p>The essay argues that you can divide software work into two categories: essential complexity and accidental complexity.</p><p><strong>Essential complexity</strong> is the work that you absolutely have to do, regardless of your tooling and hardware. For example, if you write software that calculates bonuses for salespeople, you have to define formulas for those bonuses and cover all possible edge cases. This work is the same if you have a $5B supercomputer or a $1 microcontroller.</p><p><strong>Accidental complexity</strong> is everything else: dealing with memory leaks, waiting for your code to compile, figuring out how to use a third-party library. The better your tooling and hardware resources, the less time you spend on accidental complexity.</p><p>Given this model, Brooks concluded that it was impossible for any advancement in tooling or hardware to create a 10x improvement in developer productivity:</p><blockquote><p>How much of what software engineers now do is still devoted to the accidental, as opposed to the essential? Unless it is more than 9/10 of all effort, shrinking all the accidental activities to zero time will not give an order of magnitude improvement.</p></blockquote><p>Throughout my career, people have been trying to find ways to eliminate programmers from software. For a few years, no-code platforms generated buzz by promising non-programmers all the powers of a seasoned web developer.</p><p>Brooks’ essay always reassured me that the latest buzzword platforms could never replace developers, as the platforms focused on the accidental, not the essential. Even if the platforms could magically create working code from a functional specification, you still need someone to write the spec:</p><blockquote><p>I believe the hard part of building software to be the specification, design, and testing of this conceptual construct, not the labor of representing it and testing the fidelity of the representation.</p></blockquote><p>Modern AI has thrown a wrench into Brooks’ theory, as it actually <em>does</em> reduce essential complexity. You can hand AI an incomplete or contradictory specification, and the AI will fill in the gaps by cribbing from similar specifications.</p><p>Even if AI eliminates programming as we know it, Brooks’ essay gives me hope that we’ll still need people to manage essential complexity at whatever level of abstraction that ends up being.</p><h2 id="-by-joel-spolsky-2000-1"><a href="https://www.joelonsoftware.com/2000/04/12/choices/">“Choices”</a> by Joel Spolsky (2000)<a href="#-by-joel-spolsky-2000-1">🔗</a></h2><p>I said <a href="#-by-joel-spolsky-2000">above</a> that it was hard to pick a single favorite Joel Spolsky essay, which is why I’ve chosen two.</p><p>“Choices” is about creating user interfaces and the subtle costs of giving a user power:</p><blockquote><p><strong>Every time you provide an option, you’re asking the user to make a decision.</strong> That means they will have to think about something and decide about it. It’s not necessarily a bad thing, but, in general, you should always try to minimize the number of decisions that people have to make.</p></blockquote><p>As an example, Joel shares a ridiculous dialog that appears in Windows 98 when you try to search the help documentation:</p><p><a href="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif"><img sizes="(min-width: 768px) 470px, 98vw" srcset="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif 470w" src="https://refactoringenglish.com/blog/software-essays-that-shaped-me/Stupidest_Dialog_Ever.gif" alt="" loading="lazy"></a></p><p>The dialog infuriates Joel because it interrupts the user while they’re trying to get help, and it asks them to make an uninformed about database optimization. Windows was shirking a decision and pushing it onto the user.</p><p>Joel’s essay focuses on graphical user interfaces, but I think about it wherever people might encounter my code, including on the command-line or other developers calling functions I wrote. Can I make a useful decision on my user’s behalf while still giving them power over things they care about? There are countless times where Joel’s essay has saved me from pushing a decision onto the user that I could make myself.</p><h2 id="-by-raymond-chen-2010"><a href="https://devblogs.microsoft.com/oldnewthing/20100311-00/?p=14643">“Application compatibility layers are there for the customer, not for the program”</a> by Raymond Chen (2010)<a href="#-by-raymond-chen-2010">🔗</a></h2><p>Raymond Chen is one of the longest-serving developers on the Microsoft Windows team. His blog has thousands of informative, entertaining stories about the history of Windows programming, but the one I think back to most is one about compatibility mode in Windows Vista.</p><p>A customer had contacted Raymond’s team with this request:</p><blockquote><p>Hi, we have a program that was originally designed for Windows XP and Windows Server 2003, but we found that it runs into difficulties on Windows Vista. We’ve found that if we set the program into Windows XP compatibility mode, then the program runs fine on Windows Vista. What changes do we need to make to our installer so that when the user runs it on Windows Vista, it automatically runs in Windows XP compatibility mode?</p></blockquote><p>Raymond proceeds to characterize the customer’s request as follows:</p><blockquote><p>I normally toss my garbage on the sidewalk in front of the pet store, and every morning, when they open up, somebody sweeps up the garbage and tosses it into the trash. But the pet store isn’t open on Sundays, so on Sundays, the garbage just sits there. How can I get the pet store to open on Sundays, too?</p></blockquote><p>I loved this analogy. The metaphor was so funny that I didn’t realize until just now that Raymond is in the wrong. He’s making fun of a developer whose sin is expecting Windows not to break their app after a single release.</p><p>But as is the case with a lot of Raymond Chen’s writing, it’s so funny and sharp that I can look past the flaws.</p><p>Even though I disagree with the specifics, Raymond’s post is an excellent lesson in influencing user behavior.</p><p>If you want to nudge the user to do something that helps you, think carefully about the path of least resistance from the user’s perspective, because that’s the path they’ll take.</p><p>If you show the user that dumping garbage on the sidewalk completely solves their problem, they’re going to keep dumping their garbage on the sidewalk.</p><h2 id="-by-erik-kuefler-2014"><a href="https://testing.googleblog.com/2014/07/testing-on-toilet-dont-put-logic-in.html">“Don’t Put Logic in Tests”</a> by Erik Kuefler (2014)<a href="#-by-erik-kuefler-2014">🔗</a></h2><p>I’ve always loved unit testing and took great pride in my test code. That’s why I was so horrified when this essay <a href="https://testing.googleblog.com/2024/12/tech-on-toilet-driving-software.html">appeared in my bathroom</a> and revealed that I’d been writing awful tests my whole career.</p><p>Erik’s essay shows the following unit test, which has a subtle bug:</p><div><pre tabindex="0"><code data-lang="java"><span><span>@Test <span>public</span> <span>void</span> <span>shouldNavigateToPhotosPage</span>() {
</span></span><span><span>  String baseUrl = <span>"http://plus.google.com/"</span>;
</span></span><span><span>  Navigator nav = <span>new</span> Navigator(baseUrl);
</span></span><span><span>  nav.goToPhotosPage();
</span></span><span><span>  assertEquals(baseUrl + <span>"/u/0/photos"</span>, nav.getCurrentUrl());
</span></span><span><span>}
</span></span></code></pre></div><p>When I first read the essay, I thought, “That’s exactly how I write unit tests!”</p><p>Why duplicate the <code>http://plus.google.com/</code> string in two places? Create a single source of truth, just like in production code. I did this all the time, adding helper functions, variables, and loops to eliminate redundancy from my tests.</p><p>The problem with the approach above is that it masks a subtle bug. It’s actually asserting that the URL looks like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>http://plus.google.com//u/0/photos
</span></span><span><span>                      ^^
</span></span><span><span>                    whoops
</span></span></code></pre></div><p>Erik’s essay made me realize that I shouldn’t treat test code like production code at all. The two have <a href="https://mtlynch.io/good-developers-bad-tests/#test-code-is-not-like-other-code">completely different goals and constraints</a>.</p><p>Good test code should be, above all, clear. Test code doesn’t have its own test code, so the only way to verify correctness is by inspection. A test should make it blindingly obvious to the reader what behavior it asserts. In service of that goal, you can accept redundancy to reduce complexity.</p><h2 id="-by-julia-evans-2020"><a href="https://jvns.ca/blog/2020/06/19/a-little-bit-of-plain-javascript-can-do-a-lot/">“A little bit of plain Javascript can do a lot”</a> by Julia Evans (2020)<a href="#-by-julia-evans-2020">🔗</a></h2><p>As a software engineer, I was embarrassingly late to the web. For the first 10 years of my career, I only wrote code for desktop apps and backend servers. I never bothered with HTML or JavaScript until 2017.</p><p>By the time I got serious about learning frontend development, my impression was that JavaScript was a mess of a language, <a href="https://www.computer.org/csdl/magazine/co/2012/02/mco2012020007/13rRUy08MzA">hacked together in 10 days</a>, and it had drastically different behavior in different browsers. If I was going to write web apps, I wanted something modern and sleek to protect me from all of JavaScript’s bile and warts.</p><p>So, I tried the popular web frameworks of the day: Angular, React, and Vue. I learned enough Vue to make my way around, but I was still spending an enormous amount of my time on dependency issues and framework gotchas. After all the work these frontend frameworks did to fix JavaScript, web programming still sucked.</p><p>Then, I read Julia’s essay, and I realized I’d been so confident that JavaScript needed fixing that I never gave it a chance.</p><p>At the time, I was working on <a href="https://mtlynch.io/tinypilot/">the prototype of TinyPilot</a>, which would become my first commercially successful software product. TinyPilot had a web interface that I was planning to implement with Vue, but Julia’s essay inspired me to see how far I could go with plain JavaScript. No framework, no wrapper libraries, no build step, no Node.js, just regular old JavaScript. Okay, not “old” — more like <a href="https://en.wikipedia.org/wiki/ECMAScript_version_history#9th_edition_%E2%80%93_ECMAScript_2018">ES2018</a>, but you know.</p><p>I kept expecting to hit some problem where I’d need to switch to some kind of framework or builder, but it never happened. There were still some gotchas, especially around WebComponents, but it was nothing compared to the suffering I endured with Vue and Angular.</p><p>I loved being free of the frameworks. When I had a runtime error, the stack trace wasn’t some minified, transmogrified, tree-shakified fever dream of my code. I was debugging <em>my code</em>, exactly as I wrote it. Why hadn’t I tried this sooner?</p><p>My biases about JavaScript were wrong. Modern JavaScript is pretty nice. It absorbed a lot of ideas from wrapper libraries, so now you don’t need the wrappers. And browsers got their act together to ensure consistent behavior across platforms and devices.</p><p>I haven’t integrated a JavaScript framework or build step into any new project since 2020, and I’ve never looked back. Plain JavaScript gets me 90% of the benefit of frameworks with 5% of the headache.</p><h2 id="-by-dan-mckinley-2015"><a href="https://mcfunley.com/choose-boring-technology">“Choose Boring Technology”</a> by Dan McKinley (2015)<a href="#-by-dan-mckinley-2015">🔗</a></h2><p>This is an odd essay to include in this list because I’ve never actually read it.</p><p>People have quoted this essay to me, and once I understood the idea, it felt so intuitive that I didn’t need to read it. In my interview with <a href="https://corecursive.com/">CoRecursive podcast</a> host Adam Gordon Bell, he talked about how there are certain non-fiction books where, once you understand the idea, <a href="https://refactoringenglish.com/blog/interview-adam-gordon-bell/#crafting-blog-post-titles">all you need is the title</a>. “Choose Boring Technology” is that for me.</p><p>Dan’s argument is that when you start a new project, you’re tempted to use cutting-edge technology that has lots of buzz. Google just announced a new database that scales to exabytes, and it’s 40% faster than Postgres at 20% the cost. You’d be an idiot to use Postgres when this sexy new alternative is right there!</p><p>In practice, the new technology has bugs and weaknesses, but they’re not obvious to you yet; they’re not obvious to anyone yet. So, when you run into them, you’re stuck. Postgres has its issues, but after 30 years in the field, it has battle-tested solutions for any problem you’re likely to encounter.</p><p>Dan concedes that you should use new technologies sometimes but only strategically and in limited quantities. He suggests that every business gets three “innovation tokens” to spend. If you want a flashy new database, you’ll have to spend one of your tokens.</p><p>Dan’s essay dovetails naturally with Julia’s essay. I wish I’d read either of them before I wasted all that time with frontend frameworks.</p><h2 id="-by-terence-eden-2022"><a href="https://shkspr.mobi/blog/2022/06/ive-locked-myself-out-of-my-digital-life/">“I’ve locked myself out of my digital life”</a> by Terence Eden (2022)<a href="#-by-terence-eden-2022">🔗</a></h2><p>Terence Eden is a delightful and eclectic technology blogger. He writes several new posts each week, but the one that impacted me the most was “I’ve locked myself out of my digital life.”</p><p>The article plays out what would happen if lightning struck Terence’s house and destroyed all of his possessions. He keeps his passwords to everything in a password manager, but if all his devices get destroyed, he can’t access his password manager. And he can’t fall back to hardware passkeys because those were in his house, too.</p><p>I always felt like I was pretty safe about my data because I store everything on redundant drives, and I have offsite backups on three continents with two vendors.</p><p>Terence’s post got me thinking about the many credible threats that could wipe out all of my devices simultaneously: fire, flood, electrical surge, criminal investigation. All of my data is encrypted with passwords that live in my head, so add to that list memory loss, incapacitation, or death.</p><p>Online services are bad at helping users recover from disaster. I use several services that assume it’s impossible for me to ever lose my phone, let alone my email account and every digital device in my possession.</p><p>Ever since I read Terence’s essay, I’ve been thinking more about which services and devices are critical to me, and how I could recover from a scenario like the one Terence described. The next time I bought a laptop, I set it up at the library to test whether I could recover access to my password manager and critical accounts without any of the devices in my house.</p><p>I still could do a better job at digital disaster preparedness, but Terence’s post always echoes in my head whenever I think about how to secure my devices and data. What if everything was suddenly destroyed?</p><h2 id="bonus-brad-fitzpatrick-on-parsing-user-input-2009">Bonus: Brad Fitzpatrick on parsing user input (2009)<a href="#bonus-brad-fitzpatrick-on-parsing-user-input-2009">🔗</a></h2><p>It’s technically not an essay, but there’s a quote from a software interview I constantly think about.</p><p>In 2009, as a result of <a href="https://www.joelonsoftware.com/2009/09/23/the-duct-tape-programmer/">Joel Spolsky’s gushing review</a>, (yes, again with the Joel), I read <a href="https://codersatwork.com/"><em>Coders at Work</em></a>, a collection of interviews with accomplished programmers.</p><p><a href="https://en.wikipedia.org/wiki/Brad_Fitzpatrick">Brad Fitzpatrick</a>, creator of <a href="https://en.wikipedia.org/wiki/LiveJournal">LiveJournal</a> and <a href="https://en.wikipedia.org/wiki/Memcached">Memcached</a>, appears in the book as one of the interviewees. He was only 28 years old at the time, the youngest programmer in the book and also the sweariest and most entertaining.</p><p>In response to a question about ethics in software engineering, Brad goes on an impassioned rant about input validation:</p><blockquote><p>I would like to ask that everyone is consistent on their credit-card forms to like let me put in fucking spaces or hypens. Computers are good at removing that shit. Like don’t tell me how to format my numbers.</p><p>-Brad Fitzpatrick, in <em>Coders at Work</em></p></blockquote><p>I think back to this quote whenever I try to paste a phone number into a web form, and it whines that parentheses or spaces aren’t allowed. Or worse, it truncates my phone number because of the parentheses, and <em>also</em> complains that parentheses aren’t allowed.</p><p>Whenever I create input fields in my software and think about unexpected characters, I hear Brad Fitzpatrick say, “Computers are good at removing that shit.”</p></article></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[dgsh – Directed Graph Shell (141 pts)]]></title>
            <link>https://www2.dmst.aueb.gr/dds/sw/dgsh/</link>
            <guid>45425298</guid>
            <pubDate>Tue, 30 Sep 2025 13:39:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/">https://www2.dmst.aueb.gr/dds/sw/dgsh/</a>, See on <a href="https://news.ycombinator.com/item?id=45425298">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<!-- About {{{1
================================================== -->
<section id="intro">
  
<p>
The directed graph shell, <em>dgsh</em>
(<a href="https://en.wiktionary.org/wiki/Appendix:English_pronunciation">pronounced</a> /dæɡʃ/ — <em>dagsh</em>),
provides an expressive way to construct
sophisticated and efficient big data set and stream processing pipelines
using existing Unix tools as well as custom-built components.
It is a Unix-style shell (based on <em>bash</em>)
allowing the specification of pipelines
with non-linear non-uniform operations.
These form a directed acyclic process graph,
which is typically executed by multiple processor cores,
thus increasing the operation's processing throughput.
</p>
<p>
If you want to get a feeling on how <em>dgsh</em> works in practice,
skip right down to the <a href="#examples">examples</a> section.
</p>
<p>
For a more formal introduction to <em>dgsh</em> or to cite it in your work,
see:<br>
Diomidis Spinellis and Marios Fragkoulis.
<a href="http://dx.doi.org/10.1109%2FTC.2017.2695447">Extending Unix Pipelines to DAGs</a>.
<em>IEEE Transactions on Computers</em>, 2017.
doi: 10.1109/TC.2017.2695447
</p>
</section> <!-- Introduction -->

<section id="ipc"> <!-- {{{2 -->
<h2>Inter-process communication</h2>
<p>
<em>Dgsh</em> provides two new ways
for expressing inter-process communication.

</p>
<dl>
<dt>Multipipes</dt><dd> are expressed as usual Unix pipelines,
but can connect commands with more than one output or input channel.
As an example, the <code>comm</code> command supplied with <em>dgsh</em>
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the <code>comm</code> command to the
<code>cat</code> command supplied with <em>dgsh</em>
will make the three outputs appear in sequence,
while connecting it to the
<code>paste</code> command supplied with <em>dgsh</em>
will make the output appear in its customary format.
</dd>

<dt>Multipipe blocks {{ ... }}</dt><dd>
a) send (multiple) input streams
received on their input side to the asynchronously-running
processes that reside within the block, and,
b) pass the output produced by the processes within the block as
(multiple) streams on their output side.
Multipipe blocks typically receive input from more than one channel
and produce more than one output channel.
For example, a multipipe block that runs <code>md5sum</code> and <code>wc -c</code>
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
<em>dgsh</em>-aware version of <code>tee</code> and collected by
<em>dgsh</em>-aware versions of programs such as
<code>cat</code> and <code>paste</code>.
</dd>

<dt>Stored values</dt> <dd>offer a convenient way for communicating
computed values between arbitrary processes on the graph.
They allow the storage of a data stream's
last record into a named buffer.
This record can be later retrieved asynchronously by one or more readers.
Data in a stored value can be piped into a process or out of it, or it can be read
using the shell's command output substitution syntax.
Stored values are implemented internally through Unix-domain sockets,
a background-running store program, <code>dgsh-writeval</code>, and
a reader program, <code>dgsh-readval</code>.
The behavior of a stored value's IO can be modified by adding flags to
<code>dgsh-writeval</code> and <code>dgsh-readval</code>.
</dd>
</dl>
</section>

<section id="syntax"> <!-- {{{2 -->
<h2>Syntax</h2>
<p>
A <em>dgsh</em> script follows the syntax of a <em>bash</em>(1) shell
script with the addition of <em>multipipe</em> blocks.
A multipipe block contains one or more <em>dgsh</em> simple commands,
other multipipe blocks, or pipelines of the previous two types of commands.
The commands in a multipipe block
are executed asynchronously (in parallel, in the background).
Data may be redirected or piped into and out of a multipipe block.
With multipipe blocks <em>dgsh</em> scripts form directed acyclic process graphs.
It follows from the above description that
multipipe blocks can be recursively composed.
</p>

<p>
As a simple example consider running the following command
directly within <em>dgsh</em>
</p>
<pre>{{ echo hello &amp; echo world &amp; }} | paste
</pre>
<p>
or by invoking <code>dgsh</code> with the command as an argument.
</p>
<pre>dgsh -c '{{ echo hello &amp; echo world &amp; }} | paste'
</pre>
<p>
The command will run <em>paste</em> with input from the two
<em>echo</em> processes to output <code>hello world</code>.
This is equivalent to running the following <em>bash</em> command,
but with the flow of data appearing in the natural left-to-right order.
</p>
<pre>paste &lt;(echo hello) &lt;(echo world)
</pre>

<p>
In the following larger example, which compares the performance of
different compression utilities, the script's standard input
is distributed to
three compression utilities (<em>xz</em>, <em>bzip2</em>, and <em>gzip</em>),
to assess their performance, and also to
<em>file</em> and <em>wc</em> to report the input data's type and size.
The <em>printf</em> commands label the data of each processing type.
All eight commands pass their output
to the <code>cat</code> command, which gathers their outputs
in order.
</p>

<pre>tee |
{{
	printf 'File type:\t'
	file -

	printf 'Original size:\t'
	wc -c

	printf 'xz:\t\t'
	xz -c | wc -c

	printf 'bzip2:\t\t'
	bzip2 -c | wc -c

	printf 'gzip:\t\t'
	gzip -c | wc -c
}} |
cat
</pre>

<p>
Formally, <em>dgsh</em> extends the syntax of the (modified) Unix Bourne-shell
when <code>bash</code> provided with the <code>--dgsh</code> argument
as follows.
</p>

<pre>&lt;dgsh_block&gt;     ::= '{{' &lt;dgsh_list&gt; '}}'

&lt;dgsh_list&gt;      ::= &lt;dgsh_list_item&gt; '&amp;'
                 &lt;dgsh_list_item&gt; &lt;dgsh_list&gt;

&lt;dgsh_list_item&gt; ::= &lt;simple_command&gt;
                 &lt;dgsh_block&gt;
                 &lt;dgsh_list_item&gt; '|' &lt;dgsh_list_item&gt;
</pre>
</section> <!-- syntax -->

<section id="tools"> <!-- {{{2 -->
<h2>Adapted tools</h2>
<p>
A number of Unix tools have been adapted to support multiple inputs
and outputs to match their natural capabilities.
This echoes a similar adaptation that was performed in the early
1970s when Unix and the shell got pipes and the pipeline syntax.
Many programs that worked with files were adjusted to work as filters.
The number of input and output channels of <em>dgsh</em>-compatible commands are
as follows, based on the supplied command-line arguments.
</p>
<table>
	<tbody><tr>
		<th>Tool</th>
		<th>Inputs</th>
		<th>Outputs</th>
		<th>Notes</th>
	</tr>
	<tr>
		<td>cat (<em>dgsh-tee</em>)</td>
		<td>0—N</td>
		<td>0—M</td>
		<td>No options are supported</td>
	</tr>
	<tr>
		<td>cmp</td>
		<td>0—2</td>
		<td>0—1</td>
		<td></td>
	</tr>
	<tr>
		<td>comm</td>
		<td>0—2</td>
		<td>0—3</td>
		<td>Output streams in order: lines only in first file, lines only in second one, and lines in both files</td>
	</tr>
	<tr>
		<td>cut</td>
		<td>0—1</td>
		<td>1—N</td>
		<td>With <code>--multistream</code> output each range into a different stream</td>
	</tr>
	<tr>
		<td>diff</td>
		<td>0—N</td>
		<td>1</td>
		<td>Typically two inputs. Compare an arbitrary number of input streams with the <code>--from-file</code> or <code>--to-file</code> options</td>
	</tr>
	<tr>
		<td>diff3</td>
		<td>0—3</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td>grep</td>
		<td>0—2</td>
		<td>0—4</td>
		<td>Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines</td>
	</tr>
	<tr>
		<td>join</td>
		<td>0—2</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td>paste</td>
		<td>0—N</td>
		<td>1</td>
		<td>Paste N input streams</td>
	</tr>
	<tr>
		<td>perm</td>
		<td>1—N</td>
		<td>1—N</td>
		<td>Rearrange the order of N input streams</td>
	</tr>
	<tr>
		<td>sort</td>
		<td>0—N</td>
		<td>0—1</td>
		<td>With the <code>-m</code> option, merge sort N input streams</td>
	</tr>
	<tr>
		<td>tee (<em>dgsh-tee</em>)</td>
		<td>0—N</td>
		<td>0—M</td>
		<td>Only the <code>-a</code> option is supported</td>
	</tr>
	<tr>
		<td>dgsh-readval</td>
		<td>0</td>
		<td>1</td>
		<td>Read a value from a socket</td>
	</tr>
	<tr>
		<td>dgsh-wrap</td>
		<td>0—N</td>
		<td>0—1</td>
		<td>Wrap non-dgsh commands and negotiate on their behalf</td>
	</tr>
	<tr>
		<td>dgsh-writeval</td>
		<td>1</td>
		<td>0</td>
		<td>Write a value to a socket</td>
	</tr>
</tbody></table>

<p>
In addition, POSIX user commands that receive no input
or only generate no output, when executed in a <em>dgsh</em> context
are wrapped to specify the corresponding input or output capability.
For example, an <code>echo</code> command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default <code>dgsh</code> automatically wraps all other
commands as filters.
</p><dl>
<dt> Input-only </dt><dd>
read,
write.
</dd>
<dt> Output-only </dt><dd> </dd>
alias,
ar,
basename,
c99,
cal,
cflow,
command,
date,
df,
dirname,
du,
echo,
expr,
find,
getopts,
ipcrm,
jobs,
ls,
make,
man,
printf,
ps,
pwd,
tty,
type,
ulimit,
umask,
uname,
what,
who.
</dl>
<dt> No input and output </dt><dd> </dd>
dbg,
dcd,
dchgrp,
dchmod,
dchown,
dcp,
denv,
dfalse,
dfg,
dkill,
dlink,
dln,
dmesg,
dmkdir,
dmkfifo,
dmv,
dnewgrp,
dnice,
dnohup,
drenice,
drm,
drmdir,
dsleep,
dstrip,
dtest,
dtouch,
dtrue,
dunalias,
dunlink,
dwait,
dyacc.


<p>
Finally, note that any <em>dgsh</em> script will accept and generate
the number of inputs and outputs associated with the commands or
multipipe blocks at its two endpoints.
</p>
</section> <!-- Adapted tools -->


<!-- Downloading and installation {{{1
================================================== -->
<section id="download">
  

<p>
The <em>dgsh</em> suite has been tested under
Debian and Ubuntu Linux, FreeBSD, and Mac OS X.
A Cygwin port is underway.
</p>

<p>
An installation of <a href="http://www.graphviz.org/">GraphViz</a>
will allow you to visualize the <em>dgsh</em> graphs that you specify
in your programs.
</p>

    <section id="debian"> <!-- {{{2 -->
    <h2>Debian and Ubuntu GNU/Linux</h2>
    <section>
    <h3>Prerequisites</h3>
<p>
To compile and run <em>dgsh</em> you will need to have the following commands
installed on your system:
</p><pre>make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf 
git xz-utils gettext
</pre>

To test <em>dgsh</em> you will need to have the following commands
installed in your system:
<pre>wbritish wamerican libfftw3-dev csh
curl bzip2
</pre>

    <h3>Installation steps</h3>
<p>
Go through the following steps.
</p><ol>
<li>
Recursively clone the project's source code through its
<a href="https://github.com/dspinellis/dgsh">GitHub page</a>.
<pre>git clone --recursive https://github.com/dspinellis/dgsh.git
</pre>
</li>
<li>
Configure <em>bash</em> and the Unix tools adapted for <em>dgsh</em>.
<pre>make config
</pre>
</li>
<li>
Compile all programs.
<pre>make
</pre>
</li>
<li>
Install.
<pre>sudo make install
</pre>
</li>
</ol>

<p>
By default, the program and its documentation are installed under
<code>/usr/local</code>.
You can modify this by setting the <code>PREFIX</code> variable
in the `config` step, for example:
</p><pre>make PREFIX=$HOME config
make
make install
</pre>


    <h3>Testing</h3>
<p>

Issue the following command.
</p><pre>make test
</pre>


    </section>
    <section id="freebsd"> <!-- {{{2 -->
    <h2>FreeBSD</h2>
    <section>
    <h3>Prerequisites</h3>
<p>
To compile and run <em>dgsh</em> you will need to have the following packages
installed in your system:
</p><pre>devel/automake
devel/bison
devel/check
devel/git
devel/gmake
devel/gperf
misc/help2man
print/texinfo
shells/bash
</pre>

To test <em>dgsh</em> you will need to have the following ports
installed on your system:
<pre>archivers/bzip2
ftp/curl
</pre>

    <h3>Installation steps</h3>
<p>
Go through the following steps.
</p><ol>
<li>
Recursively clone the project's source code through its
<a href="https://github.com/dspinellis/dgsh">GitHub page</a>.
<pre>git clone --recursive https://github.com/dspinellis/dgsh.git
</pre>
</li>
<li>
Configure <em>bash</em> and the Unix tools adapted for <em>dgsh</em>.
<pre>gmake config
</pre>
</li>
<li>
Compile all programs.
<pre>gmake
</pre>
</li>
<li>
Install.
<pre>sudo gmake install
</pre>
</li>
</ol>

<p>
By default, the program and its documentation are installed under
<code>/usr/local</code>.
You can modify this by setting the <code>PREFIX</code> variable
in the `config` step, for example:
</p><pre>gmake PREFIX=$HOME config
gmake
gmake install
</pre>


    <h3>Testing</h3>
<p>

Issue the following command.
</p><pre>gmake test
</pre>


    </section>
</section>
</section>

<!-- Reference {{{1
================================================== -->
<section id="reference">
  
<p>
These are the manual pages for <em>dgsh</em>, the associated helper programs
and the API
in formats suitable for browsing and printing.
The commands are listed in the order of usefulness in everyday scenarios.
</p>
<dl>
<dt> dgsh </dt><dd> directed graph shell <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh.pdf">PDF</a></dd>
<dt> dgsh-tee </dt><dd> buffer and copy or scatter standard input to one or more sinks <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-tee.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-tee.pdf">PDF</a></dd>
<dt> dgsh-wrap </dt><dd> allow any filter program to participate in an dgsh pipeline <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-wrap.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-wrap.pdf">PDF</a></dd>
<dt> dgsh-writeval </dt><dd> write values to a data store <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-writeval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-writeval.pdf">PDF</a></dd>
<dt> dgsh-readval </dt><dd> data store client <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-readval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-readval.pdf">PDF</a></dd>
<dt> dgsh-monitor </dt><dd> monitor data on a pipe <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-monitor.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-monitor.pdf">PDF</a></dd>
<dt> dgsh-parallel </dt><dd> create a semi-homongeneous dgsh parallel processing block <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-parallel.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-parallel.pdf">PDF</a></dd>
<dt> perm </dt><dd> permute inputs to outputs <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/perm.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/perm.pdf">PDF</a></dd>
<dt> dgsh-httpval </dt><dd> provide data store values through HTTP <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-httpval.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-httpval.pdf">PDF</a></dd>
<dt> dgsh-merge-sum </dt><dd> merge key value pairs, summing the values <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-merge-sum.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-merge-sum.pdf">PDF</a></dd>
<dt> dgsh-conc </dt><dd> input or output pipe concentrator for <em>dgsh</em> negotiation (used internally) <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-conc.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-conc.pdf">PDF</a></dd>
<dt> dgsh-enumerate </dt><dd> enumerate an arbitrary number of output channels (demonstration and <a href="http://istlab.dmst.aueb.gr/~dds/dgsh-egg.sh">debugging</a> tool) <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-enumerate.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh-enumerate.pdf">PDF</a></dd>
<dt> dgsh_negotiate </dt><dd> API for <em>dgsh</em>-compatible
programs to specify and obtain dgsh I/O file descriptors
<a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh_negotiate.html">HTML</a>, <a href="https://www2.dmst.aueb.gr/dds/sw/dgsh/dgsh_negotiate.pdf">PDF</a></dd>
</dl>
</section>


<!--</th>
<th>Examples</th>
<th>{{{1
==================================================</th>
<th>-->
<section id="examples">
  

<section id="compress-compare"> <!-- {{{2 -->
<h2>Compression benchmark</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/compress-compare-pretty.png" alt="Compression benchmark">
<!-- Extracted description -->
<p>
Report file type, length, and compression performance for
data received from the standard input.  The data never touches the
disk.
Demonstrates the use of an output multipipe to source many commands
from one followed by an input multipipe to sink to one command
the output of many and the use of dgsh-tee that is used both to
propagate the same input to many commands and collect output from
many commands orderly in a way that is transparent to users.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

tee |
{{
	printf 'File type:\t'
	file -

	printf 'Original size:\t'
	wc -c

	printf 'xz:\t\t'
	xz -c | wc -c

	printf 'bzip2:\t\t'
	bzip2 -c | wc -c

	printf 'gzip:\t\t'
	gzip -c | wc -c
}} |
cat
</pre>
<section id="commit-stats"> <!-- {{{2 -->
<h2>Git commit statistics</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/commit-stats-pretty.png" alt="Git commit statistics">
<!-- Extracted description -->
<p>
Process the Git history, and list the authors and days of the week
ordered by the number of their commits.
Demonstrates streams and piping through a function.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

forder()
{
	sort |
	uniq -c |
	sort -rn
}

git log --format="%an:%ad" --date=default "$@" |
tee |
{{
	echo "Authors ordered by number of commits"
	# Order by frequency
	awk -F: '{print $1}' |
	forder

	echo "Days ordered by number of commits"
	# Order by frequency
	awk -F: '{print substr($2, 1, 3)}' |
	forder
}} |
cat
</pre>
<section id="code-metrics"> <!-- {{{2 -->
<h2>C code metrics</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/code-metrics-pretty.png" alt="C code metrics">
<!-- Extracted description -->
<p>
Process a directory containing C source code, and produce a summary
of various metrics.
Demonstrates nesting, commands without input.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

{{
	# C and header code
	find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 |
	tee |
	{{

		# Average file name length
		# Convert to newline separation for counting
		echo -n 'FNAMELEN: '
		tr \\0 \\n |
		# Remove path
		sed 's|^.*/||' |
		# Maintain average
		awk '{s += length($1); n++} END {
			if (n&gt;0)
				print s / n;
			else
				print 0; }'

		xargs -0 /bin/cat |
		tee |
		{{
			# Remove strings and comments
			sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" |
			cpp -P |
			tee |
			{{
				# Structure definitions
				echo -n 'NSTRUCT: '
				egrep -c 'struct[   ]*{|struct[   ]*[a-zA-Z_][a-zA-Z0-9_]*[       ]*{'
				#}} (match preceding openings)

				# Type definitions
				echo -n 'NTYPEDEF: '
				grep -cw typedef

				# Use of void
				echo -n 'NVOID: '
				grep -cw void

				# Use of gets
	  			echo -n 'NGETS: '
	  			grep -cw gets

				# Average identifier length
				echo -n 'IDLEN: '
				tr -cs 'A-Za-z0-9_' '\n' |
				sort -u |
				awk '/^[A-Za-z]/ { len += length($1); n++ } END {
					if (n&gt;0)
						print len / n;
					else
						print 0; }'
			}}

			# Lines and characters
			echo -n 'CHLINESCHAR: '
			wc -lc |
			awk '{OFS=":"; print $1, $2}'

			# Non-comment characters (rounded thousands)
			# -traditional avoids expansion of tabs
			# We round it to avoid failing due to minor
			# differences between preprocessors in regression
			# testing
			echo -n 'NCCHAR: '
			sed 's/#/@/g' |
			cpp -traditional -P |
			wc -c |
			awk '{OFMT = "%.0f"; print $1/1000}'

			# Number of comments
			echo -n 'NCOMMENT: '
			egrep -c '/\*|//'

			# Occurences of the word Copyright
			echo -n 'NCOPYRIGHT: '
			grep -ci copyright
		}}
	}}

	# C files
	find "$@" -name \*.c -type f -print0 |
	tee |
	{{
		# Convert to newline separation for counting
		tr \\0 \\n |
		tee |
		{{
			# Number of C files
			echo -n 'NCFILE: '
			wc -l

			# Number of directories containing C files
			echo -n 'NCDIR: '
			sed 's,/[^/]*$,,;s,^.*/,,' |
			sort -u |
			wc -l
		}}

		# C code
		xargs -0 /bin/cat |
		tee |
		{{
			# Lines and characters
			echo -n 'CLINESCHAR: '
			wc -lc |
			awk '{OFS=":"; print $1, $2}'

			# C code without comments and strings
			sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" |
			cpp -P |
			tee |
			{{
				# Number of functions
				echo -n 'NFUNCTION: '
				grep -c '^{'

				# Number of gotos
				echo -n 'NGOTO: '
				grep -cw goto

				# Occurrences of the register keyword
				echo -n 'NREGISTER: '
				grep -cw register

				# Number of macro definitions
				echo -n 'NMACRO: '
				grep -c '@[   ]*define[   ][   ]*[a-zA-Z_][a-zA-Z0-9_]*('
				# Number of include directives
				echo -n 'NINCLUDE: '
				grep -c '@[   ]*include'

				# Number of constants
				echo -n 'NCONST: '
				grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l

			}}
		}}
	}}

	# Header files
	echo -n 'NHFILE: '
	find "$@" -name \*.h -type f |
	wc -l

}} |
# Gather and print the results
cat
</pre>
<section id="duplicate-files"> <!-- {{{2 -->
<h2>Find duplicate files</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/duplicate-files-pretty.png" alt="Find duplicate files">
<!-- Extracted description -->
<p>
List the names of duplicate files in the specified directory.
Demonstrates the combination of streams with a relational join.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Create list of files
find "$@" -type f |

# Produce lines of the form
# MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7
xargs openssl md5 |

# Convert each line into a "filename md5sum" pair
sed 's/^MD5(//;s/)= / /' |

# Sort by MD5 sum
sort -k2 |

tee |
{{

	# Print an MD5 sum for each file that appears more than once
	awk '{print $2}' | uniq -d

	# Promote the stream to gather it
	cat
}} |
# Join the repeated MD5 sums with the corresponding file names
# Join expects two inputs, second will come from scatter
# XXX make streaming input identifiers transparent to users
join -2 2 |

# Output same files on a single line
awk '
BEGIN {ORS=""}
$1 != prev &amp;&amp; prev {print "\n"}
END {if (prev) print "\n"}
{if (prev) print " "; prev = $1; print $2}'
</pre>
<section id="spell-highlight"> <!-- {{{2 -->
<h2>Highlight misspelled words</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/spell-highlight-pretty.png" alt="Highlight misspelled words">
<!-- Extracted description -->
<p>
Highlight the words that are misspelled in the command's first
argument.
Demonstrates stream processing with multipipes and
the avoidance of pass-through constructs to avoid deadlocks.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

export LC_ALL=C

tee |
{{
	# Find errors
	{{
		# Obtain list of words in text
		tr -cs A-Za-z \\n |
		tr A-Z a-z |
		sort -u

		# Ensure dictionary is compatibly sorted
		sort /usr/share/dict/words
	}} |
	# List errors as a set difference
	comm -23

	# Pass through text
	cat
}} |
grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2
</pre>
<section id="word-properties"> <!-- {{{2 -->
<h2>Word properties</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/word-properties-pretty.png" alt="Word properties">
<!-- Extracted description -->
<p>
Read text from the standard input and list words
containing a two-letter palindrome, words containing
four consonants, and words longer than 12 characters.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Consistent sorting across machines
export LC_ALL=C

# Stream input from file
cat $1 |

# Split input one word per line
tr -cs a-zA-Z \\n |
# Create list of unique words
sort -u |
tee |
{{
	# Pass through the original words
	cat

	# List two-letter palindromes
	sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t
		g'

	# List four consecutive consonants
	sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t
		g'

	# List length of words longer than 12 characters
	awk '{if (length($1) &gt; 12) print "l:", length($1);
		else print ""}'
}} |
# Paste the four streams side-by-side
paste |
# List only words satisfying one or more properties
fgrep :
</pre>
<section id="web-log-report"> <!-- {{{2 -->
<h2>Web log reporting</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/web-log-report-pretty.png" alt="Web log reporting">
<!-- Extracted description -->
<p>
Creates a report for a fixed-size web log file read from the standard input.
Demonstrates the combined use of multipipe blocks, writeval and readval
to store and retrieve values, and functions in the scatter block.
Used to measure throughput increase achieved through parallelism.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Output the top X elements of the input by the number of their occurrences
# X is the first argument
toplist()
{
	uniq -c | sort -rn | head -$1
	echo
}

# Output the argument as a section header
header()
{
	echo
	echo "$1"
	echo "$1" | sed 's/./-/g'
}

# Consistent sorting
export LC_ALL=C

export -f toplist
export -f header


if [ -z "${DGSH_DRAW_EXIT}" ]
then
cat &lt;&lt;EOF
			WWW server statistics
			=====================

Summary
-------
EOF
fi

tee |
{{
	# Number of accesses
	echo -n 'Number of accesses: '
	dgsh-readval -l -s nAccess

	# Number of transferred bytes
	awk '{s += $NF} END {print s}' |
	tee |
	{{
		echo -n 'Number of Gbytes transferred: '
		awk '{print $1 / 1024 / 1024 / 1024}'

		dgsh-writeval -s nXBytes
	}}

	echo -n 'Number of hosts: '
	dgsh-readval -l -q -s nHosts

	echo -n 'Number of domains: '
	dgsh-readval -l -q -s nDomains

	echo -n 'Number of top level domains: '
	dgsh-readval -l -q -s nTLDs

	echo -n 'Number of different pages: '
	dgsh-readval -l -q -s nUniqPages

	echo -n 'Accesses per day: '
	dgsh-readval -l -q -s nDayAccess

	echo -n 'MBytes per day: '
	dgsh-readval -l -q -s nDayMB

	# Number of log file bytes
	echo -n 'MBytes log file size: '
	wc -c |
	awk '{print $1 / 1024 / 1024}'

	# Host names
	awk '{print $1}' |
	tee |
	{{
		# Number of accesses
		wc -l | dgsh-writeval -s nAccess

		# Sorted hosts
		sort |
		tee |
		{{

			# Unique hosts
			uniq |
			tee |
			{{
				# Number of hosts
				wc -l | dgsh-writeval -s nHosts

				# Number of TLDs
				awk -F. '$NF !~ /[0-9]/ {print $NF}' |
				sort -u |
				wc -l |
				dgsh-writeval -s nTLDs
			}}

			# Top 10 hosts
			{{
				 call 'header "Top 10 Hosts"'
				 call 'toplist 10'
			}}
		}}

		# Top 20 TLDs
		{{
			call 'header "Top 20 Level Domain Accesses"'
			awk -F. '$NF !~ /^[0-9]/ {print $NF}' |
			sort |
			call 'toplist 20'
		}}

		# Domains
		awk -F. 'BEGIN {OFS = "."}
		            $NF !~ /^[0-9]/ {$1 = ""; print}' |
		sort |
		tee |
		{{
			# Number of domains
			uniq |
			wc -l |
			dgsh-writeval -s nDomains

			# Top 10 domains
			{{
				 call 'header "Top 10 Domains"'
				 call 'toplist 10'
			}}
		}}
	}}

	# Hosts by volume
	{{
		call 'header "Top 10 Hosts by Transfer"'
		awk '    {bytes[$1] += $NF}
		END {for (h in bytes) print bytes[h], h}' |
		sort -rn |
		head -10
	}}

	# Sorted page name requests
	awk '{print $7}' |
	sort |
	tee |
	{{

		# Top 20 area requests (input is already sorted)
		{{
			 call 'header "Top 20 Area Requests"'
			 awk -F/ '{print $2}' |
			 call 'toplist 20'
		}}

		# Number of different pages
		uniq |
		wc -l |
		dgsh-writeval -s nUniqPages

		# Top 20 requests
		{{
			 call 'header "Top 20 Requests"'
			 call 'toplist 20'
		}}
	}}

	# Access time: dd/mmm/yyyy:hh:mm:ss
	awk '{print substr($4, 2)}' |
	tee |
	{{

		# Just dates
		awk -F: '{print $1}' |
		tee |
		{{

			# Number of days
			uniq |
			wc -l |
			tee |
			{{
				awk '
					BEGIN {
					"dgsh-readval -l -x -s nAccess" | getline NACCESS;}
					{print NACCESS / $1}' |
				dgsh-writeval -s nDayAccess

				awk '
					BEGIN {
					"dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;}
					{print NXBYTES / $1 / 1024 / 1024}' |
				dgsh-writeval -s nDayMB
			}}

			{{
				 call 'header "Accesses by Date"'
				 uniq -c
			}}

			# Accesses by day of week
			{{
				 call 'header "Accesses by Day of Week"'
				 sed 's|/|-|g' |
				 call '(date -f - +%a 2&gt;/dev/null || gdate -f - +%a)' |
				 sort |
				 uniq -c |
				 sort -rn
			}}
		}}

		# Hour
		{{
			call 'header "Accesses by Local Hour"'
			awk -F: '{print $2}' |
			sort |
			uniq -c
		}}
	}}
	dgsh-readval -q -s nAccess
}} |
cat
</pre>
<section id="text-properties"> <!-- {{{2 -->
<h2>Text properties</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/text-properties-pretty.png" alt="Text properties">
<!-- Extracted description -->
<p>
Read text from the standard input and create files
containing word, character, digram, and trigram frequencies.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Consistent sorting across machines
export LC_ALL=C


# Convert input into a ranked frequency list
ranked_frequency()
{
	awk '{count[$1]++} END {for (i in count) print count[i], i}' |
	# We want the standard sort here
	sort -rn
}

# Convert standard input to a ranked frequency list of specified n-grams
ngram()
{
	local N=$1

	perl -ne 'for ($i = 0; $i &lt; length($_) - '$N'; $i++) {
		print substr($_, $i, '$N'), "\n";
	}' |
	ranked_frequency
}

export -f ranked_frequency
export -f ngram

tee |
{{
	# Split input one word per line
	tr -cs a-zA-Z \\n |
	tee |
	{{
		# Digram frequency
		call 'ngram 2 &gt;digram.txt'
		# Trigram frequency
		call 'ngram 3 &gt;trigram.txt'
		# Word frequency
		call 'ranked_frequency &gt;words.txt'
	}}

	# Store number of characters to use in awk below
	wc -c |
	dgsh-writeval -s nchars

	# Character frequency
	sed 's/./&amp;\
/g' |
	# Print absolute
	call 'ranked_frequency' |
	awk 'BEGIN {
		"dgsh-readval -l -x -q -s nchars" | getline NCHARS
		OFMT = "%.2g%%"}
		{print $1, $2, $1 / NCHARS * 100}' &gt; character.txt
}}
</pre>
<section id="static-functions"> <!-- {{{2 -->
<h2>C/C++ symbols that should be static</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/static-functions-pretty.png" alt="C/C++ symbols that should be static">
<!-- Extracted description -->
<p>
Given as an argument a directory containing object files, show which
symbols are declared with global visibility, but should have been
declared with file-local (static) visibility instead.
Demonstrates the use of dgsh-capable comm (1) to combine data from
two sources.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Find object files
find "$1" -name \*.o |

# Print defined symbols
xargs nm |

tee |
{{

  # List all defined (exported) symbols
  awk 'NF == 3 &amp;&amp; $2 ~ /[A-Z]/ {print $3}' | sort

  # List all undefined (imported) symbols
  awk '$1 == "U" {print $2}' | sort

}} |
# Print exports that are not imported
comm -23
</pre>
<section id="map-hierarchy"> <!-- {{{2 -->
<h2>Hierarchy map</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/map-hierarchy-pretty.png" alt="Hierarchy map">
<!-- Extracted description -->
<p>
Given two directory hierarchies A and B passed as input arguments
(where these represent a project at different parts of its lifetime)
copy the files of hierarchy A to a new directory, passed as a third
argument, corresponding to the structure of directories in B.
Demonstrates the use of <em>join</em> to process results from two
inputs and the use of <em>gather</em> to order asynchronously
produced results.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ]
then
  echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&gt;&amp;2
  exit 1
fi

NEWDIR="$3"

export LC_ALL=C

line_signatures()
{
  find $1 -type f -name '*.[chly]' -print |
  # Split path name into directory and file
  sed 's|\(.*\)/\([^/]*\)|\1 \2|' |
  while read dir file
  do
    # Print "directory filename content" of lines with
    # at least one alphabetic character
    # The fields are separated by  and 
    sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file"
  done |
  # Error: multi-character tab '\001\001'
  sort -T `pwd` -t -k 2
}


export -f line_signatures


{{
  # Generate the signatures for the two hierarchies
  call 'line_signatures "$1"' -- "$1"
  call 'line_signatures "$1"' -- "$2"
}} |

# Join signatures on file name and content
join -t -1 2 -2 2 |

# Print filename dir1 dir2
sed 's///g' |
awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' |

# Unique occurrences
sort -u |
tee |
{{
  # Commands to copy
  awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' |
  sort -u

  awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}'
}} |
# Order: first make directories, then copy files
# TODO: dgsh-tee does not pass along first incoming stream
cat |
sh
</pre>
<section id="committer-plot"> <!-- {{{2 -->
<h2>Plot Git committer activity over time</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/committer-plot-pretty.png" alt="Plot Git committer activity over time">
<!-- Extracted description -->
<p>
Process the Git history, and create two PNG diagrams depicting
committer activity over time. The most active committers appear
at the center vertical of the diagram.
Demonstrates image processing, mixining of synchronous and
asynchronous processing in a scatter block, and the use of an
dgsh-compliant join command.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Commit history in the form of ascending Unix timestamps, emails
git log --pretty=tformat:'%at %ae' |
# Filter records according to timestamp: keep (100000, now) seconds
awk 'NF == 2&amp; $1 &gt; 100000&amp; $1 &lt; '`date +%s` |
sort -n |
tee |
{{
	{{
		# Calculate number of committers
		awk '{print $2}' |
		sort -u |
		wc -l |
		tee |
		{{
			dgsh-writeval -s committers1

			dgsh-writeval -s committers2
			dgsh-writeval -s committers3
		}}

		# Calculate last commit timestamp in seconds
		tail -1 |
		awk '{print $1}'

		# Calculate first commit timestamp in seconds
		head -1 |
		awk '{print $1}'
	}} |
	# Gather last and first commit timestamp
	cat |
	# Make one space-delimeted record
	tr '\n' ' ' |
	# Compute the difference in days
	awk '{print int(($1 - $2) / 60 / 60 / 24)}' |
	# Store number of days
	dgsh-writeval -s days

	sort -k2	# &lt;timestamp, email&gt;

	# Place committers left/right of the median
	# according to the number of their commits
	awk '{print $2}' |
	sort |
	uniq -c |
	sort -n |
	awk '
		BEGIN {
			"dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS
			l = 0; r = NCOMMITTERS;}
		{print NR % 2 ? l++ : --r, $2}' |
	sort -k2	# &lt;left/right, email&gt;

}} |
# Join committer positions with commit time stamps
# based on committer email
join -j 2 |		# &lt;email, timestamp, left/right&gt;
# Order by timestamp
sort -k 2n |
tee |
{{
	# Create portable bitmap
	echo 'P1'

	{{
		dgsh-readval -l -q -s committers2
		dgsh-readval -l -q -s days
	}} |
	cat |
	tr '\n' ' ' |
	awk '{print $1, $2}'

	perl -na -e '
	  BEGIN {
	    open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3");
	    $ncommitters = &lt;$ncf&gt;;
	    @empty[$ncommitters - 1] = 0; @committers = @empty;
	  }
	  sub out {
		  print join("", map($_ ? "1" : "0", @committers)), "\n";
	  }

	  $day = int($F[1] / 60 / 60 / 24);
	  $pday = $day if (!defined($pday));

	  while ($day != $pday) {
		  out();
		  @committers = @empty;
		  $pday++;
	  }

	  $committers[$F[2]] = 1;

	  END { out(); }
	'
}} |
cat |
# Enlarge points into discs through morphological convolution
pgmmorphconv -erode &lt;(
cat &lt;&lt;EOF
P1
7 7
1 1 1 0 1 1 1
1 1 0 0 0 1 1
1 0 0 0 0 0 1
0 0 0 0 0 0 0
1 0 0 0 0 0 1
1 1 0 0 0 1 1
1 1 1 0 1 1 1
EOF
) |
tee |
{{
	# Full-scale image
	pnmtopng &gt;large.png
	# A smaller image
	pamscale -width 640 |
	pnmtopng &gt;small.png
}}
</pre>
<section id="parallel-word-count"> <!-- {{{2 -->
<h2>Parallel word count</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/parallel-word-count-pretty.png" alt="Parallel word count">
<!-- Extracted description -->
<p>
Count number of times each word appears in the specified input file(s)
Demonstrates parallel execution mirroring the Hadoop WordCount example
via the dgsh-parallel command.
In contrast to GNU parallel, the block generated by dgsh-parallel
has N input and output streams, which can be combined by any
dgsh-compatible tool, such as dgsh-merge-sum or sort -m.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Number of processes
N=8

# Collation order for sorting
export LC_ALL=C

# Scatter input
dgsh-tee -s |
# Emulate Java's default StringTokenizer, sort, count
dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" |
# Merge sorted counts by providing N input channels
dgsh-merge-sum $(for i in $(seq $N) ; do printf '&lt;| ' ; done)
</pre>

<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/author-compare-pretty.png" alt="Venue author compare">
<!-- Extracted description -->
<p>
Given the specification of two publication venues, read a compressed
DBLP computer science bibliography from the standard input (e.g. piped
from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally
cached copy) and output the number of papers published in each of the
two venues as well as the number of authors who have published only in
the first venue, the number who have published only in the second one,
and authors who have published in both.  The venues are specified through
the script's first two command-line arguments as a DBLP key prefix, e.g.
journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/.
Demonstrates the use of dgsh-wrap -e to have sed(1) create two output
streams and the use of tee to copy a pair of streams into four ones.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# Extract and sort author names
sorted_authors()
{
  sed -n 's/&lt;author&gt;\([^&lt;]*\)&lt;\/author&gt;/\1/p' |
  sort
}

# Escape a string to make it a valid sed(1) pattern
escape()
{
  echo "$1" | sed 's/\([/\\]\)/\\\1/g'
}

export -f sorted_authors

if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then
  echo "Usage: $0 key1 key2" 1&gt;&amp;2
  echo "Example: $0 conf/icse/ journals/software/" 1&gt;&amp;2
  exit 1
fi

gzip -dc |
# Output the two venue authors as two output streams
dgsh-wrap -e sed -n "
/^&lt;.*key=\"$(escape $1)/,/&lt;title&gt;/ w &gt;|
/^&lt;.*key=\"$(escape $2)/,/&lt;title&gt;/ w &gt;|" |
# 2 streams in 4 streams out: venue1, venue2, venue1, venue2
tee |
{{
  {{
    echo -n "$1 papers: "
    grep -c '^&lt;.* mdate=.* key='
    echo -n "$2 papers: "
    grep -c '^&lt;.* mdate=.* key='
  }}

  {{
    call sorted_authors
    call sorted_authors
  }} |
  comm |
  {{
    echo -n "Authors only in $1: "
    wc -l
    echo -n "Authors only in $2: "
    wc -l
    echo -n 'Authors common in both venues: '
    wc -l
  }}
}} |
cat
</pre>
<section id="ft2d"> <!-- {{{2 -->
<h2>Waves: 2D Fourier transforms</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/ft2d-pretty.png" alt="Waves: 2D Fourier transforms">
<!-- Extracted description -->
<p>
Create two graphs:
1) a broadened pulse and the real part of its 2D Fourier transform, and
2) a simulated air wave and the amplitude of its 2D Fourier transform.
Demonstrates using the tools of the Madagascar shared research environment
for computational data analysis in geophysics and related fields.
Also demonstrates the use of two scatter blocks in the same script,
and the used of named streams.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

mkdir -p Fig

# The SConstruct SideBySideIso "Result" method
side_by_side_iso()
{
	vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $*
}

export -f side_by_side_iso

# A broadened pulse and the real part of its 2D Fourier transform
sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \
	label1='time' label2='space' unit1= unit2= |
sfsmooth rect2=2 |
sfsmooth rect2=2 |
tee |
{{
	sfgrey pclip=100 wanttitle=n

	sffft1 |
	sffft3 axis=2 pad=1 |
	sfreal |
	tee |
	{{
		sfwindow f1=1 | sfreverse which=3
		cat
	}} |
	sfcat axis=1 "&lt;|" |
	sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space"
}} |
call_with_stdin side_by_side_iso '&lt;|' yscale=1.25 &gt;Fig/ft2dofpulse.vpl

# A simulated air wave and the amplitude of its 2D Fourier transform
sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \
	label1='time' unit1= |
sfspray n=32 d=1 o=0 |
sfput label2=space |
sflmostretch delay=0 v0=-1 |
tee |
{{
	sfwindow f2=1 | sfreverse which=2
	cat
}} |
sfcat axis=2 "&lt;|" |
tee |
{{
	sfgrey pclip=100 wanttitle=n

	sffft1 |
	sffft3 sign=1 |
	tee |
	{{
		sfreal
		sfimag
	}} |
	dgsh-wrap -e sfmath nostdin=y re="&lt;|" im="&lt;|" \
	  output="sqrt(re*re+im*im)" |
	tee |
	{{
		sfwindow f1=1 | sfreverse which=3
		cat
	}} |
	sfcat axis=1 "&lt;|" |
	sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space"
}} |
call_with_stdin side_by_side_iso '&lt;|' yscale=1.25 &gt;Fig/airwave.vpl

wait
</pre>
<section id="NMRPipe"> <!-- {{{2 -->
<h2>Nuclear magnetic resonance processing</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/NMRPipe-pretty.png" alt="Nuclear magnetic resonance processing">
<!-- Extracted description -->
<p>
Nuclear magnetic resonance in-phase/anti-phase channel conversion and
processing in heteronuclear single quantum coherence spectroscopy.
Demonstrate processing of NMR data using the NMRPipe family of programs.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

# The conversion is configured for the following file:
# http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid
var2pipe -in $1            \
 -xN            1280            -yN     256    \
 -xT            640             -yT     128    \
 -xMODE         Complex -yMODE  Complex      \
 -xSW           8000    -ySW    6000      \
 -xOBS          599.4489584     -yOBS   60.7485301      \
 -xCAR          4.73    -yCAR   118.000      \
 -xLAB          1H      -yLAB   15N      \
 -ndim          2       -aq2D   States      \
-verb  |
tee |
{{
  # IP/AP channel conversion
  # See http://tech.groups.yahoo.com/group/nmrpipe/message/389
  nmrPipe |
  nmrPipe -fn SOL |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 177 -p1 0.0 -di |
  nmrPipe -fn EXT -left -sw -verb |
  nmrPipe -fn TP |
  nmrPipe -fn COADD -cList 1 0 -time |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 0 -p1 0 -di |
  nmrPipe -fn TP |
  nmrPipe -fn POLY -auto -verb &gt;A

  nmrPipe |
  nmrPipe -fn SOL |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 177 -p1 0.0 -di |
  nmrPipe -fn EXT -left -sw -verb |
  nmrPipe -fn TP |
  nmrPipe -fn COADD -cList 0 1 -time |
  nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 |
  nmrPipe -fn ZF -auto |
  nmrPipe -fn FT |
  nmrPipe -fn PS -p0 -90 -p1 0 -di |
  nmrPipe -fn TP |
  nmrPipe -fn POLY -auto -verb &gt;B

}}

# We use temporary files rather than streams, because
# addNMR mmaps its input files. The diagram displayed in the
# example shows the notional data flow.
if [ -z "${DGSH_DRAW_EXIT}" ]
then
	addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add
	addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub
fi
</pre>
<section id="fft-block8"> <!-- {{{2 -->
<h2>FFT calculation</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/fft-block8-pretty.png" alt="FFT calculation">
<!-- Extracted description -->
<p>
Calculate the iterative FFT for n = 8 in parallel.
Demonstrates combined use of permute and multipipe blocks.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

dgsh-fft-input $1 |
perm 1,5,3,7,2,6,4,8 |
{{
	{{
		dgsh-w 1 0
		dgsh-w 1 0
	}} |
	perm 1,3,2,4 |
	{{
		dgsh-w 2 0
		dgsh-w 2 1
	}}

	{{
		dgsh-w 1 0
		dgsh-w 1 0
	}} |
	perm 1,3,2,4 |
	{{
		dgsh-w 2 0
		dgsh-w 2 1
	}}
}} |
perm 1,5,3,7,2,6,4,8 |
{{
	dgsh-w 3 0

	dgsh-w 3 1

	dgsh-w 3 2

	dgsh-w 3 3
}} |
perm 1,5,2,6,3,7,4,8 |
cat
</pre>
<section id="reorder-columns"> <!-- {{{2 -->
<h2>Reorder columns</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/reorder-columns-pretty.png" alt="Reorder columns">
<!-- Extracted description -->
<p>
Reorder columns in a CSV document.
Demonstrates the combined use of tee, cut, and paste.
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

tee |
{{
	cut -d , -f 5-6 -

	cut -d , -f 2-4 -
}} |
paste -d ,
</pre>
<section id="dir"> <!-- {{{2 -->
<h2>Directory listing</h2>
</section>
<img src="https://www2.dmst.aueb.gr/dds/sw/dgsh/dir-pretty.png" alt="Directory listing">
<!-- Extracted description -->
<p>
Windows-like DIR command for the current directory.
Nothing that couldn't be done with <code>ls -l | awk</code>.
Demonstrates use of wrapped commands with no input (df, echo).
</p>
<!-- Extracted code -->
<pre>#!/usr/bin/env dgsh

ls -n |
tee |
{{
	# Reorder fields in DIR-like way
	awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}'

	# Count number of files
	wc -l | tr -d \\n

	# Print label for number of files
	echo -n ' File(s) '

	# Tally number of bytes
	awk '{s += $5} END {printf("%d bytes\n", s)}'

	# Count number of directories
	grep -c '^d' | tr -d \\n

	# Print label for number of dirs and calculate free bytes
	df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}'
}} |
cat
</pre>

</section> <!-- Examples -->

</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An opinionated critique of Duolingo (134 pts)]]></title>
            <link>https://isomorphism.xyz/blog/2025/duolingo/</link>
            <guid>45425061</guid>
            <pubDate>Tue, 30 Sep 2025 13:19:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isomorphism.xyz/blog/2025/duolingo/">https://isomorphism.xyz/blog/2025/duolingo/</a>, See on <a href="https://news.ycombinator.com/item?id=45425061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
    <p>During the stay-at-home grim days of 2020, I started learning Spanish on Duolingo. Having a working understanding of Spanish seemed like a sensible first step towards opening a taco truck in Mexico, in case I had to run away from my doctoral studies. This July, after about 5 years I decided to end the 1800 day streak that I managed to drag on with numerous streak freezes and minimal effort lessons. While Spanish words look less foreign, and with some focus, I am able to decipher small paragraphs of grammatically simple sentences; the effort was less than a smashing success – I certainly could not be writing this essay in Spanish.</p>

<p>If Duolingo is known for anything, it has to be <strong>their gamification approach</strong>. There is no shortage of gamification mechanics on the platform: XP; potions to double your XP; leagues of gold, silver, and all sorts of other metals and minerals; treasure chests; quests; monthly quests; and so on. I have only ever paid little attention to these mechanics. While I still haven’t entirely rejected the idea that a good RPG could be a good scaffolding to teach a language, I do not think Duolingo is one.</p>

<p>Games worth their salt are not created by bolting together a collection of numerical statistics. That is how you get <a href="https://en.wikipedia.org/wiki/Cookie_Clicker" target="_blank" rel="noopener noreferrer">cookie clicker</a>. I did not have a good understanding of how the mechanics work: if I learn 10 words, how many XP do I get for my hard work? Is the Diamond League higher or lower than the Obsidian League? I could have viewed their documentation to figure it out, but there was nothing motivating me to do so. If I collect 100 XP, what does it mean for my language skills? For that matter, why do I collect extra XP when I receive a potion? Can the XP I collect be used in a way to carefully guide me towards the specific language skills I would explore next? Navigating the mechanical <em>gameplay</em> of Duolingo is neither rewarding for its own sake, nor is it helpful towards actually learning a language.</p>

<p>Duolingo is not just a poor simulacrum of the mechanical aspects of a game, but also of <strong>the social aspects</strong> of one. Who are all these people I am on the Silver league with? Having a comparable amount of XP does not give me a sense of social connection with them. When I click a button to congratulate a friend on Duolingo, I do not truly engage with their learning journey. Indeed, it is worse than hearting an instagram photo, or upvoting a reddit thread. In those cases, I am reacting to a sliver of expression from my acquaintance. Here, I am presented only with a pre-rendered text with an abstract numerical statistic. Reacting to it is deliberately frictionless: I am presented with a wall of buttons allowing me to click them with ease and without thought. When Duolingo tells me that so and so sent me a message saying “Hey, come back and learn Spanish with me!”, I don’t admire how thoughtful and encouraging my friend is; I just notice that they clicked a button to send me a pre-generated message.</p>

<p>Interactions on Duolingo were not always of the push-button variety. Duolingo <a href="https://duolingo.hobune.stream/" target="_blank" rel="noopener noreferrer">had forums</a> where users would discuss different aspects of their language learning journey. In fact, Duolingo <a href="https://streakchaser.com/language/duolingo-forums-cant-comment/" target="_blank" rel="noopener noreferrer">would link</a> each sentence to its own forum thread for discussion – discussion, which was at the very least, helpful, and at times, eye-opening. At first, these discussion threads were locked, and later removed. My hypothesis is that for the business geniuses running Duolingo, <strong>the forums</strong> were assesed to be a liability, having to moderate which were not worth spending the dollars for. The nature of interacting with people – friends or strangers, in person or online – is that sometimes bad things would happen. Even when there is no abuse or harassment going on, there is always the risk that the other person might greet you with disagreement, or worse, apathy. <a href="https://web.archive.org/web/20250916150051/http://omegle.com/" target="_blank" rel="noopener noreferrer">Many</a> <a href="https://web.archive.org/web/20250803104055/https://www.lfgss.com/conversations/401475/" target="_blank" rel="noopener noreferrer">people</a> tend to think that the risks outweigh the benefits.</p>

<p>The gamification mechanic that I did latch on to was <strong>the Streak</strong>. I generally have been critical of the green owl, but I do think that it did help me form a good habit – a net positive, despite the minuscule magnitude. Regular Duolingo users will know that the streak can be gamed away in more than one ways. <em>Streak Freezes</em> can be bought using gems (of which I happen to have 24,053 of, somehow) or be gifted by your friends, and equipped 2 at a time. Streaks wouldn’t have their social effect if there weren’t enough people with a moderate number of people with decent streaks to be sprinkled around. Maintaining the streak, even without freezes, does not have to mean that you are learning – repeating a simple lesson from several units ago would work. My 1800 day streak didn’t mean that I spent 1800 days learning Spanish; it meant that I spent a large number of days <em>engaging with the platform</em>. I later started peeking into the Japanese and Finnish courses, and the 1800 day number includes them. If you loose interest in languages, Duolingo <a href="https://blog.duolingo.com/new-subjects/" target="_blank" rel="noopener noreferrer">tells us</a> that spending time with math or music will count towards your Streak.</p>

<p>The deficiency of Duolingo’s pedagogy was first made obvious to me by the excellent audio lessons produced by <a href="https://www.languagetransfer.org/" target="_blank" rel="noopener noreferrer">Language Transfer</a>. Going through the first few lessons of Language Transfer, I was unfazed, observing that I had already learnt what was being taught. Soon, what shocked me was how quickly Language Transfer caught up to what I had managed to learn in a couple years of time. While Duolingo is great at making sure that the user comes back to the app everyday, <strong>their pedagogy</strong> is subpar. Remaining true to gamification, Duolingo prefers to throw users head-first into translation exercises. If you do not know a word, you hover over it and you arrange a given bag of words into a sentence that is hopefully meaningful. Grammar lessons are extremely minimal. The removal of the forums dedicated to the discussion of specific sentences did not help. Understanding the course outline – knowing what is taught where, or reviewing lessons – is not easy.</p>

<p>Supposedly, <strong>the Duolingo philosophy</strong> is that if you are exposed to enough sentences, you will eventually learn how to use them. I do believe this is true, and I do believe the exercises are indispensable. However, the whole process could be greatly improved by a few more lessons interspersed in the curriculum telling the student what is going on. To see this, I would ask myself, did I always internalize the grammatically correct structures even in my own mother tongue? I think not, and my language skills have improved when my parents or teachers would point out simple grammatical mistakes. <a href="https://en.wikipedia.org/wiki/Eggcorn" target="_blank" rel="noopener noreferrer">Eggcorns</a> are a closely related amusing phenomena.</p>

<p>I cannot tell if Duolingo repeats different concepts in exercises adaptively based on your mastery, or are simply fixed in the course material. Repetition is <a href="https://en.wikipedia.org/wiki/Spaced_repetition" target="_blank" rel="noopener noreferrer">good for learning</a> but Duolingo’s repetition can be frustrating. The platform’s interface is largely built around clicking <strong>a bag of jumbled</strong> words one at a time to input a translation. Once you learn a concept well enough, most of your mental energy is spent on the finding and clicking of the words rather than the translation. Thankfully, this situation can be made better by dictating your answer, as pointed out by the <a href="https://blog.duolingo.com/sneaky-pronunciation-practice/" target="_blank" rel="noopener noreferrer">official blog</a>.</p>

<p>Duolingo does include a few other formats for their lessons. There are some stories, which are interspersed in the course. The stories are short, but silly and enjoyable. There are also audio only lessons, which are also shorter and unfortunately, not as fun. From time to time, the regular lessons also ask you to speak to the microphone but in my experience, the audio recognition seems to accept the answer even if I mumble through the words. Duolingo is also known for its usage of bizarre phrases, whose shock value generates <a href="https://www.reddit.com/r/shitduolingosays/" target="_blank" rel="noopener noreferrer">social media buzz</a> and may or may not have a positive pedagogical effect.</p>

<hr>

<p>Duolingo is a neat case study in Silicon Valley ideology. Big tech embraces <a href="https://hbr.org/2016/04/blitzscaling" target="_blank" rel="noopener noreferrer">blitz-scaling</a>: the primary goal is neither financial sustainability nor the quality of materials but making the number of users grow. The faux gamification and <a href="https://www.ft.com/content/3a9d6a6c-3681-4c93-9b8f-e6e41bea30dc" target="_blank" rel="noopener noreferrer">passive-aggressive messaging</a> may be helpful with little else, but is good for user retention. The expansionism does not stop at growing the number of users; Duolingo has decided that they must loop in <a href="https://blog.duolingo.com/new-subjects/" target="_blank" rel="noopener noreferrer">music and math</a> learners as well. As we have discussed, the maxim of <em>friction reduction</em> has guided them towards optimizing away authenticity in the user interactions on the platform.</p>

<p>In April 2025, Duolingo <a href="https://web.archive.org/web/20250516041748/https://www.linkedin.com/posts/duolingo_below-is-an-all-hands-email-from-our-activity-7322560534824865792-l9vh/?rcm=ACoAAADRbfMBYDgBD6CKFgJZnb27n5NLn5v-LAo" target="_blank" rel="noopener noreferrer">decided</a> to go AI-first. Supposedly, “to teach well, [they] need to create a massive amount of content” – so much so that “doing [it] manually does not scale”. For <a href="https://blog.duolingo.com/2024-duolingo-language-report/" target="_blank" rel="noopener noreferrer">the top ten languages</a>, I cannot imagine any reasonable person saying that the lack of study material is the main obstacle towards learning. This statement spells out what the Duolingo executives value. The Duolingo CEO is not shy to admit it. In an <a href="https://www.npr.org/transcripts/1197997573" target="_blank" rel="noopener noreferrer">interview with NPR</a>, he said the following.</p>

<blockquote>
  <p>[I]f it’s our content, as in, like, our learning content, there’s so much of that - thousands and thousands and thousands of kind of sentences and words and paragraphs. That is mostly done by computers, and we probably spot-check it. But if it’s things like the user interface of Duolingo, where we say - like, you know, the button says quit, and we have to translate, that is all done with humans. And we spend a lot of effort on that, but that’s because each one of those is highly valuable.</p>
</blockquote>

<p>Yes, <em>the button that says ‘quit’</em> is more valuable than the learning material, which is only ‘probably’ spot-checked.</p>

<hr>

<p>After I moved to Japan, I dialed up my efforts to learn Japanese. For a while, I shifted over my Duolingo habits to Japanese. Because Duolingo wasn’t my only learning material for Japanese, it was glaringly obvious very soon that the Duolingo pedagogy is unhelpful and often misleading. While the Spanish learner has to introduce themselves to a few new concepts (e.g, <em>ser vs estar</em>, or reflexive verbs), the Japanese learner faces an explosion of differences. Japanese has a <a href="https://en.wikipedia.org/wiki/Japanese_writing_system" target="_blank" rel="noopener noreferrer">writing system with three components</a>; generally uses a <a href="https://en.wikipedia.org/wiki/Topic_and_comment" target="_blank" rel="noopener noreferrer">topic-comment structure</a> and often omits the topic; has a <a href="https://en.wikipedia.org/wiki/Subject%E2%80%93object%E2%80%93verb_word_order" target="_blank" rel="noopener noreferrer">subject-object-verb order</a>; has <a href="https://en.wikipedia.org/wiki/Japanese_adjectives" target="_blank" rel="noopener noreferrer">adjectives which conjugate</a>; a lot of <a href="https://en.wikipedia.org/wiki/Japanese_counter_word" target="_blank" rel="noopener noreferrer">counting suffixes</a>; <a href="https://en.wikibooks.org/wiki/Japanese/Grammar/Sentence_ending_particles" target="_blank" rel="noopener noreferrer">sentence ending particles</a> and famously, a <a href="https://en.wikipedia.org/wiki/Honorific_speech_in_Japanese" target="_blank" rel="noopener noreferrer">complex honorific system</a>. Duolingo does not break its gamification façade to teach the user some of these concepts head-on. Instead, it pretends that translating between Japanese and English is a matter of substituting phrases and shuffling them around.</p>

<p>Since I gave up on my Duolingo streak, I have started exploring <strong>other avenues</strong> to continue learning Japanese. I participate in group lessons with a tutor once a week for an hour. Believe it or not, the tutor has more charm than <a href="https://duolingo.fandom.com/wiki/Falstaff" target="_blank" rel="noopener noreferrer">Falstaff</a>. I regularly do my flashcard kanji study with <a href="https://www.wanikani.com/" target="_blank" rel="noopener noreferrer">Wanikani</a>. A newer addition to my study routine is <a href="https://bunpro.jp/" target="_blank" rel="noopener noreferrer">Bunpro</a>. My progress has been slow but evident: when I recognize that the names of the metro stations I frequent break down into simple words, they lose a little bit of their mystery but it is a satisfying revelation.</p>

<p>These platforms are a welcome contrast against the techno-accelerationist attitude of Duolingo. Instead of trying to do it all, they are extremely niche: they only teaching one language and Wanikani is focused at teaching a very specific element of it. Wanikani maintains a <a href="https://docs.api.wanikani.com/20170710/" target="_blank" rel="noopener noreferrer">public API</a>, which makes third-party <a href="https://community.wanikani.com/t/the-new-and-improved-list-of-api-and-third-party-apps/7694" target="_blank" rel="noopener noreferrer">apps and scripts</a> possible. I praise them for their welcome attitude towards interoperability instead of trying to build a closed ecosystem. Both <a href="https://community.wanikani.com/" target="_blank" rel="noopener noreferrer">Wanikani</a> and <a href="https://community.bunpro.jp/" target="_blank" rel="noopener noreferrer">Bunpro</a> have vibrant user forums. Bunpro makes actual lessons part of their critical path, instead of hoping that the user will <em>eventually figure it out</em>. When a Bunpro user feels that their lesson was not adequate, they do not have to rely on AI generated slop – Bunpro directs users to carefully-crafted lessons by <em>other people</em> (see the ‘resource’ section at the end of this <a href="https://bunpro.jp/grammar_points/%E3%81%AD" target="_blank" rel="noopener noreferrer">page</a>, for example).</p>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Imgur pulls out of UK as data watchdog threatens fine (302 pts)]]></title>
            <link>https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</link>
            <guid>45424888</guid>
            <pubDate>Tue, 30 Sep 2025 13:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out">https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out</a>, See on <a href="https://news.ycombinator.com/item?id=45424888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="singleArticle" data-story="2115228" role="main"> <article> <header><h2>A popular image hosting website has stopped its services in the UK after regulators threatened a fine.</h2><div><p><time datetime="2025-09-30T11:35:00Z"> <span>12:35, Tue, Sep 30, 2025</span> </time> <time datetime="2025-09-30T11:38:22Z"> Updated: <span>12:38, Tue, Sep 30, 2025</span> </time></p></div> </header><div data-type="article-body"><div><p><picture><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:10000px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/674x400/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:100000px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/jpeg" srcset="https://cdn.images.express.co.uk/img/dynamic/1/940x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" media="screen and (min-width:1200px)"><source type="image/avif" srcset="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.avif?r=1759232302595" media="screen"><source type="image/webp" srcset="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.webp?r=1759232302595" media="screen"><img src="https://cdn.images.express.co.uk/img/dynamic/1/590x/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" data-img="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595" alt="Web Summit 2018 - Day 4" title="Web Summit 2018 - Day 4" width="590" height="393"></picture></p><p><span>Imgur founder. <span>(Image: Getty)</span><span data-img="https://cdn.images.express.co.uk/img/dynamic/1/1200x712/secondary/imgur-founder-alan-schaaf-speaks-during-the-web-summit-2018-in-lisbon-portugal-on-november-8-6459211.jpg?r=1759232302595"></span></span></p></div><div><p>An image hosting platform with more than 130 million users has stopped being available in the UK <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/news/uk/339446/More-firms-breaching-data-rules">after regulators signalled their intention to impose penalties over concerns around children’s data.</a></p><p>The Information Commissioner’s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children’s Code strategy, which is intended to set the standards for how <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/news/politics/2089661/online-safety-law-watchdog">online services handle the personal information of young people</a>.</p><p>In a statement the ICO said: “We are aware of reports that the social media platform <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/life-style/science-technology/2091749/expressvpn-online-safety-act-2025">Imgur is currently not available in the UK</a>. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.”</p></div><div><p>Tim Capel, the ICO’s Interim Executive Director for Regulatory Supervision, said that the regulator had now issued a notice of intent to fine.</p><p>He said: “We reached our provisional findings on this investigation, and we issued a notice of intent to impose a monetary penalty on MediaLab on 10 September 2025.</p><p>“Our findings are provisional and the ICO will carefully consider any representations from MediaLab before taking a final decision whether to issue a monetary penalty.”</p><p>The ICO also confirmed that companies could not avoid accountability by withdrawing their services in the UK.</p><p>Mr Capel said: “We have been clear that exiting the UK does not allow an organisation to avoid responsibility for any prior infringement of data protection law, and our investigation remains ongoing.</p><p>“This update has been provided to give clarity on our investigation, and we will not be providing any further detail at this time.”</p></div><div><p>He added that protecting young people’s information remains a central focus: “Safeguarding children’s personal information is a key priority for the ICO and our Children’s code strategy outlines our key interventions in this area. Keeping children safe online is the responsibility of the companies offering online services to them and we will continue to hold them to account.”</p><p>Regulators did not disclose the potential size of the penalty for specific breaches it has identified.</p><p>Under UK law, the “notice of intent” process gives the company an opportunity to make representations before any final decision is made.</p><p>Imgur, founded in 2009 and acquired by Los Angeles-based MediaLab AI Inc in 2021, is an image hosting and sharing site popular for memes, viral content and online communities. It’s services appeared to become unavailable in the UK last night.</p><p>Imgur was approached for comment.</p></div><div data-lazy-function="readnext"> <header><h3>Read next</h3> </header><ul><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li><li> <var></var> <a data-link-tracking="InArticle|Link" href="https://www.express.co.uk/" data-tmdatatrack="read-next" data-tmdatatrack-articleid="" data-tmdatatrack-platform="nationals" data-tmdatatrack-source="" data-tmdatatrack-index="" data-tmdatatrack-visible="">&nbsp;</a></li></ul></div><div><p>  <span></span> <span>Invalid email</span></p><p>We use your sign-up to provide content in ways you've consented to and to improve our understanding of you. This may include adverts from us and 3rd parties based on our understanding. You can unsubscribe at any time. Read our <a href="https://www.express.co.uk/privacy-notice">Privacy Policy</a></p></div></div> </article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Founder sentenced to seven years in prison for fraudulent sale to JPMorgan (157 pts)]]></title>
            <link>https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl</link>
            <guid>45424827</guid>
            <pubDate>Tue, 30 Sep 2025 12:53:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl">https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl</a>, See on <a href="https://news.ycombinator.com/item?id=45424827">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[Pasta Cooking Time (116 pts)]]></title>
            <link>https://www.jefftk.com/p/pasta-cooking-time</link>
            <guid>45424704</guid>
            <pubDate>Tue, 30 Sep 2025 12:40:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jefftk.com/p/pasta-cooking-time">https://www.jefftk.com/p/pasta-cooking-time</a>, See on <a href="https://news.ycombinator.com/item?id=45424704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <p><span>

I generally find the numbers printed on pasta boxes for cooking time
far too high: I'll set the timer for a minute below their low-end "al
dente" time, and when I taste one it's already getting too mushy. I
decided to run a small experiment to get a better sense of how cooked
I like pasta.

</span></p><p>

I decided to use Market Basket <a href="https://en.wikipedia.org/wiki/Rigatoni">Rigatoni</a>. [1] It's a
ridged cylinder, and I measured the ridges at 1.74mm:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-ridge-thickness-big.jpg"><img src="https://www.jefftk.com/rigatoni-ridge-thickness.jpg" width="550" height="604" srcset="https://www.jefftk.com/rigatoni-ridge-thickness.jpg 550w,https://www.jefftk.com/rigatoni-ridge-thickness-2x.jpg 1100w"></a></p>


<p>

And the valleys at 1.32mm:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-valley-thickness-big.jpg"><img src="https://www.jefftk.com/rigatoni-valley-thickness.jpg" width="550" height="577" srcset="https://www.jefftk.com/rigatoni-valley-thickness.jpg 550w,https://www.jefftk.com/rigatoni-valley-thickness-2x.jpg 1100w"></a></p>


<p>

The box recommends 13-15 minutes:

</p>
<p>

<a href="https://www.jefftk.com/market-basket-rigatoni-cooking-time-big.jpg"><img src="https://www.jefftk.com/market-basket-rigatoni-cooking-time.jpg" width="550" height="453" srcset="https://www.jefftk.com/market-basket-rigatoni-cooking-time.jpg 550w,https://www.jefftk.com/market-basket-rigatoni-cooking-time-2x.jpg 1100w"></a></p>


<p>

This is a house brand pasta from a chain centered in a part of the
country with a relatively high Italian-American population, so you
might think they'd avoid the issue where Americans often cook pasta
absurdly long:

</p>
<p>

<a href="https://www.jefftk.com/market-basket-locations-big.png"><img src="https://www.jefftk.com/market-basket-locations.png" width="550" height="686" srcset="https://www.jefftk.com/market-basket-locations.png 550w,https://www.jefftk.com/market-basket-locations-2x.png 1100w"></a></p>


<p>

I boiled some water, put in the pasta, and starting at 9min I removed
a piece every 15s until I got to 14:30:

</p>
<p>

<a highlight="" href="https://www.jefftk.com/pasta-every-15s-big.jpg"><img src="https://www.jefftk.com/pasta-every-15s.jpg" width="550" height="739" srcset="https://www.jefftk.com/pasta-every-15s.jpg 550w,https://www.jefftk.com/pasta-every-15s-2x.jpg 1100w"></a></p>


<p>

Here's the minute-by-minute, cut open so you can see the center of the
noodles:

</p>
<p>

<a highlight="" href="https://www.jefftk.com/pasta-cut-open-by-time-big.jpg"><img src="https://www.jefftk.com/pasta-cut-open-by-time.jpg" width="550" height="90" srcset="https://www.jefftk.com/pasta-cut-open-by-time.jpg 550w,https://www.jefftk.com/pasta-cut-open-by-time-2x.jpg 1100w"></a></p>


<p>

My family and I tried a range of noodles, trying to <a href="https://en.wikipedia.org/wiki/Binary_search">bisect</a> our way
to the ideal cooking time.  I was happiest at 10m15s, but ok between
9m15s and 11m30s.  Julia thought 9m45s was barely underdone, while
11m45s was barely overdone.  Anna liked 10m30s.  Lily didn't like any
of them, consistently calling them "too crunchy" up through 10m45s and
then "too mushy" for 11m0s and up.  Everyone agreed that by 12m45s it
was mushy.

</p>
<p>

Instead of 13-15min, a guideline of 10-12min would make a lot more
sense in our house.  And, allegedly, the glycemic index is much lower.

</p>
<p>

My mother and her siblings grew up in Rome, and I wrote asking about
what they'd noticed here.  My uncle replied "my bias is that Americans
are wimps for soft pasta" and the others agreed.

</p>
<p>

I tried using a <a href="https://www.jefftk.com/p/christmas-microscopy">cheap
microscope</a> to investigate, whether there were interesting
structural differences, but even with an iodide stain I couldn't make
out much.  Here's 3min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-03m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-03m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-03m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-03m00s-2x.jpg 1100w"></a></p>


<p>

And 7min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-07m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-07m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-07m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-07m00s-2x.jpg 1100w"></a></p>


<p>

And 13min:

</p>
<p>

<a href="https://www.jefftk.com/rigatoni-iodide-13m00s-big.jpg"><img src="https://www.jefftk.com/rigatoni-iodide-13m00s.jpg" width="550" height="367" srcset="https://www.jefftk.com/rigatoni-iodide-13m00s.jpg 550w,https://www.jefftk.com/rigatoni-iodide-13m00s-2x.jpg 1100w"></a></p>


<p>

On the other hand, the kids and I did have fun with the microscope.

</p>
<p>

<a href="https://www.jefftk.com/jeff-nora-microscope-wrapper-big.jpg"><img src="https://www.jefftk.com/jeff-nora-microscope-wrapper.jpg" width="550" height="309" srcset="https://www.jefftk.com/jeff-nora-microscope-wrapper.jpg 550w,https://www.jefftk.com/jeff-nora-microscope-wrapper-2x.jpg 1100w"></a></p>


<p>
<br>

[1] We called these "hospital noodles" growing up, because when my
mother had been in a hospital for a long time as a kid (recovering
from being hit by an impatient driver while crossing the street) they
had served Rigatoni as their primary pasta shape.

  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How has mathematics gotten so abstract? (112 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract</link>
            <guid>45424648</guid>
            <pubDate>Tue, 30 Sep 2025 12:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract">https://lcamtuf.substack.com/p/how-has-mathematics-gotten-so-abstract</a>, See on <a href="https://news.ycombinator.com/item?id=45424648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Today, mathematics is regarded as a purely abstract science. On forums such as Stack Exchange, trained mathematicians may sneer at newcomers who ask for intuitive explanations of mathematical constructs. Indeed, persistently trying to relate the foundations of math to reality has become the calling card of online cranks.</p><p>I find this ironic: for millennia, mathematics was essentially a natural science. We had no philosophical explanation why 2 + 2 should be equal to 4; we just looked at what was happening in the real world and tried to capture the rules. The abstractions were important, of course, but they needed to be rooted in objectivity. The early development of algebra and geometry followed suit. It was never enough for the axioms to be internally consistent; the angles of your hypothetical triangle needed to match the physical world.</p><p>That said, even in antiquity, the reliance on intuition sometimes looked untenable. A particular cause for concern were the outcomes of thought experiments that involved repeating a task without end. The most famous example is Zeno’s paradox of motion. If you slept through that class, imagine the scenario of Achilles racing a tortoise:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MzH8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MzH8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 424w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 848w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1272w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png" width="1456" height="353" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:353,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MzH8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 424w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 848w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1272w, https://substackcdn.com/image/fetch/$s_!MzH8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d2d033-0511-48aa-bb71-b6732d01a9df_1638x397.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em><strong>Catch me if you can.</strong></em></figcaption></figure></div><p>We can reason that after a while, Achilles will catch up to the turtle’s original position (red dot); however, by the time he gets there, the animal will have moved some distance forward (yellow dot):</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!XS3G!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XS3G!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 424w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 848w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1272w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png" width="1456" height="478" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:478,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:144247,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!XS3G!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 424w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 848w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1272w, https://substackcdn.com/image/fetch/$s_!XS3G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6567e21-8c90-4e32-a4fb-0f548afbf46b_1638x538.png 1456w" sizes="100vw"></picture></div></a><figcaption><em>And they would have gotten away with it too…</em></figcaption></figure></div><p>Next, consider the time needed for Achilles to reach the yellow dot; once again, by the time he gets there, the turtle will have moved forward a tiny bit. This process can be continued indefinitely; the gap keeps getting smaller but never goes to zero, so we must conclude that Achilles can’t possibly win the race.</p><p>Amusingly, the problems caused by infinity lingered on the periphery of mathematics for centuries, fully surfacing only after we attempted to fix them with calculus. Calculus gave us a rigorous solution to the ancient puzzle: an infinite sum of time slices can be a finite number, so Achilles does catch up to the tortoise. Yet, to arrive at that result, the new field relied on the purported existence of infinitely small numbers (infinitesimals). The founders struggled to explain how to construct such entities, where to find them on the real number line (you can’t), and whether they’re safe to mix with real number algebra in the first place.</p><p>Over time, this prompted a number of mathematicians to try and build a more general model of mathematics, starting from the ground up — that is, from the principles of formal logic. In particular, one prominent faction of the movement sought to define numbers and arithmetic operations in a way that was fully independent of the physical realm.</p><p><span>In the late 19th century, Giuseppe Peano successfully answered this call. His system posits the existence of a single initial number — conventionally, zero — and then defines a successor function </span><em>S</em><span>.</span></p><p>This allows us to define numbers — really, just a collection of labels — solely in terms of a succession relationship:</p><div data-component-name="Latex"><p><span>\(\begin{alignat}{1}
1 &amp;= S(0) \\
2 &amp;= S(1) &amp;&amp;= S(S(0)) \\
3 &amp;= S(2) &amp;&amp;= S(S(S(0))) \\
4 &amp;= S(3) &amp;&amp;= S(S(S(S(0))))
\end{alignat}\)</span></p></div><p><span>While this might seem mundane, the scheme reduces many other problems to the process of induction and recursion. For example, to solve 2 + 2, we don’t need any </span><em>a priori </em><span>knowledge of what “2” or “+” means. Instead, we define addition using the following two rules:</span></p><div data-component-name="Latex"><p><span>\(\begin{alignat}{1}
a &amp;+ 0 &amp;&amp;= a &amp;&amp;\qquad (\textrm{rule 1}) \\
a &amp;+ S(b) &amp;&amp;= S(a+b) &amp;&amp;\qquad ( \textrm{rule 2}) 
\end{alignat}
\)</span></p></div><p>This notation may appear abstract, so to tease it out, let’s try to actually calculate 2 + 2. Because the second operand is non-zero, we can’t apply the first rule just yet. That said, from the construction of Peano numbers, we know that 2 = S(1). In light of this, we can rewrite 2 + 2 in a way that lets us use of the second rule:</p><div data-component-name="Latex"><p><span>\(\begin{array}{r l}
2 + 2 = 2 + S(1) &amp; \textrm{(by the construction of number 2)} \\
2 + S(1) = S(2 + 1) &amp; \textrm{(per rule 2)}
\end{array}\)</span></p></div><p>At this point, we’ve shown that 2 + 2 is the same as S(2 + 1). To solve the original equation, we still need to find the value of 2 + 1; this can be done by applying the same substitution technique once more:</p><div data-component-name="Latex"><p><span>\(\begin{array}{rl}
2 +1 = 2 + S(0) &amp; \textrm{(by construction)} \\
2 + S(0) = S(2+0) &amp; \textrm{(per rule 2)}
\end{array}\)</span></p></div><p><span>In effect, we have restated 2 + 2 as S(2 + 1), and then 2 + 1 as S(2 + 0). In that last instance, the second operand is zero, so we can finally apply rule 1. The rule says that </span><em>a + 0 = a</em><span>, so:</span></p><div data-component-name="Latex"><p><span>\(2 + 1 = S(2 + 0) = S(2)\)</span></p></div><p>Further, from the construction of Peano numbers, we know that our chosen label for S(2) is 3; therefore, 2 + 1 = 3.</p><p>With this equality established, we go back to the initial step where we expressed 2 + 2 as S(2 + 1) and get the final answer:</p><div data-component-name="Latex"><p><span>\(2 + 2 = S(2+1) = S(3) = 4\)</span></p></div><p><span>If you work with software, you might appreciate the following C code that implements roughly the same logic (</span><a href="https://godbolt.org/z/zWh5j1KEr" rel="">demo</a><span>):</span></p><blockquote><pre><code>#include &lt;stdio.h&gt;

struct number { char* label; struct number* next; }
  five  = { "5", NULL },   four = { "4", &amp;five }, three = { "3", &amp;four },
  two   = { "2", &amp;three }, one  = { "1", &amp;two },  zero  = { "0", &amp;one };

struct number* succ(struct number* num) { return num-&gt;next; }

struct number* pred(struct number* num) {
  struct number* ret = &amp;zero;
  while (succ(ret) != num) ret = succ(ret);
  return ret;
}

struct number* add_numbers(struct number* num_a, struct number* num_b) {
  if (num_b == &amp;zero) return num_a;
  return succ(add_numbers(num_a, pred(num_b)));
}

int main() {
  printf(”2 + 3 = %s\n”, add_numbers(&amp;two, &amp;three)-&gt;label);
}</code></pre></blockquote><p>In this program, instead of relying on built-in integers, we start with a unidirectional linked list of strings: “0” → “1” → “2” → “3” → “4” → “5”. This data structure encodes the successor relationship between the labels without giving them any further meaning.</p><p><span>Next, we define a trivial helper called </span><em>succ(x), </em><span>which returns the successor of </span><em>x,</em><span> along with a slightly more complicated function called </span><em>pred(x)</em><span>, which finds the element to which </span><em>x </em><span>is the successor. Finally, </span><em>add_numbers(a, b)</em><span> is a straightforward implementation of the recursive rules for Peano addition, as outlined earlier on.</span></p><p>Again, the merit of this approach is that it lets us model arithmetic without any external assumptions about the nature of numbers, the significance of the addition operator, and so forth. We used familiar labels (0, 1, 2, 3, 4, …), but we could’ve used some other ordered collection of abstract symbols (🥔, 🎵, 🐸, 🌀, 🐱, …). If so, we’d have gotten an equivalent model of math in which 🐸 + 🐸= 🐱.</p><p>Of course, Peano arithmetic is too cumbersome for everyday tasks; instead, it serves as a minimalist model for theoretical work. It is used similarly to how computer scientists use Turing machines; no one wants to browse the internet on a Turing-style computer, but if you proved that P = NP for a Turing machine, this would have implications for more practical computing architectures too.</p><p><span>Giuseppe Peano’s axiomatic approach was revolutionary and led to breakthroughs such as the </span><a href="https://lcamtuf.substack.com/p/monkeys-typewriters-and-busy-beavers" rel="">Gödel incompleteness theorem</a><span>; however, it still didn’t offer a particularly good model of infinite quantities. For that, mathematicians needed to turn to an even more exotic framework: set theory.</span></p><p>In set theory, numbers are conventionally defined as labels for specific, ordered sets. To get started with the construction process, we only need an empty set ({}), which we label as zero:</p><p>To define the successor number, we add an element to the set. To avoid inventing arbitrary new elements, we can simply embed the previously-conjured number zero in the successor set:</p><p>If this seems confusing, you can think of set as boxes. We started with an empty box with zero items inside; we then sealed the box and placed it in a larger container, so the larger box now contains a single element. For this tally, the contents of that smaller, sealed box are of no consequence.</p><p><span>After that, we can’t define the next successor as {0, 0}; this is because in set theory, every set element must be unique. That said, as discussed earlier, a “naked” element </span><em>n </em><span>is distinct from a box containing that element (i.e., a new set </span><em>{n}</em><span>), so we can do this:</span></p><p>Note that in our model, 1 = {0}, so the construction method shown above is equivalent to saying that 2 = {0, 1}.</p><p>To get to the third successor, we need to put one more element in the set. At this point, we can’t reuse 0 or 1, but we can embed the recently-created set representing 2:</p><div data-component-name="Latex"><p><span>\(3 = \{ 0, 1, 2 \} = \{ 0, \{ 0 \}, \{ 0, \{ 0 \} \} \}\)</span></p></div><p>This process can continue for as long as we’d like, e.g.:</p><div data-component-name="Latex"><p><span>\(4 = \{ 0, 1, 2, 3 \} = \{ 0, \{0\}, \{0, \{0\}\}, \{0, \{0\}, \{0, \{0\}\}\} \}
\)</span></p></div><p>In set theory, the labels we’re creating are called ordinals. Note that every ordinal is an ordered set of all the preceding ordinals, and that the set never contains itself.</p><p><span>If you’re seeing parallels to the iterative construction of Peano numbers, this is not an accident; the two approaches are conceptually similar, it’s just that in this instance, the underlying mathematical structure of each number is spelled out more explicitly. The general algorithm is that we build number </span><em>n + 1</em><span> by joining the preceding set </span><em>n</em><span> and a copy of </span><em>n </em><span>embedded inside a new set. The set-joining operation is known as union (∪), so we can formalize a Peano-like successor function for ordinals as:</span></p><p><span>Almost all mathematicians accept the existence of infinite sets; a common example would be the set of all natural numbers, ℕ. Every natural number itself is finite, but there is no upper limit on how large these numbers can get; whenever you pick some </span><em>n</em><span>, I can always best you by shouting “</span><em>n </em><span>+ 1”.</span></p><p>The ordered set of all natural numbers looks like the product of our method for constructing cardinals — that is, if we allowed the process to continue without end:</p><div data-component-name="Latex"><p><span>\(\mathbb{N} = \{0, 1, 2, 3, 4, ...\}\)</span></p></div><p>It’s tempting to ask if the set can function as an infinite ordinal — i.e., an infinite number — and if yes, what numerical properties does it have?</p><p><span>Well, we can say right off the bat that the ordinal we’re talking about wouldn’t be a member of ℕ: every element of ℕ is finite. We can also conclude that the ordinal must not be a successor to any natural number: if </span><em>n</em><span> is a member of ℕ, then so is </span><em>n + 1</em><span>, so the presence of a successor relationship would lead to the same contradiction.</span></p><p>Can such an unmoored, infinite ordinal exist? Well, that’s up to us to decide: conjuring it doesn’t lead to any outright paradoxes and opens up some weird but occasionally useful math. </p><p><span>We can name this ordinal </span><em>ω</em><span>; again, its set-theoretic representation is just:</span></p><div data-component-name="Latex"><p><span>\(\omega = \{0, 1, 2, 3, 4, ...\}  = \mathbb{N}\)</span></p></div><p>Of course, inventing a symbol isn’t much of an accomplishment; the big question is whether, under the axioms of set theory, we can derive any useful arithmetic for this mysterious entity.</p><p>In school, you might have been exposed to notation along the lines of:</p><p><span>In an </span><a href="https://lcamtuf.substack.com/p/09999-1" rel="">earlier article</a><span>, I quipped that this notation is just a glorified calculator error message — all it tells us is that the result is too large for reals:</span></p><div data-component-name="Latex"><p><span>\(\texttt{&lt;Error&gt;} + 1 = \texttt{&lt;Error&gt;}\)</span></p></div><p><span>That said, if you’re accustomed to this way of thinking about infinity, it’s tempting to assume that the rule should apply to actual infinite numbers — i.e. that </span><em>ω </em><span>should be the same as</span><em> ω</em><span> + 1</span><em>. </em><span>Let’s test that hypothesis.</span></p><p><span>In line with the Peano rules, we can express </span><em>ω</em><span> + 1 as the application of the ordinal successor operation to the first operand. As a reminder, the successor operation takes the original infinite set of natural numbers (</span><em>ω = </em><span>ℕ) and then embeds that set as a new element to construct the next ordinal. We get:</span></p><div data-component-name="Latex"><p><span>\(\omega + 1 = S(\omega) = \omega \cup \{\omega\} = \{ 0, 1, 2, 3, 4, ..., \omega \}\)</span></p></div><p><span>We have previously established that </span><em>ω </em><span>itself cannot be a member of </span><em> </em><span>ℕ, because that would make it a natural number, and therefore, a finite quantity. Yet, the newly-constructed set corresponding to </span><em>ω</em><span> + 1 evidently </span><em>does</em><span> contain that element; this tells us that the set is categorically different from ℕ. We must conclude that </span><em>ω </em><span>≠ </span><em>ω</em><span> + 1</span><em>. </em><span>More specifically, because </span><em>ω</em><span> + 1 contains </span><em>ω</em><span>, it sits higher in the rank of ordinals, and we can assert that </span><em>ω &lt; ω</em><span> + 1.</span></p><p><span>But lest we get too cozy with this new reality: addition involving infinite ordinals is not necessarily commutative! To illustrate, let’s construct the set representing 1 + </span><em>ω</em><span>. As before, we rewrite addition as a (repeated) application of the successor function to the first operand:</span></p><div data-component-name="Latex"><p><span>\(1 + \omega =  \underbrace{...S(S(S(1)))}_{\textrm{repeats } \omega \textrm{ times}}\)</span></p></div><p>Recall that we defined the ordinal 1 as a single-element set containing zero: {0}. Starting from that set and applying the successor function, we end up constructing ordinal 2 — another name for {0, 1}. The next application of the nets us 3, aka {0, 1, 2}:</p><div data-component-name="Latex"><p><span>\(\begin{align}
1 &amp;= \{0\} \\
S(1) &amp;= \{0, 1\} \\
S(S(1)) &amp;= \{0, 1, 2\} \\
...
\end{align}\)</span></p></div><p><span>If we repeat this operation </span><em>ω </em><span>times, we obtain an infinite set { 0, 1, 2, 3, 4, … }. Note that the element </span><em>ω </em><span>itself can never make it into the set: it’s not a successor of any natural number, so it can’t be reached by repeatedly incrementing one.</span></p><p><span>Upon closer inspection, the resulting 1 + </span><em>ω </em><span>set is indistinguishable from the set of natural numbers, ℕ. We know that </span><em>ω </em><span>is also just another name for</span><em> </em><span>ℕ, so we can write the following equality: 1 + </span><em>ω = ω. </em><span>Few paragraphs earlier, we showed that </span><em>ω &lt; ω + 1. </em><span>This leads to a surprising result: 1 + </span><em>ω &lt; ω + 1.</em></p><p><span>One might ask if non-commutative addition is a violation of one of the axioms of standard arithmetic. It isn’t, on a technicality: the rules apply only to finite numbers, such as the members of ℕ. Luckily for us, </span><em>ω </em><span>isn’t invited to that club.</span></p><p><span>Before we wrap up, let’s have a look at another interesting corner case: </span><em>ω + ω </em><span>(aka </span><em>ω </em><span>· 2). To calculate this ordinal, we start with </span><em>ω = </em><span>{ 0, 1, 2, 3, 4, … } and then iteratively extend the set through the successor operation. We first append </span><em>ω, </em><span>then </span><em>ω </em><span>+ 1, then </span><em>ω + 2, </em><span>and so on:</span></p><div data-component-name="Latex"><p><span>\(\omega \cdot 2 = \{ 0, 1, 2, 3, 4, ..., \omega, \omega + 1, \omega + 2, ... \}\)</span></p></div><p><span>The tail end of this ordered set is an infinite sequence of successors to </span><em>ω; </em><span>as in all the earlier cases, </span><em>ω </em><span>· 2 can’t be a member of itself, so </span><em>ω </em><span>· 2 must not be reachable by incrementing</span><em> ω. </em><span>This is analogous to how </span><em>ω </em><span>couldn’t be reached by incrementing any finite number</span><em>; </em><span>the discontinuity repeats for each multiple of </span><em>ω.</em></p><p>It’s hard not to notice that our set-theoretic numbers seem to be describe the size (element count) of the underlying set:</p><div data-component-name="Latex"><p><span>\(5 = \{0, 1, 2, 3, 4\}\)</span></p></div><p>For finite sets, this notion of size aligns with common sense. But when we look at the non-commutative addition and the discontinuities of infinite ordinals, one might start to wonder if trying to “count” elements is still a meaningful way to characterize sets in that realm.</p><p>There are several other ways to reason about this problem without resorting to counting. The simplest rule we can come up with is that one set is a strict subset of another, the first set could be described as smaller than the second one.</p><p>Another approach is to consider two sets to be of equivalent “magnitude” if you can map their elements one-to-one. It doesn’t matter which element gets mapped to which, as long as there are no orphaned members on either side:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!O98b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!O98b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 424w, https://substackcdn.com/image/fetch/$s_!O98b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 848w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1272w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png" width="1456" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:178037,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!O98b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 424w, https://substackcdn.com/image/fetch/$s_!O98b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 848w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1272w, https://substackcdn.com/image/fetch/$s_!O98b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50e61248-a7e5-4e13-bcc5-fd3da6abfc12_1740x988.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>One equivalent pair (top) and two non-equivalent ones (bottom).</em></figcaption></figure></div><p><span>This particular measure of equivalency on the basis of a one-to-one mapping is called </span><em>cardinality</em><span>.</span></p><p>The concepts are dead simple for finite sets, but consider a set of natural numbers next to a set of every even number (E). Obviously, E is strict subset of natural numbers, so by our first rule, we could say that E is smaller than ℕ. Yet, if we’re talking about a one-to-one mapping for a pair of infinite sets, the following approach is perfectly fine:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2Ood!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2Ood!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 424w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 848w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png" width="1456" height="874" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:874,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:189004,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/174503112?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2Ood!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 424w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 848w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1272w, https://substackcdn.com/image/fetch/$s_!2Ood!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff31ae3ac-6c8b-4137-86c8-18ccfd09d334_2088x1254.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>This is legal in 47 states.</em></figcaption></figure></div><p><span>After all, the sets are infinite, so we never run out of elements. We say that the set of natural numbers has a specific cardinality — aleph-null, ℵ</span><sub>0</sub><span> — and that the cardinality of the set of even numbers is the same.</span></p><p>Do any other infinite cardinalities exist? The answer, as demonstrated by Georg Cantor, appears to be yes! Imagine some arbitrary one-to-one mapping from every single natural number to reals (ℝ), e.g.:</p><div data-component-name="Latex"><p><span>\(\begin{array}{c}
1 &amp; \rightarrow &amp; 0.\underline{\textbf{1}}23456... \\
2 &amp; \rightarrow &amp; 0.6\underline{\textbf{5}}4321... \\
3 &amp; \rightarrow &amp; 0.99\underline{\textbf{9}}999... \\
4 &amp; \rightarrow &amp; 0.454\underline{\textbf{5}}45... \\
5 &amp; \rightarrow &amp; 0.1111\underline{\textbf{1}}1... \\
6 &amp; \rightarrow &amp; 0.03133\underline{\textbf{7}}... \\
&amp; ... &amp;
\end{array}\)</span></p></div><p>There’s nothing special about these values; the only point is that we have a distinct real number assigned to every member of ℕ. There are obviously at least as many reals as there are natural numbers, so this is something we should be able to do.</p><p><span>The remaining question is whether there are any orphaned reals left once all the natural numbers are used up. To figure this out, let’s try to construct a new real number, </span><em>d</em><span>. We start by looking at the first row above:</span></p><div data-component-name="Latex"><p><span>\(1 \rightarrow 0.\underline{\textbf{1}}23456...
\)</span></p></div><p><span>We take the underlined (first) decimal digit of the real and then choose </span><em>any value other than this one</em><span> for the corresponding digit in </span><em>d</em><span>. In this case, the offending digit is 1, so we can pick 0, 2, 3, 4, 5, 6, 7, 8, or 9. Let’s go with 9:</span></p><div data-component-name="Latex"><p><span>\(d = 0.\textbf{9} \textrm{ (...to be continued)}\)</span></p></div><p><span>We then proceed to the second row, this time looking at at the second decimal digit — essentially, following the diagonal pattern underlined in the assignment diagram. Once again, for the second decimal digit of </span><em>d</em><span>, we choose any value other than the actual digit marked in row 2; since the highlighted value is 5, we can pick 2 instead:</span></p><div data-component-name="Latex"><p><span>\(d = 0.9\textbf{2} \textrm{ (...more to come)}\)</span></p></div><p>In row three, we can replace 9 with 0; in row four, let’s substitute 5 with 4. For row five, we trade 1 for 3; in row six, we use 5 instead of 7. We keep following the diagonal pattern to infinity:</p><p><span>What’s special about this result? Well, by construction, </span><em>d </em><span>differs by at least one digit from every single real number in our mapping! For example, it can never match row 1 because the first decimal digit is 9 instead of 1; it also doesn’t match row 2 because the second decimal digit is 2 instead of 5.</span></p><p><span>That is to say, </span><em>d</em><span> can’t possibly appear anywhere on the list that assigned a real to every integer. Thus, we have found an orphaned member of ℝ; the cardinality of natural numbers (ℵ</span><sub>0</sub><span>) is evidently less than the cardinality of reals (also known as the </span><em>cardinality of the continuum</em><span>).</span></p><p>Are there any cardinalities in between? Mathematicians don’t think so, but this hypothesis is provably undecidable within the traditional axioms of set theory.</p><p>Maybe? If infinity lurks in some dark corners of the physical universe, we probably have no way of ascertaining its numerical properties. In the absence of this, we have a toolkit for creating weird worlds that restate the rules of formal logic in increasingly mind-bending ways — and sometimes help prove a theorem or two.</p><p>Because the behavior of infinite sets is bizarre, there is a school of mathematics that rejects their existence. Heck, there is a small number of mathematicians who reject infinity altogether. The difficulty is that such decisions require discarding vast amounts of useful math — or at the very least, tossing out an explanation of why we’re doing that math in a particular way.</p><p>On some level, this might not be a big deal: calculus is usually still taught without providing a rigorous justification for limits or infinitesimals. On the flip side, as almost any calculus student will attest, it’s an intellectually unsatisfying approach.</p><p>Just as important, without all these wonderfully confusing notions of infinity, how do you keep the riff-raff out of math?</p><p><em><span>👉 Reader exclusive: for an essential Peano arithmetic calculator, </span><a href="https://lcamtuf.coredump.cx/peano/" rel="">click here</a><span>. </span></em></p><p><em><span>If you’re interested in beavers in addition to turtles, you’re probably going to enjoy </span><a href="https://lcamtuf.substack.com/p/monkeys-typewriters-and-busy-beavers" rel="">this article</a><span>. And if you like the content, please subscribe; there’s no better way to stay in touch with the writers you like.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comprehension debt: A ticking time bomb of LLM-generated code (469 pts)]]></title>
            <link>https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/</link>
            <guid>45423917</guid>
            <pubDate>Tue, 30 Sep 2025 10:37:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/">https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/</a>, See on <a href="https://news.ycombinator.com/item?id=45423917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	<main id="main">
		
<article id="post-2299">
	<!-- .entry-header -->

	
	
	<div>
		
<p>An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models. </p>



<p>If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.</p>



<p>What <em>is </em>new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.</p>



<p>Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.</p>



<p>But <em>some </em>teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a <em>lot </em>of them.</p>



<p>When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that <em>at some point</em> that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time. </p>



<p>But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it. </p>



<p>“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.</p>



<p>It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.</p>



<p>And we’re sitting on a rapidly growing mountain of it.</p>

<div>
	<p><img referrerpolicy="no-referrer" alt="Unknown's avatar" src="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G" srcset="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G 1x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=63&amp;d=identicon&amp;r=G 1.5x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=84&amp;d=identicon&amp;r=G 2x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=126&amp;d=identicon&amp;r=G 3x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=168&amp;d=identicon&amp;r=G 4x" height="42" width="42" loading="lazy" decoding="async">	</p><!-- .author-avatar -->

	<!-- .author-description -->
</div><!-- .author-info -->
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-2299 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can you use GDPR to circumvent BlueSky's adult content blocks? (102 pts)]]></title>
            <link>https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/</link>
            <guid>45423363</guid>
            <pubDate>Tue, 30 Sep 2025 08:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/">https://shkspr.mobi/blog/2025/09/can-you-use-gdpr-to-circumvent-blueskys-adult-content-blocks/</a>, See on <a href="https://news.ycombinator.com/item?id=45423363">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Disqus Turned My Blog into an Ad Farm – So I Killed It (530 pts)]]></title>
            <link>https://ryansouthgate.com/goodbye-disqus/</link>
            <guid>45423268</guid>
            <pubDate>Tue, 30 Sep 2025 08:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ryansouthgate.com/goodbye-disqus/">https://ryansouthgate.com/goodbye-disqus/</a>, See on <a href="https://news.ycombinator.com/item?id=45423268">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><h4 id="intro">Intro<a href="#intro"></a></h4><p>This will be a short and sweet post. As I’m not big on goodbyes.</p><p>Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.</p><p>At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.</p><p>WELL…….I’ve just seen the ads, and they look horrific!!!</p><h4 id="apologies">Apologies<a href="#apologies"></a></h4><p>I have a <a href="https://pi-hole.net/" target="_blank">Pihole</a> set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a <a href="https://www.wireguard.com/" target="_blank">Wireguard VPN</a> which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.</p><p>After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, I’d forgotten what the typical internet experience feels like.</p><p>I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.</p><p>There were none, instead I was greeted by some horribly formatted and obviously scammy ads:</p><figure><a><img data-src="./disqus_ads.webp" data-action="zoom" alt="Horrible Disqus Ads" src="https://ryansouthgate.com/goodbye-disqus/disqus_ads.webp"></a></figure><p>For the people who read this blog, I’m sorry.</p><p>I became “blind” to what the web is really like for most users. I’ve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:</p><figure><a><img data-src="./disqus_traffic.webp" data-action="zoom" alt="Screenshot of Firefox Dev Tools. Showing a worrying amount of tracking requests" src="https://ryansouthgate.com/goodbye-disqus/disqus_traffic.webp"></a></figure><p>With this post, I’ve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data.
I want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.</p><h4 id="any-alternatives">Any Alternatives?<a href="#any-alternatives"></a></h4><p>I’m not entirely sure comments are needed here. There are other ways to reach me, for example; <a href="https://github.com/ryansouthgate" target="_blank">GitHub</a> or <a href="https://twitter.com/ryan_southgate" target="_blank">Twitter/X</a>. But having a place for discussion under each post can be valuable.
If you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), I’d love to hear them! Please reach out if you’ve found something that works well.</p><p>Thanks as always for reading - your trust matters to me.</p><p><em>Sorry again for the mess!</em></p><hr><hr></div></article></div>]]></description>
        </item>
    </channel>
</rss>