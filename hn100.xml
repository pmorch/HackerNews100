<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 May 2024 17:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Jim Simons has died (200 pts)]]></title>
            <link>https://www.simonsfoundation.org/2024/05/10/simons-foundation-co-founder-mathematician-and-investor-jim-simons-dies-at-86/</link>
            <guid>40320406</guid>
            <pubDate>Fri, 10 May 2024 15:51:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.simonsfoundation.org/2024/05/10/simons-foundation-co-founder-mathematician-and-investor-jim-simons-dies-at-86/">https://www.simonsfoundation.org/2024/05/10/simons-foundation-co-founder-mathematician-and-investor-jim-simons-dies-at-86/</a>, See on <a href="https://news.ycombinator.com/item?id=40320406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>It is with great sadness that the Simons Foundation announces the death of its co-founder and chair emeritus, James Harris Simons, on May 10, 2024, at the age of 86, in New York City.</p>
<p>Jim (as he preferred to be called) was an award-winning mathematician, a legend in quantitative investing, and an inspired and generous philanthropist.</p>
<p>Together with his wife, Simons Foundation chair Marilyn Simons, he gave billions of dollars to hundreds of philanthropic causes, particularly those supporting math and science research and education. In 1994, they<a href="https://www.simonsfoundation.org/"> established the Simons Foundation</a>, which supports scientists and organizations worldwide in advancing the frontiers of research in mathematics and the basic sciences.</p>
<p>Jim was active in the work of the Simons Foundation until the end of his life, and his curiosity and lifelong passion for math and basic science were an inspiration to those around him. He was determined to make a meaningful difference in the level of support that mathematics and basic sciences received in the United States, notably by sponsoring projects that were important but unlikely to find funding elsewhere.</p>
<p>Over its 30-year history, the Simons Foundation’s work has led to breakthroughs in our understanding of autism, the origins of the universe, cellular biology and computational science. Jim and Marilyn’s giving continues to support the next generation of mathematicians and scientists at schools and universities in New York City and around the world.</p>
<p>Jim <a href="https://www.simonsfoundation.org/2024/05/10/remembering-the-life-and-careers-of-jim-simons/">frequently said that he went through three phases in his professional life</a>: mathematician, investor and philanthropist. He previously chaired the math department at Stony Brook University in New York, and his mathematical breakthroughs during that time are now instrumental to fields such as string theory, topology and condensed matter physics.</p>
<p>In 1978, Jim founded what would become <a href="https://www.rentec.com/Home.action?index=true">Renaissance Technologies</a>, a hedge fund that pioneered quantitative trading and became one of the most profitable investment firms in history. He then turned his focus to making a difference in the world through the<a href="https://www.simonsfoundation.org/"> Simons Foundation</a>,<a href="https://www.sfi.org.bm/"> Simons Foundation International</a>,<a href="https://www.mathforamerica.org/"> Math for America</a> and other philanthropic efforts.</p>
<p>“Jim was an exceptional leader who did transformative work in mathematics and developed a world-leading investment company,” says Simons Foundation president David Spergel. “Together with Marilyn Simons, the current Simons Foundation board chair, Jim created an organization that has already had enormous impact in mathematics, basic science and our understanding of autism. The Simons Foundation, an in-perpetuity foundation, will carry their vision for philanthropy into the future.”</p>
<p>Jim Simons is survived by his wife, three children, five grandchildren, a great-grandchild, and countless colleagues, friends and family who fondly recall his genuine curiosity and quick wit.</p>
<p>We know that many people have stories, messages and memories they would like to share about Jim. Please send them to <a href="https://www.simonsfoundation.org/cdn-cgi/l/email-protection#eb8489988e999d82858cab9882868485988d849e858f8a9f828485c584998c"><span data-cfemail="f09f9283958286999e97b083999d9f9e83969f859e949184999f9ede9f8297">[email&nbsp;protected]</span></a>.</p>
<p>Information on memorial services and other events honoring Jim’s life and legacy <a href="https://www.simonsfoundation.org/observing/">will be posted on the Simons Foundation website</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: A web debugger an ex-Cloudflare team has been working on for 4 years (283 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40318542</link>
            <guid>40318542</guid>
            <pubDate>Fri, 10 May 2024 13:08:38 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40318542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN, I wanted to show you a product a small team and I have been working on for 4 years. <a href="https://jam.dev/" rel="nofollow">https://jam.dev</a></p><p>It’s called Jam and it prevents product managers (like I used to be) from being able to create vague and un-reproducible bug tickets (like I used to create).</p><p>It’s actually really hard as a non-engineer to file useful bug tickets for engineers. Like, sometimes I thought I included a screenshot, but the important information the engineer needed was what was actually right outside the boundary of the screenshot I took. Or I'd write that something "didn't work" but the engineer wasn't sure if I meant that it returned an error or if it was unresponsive. So the engineer would be frustrated, I would be frustrated, and fixing stuff would slow to a halt while we went back and forth to clarify how to repro the issue over async Jira comments.</p><p>It’s actually pretty crazy that while so much has changed in how we develop software (heck, we have types in javascript now*), the way we capture and report bugs is just as manual and lossy as it was in the 1990’s. We can run assembly in the browser but there’s still no tooling to help a non-engineer show a bug to an engineer productively.</p><p>So that’s what Jam is. Dev tools + video in a link. It’s like a shareable HAR file synced to a video recording of the session. And besides video, you can use it to share an instant replay of a bug that just happened — basically a 30 second playback of the DOM as a video.</p><p>We’ve spent a lot of time adding in a ton of niceties, like Jam writes automatic repro steps for you, and Jam’s dev tools use the same keyboard shortcuts you’re used to in Chrome dev tools, and our team’s personal favorite: Jam parses GraphQL responses and pulls out mutation names and errors (which is important because GraphQL uses one endpoint for all requests and always returns a 200, meaning you usually have to sift through every GraphQL request when debugging to find the one you’re looking for)</p><p>We’re now 2 years in to the product being live and people have used Jam to fix more than 2 million bugs - which makes me so happy - but there’s still a ton to do. I wanted to open up for discussion here and get your feedback and opinions how can we make it even more valuable for you debugging?</p><p>The worst part of the engineering job is debugging and not even being able to repro the issue, it’s not even really engineering, it’s just a communication gap, one that we should be able to solve with tools. So yeah excited to get your feedback and hear your thoughts how we can make debugging just a little less frustrating.</p><p>(Jam is free to use forever — there is a paid tier for features real companies would need, but we’re keeping a large free plan forever. We learned to build products at Cloudflare and free tier is in our ethos, both my co-founder and I and about half the team is ex-Cloudflare) and what we loved there is how much great feedback we’d get because the product was mostly  free to use. We definitely want to keep that going at Jam.)</p><p>By the way, we’re hiring engineers and if this is a problem that excites you, we’d love to chat: jam.dev/careers</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Popover API (243 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/API/Popover_API</link>
            <guid>40317740</guid>
            <pubDate>Fri, 10 May 2024 11:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Popover_API">https://developer.mozilla.org/en-US/docs/Web/API/Popover_API</a>, See on <a href="https://news.ycombinator.com/item?id=40317740">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en-US"><header><details><summary><span role="img" aria-label="Baseline Check"></span><h2>Baseline<!-- --> <span>2024</span></h2><p>Newly available</p><span></span></summary></details></header><p>The <strong>Popover API</strong> provides developers with a standard, consistent, flexible mechanism for displaying popover content on top of other page content. Popover content can be controlled either declaratively using HTML attributes, or via JavaScript.</p><section aria-labelledby="concepts_and_usage"><h2 id="concepts_and_usage"><a href="#concepts_and_usage">Concepts and usage</a></h2><div><p>A very common pattern on the web is to show content over the top of other content, drawing the user's attention to specific important information or actions that need to be taken. This content can take several different names — overlays, popups, popovers, dialogs, etc. We will refer to them as popovers through the documentation. Generally speaking, these can be:</p>
<ul>
  <li><strong>modal</strong>, meaning that while a popover is being shown, the rest of the page is rendered non-interactive until the popover is actioned in some way (for example an important choice is made).</li>
  <li><strong>non-modal</strong>, meaning that the rest of the page can be interacted with while the popover is being shown.</li>
</ul>
<p>Popovers created using the Popover API are always non-modal. If you want to create a modal popover, a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog"><code>&lt;dialog&gt;</code></a> element is the right way to go. There is significant overlap between the two — you might for example want to create a popover that persists, but control it using declarative HTML. You can turn a <code>&lt;dialog&gt;</code> element into a popover (<code>&lt;dialog popover&gt;</code> is perfectly valid) if you want to combine popover control with dialog semantics.</p>
<p>Typical use cases for the popover API include user-interactive elements like action menus, custom "toast" notifications, form element suggestions, content pickers, or teaching UI.</p>
<p>You can create popovers in two different ways:</p>
<ul>
  <li>Declaratively, via a set of new HTML attributes. A simple popover with a toggle button can be created using the following code:
    <div><pre data-signature="KdnYlrjDZk+9KBeXtIzbj2uatLAvVEOa7d4apg36UVg="><code><span><span><span>&lt;</span>button</span> <span>popovertarget</span><span><span>=</span><span>"</span>mypopover<span>"</span></span><span>&gt;</span></span>Toggle the popover<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
<span><span><span>&lt;</span>div</span> <span>id</span><span><span>=</span><span>"</span>mypopover<span>"</span></span> <span>popover</span><span>&gt;</span></span>Popover content<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
</code></pre></div>
  </li>
  <li>Via a JavaScript API. For example, <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/togglePopover"><code>HTMLElement.togglePopover()</code></a> can be used to toggle a popover between shown and hidden.</li>
</ul>
<p>There are also new events to react to a popover being toggled, and CSS features to aid in styling popovers. All the new features are listed below.</p>
<p>See <a href="https://developer.mozilla.org/en-US/docs/Web/API/Popover_API/Using">Using the popover API</a> for a detailed guide to using this API.</p></div></section><section aria-labelledby="html_attributes"><h2 id="html_attributes"><a href="#html_attributes">HTML attributes</a></h2><div><dl>
  <dt id="popover"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a></dt>
  <dd>
    <p>A global attribute that turns an element into a popover element; takes a popover state (<code>"auto"</code> or <code>"manual"</code>) as its value.</p>
  </dd>
  <dt id="popovertarget"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a></dt>
  <dd>
    <p>Turns a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button"><code>&lt;button&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input"><code>&lt;input&gt;</code></a> element into a popover control button; takes the ID of the popover element to control as its value.</p>
  </dd>
  <dt id="popovertargetaction"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a></dt>
  <dd>
    <p>Specifies the action to be performed (<code>"hide"</code>, <code>"show"</code>, or <code>"toggle"</code>) on the popover element being controlled by a control <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button"><code>&lt;button&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input"><code>&lt;input&gt;</code></a>.</p>
  </dd>
</dl></div></section><section aria-labelledby="css_features"><h2 id="css_features"><a href="#css_features">CSS features</a></h2><div><dl>
  <dt id="backdrop"><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::backdrop"><code>::backdrop</code></a></dt>
  <dd>
    <p>The <code>::backdrop</code> pseudo-element is a full-screen element placed directly behind popover elements, allowing effects to be added to the page content behind the popover(s) if desired (for example blurring it out).</p>
  </dd>
  <dt id="popover-open"><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:popover-open"><code>:popover-open</code></a></dt>
  <dd>
    <p>The <code>:popover-open</code> pseudo-class matches a popover element only when it is in the showing state — it can be used to style popover elements when they are showing.</p>
  </dd>
</dl></div></section><section aria-labelledby="interfaces"><h2 id="interfaces"><a href="#interfaces">Interfaces</a></h2><div><dl>
  <dt id="toggleevent"><a href="https://developer.mozilla.org/en-US/docs/Web/API/ToggleEvent"><code>ToggleEvent</code></a></dt>
  <dd>
    <p>Represents an event notifying the user when a popover element's state toggles between showing and hidden. It is the event object for the <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/beforetoggle_event" title="beforetoggle"><code>beforetoggle</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/toggle_event" title="toggle"><code>toggle</code></a> events, which fire on popovers when their state changes.</p>
  </dd>
</dl></div></section><section aria-labelledby="extensions_to_other_interfaces"><h2 id="extensions_to_other_interfaces"><a href="#extensions_to_other_interfaces">Extensions to other interfaces</a></h2></section><section aria-labelledby="instance_properties"><h3 id="instance_properties"><a href="#instance_properties">Instance properties</a></h3><div><dl>
  <dt id="htmlelement.popover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/popover"><code>HTMLElement.popover</code></a></dt>
  <dd>
    <p>Gets and sets an element's popover state via JavaScript (<code>"auto"</code> or <code>"manual"</code>), and can be used for feature detection. Reflects the value of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a> global HTML attribute.</p>
  </dd>
  <dt id="htmlbuttonelement.popovertargetelement"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLButtonElement/popoverTargetElement"><code>HTMLButtonElement.popoverTargetElement</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLInputElement/popoverTargetElement"><code>HTMLInputElement.popoverTargetElement</code></a></dt>
  <dd>
    <p>Gets and sets the popover element being controlled by the control button. The JavaScript equivalent of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a> HTML attribute.</p>
  </dd>
  <dt id="htmlbuttonelement.popovertargetaction"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLButtonElement/popoverTargetAction"><code>HTMLButtonElement.popoverTargetAction</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLInputElement/popoverTargetAction"><code>HTMLInputElement.popoverTargetAction</code></a></dt>
  <dd>
    <p>Gets and sets the action to be performed (<code>"hide"</code>, <code>"show"</code>, or <code>"toggle"</code>) on the popover element being controlled by the control button. Reflects the value of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a> HTML attribute.</p>
  </dd>
</dl></div></section><section aria-labelledby="instance_methods"><h3 id="instance_methods"><a href="#instance_methods">Instance methods</a></h3><div><dl>
  <dt id="htmlelement.hidepopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/hidePopover"><code>HTMLElement.hidePopover()</code></a></dt>
  <dd>
    <p>Hides a popover element by removing it from the top layer and styling it with <code>display: none</code>.</p>
  </dd>
  <dt id="htmlelement.showpopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/showPopover"><code>HTMLElement.showPopover()</code></a></dt>
  <dd>
    <p>Shows a popover element by adding it to the top layer.</p>
  </dd>
  <dt id="htmlelement.togglepopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/togglePopover"><code>HTMLElement.togglePopover()</code></a></dt>
  <dd>
    <p>Toggles a popover element between the showing and hidden states.</p>
  </dd>
</dl></div></section><section aria-labelledby="events"><h3 id="events"><a href="#events">Events</a></h3><div><dl>
  <dt id="htmlelement"><a href="#htmlelement"><code>HTMLElement</code></a> <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/beforetoggle_event" title="beforetoggle"><code>beforetoggle</code></a> event</dt>
  <dd>
    <p>Fired just before a popover element's state changes between showing and hidden, or vice versa.</p>
  </dd>
  <dt id="htmlelement_2"><a href="#htmlelement_2"><code>HTMLElement</code></a> <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/toggle_event" title="toggle"><code>toggle</code></a> event</dt>
  <dd>
    <p>Fired just after a popover element's state changes between showing and hidden, or vice versa. This event already existed to signal state changes on <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/details"><code>&lt;details&gt;</code></a> elements, and it seemed logical to extend it for popover elements.</p>
  </dd>
</dl></div></section><section aria-labelledby="examples"><h2 id="examples"><a href="#examples">Examples</a></h2><div><p>See our <a href="https://mdn.github.io/dom-examples/popover-api/" target="_blank">Popover API examples landing page</a> to access the full collection of MDN popover examples.</p></div></section><h2 id="specifications"><a href="#specifications">Specifications</a></h2><table><thead><tr><th scope="col">Specification</th></tr></thead><tbody><tr><td><a href="https://html.spec.whatwg.org/multipage/popover.html#dom-popover">HTML Standard<!-- --> <br><small># <!-- -->dom-popover</small></a></td></tr></tbody></table><h2 id="browser_compatibility"><a href="#browser_compatibility">Browser compatibility</a></h2><p>BCD tables only load in the browser</p><section aria-labelledby="see_also"><h2 id="see_also"><a href="#see_also">See also</a></h2><div><ul>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a> HTML global attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a> HTML attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a> HTML attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::backdrop"><code>::backdrop</code></a> CSS pseudo-element</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:popover-open"><code>:popover-open</code></a> CSS pseudo-class</li>
</ul></div></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is the largest root of a random polynomial more likely to be real than complex? (123 pts)]]></title>
            <link>https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex</link>
            <guid>40316788</guid>
            <pubDate>Fri, 10 May 2024 09:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex">https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex</a>, See on <a href="https://news.ycombinator.com/item?id=40316788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                
<p><em>This question might be hard because <a href="https://math.stackexchange.com/questions/4908114/are-the-root-of-a-polynomial-with-the-largest-or-the-smallest-modulus-more-likel">it got <span>$35$</span> upvotes in MSE and also had a <span>$200$</span> points bounty</a> by Jyrki Lahtonen but it was unanswered. So I am posting it in MO.</em></p>
<p>The number of real roots of a random polynomial with real coefficients is much smaller than the number of complex roots. Assume that the coefficients are independently and uniformly random in <span>$(-1,1)$</span> for if not then we can divide each coefficient by the coefficient with the largest absolutely value to scale each coefficient to <span>$(-1,1)$</span>. The number of real roots of a polynomial of degree <span>$n$</span> is asymptotic to <span>$\displaystyle \frac{2\log n}{\pi} + o(1)$</span>. This means that the number of complex roots is approximately <span>$\displaystyle n - \frac{2\log n}{\pi}$</span>. Similar asymptotics hold for <a href="https://arxiv.org/pdf/1409.4128" rel="noreferrer">other distribution of the coefficients</a>.</p>
<blockquote>
<p>**Definition **: <em>The largest (or smallest) root of a polynomial is the root with the largest (or smallest) modulus.</em></p>
</blockquote>
<p><a href="https://i.sstatic.net/TpOFTU8J.png" rel="noreferrer"><img src="https://i.sstatic.net/TpOFTU8J.png" alt="Roots scatter plot"></a></p>
<p>The above graph shows the roots one such polynomial with degree <span>$101$</span>. The largest root is in the top right corner in green.</p>
<p>We can ask if the largest (or the smallest) root more likely to be real or complex? Since there are exponentially more complex roots than real roots as seen from the above asymptotic, my naive guess was that the largest (or the smallest) root is more likely to be complex. However experimental data proved to be quite counterintuitive.</p>
<p><a href="https://i.sstatic.net/nuuqhaXP.png" rel="noreferrer"><img src="https://i.sstatic.net/nuuqhaXP.png" alt="Probability that the largest root is real"></a></p>
<p>The data shows that:</p>
<ol>
<li>Probability that the largest (or smallest) root is real is greater than the probability that it is complex.</li>
<li>And this probability decreases to some value near <span>$1/2$</span> as <span>$n \to \infty$</span> as shown in the above graph (created using a Monte Carlo simulation with <span>$10^5$</span> trials for each value of <span>$n$</span>).</li>
<li><strong>Note</strong>: Instead of uniform distribution, if we assume that the coefficients are normally distributed with mean <span>$0$</span> and standard deviation <span>$1$</span> and scaled to <span>$(-1,1)$</span>, the above observation and limiting probabilities hold.</li>
</ol>
<p>It is counterintuitive that despite being much (exponentially) fewer in number, real roots are more likely to contain the both largest and the smallest roots of a random polynomial. <em>In this sense, the largest and the smallest roots are both biased towards reals</em>.</p>
<p><strong>Question 1</strong>: What is the reason for this bias?</p>
<p><strong>Question 2</strong>: Does the probability that the largest (or the smallest) root of a polynomial of degree <span>$n$</span> is real converge (to some value near <span>$\frac{1}{2}$</span> as <span>$n \to \infty$</span>)?</p>
<p><strong>Note</strong>: We can quantify the observed bias as follows. Let <span>$P(L|R)$</span> be the probability that a root is the largest given that it is real and let <span>$P(L|C)$</span> be the probability that a root is the largest given that it is complex. Similarly, let <span>$P(S|R)$</span> be the probability that a root is the smallest given that it is real and let <span>$P(S|C)$</span> be the probability that a root is the smallest given that it is complex. Then the experimental data says that</p>
<p><span>$$
P(L|R) = P(S|R) \approx \frac{\pi}{4\log n},
$$</span></p>
<p><span>$$
P(L|C) = P(S|C) \approx \frac{\pi}{2n\pi - 4\log n}.
$$</span></p>
<p><strong>Update</strong>: In the <a href="https://math.stackexchange.com/questions/4908114/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex">linked MSE post</a>, it has now been proved that the probability that the largest root is real is at least</p>
<p><span>$$
\frac{23-16\sqrt{2}}{6} \approx 6.2 \%
$$</span></p>
<p><strong>Related</strong>: <a href="https://math.stackexchange.com/questions/4906738/what-is-the-probability-that-the-absolute-value-of-the-roots-of-a-polynomial-of">What is the probability that the absolute value of the roots of a polynomial of degree <span>$n$</span> is greater than <span>$x$</span></a>?</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elvish, expressive programming language and a versatile interactive shell (144 pts)]]></title>
            <link>https://elv.sh</link>
            <guid>40316010</guid>
            <pubDate>Fri, 10 May 2024 06:38:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elv.sh">https://elv.sh</a>, See on <a href="https://news.ycombinator.com/item?id=40316010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
    <article>
      
      <div>
        <div>
<p><strong>Elvish</strong> (<em>noun</em>):</p>
<ol>
<li>
<p>A powerful scripting language.</p>
</li>
<li>
<p>A shell with useful interactive features built-in.</p>
</li>
<li>
<p>A statically linked binary for Linux, BSDs, macOS or Windows.</p>
</li>
</ol>
</div>
<section>
<header>
<p>Powerful modern shell scripting</p>
</header>
<div>
<p>Write readable and maintainable scripts - no cryptic operators, no
double-quoting every variable.</p>
<pre><header><p>jpg-to-png.elv <a href="https://elv.sh/learn/scripting-case-studies.html#jpg-to-png.elv">(explainer)</a></p>
</header><code><span>for</span> <span>x</span> <span>[</span>*.jpg<span>]</span> <span>{</span>
  <span>gm</span> convert <span>$x</span> <span>(</span><span>str:trim-suffix</span> <span>$x</span> .jpg<span>)</span>.png
<span>}</span>
</code></pre>
<p>Power up your workflows with data structures and functional programming.</p>
<pre><header><p>update-servers-in-parallel.elv <a href="https://elv.sh/learn/scripting-case-studies.html#update-servers-in-parallel.elv">(explainer)</a></p>
</header><code><span>var</span> <span>hosts</span> <span>=</span> <span>[</span><span>[</span><span>&amp;</span>name=a <span>&amp;</span>cmd=<span>'apt update'</span><span>]</span>
             <span>[</span><span>&amp;</span>name=b <span>&amp;</span>cmd=<span>'pacman -Syu'</span><span>]</span><span>]</span>
<span># peach = "parallel each"</span>
<span>peach</span> <span>{</span><span>|</span>h<span>|</span> <span>ssh</span> root@<span>$h</span><span>[</span>name<span>]</span> <span>$h</span><span>[</span>cmd<span>]</span> <span>}</span> <span>$hosts</span>
</code></pre>
<p>Catch errors before code executes.</p>
<pre><header><p>Terminal: elvish <a href="https://elv.sh/learn/scripting-case-studies.html#catching-errors-early">(explainer)</a></p>
</header><code>~&gt; <span>var</span> <span>project</span> <span>=</span> ~/project
~&gt; <span>rm</span> -rf <span>$projetc</span>/bin
compilation error: variable $projetc not found
</code></pre>
</div>
</section>
<section>
<header>
<p>Run it anywhere</p>
</header>
<div>
<p>Elvish comes in a single statically linked binary for your laptop, your server,
your PC, or your Raspberry Pi.</p>
<pre><header><p>Terminal: Raspberry Pi</p>
</header><code>~&gt; <span>wget</span> dl.elv.sh/linux-arm64/elvish-HEAD.tar.gz
~&gt; <span>tar</span> -C /usr/local/bin -xvf elvish-HEAD.tar.gz
elvish
~&gt; <span>elvish</span>
</code></pre>
<p>Use Elvish in your CI/CD pipelines. Convenient shell syntax and modern
programming language - why not both?</p>
<pre><header><p>github-actions.yaml</p>
</header><code>steps:
  - uses: elves/setup-elvish@v1
    with:
      elvish-version: HEAD
  - name: Run something with Elvish
    shell: elvish {0}
    run: |
      echo Running Elvish $version
</code></pre>
</div>
</section>
<section>
<header>
<p>Interactive shell with batteries included</p>
</header>
<div>
<p>Press <kbd>Ctrl-L</kbd> for directory history, and let Elvish find
<code>java/com/acme/project</code> for you.</p>
<pre><header><p>Terminal: elvish - directory history <a href="https://elv.sh/learn/tour.html#directory-history">(more)</a></p>
</header><code>~&gt;                                          <span>elf@host</span>
<span> LOCATION </span> j
<span> 10 ~/java/com/acme/project/utilities               </span>
 10 ~/java/com/acme/project
 10 /opt/java
</code></pre>
<p>Press <kbd>Ctrl-R</kbd> for command history. That beautiful <code>ffmpeg</code> command you
crafted two months ago is still there.</p>
<pre><header><p>Terminal: elvish - command history <a href="https://elv.sh/learn/tour.html#command-history">(more)</a></p>
</header><code>~&gt;                                          <span>elf@host</span>
<span> HISTORY (dedup on) </span> ff                 <span>Ctrl-D</span> dedup
  34 ffmpeg -i input.mp4 -c:v libx264 -c:a aac outpu
<span>  35 ffmpeg -i input.mp4 -vf "transpose=1,scale=640:</span>
</code></pre>
<p>Press <kbd>Ctrl-N</kbd> for the builtin file manager. Explore directories and
files without leaving the comfort of your shell.</p>
<pre><header><p>Terminal: elvish - file manager <a href="https://elv.sh/learn/tour.html#navigation-mode">(more)</a></p>
</header><code>~/elvish&gt;                                   <span>elf@host</span>
<span> NAVIGATING </span>             <span>Ctrl-H</span> hidden <span>Ctrl-F</span> filter
<span> bash  </span> <span> 1.0-release.m </span><span> </span> 1.0 has not been released y
<span> elvis </span>  CONTRIBUTING. <span> </span>
<span> zsh   </span>  Dockerfile    <span> </span>
         LICENSE       <span> </span>
         Makefile      <span>│</span>
         PACKAGING.md  <span>│</span>
         README.md     <span>│</span>
         SECURITY.md   <span>│</span>
</code></pre>
</div>
</section>


      </div>
      
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roman Tyrian purple snail dye found in UK for first time (143 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cjje132jvygo</link>
            <guid>40315970</guid>
            <pubDate>Fri, 10 May 2024 06:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cjje132jvygo">https://www.bbc.com/news/articles/cjje132jvygo</a>, See on <a href="https://news.ycombinator.com/item?id=40315970">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Player-Driven Emergence in LLM-Driven Game Narrative (105 pts)]]></title>
            <link>https://arxiv.org/abs/2404.17027</link>
            <guid>40315434</guid>
            <pubDate>Fri, 10 May 2024 04:05:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.17027">https://arxiv.org/abs/2404.17027</a>, See on <a href="https://news.ycombinator.com/item?id=40315434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+X">Xiangyu Peng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Quaye,+J">Jessica Quaye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+W">Weijia Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brockett,+C">Chris Brockett</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dolan,+B">Bill Dolan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jojic,+N">Nebojsa Jojic</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=DesGarennes,+G">Gabriel DesGarennes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lobb,+K">Ken Lobb</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+M">Michael Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leandro,+J">Jorge Leandro</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+C">Claire Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rao,+S">Sudha Rao</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2404.17027">View PDF</a>
    <a href="https://arxiv.org/html/2404.17027v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We explore how interaction with large language models (LLMs) can give rise to emergent behaviors, empowering players to participate in the evolution of game narratives. Our testbed is a text-adventure game in which players attempt to solve a mystery under a fixed narrative premise, but can freely interact with non-player characters generated by GPT-4, a large language model. We recruit 28 gamers to play the game and use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player's gameplay. We find that through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes that were not a part of the original narrative but have potential for being fun and engaging. Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Sudha Rao [<a href="https://arxiv.org/show-email/4788f6e5/2404.17027">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 25 Apr 2024 20:39:44 UTC (1,287 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Object oriented design patterns in the Linux kernel (2011) (102 pts)]]></title>
            <link>https://lwn.net/Articles/444910/</link>
            <guid>40315400</guid>
            <pubDate>Fri, 10 May 2024 03:58:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/444910/">https://lwn.net/Articles/444910/</a>, See on <a href="https://news.ycombinator.com/item?id=40315400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>This article brought to you by LWN subscribers</b><p>Subscribers to LWN.net made this article — and everything that
       surrounds it — possible.  If you appreciate our content, please
       <a href="https://lwn.net/subscribe/">buy a subscription</a> and make the next
       set of articles possible.</p></div>
           </center>
           <p>
Despite the fact that the Linux Kernel is mostly written in C, it
makes broad use of some techniques from the field of object-oriented
programming.
Developers wanting to use these object-oriented techniques receive
little support or guidance from the language and so are left to fend
for themselves.  As is often the case, this is a double-edged sword.
The developer has enough flexibility to do really cool things, and
equally the flexibility to do really stupid things, and it isn't
always clear at first glance which is which, or more accurately: where
on the spectrum a particular approach sits.
</p><p>
Instead of looking to the language to provide guidance, a software
engineer must look to established practice to find out what works well
and what is best avoided.  Interpreting established practice is not
always as easy as one might like and the effort, once made, is worth
preserving.
To preserve that effort on your author's part, this article brings
another installment in an
<a href="https://lwn.net/Articles/336224/">occasional series</a> on Linux Kernel Design
Patterns and attempts to set out - with examples - the design patterns
in the Linux Kernel which effect an object-oriented style of
programming.
</p><p>
Rather than providing a brief introduction to the object-oriented
style, tempting though that is, we will assume the reader has a basic
knowledge of objects, classes, methods, inheritance, and similar
terms.  For those as yet unfamiliar with these, there are plenty of
resources to be found elsewhere on the web.
</p><p>
Over two weeks we will look for patterns in just two areas:  method
dispatch and data inheritance.  Despite their apparent
simplicity they lead to some rich veins for investigation.
This first article will focus on method dispatch.
</p>
<h3>Method Dispatch</h3>
<p>
The large variety of styles of inheritance and rules for its usage in
languages today seems to suggest that there is no uniform
understanding of what "object-oriented" really means.  The term is a bit like
"love": everyone thinks they know what it means but when you get down
to details people can find they have very different ideas.

While what it means to be "oriented" might not be clear, what we mean
by an "object" does seem to be uniformly agreed upon.  It is simply an
abstraction comprising both state and behavior.  An object is like a
record (Pascal) or struct (C), except that some of the names of members
refer to functions which act on the other fields in the object.
These function members are sometimes referred to a "methods".
</p><p>
The most obvious way to implement objects in C is to declare a
"struct" where some fields are pointers to functions which take a
pointer to the struct itself as their first argument.  The calling
convention for method "foo" in object "bar" would simply be:
<tt>bar-&gt;foo(bar, ...args);</tt>

While this pattern is used in the Linux kernel it is not the dominant
pattern so we will leave discussion of it until a little later.
</p><p>
As methods (unlike state) are not normally changed on a per-object
basis, a more common and only slightly less obvious approach is to
collect all the methods for a particular class of objects into a
separate structure, sometimes known as a "virtual function table" or
<a href="http://en.wikipedia.org/wiki/Virtual_method_table">vtable</a>.

The object then has a single pointer to this table rather than a
separate pointer for each method, and consequently uses less memory.
</p><p>
This then leads to our first pattern - a <b>pure vtable</b> being a
structure which contains only function pointers where the first argument of
each is a pointer to some other structure (the object type) which itself
contains a pointer to this vtable.

Some simple examples of this in the Linux kernel are the
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1062"><tt>file_lock_operations</tt></a>
structure which contains two function pointers
each of which take a pointer to a <tt>struct file_lock</tt>, and the
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/seq_file.h#L29"><tt>seq_operations</tt></a> vtable which contains four function pointers
which each operate on a <tt>struct seq_file</tt>.

These two examples display an obvious naming pattern - the structure
holding a vtable is named for the structure holding the object
(possibly abbreviated) followed by "_operations".  While this pattern is
common it is by no means universal.   Around the time of 2.6.39
there are approximately 30 "*_operations" structures along with well over
100 "*_ops" structures, most if not all of which are vtables of some
sort. There are also several structs such as <tt>struct mdk_personality</tt>
which are essentially vtables but do not have particularly helpful
names.
</p><p>
Among these nearly 200 vtable structures there is plenty of variability
and so plenty of scope to look for interesting patterns.  In
particular we can look for common variations from the "pure vtable"
pattern described above and determine how these variations contribute
to our understanding of object use in Linux.
</p>
<h4>NULL function pointers</h4>
<p>
The first observation is that some function pointers in some vtables
are allowed to be NULL.  Clearly trying to call such a function would
be futile, so the code that calls into these methods generally
contains an explicit test for the pointer being NULL.  There are a few
different reasons for these NULL pointers.

Probably easiest to justify is the incremental development
reason.  Because of the way vtable structures are initialized, adding
a new function pointer to the structure definition causes all existing
table declarations to initialise that pointer to NULL.  Thus it is
possible to add a caller of the new method before any instance
supports that method, and have it check for NULL and perform a default
behavior.   Then as incremental development continues those vtable
instances which need it can get non-default methods.
</p><p>
A recent example is commit
<a href="http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=77af1b2641faf4">77af1b2641faf4</a> adding
<tt>set_voltage_time_sel()</tt> to <tt>struct regulator_ops</tt> which acts on
<tt>struct regulator_dev</tt>.  Subsequent commit
<a href="http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=42ab616afe8844">42ab616afe8844</a>
defines that method
for a particular device.  This is simply the most recent example of a
very common theme.
</p><p>
Another common reason is that certain methods are not particularly
meaningful in certain cases so the calling code simply tests for NULL
and returns an appropriate error when found.  There are multiple
examples of this in the virtual filesystem (VFS) layer.  For instance,
the <tt>create()</tt> function in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1569"><tt>inode_operations</tt></a>
is only meaningful if the inode in question is a
directory.  So <tt>inode_operations</tt> structures for non-directories
typically have NULL for the <tt>create()</tt> function (and many others) and
the calling code in <tt>vfs_create()</tt> checks for NULL and returns <tt>-EACCES</tt>.
</p><p>
A final reason that vtables sometimes contain NULL is that an element
of functionality might be being transitioned from one interface to
another.  A good example of this is the <tt>ioctl()</tt> operation in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1537"><tt>file_operations</tt></a>.
In 2.6.11, a new method, <tt>unlocked_ioctl()</tt> was added
which was called without the big kernel lock held.  In 2.6.36, when all
drivers and filesystems had been converted to use <tt>unlocked_ioctl()</tt>,
the original <tt>ioctl()</tt> was finally removed.  During this transition a file system
would typically define only one of two, leaving the other defaulting to NULL.
</p><p>
A slightly more subtle example of this is <tt>read()</tt> and <tt>aio_read()</tt>, also in
<tt>file_operations</tt>, and the corresponding <tt>write()</tt> and <tt>aio_write()</tt>.
<tt>aio_read()</tt> was introduced to support asynchronous IO, and if it is
provided the regular synchronous <tt>read()</tt> is not needed (it is effected
using <tt>do_sync_read()</tt> which calls the <tt>aio_read()</tt> method).  In this case
there appears to be no intention of ever removing <tt>read()</tt> - it will
remain for cases where async IO is not relevant such as special
filesystems like procfs and sysfs.  So it is still the case that only one of each
pair need be defined by a filesystem, but it is not simply a transition, it is
a long-term state.
</p><p>
Though there seem to be several different reasons for a NULL function pointer,
almost every case is an example of one simple pattern - that of providing a
default implementation for the method.  In the "incremental development"
examples and the non-meaningful method case, this is fairly straightforward.
e.g. the default for <tt>inode-&gt;create()</tt> is simply to return an error.
In the interface transition case it is only slightly less obvious.  The default for
<tt>unlocked_ioctl()</tt> would be to take the kernel lock and then call
the <tt>ioctl()</tt> method.  The
default for <tt>read()</tt> is exactly <tt>do_sync_read()</tt> and some filesystems such
as <tt><a href="http://lxr.linux.no/#linux+v2.6.39/fs/ext3/file.c#L55">ext3</a></tt>
actually provide this value explicitly rather than using
"NULL" to indicate a default.
</p><p>
With that in mind, a little reflection suggests that if the real goal is to
provide a default, then maybe the best approach would be to explicitly give a
default rather than using the circuitous route of using a default of NULL and
interpreting it specially.
</p><p>
While NULL is certainly the easiest value to provide as a default - as the C
standard assures us that uninitialized members of a structure do get set to
NULL - it is not very much harder to set a more meaningful default.
I am indebted to LWN reader
<a href="https://lwn.net/Articles/437878/">wahern</a> for the observation that
C99 allows fields in a structure to be initialized multiple times with only the
final value taking effect and that this allows easy setting of default values
such as by following the simple model:
</p>
<pre>    #define FOO_DEFAULTS  .bar = default_bar, .baz = default_baz
    struct foo_operations my_foo = { FOO_DEFAULTS,
	.bar = my_bar,
    };
</pre><p>
This will declare <tt>my_foo</tt> with a predefined default value for <tt>baz</tt> and a
localized value for <tt>bar</tt>.  Thus for the small cost of defining a few "default"
functions and including a "<tt>_DEFAULTS</tt>" entry to each declaration, the
default value for any field can easily be chosen when the field is first
created, and automatically included in every use of the structure.
</p><p>
Not only are meaningful defaults easy to implement, they can lead to a more
efficient implementation.  In those cases where the function pointer actually
is NULL it is probably faster to test and branch rather than to make an
indirect function call.  However the NULL case is very often
the exception rather than the rule, and optimizing for an
exception is not normal practice.  In the more common case when
the function pointer is not NULL, the test for NULL is simply a
waste of code space and a waste of execution time.  If we
disallow NULLs we can make all call sites a little bit smaller and simpler.
</p><p>
In general, any testing performed by the caller before calling a method
can be seen as an instance of the "mid-layer mistake" discussed
<a href="https://lwn.net/Articles/336262/">in a previous article</a>.  It shows that the
mid-layer is making 
assumptions about the behavior of the lower level driver rather than simply
giving the driver freedom to behave in whatever way is most
suitable.  This may not always be an expensive mistake, but it
is still best avoided where possible.

Nevertheless there is a clear pattern in the Linux kernel that
pointers in vtables can sometimes be NULLable, typically though not
always to enable a transition, and the call sites should in these
cases test for NULL before proceeding with the call.
</p><p>
The observant reader will have noticed a hole in the above logic
denouncing the use NULL pointers for defaults.  In the case where the
default is the common case and where performance is paramount, the
reasoning does not hold and a NULL pointer could well be justified.
Naturally the Linux kernel provides an example of such a case for our
examination.
</p><p>
One of the data structures used by the VFS for caching filesystem
information is the "dentry".  A "dentry" represents a name in the
filesystem, and so each "dentry" has a parent, being the directory
containing it, and an "inode" representing the named file.  The dentry
is separate from the inode because a single file can have multiple
names (so an "inode" can have multiple "dentry"s).

There is a <tt>dentry_operations</tt> vtable with a number of operations including,
for example, "d_compare" which will compare two names and "d_hash"
which will generate a hash for the name to guide the storage of the
"dentry" in a hash table.  Most filesystems do not need this flexibility.  They
treat names as uninterpreted strings of bytes so the default compare and hash
functions are the common case.  A few filesystems define these to
handle case-insensitive names but that is not the norm.
</p><p>
Further, filename lookup is a common operation in Linux and so
optimizing it is a priority.  Thus these two operations
appear to be good candidates where a test for NULL and an inlined
default operation might be appropriate.  What we find though is that
when such an optimization is warranted it is not by itself enough.

The code that calls <tt>d_compare()</tt> and <tt>d_hash()</tt> (and a couple of other dentry
operations) does not test these functions for NULL directly.  Rather
they require that a few flag bits (DCACHE_OP_HASH, DCACHE_OP_COMPARE)
in the "dentry" are set up to indicate whether the common default
should be used, or whether the function should be called.  As the flag
field is likely to be in cache anyway, and the <tt>dentry_operations</tt>
structure will often be not needed at all, this avoids a memory fetch
in a hot path.
</p><p>
So we find that the one case where using a NULL function pointer to
indicate a default could be justified, it is not actually used; instead, a
different, more efficient,
mechanism is used to indicate that the default method is requested.
</p>
<h4>Members other than function pointers</h4>
<p>
While most vtable-like structures in the kernel contain exclusively
function pointers, there are a significant minority that have
non-function-pointer fields.  Many of these appear on the surface
quite arbitrary and a few closer inspections suggest that some of them
result of poor design or bit-rot and their removal would only improve
the code.
</p><p>
There is one exception to the "functions only" pattern that occurs
repeatedly and provides real value, and so is worth exploring.
This pattern is seen in its most general form in
<a href="http://lxr.linux.no/#linux+v2.6.39/drivers/md/md.h#L348"><tt>struct mdk_personality</tt></a>
which provides operations for a particular software
RAID level.  In particular this structure contains an "owner", a
"name", and a "list".  The "owner" is the module that provides the
implementation.  The "name" is a simple identifier: some vtables have
string names, some have numeric names, and it is often called
something different like "version", "family", "drvname", or
"level".  But conceptually it is still a name.  In the present example
there are two names, a string and a numeric "level".
</p><p>
The "list", while part of the same functionality, is less
common. The mdk_personality structure has a <tt>struct list_head</tt>, as does
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/textsearch.h#L37"><tt>struct ts_ops</tt></a>.
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1793"><tt>struct file_system_type</tt></a> 
has a simple pointer to the next
<tt>struct file_system_type</tt>.

The underlying idea here is that for any particular implementation of
an interface (or "final" definition of a class) to be usable, it must
be registered in some way so that it can be found.  Further, once it
has been found it must be possible to ensure that the module holding
the implementation is not removed while it is in use.
</p><p>
There seem to be nearly as many styles of registration against an
interface in Linux as there are interfaces to register against, so
finding strong patterns there would be a difficult task.  However it
is fairly common for a "vtable" to be treated as the primary handle on
a particular implementation of an interface and to have an "owner"
pointer which can be used to get a reference on the module which
provides the implementation.
</p><p>
So the pattern we find here is that a structure of function pointers
used as a "vtable" for object method dispatch should normally contain
<b>only</b> function pointers.  Exceptions require clear justification.  A
common exception allows a module pointer and possible other fields such
as a name and a list pointer. These fields are used to support
the registration protocol for the particular interface.
When there is no list pointer it is very likely that the entire vtable will
be treated as read-only.  In this case the vtable will often be declared as
a <tt>const</tt> structure and so could even be stored in read-only memory.

</p>
<h4>Combining Methods for different objects</h4>
<p>
A final common deviation from the "pure vtable" pattern that we see in
the Linux kernel occurs when the first argument to the function is not
always the same object type.  In a pure vtable which is referenced by
a pointer in a particular data structure, the first argument of
each function is exactly that data structure.  What reason could there
be for deviating from that pattern?  It turns out that there are few,
some more interesting than others.
</p><p>
The simplest and least interesting explanation is that, for no apparent
reason, the target data structure is listed elsewhere in the argument
list.  For example all functions in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fb.h#L623"><tt>struct
fb_ops</tt></a>
take a <tt>struct fb_info</tt>.  While in 18 cases that structure is the first argument, in
five cases it is the last.  There is nothing obviously wrong with this
choice and it is unlikely to confuse developers.  It is only a problem
for data miners like your author who need to filter it out as an
irrelevant pattern.
</p><p>
A slight deviation on this pattern is seen in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/rfkill.h#L145"><tt>struct rfkill_ops</tt></a>
where two functions take a <tt>struct rkfill</tt> but the third - <tt>set_block()</tt> -
takes a <tt>void *data</tt>.  Further investigation shows that this opaque
<tt>data</tt> is exactly that which is stored in <tt>rfkill-&gt;data</tt>, so <tt>set_block()</tt>
could easily be defined to take a <tt>struct rfkill</tt> and simply to follow
the <tt>-&gt;data</tt> link itself.  This deviation is sufficiently non-obvious
that it could conceivably confuse developers as well as data miners
and so should be avoided.
</p><p>
The next deviation in seen for example in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/suspend.h#L108"><tt>platform_suspend_ops</tt></a>,
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/oprofile.h#L51"><tt>oprofile_operations</tt></a>,
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/security.h#L1375"><tt>security_operations</tt></a>
and a few others.  These
take an odd assortment of arguments with no obvious pattern.  However
these are really very different sorts of vtable structures in that the
object they belong to are singletons.  There is only one active
platform, only one profiler, only one security policy.  Thus the
"object" on which these operations act is part of the global state and
so does not need to be included in the arguments of any functions.
</p><p>
Having filtered these two patterns out as not being very interesting
we are left with two that do serve to tell us something about object
use in the kernel.
</p><p>
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/quota.h#L301"><tt>quota_format_ops</tt></a>
and
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/exportfs.h#L167"><tt>export_operations</tt></a>
are two different operations structures that operate on a variety of different
data structures.  In each case the apparent primary object (e.g. a
<tt>struct super_block</tt> or a <tt>struct dentry</tt>) already has a vtable
structure dedicated to it (such as <tt>super_operations</tt> or
<tt>dentry_operations</tt>) and these new structures add new operations.  In
each case the new operations form a cohesive unit providing a related
set of functionality - whether supporting disk quotas or NFS export.
They don't all act on the same object simply because the functionality
in question depends on a variety of objects.
</p><p>
The best term from the language of object-oriented programming for this
is probably the "<a href="http://en.wikipedia.org/wiki/Mixin">mixin</a>".
Though the fit may not be perfect -
depending on what your exact understanding of mixin is - the idea of
bringing in a collection of functionality without using strict hierarchical
inheritance is very close to the purpose of <tt>quota_format_ops</tt> and
<tt>export_operations</tt>.
</p><p>
Once we know to be on the lookout for mixins like these we can find
quite a few more examples.  The pattern to be alert for is not the one
that led us here - an operations structure that operates on a variety
of different objects - but rather the one we found where the functions
in an "operations" structure operate on objects that already have their
own "operations" structure.  When an object has a large number of
operations that are relevant and these operations naturally group into
subsets, it makes a lot of sense to divide them into separate
vtable-like structures.  There are several examples of this in the
networking code where for instance both
<a href="http://lxr.linux.no/#linux+v2.6.39/include/net/tcp.h#L661"><tt>tcp_congestion_ops</tt></a>
and
<a href="http://lxr.linux.no/#linux+v2.6.39/include/net/inet_connection_sock.h#L38"><tt>inet_connection_sock_af_ops</tt></a>
operate (primarily) on a <tt>struct sock</tt>,
which itself has already got a small set of dedicated operations.
</p><p>
So the pattern of a "mixin" - at least as defined as a set of operations
which apply to one or more objects without being the primary
operations for those objects - is a pattern that is often found in the
kernel and appears to be quite valuable in allowing better
modularization of code.
</p><p>
The last pattern which explains non-uniform function targets is
probably the most interesting, particularly in its contrast to the
obvious application of object-oriented programming style.

Examples of this pattern abound with
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/libata.h#L803"><tt>ata_port_operations</tt></a>,
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/tty_driver.h#L243"><tt>tty_operations</tt></a>,
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/nfs_xdr.h#L1125"><tt>nfs_rpc_ops</tt></a>
and
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/atmdev.h#L388"><tt>atmdev_ops</tt></a>
all appearing as
useful examples.  However we will focus primarily on some examples
from the filesystem layer, particularly
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1613"><tt>super_operations</tt></a>
and
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/fs.h#L1569"><tt>inode_operations</tt></a>.
</p><p>
There is a strong hierarchy of objects in the implementation of a
filesystem where the filesystem - represented by a "super_block" - has a
number of files (<tt>struct inode</tt>) which may have a number of names or
links (<tt>struct dentry</tt>).  Further each file might store data in the page
cache (<tt>struct address_space</tt>) which comprises a number of individual
pages (<tt>struct page</tt>).

There is a sense in which all of these different objects belong to the
filesystem as a whole.  If a page needs to be loaded with data from a
file, the filesystem knows how to do that, and it is probably the same
mechanism for every page in every file.  Where it isn't always the
same, the filesystem knows that too.  So we could conceivably store
every operation on every one of these objects in the
<tt>struct super_block</tt>, as it represents the filesystem and could know what to
do in each case.
</p><p>
In practice that extreme is not really helpful.  It is quite likely
that while there are similarities between the storage of a regular file
and a directory, there are also important differences and being able
to encode those differences in separate vtables can be helpful.
Sometimes small symbolic links are stored directly in the inode while
larger links are stored like the contents of a regular file.  Having
different <tt>readlink()</tt> operations for the two cases can make the code a
lot more readable.
</p><p>
While the extreme of every operation attached to the one central
structure is not ideal, it is equally true that the opposite extreme
is not ideal either.  The <tt>struct page</tt> in Linux does not have a
vtable pointer at all - in part because we want to keep the structure
as small as possible because it is so populous.  Rather the
<tt>address_space_operations</tt> structure contains the operations that act
on a page.  Similarly the <tt>super_operations</tt> structure contains some
operations that apply to inodes, and <tt>inode_operations</tt> contains some
operations that apply to dentries.
</p><p>
It is clearly possible to have operations structures attached to a
parent of the target object - providing the target holds a reference
to the parent, which it normally does - though it is not quite so clear that
it is always beneficial.  In the case of <tt>struct page</tt> which avoids
having a vtable pointer altogether the benefit is clear.  In the case
of <tt>struct inode</tt> which has its own vtable pointer, the benefit of
having some operations (such as <tt>destroy_inode()</tt> or <tt>write_inode()</tt>)
attached to the super_block is less clear.
</p><p>
As there are several vtable structures where any given function
pointer could be stored, the actual choice is in many cases little
more than historical accident.  Certainly the proliferation
of <tt>struct dentry</tt> operations in <tt>inode_operations</tt> seems to be
largely due to the fact that some of them used to act directly
on the inode, but changes in the VFS eventually required this
to change.  For example in 2.1.78-pre1, each of <tt>link()</tt>,
<tt>readlink()</tt>, <tt>followlink()</tt> (and some others which are now
defunct) were 
<a href="http://git.kernel.org/?p=linux/kernel/git/davej/history.git;a=commitdiff;h=5770cca79b11eaf962b41008c2eb1e845c3aae51#patch45">changed</a>
from taking a <tt>struct inode</tt> to
take a <tt>struct dentry</tt> instead.  This set the scene for "dentry"
operations to be in <tt>inode_operations</tt>, so when setattr and getattr
were added for 2.3.48, it probably seemed completely natural to
include them in <tt>inode_operations</tt> despite the fact that they acted
primarily on a dentry.
</p><p>
Possibly we could simplify things by getting rid of
<tt>dentry_operations</tt> altogether.  Some operations that act on dentries
are already in <tt>inode_operations</tt> and <tt>super_operations</tt> - why not move
them all there?  While dentries are not as populous as <tt>struct page</tt>
there are still a lot of them and removing the "d_op" field could save
5% of the memory used by that structure (on x86-64).
</p><p>
With two exceptions, every active filesystem only has a single dentry
operations structure in effect.  Some filesystem implementations like
"vfat" define two - e.g. one with case-sensitive matching and one with
case-insensitive matching - but there is only one active per
super-block.  So it would seem that the operations in
dentry_operations could be moved to <tt>super_operations</tt>, or at
least accessed through "s_d_op".

The two exceptions are ceph and procfs.  These filesystems use
different <tt>d_revalidate()</tt> operations in different parts of the
filesystem and - in the case of procfs - different <tt>d_release()</tt>
operations.  The necessary distinctions could easily be made in
per-superblock versions of these operations.  Do these cases justify the 5%
space cost?  Arguably not.
</p>
<h4>Directly embedded function pointers</h4>
<p>
Finally it is appropriate to reflect on the alternate pattern
mentioned at the start, where function pointers are stored directly in
the object rather than in a separate vtable structure.

This pattern can be seen in
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/blkdev.h#L263"><tt>struct request_queue</tt></a>
which has nine function
pointers,
<a href="http://lxr.linux.no/#linux+v2.6.39/include/linux/efi.h#L251"><tt>struct efi</tt></a>
which has ten function pointers, and
<a href="http://lxr.linux.no/#linux+v2.6.39/include/net/sock.h#L238"><tt>struct sock</tt></a>
which has six function pointers.
</p><p>
The cost of embedded pointers is obviously space.  
When vtables are used, there is only one copy
of the vtable and multiple copies of an object (in most cases) so if
more than one function pointer is needed, a vtable would save space.

The cost of a vtable is an extra memory reference, though cache might
reduce much of this cost in some cases.  A vtable also has a cost of
flexibility.  When each object needs exactly the same set of operations
a vtable is good, but if there is a need to individually tailor some
of the operations for each object, then embedded function pointer can
provide that flexibility.  This is illustrated quite nicely by the
comment with "zoom_video" in <tt>struct pcmcia_socket</tt>
</p>
<pre>	/* Zoom video behaviour is so chip specific its not worth adding
	   this to _ops */
</pre><p>
So where objects are not very populous, where the list of function
pointers is small, and where multiple mixins are needed,
embedded function pointers are used instead of a separate vtable.
</p>
<h4>Method Dispatch Summary</h4>
<p>
If we combine all the pattern elements that we have found in
Linux we find that:
</p>
<blockquote>
<p>
Method pointers that operate on a particular type of object are
normally collected in a vtable associated directly with that
object, though they can also appear:
</p>
<ul>
<li> In a mixin vtable that collects related functionality which
    may be selectable independently of the base type of the
    object.
</li><li> In the vtable for a "parent" object when doing so avoids
    the need for a vtable pointer in a populous object
</li><li> Directly in the object when there are few method pointers,
    or they need to be individually tailored to the particular
    object.
</li></ul>
<p>
These vtables rarely contain anything other than function
pointers, though fields needed to register the object class can
be appropriate.  Allowing these function pointers to be NULL is
a common but not necessarily ideal technique for handling
defaults.
</p>
</blockquote>
<p>
So in exploring the Linux Kernel code we have found that even
though it is not written in an object-oriented language, it
certainly contains objects, classes (represented as vtables),
and even mixins.  It also contains concepts not normally
found in object-oriented languages such as delegating
object methods to a "parent" object.
</p><p>
Hopefully understanding these different patterns and the
reasons for choosing between them can lead to more uniform
application of the patterns across the kernel, and hence make
it easier for a newcomer to understand which pattern is being
followed.

In the second part of our examination of object
oriented patterns we will explore the various ways that 
data inheritance is achieved in the Linux kernel
and discuss the strengths and weaknesses of each approach so
as to see where each is most appropriate.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Development_model-Patterns">Development model/Patterns</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Brown_Neil">Brown, Neil</a></td></tr>
            </tbody></table><br clear="all">
<hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/444910/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The most backdoor-looking bug I've ever seen (2021) (129 pts)]]></title>
            <link>https://words.filippo.io/dispatches/telegram-ecdh/</link>
            <guid>40315274</guid>
            <pubDate>Fri, 10 May 2024 03:33:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/telegram-ecdh/">https://words.filippo.io/dispatches/telegram-ecdh/</a>, See on <a href="https://news.ycombinator.com/item?id=40315274">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <!--kg-card-begin: markdown--><p>This is the story of a bug that was discovered and fixed in Telegram's self-rolled cryptographic protocol about seven years ago. The bug didn't get any press, and no one seems to know about it, probably because it was only published in Russian.</p>
<p>To this day, it's the most backdoor-looking bug I've ever seen.</p>
<p>Google Translate does a good enough job on the <a href="https://habrahabr.ru/post/206900/?ref=words.filippo.io">original article</a>, which is still available on Habr, but I'm going to walk you through it along with some context.</p>
<p>Telegram is a popular chat app that uses its own... bizarre protocol to encrypt chats, called MTProto. The protocol is used both to encrypt all messages to the Telegram server, and to encrypt opt-in 1:1 end-to-end "Secret Chats".<sup><a href="#fn1" id="fnref1">[1]</a></sup> In text I can't do justice to the facial expressions of cryptographers when you mention Telegram's protocol, so just believe me that it's <em>weird</em>.</p>
<p>The current consensus seems to be that the latest version is not broken in known ways that are severe or relevant enough to affect end users, assuming the implementation is correct. That is about as safe as leaving exposed wires around your house because they are either not live or placed high enough that no one should touch them.</p>
<p>The original version was, however, completely broken, in the most puzzling of ways.</p>
<p>End-to-end Telegram chat sessions use <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange?ref=words.filippo.io">finite-field Diffie-Hellman</a><sup><a href="#fn2" id="fnref2">[2]</a></sup> to establish a shared key between the two participants. The negotiation happens through messages relayed by the Telegram server. Diffie-Hellman is a fundamental building block of many cryptosystems, and it allows two parties to establish a shared secret that any eavesdroppers can't derive. It is however only one part of a secure key exchange, because an attacker capable of intercepting the messages could simply establish two separate sessions with the two parties, carrying out a <a href="https://en.wikipedia.org/wiki/Person-in-the-middle_attack?ref=words.filippo.io">Person-in-the-Middle</a> attack. The parties need some way to verify they derived the same secret. In TLS, they use a signature from a certificate. In most secure chat apps, there is a fingerprint ("Safety Numbers" in Signal) that the two parties can compare out-of-band.<sup><a href="#fn3" id="fnref3">[3]</a></sup> What's important is that if the two sides derived the same secret, they can be sure no one else has access to it.</p>
<p>The Telegram key exchange is described in <a href="https://web.archive.org/web/20131220000537/https://core.telegram.org/api/end-to-end#key-generation">the "Key Generation" section of Telegram's end-to-end API docs</a>. Concretely, Alice requests the DH parameters <code>(p, g)</code> from Telegram, painstakingly verifies them, computes a random <code>a</code> value, and sends <code>g^a mod p</code> to Telegram. Bob receives <code>(p, g, g^a mod p)</code>, similarly computes <code>b</code> and <code>g^b mod p</code>, and sends the latter back (along with a truncated hash of the derived key, for some reason).</p>
<p>Now, normally the two sides would compute the shared key as <code>(g^a)^b mod p</code> and <code>(g^b)^a mod p</code>. Instead, the original version of MTProto computed it as</p>
<pre><code>(g^a)^b mod p XOR nonce
</code></pre>
<p>where <code>nonce</code> was an arbitrary, supposedly random value sent by the server along with the peer's public contribution.</p>
<p>This was a completely non-standard and useless addition, and all it did was let the server perform an undetected Person-in-the-Middle attack. Let's see how.</p>
<p>In a normal PitM, the server negotiates two separate Diffie-Hellman sessions with Alice and Bob, who end up with different shared keys, which they could detect by comparing fingerprints.</p>
<pre><code>Alice                     Telegram              Bob

a = random()       
A = g^a mod p       -&gt;
                        t = random()
                        T = g^t mod p -&gt;
                                          b = random()
                                      &lt;-  B = g^b mod p
                                          key = T^b mod p
                    &lt;-  T
key = T^a mod p

                    T^a mod p != T^b mod p
</code></pre>
<p>With the nonce addition, however, the server could "fix" Alice's key to match Bob's by manipulating Alice's nonce. The two parties would end up with the same fingerprint, and couldn't tell that an attack happened, but the server (and no one else) would know the shared key, allowing it to decrypt all messages.</p>
<pre><code>nonce_bob = random()
key_bob = T^b mod p  XOR  nonce_bob

nonce_alice = A^t mod p  XOR  B^t mod p  XOR  nonce_bob
key_alice = T^a mod p  XOR  nonce_alice =
  T^a mod p  XOR  (A^t mod p  XOR  B^t mod p  XOR  nonce_bob) =
  B^t mod p  XOR  nonce_bob = key_bob
</code></pre>
<p>Why do I say this addition was useless? Because it literally had no purpose! Indeed, the vulnerability was <a href="https://web.archive.org/web/diff/20131220000537/20131225140924/http://core.telegram.org/api/end-to-end">fixed by silently removing the nonce step from the docs</a>.<sup><a href="#fn4" id="fnref4">[4]</a></sup> <a href="https://core.telegram.org/constructor/encryptedChatRequested?layer=11&amp;ref=words.filippo.io">A later API revision</a> removed the nonce parameter with the caption "Improve secret chats". All <a href="https://web.archive.org/web/20131028041748/http://core.telegram.org/constructor/encryptedChatRequested">the original API reference</a> said about the nonce is "Random server sequence for calculation of key".</p>
<p>I never heard a plausible explanation for why the designers of MTProto went out of their way to add useless complexity to their protocol, with the only outcome of making undetectable interception possible.</p>
<p><strong>Edit (2021-01-11)</strong>: <a href="https://twitter.com/asdofindia/status/1348491279798128641?ref=words.filippo.io">@asdofindia linked me on Twitter</a> to <a href="https://telegram.org/blog/crowdsourcing-a-more-secure-future?ref=words.filippo.io">an official statement by Telegram about this</a> that I couldn't find anymore. It claims the nonce was there to protect clients with weak random number generators. Here's what I had buried into a footnote when I couldn't find a citation to attribute that explanation to Telegram:</p>
<blockquote>
<p>This doesn't make sense for a number of reasons: 1) clients with weak randomness are likely to be toast anyway, because Telegram's bizarro not-a-MAC relies on randomness in the payload to avoid an offline decryption oracle (there is a plaintext hash of the payload on the wire, I told you this was weird!); 2) the API also allows clients to request random bytes from the server to XOR with their secret share; and 3) defending against weak randomness by relying on a server contribution defends against everything but the server, which is the relevant attacker in the end-to-end setting. (Said another way, anyone that can intercept client-server messages can see the extra randomness, making it moot.) Non-practitioners might think this is a reasonable defense in depth, belts and suspenders kind of thing, but in cryptography engineering adding complexity to defend against scenarios that lead to compromise anyway is simply pointless.</p>
</blockquote>
<p>Anyway, it's been a while, the world is a different place now, and maybe <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor?ref=words.filippo.io">Hanlon's razor</a> cuts deeper than I thought. I think there are better reasons not to use Telegram today than this old bug<sup><a href="#fn1" id="fnref1:1">[1:1]</a></sup>, but it's still what I think about every time people talk about far-fetched "bugdoors". The bar is high!</p>
<h2 id="the-picture">The picture</h2>
<p>In other news, this newsletter is going to pivot into Rome photoblogging. (Not really, if you made it this far and like cryptography engineering, you should <a href="https://buttondown.email/cryptography-dispatches?tag=header&amp;ref=words.filippo.io">subscribe</a> or <a href="https://twitter.com/FiloSottile?ref=words.filippo.io">follow me on Twitter</a>.)</p>
<p><img src="https://words.filippo.io/content/images/2022/01/ee618b89-a8fa-45a2-af01-6f9955d2c99a.jpeg" alt="St. Peter's reflecting in the Tevere" loading="lazy"></p>
<hr>
<section>
<ol>
<li id="fn1"><p>By the way, aside from all the cryptographic weirdness and the unexplained backdoor-looking bug, the real reason you should not trust Telegram's encryption is that it's off by default, inconvenient to use, and simply unavailable in groups, meaning most messages flow unencrypted on Telegram's servers. Nonetheless, Telegram markets itself as a secure chat app, with misleading copy along the lines of "everything is encrypted, Secret Chats are just <em>more</em> encrypted!" They explain in their FAQ that it's all about backups, and that other more secure apps "<a href="https://telegram.org/faq?ref=words.filippo.io#dev_page_content:~:text=Other%20apps%20ignore%20the%20need%20for,before%20ever%20reaching%20a%20million%20users.">never reach a million users</a>". <a href="https://twitter.com/signalapp/status/1347240006444675072?ref=words.filippo.io">In other news</a>. <a href="#fnref1">↩︎</a> <a href="#fnref1:1">↩︎</a></p>
</li>
<li id="fn2"><p>Diffie-Hellman over finite fields is how it was originally designed, but today we'd use Elliptic-Curve Diffie-Hellman, which is faster, has smaller outputs, and is safer. FFDH has many of <a href="https://buttondown.email/cryptography-dispatches/archive/557475c5-9781-47e0-a640-5734bc849bc7?ref=words.filippo.io">the same issues as DSA</a> (FFDH is to DSA like ECDH is to ECDSA and EdDSA.) Current-day MTProto 2.0 still uses FFDH, but that's far from the most anachronistic choice in it. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>This is admittedly not a particularly strong authentication strategy, but it relies on the assumption that even if 1% of users check their fingerprints, systematic PitM is likely to be detected, and high-risk users can be extra careful and consistently check fingerprints. I hope solutions like key transparency can improve this picture in the coming years without changing the default UX. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Can we talk about how cool the Wayback Machine Compare feature is? Now is a good time to <a href="https://archive.org/donate/">donate to the Internet Archive</a>, by the way. <a href="#fnref4">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CS388: Natural Language Processing (129 pts)]]></title>
            <link>https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html</link>
            <guid>40314236</guid>
            <pubDate>Fri, 10 May 2024 00:09:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html">https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html</a>, See on <a href="https://news.ycombinator.com/item?id=40314236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">


<p>These are the course materials for an online masters course in NLP. All lectures are videos available on YouTube.</p>

<p><b>Note on enrollment for on-campus students:</b> This course is listed in the course catalog as "Natural Language Processing-WB". It is a partially asynchronous
course taught for certain online masters programs at UT ("Option III" programs, as the university calls them). If you are a student enrolled on-campus at UT Austin,
you are <b>not</b> eligible to take this course. This is a hard requirement from
the university due to the fact that this course is part of an Option III program. There is an on-campus version of CS388 that is typically
taught once per year by either me, Eunsol Choi, or Ray Mooney, which you are eligible to take (or CS371N if you're an undergraduate student). Regardless, you are free to consult the materials here!

</p><h2>Assignments</h2>

<p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a1.pdf">Assignment 1: Linear Sentiment Classification</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a1-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a2.pdf">Assignment 2: Feedforward Neural Networks, Word Embeddings, and Generalization</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a2-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3.pdf">Assignment 3: Transformer Language Modeling</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a4.pdf">Assignment 4: Factuality and ChatGPT</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a4-distrib.tgz">[code and dataset download]</a>
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/fp.pdf">Final Project: Dataset Artifacts</a></b> <a href="https://github.com/gregdurrett/fp-dataset-artifacts">[code and dataset download]</a> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/project-ex-1.pdf">[example 1]</a> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/project-ex-2.pdf">[example 2]</a> <b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/peer-assessment.pdf">[peer assessment instructions]</a></b>
<!--<p><b><a href="a6.pdf">[SPRING 2021 VERSION] Assignment 6 (Final Project): Domain Adaptation for Question Answering</a></b> <a href="https://github.com/gregdurrett/nlp-qa-finalproj/">[code and dataset download]</a>-->

</p><h2>Lecture Videos and Readings</h2>

<p><b><a href="https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7">YouTube playlist containing all videos</a></b></p>

<p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/slides-notes.tgz">Download the slides and handwritten notes here (88MB tgz)</a></b></p>

<table bordercolor="#111111" id="AutoNumber5">
  <tbody>
    <tr>
      <td><b>Topics and Videos</b></td>
      <td><b>Readings</b></td>
    </tr>
    <tr><td colspan="2"><b>Week 1: Intro and Linear Classification</b></td></tr>
    <tr>
     <td><a href="https://youtu.be/Mz8-LTednt4">Course Preview</a>
     </td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/k5p8teUNHX4">Introduction</a>
      </td>
      <td><p>Note: this introduction video is from an older run of the class and references an outdated schedule. Please refer
          to the new course structure here.
          </p>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DVxR3AwdxoA">Linear Binary
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          2.0-2.5, 4.2-4.4.1</a><p>
        <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf">Perceptron and logistic regression</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0jSElGFUxro">
          Sentiment Analysis and Basic Feature Extraction</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          4.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/_We4tlPkaj0">Basics of
          Learning, Gradient Descent</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tMGv5ZcuVP4">Perceptron</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hhTkyP7EzGw">Perceptron as
          Minimizing Loss</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0naHFT07ja8">Logistic
          Regression</a>
      </td>
      <td><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf">Perceptron and LR connections</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/cKbnEmjxnOY">Sentiment
          Analysis</a>
      </td>
      <td><a href="https://www.aclweb.org/anthology/W02-1011/" target="_blank">Thumbs up? Sentiment Classification using
          Machine Learning Techniques</a> Bo Pang et al., 2002<p>
        <a href="https://www.aclweb.org/anthology/P12-2018/" target="_blank">Baselines and Bigrams: Simple, Good
          Sentiment and Topic Classification</a> Sida Wang and Christopher Manning, 2012</p><p>
        <a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank">Convolutional Neural Networks for Sentence
          Classification</a> Yoon Kim, 2014</p><p>
        <a href="https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md" target="_blank">[GitHub] NLP Progress on Sentiment Analysis</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/65ui-GdtY0Q">Optimization
          Basics</a>
      </td>
      <td></td>
    </tr>

    <tr><td colspan="2"><b>Week 2: Multiclass and Neural Classification</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/My6GaGhqxdI">Multiclass
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 4.2</a><p>
        <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf">Multiclass lecture note</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/EA627DC7k6M">Multiclass
          Perceptron and Logistic Regression</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/va2i7LXt9zI">Multiclass
          Classification Examples</a></td>
      <td><a href="https://www.aclweb.org/anthology/D15-1075/" target="_blank">A large annotated corpus for learning
          natural language inference</a> Sam Bowman et al., 2015<p>
        <a href="https://www.aclweb.org/anthology/D13-1193/" target="_blank">Authorship Attribution of
          Micro-Messages</a> Roy Schwartz et al., 2013
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/N4f2-S19LME">Fairness in
          Classification</a></td>
      <td><a href="https://arxiv.org/pdf/1811.10104.pdf" target="_blank">50 Years of Test (Un)fairness: Lessons for
          Machine Learning</a> Ben Hutchinson and Margaret Mitchell, 2018<p>
        <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" target="_blank">[Article] Amazon scraps secret AI recruiting tool that showed bias against women</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DU_p-RBy5gM">Neural
          Networks</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rdohzaGa8aE">Neural Network
          Visualization</a></td>
      <td><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">[Blog] Neural Networks,
          Manifolds, and Topology</a> Chris Olah</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/8WhPYIWyR5g">Feedforward
          Neural Networks, Backpropagation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          Chapter 3.1-3.3</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/IRZCQO18QAI">Neural Net
          Implementation</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KPZb2rYS4BE">Neural Net
          Training, Optimization</a></td>
      <td><a href="https://dl.acm.org/doi/10.5555/2627435.2670313">Dropout: a simple way to prevent neural networks from
          overfitting</a> Nitish Srivastava et al., 2014 <p>

        <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
          Internal Covariate Shift</a> Sergey Ioffe and Christian Szegedy, 2015</p><p>

        <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> Durk Kingma and Jimmy Ba,
        2015</p><p>

        <a href="https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html">The Marginal
          Value of Adaptive Gradient Methods in Machine Learning</a> Ashia Wilson et al., 2017
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 3: Word Embeddings</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/8EqQROdVPyM">Word
          Embeddings</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hznxqCIrzSQ">Skip-gram</a>
      </td>
      <td><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed
          Representations of Words and Phrases and their Compositionality</a> Tomas Mikolov et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/gpP-depOUwg">Other Word
          Embedding Methods</a></td>
      <td><a href="https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html" target="_blank">A Scalable Hierarchical Distributed Language Model</a> Andriy Mnih and Geoff Hinton, 2008<p>
        <a href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank">Neural Word Embedding as Implicit Matrix Factorization</a> Omer Levy and Yoav Goldberg, 2014</p><p>
        <a href="https://www.aclweb.org/anthology/D14-1162/" target="_blank">GloVe: Global Vectors for Word
          Representation</a> Jeffrey Pennington et al., 2014</p><p>
        <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with
          Subword Information</a> Piotr Bojanowski et al., 2016
    </p></td></tr>

    <tr>
      <td><a href="https://youtu.be/J_227g77Jqg">Bias in Word
          Embeddings</a></td>
      <td><a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" target="_blank">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
          Embeddings</a> Tolga Bolukbasi et al., 2016<p>
        <a href="https://www.aclweb.org/anthology/N19-1062/" target="_blank">Black is to Criminal as Caucasian is to
          Police: Detecting and Removing Multiclass Bias in Word Embeddings</a> Thomas Manzini et al., 2019</p><p>
        <a href="https://www.aclweb.org/anthology/N19-1061/" target="_blank">Lipstick on a Pig: Debiasing Methods Cover
          up Systematic Gender Biases in Word Embeddings But do not Remove Them</a> Hila Gonen and Yoav Goldberg, 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/3pwwdHuH0I4">Applying
          Embeddings, Deep Averaging Networks</a></td>
      <td><a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals
          Syntactic Methods for Text Classification</a> Mohit Iyyer et al., 2015</td>
    </tr>


    <tr><td colspan="2"><b>Week 4: Language Modeling and Self-Attention</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/J-yHbD8LYCM">
          n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Yfug5eIQh5w">
          Smoothing in n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.2</a></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/ImW4vJ5XZQc">
          LM Evaluation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.4</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/59NrmwAdOWA">
          Neural Language Models</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/xvnnA04JVQo">
          RNNs and their Shortcomings</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.3</a><p>
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">[Blog] Understanding LSTMs</a> Chris Olah</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q7HY7tpWWi8">
          Attention</a></td>
      <td><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate
</a> Dzmitry Bahdanau et al., 2015</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/10l2NXStROU">
          Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/nHXrdLMo8Uk">
          Multi-Head Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<p>
        <a href="http://jalammar.github.io/illustrated-transformer/">[Blog] The Illustrated Transformer</a> Jay Alammar</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a8sTGth7PoU">
          Position Encodings</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<p>
        <a href="https://arxiv.org/abs/2108.12409" target="_blank">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a> Ofir Press et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2305.19466" target="_blank">The Impact of Positional Encoding on Length Generalization in Transformers</a> Amirhossein Kazemnejad et al., 2023
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 5: Transformers and Decoding</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/sLsUD-RcDqg">
          Transformer Architecture</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/1Efx04lHa7w">
          Using Transformers</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/htyspM3FrMg">
          Transformer Language Modeling</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DPvDL8L4Dqo">
          Transformer Extensions</a></td>
      <td>
        <a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a> Jared Kaplan et al., 2020<p>
        <a href="https://arxiv.org/abs/2009.06732" target="_blank">Efficient Transformers: A Survey</a> Yi Tay et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2009.14794" target="_blank">Rethinking Attention with Performers</a> Krzysztof Choromanski et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2004.05150" target="_blank">Longformer: The Long-Document Transformer</a> Iz Beltagy et al., 2021

      </p></td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/wltqDbhlcJ0">
          Beam Search</a></td>
      <td>
      </td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/JETxaSaj6_k">
          Nucleus Sampling</a></td>
      <td>
         <a href="https://arxiv.org/abs/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a> Ari Holtzman et al., 2019
      </td>
    </tr>


    <tr><td colspan="2"><b>Week 6: Pre-training, seq2seq LMs</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/dya_QNFvtiQ">
          BERT: Masked Language Modeling</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/g96oi4ihc_E">
          BERT: Model and Applications</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019<p>
        <a href="https://www.aclweb.org/anthology/W19-4302/" target="_blank">To Tune or Not to Tune? Adapting Pretrained
          Representations to Diverse Tasks</a> Matthew Peters et al., 2019</p><p>
        <a href="https://arxiv.org/pdf/1804.07461.pdf" target="_blank">GLUE: A Multi-Task Benchmark and Analysis
          Platform for Natural Language Understanding</a> Alex Wang et al., 2019</p><p>
        <a href="https://arxiv.org/abs/1906.04341" target="_blank">What Does BERT Look At? An Analysis of BERT's Attention
           </a> Kevin Clark et al., 2019
        <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining
          Approach</a> Yinhan Liu et al., 2019
      </p></td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/TKZkvqb-qpM">
          Seq2seq Models</a></td>
      <td>
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/M9L3gk4ITec">
          BART</a></td>
      <td><a href="https://arxiv.org/abs/1910.13461" target="_blank">BART: Denoising Sequence-to-Sequence Pre-training
          for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et al., 2019
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/b6KFaT8mK4g">
          T5</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">Exploring the Limits of Transfer Learning with a
          Unified Text-to-Text Transformer</a> Colin Raffel et al., 2020<p>
        <a href="https://arxiv.org/abs/2005.00700" target="_blank">UnifiedQA: Crossing Format Boundaries With a Single QA System</a> Daniel Khashabi et al., 2020</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/WA16JelEkkg">
          Word Piece and Byte Pair Encoding</a></td>
      <td><a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank">Neural Machine Translation of Rare Words with
          Subword Units</a> Rico Sennrich et al., 2016<p>
        <a href="https://arxiv.org/pdf/2004.03720.pdf" target="_blank">Byte Pair Encoding is Suboptimal for Language
          Model Pretraining</a> Kaj Bostrom and Greg Durrett, 2020
      </p></td>
    </tr>
    <tr><td colspan="2"><b>Week 7-8: Structured Prediction: Part-of-speech, Syntactic Parsing</b>
      <p>Note: this unit was previously presented as Week 4 right after classification. There are a few references
          to it being our first brush with structured models. In this structure of the course, it's still true that it's our first exposure to
          models dealing with linguistic structure as opposed to surface-level sequential structure (i.e., token sequences in generation).</p>
      </td></tr>
    <tr>
      <td><a href="https://youtu.be/Llw6qfeAWDs">Part-of-Speech
          Tagging</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 8.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/yQZ0mDW-U3g">Sequence
          Labeling, Tagging with Classifiers</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/FeLtLLbn4qU">Hidden Markov
          Models</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dVF7LZkbl9g">
          HMMs: Parameter Estimation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Ks7IrsjhqSo">
          HMMs: Viterbi Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.3</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/wijpAX_LLXo">
          HMMs for POS Tagging</a></td>
      <td><a href="https://arxiv.org/abs/cs/0003055" target="_blank">TnT - A Statistical Part-of-Speech Tagger</a>
        Thorsten Brants, 2000<p>
        <a href="https://www.aclweb.org/anthology/W00-1308/" target="_blank">Enriching the Knowledge Sources Used in a
          Maximum Entropy Part-of-Speech Tagger</a> Kristina Toutanvoa and Christopher Manning, 2000</p><p>
        <a href="https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14" target="_blank">Part-of-Speech Tagging
          from 97% to 100%: Is It Time for Some Linguistics?</a> Christopher Manning, 2011</p><p>
        <a href="https://www.aclweb.org/anthology/D17-1309.pdf" target="_blank">Natural Language Processing with Small
          Feed-Forward Networks</a> Jan Botha et al., 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/zDPUKQKDaMM">
          Constituency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.1-10.2</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q3dLP9YQLPA">
          Probabilistic Context-Free Grammars</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3-10.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QeDb6mSDSqs">
          CKY Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/f1o1_bPWzM0">
          Refining Grammars</a></td>
      <td><a href="https://www.aclweb.org/anthology/P03-1054/" target="_blank">Accurate Unlexicalized Parsing</a> Dan Klein
        and Chris Manning, 2003<p>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.5</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dbDjKCc4R3E">
          Dependencies</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.1</a><p>
<a href="https://www.aclweb.org/anthology/Q13-1002/" target="_blank">Finding Optimal 1-Endpoint-Crossing
          Trees</a> Emily Pitler et al., 2013</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ypoaw7lJ6Rk">
          Transition-based Dependency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.3</a>
      </td>
    </tr>
    <tr><td colspan="2"><b>Week 9: Modern Large Language Models</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/jn41DLgnqek">
          GPT-3</a></td>
      <td>
       <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> Alec Radford et al., 2019<p>
       <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> Tom B. Brown et al., 2020</p><p>
       <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a> Hugo Touvron et al., 2023</p><p>Llama 2 is one of the latest models with publicly available weights (although it is not fully open-source, as many details of the training are not public).</p>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YCq6b31Jb6E">
          Zero-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2212.04037">Demystifying Prompts in Language Models via Perplexity Estimation</a> Hila Gonen et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/JSBjj09xJeM">
          Few-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> Tony Z. Zhao et al., 2021<p>
       <a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a> Percy Liang et al., 2022</p><p>
       <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a> Sewon Min et al., 2022
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/mUthsZ_Aivo">
          Understanding ICL: Induction Heads</a></td>
      <td>
        <a href="https://arxiv.org/abs/2209.11895">In-context Learning and Induction Heads</a> Catherine Olsson et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YT3VSlDjrVU">
          Instruction Tuning</a></td>
      <td>
        <a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> Victor Sanh et al., 2021<p>
        <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a> Hyung Won Chung et al., 2022
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DwAdhx6GFh8">
          Reinforcement Learning from Human Feedback (RLHF)</a></td>
      <td>
        <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> Long Ouyang et al., 2022<p>
        <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">[Website] Stanford Alpaca: An Instruction-following LLaMA Model</a> Rohan Taori et al., 2023
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bQZvmQUlqcs">
          Factuality of LLMs</a></td>
      <td>
        <a href="https://arxiv.org/abs/2212.07981">Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation</a> Yixin Liu et al., 2023<p>
        <a href="https://arxiv.org/abs/2303.01432">WiCE: Real-World Entailment for Claims in Wikipedia</a> Ryo Kamoi et al., 2023</p><p>
        <a href="https://arxiv.org/abs/2111.09525">SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization</a> Philippe Laban et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a> Sewon Min et al., 2023</p><p>
        <a href="https://arxiv.org/abs/2210.08726">RARR: Researching and Revising What Language Models Say, Using Language Models</a> Luyu Gao et al., 2022
      </p></td>
    </tr>
    <tr><td colspan="2"><b>Week 10: Explanations</b></td></tr>


    <tr>
      <td><a href="https://youtu.be/Nr0_xYEso-4">
          Explainability in NLP</a></td>
      <td><a href="https://arxiv.org/pdf/1606.03490.pdf" target="_blank">The Mythos of Model Interpretability</a> Zach Lipton,
        2016<p>
        <a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals Syntactic
          Methods for Text Classification</a> Mohit Iyyer et al., 2015</p><p>
        <a href="https://arxiv.org/pdf/1812.08951.pdf" target="_blank">Analysis Methods in Neural Language Processing: A
          Survey</a> Yonatan Belinkov and Jim Glass, 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ZVElc4CvHpk">
          Local Explanations: Highlights</a></td>
      <td><a href="https://arxiv.org/pdf/1602.04938.pdf" target="_blank">"Why Should I Trust You?" Explaining the
          Predictions of Any Classifier</a> Marco Tulio Ribeiro et al., 2016<p>
        <a href="https://arxiv.org/pdf/1703.01365.pdf" target="_blank">Axiomatic Attribution for Deep Networks</a>
        Mukund Sundararajan et al., 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a6u6WM5wcLQ">
          Model Probing</a></td>
      <td><a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank">BERT Rediscovers the Classical NLP Pipeline</a>
        Ian Tenney et al., 2019<p>
        <a href="https://arxiv.org/pdf/1905.06316.pdf" target="_blank">What Do You Learn From Context? Probing For
          Sentence Structure In Contextualized Word Represenations</a> Ian Tenney et al., 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/RXYaMZcDIWU">
          Annotation Artifacts</a></td>
      <td><a href="https://www.aclweb.org/anthology/N18-2017/" target="_blank">Annotation Artifacts in Natural Language
          Inference Data</a> Suchin Gururangan et al., 2018<p>
        <a href="https://www.aclweb.org/anthology/S18-2023/" target="_blank">Hypothesis Only Baselines in Natural
          Language Inference</a> Adam Poliak et al., 2018</p><p>
        <a href="https://www.aclweb.org/anthology/P18-1176/" target="_blank">Did the Model Understand the Question?</a>
        Pramod Kaushik Mudrakarta et al., 2018</p><p>
        <a href="https://www.aclweb.org/anthology/D18-1009.pdf" target="_blank">Swag: A Large-Scale Adversarial Dataset
          for Grounded Commonsense Inference</a> Rowan Zellers et al., 2018
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/bXHM5t_ejsc">
          Text Explanations</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1603.08507.pdf" target="_blank">Generating Visual Explanations</a> Lisa-Anne Hendricks et
        al., 2016<p>
        <a href="https://arxiv.org/abs/1812.01193" target="_blank">e-SNLI: Natural Language Inference with Natural Language Explanations</a> Oana-Maria Camburu et
        al., 2018</p><p>
        <a href="https://arxiv.org/pdf/2004.05569.pdf" target="_blank">Explaining Question Answering Models through Text
          Generation</a> Veronica Latcinnik and Jonathan Berant, 2020
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tNGu3EqJbKc">
          Chain-of-thought</a></td>
      <td>
        <a href="https://arxiv.org/abs/1705.04146" target="_blank">Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems</a> Wang Ling et al., 2017<p>
        <a href="https://arxiv.org/abs/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> Jason Wei et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2205.03401" target="_blank">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</a> Xi Ye and Greg Durrett, 2022</p><p>
        <a href="https://arxiv.org/abs/2205.11916" target="_blank">Large Language Models are Zero-Shot Reasoners</a> Takeshi Kojima et al., 2022
      </p></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/9sFyzMywKmo">
          Chain-of-thought: Extensions and Analysis</a></td>
      <td>
        <a href="https://arxiv.org/pdf/2211.13892.pdf" target="_blank">Complementary Explanations for Effective In-Context Learning</a> Xi Ye et al., 2023<p>
        <a href="https://arxiv.org/abs/2211.10435" target="_blank">PAL: Program-aided Language Models</a> Luyu Gao et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2210.03350" target="_blank">Measuring and Narrowing the Compositionality Gap in Language Models</a> Ofir Press et al., 2022
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 11: Question Answering, Dialogue Systems</b></td></tr>
    
    <tr>
      <td><a href="https://youtu.be/gnUSE0fCbso">
          Reading comprehension intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JRI3RwRBnMY">
          Reading comprehension: setup and baselines</a></td>
      <td><a href="https://www.aclweb.org/anthology/D13-1020.pdf" target="_blank">MCTest: A Challenge Dataset for the
          Open-Domain Machine Comprehension of Text</a> Matthew Richardson et al., 2013<p>
        <a href="https://www.aclweb.org/anthology/D16-1264/" target="_blank">SQuAD: 100,000+ Questions for Machine
          Comprehension of Text</a> Pranav Rajpurkar et al., 2016
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/F8hWZ4xaVkA">
          BERT for QA</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tCvAHmrxPvY">
          Problems with Reading Comprehension</a></td>
      <td><a href="https://www.aclweb.org/anthology/D17-1215/" target="_blank">Adversarial Examples for Evaluating
          Reading Comprehension Systems</a> Robin Jia and Percy Liang, 2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/P-j_zeS0Pa8">
          Open-domain QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1704.00051" target="_blank">Reading Wikipedia to Answer Open-Domain Questions</a> Danqi Chen et al., 2017<p>
        <a href="https://www.aclweb.org/anthology/P19-1612.pdf" target="_blank">Latent Retrieval for Weakly Supervised
          Open Domain Question Answering</a> Kenton Lee et al., 2019</p><p>
        <a href="https://ai.google.com/research/NaturalQuestions" target="_blank">[Website] Natural Questions</a> Tom Kwiatkowski et al., 2019</p><p>Most modern open-domain QA systems are either "closed-book" models like ChatGPT or "open-book" models that do retrieval, similar
          to the Chen et al. and Lee et al. papers above. These are typically described under the general framework of <a href="https://arxiv.org/pdf/2005.11401.pdf">retrieval-augmented generation</a> and an example of
          how these systems work is <a href="https://arxiv.org/abs/2112.09332">WebGPT</a> (similar to the "new Bing" chatbot).</p>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/jpRwa2iE_z8">
          Multi-hop QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1809.09600" target="_blank">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
             </a> Zhilin Yang et al., 2018<p>
        <a href="https://www.aclweb.org/anthology/N19-1405/" target="_blank">Understanding Dataset Design Choices for
          Multi-hop Reasoning</a> Jifan Chen and Greg Durrett, 2019</p><p>
        <a href="https://openreview.net/forum?id=SJgVHkrYDH" target="_blank">Learning to Retrieve Reasoning Paths over
          Wikipedia Graph for Question Answering</a> Akari Asai et al., 2020</p><p>Modern QA systems operating over the web are largely multi-hop by default; multi-hop QA has been subsumed by open-domain QA to a large extent. For a more recent multi-hop QA dataset, see <a href="https://arxiv.org/abs/2205.12665">QAMPARI</a></p>

      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/vAZ7VlLXReE">
          Dialogue: Chatbots</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JXfAkX7kvnM">
          Task-Oriented Dialogue</a></td>
      <td><a href="https://arxiv.org/pdf/1811.01241.pdf" target="_blank">Wizards of Wikipedia: Knowledge-Powered
          Conversational Agents</a> Emily Dinan et al., 2019<p>
          <a href="https://arxiv.org/abs/2009.11423" target="_blank">Task-Oriented Dialogue as Dataflow Synthesis</a> Semantic Machines, 2020</p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hc7P3QukmJk" target="_blank">
          Neural Chatbots</a></td>
      <td>
        <a href="https://arxiv.org/abs/1506.06714">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a> Alessandro Sordoni et al., 2015<p>
        <a href="https://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a> Jiwei Li et al., 2016</p><p>
        <a href="https://arxiv.org/pdf/2004.13637.pdf">Recipes for building an open-domain chatbot</a> Stephen Roller et al., 2020</p><p>Note: an updated version of BlenderBot is described in <a href="https://arxiv.org/abs/2208.03188">Kurt Shuster et al.</a>.
              Other chatbots discussed, like <a href="https://character.ai/">character.ai</a>, can be found online and you can play with them, but less information
              about their precise internals is available in published papers.</p>
      </td>
          
    </tr>

    <tr><td colspan="2"><b>Week 12: Machine Translation, Summarization</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/9KAZ4-gKj9g">
          Machine Translation Intro</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Oup0DEYJXEQ">
          MT: Framework and Evaluation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dzOuPhBmFtE">
          MT: Word alignment</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/mbtk3VCG_2A">
          MT: IBM Models</a></td>
      <td><a href="https://www.aclweb.org/anthology/C96-2141.pdf" target="_blank">HMM-Based Word Alignment in
          Statistical Translation</a> Stephan Vogel et al., 1996</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0k8b5jGk-h4">
          Phrase-based Machine Translation</a></td>
      <td><a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/pharaoh-amta2004.pdf" target="_blank">Pharaoh: A
          Beam Search Decoder for Phrase-Based Statistical Machine Translation Models</a> Philipp Koehn, 2004<p>
        <a href="https://www.aclweb.org/anthology/P03-1021/" target="_blank">Minimum Error Rate Training in Statistical
          Machine Translation</a> Franz Och, 2003</p><p>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.4</a></p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bcP4b_4HQ8A">
          Neural and Pre-Trained Machine Translation</a></td>
      <td>
        <a href="https://arxiv.org/abs/1905.11901" target="_blank">Revisiting Low-Resource Neural Machine Translation: A Case Study</a> Rico Sennrich and Biao Zhang, 2019<p>
        <a href="https://aclanthology.org/2020.acl-main.688/" target="_blank">In Neural Machine Translation, What Does Transfer Learning Transfer?</a> Alham Fikri Aji et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2001.08210" target="_blank">Multilingual Denoising Pre-training for Neural Machine Translation</a> Yinhan Liu et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2302.14520" target="_blank">Large Language Models Are State-of-the-Art Evaluators of Translation Quality</a> Tom Kocmi and Christian Federmann, 2023
      </p></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/lBDI1CBNe_U">
          Summarization Intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QWt2E3m00kA">
          Extractive Summarization</a></td>
      <td><a href="https://dl.acm.org/doi/10.1145/290941.291025" target="_blank">The use of MMR, diversity-based
          reranking for reordering documents and producing summaries</a> Jaime Carbonell and Jade Goldstein, 1998<p>
        <a href="https://arxiv.org/abs/1109.2128" target="_blank">LexRank: Graph-based Lexical
          Centrality as Salience in Text Summarization</a> Gunes Erkan and Dragomir Radev, 2004</p><p>
        <a href="https://www.aclweb.org/anthology/W09-1802/" target="_blank">A Scalable Global Model for
          Summarization</a> Dan Gillick and Benoit Favre, 2009</p><p>
        <a href="https://www.aclweb.org/anthology/W17-4511/" target="_blank">Revisiting the Centroid-based Method: A
          Strong Baseline for Multi-Document Summarization</a> Demian Gholipour Ghalandari, 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/feLTtTilycY">
          Pre-trained Summarization and Factuality</a></td>
      <td><a href="https://www.aclweb.org/anthology/2020.acl-main.703/" target="_blank">BART: Denoising
          Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et
        al., 2019<p>
        <a href="https://arxiv.org/abs/1912.08777" target="_blank">PEGASUS:
          Pre-training with Extracted Gap-sentences for Abstractive Summarization</a> Jingqing Zhang et al., 2020</p><p>
        <a href="https://arxiv.org/pdf/2010.05478.pdf" target="_blank">Evaluating Factuality in Generation with
          Dependency-level Entailment</a> Tanya Goyal and Greg Durrett, 2020</p><p>
        <a href="https://arxiv.org/abs/2004.04228" target="_blank">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries</a> Alex Wang et al., 2020</p><p>Note: while the specific fine-tuned modeling approaches and factuality detection systems are no longer state-of-the-art as stated in the video,
              they are representative of ideas from pre-training 
              that are still used today. For discussion of how LLMs relate to summarization, see <a href="https://arxiv.org/abs/2209.12356">News Summarization and Evaluation in the Era of GPT-3
</a> by Tanya Goyal, Junyi Jessy Li, and Greg Durrett</p>
      </td>
    </tr>

    <tr><td colspan="2"><b>Week 13-14: Multilinguality, Language Grounding, Ethical Issues</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/ettP9Ayrho8">
          Morphology</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rTSCLfxdxrI">
          Cross-lingual Tagging and Parsing</a></td>
      <td><a href="https://www.aclweb.org/anthology/P11-1061/" target="_blank">Unsupervised Part-of-Speech Tagging with
          Bilingual Graph-Based Projections</a> Dipanjan Das and Slav Petrov, 2011<p>
        <a href="https://www.aclweb.org/anthology/D11-1006/" target="_blank">Multi-Source Transfer of Delexicalized
          Dependency Parsers</a> Ryan McDonald et al., 2011
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KNBRb8sjzOA">
          Cross-lingual Pre-training</a></td>
      <td><a href="https://arxiv.org/pdf/1602.01925.pdf" target="_blank">Massively Multilingual Word Embeddings</a>
        Waleed Ammar et al., 2016<p>
        <a href="https://www.aclweb.org/anthology/Q19-1038.pdf" target="_blank">Massively Multilingual Sentence
          Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a> Mikel Artetxe and Holger Schwenk, 2019</p><p>
        <a href="https://www.aclweb.org/anthology/P19-1493.pdf" target="_blank">How multilingual is Multilingual
          BERT?</a> Telmo Pires et al., 2019
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/ayNoMmoXnd8">
          Language Grounding</a></td>
      <td>
        <a href="https://aclanthology.org/2020.acl-main.463/">Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data</a> Emily Bender and Alexander Koller, 2020<p>
        <a href="https://arxiv.org/abs/2104.10809">Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?</a> Will Merrill et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2209.12407">Entailment Semantics Can Be Extracted from an Ideal Language Model</a> Will Merrill et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2004.10151">Experience Grounds Language</a> Yonatan Bisk et al., 2020
      </p></td>
    </tr>
    <tr>
      <td><a href="https://www.youtube.com/watch?v=pkdV-iddZxk">
          Language and Vision</a></td>
      <td>
       <a href="https://arxiv.org/abs/1505.00468" target="_blank">VQA: Visual Question Answering</a> Aishwarya Agrawal et al., 2015<p>
       <a href="https://arxiv.org/abs/2103.00020" target="_blank">Learning Transferable Visual Models From Natural Language Supervision</a> Alex Radford et al., 2021
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/tTkAjNkXvH4">
          Ethics: Bias</a></td>
      <td>
        <a href="https://aclanthology.org/P16-2096.pdf" target="_blank">The Social Impact of Natural Language Processing</a> Dirk Hovy and Shannon Spruit, 2016<p>
        <a href="https://arxiv.org/pdf/1707.09457.pdf" target="_blank">Men Also Like Shopping:
Reducing Gender Bias Amplification using Corpus-level Constraints</a> Jieyu Zhao et al., 2017
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/0haVbW2ouzw">
          Ethics: Exclusion</a></td>
      <td>
        <a href="https://arxiv.org/abs/2205.12247" target="_blank">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</a> Da Yin et al., 2022<p>
        <a href="https://arxiv.org/abs/2109.13238" target="_blank">Visually Grounded Reasoning across Languages and Cultures</a> Fangyu Liu et al., 2021
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/2PSzHb08Xm4">
          Ethics: Dangers of Automation</a></td>
      <td>
       <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a> Emily Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell, 2021<p>
       <a href="https://arxiv.org/abs/2009.11462" target="_blank">RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a> Samuel Gehman et al., 2020
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hp8qfbsp57M">
          Ethics: Unethical Use and Paths Forward</a></td>
      <td>
       <a href="https://arxiv.org/pdf/1803.09010.pdf">Datasheets for Datasets</a> Timnit Gebru et al., 2018<p>
       <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372873">Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing</a> Deb Raji et al., 2020
      </p></td>
    </tr>

  </tbody>
</table>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wprs – rootless remote desktop for Wayland (and X11, via XWayland) applications (137 pts)]]></title>
            <link>https://github.com/wayland-transpositor/wprs</link>
            <guid>40313798</guid>
            <pubDate>Thu, 09 May 2024 23:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wayland-transpositor/wprs">https://github.com/wayland-transpositor/wprs</a>, See on <a href="https://news.ycombinator.com/item?id=40313798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">wprs</h2><a id="user-content-wprs" aria-label="Permalink: wprs" href="#wprs"></a></p>
<p dir="auto">Like <a href="https://en.wikipedia.org/wiki/Xpra" rel="nofollow">xpra</a>, but for Wayland, and written in
Rust.</p>
<p dir="auto">wprs implements rootless remote desktop access for remote Wayland (and X11, via
XWayland) applications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto"><code>cargo build --profile=release-lto  # or release, but debug is unusably slow</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">On the remote host, enable wprsd:</p>
<div dir="auto" data-snippet-clipboard-copy-content="loginctl enable-linger
systemctl --user enable wprsd.service
systemctl --user start wprsd.service"><pre>loginctl enable-linger
systemctl --user <span>enable</span> wprsd.service
systemctl --user start wprsd.service</pre></div>
<p dir="auto">On the local host:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)
wprs <remote_host> run <application>

# stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)
wprs <remote_host> detach

# attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)
wprs <remote_host> attach"><pre><span><span>#</span> starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> run <span>&lt;</span>application<span>&gt;</span>

<span><span>#</span> stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> detach

<span><span>#</span> attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> attach</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">System Tuning</h2><a id="user-content-system-tuning" aria-label="Permalink: System Tuning" href="#system-tuning"></a></p>
<p dir="auto">Increasing linux's socket buffer limits as described in
<a href="https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces" rel="nofollow">https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces</a>
will result in improved performance.</p>
<p dir="auto">TODO: test ssh socket forwarding performance with different values of
wmem_default. wprs uses setsockopt to increase its buffer size, but it doesn't
seem that ssh does.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Files</h2><a id="user-content-configuration-files" aria-label="Permalink: Configuration Files" href="#configuration-files"></a></p>
<p dir="auto">You can create configuration files for <code>wprsc</code> and <code>wprsd</code> instead of passing additional
arguments to <code>wprs</code>. To see what options are available, run <code>wprsc --help</code> and
<code>wprsd --help</code>.</p>
<p dir="auto">To generate the default configs, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your local machine
wprsc --print-default-config-and-exit=true > ~/.config/wprs/wprsc.ron"><pre><span><span>#</span> on your local machine</span>
wprsc --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsc.ron</pre></div>
<p dir="auto">and</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your remote machine
wprsd --print-default-config-and-exit=true > ~/.config/wprs/wprsd.ron"><pre><span><span>#</span> on your remote machine</span>
wprsd --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsd.ron</pre></div>
<p dir="auto">Then update the <code>wprsc.ron</code> and <code>wprsd.ron</code> files with your desired settings.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current Limitations</h2><a id="user-content-current-limitations" aria-label="Permalink: Current Limitations" href="#current-limitations"></a></p>
<p dir="auto">Currently only the the Core and XDG shell protocols are implemented. In
particular, hardware rendering/dmabuf support is not yet implemented.</p>
<ul dir="auto">
<li>Damage passthough is not yet implemented.</li>
<li>Touch event support is not yet implemented.</li>
<li>Drag-and-drop may be wonky in some cases.</li>
<li>XWayland drag-and-drop is not (yet?) implemented.</li>
<li>webauthn security keys don't yet work in browsers</li>
</ul>
<p dir="auto">Generally, wprs will aim to support as many protocols as feasible, it's a
question of time and prioritization.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">On the remote (server) side, <code>wprsd</code> implements a wayland compositor using
<a href="https://github.com/Smithay/smithay">Smithay</a>. Instead of compositing and
rendering though, wprsd serializes the state of the wayland session and sends it
to the connected wprsc client using a custom protocol.</p>
<p dir="auto">On the local (client) side, <code>wprsc</code> implements a wayland client (using the
<a href="https://github.com/Smithay/client-toolkit">Smithay Client Toolkit</a> that creates
local wayland objects that correspond to remote wayland objects. For example, if
a remote application running against wprsd creates a surface and an
xdg-toplevel, wprsc will create a surface with the same contents, an
xdg-toplevel with the same metadata, etc.. From the local compositor's point of
view, wprsc is just a normal application with a bunch of windows. Input and
other events from the local compositor that wprsc are serialized and sent to
wprsd, which forwards them to the appropriate application (the owner of the
surface which the wprsc surface which received the events corresponds to).</p>
<p dir="auto">wprs supports session resumption (wprsc disconnection and later reconnection and
wprsc restarts). The wayland protocol is not natively resumable in this way
because it relies on shared state between the compositor and client
applications. By implementing a wayland compositor locally relative to the
application, wprsd stores all state necessary for wayland applications and is
also able to store sufficient state (e.g., the buffer contents for each surface
as of the last commit) for a newly-connected wprsc to correctly set up all
necessary wayland objects. wprsc is stateless, but wprsd is not, so a wprsd
restart will still terminate all wayland applications running against it, like
with any other wayland compositor.</p>
<p dir="auto">Communication between wprsd and wprsc happens over unix domain sockets; wprsd
creates a socket and wprsc connects to it. The default mode of operation is to,
on the client side, use ssh to forward a local socket to the remote wprsd
socket, but a different transport could be used with, for example, socat or a
custom proxy application. A launcher script (<code>wprs</code>) is provided which sets up
the ssh socket forwarding.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Protocol</h3><a id="user-content-protocol" aria-label="Permalink: Protocol" href="#protocol"></a></p>
<p dir="auto">The custom protocol used to serialize and transmit wayland state between wprsc
and wprsd is a simplified version of the wayland protocol. Wayland objects are
represented as rust types and serialized using
<a href="https://github.com/rkyv/rkyv">rkyv</a>. Unlike the wayland protocol, the wprs
protocol tries to be idempotent when possible. For example, instead of the
repeated back-and-forth involved in created a surface, creating an xdg-surface,
creating an xdg-toplevel, waiting for it to be configured, creating a buffer,
attaching the buffer, and comitting it, wprsd will send a single commit message
to wprsc with the complete state of the surface (surface's attached buffer
contents (if any), its role (if any) and any associated metadata, etc.) and
wprsc will execute the appropriate dance with the local compositor.</p>
<p dir="auto">Frame callbacks are scheduled locally by wprsd at the configured framerate, they
are not forwarded from wprsc as that would introduce an unacceptable amount of
frame latency due to network round-trips. When no wprsc is connected, wprsd
pauses sending frame callbacks to wayland applications.</p>
<p dir="auto">Buffer compression is handled using a custom multithreaded and SIMD-accelerated
lossless image compression algorithm:</p>
<ol dir="auto">
<li>Transpose the image from an <a href="https://en.wikipedia.org/wiki/AoS_and_SoA" rel="nofollow">array of structures to a struct of
arrays</a>. This makes the subequent
steps significantly faster by letting them be implemented with SIMD
instructions and additionally improves the compression ratio because each
color channel is more closely spatially correlated with itself than with the
other
channels.</li>
<li>Apply an adjacent (wrapping) difference to each color channel (differential
pulse-code modulation). This improves the compression ratio by taking
advantage of spatial correlation and transforms (for example) a solid-colored
line into a single color byte and then a sequence of 0-bytes, or a gradient
into a sequence of 1-bytes, etc.</li>
<li>Transform each color channel into a
<a href="https://en.wikipedia.org/wiki/Y%E2%80%B2UV" rel="nofollow">YUV</a>-like color space: <code>y := g, u := b - g, v := r - g, a := a</code>. This improves the compression ratio in a
similar way as the previous step but by taking advantage of cross-color
correlation.</li>
<li>Compress the data with zstd.
This algorithm was designed for reasonably good compression ratios while being
extremely last: single-digit milliseconds per frame. Decompression is done by
inverting those steps.</li>
</ol>
<p dir="auto">This protocol is <em>not stable</em>: there is no guarantee that different versions of
wprsc and wprsd, or wprsc and wprsd built with different versions of
dependencies or even rustc will be compatible. This may change in the future,
but it will not happen soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comparison to Waypipe</h3><a id="user-content-comparison-to-waypipe" aria-label="Permalink: Comparison to Waypipe" href="#comparison-to-waypipe"></a></p>
<p dir="auto"><a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a>'s model is analogous
to X forwarding, while wprs's model is analgous to Xpra. Waypipe ~transparently
forwards messages between the local compositor and the remote application, so
the client ends up being stateful and sessions can only be resumed through
network reconnections, not client restarts. There are tradeoffs to the two
approaches. Waypipe's approach is partially forward-compatible: it can support
new wayland protocols automatically, however those protocols may be broken if
they use shared resources in a way that waypipe doesn't know how to handle.
wprs, on the other hand, requires explicit implementation for every wayland
protocol.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">XWayland</h3><a id="user-content-xwayland" aria-label="Permalink: XWayland" href="#xwayland"></a></p>
<p dir="auto">XWayland support is implemented as a separate binary, <code>xwayland-xdg-shell</code>. The
binary implements a wayland compositor (but only for the protocol features used
by xwayland) and client, just like wprsd and wprsc, but in a single binary (so
skipping the serialization/deserialization). This is the same model as
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>,
which is itself inspired by
<a href="https://chromium.googlesource.com/chromiumos/platform2/+/main/vm_tools/sommelier/" rel="nofollow">sommelier</a>.
xwayland-xdg-shell was primarily written (instead of just using
xwayland-proxy-virtwl) so as to share a common design/codebase with wprs and to
make use of common wayland development in the form of Smithay and its wayland
crates. Additionally, xwayland-xdg-shell is more narrowly focused and its sole
purpose is xwayland support, not virtio-gpu or virtwl.</p>
<p dir="auto">Like xwayland-proxy-virtwl, xwayland-xdg-proxy can be used to implement external
xwayland support for any wayland compositor instead of re-implementing it inside
the compositor. Aside from eliminating the need to implement xwayland support in
every compositor, this approach has been reported to result in better xwayland
scaling than native xwayland support in some compositor, and it allows xwayland
applications to be treated more like regular wayland applications instead of
getting special access.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security</h3><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">wprsd is a wayland compositor, so it has access to all surfaces displayed by
applications running against it and it can inject input into them. Any process
which implements the wprs protocol and connects to the wprs socket will have the
same access. For that reason, the wprs socket is created in a directory which
only the user has access to ($XDG_RUNTIME_DIR) and the socket itself is only
readable/writable by the user. Malicious applications running as the same user
as wprsd can still access this socket, but at that point you have bigger
problems.</p>
<p dir="auto">wprs does not do any auth of its own, it relies entirely on whatever transport
is being used (ssh, in the default case).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">Huge thanks to the following excellent projects for making this project
significantly easier than it otherwise would have been:</p>
<ul dir="auto">
<li><a href="https://github.com/Smithay">Smithay</a></li>
<li><a href="https://github.com/rkyv/rkyv">rkyv</a></li>
<li><a href="https://github.com/tokio-rs/tracing">tracing</a></li>
<li><a href="https://github.com/wolfpld/tracy">Tracy</a></li>
</ul>
<p dir="auto">Thanks to <a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a> and
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>
for paving the way in this problem space.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple apologizes for iPad 'Crush' ad that 'missed the mark' (450 pts)]]></title>
            <link>https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</link>
            <guid>40313733</guid>
            <pubDate>Thu, 09 May 2024 22:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology">https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</a>, See on <a href="https://news.ycombinator.com/item?id=40313733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Apple has apologized after a commercial meant to showcase its <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">brand-new iPad Pro</a> drew widespread criticism among the creative community. In a statement <a href="https://adage.com/article/digital-marketing-ad-tech-news/apple-apologizes-ipad-pro-crushed-ad-it-missed-mark/2559321">provided to <em>Ad Age</em></a>, Tor Myhren, Apple’s vice president of marketing, said the company “missed the mark.”</p><p>“Creativity is in our DNA at Apple, and it’s incredibly important to us to design products that empower creatives all over the world,” Myhren told <em>Ad Age</em>. “Our goal is to always celebrate the myriad of ways users express themselves and bring their ideas to life through iPad. We missed the mark with this video, and we’re sorry.”</p><p>On Tuesday, Apple introduced the <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">M4-powered iPad Pro</a>, which the company described as its thinnest product ever. To advertise all the creative possibilities with the iPad, it released a “Crush!” commercial that shows things like a piano, record player, paint, and other works flattening under the pressure of a hydraulic press. At the end, only one thing remains: an iPad Pro.</p><p>The ad rubbed some creatives the wrong way. Hugh Grant <a href="https://twitter.com/HackedOffHugh/status/1788183871504204257">called it</a> a “destruction of human experience,” while <em>Handmaid’s Tale</em> director Reed Morano <a href="https://twitter.com/reedmorano/status/1788298509780685261">told Apple CEO</a> Tim Cook to “read the room” in a post on X. Apple didn’t immediately respond to <em>The Verge</em>’s request for comment.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The world has probably passed peak pollution (193 pts)]]></title>
            <link>https://www.sustainabilitybynumbers.com/p/peak-pollution</link>
            <guid>40313451</guid>
            <pubDate>Thu, 09 May 2024 22:11:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sustainabilitybynumbers.com/p/peak-pollution">https://www.sustainabilitybynumbers.com/p/peak-pollution</a>, See on <a href="https://news.ycombinator.com/item?id=40313451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>The health impacts of air pollution are often underrated. There are a </span><a href="https://ourworldindata.org/data-review-air-pollution-deaths" rel="">range of estimates</a><span> for how many people die prematurely from local air pollution every year.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-144199988" href="https://www.sustainabilitybynumbers.com/p/peak-pollution#footnote-1-144199988" target="_self" rel="">1</a></span><span> All are in the low millions. The World Health Organization </span><a href="https://www.who.int/health-topics/air-pollution" rel="">estimates around</a><span> 7 million.</span></p><p>The good news, then, is that the world is probably passed “peak pollution”. I say “probably” because confidently declaring a peak is, apparently, the best way to make sure it doesn’t happen.</p><p><span>Here, I’m talking specifically about emissions of harmful </span><em>local</em><span> air pollutants: gases like nitrogen oxides (NOx), sulphur dioxide which causes acid rain, carbon monoxide, black carbon, organic carbon, non-methane volatile organic compounds. I’m not talking about greenhouse gases.</span></p><p><span>The </span><a href="https://github.com/JGCRI/CEDS/tree/master" rel="">Community Emissions Data System (CEDS)</a><span> recently extended its long-term dataset on emissions of air pollutants up to the end of 2022.</span></p><p><span>I updated this data in our explorer tool </span><a href="https://ourworldindata.org/explorers/air-pollution" rel="">on Our World in Data</a><span> (where you can explore the trends by country).</span></p><p>What’s striking is that emissions appear to have peaked for all of these pollutants, with the exception of ammonia, which is almost entirely produced by agriculture. Organic carbon and NMVOCs are not quite out of the clear yet, but might not reach their previous peaks again.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Of course, emissions are not falling everywhere. They’ve fallen steeply in richer countries like the US and much of Europe. And the big turning point for the global figures has been the rapid turnaround in China. Emissions have declined rapidly in the last decade, with huge gains for public health.</p><p><span>It’s in low and lower-middle income countries where emissions are still rising, and pollution levels in cities are the highest. This is not surprising: air pollution is one of the few areas where the “</span><a href="https://en.wikipedia.org/wiki/Kuznets_curve#Environmental_Kuznets_curve" rel="">Environmental Kuznets Curve</a><span>” tells a pretty accurate and consistent story.</span></p><p>Air pollution increases as countries develop, gain access to energy, and industrialise. They then fall once a country gets rich enough to impose pollution standards and limits without infringing on development and the move away from energy poverty.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The goal now is to see if countries can move through this curve much faster – and with lower levels of pollution – than countries like the US or the UK did. This should be doable: we’ve learned a lot over the last 50 years about how to produce energy with less pollution, what technologies work and don’t work, and have reduced the costs of solutions that were expensive in their early days.</p><p><span>Note that this is not a finger-pointing exercise where rich countries tell poorer ones not to pollute. We’re mostly talking about </span><em>local</em><span> air pollution. The negative impacts of pollution are felt by domestic populations. It’s about how we ensure that the poorest countries can gain access to energy, alleviate poverty, and develop while limiting the number of people who die prematurely from air pollution in the process.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cubic millimetre of brain mapped in spectacular detail (169 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-01387-9</link>
            <guid>40313193</guid>
            <pubDate>Thu, 09 May 2024 21:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-01387-9">https://www.nature.com/articles/d41586-024-01387-9</a>, See on <a href="https://news.ycombinator.com/item?id=40313193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Researchers have mapped a tiny piece of the human brain in astonishing detail. The resulting cell atlas, which was described today in <i>Science</i><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> and is <a href="https://h01-release.storage.googleapis.com/data.html" data-track="click" data-label="https://h01-release.storage.googleapis.com/data.html" data-track-category="body text link">available online</a>, reveals new patterns of connections between brain cells called neurons, as well as cells that wrap around themselves to form knots, and pairs of neurons that are almost mirror images of each other.</p><p>The 3D map covers a volume of about one cubic millimetre, one-millionth of a whole brain, and contains roughly 57,000 cells and 150 million synapses — the connections between neurons. It incorporates a colossal 1.4 petabytes of data. “It’s a little bit humbling,” says Viren Jain, a neuroscientist at Google in Mountain View, California, and a co-author of the paper. “How are we ever going to really come to terms with all this complexity?”</p><h2>Slivers of brain</h2><p>The brain fragment was taken from a 45-year-old woman when she underwent surgery to treat her epilepsy. It came from the cortex, a part of the brain involved in learning, problem-solving and processing sensory signals. The sample was immersed in preservatives and stained with heavy metals to make the cells easier to see. Neuroscientist Jeff Lichtman at Harvard University in Cambridge, Massachusetts, and his colleagues then cut the sample into around 5,000 slices — each just 34 nanometres thick — that could be imaged using electron microscopes.</p><p>Jain’s team then built artificial-intelligence models that were able to stitch the microscope images together to reconstruct the whole sample in 3D. “I remember this moment, going into the map and looking at one individual synapse from this woman’s brain, and then zooming out into these other millions of pixels,” says Jain. “It felt sort of spiritual.”</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Rendering of a neuron with a round base and many branches, on a black background." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg">
  <figcaption>
   <p><span>A single neuron (white) shown with 5,600 of the axons (blue) that connect to it. The synapses that make these connections are shown in green.</span><span>Credit: Google Research &amp; Lichtman Lab (Harvard University). Renderings by D. Berger (Harvard University)</span></p>
  </figcaption>
 </picture>
</figure><p>When examining the model in detail, the researchers discovered unconventional neurons, including some that made up to 50 connections with each other. “In general, you would find a couple of connections at most between two neurons,” says Jain. Elsewhere, the model showed neurons with tendrils that formed knots around themselves. “Nobody had seen anything like this before,” Jain adds.</p><p>The team also found pairs of neurons that were near-perfect mirror images of each other. “We found two groups that would send their dendrites in two different directions, and sometimes there was a kind of mirror symmetry,” Jain says. It is unclear what role these features have in the brain.</p><h2>Proofreaders needed</h2><p>The map is so large that most of it has yet to be manually checked, and it could still contain errors created by the process of stitching so many images together. “Hundreds of cells have been ‘proofread’, but that’s obviously a few per cent of the 50,000 cells in there,” says Jain. He hopes that others will help to proofread parts of the map they are interested in. The team plans to produce similar maps of brain samples from other people — but a map of the entire brain is unlikely in the next few decades, he says.</p><p>“This paper is really the tour de force creation of a human cortex data set,” says Hongkui Zeng, director of the Allen Institute for Brain Science in Seattle. The vast amount of data that has been made freely accessible will “allow the community to look deeper into the micro-circuitry in the human cortex”, she adds.</p><p>Gaining a deeper understanding of how the cortex works could offer clues about how to treat some psychiatric and neurodegenerative diseases. “This map provides unprecedented details that can unveil new rules of neural connections and help to decipher the inner working of the human brain,” says Yongsoo Kim, a neuroscientist at Pennsylvania State University in Hershey.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sioyek is a PDF viewer with a focus on textbooks and research papers (291 pts)]]></title>
            <link>https://github.com/ahrm/sioyek</link>
            <guid>40313143</guid>
            <pubDate>Thu, 09 May 2024 21:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ahrm/sioyek">https://github.com/ahrm/sioyek</a>, See on <a href="https://news.ycombinator.com/item?id=40313143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sioyek</h2><a id="user-content-sioyek" aria-label="Permalink: Sioyek" href="#sioyek"></a></p>
<p dir="auto">Sioyek is a PDF viewer with a focus on textbooks and research papers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
<li><a href="#install">Installation</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#feature-video-overview">Video Demo</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#build-instructions">Build Instructions</a></li>
<li><a href="#donation">Buy Me a Coffee (or a Book!)</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Official packages</h3><a id="user-content-official-packages" aria-label="Permalink: Official packages" href="#official-packages"></a></p>
<p dir="auto">There are installers for Windows, macOS and Linux. See <a href="https://github.com/ahrm/sioyek/releases">Releases page</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebew Cask</h3><a id="user-content-homebew-cask" aria-label="Permalink: Homebew Cask" href="#homebew-cask"></a></p>
<p dir="auto">There is a homebrew cask available here: <a href="https://formulae.brew.sh/cask/sioyek" rel="nofollow">https://formulae.brew.sh/cask/sioyek</a>. Install by running:</p>
<div data-snippet-clipboard-copy-content="brew install --cask sioyek"><pre><code>brew install --cask sioyek
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Third-party packages for Linux</h3><a id="user-content-third-party-packages-for-linux" aria-label="Permalink: Third-party packages for Linux" href="#third-party-packages-for-linux"></a></p>
<p dir="auto">If you prefer to install sioyek with a package manager, you can look at this list. Please note that they are provided by third party packagers. USE AT YOUR OWN RISK! If you're reporting a bug for a third-party package, please mention which package you're using.</p>
<table>
<thead>
<tr>
<th>Distro</th>
<th>Link</th>
<th>Maintainer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flathub</td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">@nbenitez</a></td>
</tr>
<tr>
<td>Alpine</td>
<td><a href="https://pkgs.alpinelinux.org/packages?name=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/jirutka">@jirutka</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek" rel="nofollow">AUR sioyek</a></td>
<td><a href="https://github.com/goggle">@goggle</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-git/" rel="nofollow">AUR sioyek-git</a></td>
<td><a href="https://github.com/hrdl-github">@hrdl-github</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-appimage/" rel="nofollow">AUR sioyek-appimage</a></td>
<td><a href="https://github.com/DhruvaSambrani">@DhruvaSambrani</a></td>
</tr>
<tr>
<td>Debian</td>
<td><a href="https://packages.debian.org/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
<tr>
<td>NixOS</td>
<td><a href="https://search.nixos.org/packages?channel=unstable&amp;show=sioyek&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/podocarp">@podocarp</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/Publishing/sioyek" rel="nofollow">Publishing</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/openSUSE:Factory/sioyek" rel="nofollow">Factory</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>Ubuntu</td>
<td><a href="https://packages.ubuntu.com/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">You can view the official documentation <a href="https://sioyek-documentation.readthedocs.io/en/latest/" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Video Overview</h2><a id="user-content-feature-video-overview" aria-label="Permalink: Feature Video Overview" href="#feature-video-overview"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=yTmCI0Xp5vI" rel="nofollow"><img src="https://camo.githubusercontent.com/3bbdb00658bc336a5da25c541e8141b7d38738232eba44310c960639633bb396/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f79546d43493058703576492f302e6a7067" alt="Sioyek feature overview" data-canonical-src="https://img.youtube.com/vi/yTmCI0Xp5vI/0.jpg"></a></p>
<p dir="auto">For a more in-depth tutorial, see this video:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=RaHRvnb0dY8" rel="nofollow"><img src="https://camo.githubusercontent.com/b1817153034c5bf93e9b14a37292ae3bf2e949d900169928f9720ece7832d452/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f52614852766e62306459382f302e6a7067" alt="Sioyek Tutorial" data-canonical-src="https://img.youtube.com/vi/RaHRvnb0dY8/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Open</h3><a id="user-content-quick-open" aria-label="Permalink: Quick Open" href="#quick-open"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description recent_docs.mp4">recent_docs.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can quickly search and open any file you have previously interacted with using sioyek.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table of Contents</h3><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description toc.mp4">toc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can search and jump to table of contents entries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Smart Jump</h3><a id="user-content-smart-jump" aria-label="Permalink: Smart Jump" href="#smart-jump"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description jump.mp4">jump.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can jump to any referenced figure or bibliography item <em>even if the PDF file doesn't provide links</em>. You can also search the names of bibliography items in google scholar/libgen by middle clicking/shift+middle clicking on their name.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overview</h3><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description overview.mp4">overview.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can open a quick overview of figures/references/tables/etc. by right clicking on them (Like Smart Jump, this feature works even if the document doesn't provide links).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mark</h3><a id="user-content-mark" aria-label="Permalink: Mark" href="#mark"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description mark.mp4">mark.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Sometimes when reading a document you need to go back a few pages (perhaps to view a definition or something) and quickly jump back to where you were. You can achieve this by using marks. Marks are named locations within a PDF file (each mark has a single character name for example 'a' or 'm') which you can quickly jump to using their name. In the aforementioned example, before going back to the definition you mark your location and later jump back to the mark by invoking its name. Lower case marks are local to the document and upper case marks are global (this should be very familiar to you if you have used vim).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bookmarks</h3><a id="user-content-bookmarks" aria-label="Permalink: Bookmarks" href="#bookmarks"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description bookmarks.mp4">bookmarks.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Bookmarks are similar to marks except they are named by a text string and they are all global.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Highlights</h3><a id="user-content-highlights" aria-label="Permalink: Highlights" href="#highlights"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description highlights.mp4">highlights.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Highlight text using different kinds of highlights. You can search among all the highlights.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Portals (this feature is most useful for users with multiple monitors)</h3><a id="user-content-portals-this-feature-is-most-useful-for-users-with-multiple-monitors" aria-label="Permalink: Portals (this feature is most useful for users with multiple monitors)" href="#portals-this-feature-is-most-useful-for-users-with-multiple-monitors"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description portal.mp4">portal.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Suppose you are reading a paragraph which references a figure which is not very close to the current location. Jumping back and forth between the current paragraph and the figure can be very annoying. Using portals, you can link the paragraph's location to the figure's location. Sioyek shows the closest portal destination in a separate window (which is usually placed on a second monitor). This window is automatically updated to show the closest portal destination as the user navigates the document.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description config.mp4">config.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can customize all key bindings and some UI elements by editing <code>keys_user.config</code> and <code>prefs_user.config</code>. The default configurations are in <code>keys.config</code> and <code>prefs.config</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build Instructions</h2><a id="user-content-build-instructions" aria-label="Permalink: Build Instructions" href="#build-instructions"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux</h3><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fedora</h4><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>
<p dir="auto">Run the following commands to install dependencies, clone the repository and compile sioyek on Fedora (tested on Fedora Workstation 36).</p>
<div data-snippet-clipboard-copy-content="sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Generic distribution</h4><a id="user-content-generic-distribution" aria-label="Permalink: Generic distribution" href="#generic-distribution"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install Qt 5 and make sure <code>qmake</code> is in <code>PATH</code>.</p>
<p dir="auto">Run <code>qmake --version</code> to make sure the <code>qmake</code> in path is using Qt 5.x.</p>
</li>
<li>
<p dir="auto">Install <code>libharfbuzz</code>:</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="sudo apt install libharfbuzz-dev"><pre><code>sudo apt install libharfbuzz-dev
</code></pre></div>
<ol start="3" dir="auto">
<li>Clone the repository and build:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ol dir="auto">
<li>Install Visual Studio (tested on 2019, other relatively recent versions should work too)</li>
<li>Install Qt 5 and make sure qmake is in <code>PATH</code>.</li>
<li>Clone the repository and build using 64 bit Visual Studio Developer Command Prompt:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<ol dir="auto">
<li>Install Xcode.</li>
<li>Clone the repository and build: (The code below is in Zsh, which is the default shell on macOS.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
chmod +x build_mac.sh

brew install 'qt@5' freeglut mesa harfbuzz

export PATH=&quot;/opt/homebrew/opt/qt@5/bin:$PATH&quot;
#: The above is needed to make =qmake= from =qt= be found.
#: Find the path using =brew info 'qt@5'=.

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)"><pre>(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
<span>cd</span> sioyek
chmod +x build_mac.sh

brew install <span><span>'</span>qt@5<span>'</span></span> freeglut mesa harfbuzz

<span>export</span> PATH=<span><span>"</span>/opt/homebrew/opt/qt@5/bin:<span>$PATH</span><span>"</span></span>
<span><span>#</span>: The above is needed to make =qmake= from =qt= be found.</span>
<span><span>#</span>: Find the path using =brew info 'qt@5'=.</span>

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donation</h2><a id="user-content-donation" aria-label="Permalink: Donation" href="#donation"></a></p>
<p dir="auto">If you enjoy sioyek, please consider donating to support its development.</p>
<p dir="auto"><a href="https://www.buymeacoffee.com/ahrm" rel="nofollow"><img src="https://camo.githubusercontent.com/4412aa44a78a18c03862fd7da2de5bd81e3817a3adec90fdd41671170a206abd/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f64656661756c742d6f72616e67652e706e67" alt="Buy Me A Coffee" height="41" width="174" data-canonical-src="https://cdn.buymeacoffee.com/buttons/default-orange.png"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How bad are satellite megaconstellations for astronomy? (104 pts)]]></title>
            <link>https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/</link>
            <guid>40312469</guid>
            <pubDate>Thu, 09 May 2024 20:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/">https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/</a>, See on <a href="https://news.ycombinator.com/item?id=40312469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div id="attachment_41665"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-scaled.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-41665" src="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-263x350.jpg" alt="" width="263" height="350" srcset="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-263x350.jpg 263w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-768x1024.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-150x200.jpg 150w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-1152x1536.jpg 1152w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-1536x2048.jpg 1536w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-scaled.jpg 1920w" sizes="(max-width: 263px) 100vw, 263px"></a></p><p id="caption-attachment-41665">Wait a minute.<br>Image credit: Barbara David</p></div>
<p>Over the last number of years, our planet has become encircled by Starlink, OneWeb, and other “megaconstellation” satellites.</p>
<p>Yes, the emergence of those megaconstellations offers great benefit for humanity. But in a wait-a-minute pause, there are also substantial costs, including the imposition on humankind’s ongoing and growing thirst for astronomical peering into the surrounding universe.</p>
<p>That’s the view of David Koplow, the Scott K. Ginsburg Professor of Law at Georgetown University Law Center in Washington, D.C.</p>
<div id="attachment_31178"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2.jpg"><img decoding="async" aria-describedby="caption-attachment-31178" src="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-350x233.jpg" alt="" width="350" height="233" srcset="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-350x233.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-1024x682.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-200x133.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-768x512.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2.jpg 1280w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-31178">Starlink constellation pass overhead near Carson National Forest, New Mexico, photographed soon after launch. &nbsp;<br>SpaceX Starlink Satellites over Carson National Forest, New Mexico, photographed soon after launch.<br>Credit: Mike Lewinsky/Creative Commons Attribution 2.0</p></div>
<p>“We are just beginning to appreciate how bad the disruption can be for land-based and space-based telescopes, and as more and more satellite overflights occur, the problems will only intensify,” Koplow told <em>Inside Outer Space</em>.</p>
<p><strong>Legal rights</strong></p>
<p>Koplow’s concerns have been voiced in several scholarly works, the titles of which underscore his qualms, such as: “<em>Large Constellations of Small Satellites: The Good, the Bad, the Ugly and the Illegal</em>,” as well as “<em>Blinded by the Light: Resolving the Conflict Between Satellite Megaconstellations and Astronomy</em>.”</p>
<div id="attachment_22571"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n.jpg"><img decoding="async" aria-describedby="caption-attachment-22571" src="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-350x240.jpg" alt="" width="350" height="240" srcset="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-350x240.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-1024x703.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-200x137.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-768x527.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n.jpg 1138w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-22571">Starlink satellites visible in a mosaic of an astronomical image.<br>Courtesy of NSF’s<br>National Optical-Infrared Astronomy Research Laboratory/NSF/AURA/CTIO/DELVE)</p></div>
<p>&nbsp;“The world has mostly been assuming that the relevant international law basically allows the satellite companies to do whatever they want in space, while forcing the observatories to adapt as well as they can,” Koplow advised.&nbsp;</p>
<p>But in reality, Koplow continues, the legal regime is not so one-sided. “Astronomers also have legal rights to free use of space, and they need not stand by idly while their profession is damaged.”</p>
<p><strong>Hair on fire</strong></p>
<p>Koplow points out that in 2019 the world of optical and radio astronomy changed abruptly and massively when the first SpaceX batch of 60 Starlink satellites was lofted.</p>
<p>“Jolted by the sudden brightness of those spacecraft, and alarmed by the prospect of their legions of successors, observatories scrambled to respond,” Koplow observes.</p>
<p>They did so by studying and documenting the true dimensions of the problem, beginning to invent or conceptualize mitigation measures, and entering into discussions with SpaceX and other companies.</p>
<p>“Some astronomers see this as a true ‘hair on fire’ emergency, heralding irretrievable losses to space science; others present a more sanguine face, depicting this as yet another challenge to be surmounted in surveying a decreasingly pristine sky,” Koplow remarks.</p>
<div id="attachment_36401"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-scaled.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-36401" src="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-350x173.jpg" alt="" width="350" height="173" srcset="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-350x173.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-1024x507.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-200x99.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-768x381.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-1536x761.jpg 1536w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-2048x1015.jpg 2048w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-36401">Image credit: ESO/P. Horálek</p></div>
<p><strong>Incipient clash</strong></p>
<p>That said, the astronomical community has related that the time and the financial costs of conducting effective astronomy will rise considerably, Koplow says, “and that some important data will simply be irretrievable, with concomitant losses for science and the future exploration and use of space.”</p>
<p>In his “Blinded by the Light” treatise, Koplow describes the incipient clash between satellite megaconstellations and astronomy, assesses the relevant international and domestic legal authorities, and proposes compromise solutions to mitigate the damage.</p>
<p>“Overall, the thesis is that a better balance must be struck between these competing types of space activities,” Koplow adds, “without ceding to either a comprehensive right to proceed in disregard of the key functions of the other.”</p>
<div id="attachment_12276"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-12276" src="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-350x144.jpg" alt="" width="350" height="144" srcset="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-350x144.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-200x82.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-768x316.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-1024x421.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb.jpg 1200w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-12276">Credit: OneWeb</p></div>
<p><strong>Voluntary measures</strong></p>
<p>Koplow acknowledges that some satellite companies have voluntarily invested considerable corporate talent and money in efforts to mitigate their interference with astronomy.&nbsp;</p>
<p>“But these voluntary measures are not adequate to solve the problem, they are not durable and reliable, and they have not been adopted by all the companies,” Koplow suggests.</p>
<p>“A stronger response is necessary,” Koplow concludes.</p>
<p>To gain access to “<em>Blinded by the Light: Resolving the Conflict Between Satellite Megaconstellations and Astronomy” </em>go to: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4346299">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4346299</a></p>
<p><em>To review the paper “Three Things I Hate About Large Constellations of Small Satellites” </em>go to: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4503593">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4503593</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The history of 'OK' (2023) (104 pts)]]></title>
            <link>https://people.howstuffworks.com/history-ok.htm</link>
            <guid>40312434</guid>
            <pubDate>Thu, 09 May 2024 20:07:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.howstuffworks.com/history-ok.htm">https://people.howstuffworks.com/history-ok.htm</a>, See on <a href="https://news.ycombinator.com/item?id=40312434">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="editorial-body">

					
	
				
	
																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																									
				


	


	<div id="page0" data-slide="0" data-track-gtm="Content">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif">
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH19fQ=="><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH19fQ=="><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==">
				<img src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==" alt="OK word" width="828" height="465">
			</picture>
			

		
	

					
	
	<figcaption>
											The spread of "OK" shows how important an all-purpose word can be. <span>Foxys Graphic/Shutterstock</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>"OK" is probably the <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">most spoken word</a> in the world — besides English, people say "OK" in a dozen languages, including Spanish, Italian and Russian — and yet almost nobody can tell you what those two letters stand for or where the word came from.</p>

		
	
					
				
				


	
		<p>Was it borrowed from the <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">indigenous Choctaw word</a> "okeh," meaning, roughly, "OK"? Did it originate with a Boston baker named Otto Kimmel who liked to <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1">frost his initials</a> into his cookies? Does it have anything to do with the state of Oklahoma (OK) or the musical "<a href="https://youtu.be/kGXu0j6QDJ8">Oklahoma!</a>"?</p>

		
	
								

		
	
											

						
				
				


	
		<p>Nope, nope and nope. In fact, there's <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">no evidence</a> that "okeh" was part of the Chocktaw language.</p>

		
	
													
				
				


	
		<p>"OK is the greatest American word," says <a href="https://cla.umn.edu/about/directory/profile/aliber">Anatoly Liberman</a>, a linguist, translator and language professor at the University of Minnesota. "The history of OK is a history of incredible success, but nobody could have predicted that success."</p>

		
	
					
				
				


	
		<p>As you'll see, OK began as a piece of insider slang from the late 1830s and rode a (losing) presidential campaign to nationwide fame and eventually worldwide ubiquity.</p>

		
	
					
				
						
						

				
				

		
	




</div>		<div data-track-gtm="TOC">
			<p><strong>Contents</strong></p><ol>
						
					
																
										
					<li>
						<a data-target="pt1" href="#pt1">The Acronym Craze of the 1830s</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt2" href="#pt2">Misspelling Words Was Also a Thing</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt3" href="#pt3">The Very First Use of OK</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt4" href="#pt4">"Old Kinderhook" Takes "OK" National</a>
					</li>
										</ol>
		</div>



		
	<div id="page-wrap1" x-data="{ pageVisible : true }"><h2 data-page-nbr="1" data-logged="false" data-page-url="history-ok1.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page1" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Acronym Craze of the 1830s</span>

	
</span>		</h2>

				

<div id="page1" data-slide="1" data-track-gtm="Content" x-show.transition="pageVisible === true">	
						
	





	
		<p>In the early 19th century, new printing technologies dramatically reduced the cost of publishing a daily newspaper, and there was a resulting explosion of inexpensive new dailies known collectively as the <a href="https://blogs.ubc.ca/etec540sept09/2009/10/19/the-rise-of-penny-newspapers-and-their-influence-on-mass-media/">penny press</a>. Competing for readers, penny papers in cities like New York, Philadelphia and Boston published not only straight news stories, but also witty takes on the latest political scandals, social scenes and popular trends.</p>

		
	
					
				
				


	
		<p>Think of it as the internet of the 1830s. And much like the internet, the lively back-and-forth chatter between penny paper editors gave birth to a new way of writing and eventually a new way of speaking.</p>

		
	
								

		
	
										
				
				


	
		<p>"Beginning in the summer of 1838, there developed in Boston a remarkable vogue of using abbreviations. It might well be called a craze," <a href="https://www.jstor.org/stable/453580">wrote</a> the famed etymologist <a href="https://www.nytimes.com/2002/10/18/nyregion/allen-read-96-the-ok-expert-is-dead.html">Allen Walker Read</a>, who was the first person to trace the full history of OK.</p>

		
	
					
				
				


	
		<p>Take these examples from Boston's Morning Post, whose editor, Charles Gordon Greene, sprinkled his columns with winking acronyms for everything and anything:</p>

		
	
					
				
				


	
		<div>
	<ul><li><span>O.F.M. ("our first men")</span></li><li><span>W.O.O.O.F.C. ("with one of our first citizens")</span></li><li><span>R.T.B.S. ("remains to be seen")</span></li><li><span>D.L.E.C. ("do let 'em come")</span></li><li><span>G.T.D.H.D. ("give the devil his due")</span></li><li><span>W.Y.G. ("will you go?")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>By 1939, the "initial language," as it was sometimes called, had arrived in New York City and had already leapt from print to fashionable slang. "This is a species of spoken shorthand, which is getting into very general use among loafers and gentlemen of the fancy," <a href="https://www.jstor.org/stable/453580">wrote the editors</a> of New York's Evening Tattler.</p>

		
	
					
				
				


	
		<p>The editors even claimed to have overheard a conversation between two young sweethearts, where the girl turned to her beau and said, "O.K.K.B.W.P." "What could she have meant," wrote the Evening Tattler, "but 'One Kind Kiss Before We Part'?"</p>

		
	
					
				
				
	
									
				

		
	




</div>

			</div>


		
	<div id="page-wrap2" x-data="{ pageVisible : true }"><h2 data-page-nbr="2" data-logged="false" data-page-url="history-ok2.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page2" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>Misspelling Words Was Also a Thing</span>

	
</span>		</h2>

				

<div id="page2" data-slide="2" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>In addition to the abbreviation craze, 19th-century Americans thought it was really funny to purposely misspell stuff. Read, the etymologist, cited the example of the comic writer George W. Arnold, who used the pen name "Joe Strickland" to write mangled letters to his fictional family, like this one from a trip abroad: "when I got here tha axt me if I was evver in Turky before. no ses I. but i've had a darn menny turkeys in me."</p>

		
	
					
				
				


	
		<p>By the late 1830s, the (hilarious) misspelling trend had combined with the acronym craze to produce punchy abbreviations like:</p>

		
	
								

		
	
										
				
				


	
		<div>
	<ul><li><span>K.G. for "no go" (as if spelled "know go")</span></li><li><span>K.Y. for "no use" (as if spelled "know yuse")</span></li><li><span>O.W. for "all right" (as if spelled "oll wright")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>Absolutely no one says K.G. or O.W. anymore, but believe it or not, that witty wordplay laid the groundwork for the arrival of a two-letter abbreviation that would conquer the world.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap3" x-data="{ pageVisible : true }"><h2 data-page-nbr="3" data-logged="false" data-page-url="history-ok3.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page3" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Very First Use of OK</span>

	
</span>		</h2>

				

<div id="page3" data-slide="3" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>Before we get to the fateful date of March 21, 1839, let's tip our hats one more time to Allen Walker Read, the man who solved the mystery of OK's origins. Keep in mind that Read was working in the 1960s, decades before searchable digital newspaper archives.</p>

		
	
					
				
				


	
		<p>"Read must have spent hundreds of hours digging through tons and tons of physical newspapers, journals, private letters and other documents," says Liberman, who writes the weekly <a href="https://blog.oup.com/category/series-columns/oxford_etymologist/">Oxford Etymologist</a> blog and knows firsthand how hard it is to track down the history of words. "What that man did was absolutely astounding."</p>

		
	
								

		
	
										
				
				


	
		<p>OK, back to our story.</p>

		
	
					
				
				


	
		<p>In the spring of 1839, the editor of Boston's Morning Post, Charles Gordon Greene, was engaged in some good-natured trash talk with the editors of the Providence Journal in Rhode Island<i>.</i> It had to do with a semi-satirical citizens group in Boston called the Anti-Bell-Ringing Society (or A.B.R.S.), of which Greene was a member.</p>

		
	
					
				
				


	
		<p>The Providence paper poked fun at Greene and the A.B.R.S. and Greene had to set the record straight. So it was that on March 21, 1839, at the end of a short paragraph defending the A.B.R.S., Greene printed the following words: "<i>o.k.</i> — all correct."</p>

		
	
					
				
				


	
		<p>See what he did there? Similar to using O.W. for "oll wright," Greene had coined a new misspelled acronym: O.K. for "oll korrect." Three days after Greene introduced OK to the world, the Providence Journal editors responded with an "O.K." of their own.</p>

		
	
					
				
				


	
		<p>Like other offbeat acronyms of the day, O.K. was an inside joke randomly thrust into general circulation. But unlike O.W. or K.G., which enjoyed brief popularity in the 1830s, O.K. didn't die out.</p>

		
	
					
				
				


	
		<p>"Nobody knew that this facetious abbreviation would have such a long and happy life," says Liberman.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap4" x-data="{ pageVisible : true }"><h2 data-page-nbr="4" data-logged="false" data-page-url="history-ok4.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page4" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>"Old Kinderhook" Takes "OK" National</span>

	
</span>		</h2>

				

<div id="page4" data-slide="4" data-track-gtm="Content" x-show.transition="pageVisible === true">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif">
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9fX0="><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9fX0="><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=">
				<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAJCAQAAACRI2S5AAAAEElEQVR42mNkIAAYRxWAAQAG9gAKqv6+AwAAAABJRU5ErkJggg==" data-src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=" alt="Martin Van Buren 1840 campaign" width="828" height="648">
			</picture>
			

		
	

					
	
	<figcaption>
											This political cartoon shows Martin Van Buren, who ran unsuccessfully against William Henry Harrison, known as the "log cabin and hard cider" candidate, during the 1840 presidential campaign. Van Buren's "OK" nickname is prominent. <span>Bettman/Getty Images</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>If you thought that the word OK <a href="https://www.npr.org/templates/story/story.php?storyId=5170008">originated</a> with Martin Van Buren, you'd be half right. The eighth president of the United States <a href="https://www.nps.gov/mava/index.htm">hailed from the small town</a> of Kinderhook, New York. Like his mentor and fellow Democrat Andrew Jackson, who was known as "Old Hickory," Van Buren's nickname was "Old Kinderhook."</p>

		
	
					
				
				


	
		<p>In the 1840 presidential election, William Henry Harrison and the Whig party challenged the incumbent Van Buren. Harrison's supporters came up with the catchy (for its time) campaign slogan (<a href="https://potus-geeks.livejournal.com/425230.html">and song</a>), "Tippecanoe and Tyler Too." The Democrats swung back with a slogan of their own: "O.K." <a href="https://www.history.com/news/the-birth-of-ok-175-years-ago">As in</a>, "Old Kinderhook is OK!"</p>

		
	
								

		
	
										
				
				


	
		<p>"[Van Buren] got the nickname Old Kinderhook, and early in 1840, OK clubs sprung up with the slogan, 'OK is OK.' So taking that funny little word and making it a mainstay of the political conversation in 1840, suddenly OK was <i>way</i> OK," said the late linguist Allan Metcalf <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">in a 2010 NPR interview</a>. Metcalf was the author of "<a href="https://www.amazon.com/OK-Improbable-Story-Americas-Greatest/dp/0199892539">OK: The Improbable Story of America's Greatest Word</a>." Van Buren lost badly, but OK definitely won.</p>

		
	
					
				
				


	
		<p>After 1840, the word spread like wildfire and never looked back. Originally, OK appeared in telegraph messages (which may account for its international spread) and documents but not in everyday speech as it was "slangy." But that changed over time. </p>

		
	
					
				
				


	
		<p><a href="https://www.bbc.com/news/magazine-12503686">In an article for BBC Magazine</a>, Metcalf speculated as to why OK was popular all over the world: "It's not that it was needed to 'fill a gap' in any language. Before 1839, English speakers had 'yes,' 'good,' 'fine,' 'excellent,' 'satisfactory' and 'all right.' What OK provided that the others did not was neutrality, a way to affirm or to express agreement without having to offer an opinion. ... OK allows us to view a situation in simplest terms, just OK or not."</p>

		
	
					
				
				
	
					

				

		
	




</div>

			</div>





				</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's always TCP_NODELAY (762 pts)]]></title>
            <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
            <guid>40310896</guid>
            <pubDate>Thu, 09 May 2024 17:54:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2024/05/09/nagle.html">https://brooker.co.za/blog/2024/05/09/nagle.html</a>, See on <a href="https://news.ycombinator.com/item?id=40310896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>It's not the 1980s anymore, thankfully.</p>

<p>The first thing I check when debugging latency issues in distributed systems is whether <a href="https://linux.die.net/man/7/tcp">TCP_NODELAY</a> is enabled. And it’s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.</p>

<p>First, let’s be clear about what we’re talking about. There’s no better source than John Nagle’s <a href="https://datatracker.ietf.org/doc/html/rfc896">RFC896</a> from 1984<sup><a href="#foot1">1</a></sup>. First, the problem statement:</p>

<blockquote>
  <p>There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.</p>
</blockquote>

<p>In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many <code>write</code> calls. Nagle’s proposal for fixing this was simple and smart:</p>

<blockquote>
  <p>A  simple and elegant solution has been discovered.</p>
</blockquote>

<blockquote>
  <p>The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.</p>
</blockquote>

<p>When many people talk about Nagle’s algorithm, they talk about timers, but RFC896 doesn’t use any kind of timer other than the round-trip time on the network.</p>

<p><em>Nagle’s Algorithm and Delayed Acks</em></p>

<p>Nagle’s nice, clean, proposal interacted poorly with another TCP feature: delayed <code>ACK</code>. The idea behind delayed <code>ACK</code> is to delay sending the acknowledgement of a packet at least until there’s some data to send back (e.g. a <code>telnet</code> session echoing back the user’s typing), or until a timer expires. <a href="https://datatracker.ietf.org/doc/html/rfc813">RFC813</a> from 1982 is that first that seems to propose delaying <code>ACKs</code>:</p>

<blockquote>
  <p>The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.</p>
</blockquote>

<p>which is then formalized further in <a href="https://datatracker.ietf.org/doc/html/rfc1122">RFC1122</a> from 1989. The interaction between these two features causes a problem: Nagle’s algorithm is blocking sending more data until an <code>ACK</code> is received, but delayed ack is delaying that <code>ack</code> until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.</p>

<p>This is a point Nagle has made himself several times. For example in this <a href="https://news.ycombinator.com/item?id=10608356">Hacker News comment</a>:</p>

<blockquote>
  <p>That still irks me. The real problem is not tinygram prevention. It’s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.</p>
</blockquote>

<p>As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.</p>

<p><em>Is Nagle blameless?</em></p>

<p>Unfortunately, it’s not just delayed ACK. Even without delayed ack and that <em>stupid fixed timer</em>, the behavior of Nagle’s algorithm probably isn’t what we want in distributed systems. A single in-datacenter RTT is typically around 500μs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win.</p>

<p>To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don’t. Partially that’s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.</p>

<p>The core concern of not sending tiny messages is still a very real one, but we’ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn’t going to be very efficient, no matter what Nagle’s algorithm does.</p>

<p><em>Is Nagle needed?</em></p>

<p>First, the uncontroversial take: if you’re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable <code>TCP_NODELAY</code> (disable Nagle’s algorithm) without worries. You don’t need to feel bad. It’s not a sin. It’s OK. Just go ahead.</p>

<p>More controversially, I suspect that Nagle’s algorithm just isn’t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, <code>TCP_NODELAY</code> should be the default. That’s going to make some “<code>write</code> every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.</p>

<p><em>Footnotes</em></p>

<ol>
  <li><a name="foot1"></a> I won’t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: “This condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.”</li>
</ol>


</div></div>]]></description>
        </item>
    </channel>
</rss>