<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 01 Dec 2024 01:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Ntfs2btrfs does in-place conversion of NTFS filesystem to the open-source Btrfs (176 pts)]]></title>
            <link>https://github.com/maharmstone/ntfs2btrfs</link>
            <guid>42283950</guid>
            <pubDate>Sat, 30 Nov 2024 20:50:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/maharmstone/ntfs2btrfs">https://github.com/maharmstone/ntfs2btrfs</a>, See on <a href="https://news.ycombinator.com/item?id=42283950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Ntfs2btrfs</h2><a id="user-content-ntfs2btrfs" aria-label="Permalink: Ntfs2btrfs" href="#ntfs2btrfs"></a></p>
<p dir="auto">Ntfs2btrfs is a tool which does in-place conversion of Microsoft's NTFS
filesystem to the open-source filesystem Btrfs, much as <code>btrfs-convert</code>
does for ext2. The original image is saved as a reflink copy at
<code>image/ntfs.img</code>, and if you want to keep the conversion you can delete
this to free up space.</p>
<p dir="auto">Although I believe this tool to be stable, please note that I take no
responsibility if something goes awry!</p>
<p dir="auto">You're probably also interested in <a href="https://github.com/maharmstone/btrfs">WinBtrfs</a>,
which is a Btrfs filesystem driver for Windows.</p>
<p dir="auto">Thanks to <a href="https://github.com/ebiggers">Eric Biggers</a>, who <a href="https://github.com/ebiggers/ntfs-3g-system-compression/">successfully reverse-engineered</a> Windows 10's
"WOF compressed data", and whose code I've used here.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">On Windows, from an Administrator command prompt:</p>
<p dir="auto"><code>ntfs2btrfs.exe D:\</code></p>
<p dir="auto">Bear in mind that it won't work with your boot drive or a drive containing a
pagefile that's currently in use.</p>
<p dir="auto">If you are using WinBtrfs, you will need to clear the readonly flag on the
<code>image</code> subvolume before you can delete it.</p>
<p dir="auto">On Linux, as root:</p>
<p dir="auto"><code>ntfs2btrfs /dev/sda1</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">On Windows, go to the <a href="https://github.com/maharmstone/ntfs2btrfs/releases">Releases page</a> and
download the latest Zip file, or use <a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/ntfs2btrfs.json">Scoop</a>.</p>
<p dir="auto">For Linux:</p>
<ul dir="auto">
<li><a href="https://aur.archlinux.org/packages/ntfs2btrfs" rel="nofollow">Arch</a></li>
<li><a href="https://src.fedoraproject.org/rpms/ntfs2btrfs" rel="nofollow">Fedora</a> (thanks to <a href="https://github.com/Conan-Kudo">Conan-Kudo</a>)</li>
<li>Gentoo - available as sys-fs/ntfs2btrfs in the guru repository</li>
<li><a href="https://packages.debian.org/ntfs2btrfs" rel="nofollow">Debian</a> (thanks to <a href="https://github.com/alexmyczko">alexmyczko</a>)</li>
<li><a href="https://packages.ubuntu.com/ntfs2btrfs" rel="nofollow">Ubuntu</a> (thanks to <a href="https://github.com/alexmyczko">alexmyczko</a>)</li>
<li><a href="https://build.opensuse.org/package/show/filesystems/ntfs2btrfs" rel="nofollow">openSUSE</a> (thanks to David Sterba)</li>
</ul>
<p dir="auto">For other distributions or operating systems, you will need to compile it yourself - see
below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Changelog</h2><a id="user-content-changelog" aria-label="Permalink: Changelog" href="#changelog"></a></p>
<ul dir="auto">
<li>
<p dir="auto">20240115</p>
<ul dir="auto">
<li>Fixed compilation on GCC 14 (<code>-Werror=incompatible-pointer-types</code> now enabled by default)</li>
</ul>
</li>
<li>
<p dir="auto">20230501</p>
<ul dir="auto">
<li>Fixed inline extent items being written out of order (not diagnosed by <code>btrfs check</code>)</li>
<li>Fixed metadata items being written with wrong level value (not diagnosed by <code>btrfs check</code>)</li>
<li>ADSes with overly-long names now get skipped</li>
</ul>
</li>
<li>
<p dir="auto">20220812</p>
<ul dir="auto">
<li>Added --no-datasum option, to skip calculating checksums</li>
<li>LXSS / WSL metadata is now preserved</li>
<li>Fixed lowercase drive letters not being recognized</li>
<li>Fixed crash due to iterator invalidation (thanks to nyanpasu64)</li>
<li>Fixed corruption when NTFS places file in last megabyte of disk</li>
</ul>
</li>
<li>
<p dir="auto">20210923</p>
<ul dir="auto">
<li>Added (Btrfs) compression support (zlib, lzo, and zstd)</li>
<li>Added support for other hash algorithms: xxhash, sha256, and blake2</li>
<li>Added support for rolling back to NTFS</li>
<li>Added support for NT4-style security descriptors</li>
<li>Increased conversion speed for volume with many inodes</li>
<li>Fixed bug when fragmented file was in superblock location</li>
<li>Fixed buffer overflow when reading security descriptors</li>
<li>Fixed bug where filesystems would be corrupted in a way that <code>btrfs check</code> doesn't pick up</li>
</ul>
</li>
<li>
<p dir="auto">20210523</p>
<ul dir="auto">
<li>Improved handling of large compressed files</li>
</ul>
</li>
<li>
<p dir="auto">20210402 (source code only release)</p>
<ul dir="auto">
<li>Fixes for compilation on non-amd64 architectures</li>
</ul>
</li>
<li>
<p dir="auto">20210105</p>
<ul dir="auto">
<li>Added support for NTFS compression</li>
<li>Added support for "WOF compressed data"</li>
<li>Fixed problems caused by sparse files</li>
<li>Miscellaneous bug fixes</li>
</ul>
</li>
<li>
<p dir="auto">20201108</p>
<ul dir="auto">
<li>Improved error handling</li>
<li>Added better message if NTFS is corrupted or unclean</li>
<li>Better handling of relocations</li>
</ul>
</li>
<li>
<p dir="auto">20200330</p>
<ul dir="auto">
<li>Initial release</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compilation</h2><a id="user-content-compilation" aria-label="Permalink: Compilation" href="#compilation"></a></p>
<p dir="auto">On Windows, open the source directory in a recent version of MSVC, right-click
on CMakeLists.txt, and click Compile.</p>
<p dir="auto">On Linux:</p>
<div data-snippet-clipboard-copy-content="mkdir build
cd build
cmake ..
make"><pre><code>mkdir build
cd build
cmake ..
make
</code></pre></div>
<p dir="auto">You'll also need <a href="https://github.com/fmtlib/fmt">libfmt</a> installed - it should be
in your package manager.</p>
<p dir="auto">Compression support requires zlib, lzo, and/or zstd - again, they will be in your
package manager. See also the cmake options WITH_ZLIB, WITH_LZO, and WITH_ZSTD,
if you want to disable this.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What works</h2><a id="user-content-what-works" aria-label="Permalink: What works" href="#what-works"></a></p>
<ul dir="auto">
<li>Files</li>
<li>Directories</li>
<li>Symlinks</li>
<li>Other reparse points</li>
<li>Security descriptors</li>
<li>Alternate data streams</li>
<li>DOS attributes (hidden, system, etc.)</li>
<li>Rollback to original NTFS image</li>
<li>Preservation of LXSS metadata</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What doesn't work</h2><a id="user-content-what-doesnt-work" aria-label="Permalink: What doesn't work" href="#what-doesnt-work"></a></p>
<ul dir="auto">
<li>Windows' old extended attributes (you're not using these)</li>
<li>Large (i.e &gt;16KB) ADSes (you're not using these either)</li>
<li>Preservation of the case-sensitivity flag</li>
<li>Unusual cluster sizes (i.e. not 4 KB)</li>
<li>Encrypted files</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Can I boot Windows from Btrfs with this?</h2><a id="user-content-can-i-boot-windows-from-btrfs-with-this" aria-label="Permalink: Can I boot Windows from Btrfs with this?" href="#can-i-boot-windows-from-btrfs-with-this"></a></p>
<p dir="auto">Yes, if the stars are right. See <a href="https://github.com/maharmstone/quibble">Quibble</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD Disables Zen 4's Loop Buffer (116 pts)]]></title>
            <link>https://chipsandcheese.com/p/amd-disables-zen-4s-loop-buffer</link>
            <guid>42283933</guid>
            <pubDate>Sat, 30 Nov 2024 20:47:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/amd-disables-zen-4s-loop-buffer">https://chipsandcheese.com/p/amd-disables-zen-4s-loop-buffer</a>, See on <a href="https://news.ycombinator.com/item?id=42283933">Hacker News</a></p>
Couldn't get https://chipsandcheese.com/p/amd-disables-zen-4s-loop-buffer: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[You must read at least one book to ride (157 pts)]]></title>
            <link>https://ludic.mataroa.blog/blog/you-must-read-at-least-one-book-to-ride/</link>
            <guid>42282717</guid>
            <pubDate>Sat, 30 Nov 2024 17:19:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ludic.mataroa.blog/blog/you-must-read-at-least-one-book-to-ride/">https://ludic.mataroa.blog/blog/you-must-read-at-least-one-book-to-ride/</a>, See on <a href="https://news.ycombinator.com/item?id=42282717">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
        

        

        <div itemprop="articleBody">
            <p>Two things are true.</p>
<p>The first is going to sound bad in a culture where engineers are encouraged to apologize for existing while it is totally acceptable for a grifter that can't code to insist that they're a "thought leader". It is that, in my immediate professional environment, I am consistently acknowledged to be one of the best engineers around. As a rough rule, I study more than the average engineer around me by up to two orders of magnitude. I've been made an offer at the senior level at one of the best companies in the state, various Serious People happily re-employ me, and become incensed when I see lazy commit messages. I'm doin' pretty good.</p>
<p>The second, and this one is going to sound bad because I sometimes need to convince people to employ me, is that I am <em>clearly worse</em> than almost everyone that emails me along all of these dimensions. I only have a dim understanding of how my 3-4 years of experience coming from a strong background in <em>psychology</em> has rounded to "senior engineer", I've only ever written tests for personal projects because no employer I've ever seen actually had <em>any</em> working tests or interest in getting them, and I wrote the entirety of my Master's thesis code without version control because one of the best universities in the country doesn't teach it. In short, I've never solved a <em>truly</em> hard problem, I'm just out here clicking the "save half a million dollar" button that no one else noticed. I'm a fucking moron.</p>
<p>I know the second is true because I see how truly complex some of the things people do for a living are, and I know the first is true because I have been effortlessly <em>cruising</em> my way to the top 3-4% of the country's income bracket as an immigrant. <em>How are they both true?</em></p>
<h2 id="i">I.</h2>
<p>Throughout high school, I was a truly horrendous art student. It was my least favourite class, as I had recently moved to an international school, and the local Malaysian syllabus didn't waste time on things like "creativity", or "culture", or really anything other than maths and language. I'd turn up, I'd not really understand how the other students from Western backgrounds were translating mental images into drawings so effortlessly, and I'd score the lowest possible passing grade. Eventually I decided that I "wasn't artsy" and didn't draw anything other than doodled cubes for about ten years.</p>
<p>A great deal of life experience later, I decided that it simply couldn't be that hard, and it would be an amazing accomplishment to get through this barrier. In 2022, I tried a course called <a href="https://drawabox.com/">Drawabox</a>, which was incredibly boring and led to no progress. On a whim, I decided to look on <em>Hackernews</em>, a place that at the time I thought of as exclusively for technical articles, for a good drawing tutorial, whereas previously I had allowed myself to be guided onto the rocky shoals of Reddit. They are actually dumber on there, sorry Reddit.</p>
<p>This led me to Betty Edward's <a href="https://www.drawright.com/">Drawing On The Right Side Of The Brain</a>, a book whose title profoundly upsets me as someone that left psychology due to the field's horrendous epistemology. The book came with glowing recommendations, but it also came with <a href="https://www.drawright.com/before-after/marks-before-and-after-selfportrait-drawings">before and after</a> pictures that I assumed were too good to be true. The <em>after</em> picture felt so unattainable that saying you could get to that level felt like a weight loss scam. Still, I was determined, and my intuition said the testimonials were real despite all of that, so I gave it a go.</p>
<p>The first thing the book asks you to do is draw your hand as best as you can, so that you've got a frame of reference. I sat still for 30-45 minutes and poured my heart and soul into it. This is the result I got.</p>
<p><img alt="A malformed drawing of a left hand, with too-long fingers and a bulbous thumb." src="https://ludic.mataroa.blog/images/73ba7ae7.jpeg"></p>
<p>This was <em>the best drawing I had ever made in my life</em> at the time, and it clearly leaves a lot to be desired. The book continues by providing some theory and exercises. The first exercise after this was to trace the lines in a drawing by eye, and if I recall, you are asked to trace it upside-down.</p>
<p><img alt="Upside_Down_Man.jpeg" src="https://ludic.mataroa.blog/images/97b580f4.jpeg"></p>
<p>This was actually more-or-less exactly what the drawing was. At this point, I have done exactly two drawings, with no unshown practice of what I assumed was the vital mechanical skill of drawing, but decide to trace two more (from the internet this time) while dogsitting at a friend's house. You will note that these are both strongly nerdy fantasy-themed, because at the end of the day, I am very much a stereotypical programmer.</p>
<p><img alt="Knight_Drawing.jpeg" src="https://ludic.mataroa.blog/images/ae262928.jpeg"></p>
<p><img alt="Dragon_Drawing.jpeg" src="https://ludic.mataroa.blog/images/83f658e8.jpeg"></p>
<p>Now <em>these</em> are the best drawings of my life, <em>again</em> with no unseen practice. They're a bit unclean and I made the initial components <em>way too big</em> so they both don't fit on the page. But at this point, as simple as these look, I am flabbergasted at the results I've gotten. I've lost 100 KGs with this One Simple Trick, doctors hate me (but this is because I didn't go into medicine like my parents wanted).</p>
<p>I have no idea if this newfound skill carries over when I'm not tracing lines, and instead drawing from life. I read a chapter or two more, and finally, with great trepidation, I take a stab at drawing my hand again. And is thus that, in about six hours of reading and practice spread over months, I moved from this:</p>
<p><img alt="A malformed drawing of a left hand, with too-long fingers and a bulbous thumb." src="https://ludic.mataroa.blog/images/73ba7ae7.jpeg"></p>
<p>To this:</p>
<p><img alt="A pretty good hand drawn from a foreshortened perspective!" src="https://ludic.mataroa.blog/images/d02dbded.jpeg"></p>
<p><img alt="A hand making a finger gun!" src="https://ludic.mataroa.blog/images/b4dada97.jpeg"></p>
<p>I was prepared to go <em>my entire life</em> missing out on the joys of art until I picked up the right book. I am still typically the worst artist when I enter a room full of artists, but people <em>outside those rooms</em> (who didn't read one book) occasionally think I'm pretty okay, which is something I never expected to experience.</p>
<h2 id="ii-the-one-book-barrier">II. The One Book Barrier</h2>
<p>With this in mind, there seem to be two major clumps of engineers.</p>
<p>There are the engineers who have read 1+ books on a given topic, and sometimes on <em>several</em> topics, and they all come off as extremely competent. These are, for the most part, the people that make up the audience of this blog. Of course, you do not <em>literally</em> need to read a book - a sufficiently high volume of technical blogs or courses are probably the equivalent at varying levels of efficiency - but take it for granted there is some Large Sum of information that someone has studied.</p>
<p>Then there are engineers (and people in every profession) who <em>never try for the entirety of their careers</em>, and this is the majority of every profession. I spoke to a reader who is employed as an extremely high-level engineer, Seth Newman - by the way, I'll connect you if you want to hire a genius data engineer in the U.S - who described the average professional as sleepwalking through their working lives, and this rung true. Of course, they are not <em>literally</em> asleep so something else is going wrong, but the description still seemed apt. There is movement - enough movement to fall down a flight of stairs - but none of the awareness needed to avoid such an outcome.</p>
<p>Then there are people like myself, where I have read <em>exactly</em> one good book on most topics relevant to my profession, but have never gone particularly deep. I have a firm grasp of the Git data model through Pro Git, for example, but know nothing about the algorithms underlying it - which is still enough to dominate a truly randomly selected engineer. I've read enough of the arcane tomes that I can cast a cantrip and it looks like magic to the sleepwalking, but I look at a project like <a href="https://github.com/evennia/evennia">Evennia</a> and realize that some people are out there casting Quickened Silent Still Maximized Disintegrate, which is <em>extremely impressive</em> for the non-D&amp;D nerds out there. I have spoken to high-performers in all sorts if fields at this point, and <em>every single one</em> has said that their profession is plagued by people that are essentially not even trying. I'm not sure how this is possible in things like medicine - have you seen how hard they have to work to pass those exams? - but it is nonetheless true.</p>
<h2 id="iii-our-endless-numbered-levels">III. Our Endless, Numbered Levels</h2>
<p>In the same conversation with Seth, I asked about what allowed him to work on the type of problem he was involved in, while I'm out here just trying to convince people to stop applying "flexible" schemas to every problem as if MongoDB has taken their kids hostage. This led to the topic of how deep specializations can go, and just how goddamn good people can get. I know very little about basketball, but Seth linked me to <a href="https://www.youtube.com/watch?v=i93vF0WOX6w">this video</a> where one of the worst players in the NBA, ten years post-retirement and out-of-shape, assassinates both amateur and professional athletes from lesser tiers. And he's <em>one of the worst players</em>, or so I've been told by people that actually know basketball.</p>
<p>But I can give a good example from my own experience too.</p>
<p>Here in Melbourne, I was considered a decent sabre fencer. I cleaned up most other amateurs, but there were always a couple of people around that would just wipe me out then go compete for the State championship. In particular, I've mentioned in <a href="https://ludic.mataroa.blog/blog/fury-driven-development/">earlier writing</a> that I trained with a guy that eventually won Nationals here.</p>
<p>That guy once fenced someone trying to qualify for the Olympics, and was unable to score a <em>single</em> point on him. So we know that someone <em>struggling</em> to qualify for the Olympics is way better than the best fencer I know, and arguably the best in the country.</p>
<p>Malaysia entered one fencer into the 2012 Olympics, Yu Peng Kean, who was eliminated <a href="https://en.wikipedia.org/wiki/Fencing_at_the_2012_Summer_Olympics_%E2%80%93_Men%27s_sabre"><em>15 to 1</em></a> by the guy that actually won that year. There are fencers that seriously train that can barely score a point on me. I can barely score against the top guy in Australia. That guy can barely score against someone trying to make the Olympics, and <em>that</em> guy can probably barely score against the guy that actually won the whole thing.</p>
<p>This is actually hardly a surprising result. One only needs to look at Magnus Carlsen's absolute domination of chess players that have trained their whole life, but it <em>feels</em> different when you're actually going up against it yourself. Competing against the top fencer in Australia feels <em>oppressive</em>. He is always effortlessly slightly out of reach, slightly faster, slightly more precise, and you're left feeling like a toddler trying to tackle an adult. And because I <em>do</em> have that level of edge over some people, untrained as they are, I know that's <em>exactly</em> what it's like from his perspective.</p>
<p>And there are two steps <em>at least</em> that large <em>after</em> you get to the National champion level? <em>Jesus Christ</em>.</p>
<h2 id="iv-incentives">IV. Incentives</h2>
<p>This is an aside, but there are many areas where I sleepwalk. I have mentioned many times that I am <em>not</em> making good progress on my piano lessons, and while it is true that I feel like I lack some natural talent in the area, it is also true that I simply don't practice enough.</p>
<p>However, I don't play the piano professionally and no one would pay me to do so. I can't help but feel that something has gone dreadfully wrong in society in that we've decided to start incentivizing people with no talent or interest to participate in the technology space. Many of them are extremely wide awake in other areas, be it sports, art, mathematics, whatever. Unfortunately for complex reasons related to some sectors having more money than they know what to do with, large organizations being impossibly hard to run well, and talking magazines covers trying to launder company funds into personal status, we're willing to pay people to be extremely bad programmers and extremely bad leaders. In fact, calling yourself a PowerBI developer is probably the single easiest way in the world to slack off 6+ hours a day in air-conditioned rooms at <em>well</em> above an average wage.</p>
<p>But since they're producing nothing <em>anyway</em>, or are a net negative to society accounting for opportunity cost, I can't help but feel that they'd somehow be so much more fulfilled if they got the sack somehow and we provided them all that money to just like, take care of their children and stop fucking around with databases they keep damaging. Of course, that likely isn't actually feasible (especially at executive compensation) but I can't stop thinking of this quote from the late Christopher Hitchens:</p>
<blockquote>
<p><a href="https://youtu.be/NegtQIkhz6g?si=O_5rhL0_cG9VvuD0&amp;t=24">My worst job was my first job.</a> It was very important to me, because in those days you had to be a member of the union to get a job in Fleet Street, in London journalism. And I couldn't get into the union so I couldn't get a job. And if you couldn't get a job then you couldn't get a union card. But finally there was a new job created - for a new magazine - I can still pronounce its horrible name, "The London Times Higher Education Supplement". And because it was a new job I didn't need to join the union, it was a new creation, so I got job and union card, and I was so proud, and so happy. I was the social science editor... and I cannot tell you how boring it was and how bad I was at it. And how I would literally watch the clock as the long day crept by. But it was a job, and it brought me to London, and it made me a member of the National Union of Journalists. And it was everything I wanted and it made me miserable. Finally, I was so bad at it that it could no longer be concealed from the editor. I got the sack, very traditionally in Britain, I got the sack on Christmas Eve. I was very depressed, I thought "how will I ever recover", "I'll never get another job", "maybe I can't even keep my union card". But I still sometimes think about it, and think "If I had been any good at that job, I might still be doing it."</p>
</blockquote>
<h2 id="v">V.</h2>
<p>Something about having your skin in the game is that your relationship with sleepwalkers changes. That is, instead of resenting them, you're <em>glad that they're so bad</em> because they give you people to beat without having to dedicate your entire life to perfect performance.</p>
<p>There's some great reading on a similar concept in video games by Dan Luu, on how it takes <a href="https://danluu.com/p95-skill/">very little effort</a> to be a high performer. To be a high performer either means that you can do some fairly unambiguous task (like a backflip) <em>or</em> it can mean that you're performing well relative to other people.</p>
<p>Reading one book <em>typically</em> gets you to the point where you can do things like "add new functionality to this React app without incurring technical debt". This is good enough to make an ethical living, so long as you intelligently chose to do a thing that society pays for - sorry if you didn't, I'd build society differently if I was writing the rules. Reading multiple books is how you get good enough to compete for the absolute highest-paid jobs out there, where the fact you can do the above in a day instead of a week matters, and in the latter case you're simply going to have to read N+1 books, where N is whatever your competition is reading. It's an arms race.</p>
<p>But because Deloitte and the average developer don't read <em>any</em> books, they're just easy pickings. When you're trading off all your skin-in-the-game and working a permanent job, your interactions with these people leave you feeling helpless. You have to work with whoever the boss tells you too, and if they suck then you just have to deal with that.</p>
<p>If you're a bit more mercenary, then you get to turn up to interviews/meetings and dunk on them all day, and still have time to work out and be lazy. The last time I interviewed at a serious company, I was informed that I obliterated the competition so thoroughly that I "turned the planet into glass from orbit", and <em>I was still a weaker engineer than any of the seniors already at that company</em>. There are some jobs where I can't compete with the empty-suited masses, like perhaps government contracts, but that's because those industries aren't in the business of buying what I'm selling. I'm selling results and trust. Big enterprise is buying plausible lies that get executives promoted. I'll beat big consultants <em>every</em> time with the kind of person I actually want to work with.</p>
<p>Heck, you could probably eliminate almost all dud candidates during technical interviews by asking what their favourite tech book is, then only talking to candidates where you know enough to check if they've read that book. You'd have a false negative on anyone that read a great book you didn't know about, but I'm guessing almost no false positives.</p>
<h2 id="vi">VI.</h2>
<p>But now we get to a complicated case - what about the people that try very hard but nonetheless get no results? I know several people that run teams incompetently but are <em>genuinely trying</em>. However, no amount of training seems to get them there. They harass engineers, they panic, they don't know how to hire, they usually overestimate their own skill, but they're <em>trying</em>. They just <em>keep</em> trying to get Scrum right. They're sleepwalking straight into a lake, and all we can see are the flailing arms of a drowning person.</p>
<p>There are all sorts of things that cause this outcome, but I wanted to highlight a meta-skill that many of us have but can't quite articulate, and I suspect it's what they're missing. That is, how do you know what books to read?</p>
<p>For example, I started with <em>Drawabox</em> which was plausible on its face, but found that Betty Edward's book was substantially better for my goals. In fact, not only was it better, Drawabox simply assumed that I knew the One Weird Trick that Edwards highlighted, and moved straight on to mechanical skill - it is entirely possible I could have gotten through the whole course and had no appreciable progress with that missing puzzle piece. There were actually quite a few resources available, but I had some inkling I was on the wrong track. How do we have people that are basically doing the same thing with Scrum and not thinking "Wait, this is <em>not</em> working, what am I doing wrong? Is this even remotely the right course?"</p>
<p>I don't think I can explain <em>exactly</em> how I do it, or if there's a precise algorithm. It certainly seems like you need a little bit of bullshit detection, so in the tech space I'll typically only read something from a person who has built something impressive themselves. Big points for maintaining something open source or otherwise having some indicator of knowledge that is impossible to fake. If they've built something more nebulous, like "a big company", they score far fewer points because you can luck out or bullshit your way there. You can't lie your way into code compiling.</p>
<p>There are some other weird rules too, none of them hard and fast. Catchy names tend to result in getting dinged for a bit. The best books seem to have either boring or sophisticated covers. Anything with the word "leadership" in it is probably nonsense. The more the author brags about their awards, the more I think they're lying. A colloquial tone (like mine, sadly) tends to lose a few points, though it isn't a knockdown if the topics are nuanced. Thinking about this now, these are all things that seem extremely convincing to the drowning sleepwalkers.</p>
<p>Management at my day job brought us an Agile consultant the other day, who they loved, but they asked the engineers "How highly we'd rate our Agile training on a scale of 1 - 5" when that questions doesn't even mean anything. Some people <em>loved it</em>, and there's almost a 100% correlation with their love of that session and their reading of the wrong books.</p>
<p>Even <em>within</em> a book, you have to be able to discard large segments that don't have merit. The Phoenix Project is a good example: Gene Kim has written a whole book about organizational transformation, but he only put <em>one</em> bad actor into the whole book then made everyone else super competent and earnest. That's <em>not what big companies are like</em>, so you need to have enough of your brain switched on to get the good ideas then realize the rest of it is cringe-worthy leadership fanfic.</p>
<p>Not having whatever that ability is seems to cripple many people's ability to learn and better themselves. This is why it's so concerning when you hear some executives disclose their reading material - it is typically <em>immediately obvious</em> that there is almost no way they are actually good at their job. It is similarly why my eyes roll back into my skull when a leader tells me they do their learning on LinkedIn - like, what the fuck are you sickos on about? Have vendors been causing you brain damage to make sales easier?</p>
<h2 id="vii">VII.</h2>
<p>I was going to say that this was a post about how we should all take a moment to reflect on the unreasonable effectiveness of cracking open a book in the age of YouTube, but I guess what I actually want to say is <em>do not under any circumstances encourage people to read books</em>, it's easy money for the rest of us. My next blog post is going to be filled with anti-Git propaganda and links to Scrum resources. Godspeed, you glorious bastards.</p>
        </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How honeycrisp apples went from marvel to mediocre (221 pts)]]></title>
            <link>https://www.seriouseats.com/how-honeycrisp-apples-went-from-marvel-to-mediocre-8753117</link>
            <guid>42282476</guid>
            <pubDate>Sat, 30 Nov 2024 16:44:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seriouseats.com/how-honeycrisp-apples-went-from-marvel-to-mediocre-8753117">https://www.seriouseats.com/how-honeycrisp-apples-went-from-marvel-to-mediocre-8753117</a>, See on <a href="https://news.ycombinator.com/item?id=42282476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mntl-sc-page_1-0" data-sc-sticky-offset="100" data-sc-ad-label-height="11" data-sc-ad-track-spacing="100" data-sc-min-track-height="250" data-sc-max-track-height="600" data-sc-breakpoint="50em" data-sc-load-immediate="5" data-sc-content-positions="[1, 1, 1, 1250, 1, 1, 1, 1]" data-bind-scroll-on-start="true"><p id="mntl-sc-block_1-0"> It was a chilly Saturday morning in October, and at my local grocery store, shoppers were browsing the apple selection: piles of Gala, Pink Lady, Golden Delicious, Fuji, Snapdragon, and Honeycrisp beckoned. I lingered over the organic Honeycrisps, pausing to look at the $3.99-per-pound price tag, before filling my produce bag with several conventional Galas, which sold for a more reasonable $1.69 per pound. Though I had my heart set on the Honeycrisps, I’d recently had one too many bland, mealy ones with none of the fruit’s signature snap and sweet, tangy flavor, and I was unsure if I was ready to take that risk again, especially given the price.&nbsp;
</p>

<div id="mntl-sc-block_3-0"><p> It would have been an easier decision if Honeycrisps were as good today as they used to be. I first tasted one 10 years ago, standing at my mother-in-law’s kitchen counter in St. Louis on a cool September day. I grasped the rosy fruit she handed me and took a bite. The apple’s paper-thin skin produced an audible crunch, and a burst of sweet, tart juice immediately filled my mouth. I chewed carefully. I couldn’t recall the last time I ate an apple for pleasure, on its own—not in my hand as a grab-and-go breakfast as I rushed out of the house, not sliced up and slathered with <a href="https://www.seriouseats.com/creamy-peanut-butter-taste-test-8627519" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">nut butter</a>, and not peeled, cored, chopped, and baked into a <a href="https://www.seriouseats.com/pennsylvania-dutch-apple-pie-recipe-8399305" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="2">pie</a>. The Honeycrisp apple was revelatory for me: It was an apple that I truly enjoyed eating on its own.&nbsp;</p><p>And I did, for several years, until I noticed that the Honeycrisp apples I bought were, with increasing frequency, a miss. There were a few good ones here and there, but I often came across Honeycrisp apples that were dry and mealy. Beyond the hefty price tag, there was little to distinguish them from other standard apple varieties. <span></span>Honeycrisps from my farmers market were typically better than those I purchased from the grocery store, but even those Hudson Valley–grown apples weren’t immune. As recently as September of this year, I had several Honeycrisp apples from a local farm that were terribly mushy and flavorless, making me wonder if they had mistakenly labeled another apple variety—nothing about those apples was like the fruit I had once loved.&nbsp;</p><p>I’m not the only one who has noticed the fluctuation in quality. My colleagues <a href="https://www.seriouseats.com/daniel-gritzer-5118638" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="3">Daniel</a> and <a href="https://www.seriouseats.com/megan-o-steintrager-7371874" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="4">Megan</a> have both had their fair share of inferior Honeycrisps in the past couple of years. I also found multiple instances of people complaining about Honeycrisp quality on Reddit: Three years ago, <a href="https://www.reddit.com/r/NoStupidQuestions/comments/t16tq8/why_do_honeycrisp_apples_suck_now/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="5">a user wrote</a> that the Honeycrisp apples they bought were “unrecognizable from the big sweet apples from the late 00s and 10s.” Another user, <a href="https://www.reddit.com/r/rant/comments/19bcwve/what_the_hell_happened_to_honeycrisp_apples/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="6">who posted earlier this year</a>, mourned the loss of the “super sweet and crisp” apples they were able to find 10 years ago. The Honeycrisps of today, they wrote, are “bitter and barely sweet at all" and "On top of that they aren’t crisp either!”</p><p>What went wrong? <span></span>The answer is both simpler and more complex than you might think, and it’s impossible to answer that question without looking at how the Honeycrisp apple came about—and how it shot to stardom so quickly.
</p></div>

<figure id="mntl-sc-block_5-0"> 
<figcaption id="mntl-figure-caption_1-0"> <span><p>Getty Images / brizmaker</p></span>
</figcaption></figure>
<h2 id="mntl-sc-block_6-0"> <span> The Honeycrisp: Origins and Rise to Stardom </span> </h2>
<p id="mntl-sc-block_7-0"> In 1983, David Bedford, one of the seed breeders behind the Honeycrisp apple and a research scientist at the University of Minnesota, had his first taste of the fruit. Crisp and juicy with a pleasant tanginess, the apple was unlike any he’d had before. “It caused me some question,” he tells me, recalling the sensory shock he experienced. “I remember biting it and thinking, well, what’s going on here?”&nbsp;He describes picking up textural and flavor notes similar to Asian pears and watermelons, and trying to decide if the fruit was underripe or overripe. "I don't know if it was a moment or a day or a week that it took me to decide, I don't know what it is, but it's good." The tree, labeled MN1711, bore fruit that was a cross between the Keepsake apple and another experimental variety identified only as MN1627; the tree had failed a winter hardiness test, and the university’s apple breeding program had designated it for the compost heap. Bedford, however, decided to give the tree another chance. It paid off, because it yielded what has since become <a href="https://www.sos.state.mn.us/about-minnesota/state-symbols/state-fruit-honeycrisp-apple/#:~:text=The%20Honeycrisp%E2%84%A2%20apple%20%28Malus,tree%20with%20high%20quality%20fruit." data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">Minnesota’s state fruit</a> and one of the most popular apple varieties today.
</p>

<p id="mntl-sc-block_9-0"> Together with Dr. Jim Luby, the former director of the University of Minnesota’s fruit breeding program, Bedford worked on improving the hardiness, texture, and flavor of the apple—placing it in the university’s evaluation program and observing it under different conditions—until they thought it was good enough to release to the public in 1991. “We had convinced ourselves on the breeding team that this is good, but we had no idea really what the rest of the world was thinking,” Bedford says. “It became clear in time that the world—the consumers—really did like this texture.”&nbsp;
</p>

<p id="mntl-sc-block_11-0"> For much of the 1960s and 1970s, Bedford tells me, there seemed to be no interest beyond the Red Delicious, the one “nice big shiny red apple that you could have year-round.” The Red Delicious was the result of the industrialization of the food system: National grocery stores and distributors wanted durable, aesthetically pleasing fruit that could be transported and stored easily, taste be damned. The skin was thick and leathery like naugahyde, with sweet, insipid flesh.
</p>

<p id="mntl-sc-block_13-0"> When Grady Auvil, the founder of Washington-based fruit company Auvil Fruit, began importing Granny Smith apples from New Zealand to the United States in the 1970s, it was a refreshing break from the Red Delicious for American consumers. The Granny Smith paved the way for the Honeycrisp: Americans welcomed the green apple’s tart flavor and crunch, signaling to growers and retailers that consumers were ready for different kinds of apples. When Bedford and Luby introduced Honeycrisp seedlings to nurseries and farmers in 1991, “there was at least some open-mindedness,” says Bedford. “Consumers had been sort of awakened to this idea that there was more to apples than Red Delicious.”&nbsp;
</p>

<p id="mntl-sc-block_15-0"> The Honeycrisp apple redefined what an apple could be. It was different from any other apple most American shoppers had encountered before, especially for consumers who frequented conventional grocery stores rather than farmers markets, where tastier heirloom varieties could be found even during the heyday of the Red Delicious. Unlike many other apple varieties, the Honeycrisp apple, journalists Deena Shanker and Lydia Mulvany noted in <a href="https://www.bloomberg.com/news/articles/2018-11-08/the-curse-of-the-honeycrisp-apple" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">Bloomberg</a> in 2008, “wasn’t bred to grow, store, or ship well," Instead, "It was bred for taste: crisp, with balanced sweetness and acidity.” Earlier this year, <a href="https://www.scientificamerican.com/article/apples-have-never-tasted-so-delicious-heres-why/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="2">Bedford told <em>Scientific American</em></a> that you could separate the world of commercial apples into two phases: before Honeycrisp and after Honeycrisp. Before the variety’s debut, common grocery store apples were either soft and mealy or firm and dense. The Honeycrisp introduced the concept of a crisp apple to the public and, Bedford says, set a new bar for both customers and breeders—so much so that Bedford estimates, unofficially, that 50% of the new apple varieties coming onto the market today are Honeycrisp offspring.&nbsp;
</p>

<p id="mntl-sc-block_17-0"> This success is due to the fact that the Honeycrisp is—no exaggeration—built differently. It has a remarkably thin skin, and a crispness that is the result of the Honeycrisp having much larger cells than other apples. Apple cells contain vacuoles filled with juice; the cells are stacked on top of one another and held together by the lamella, or what Bedford describes as the “glue” that gives an apple its firm, crunchy texture. When you bite into an apple, your teeth cut through razor-thin skin and the layers of cells, fracturing the vacuoles of juice. It's these oversized cells that give the Honeycrisp its unique flavor and texture, making for a truly delicious apple with a crisp texture that people have come to crave.&nbsp;
</p>

<p id="mntl-sc-block_19-0"> Because the Honeycrisp was designed to thrive in Minnesota’s climate, Bedford and Luby made the apple available in the rest of the Midwest first, where growing conditions were fairly similar to those in the apple’s home state. Though nurseries began selling Honeycrisp cuttings in 1991, it took several years for the fruit to arrive at farmers markets and grocery stores in the Midwest. And when it did, it quickly became a word-of-mouth phenomenon.&nbsp;
</p>

<div id="mntl-sc-block_21-0"><p> People could not get enough. And unlike common apple varieties like the Red Delicious, Golden Delicious, or Granny Smith, the Honeycrisp wasn’t available to purchase year-round. Instead, it was only sold from September, when the apple was at its peak, to February. This scarcity drove up demand even more.</p><p>“People would go to their local apple orchard or to their supermarket because they had heard about [the Honeycrisp apple] in Minnesota or they tasted something,” Dr. Matthew Clark, the head of the University of Minnesota’s fruit breeding program, tells me. “Word got out, people were wanting it,” as the eating experience was “unlike any other.” Soon, growers were planting the Honeycrisp in New York and Washington.&nbsp;
</p></div>

<figure id="mntl-sc-block_23-0"> 
<figcaption id="mntl-figure-caption_2-0"> <span><p>Getty Images / Karolina Wojtasik/Bloomberg via Getty Images</p></span>
</figcaption></figure>
<h2 id="mntl-sc-block_24-0"> <span> Signs of Trouble </span> </h2>
<p id="mntl-sc-block_25-0"> The tree, however, proved difficult to grow, especially in Washington State, the heart of commercial apple production in the United States. (According to the US Apple Association, Washington is projected to produce 179 million bushels—about 63% of all the apples grown in the United States—in the 2024/2025 calendar year, making it the country’s top apple growing state.) “Really a variety cannot be successful unless it’s grown commercially in Washington,” Bedford says. “We sent trees out, they tested it, and I had more than one grower call me and say, ‘That’s the worst tree I’ve ever tried to grow here. I’m pulling all the trees out.’” Not only is the fruit a poor fit for the state’s climate, which is much warmer than Minnesota, but it’s also prone to several physiological and storage disorders, like bitter pit and soft scald, which can affect both the presentation and eating quality of the fruit when it’s stored for an extended period of time.&nbsp;
</p>

<p id="mntl-sc-block_27-0"> In order to ensure the health of the tree, it’s essential to thin or selectively remove parts of it, a labor-intensive process. “Even if you’ve done all that hand-thinning and invested a lot in the crop, you can lose a lot of it to [bitter pit],” Josh Morgenthau, the owner of Fishkill Farms in Fishkill, New York, says. “It’s very fickle.” Unfortunately, even when farmers apply all of the best practices for ensuring the quality of their Honeycrisp crop, bitter pit can continue to show up in storage, and Morgenthau estimates that about 20% of fruit that looks clean when picked is no longer sellable because bitter pit shows up after a few months.
</p>

<p id="mntl-sc-block_29-0"> The fruit’s extraordinarily thin skin may be pleasant for biting through, but it also means the apple is prone to sunburn, in which the parts of the apple that get more sun exposure experience what scientists call “<a href="https://fruit.wisc.edu/2024/08/01/understanding-sunburn-in-apples-causes-symptoms-and-management/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">tissue collapse</a>,” causing the fruit to turn brown or black. The delicate skin also makes it time-consuming to harvest: To prevent the apple’s sharp stems from puncturing neighboring apples in storage, the stems must be clipped extra-short. “Now, if you only had to do a couple hundred of those a day, no big deal,” Bedford muses. “But when you’re picking hundreds of thousands of these things, that slows down the picking process, which increases your costs.” (Dr. Kate Evans, the breeder at Washington State University who came up with the Cosmic Crisp apple, tells me that “something like 10 billion apples a year get picked by hand in the state of Washington.”)
</p>

<p id="mntl-sc-block_31-0"> Despite the challenges, growers in Washington—enticed by the profits the Honeycrisp could potentially bring and ignoring their initial bad experiences with it—eventually ended up planting acres and acres of Honeycrisp trees. As of 2017, the apple variety made up 13% of Washington’s apple acreage, making it the state’s <a href="https://wpcdn.web.wsu.edu/cahnrs/uploads/sites/5/2020/11/TB70E.pdf" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">fourth-largest cultivar</a> after Red Delicious, Gala, and Fuji. “Farmers don’t miss out on an opportunity for something new and exciting,” Clark says. “Growing apples has tight margins and Honeycrisp and other premium apples give growers an opportunity to make some money and increase those margins.” Given the perceived quality and popularity of Honeycrisps, the variety could sell for far more than many other kinds of apples, making it possible for farmers to make a good deal more money on their crop.&nbsp;
</p>

<p id="mntl-sc-block_33-0"> Then there’s the question of storage. Honeycrisp apples can spend up to seven months in common storage (which refers to a climate at 37ºF/2.7ºC) or 10-plus months in controlled atmosphere storage, a reduced oxygen environment near freezing conditions (typically 32ºF/0ºC) that slows down the respiration rate of apples and prevents further ripening. Dr. R. Karina Gallardo, an economics professor at Washington State University, tells me that the longer the storage time, the higher the probability of disorders—which means the more likely it is that consumers purchase a poor-tasting apple.&nbsp;
</p>

<p id="mntl-sc-block_35-0"> An apple, however, doesn’t have to be stored very long to develop less-than-ideal flavors and textures. Though Honeycrisps are considered a good storage apple, a fruit that “stores well” could mean many things: It may look perfectly good, but doesn’t guarantee it will still taste good. “An apple can be pretty soft and mealy in six months,” Bedford says. “There’s no magic time for all apples.” There are numerous factors that can impact the quality of an apple in storage—especially when it’s a fickle variety like Honeycrisp, which requires careful tending to at every stage of its life.&nbsp;
</p>

<p id="mntl-sc-block_37-0"> Many farmers who invested heavily in planting Honeycrisp trees likely did not take into account just how difficult it would be to grow, harvest, and store the apples. And maybe some just decided it was worth the risk. At its most expensive, at the peak of the Honeycrisp craze in 2012 and 2013, the apple fetched a hefty price nationwide, with <a href="https://www.esquire.com/food-drink/food/a20018/honeycrisp-price-explained/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1"><em>Esquire</em></a> reporting it at of $4.50 per pound in New York City.
</p>

<p id="mntl-sc-block_39-0"> To satiate the public’s hunger for the Honeycrisp, a once highly seasonal apple available only in Minnesota, growers have made the apple variety available year-round by planting enough fruit to store for long periods of time. Planting the Honeycrisp in Washington marked not only the shift of the apple from its place of origin—Minnesota—to a growing region it wasn’t well suited to, but was also a shift from a more small-scale, local apple industry to one that was geared towards Big Apple from the start. Growers in Washington never intended to sell their tidy little Honeycrisp crop at local markets during its short season—they wanted to supply the apples year-round, and in large enough quantities to stock supermarket shelves across the country in order to make some serious money.&nbsp;
</p>

<p id="mntl-sc-block_41-0"> The move to Washington facilitated the arrival of the Honeycrisp everywhere and made it possible for consumers to purchase the apple variety wherever and whenever they wanted. All the problems with the Honeycrisp became much more common once the apple was grown and distributed on such a large scale; as Cornell University pomology professor Ian Merwin told <a href="https://www.axios.com/local/twin-cities/2024/02/25/honeycrisp-apple-prices-fall-supply-quality-questions" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">Axios</a> reporter Nick Halter, “There is no question that the quality that’s in the market is not what it was 10 years ago.” Apples are spending longer than ever in storage, and “even with advances in refrigeration in technology, that further erodes their quality.”<br>
</p>

<figure id="mntl-sc-block_43-0"> 
<figcaption id="mntl-figure-caption_3-0"> <span><p>Getty Images / Chris Ratcliffe/Bloomberg via Getty Images</p></span>
</figcaption></figure>
<h2 id="mntl-sc-block_44-0"> <span> Where the Honeycrisp Stands Today </span> </h2>
<p id="mntl-sc-block_45-0"> Apple growers very possibly over-invested in the Honeycrisp crop without truly understanding that they likely couldn’t deliver a premium product year-round on such a large scale—especially with such a capricious variety grown outside its native zone. For many consumers, the Honeycrisp crop of today has not lived up to the apple’s reputation, and for the first time ever, there is an oversupply of Honeycrisp apples. With a <a href="https://www.linkedin.com/pulse/predictions-2024-honeycrisp-pricing-continue-slide-james-williams-ampsc/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">surplus that is 71% higher</a> than the five-year average, the national average for the cost of the apple is just $1.70 per pound. It is the cheapest the apple has ever been—and possibly the least satisfying and delicious it’s ever been.&nbsp;
</p>

<p id="mntl-sc-block_47-0"> As Bedford noted above, it is impossible for an apple variety to be “successful” unless it is grown in Washington. But what does success even mean? Turning the Honeycrisp into yet another commodity ultimately defeats the purpose of what Bedford and Luby were trying to achieve: a truly delicious apple with excellent eating quality. The Honeycrisp is a victim of its own success, and has become exactly what Bedford and Luby despised about the variety’s predecessors: a boring commodity apple.
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust-Query (181 pts)]]></title>
            <link>https://blog.lucasholten.com/rust-query-announcement/</link>
            <guid>42280570</guid>
            <pubDate>Sat, 30 Nov 2024 09:29:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.lucasholten.com/rust-query-announcement/">https://blog.lucasholten.com/rust-query-announcement/</a>, See on <a href="https://news.ycombinator.com/item?id=42280570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    
  <header>
    <p>
      <time datetime="2024-11-24">2024-11-24</time>
    
      <strong>Announcing rust-query</strong>
    </p>
      <ul role="list">
        <li role="listitem"><a href="https://blog.lucasholten.com/tags/database/">database</a></li>
      
        <li role="listitem"><a href="https://blog.lucasholten.com/tags/rust/">rust</a></li>
      </ul>
    </header>

    
  <h2 id="safe-relational-database-queries-using-the-rust-type-system">Safe relational database queries using the Rust type system</h2>
<p>Do you want to persist your data safely without migration issues and easily write complicated queries? All of this without writing a single line of SQL? If so, then <a href="https://github.com/LHolten/rust-query">I am making <code>rust-query</code></a> for you!</p>
<blockquote>
<p>This is my first blog post about <code>rust-query</code>, a project I've been working on for many months. I hope you like it!</p>
</blockquote>
<h3 id="rust-and-databases">Rust and Databases</h3>
<p>There is only one reason why I made this library and it is because I don't like the current options for interacting with a database from Rust. The existing libraries don't provide the compile time guarantees that I want and are verbose or awkward like SQL.</p>
<p>The reason I care so much is that databases are really cool. They solve a huge problem of making crash-resistant software with support for atomic transactions.</p>
<h2 id="structured-query-language-sql-is-a-protocol">Structured Query Language (SQL) is a protocol</h2>
<p>For those who don't know, SQL is <strong>the</strong> standard when it comes to interacting with databases. So much so that almost all databases only accept queries in some dialect of SQL.</p>
<p>My opinion is that SQL should be for computers to write. This would put it firmly in the same category as LLVM IR. The fact that it is human-readable is useful for debugging and testing, but I don't think it's how you want to write queries.</p>
<h2 id="introducing-rust-query">Introducing <code>rust-query</code></h2>
<p><a href="https://crates.io/crates/rust-query"><code>rust-query</code></a> is my answer to relational database queries in Rust. It's an opinionated library that deeply integrates with Rust's type system to make database operations feel Rust-native.</p>
<h2 id="key-features-and-design-decisions">Key Features and Design Decisions</h2>
<p>I could write a blog post about each one of these, but let's keep it short for now:</p>
<ul>
<li><strong>Explicit table aliasing</strong>: Joining a table gives back a dummy representing that table <code>let user = User::join(rows);</code>.</li>
<li><strong>Null safety</strong>: Optional values in queries have <code>Option</code> type, requiring special care to handle.</li>
<li><strong>Intuitive aggregates</strong>: Our aggregates are guaranteed to give a single result for every row they're joined on. After trying it, you'll see this is much more intuitive than traditional <code>GROUP BY</code> operations.</li>
<li><strong>Type-safe foreign key navigation</strong>: Database constraints are like type signatures, so you can rely on them for your queries with easy-to-use implicit joins by foreign key (e.g., <code>track.album().artist().name()</code>).</li>
<li><strong>Type-safe unique lookups</strong>: For example, you can get an <code>Option&lt;Rating&gt;</code> dummy with <code>Rating::unique(my_user, my_story)</code>.</li>
<li><strong>Multi-versioned schema</strong>: It's declarative and you can see the differences between all past versions of the schema at once!</li>
<li><strong>Type-safe migrations</strong>: Migrations have all the power of queries and can use arbitrary Rust code to process rows. Ever had to consult something outside the database for use in a migration? Now you can!</li>
<li><strong>Type-safe unique conflicts</strong>: Inserting and updating rows in tables with unique constraints results in specialized error types.</li>
<li><strong>Row references tied to transaction lifetime</strong>: Row references can only be used while the row is guaranteed to exist.</li>
<li><strong>Encapsulated typed row IDs</strong>: The actual row numbers are never exposed from the library API. Application logic should not need to know about them.</li>
</ul>
<h2 id="let-s-see-it">Let's see it!</h2>
<p>You always start by defining a schema. With <code>rust-query</code> it's easy to migrate to a different schema later.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>schema</span><span>]
</span><span>enum </span><span>Schema {
</span><span>    User {
</span><span>        name: </span><span>String</span><span>,
</span><span>    },
</span><span>    Story {
</span><span>        author: User,
</span><span>        title: </span><span>String</span><span>,
</span><span>        content: </span><span>String
</span><span>    },
</span><span>    #[</span><span>unique</span><span>(user, story)]
</span><span>    Rating {
</span><span>        user: User,
</span><span>        story: Story,
</span><span>        stars: </span><span>i64
</span><span>    },
</span><span>}
</span><span>use </span><span>v0::</span><span>*</span><span>;
</span></code></pre>
<p>Schema defintions in <code>rust-query</code> use enum syntax, but no actual enum is defined here.
This schema defines three tables with specified columns and relationships:</p>
<ul>
<li>Using another table name as a column type creates a foreign key constraint.</li>
<li>The <code>#[unique]</code> attribute creates named unique constraints.</li>
<li>The <code>#[schema]</code> macro parses the enum syntax and generates a module <code>v0</code> that contains the database API.</li>
</ul>
<h3 id="writing-queries">Writing Queries</h3>
<p>First, let's see how to insert some data into our schema:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>insert_data</span><span>(txn: </span><span>&amp;</span><span>mut </span><span>TransactionMut&lt;Schema&gt;) {
</span><span>    </span><span>// Insert users
</span><span>    </span><span>let</span><span> alice </span><span>=</span><span> txn.</span><span>insert</span><span>(User {
</span><span>        name: "</span><span>alice</span><span>",
</span><span>    });
</span><span>    </span><span>let</span><span> bob </span><span>=</span><span> txn.</span><span>insert</span><span>(User {
</span><span>        name: "</span><span>bob</span><span>",
</span><span>    });
</span><span>    
</span><span>    </span><span>// Insert a story
</span><span>    </span><span>let</span><span> dream </span><span>=</span><span> txn.</span><span>insert</span><span>(Story {
</span><span>        author: alice,
</span><span>        title: "</span><span>My crazy dream</span><span>",
</span><span>        content: "</span><span>A dinosaur and a bird...</span><span>",
</span><span>    });
</span><span>    
</span><span>    </span><span>// Insert a rating - note the try_insert due to the unique constraint
</span><span>    </span><span>let</span><span> rating </span><span>=</span><span> txn.</span><span>try_insert</span><span>(Rating {
</span><span>        user: bob,
</span><span>        story: dream,
</span><span>        stars: </span><span>5</span><span>,
</span><span>    }).</span><span>expect</span><span>("</span><span>no rating for this user and story exists yet</span><span>");
</span><span>}
</span></code></pre>
<p>A few important points about insertions:</p>
<ul>
<li>We need a mutable transaction (<code>TransactionMut</code>) to modify the database.</li>
<li>Insert operations return references to the newly inserted rows.</li>
<li>When inserting into tables with unique constraints, use <code>try_insert</code> to handle potential conflicts.</li>
<li>The error type of <code>try_insert</code> is based on how many unique constraints the table has.</li>
</ul>
<p>Now let's query this data:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>query_data</span><span>(txn: </span><span>&amp;</span><span>Transaction&lt;Schema&gt;) {
</span><span>    </span><span>let</span><span> results </span><span>=</span><span> txn.</span><span>query</span><span>(|rows| {
</span><span>        </span><span>let</span><span> story </span><span>= </span><span>Story::join(rows);
</span><span>        </span><span>let</span><span> avg_rating </span><span>= aggregate</span><span>(|rows| {
</span><span>            </span><span>let</span><span> rating </span><span>= </span><span>Rating::join(rows);
</span><span>            rows.</span><span>filter_on</span><span>(rating.</span><span>story</span><span>(), </span><span>&amp;</span><span>story);
</span><span>            rows.</span><span>avg</span><span>(rating.</span><span>stars</span><span>().</span><span>as_float</span><span>())
</span><span>        });
</span><span>        rows.</span><span>into_vec</span><span>((story.</span><span>title</span><span>(), avg_rating))
</span><span>    });
</span><span>
</span><span>    </span><span>for </span><span>(title, avg_rating) </span><span>in</span><span> results {
</span><span>        println!("</span><span>story '</span><span>{title}</span><span>' has avg rating </span><span>{avg_rating:?}</span><span>");
</span><span>    }
</span><span>}
</span></code></pre>
<p>Key points about queries:</p>
<ul>
<li><code>rows</code> represents the current set of rows in the query.</li>
<li>Joins can add rows and filters can remove rows. <details>By joining a table like <code>Story</code>, the <code>rows</code> set is mutated to be the Cartesian product of itself and the rows from the joined table. The query above only has a single <code>join</code>, so we know it will give exactly one result for each row in the <code>Story</code> table.</details> </li>
<li>Using <code>aggregate</code> to calculate an aggregate, does not change the number of rows in the query.</li>
<li><code>rows.filter_on</code> can be used to filter rows in the aggregate to match a value from the outer scope.</li>
<li>The <code>rows.avg</code> method returns the average of the rows in the aggregate, if there are no rows then the average will evaluate to <code>None</code>.</li>
<li>Results can be collected into vectors of tuples or structs.</li>
</ul>
<h3 id="schema-evolution-and-migrations">Schema Evolution and Migrations</h3>
<p>Let's say you want to add an email address to each user. Here's how you'd create the new schema version:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>schema</span><span>]
</span><span>#[</span><span>version</span><span>(0..</span><span>=</span><span>1)]
</span><span>enum </span><span>Schema {
</span><span>    User {
</span><span>        name: </span><span>String</span><span>,
</span><span>        #[</span><span>version</span><span>(1..)]
</span><span>        email: </span><span>String</span><span>,
</span><span>    },
</span><span>    </span><span>// ... rest of schema ...
</span><span>}
</span><span>use </span><span>v1::</span><span>*</span><span>;
</span></code></pre>
<p>And here's how you'd migrate the data:</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> m </span><span>=</span><span> m.</span><span>migrate</span><span>(v1::update::Schema {
</span><span>    user: </span><span>Box</span><span>::new(|old_user| {
</span><span>        Alter::new(v1::update::UserMigration {
</span><span>            email: old_user
</span><span>                .</span><span>name</span><span>()
</span><span>                .</span><span>map_dummy</span><span>(|name| format!("</span><span>{name}</span><span>@example.com</span><span>")),
</span><span>        })
</span><span>    }),
</span><span>});
</span></code></pre>
<ul>
<li>The <code>v1::update</code> module contains structs defining the difference between schema <code>v0</code> and schema <code>v1</code>.</li>
<li>We use these structs to implement the migration. This way the migration is type checked against both the old and new schemas.</li>
<li>Note that inside migrations we can execute all the single-row queries we want: aggregates, unique constraint lookups etc.!</li>
<li>We can also use <code>map_dummy</code> with arbitrary Rust to process rows further.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p><code>rust-query</code> represents a fresh approach to database interactions in Rust, prioritizing:</p>
<ul>
<li>Checking everything possible at compile time.</li>
<li>Making it possible to compose queries with each other and arbitrary Rust.</li>
<li>Enabling schema evolution with type-checked migrations.</li>
</ul>
<p>While still in development, the library already allows building experimental database-backed applications in Rust. I encourage you to try it out and provide feedback through <a href="https://github.com/LHolten/rust-query">GitHub</a> issues!</p>
<blockquote>
<p>The library currently uses SQLite as its only backend, chosen for its embedded nature. This will not change anytime soon, as one backend is most practical while <code>rust-query</code> is in development.</p>
</blockquote>


    
  
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Screenshots of Test Equipment Old and New (111 pts)]]></title>
            <link>https://tomverbeure.github.io/2024/11/29/Making-Screenshots-of-Test-Equipment.html</link>
            <guid>42279792</guid>
            <pubDate>Sat, 30 Nov 2024 05:36:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomverbeure.github.io/2024/11/29/Making-Screenshots-of-Test-Equipment.html">https://tomverbeure.github.io/2024/11/29/Making-Screenshots-of-Test-Equipment.html</a>, See on <a href="https://news.ycombinator.com/item?id=42279792">Hacker News</a></p>
Couldn't get https://tomverbeure.github.io/2024/11/29/Making-Screenshots-of-Test-Equipment.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Engineering Sleep (343 pts)]]></title>
            <link>https://minjunes.ai/posts/sleep/index.html</link>
            <guid>42279454</guid>
            <pubDate>Sat, 30 Nov 2024 04:33:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minjunes.ai/posts/sleep/index.html">https://minjunes.ai/posts/sleep/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=42279454">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Engineering Sleep</span></p><p><span>Background</span><span>&nbsp;</span></p><p><span>Sleep claims a third of human life. Like water, it’s not a desire but a necessity. Sleep rules virtually every important system: brain, heart, mood, and immunity. Nature’s terms are harsh. Sleep eight hours or face mental and physical decay. Can we rewrite the terms in our favor? Can we sleep less, but still feel refreshed? I believe we can, and that now is the best time to start engineering sleep.</span></p><p><span>Rare mutations suggest a great variation in sleep efficiency between people</span><span>.</span><span>&nbsp;A small proportion of the population have Familial Natural Short Sleep (FNSS), a benign mutation that allows them to sleep 1-2 hours less than the recommended 7-9 hours, without experiencing the negative effects of sleep deprivation [</span><span><a target="_blank" rel="noopener noreferrer" href="https://my.clevelandclinic.org/health/diseases/short-sleeper-syndrome-sss&amp;sa=d&amp;source=editors&amp;ust=1732929030565618&amp;usg=aovvaw0vywxxv5vmhkrvjzmx">1</a></span><span>]</span><span>. </span></p><p><span>Contrary to symptoms of chronic sleep deprivation, people with FNSS are “healthy, energetic, optimistic, with high pain threshold, and do not seem to suffer adverse effects of chronic restricted sleep” [</span><span><a target="_blank" rel="noopener noreferrer" href="https://journals.lww.com/neurotodayonline/fulltext/2019/12050/a_genetic_mutation_for_short_sleep_prevents_memory.8.aspx">2</a></span><span>]</span><span>. This goes against everything we know about sleep. The most plausible explanation is that people with FNSS are more efficient sleepers. Whichever functions of sleep make it so crucial, they are doing it faster and better.</span></p><p><span>The Sleep Mutation</span></p><p><span>How does FNSS work? Five genes have been implicated in the FNSS phenotype, but DEC2 is the most studied. In 2009, professor Ying-Hui Fu at UCSF discovered a DEC2 point mutation from two individuals in the same family who slept 6.25 hours on average [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/19679812/">3</a></span><span>]. DEC2 codes for a repressive transcription factor (a protein that inhibits the expression of some gene). Normally, the gene that this transcription factor represses is responsible for expressing orexin, a neurotransmitter. In the mutation, proline is replaced by arginine at position 384 in exon 5 (DEC2P384R), disrupting its ability to repress orexin expression. Consequently, more orexin is expressed in individuals with this mutation. The UCSF group hypothesizes that this elevated level of orexin expression partially explains reduced sleep [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/29531056/">4</a></span><span>]. </span></p><p><span><img alt="image3" src="https://minjunes.ai/posts/sleep/images/image3.jpg" title=""></span></p><p><span>Fig 1. In normal humans, Dec2 weakens E12/Myod1’s binding affinity to the Ebox1 promoter site of prepro-orexin, which is responsible for endogenous orexin synthesis. In FNSS mutants, the DEC2P384R interaction with the E12/Myod1 complex is weaker, and there is greater orexin expression.</span></p><p><span>Two decades of sleep research supports the link between orexin and sleep [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/35851580/">5</a></span><span>]. In both narcolepsy and insomnia, orexin is the key neurotransmitter that modulates awakeness. A deficit of orexin producing neurons is responsible for excessive sleepiness in narcolepsy [</span><span><a target="_blank" rel="noopener noreferrer" href="https://sleep.hms.harvard.edu/education-training/public-education/sleep-and-health-education-program/sleep-health-education-4#:~:text=Research%20has%20revealed%20that%20narcolepsy,in%20the%20development%20of%20narcolepsy">6</a></span><span>]. An overexpression of orexin is responsible for hypervigilance in insomnia [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37086045/">7</a></span><span>]. Throughout the day and night, we move between the wake-sleep axis defined by orexin levels, which are lowest in the middle of the day and highest during the transition from NREM to REM sleep [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37686711/">8</a></span><span>].</span></p><p><span>Orexin is a commercially validated lever for controlling sleep. As of late 2024, there are eight orexin receptor agonists (promotes firing of neuron) in clinical trials for treating narcolepsy and hypersomnia, and two orexin receptor antagonists (inhibits firing of neuron) on the market for treating insomnia. To summarize orexin, too little of it makes you sleepy, and too much of it makes you unable to sleep.</span></p><p><span>But if elevated orexin levels explain reduced sleep in both FNSS carriers and insomniacs, why is one sleep deprived but not the other? We don’t know. Variation in dynamics of when, where, and how much orexin is released could explain the difference. Also, FNSS carriers might have developed compensatory mechanisms to cope with elevated orexin, leading to more efficient sleep. Experiments to reproduce FNSS will give us answers. </span></p><p><span>Reproducing FNSS</span></p><p><span>Given our current knowledge of FNSS, has anyone tried to reproduce it? A true reproduction would be safe and effective over the lifetime of the host, just like we see in the natural phenotype. The closest attempt was by the UCSF group that identified the DEC2P384R mutation. In their pioneering 2009 study, the group embryonically edited human DEC2P384R into transgenic mice and saw a 1-2 hour reduction in sleep. However, we don’t know if it was safe and effective over the lifetime of the mice. They recorded sleep architecture and sleep recovery during a 24-hour window in six to eight month old mice, tracking no other health markers [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/19679812/">3</a></span><span>]. </span></p><p><span>The study intervened at the embryo level of the host and saw short-term success reproducing FNSS. But what we’re really interested in is adulthood intervention and lifelong efficacy. Giving normal people the ability to sleep more efficiently is the ultimate goal. Expressing the DEC2P384R mutation in normal adult animals and conducting a lifelong study would answer this question. Two possible pathways to reproducing FNSS are reviewed below. </span></p><p><span>Path I: Orexin Agonists</span></p><p><span><img alt="" src="https://minjunes.ai/posts/sleep/images/image2.jpg" title=""></span></p><p><span>Fig 2. Pathway I success case</span></p><p><span>Approach</span></p><p><span>Orally dose orexin receptor agonists. The mechanism leverages direct receptor activation, similar to drugs currently in clinical trials for treating narcolepsy. These small molecules are designed for optimal blood-brain barrier penetration and selective binding to orexin receptors. </span></p><p><span>Unknowns</span></p><p><span>Primarily, we don’t know if elevated orexin levels explain the FNSS phenotype. Also, we don’t know the effects of chronic orexin receptor activation on sleep architecture and cognition. Pharma companies developing orexin agonists have data on short-term sleep effects, but none of them have published data on long term effects [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/36108771/">9</a></span><span>]. Also unknown are tolerance and withdrawal effects over time: like other receptor agonists (think nicotine), we may see diminishing effects, and withdrawal effects on return to baseline. Finally, variations in individual response are unknown. </span></p><p><span>Path II: Gene Therapy</span></p><p><span><img alt="" src="https://minjunes.ai/posts/sleep/images/image5.jpg" title=""></span></p><p><span>Fig 3. </span><span>Pathway II success case</span></p><p><span>Approach</span></p><p><span>Replicate the natural FNSS mutation through episomal expression. Episomal expression is when the gene is expressed from a piece of DNA that is outside the cell’s chromosomal DNA. Since chromosomal DNA is left alone, there is no risk of passing down the mutation to offspring. For this approach, we use Adeno-Associated Virus serotype 9 (AAV9) vectors to deliver the DEC2P384R gene to orexin-expressing neurons in adult mice. The vectors (the piece of extra chromosomal DNA) remain in the nucleus, continuously synthesizing the mutant DEC2 protein. This aims to partially mirror the mechanism seen in FNSS.</span></p><p><span>Unknowns</span></p><p><span>We are more certain that DEC2P384R explains FNSS, but we don’t know if expressing it in adulthood works. We also don’t know off-target effects on DEC2-regulated pathways beyond sleep. A specific unknown to episomal expression is the competition dynamics between DEC2P384R and native DEC2. The usual unknowns of variations between individual responses, particularly immune response, apply. </span></p><p><span>Overview of Pathways</span></p><p><span><img alt="image1" src="https://minjunes.ai/posts/sleep/images/image1.png" title=""></span></p><p><span>Too good to be true? &nbsp;</span></p><p><span>Around 90 families with FNSS have been identified to date [</span><span><a target="_blank" rel="noopener noreferrer" href="https://reporter.nih.gov/search/n0rjIH9BFE6TYwe-IE46Gw/project-details/10893516">10</a></span><span>]. If FNSS is truly benign, why is it so rare? Shouldn’t more efficient sleep confer a survival advantage? It could be that the mutation really is benign, but does not help reproductive success. But, the mutation could also have negative fitness effects that are not observed.</span></p><p><span><img alt="" src="https://minjunes.ai/posts/sleep/images/image4.jpg" title=""></span></p><p><span>Fig 4. Fisher-Wright simulation showing allele frequency dynamics with 10% fitness penalty across population sizes (N=100, 1,000, 10,000). Initial carrier frequency 1%, tracked for 20 generations over 100 simulations. Solid lines show means; shaded regions show standard deviations.</span></p><p><span>Under the Fisher-Wright model, harmful mutations can appear neutral when tracking small populations across just a few generations. If the mutation has a tiny effective population size, limited generational depth, and low carrier frequency, it would be hard to distinguish between neutral drift and negative selection.</span></p><p><span>Fortunately, there is no risk of the mutation being passed down to offsprings in either the orexin agonist pathway or the gene therapy pathway. So we can rule out the nightmare scenario of offspring effects gone wrong. Instead, the risks are concentrated in medium to long term health of individuals who undergo therapy. As of now, we simply don’t have enough data to profile risk factors. More experiments are needed to know if “FNSS for all” is too good to be true. </span></p><p><span>Where is my better sleep? </span></p><p><span>People with FNSS are living proof that we don’t need 7-9 hours of sleep to be healthy. We already don’t get enough sleep. 34% of Americans are chronically sleep deprived [</span><span><a target="_blank" rel="noopener noreferrer" href="https://news.gallup.com/poll/642704/americans-sleeping-less-stressed.aspx">11</a></span><span>]. What if they could keep sleeping less, but with no consequences? That’s possible with advanced sleep engineering. Here’s what else would be possible: falling asleep and waking at will, sleeping 4 hours but feeling like you slept 8 hours, always in perfect mental and physical condition. Considering the huge upside of engineering sleep, an unreasonably small number of experiments have studied FNSS. </span></p><p><span>Due to their relatively singular effect on sleep, FNSS mutations are a gold mine for studying sleep. But, there have been only two attempts to mimic FNSS outside of Fu et al: a study that found better memory consolidation in sleep deprived mice [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/31619542/">12</a></span><span>], and another that found greater longevity in flies [</span><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37163058/">13</a></span><span>]. None have been lifetime studies in mammals, which are most relevant to therapy development. </span></p><p><span>15 years after its pioneering work that identified DEC2P384R, Ying-Hui Fu’s lab is the only group that came close to reverse engineering FNSS. Perhaps this represents what J Storss Halls called a “civilizational failure of nerve”, where institutions become pathologically risk-averse, more focusing on preventing downside risks than enabling upside potential [</span><span><a target="_blank" rel="noopener noreferrer" href="https://press.stripe.com/where-is-my-flying-car#:~:text=In%20Where%20Is%20My%20Flying,that%20started%20in%20the%201970s.">14</a></span><span>]. Scientific and technological progress rests on the willingness to experiment. If existing institution’s won’t give us better sleep, we should build ones that do.</span></p><p><span>Next Steps</span></p><p><span>Contact me if you are interested in: </span></p><ul><li><span>Expanding the known FNSS database, and sequencing everyone in it</span></li><li><span>Testing pathways I and II</span></li><li><span>Funding the above</span></li></ul><p><span>Special thanks to Andy Kong, Ishan Goel, Tazik Shahjahan, and Mae Richardson for valuable feedback. </span></p><p><span>References</span></p><ol start="1"><li><span><a target="_blank" rel="noopener noreferrer" href="https://my.clevelandclinic.org/health/diseases/short-sleeper-syndrome-sss&amp;sa=d&amp;source=editors&amp;ust=1732929030565618&amp;usg=aovvaw0vywxxv5vmhkrvjzmx">Short Sleeper Syndrome</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://journals.lww.com/neurotodayonline/fulltext/2019/12050/a_genetic_mutation_for_short_sleep_prevents_memory.8.aspx">A Genetic Mutation for Short Sleep Prevents Memory Deficits in a Mouse Model</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/19679812/">He Y, Jones CR, Fujiki N, Xu Y, Guo B, Holder JL Jr, Rossner MJ, Nishino S, Fu YH. The transcriptional repressor DEC2 regulates sleep length in mammals. Science. 2009 Aug 14;325(5942):866-70. doi: 10.1126/science.1174443. PMID: 19679812; PMCID: PMC2884988.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/29531056/">Hirano A, Hsu PK, Zhang L, Xing L, McMahon T, Yamazaki M, Ptáček LJ, Fu YH. DEC2 modulates orexin expression and regulates sleep. Proc Natl Acad Sci U S A. 2018 Mar 27;115(13):3434-3439. doi: 10.1073/pnas.1801693115. Epub 2018 Mar 12. PMID: 29531056; PMCID: PMC5879715.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/35851580/">De Luca R, Nardone S, Grace KP, Venner A, Cristofolini M, Bandaru SS, Sohn LT, Kong D, Mochizuki T, Viberti B, Zhu L, Zito A, Scammell TE, Saper CB, Lowell BB, Fuller PM, Arrigoni E. Orexin neurons inhibit sleep to promote arousal. Nat Commun. 2022 Jul 18;13(1):4163. doi: 10.1038/s41467-022-31591-y. PMID: 35851580; PMCID: PMC9293990.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://sleep.hms.harvard.edu/education-training/public-education/sleep-and-health-education-program/sleep-health-education-4#:~:text=Research%20has%20revealed%20that%20narcolepsy,in%20the%20development%20of%20narcolepsy">https://sleep.hms.harvard.edu/education-training/public-education/sleep-and-health-education-program/sleep-health-education-4#:~:text=Research%20has%20revealed%20that%20narcolepsy,in%20the%20development%20of%20narcolepsy</a></span><span>.</span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37086045/">Muehlan C, Roch C, Vaillant C, Dingemanse J. The orexin story and orexin receptor antagonists for the treatment of insomnia. J Sleep Res. 2023 Dec;32(6):e13902. doi: 10.1111/jsr.13902. Epub 2023 Apr 22. PMID: 37086045.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37686711/">Mogavero, M. P., Godos, J., Grosso, G., Caraci, F., &amp; Ferri, R. (2023). Rethinking the Role of Orexin in the Regulation of REM Sleep and Appetite. Nutrients, 15(17), 3679. https://doi.org/10.3390/nu15173679</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/36108771/">Ishikawa T, Hara H, Kawano A, Kimura H. Danavorexton, a selective orexin 2 receptor agonist, provides a symptomatic improvement in a narcolepsy mouse model. Pharmacol Biochem Behav. 2022 Oct;220:173464. doi: 10.1016/j.pbb.2022.173464. Epub 2022 Sep 13. PMID: 36108771.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://reporter.nih.gov/search/n0rjIH9BFE6TYwe-IE46Gw/project-details/10893516">https://reporter.nih.gov/search/n0rjIH9BFE6TYwe-IE46Gw/project-details/10893516</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://news.gallup.com/poll/642704/americans-sleeping-less-stressed.aspx">https://news.gallup.com/poll/642704/americans-sleeping-less-stressed.aspx</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/31619542/">Xing, L., Shi, G., Mostovoy, Y., Gentry, N. W., Fan, Z., McMahon, T. B., Kwok, P. Y., Jones, C. R., Ptáček, L. J., &amp; Fu, Y. H. (2019). Mutant neuropeptide S receptor reduces sleep duration with preserved memory consolidation. Science translational medicine, 11(514), eaax2014. https://doi.org/10.1126/scitranslmed.aax2014</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://pubmed.ncbi.nlm.nih.gov/37163058/">Pandey P, Wall PK, Lopez SR, Dubuisson OS, Zunica ERM, Dantas WS, Kirwan JP, Axelrod CL, Johnson AE. A familial natural short sleep mutation promotes healthy aging and extends lifespan in Drosophila. bioRxiv [Preprint]. 2023 Apr 26:2023.04.25.538137. doi: 10.1101/2023.04.25.538137. PMID: 37163058; PMCID: PMC10168263.</a></span></li><li><span><a target="_blank" rel="noopener noreferrer" href="https://press.stripe.com/where-is-my-flying-car#:~:text=In%20Where%20Is%20My%20Flying,that%20started%20in%20the%201970s.">Hall, J. S. (2021). Where is my flying car? Stripe Press.</a></span></li></ol></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If not React, then what? (195 pts)]]></title>
            <link>https://infrequently.org/2024/11/if-not-react-then-what/</link>
            <guid>42279172</guid>
            <pubDate>Sat, 30 Nov 2024 03:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infrequently.org/2024/11/if-not-react-then-what/">https://infrequently.org/2024/11/if-not-react-then-what/</a>, See on <a href="https://news.ycombinator.com/item?id=42279172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      <!-- BLOCK_CONTENT -->

  
  <article>
    <header>
      <h2>
        <a href="https://infrequently.org/2024/11/if-not-react-then-what/">If Not React, Then What?</a>
      </h2>
      <h3>Frameworkism isn't delivering. The answer isn't a different tool, it's the courage to do engineering.</h3>
      
    </header>
    <p>Over the past decade, my work has centred on partnering with teams to build ambitious products for the web across both desktop and mobile. This has provided a ring-side seat to a sweeping variety of teams, products, and technology stacks across more than 100 engagements.</p>
<p>While I'd <em>like</em> to be spending most of this time working through improvements to web APIs, the majority of time spent with partners goes to <a href="https://infrequently.org/2024/08/object-lesson/">remediating performance and accessibility issues caused by "modern" frontend frameworks (React, Angular, etc.) and the culture surrounding them.</a> These issues are most pronounced today in React-based stacks.</p>
<p>This is disquieting because <a href="#fn-why-not-1">React is legacy technology,</a> but it continues to appear in greenfield applications.</p>
<p>Surprisingly, some continue to insist that React is "modern." Perhaps we can square the circle if we understand "modern" to apply to React in the way it applies to art. Neither demonstrate contemporary design or construction techniques. They are not built to meet current needs or performance standards, but stand as expensive <em>objets</em> that harken back to the peak of an earlier era's antiquated methods.</p>
<nav><ul><li><a href="#the-rule-of-least-client-side-complexity">The Rule Of Least Client-Side Complexity </a></li><li><a href="#ok%2C-but-what%2C-then%3F">OK, But What, Then? </a></li><li><a href="#and-nothing-of-value-was-lost">And Nothing Of Value Was Lost </a></li><li><a href="#vignettes">Vignettes </a><ul><li><a href="#informational">Informational </a></li><li><a href="#e-commerce">E-Commerce </a></li><li><a href="#media">Media </a></li><li><a href="#social">Social </a></li><li><a href="#productivity">Productivity </a></li><li><a href="#other-application-classes">Other Application Classes </a></li></ul></li><li><a href="#%22but...%22">"But..." </a><ul><li><a href="#%22...we-need-to-move-fast%22">"...we need to move fast" </a></li><li><a href="#%22...it-works-for-facebook%22">"...it works for Facebook" </a></li><li><a href="#%22...our-teams-already-know-react%22">"...our teams already know React" </a></li><li><a href="#%22...we-need-to-be-able-to-hire-easily%22">"...we need to be able to hire easily" </a></li><li><a href="#%22...everyone-has-fast-phones-now%22">"...everyone has fast phones now" </a></li><li><a href="#%22...react-is-industry-standard%22">"...React is industry-standard" </a></li><li><a href="#%22...the-ecosystem...%22">"...the ecosystem..." </a></li><li><a href="#%22...next.js-can-be-fast-(enough)%22">"...Next.js can be fast (enough)" </a></li><li><a href="#%22...react-native!%22">"...React Native!" </a></li></ul></li><li><a href="#references">References </a></li></ul></nav>
<p>In the hope of steering the <em>next</em> team away from the rocks, I've found myself penning <a href="https://infrequently.org/2023/02/the-market-for-lemons/">advocacy pieces</a> and <a href="https://infrequently.org/series/performance-inequality/">research into the state of play</a>, as well as <a href="https://www.youtube.com/live/rU3-hOhYW2Y?si=XysGZKAyvAhsnpuo&amp;t=28267">giving talks</a> to alert managers and developers of the dangers of today's misbegotten frontend orthodoxies.</p>
<p>In short, nobody should start a new project in the 2020s based on React. Full stop.<sup><a href="#fn-why-not-1" id="fnref-why-not-1">[1]</a></sup></p>
<h2 id="the-rule-of-least-client-side-complexity" tabindex="-1">The Rule Of Least Client-Side Complexity <a href="#the-rule-of-least-client-side-complexity">#</a></h2>
<p>Code that runs on the server can be fully costed. Performance and availability of server-side systems are under the control of the provisioning organisation, and latency can be actively managed by developers and DevOps engineers.</p>
<p>Code that runs on the client, by contrast, is running on The Devil's Computer.<sup><a href="#fn-the-devils-computer-2" id="fnref-the-devils-computer-2">[2]</a></sup> Nothing about the experienced latency, client resources, or even available APIs are under the developer's control.</p>
<p>Client-side web development is perhaps best conceived of as <em>influence-oriented programming</em>. Once code has left the datacenter, all a web developer can do is send <a href="https://piccalil.li/blog/a-handful-of-reasons-javascript-wont-be-available/">thoughts and prayers.</a></p>
<p>As a result, an unreasonably effective strategy is to send less code. In practice, this means favouring HTML and CSS over JavaScript, as they degrade gracefully and feature higher compression ratios. Declarative forms generate more functional UI per byte sent. These improvements in resilience and reductions in costs are beneficial in compounding ways over a site's lifetime.</p>
<p>Stacks based on React, Angular, and other legacy-oriented, desktop-focused JavaScript frameworks generally take the opposite bet. These ecosystems pay lip service the controls that are necessary to prevent horrific <a href="https://infrequently.org/2024/08/object-lesson/#fn-receipts-1">profliferations of unnecessary client-side cruft.</a> The predictable consequence are NPM-algamated bundles full of redundancies like <code>core-js</code>, <code>lodash</code>, <code>underscore</code>, polyfills for browsers that no longer exist, userland ECC libraries, <code>moment.js</code>, and a hundred other horrors.</p>
<p>This culture is so out of hand that it seems 2024's React developers are constitutionally unable to build chatbots without including all of these 2010s holdovers, plus at least one extremely chonky MathML or TeX formatting library to display formlas; something needed in vanishingly few sessions.</p>
<p>Tech leads and managers need to break this spell and force ownership of decisions affecting the client. In practice, this means forbidding React in all new work.</p>
<h2 id="ok%2C-but-what%2C-then%3F" tabindex="-1">OK, But What, Then? <a href="#ok%2C-but-what%2C-then%3F">#</a></h2>
<p>This question comes in two flavours that take some work to tease apart:</p>
<ul>
<li>The narrow form:<p><em>"Assuming we have a well-qualified need for client-side rendering, what specific technologies would you recommend instead of React?"</em></p></li>
<li>The broad form:<p><em>"Our product stack has bet on React and the various mythologies that the cool kids talk about on React-centric podcasts. You're asking us to rethink the whole thing. Which silver bullet should we adopt instead?"</em></p></li>
</ul>
<p>Teams that have grounded their product decisions appropriately can productively work through the narrow form by running truly objective bakeoffs. Building multiple small PoCs to determine each approach's scaling factors and limits can even be a great deal of fun.<sup><a href="#fn-ballot-stuffing-3" id="fnref-ballot-stuffing-3">[3]</a></sup> It's the rewarding side of real engineering, trying out new materials under well-understood constraints to improve user outcomes.</p>
<blockquote>
<p><strong>Note</strong>: Developers building SPAs or islands of client-side interactivity are spoilt for choice. This blog won't recommend a specific tool, but Svelte, Lit, FAST, Solid, Qwik, Marko, HTMX, Vue, Stencil, and a dozen other contemporary frameworks are worthy of your attention.</p>
<p>Despite their lower initial costs, teams investing in any of them will still require strict controls on client-side payloads and complexity, as JavaScript remains at least 3x more expensive than equivalent HTML and CSS, byte-for-byte.</p>
</blockquote>
<p>In almost every case, the constraints on tech stack decisions have materially shifted since they were last examined, or the realities of a site's user base are vastly different than product managers and tech leads expect. Gathering data on these factors allows for first-pass cuts about stack choices, winnowing quickly to a smaller set of options to run bakeoffs for.<sup><a href="#fn-cut-lines-4" id="fnref-cut-lines-4">[4]</a></sup></p>
<p>But the teams we spend the most time with aren't in that position.</p>
<p>Many folks asking <em>"if not React, then what?"</em> think they're asking in the narrow form but are grappling with the broader version. A shocking fraction of (decent, well-meaning) product managers and engineers haven't thought through the whys and wherefores of their architectures, opting instead to go with what's popular in a sort of responsibility fire brigade.<sup><a href="#fn-buck-passing-5" id="fnref-buck-passing-5">[5]</a></sup></p>
<p>For some, provocations to abandon React create an unmoored feeling, a suspicion that they might not understand the world any more.<sup><a href="#fn-off-brand-therapy-6" id="fnref-off-brand-therapy-6">[6]</a></sup> Teams in this position are working through the epistemology of their values and decisions.<sup><a href="#fn-shortcuts-7" id="fnref-shortcuts-7">[7]</a></sup> <em>How</em> can they know their technology choices are better than the alternatives? <em>Why</em> should they pick one stack over another?</p>
<p>Many need help orienting themselves as to which end of the telescope is better for examining frontend problems. Frameworkism is now the dominant creed of frontend discourse. It insists that all user problems will be solved if teams just framework <em>hard enough</em>. This is non-sequitur, if not entirely backwards. In practice, <strong>the only thing that makes web experiences good is caring about the user experience</strong> — specifically, the experience of folks at the margins. Technologies come and go, but what always makes the difference is giving a toss about the user.</p>
<p>In less vulgar terms, the struggle is to convince managers and tech leads that they need to <a href="https://www.gov.uk/guidance/government-design-principles"><em>start with user needs</em>.</a> Or as <a href="https://public.digital/">Public Digital</a> puts it, <em><a href="https://public.digital/pd-insights/blog/2018/10/internet-era-ways-of-working#:~:text=1.%20Design%20for%20user%20needs%2C%20not%20organisational%20convenience">"design for user needs, not organisational convenience"</a></em></p>
<p>The essential component of this mindset shift is replacing hopes based on promises with constraints based on research and evidence. This aligns with what it means to commit wanton acts of engineering because engineering is the practice of designing solutions to problems for users and society under known constraints.</p>
<p>The opposite of engineering is imagining that constraints do not exist or do not apply to <em>your</em> product. The shorthand for this is <a href="https://archive.org/details/on-bullshit-by-harry-frankfurt">"bullshit."</a></p>
<p>Rejecting an engrained practice of bullshitting does not come easily. Frameworkism preaches that the way to improve user experiences is to adopt more (or different) tooling from the framework's ecosystem. This provides adherents with something to <em>do</em> that looks plausibly like engineering, except it isn't. It can even become a totalising commitment; solutions to user problems outside the framework's expanded cinematic universe are unavailable to the frameworkist. Non-idiomatic patterns that unlock significant wins for users are bugs to be squashed. And without data or evidence to counterbalance bullshit artists's assertions, who's to say they're wrong? Orthodoxy unmoored from measurements of user outcomes predictably spins into <a href="https://styled-components.com/">abstruse absurdities.</a> Heresy, eventually, is perceived to carry heavy sanctions.</p>
<p>It's all nonsense.</p>
<p>Realists do not wallow in abstraction-induced hallucinations about user experiences; they measure them. Realism requires <a href="https://infrequently.org/2024/01/performance-inequality-gap-2024/">reckoning with the world as it is,</a> not as we wish it to be, and in that way, it's the opposite of frameworkism.</p>
<p>The most effective tools for breaking this spell are techniques that give managers a user-centred view of their system's performance. This can take the form of <a href="https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic">RUM data,</a> such as <a href="https://web.dev/articles/vitals">Core Web Vitals</a> (<a href="https://cruxvis.withgoogle.com/">check yours now!</a>), or lab results from well-configured test-benches (e.g., <a href="https://webpagetest.org/">WPT</a>). Instrumenting critical user journeys and talking through business goals are quick follow-ups that enable teams to seize the momentum and formulate business cases for change.</p>
<p>RUM and bench data sources are essential antidotes to frameworkism because they provide data-driven baselines to argue about. Instead of accepting the next increment of framework investment on faith, teams armed with data can begin to weigh up the actual costs of fad chasing versus likely returns.</p>
<h2 id="and-nothing-of-value-was-lost" tabindex="-1">And Nothing Of Value Was Lost <a href="#and-nothing-of-value-was-lost">#</a></h2>
<p>Prohibiting the spread of React (and other frameworkist totems) by policy is both an incredible cost-saving tactic and a helpful way to reorient teams towards delivery for users. However, better results only arrive once frameworkism itself is eliminated from decision-making. Avoiding one class of mistake won't pay dividends if we spend the windfall on investments within the same error category.</p>
<p>A general answer to the broad form of the problem has several parts:</p>
<ul>
<li><strong>User focus</strong>: decision-makers must accept that they are directly accountable for the results of their engineering choices. No buck-passing is allowed. Either the system works well for users, including those at the margins, or it doesn't. Systems that are not performing are to be replaced with versions that do. There are no sacred cows, only problems to be solved with the appropriate application of constraints.</li>
<li><strong>Evidence</strong>: the essential shared commitment between management and engineering is a dedication to realism. Better evidence must win.</li>
<li><strong>Guardrails</strong>: policies must be implemented to ward off hallucinatory frameworkist assertions about how better experiences are delivered. Good examples of this include the UK Government Digital Service's <a href="https://www.gov.uk/service-manual/technology/using-progressive-enhancement">requirement that services be built using progressive enhancement techniques.</a> Organisations can tweak guidance as appropriate (e.g., creating an escalation path for exceptions), but the important thing is to set a baseline. Evidence boiled down into policy has power.</li>
<li><strong>Bakeoffs</strong>: no new system should be deployed without a clear set of critical user journeys. Those journeys embody what we expect users to do most frequently in our systems, and once those definitions are in hand, we can do bakeoffs to test how well various systems deliver, given the constraints of the expected marginal user. This process description puts the product manager's role into stark relief. Instead of suggesting an endless set of experiments to run (often poorly), they must define a product thesis and commit to an understanding of what success means. This will be uncomfortable. It's also the job. Graciously accept the resignations of PMs who decide managing products is not in their wheelhouse.</li>
</ul>
<h2 id="vignettes" tabindex="-1">Vignettes <a href="#vignettes">#</a></h2>
<p>To see how realism and frameworkism differ in practice, it's helpful to work a few examples. As background, recall that our rubric<sup><a href="#fn-rubrics-8" id="fnref-rubrics-8">[8]</a></sup> for choosing technologies is based on the number of manipulations of primary data (updates) and session length. Some classes of app feature long sessions and many incremental updates to the primary information of the UI. In these (rarer) cases, a local data model can be helpful in supporting timely application of updates, <em>but this is the exception</em>.</p>
<figure>
<picture width="3528" height="1478">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2023/02/the-market-for-lemons/depth-and-frequency-small.png" alt="Sites with short average sessions cannot afford much JS up-front." width="3528" height="1478" decoding="async">
</picture>

  <figcaption>Sites with short average sessions cannot afford much JS up-front.</figcaption>
</figure>
<p>It's only in these exceptional instances that SPA architectures should be considered.</p>
<picture width="1281" height="1450">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.png" alt="Very few sites will meet the qualifications to be built as an SPA" width="1281" height="1450" decoding="async" loading="lazy">
</picture>

<p>And only when an SPA architecture is required should tools designed to support optimistic updates against a local data model — including "frontend frameworks" and "state management" tools — ever become part of a site's architecture.</p>
<p><strong>The choice isn't between JavaScript frameworks, it's whether SPA-oriented tools should be entertained <em>at all</em>.</strong></p>
<p>For most sites, the answer is a clearly <em>"no".</em></p>
<h3 id="informational" tabindex="-1">Informational <a href="#informational">#</a></h3>
<p>Sites built to inform should almost always be built using semantic HTML with optional <a href="https://www.gov.uk/service-manual/technology/using-progressive-enhancement">progressive enhancement as necessary.</a></p>
<p>Static site generation tools like Hugo, Astro, 11ty, and Jekyll work well for many of these cases. Sites that have content that changes more frequently should look to "classic" CMSes or tools like WordPress to generate HTML and CSS.</p>
<p>Blogs, marketing sites, company home pages, public information sites, and the like should minimise client-side JavaScript payloads to the greatest extent possible. They should never be built using frameworks that are designed to enable SPA architectures.<sup><a href="#fn-no-ssr-doesnt-count-9" id="fnref-no-ssr-doesnt-count-9">[9]</a></sup></p>
<h4 id="why-semantic-markup-and-optional-progressive-enhancement-are-the-right-choice" tabindex="-1">Why Semantic Markup and Optional Progressive Enhancement Are The Right Choice <a href="#why-semantic-markup-and-optional-progressive-enhancement-are-the-right-choice">#</a></h4>
<p>Informational sites have short sessions and server-owned application data models; that is, the source of truth for what's displayed on the page is always the server's to manage and own. This means that there is no need for a client-side data model abstraction or client-side component definitions that might be updated from such a data model.</p>
<blockquote>
<p><strong>Note</strong>: many informational sites include productivity components as distinct sub-applications. CMSes such as Wordpress are comprised of two distinct surfaces; a low-traffic, high-interactivity editor for post authors, and a high-traffic, low-interactivity viewer UI for readers. Progressive enhancement should be considered for both, but is an absolute must for reader views which do not feature long sessions.<sup><a href="#fn-rubrics-8" id="fnref-rubrics-8:1">[8:1]</a></sup></p>
</blockquote>
<h3 id="e-commerce" tabindex="-1">E-Commerce <a href="#e-commerce">#</a></h3>
<p>E-commerce sites should be built using server-generated semantic HTML and progressive enhancement.</p>
<figure>
<a href="https://treo.sh/sitespeed/www.amazon.com/vs/www.walmart.com/vs/www.wayfair.com?metrics=lcp%2Cr&amp;formFactor=phone" alt="A large and stable performance gap between Amazon and its React-based competitors demonstrates how poorly SPA architectures perform in e-commerce applications. More than 70% of Walmart's traffic is mobile, making their bet on Next.js particularly problematic for the business." target="_new">
<picture width="1823" height="1290">
  <source sizes="(max-width: 1200px) 70vw, 600px" srcset="https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=3600 2400w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=2400 1600w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=1800 1200w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=1200   800w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=900   600w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=750   500w,
                  https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif?nf_resize=fit&amp;w=600   400w">
<img src="https://infrequently.org/2024/11/if-not-react-then-what/amazon-walmart-wayfair.avif" alt="A large and stable performance gap between Amazon and its React-based competitors demonstrates how poorly SPA architectures perform in e-commerce applications. More than 70% of Walmart's traffic is mobile, making their bet on Next.js particularly problematic for the business." width="1823" height="1290" decoding="async" loading="lazy">
</picture>

</a>
  <figcaption>A large and stable performance gap between Amazon and its React-based competitors demonstrates how poorly SPA architectures perform in e-commerce applications. More than 70% of Walmart's traffic is mobile, making their bet on Next.js particularly problematic for the business.</figcaption>
</figure>
<p>Many tools are available to support this architecture. Teams building e-commerce experiences should prefer stacks that deliver <em>no JavaScript by default</em>, and buttress that with controls on client-side script to prevent <a href="https://wpostats.com/">regressions in material business metrics.</a></p>
<h4 id="why-progressive-enhancement-is-the-right-choice" tabindex="-1">Why Progressive Enhancement Is The Right Choice <a href="#why-progressive-enhancement-is-the-right-choice">#</a></h4>
<p>The general form of e-commerce sites has been stable for more than 20 years:</p>
<ul>
<li>Landing pages with current offers and a search function for finding products</li>
<li>Search results pages which allow for filtering and comparison of products</li>
<li>Product-detail pages that host media about products, ratings, reviews, and recommendations for alternatives</li>
<li>Cart management, checkout, and account management screens</li>
</ul>
<p>Across all of these page types, a pervasive login and cart status widget will be displayed. This widget, and the site's logo, are sometimes the <em>only</em> consistent page elements across an e-commerce site.</p>
<p>Consistent experience has demonstrated that the commonality of UI elements is low, that sessions have highly variable lengths, and that content freshness is paramount. These factors argue for server-owned data models and application state. The best way to reduce latency in these experiences is to optimise for lightweight individual pages. Aggressive asset caching, image optimisation, and server-side page-weight reduction strategies can all help.</p>
<h3 id="media" tabindex="-1">Media <a href="#media">#</a></h3>
<p>Media consumption sites vary considerably in session length and data update potential. Most should <em>start</em> as progressively-enhanced markup-based experiences, adding complexity over time as product changes warrant it.</p>
<h4 id="why-progressive-enhancement-and-islands-may-be-the-right-choice" tabindex="-1">Why Progressive Enhancement and Islands May Be The Right Choice <a href="#why-progressive-enhancement-and-islands-may-be-the-right-choice">#</a></h4>
<p>Many interactive elements on media consumption sites can be modeled as distinct islands of interactivity (e.g., comment threads). Many of these components present independent data models and can therefore be modeled as Web Components within a larger (static) page.</p>
<h4 id="when-an-spa-may-be-appropriate" tabindex="-1">When An SPA May Be Appropriate <a href="#when-an-spa-may-be-appropriate">#</a></h4>
<p>This model breaks down when media playback must continue across media browsing (think "mini-player" UIs). A fundamental limitation of today's web platform is that it is not possible to preserve some elements from a page across top-level navigations. Sites that must support features like this should consider using SPA technologies while setting strict guardrails for the allowed size of client-side JS per page.</p>
<p>Another reason to consider client-side logic for a media consumption app is offline playback. Managing a local (Service Worker-backed) media cache requires application logic and a way to synchronise information with the server.</p>
<p>Lightweight SPA-oriented frameworks may be appropriate here, along with connection-state resilient data systems such as <a href="https://zero.rocicorp.dev/">Zero</a> or <a href="https://github.com/yjs/yjs">Y.js</a>.</p>

<p>Social media apps feature significant variety in session lengths and media capabilities. Many present infinite-scroll interfaces and complex post editing affordances. These are natural dividing lines in a design that align well with session depth and client-vs-server data model locality.</p>
<h4 id="why-progressive-enhancement-may-be-the-right-choice" tabindex="-1">Why Progressive Enhancement May Be The Right Choice <a href="#why-progressive-enhancement-may-be-the-right-choice">#</a></h4>
<p>Most social media experiences involve a small, fixed number of actions on top of a server-owned data model ("liking" posts, etc.) as well as distinct update phase for new media arriving at an interval. This model works well with a hybrid approach as is found in <a href="https://hotwired.dev/">Hotwire</a> and many <a href="https://htmx.org/">HTMX applications.</a></p>
<h4 id="when-an-spa-may-be-appropriate-1" tabindex="-1">When An SPA May Be Appropriate <a href="#when-an-spa-may-be-appropriate-1">#</a></h4>
<p>Islands of deep interactivity may make sense in social media applications, and aggressive client-side caching (e.g., for draft posts) may aid in building engagement. It may be helpful to think of these as unique app sections with distinct needs from the main site's role in displaying content.</p>
<p>Offline support may be another reason to download a data model and a snapshot of user state to the client. This should be done in conjunction with an approach that builds resilience for the main application. Teams in this situation should consider a Service Worker-based, multi-page app with "stream stitching" architecture which primarily delivers HTML to the page but enables offline-first logic and synchronisation. Because offline support is so invasive to an architecture, this requirement must be identified up-front.</p>
<blockquote>
<p><strong>Note</strong>: Many assume that SPA-enabling tools and frameworks are required to build compelling Progressive Web Apps that work well offline. <em>This is not the case.</em> PWAs can be built using <a href="https://developer.chrome.com/docs/workbox/faster-multipage-applications-with-streams">stream-stitching</a> architectures that apply the equivalent of server-side templating to data on the client, within a Service Worker.</p>
<p>With the advent of <a href="https://developer.chrome.com/docs/web-platform/view-transitions/cross-document">multi-page view transitions</a>, MPA architecture PWAs can present fluid transitions between user states without heavyweight JavaScript bundles clogging up the main thread. It may take several more years for the framework community to digest the implications of these technologies, but they are available <em>today</em> and work exceedingly well, both as foundational architecture pieces and as progressive enhancements.</p>
</blockquote>
<h3 id="productivity" tabindex="-1">Productivity <a href="#productivity">#</a></h3>
<p>Document-centric productivity apps may be the hardest class to reason about, as collaborative editing, offline support, and lightweight (and fast) "viewing" modes with full document fidelity are all general product requirements.</p>
<p>Triage-oriented data stores (e.g. email clients) are also prime candidates for the potential benefits of SPA-based technology. But as with all SPAs, the ability to deliver a better experience hinges both on session depth and up-front payload cost. It's easy to lose this race, as this blog <a href="https://infrequently.org/2022/03/a-unified-theory-of-web-performance/">has examined in the past.</a></p>
<p>Editors of all sorts are a natural fit for local data models and SPA-based architectures to support modifications to them. However, the endemic complexity of these systems ensures that performance will remain a constant struggle. As a result, teams building applications in this style should consider strong performance guardrails, identify critical user journeys up-front, and ensure that instrumentation is in place to ward off unpleasant performance surprises.</p>
<h4 id="why-spas-may-be-the-right-choice" tabindex="-1">Why SPAs May Be The Right Choice <a href="#why-spas-may-be-the-right-choice">#</a></h4>
<p>Editors frequently feature many updates to the same data (e.g., for every keystroke or mouse drag). Applying updates optimistically and only informing the server asynchronously of edits can deliver a superior experience across long editing sessions.</p>
<p>However, teams should be aware that editors may also perform double duty as viewers and that the weight of up-front bundles may not be reasonable for both cases. Worse, it can be hard to tease viewing sessions apart from heavy editing sessions at page load time.</p>
<p>Teams that succeed in these conditions build extreme discipline about the modularity, phasing, and order of delayed package loading based on user needs (e.g., only loading editor components users need when they require them). Teams that get stuck tend to fail to apply controls over which team members can approve changes to critical-path payloads.</p>
<h3 id="other-application-classes" tabindex="-1">Other Application Classes <a href="#other-application-classes">#</a></h3>
<p>Some types of apps are intrinsically interactive, focus on access to local device hardware, or center on manipulating media types that HTML doesn't handle intrinsically. Examples include <a href="https://www.onshape.com/en/features/drawings">3D CAD systems</a>, <a href="https://vscode.dev/">programming editors</a>, <a href="https://www.xbox.com/en-US/play">game streaming services</a>), <a href="https://proxx.app/">web-based games</a>, <a href="https://squoosh.app/">media-editing</a>, and <a href="https://webmidijs.org/showcase/">music-making systems.</a> These constraints often make complex, client-side JavaScript-based UIs a natural fit, but each should be evaluated in a similarly critical style as when building productivity applications:</p>
<ul>
<li>What are the critical user journeys</li>
<li>How deep will average sessions be?</li>
<li>What metrics will we track to ensure that performance remains acceptable?</li>
<li>How will we place tight controls on critical-path script and other resources?</li>
</ul>
<p>Success in these app classes is possible on the web, but extreme care is required.</p>
<blockquote>
<p><strong>A Word On Enterprise Software</strong>: Some of the worst performance disasters I've helped remediate are from a cateogry we can think of, generously, as "enterprise line-of-business apps". Dashboards, worfklow systems, corporate chat apps, that sort of thing.</p>
<p>Teams building these experiences frequently assert that <em>"startup performance isn't that important because people start our app in the morning and keep it open all day"</em>. At the limit, this can be true, but what they omit is that performance is <em>cultural</em>. A team that fails to define and measure critical user journeys that include loading will <em>absolutely</em> fail to define and measure interactivity metrics post-load.</p>
<p>The old saying <em>"how you do anything is how you do everything"</em> is never more true than in software usability.</p>
<p>One consequence cultures that fail to <a href="https://www.gov.uk/guidance/government-design-principles#first">put the user first</a> are products whose usability is so poor that attributes which <em>didn't</em> matter at the time of sale (like performance) become reasons to switch.</p>
<p>If you've ever had the distinct displeasure of using Concur or Workday, you'll understand what I mean. Challengers win business from these incumbents not by being wonderful, but simply by being usable. The incumbents are generally powerless to respond because their problems are now rooted deeply in the behaviours they rewarded in hiring and promotion along the way. The resulting management blindspot becomes a self-reinforcing norm that no single leader can shake.</p>
<p>This is, in part, why allowing a culture of user disrespect in favour of developer veneration is caustic to product usability brand value over time. The only antidote is to stamp it out wherever it arises by demanding user-focused realism in decisiomaking.</p>
</blockquote>
<h2 id="%22but...%22" tabindex="-1">"But..." <a href="#%22but...%22">#</a></h2>
<p>Managers and tech leads that have become wedded to frameworkism often have to work through a series of easily falsified rationales offered by other Over Reactors in service of their chosen ideology. Note, as you read, that none of these protests put the lived user experience front-and-centre. This admission by omission is a reliable property of the sorts of conversations that these sketches are drawn from.</p>
<h3 id="%22...we-need-to-move-fast%22" tabindex="-1">"...we need to move fast" <a href="#%22...we-need-to-move-fast%22">#</a></h3>
<p>This chestnut should always be answered with a question: <em>"for how long?"</em></p>
<p>This is because the dominant outcome of fling-stuff-together-with-NPM, feels-fine-on-my-$3K-laptop development is to cause teams to get stuck in the mud much sooner than anyone expects. From major accessibility defects to brand-risk levels of lousy performance, the consequence of this sort of thinking has been crossing my desk every week for a decade now.</p>
<p>The one thing I can tell you that all of these teams and products have in common is that <em>they are not moving faster</em>. Brands you've heard of and websites you used <em>this week</em> have come in for help, which we've dutifully provided. The general prescription is <em>spend a few weeks/months unpicking this Gordian knot of JavaScript.</em> The time spent in remediation <em>does</em> fix the revenue and accessibility problems that JavaScript exuberance cause, but teams are dead in the water while they belatedly add ship gates and bundle size controls and processes to prevent further regression.</p>
<p>This necessary, painful, and expensive remediation generally comes at the worst time and with little support, owing to the JS-industrial-complex's omerta. Managers trapped in these systems experience a sinking realisation that choices made in haste are not so easily revised. Complex, inscrutable tools introduced in the "move fast" phase are now systems that teams must dedicate time to learn, understand deeply, and affrimatively operate. All the while the pace of feature delivery is dramatically reduced.</p>
<p>This isn't what managers <em>think</em> they're signing up for when acccepting <em>"but we need to move fast!"</em></p>
<p>But let's take the assertion at face value and assume a team that <em>won't</em> get stuck in the ditch (🤞): the idea embedded in this statemet is, roughly, that there <em>isn't</em> time to do it right (so React?), but there <em>will be</em> time to do it over.</p>
<p>But this is in direct contention with identifying <a href="https://en.wikipedia.org/wiki/Product-market_fit">product-market-fit</a>.</p>
<p>Contra the received wisdom of valley-dwellers, the way to find who will want your product is to <a href="https://startupljackson.com/post/83244692828/html-first">make it as widely available as possible, <em>then</em> to add UX flourishes.</a></p>
<p>Teams I've worked with are frequently astonished to find that removing barriers to use opens up new markets, or improves margins and outcomes in parts of a world they had under-valued.</p>
<p>Now, if you're selling <a href="https://en.wikipedia.org/wiki/Veblen_good">Veblen goods,</a> by all means, prioritise anything but accessibility for folks on the margins. But in literally every other category, the returns to quality can be best understood as clarity of product thesis. A low-quality experience — which is what is being proposed when React is offered as an expedient — is a drag on the core growth argument for your service. And if the goal is scale, rather than exclusivity, building for legacy desktop browsers that Microsoft won't even sell you is a strategic error.</p>
<h3 id="%22...it-works-for-facebook%22" tabindex="-1">"...it works for Facebook" <a href="#%22...it-works-for-facebook%22">#</a></h3>
<p>To a statistical certainty, you aren't making Facebook. Your problems likely look nothing like Facebook's early 2010s problems, and even if they did, following their lead <a href="http://okayfail.com/garden/youre-not-facebook-why-use-their-tools.html">is a <em>terrible</em> idea.</a></p>
<p>And these tools <a href="https://treo.sh/sitespeed/m.facebook.com/vs/instagram.com/vs/www.threads.net?metrics=lcp%2Cr&amp;formFactor=phone"><em>aren't even working for Facebook</em>.</a> They just happen to be a monopoly in various social categories and so can afford to light money on fire. If that doesn't describe your situation, it's best not to overindex on narratives premised on Facebook's perceived success.</p>
<h3 id="%22...our-teams-already-know-react%22" tabindex="-1">"...our teams already know React" <a href="#%22...our-teams-already-know-react%22">#</a></h3>
<p>React developers <em>are web developers</em>. They have to operate in a world of CSS, HTML, JavaScript, and DOM. It's inescapable. This means that React is the most fungible layer in the stack. Moving between templating systems (which is what JSX is) is what web developers have done fluidly for more than 30 years. Even folks with deep expertise in, say, Rails and ERB, can easily knock out Django or Laravel or Wordpress or 11ty sites. There are differences, sure, but every web developer is a polyglot.</p>
<p>React knowledge is also not particularly valuable. Any team familiar with React's...baroque...conventions can easily master Preact, Stencil, Svelte, Lit, FAST, Qwik, or any of a dozen faster, smaller, reactive client-side systems that demand less mental bookkeeping.</p>
<h3 id="%22...we-need-to-be-able-to-hire-easily%22" tabindex="-1">"...we need to be able to hire easily" <a href="#%22...we-need-to-be-able-to-hire-easily%22">#</a></h3>
<p>The tech industry has just seen many of the most talented, empathetic, and user-focused engineers I know laid off for no reason other than their management couldn't figure out that there would be some mean reversion post-pandemic. Which is to say, there's a fire sale on talent right now, and you can ask for <em>whatever skills you damn well please</em> and get good returns.</p>
<p>If you cannot attract folks who know web standards and fundamentals, reach out. I'll personally help you formulate recs, recruiting materials, hiring rubrics, and promotion guides to value these folks the way you should: as underpriced heroes that will do incredible good for your products at a fraction of the cost of solving the next problem the React community is finally acknowledging that frameworkism itself caused.</p>
<h4 id="resumes-aren't-murder%2Fsuicide-pacts" tabindex="-1">Resumes Aren't Murder/Suicide Pacts <a href="#resumes-aren't-murder%2Fsuicide-pacts">#</a></h4>
<p><em>Even if</em> you decide you want to run interview loops to filter for React knowledge, that's not a good reason to use it! Anyone who can master the dark thicket of build tools, typescript foibles, and the million little ways that JSX's fork of HTML and JavaScript syntax trips folks up is <em>absolutely</em> good enough to work in a different system.</p>
<p>Heck, they're <em>already</em> working in an ever-shifting maze of faddish churn. <a href="https://polotek.net/posts/the-frontend-treadmill/">The treadmill is real,</a> which means that the question isn't <em>"will these folks be able to hit the ground running?"</em> (answer: no, they'll spend weeks learning your specific setup regardless), it's <em>"what technologies will provide the highest ROI over the life of our team?"</em></p>
<p>Given the <a href="">extremely high</a> costs of React and other frameworkist prescriptions, the odds that this calculus will favour the current flavour of the week over the lifetime of even a single project are vanishingly small.</p>
<h4 id="the-bootcamp-thing" tabindex="-1">The Bootcamp Thing <a href="#the-bootcamp-thing">#</a></h4>
<p>It makes me nauseous to hear managers denigrate talented engineers, and there seems to be a rash of it going around. The idea that folks who come out of bootcamps — folks who just <em>paid to learn whatever was on the syllabus</em> — aren't able or willing to pick up some alternative stack is bollocks.</p>
<p>Bootcamp grads might be junior, and they are generally steeped in varying strengths of frameworkism, but they're not stupid. They want to do a good job, and it's management's job to define what that is. Many new grads might know React, but they'll learn a dozen other tools along the way, and React is by far <a href="https://joshcollinsworth.com/blog/antiquated-react#frameworks-arent-as-hard-to-learn-anymore">the most (unnecessarily) complex of the bunch.</a> The idea that folks who have mastered the horrors of <code>useMemo</code> and friends can't take on board <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components">DOM lifecycle methods</a> or <a href="https://www.youtube.com/watch?v=cCOL7MC4Pl0">the event loop</a> or modern CSS is <em>insulting</em>. It's unfairly stigmatising and limits the organisation's potential.</p>
<p>In other words, definitionally atrocious management.</p>
<h3 id="%22...everyone-has-fast-phones-now%22" tabindex="-1">"...everyone has fast phones now" <a href="#%22...everyone-has-fast-phones-now%22">#</a></h3>
<p>For more than a decade, the core premise of frameworkism has been that client-side resources are cheap (or are getting increasingly inexpensive) and that it is, therefore, reasonable to trade some end-user performance for developer convenience.</p>
<p>This has been an absolute debacle. Since at least 2012 onward, the rise of mobile falsified this contention, <a href="https://infrequently.org/series/performance-inequality/">and (as this blog has meticulously catalogued) we are only <em>just</em> starting to turn the corner.</a></p>
<p>Frameworkist assertions that <em>"everyone has fast phones"</em> is many things, but first and foremost it's an admission that the folks offering this out don't know what they're talking about, and they hope you don't either.</p>
<p>No business trying to make it on the web can afford what these folks are selling, and you are under no obligation to offer your product as sacrifice to a false god.</p>
<h3 id="%22...react-is-industry-standard%22" tabindex="-1">"...React is industry-standard" <a href="#%22...react-is-industry-standard%22">#</a></h3>
<p>This is, at best, a comforting fiction.</p>
<p>At worst, it's a knowing falsity that serves to omit the variability in React-based stacks because, you see, React isn't one thing. It's more of a lifestyle, complete with choices to make about React itself (function components or class components?) languages and compilers (typescript or nah?), package managers and dependency tools (npm? yarn? pnpm? turbo?), bundlers (webpack? esbuild? swc? rollup?), meta-tools (vite? turbopack? nx?), "state management" tools (redux? mobx? apollo? something that actually <em>manages</em> state?) and so on and so forth. And that's before we discuss plugins to support different CSS transpilation or the totally optional side-quests that frameworkists have led many of the teams I've consulted with down; "CSS-in-JS" being one particularly risible example.</p>
<p>Across more than 100 consulting engagements, I've never seen two identical React setups save smaller cases where developers have yet to add to the defaults of Create React App (which itself changed dramatically over the years before it finally being removed from the React docs as the best way to get started).</p>
<p>There's nothing standard about any of this. It's all change, all the time, and anyone who tells you differently is not to be trusted.</p>
<h4 id="the-bare-(assertion)-minimum" tabindex="-1">The Bare (Assertion) Minimum <a href="#the-bare-(assertion)-minimum">#</a></h4>
<p>Hopefully, you'll forgive a digression into how the <em>"React is industry standard"</em> misdirection became so embedded.</p>
<p>Given the overwhelming evidence that this stuff <a href="https://treo.sh/sitespeed/www.instagram.com?formFactor=phone&amp;mapMetric=r&amp;metrics=lcp%2Cr">isn't even working</a> on the sites of the <a href="https://treo.sh/sitespeed/www.airbnb.com?formFactor=phone&amp;mapMetric=r&amp;metrics=lcp%2Cr">titular React poster children,</a> how did we end up with React in so many nooks and crannies of contemporary frontend?</p>
<p>Pushy know-it-alls, that's how. Frameworkists have a way of hijacking every conversation with bare assertions like <em>"virtual DOM means it's fast"</em> without ever understanding anything about how browsers work, let alone the GC costs of their (extremely chatty) alternatives. This same ignorance allows them to confidently assert that React is "fine" when cheaper alternatives exist in every dimension.</p>
<p>These are not serious people. You do not have to take them seriously. But you <em>do</em> have to oppose them and create data-driven structures that put users first. The long-term costs of these errors are enormous, as witnessed by the parade of teams needing our help to achieve minimally decent performance using stacks that were supposed to be "performant" (sic).</p>
<h3 id="%22...the-ecosystem...%22" tabindex="-1">"...the ecosystem..." <a href="#%22...the-ecosystem...%22">#</a></h3>
<p>Which part, exactly? Be <em>extremely</em> specific. Which packages are so valuable, yet wedded entirely to React, that a team should not entertain alternatives? Do they <em>really</em> not work with Preact? How much money <em>exactly</em> is the right amount to burn to use these libraries? Because that's the debate.</p>
<p>Even if you get the benefits of "the ecosystem" at Time 0, why do you think that will continue to pay out?</p>
<p>Every library is presents stochastic risk of abandoment. Even the <a href="https://create-react-app.dev/">most heavily used systems</a> fall out of favour with the JS-industrial-complex's in-crowd, stranding you in the same position as you'd have been in if you accepted ownership of more of your stack up-front, but with less experience and agency. Is that a good trade? Does your boss agree?</p>
<p>And if you don't mind me asking, <a href="https://dev.to/srmagura/why-were-breaking-up-wiht-css-in-js-4g9b">how's that "CSS-in-JS" adventure working out?</a> Still writing class components, or did you have a big forced (and partial) migration that's still creating headaches?</p>
<p>The truth is that every single package that is part of a repo's <code>devDependencies</code> is, or will be, fully owned by the consumer of the package. The only bulwark against uncomfortable surprises, then, is to consider NPM dependencies like a sort of high-interest debt <a href="https://en.wikipedia.org/wiki/Collateral_(finance)">collateralized</a> by future engineering capacity.</p>
<p>The best way to minimize costs, then, is to fully examine and approve <em>the entire chain of dependencies</em> that are constitute your UI and all of the build systems that participate in app construction. If your team is not comfortable agreeing to own, patch, and improve every single one of those systems, they should not be part of your stack.</p>
<h3 id="%22...next.js-can-be-fast-(enough)%22" tabindex="-1">"...Next.js can be fast (enough)" <a href="#%22...next.js-can-be-fast-(enough)%22">#</a></h3>
<p>Do you feel lucky, punk? Do you?</p>
<p>Because you'll have to be lucky to <a href="https://infrequently.org/2024/10/platforms-are-competitions/#fn-failure-on-repeat-2">beat the odds.</a></p>
<p>Sites built with Next.js perform materially worse than those from HTML-first systems like <a href="https://www.11ty.dev/">11ty</a>, <a href="https://astro.build/">Astro</a>, et al.</p>
<p>It simply does not scale, and the fact that it drags React behind it like a ball and chain is a double demerit. The <a href="https://fediverse.zachleat.com/@zachleat/111524558114433017">chonktastic default payload</a> of delay-loaded JS in any Next.js site will compete with ads and other business-critical deferred content for bandwidth, and that's before any custom components or routes are added. Which is to say, Next.js is a fast way to lose a lot of money while getting <a href="https://thenewstack.io/opennext-gets-closer-to-making-next-js-truly-portable/">locked in to a VC-backed startup's proprietary APIs.</a></p>
<p>Next.js starts bad and only gets worse from a shocking baseline. No wonder the only Next sites that seem to perform well are those that enjoy overwhelmingly wealthy userbases, hand-tuning assistance from Vercel, or both.</p>
<p>So, do you feel lucky?</p>
<h3 id="%22...react-native!%22" tabindex="-1">"...React Native!" <a href="#%22...react-native!%22">#</a></h3>
<p>React Native is a good way to make a slow app that <a href="https://www.sitepoint.com/react-native-apps-performance-tips/">requires constant hand-tuning</a> and an <em>excellent</em> way to make <a href="https://treo.sh/sitespeed/bsky.app/vs/twitter.com/vs/elk.zone?formFactor=phone&amp;metrics=lcp%2Cr">a terrible website.</a> It has also been <a href="https://medium.com/airbnb-engineering/sunsetting-react-native-1868ba28e30a">abandoned by it's poster children.</a></p>
<p>Companies that want to deliver compelling mobile experiences into app stores from the same codebase as their web site are better served investigating <a href="https://developer.chrome.com/docs/android/trusted-web-activity">Trusted Web Activities</a> and <a href="https://www.pwabuilder.com/">PWABUilder.</a> If those don't work, <a href="https://capacitorjs.com/">Capacitor</a> and <a href="https://cordova.apache.org/">Cordova</a> can deliver similar benefits. These approaches make most native capabilities available, but centralises UI investment in the web side, providing visibility and control via a single execution path. This, in turn, reduces duplicate optimisation and accessibility headaches.</p>
<h2 id="references" tabindex="-1">References <a href="#references">#</a></h2>
<p>These are essential guides for frontend realism. I recommend interested tech leads, engineering managers, and product managers digest them all:</p>
<ul>
<li><a href="https://www.gov.uk/service-manual/technology/using-progressive-enhancement">"Building a robust frontend using progressive enhancement"</a> from the UK's Government Digital Service.</li>
<li><a href="https://muan.co/posts/javascript">"JavaScript dos and donts"</a> by Github alumnus Mu-An Chiou.</li>
<li><a href="https://crukorg.github.io/engineering-guidebook/docs/best_practices/frontend/choosing_your_stack/">"Choosing Your Stack"</a> from Cancer Research UK</li>
<li><a href="https://alexsexton.com/blog/2014/11/the-monty-hall-rewrite">"The Monty Hall Rewrite"</a> by Alex Sexton, which breaks down the essential ways that a failure to run an honest bakeoff harms decision-making.</li>
<li><a href="https://joshcollinsworth.com/blog/antiquated-react">"Things you forgot (or never knew) because of React"</a> by Josh Collinsworth, which enunciates just how baroque and parochial React's culture has become.</li>
<li><a href="https://polotek.net/posts/the-frontend-treadmill/">"The Frontend Treadmill"</a> by Marco Rogers explains the costs of frameworkism better than I ever could.</li>
<li><a href="https://kellanem.com/notes/new-tech">"Questions for a new technology"</a> by Kellan Elliott-McCrea and Glyph's <a href="https://blog.glyph.im/2024/07/against-innovation-tokens.html">"Against Innovation Tokens"</a>. Together, they set a well-focused lens for thinking about how frameworkism is antithetical to functional engineering culture.</li>
</ul>
<p>These pieces are from teams and leaders that have succeeded in outrageously effective ways by applying the realist tentants of <em>looking around for themselves</em> and <em>measuring things.</em> I wish you the same success.</p>


<hr role="heading" aria-level="2" aria-label="Footnotes">
<section>
  <ol><li id="fn-why-not-1"><p>Why not React? <a href="https://www.zachleat.com/web/react-criticism/">Dozens of reasons,</a> but a shortlist must include:</p>
<ul>
<li><strong>React is legacy technology.</strong> It was built for a world where IE 6 still had measurable share, and it shows.
<ul>
<li>React's synthetic event system and hard-coded element list are a direct consequence of IE's limitations. Independently, these create portability and performance hazards. Together, they become a driver of lock-in.</li>
<li>No contemporary framework contains equivalents because <em>no other framework is fundamentally designed around the need to support IE</em>.</li>
<li>It's beating a dead horse, but <a href="https://support.microsoft.com/en-us/office/which-browsers-work-with-microsoft-365-for-the-web-and-microsoft-365-add-ins-ad1303e0-a318-47aa-b409-d3a5eb44e452"><em>Micrsoft's</em> flagship apps do not support IE</a> and <a href="https://learn.microsoft.com/en-us/lifecycle/products/internet-explorer-11">you cannot buy support for IE.</a> All versions of IE have been <a href="https://techcommunity.microsoft.com/blog/windows-itpro-blog/internet-explorer-11-desktop-app-retirement-faq/2366549">forcibly removed from Win10</a> and <a href="https://gs.statcounter.com/browser-market-share/desktop/worldwide">it has not appeared above the noise</a> in global browser market share stats for <a href="https://gs.statcounter.com/browser-market-share/desktop/worldwide/2020">more than 4 years.</a> New projects will never encouter IE, and it's vanishingly unlikely that existing applications need to support it.</li>
</ul>
</li>
<li><strong>Virtual DOM <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">was <em>never</em> fast.</a></strong>
<ul>
<li>React <a href="https://web.archive.org/web/diff/20140613075553/20150403030402/http://facebook.github.io/react/">backed away</a> from incorrect, overheated performance claims almost immediately.<sup><a href="#fn-ignorance-or-malice-10" id="fnref-ignorance-or-malice-10">[10]</a></sup></li>
<li>In addition to being unnecessary to achieve reactivity, React's diffing model and poor support for dataflow management conspire to <a href="https://www.eventbrite.com/engineering/a-story-of-a-react-re-rendering-bug/">regularly generate</a> <a href="">extra main-thread work</a> at inconvenient times. The "solution" is to <a href="https://www.joshwcomeau.com/react/why-react-re-renders/">learn (and zealously apply)</a> a set of <a href="https://alexsidorenko.com/blog/react-render-usememo">extremely baroque</a>, React-specific solutions to problems React itself causes.</li>
<li>The only thing that react's doubled-up work model can, in theory, make faster faster is that it can help programmers avoid <a href="https://web.dev/articles/avoid-large-complex-layouts-and-layout-thrashing#avoid_forced_synchronous_layouts">reading back style and layout information at inconvenient times.</a></li>
<li>In practice, React does not actually prevent this anti-pattern. Nearly every React app that crosses my desk is littered with layout thrashing bugs.</li>
<li>The only defensible performance claims Reactors make for their work-doubling system are phrased as a trade; e.g. <em>"CPUs are fast enough now that we can afford to do work twice for developer convenience."</em>
<ul>
<li>Except they aren't. CPUs stopped getting faster about the same time as Reactors began to perpetuate this myth. This did not stop them from pouring JS into the ecosystem in as though the old trends had held, with <a href="https://infrequently.org/2024/08/object-lesson/">predictably disasterous results</a>
<picture>
<a target="_new" href="https://infrequently.org/2024/01/performance-inequality-gap-2024/single_core_scores.png">
<img src="https://infrequently.org/2024/01/performance-inequality-gap-2024/single_core_scores.png" alt="Sales volumes of the high-end devices that continues to get faster stagnated over the past decade. Meanwhile, the low end exploded in volume whole remaining stubbornly fixed in performance." decoding="async" loading="lazy"></a>
</picture></li>
<li>It isn't even necessary to do all the work twice to get reactivity! Every other reactive component system from the past decade is significantly more efficient, <a href="https://bundlephobia.com/scan-results?packages=react,react-dom,preact,lit,svelte,vue,solid-js,lwc">weighs less on the wire,</a> and preserves the advantages of reactivitiy <em>without</em> creating horrible "re-render debugging" hunts that take weeks away from getting things done.</li>
</ul>
</li>
</ul>
</li>
<li><strong>React's thought leaders have been wrong about <a href="https://infrequently.org/series/performance-inequality/">frontend's constraints for more than a decade.</a></strong>
<ul>
<li>Why you trust them now? <a href="https://cruxvis.withgoogle.com/#/?view=cwvsummary&amp;url=https%3A%2F%2Fm.facebook.com%2F&amp;identifier=origin&amp;device=PHONE&amp;periodStart=0&amp;periodEnd=-1&amp;display=p75s">Their own websites</a> perform <a href="https://cruxvis.withgoogle.com/#/?view=cwvsummary&amp;url=https%3A%2F%2Fwww.instagram.com%2F&amp;identifier=origin&amp;device=PHONE&amp;periodStart=0&amp;periodEnd=-1&amp;display=p75s">poorly</a> in the real world.</li>
</ul>
</li>
<li><strong>The money you'll save can be measured in truck-loads.</strong>
<ul>
<li>Teams that correctly cabin complexity to the server side can avoid paying inflated salaries to begin with.</li>
<li>Teams that <em>do</em> build SPAs can more easily control the costs of those architectures by starting with a cheaper baseline and building a <a href="https://infrequently.org/2022/05/performance-management-maturity/">mature performance culture</a> into their organisations from the start.</li>
</ul>
</li>
<li>Not for nothing, but avoiding React will insulate your team from the assertion-heavy, data-light React discourse.</li>
</ul>
<p>Why pick a slow, backwards-looking framework whose architecture is compromised to serve legacy browsers when smaller, faster, better alternatives with all of the upsides (and none of the downsides) have been production-ready and successful <em>for years</em>? <a href="#fnref-why-not-1">⇐</a></p>
</li>
<li id="fn-the-devils-computer-2"><p>Frontend web development, like other types of client-side programming, is under-valued by "generalists" who do not respect just how <em>freaking hard</em> it is to deliver fluid, interactive experiences on devices you don't own and can't control. Web development turns this up to eleven, presenting a wicked effective compression format (HTML &amp; CSS) for UIs but forcing most experiences to be downloaded at runtime across high-latency, narrowband connections with little to no caching. On low-end devices. With no control over which browser will execute the code.</p>
<p>And yet, browsers and web developers frequently collude to deliver outstanding interactivity under these conditions. Often enough, that "generalists" don't give the repeated miracle of HTML-centric Wikipedia and MDN articles loading consistently quickly, as they gleefully clog those narrow pipes with amounts of heavyweight JavaScript that are incompatible with consistently delivering good user experience. All because they neither understand nor respect client-side constraints.</p>
<p>It's enough to make thoughtful engineers tear their hair out. <a href="#fnref-the-devils-computer-2">⇐</a> <a href="#fnref-the-devils-computer-2:1">⇐</a></p>
</li>
<li id="fn-ballot-stuffing-3"><p>Tom Stoppard's classic quip that <em>"it's not the voting that's democracy; it's the counting"</em> chimes with the importance of impartial and objective criteria for judging the results of bakeoffs.</p>
<p>I've witnessed more than my fair share of stacked-deck proof-of-concept pantomimes, often inside very large organisations with tremendous resources and managers who say all the right things. But honesty demands more than lip service.</p>
<p>Organisations that are just looking for a baroque way to excuse their pre-ordained outcomes should skip the charade. It will only make good people cynical and increase resistance to your agenda. Teams that want to set bales of benajmins on fire because frameworkism shouldn't be afraid to say what they want. They were going to get it anyway, after all. <a href="#fnref-ballot-stuffing-3">⇐</a></p>
</li>
<li id="fn-cut-lines-4"><p>An example of easy cut lines for teams considering contemporary development might be browser support versus base bundle size.</p>
<p>In 2024, no new application will need to support IE or even legacy versions of Edge. They are not a measurable part of the ecosystem. This means that tools that took the design constraints imposed by IE as a given can be discarded from consideration, given that the extra client-side weight they required to service IE's quirks makes them uncompetitive from a bundle size perspective.</p>
<p>This eliminates React, Angular, and Ember from consideration without a single line of code being written; a tremendous savings of time and effort.</p>
<p>Another example is lock-in. Do systems support interoperability across tools and frameworks? Or will porting to a different system require a total rewrite? A decent proxy for this choice is Web Components support. Teams looking to avoid lock-in can remove systems that do not support Web Components as an export and import format from consideration. This will still leave many contenders, but management can rest assured they will not leave the team high-and-dry.<sup><a href="#fn-reversability-13" id="fnref-reversability-13">[13]</a></sup> <a href="#fnref-cut-lines-4">⇐</a></p>
</li>
<li id="fn-buck-passing-5"><p>The stories we hear when interviewing members of these teams have an unmistakable buck-passing flavour. Engineers will claim (without evidence) that React is a great<sup><a href="#fn-or-whatever-12" id="fnref-or-whatever-12">[12]</a></sup> choice for their blog/e-commerce/marketing-microsite because <em>"it needs to be interactive"</em> -- by which they mean it has a Carousel and maybe a menu and some parallax scrolling. None of this is an argument for React <em>per se</em>, but it can sound plausible to managers who trust technical staff about technical matters.</p>
<p>Others claim that "it's an SPA". <em>Should</em> it be a Single Page App? Most are unprepared to answer that question for the simple reason they haven't thought it through.<sup><a href="#fn-rubrics-8" id="fnref-rubrics-8:2">[8:2]</a></sup></p>
<p>For their part, contemporary product managers seem to spend a great deal of time doing things that do not have any relationship to managing the essential qualities of their products. Most need help making sense of the RUM data already available to them. Few are in touch with the device and network realities of their current and future (🤞) users. PMs that clearly articulate critical-user-journeys for their teams are like hen's teeth. And I can count on one hand teams that have run bakeoffs — without resorting to binary. <a href="#fnref-buck-passing-5">⇐</a></p>
</li>
<li id="fn-off-brand-therapy-6"><p>It's no exaggeration to say that team leaders encountering evidence that their React (or Angular, etc.) technology choices are letting down users and the business <em>go through some things</em>.</p>
<p>Follow-the-herd choice-making is an adaptation to prevent their specific decisions from standing out — tall poppies and all that — and it's uncomfortable when those decisions receive belated scrutiny. But when the evidence is incontrovertible, needs must. This creates cognitive dissonance.</p>
<p>Few teams are so entitled and callous that they wallow in denial. Most want to improve. They don't come to work every day to make a <em>bad</em> product; they just thought the herd knew more than they did. It's disorienting when that turns out not to be true. That's more than understandable.</p>
<p>Leaders in this situation work through the <a href="https://en.wikipedia.org/wiki/Five_stages_of_grief">stages of grief</a> in ways that speak to their character.</p>
<p>Strong teams own the reality and look for ways to learn more about their users and the constraints that should shape product choices. The goal isn't to justify <em>another</em> rewrite but to find targets team can work towards, breaking down complexity into actionable next steps. This is hard and often unfamiliar work, but it is rewarding. Setting accurate goalposts can also help the team take credit as they make progress remediating the current mess. These are all markers of teams on the way to <a href="https://infrequently.org/2022/05/performance-management-maturity/">improving their performance management maturity.</a></p>
<p>Others can get stuck in anger, bargaining, or depression. Sadly, these teams are taxing to help; some revert to the mean. Supporting engineers and PMs through emotional turmoil is a big part of a performance consultant's job. The stronger the team's attachment to <a href="https://infrequently.org/2023/02/the-market-for-lemons/">React community narratives</a>, the harder it can be to accept responsibility for defining success in terms that map directly to the success of a product's users. Teams climb out of this hole when they base constraints for choosing technologies on users' lived experiences within their own products.</p>
<p>But consulting experts can only do so much. Tech leads and managers that continue to prioritise "Developer Experience" (without metrics, natch) and "the ecosystem" (pray tell, which parts?) in lieu of user outcomes can remain beyond reach, no matter how much empathy and technical analysis is provided. Sometimes, you have to cut bait and hope time and the costs of ongoing failure create the necessary conditions for change. <a href="#fnref-off-brand-therapy-6">⇐</a></p>
</li>
<li id="fn-shortcuts-7"><p>Most are substituting (perceived) popularity for the work of understanding users and their needs. Starting with user needs creates constraints that teams can then use to work backwards from when designing solutions to accomplish a particular user experience.</p>
<p>Subbing in short-term popularity contest winners for the work of understanding user needs goes hand-in-glove with failures to set and respect business constraints. It's common to hear stories of companies shocked to find the PHP/Python/etc. system they are replacing with the New React Hotness will require multiples of currently allocated server resources <em>for the same userbase</em>. The impacts of inevitably worse client-side lag cost, too, but only show up later. And all of these costs are on top of the larger salaries and bigger teams the New React Hotness invariably requires.</p>
<p>One team I chatted to shared that their avoidance of React was tantamount to a trade secret. If their React-based competitors understood how expensive React stacks are, they'd lose their (considerable) margin advantage. Wild times. <a href="#fnref-shortcuts-7">⇐</a></p>
</li>
<li id="fn-rubrics-8"><p>Should a site be built as a Single Page App?</p>
<p>A good way to work this question is to ask <em>"what's the point of an SPA?"</em>. The answer is that they <em>can</em> (in theory) reduce interaction latency, which <em>implies</em> many interactions per session. It's also an (implicit) claim about the costs of loading code up-front versus on-demand. This sets us up to create a rule of thumb.</p>
<picture>
  <img src="https://infrequently.org/2024/11/if-not-react-then-what/spa-decision-tree.avif" alt="Should this site be built as a Single Page App? A decision tree. (hint: at best, maybe)" decoding="async" loading="lazy">
</picture>
<p>Sites should only be built as SPAs, or with SPA-premised technologies if (and only if):</p>
<ul>
<li>They are known to have long sessions (more than ten minutes) on average</li>
<li>More than ten updates are applied to the <em>same</em> (primary) data</li>
</ul>
<p>This instantly disqualifies almost every e-commerce experience, for example, as sessions generally involve traversing pages with entirely different primary data rather than updating a subset of an existing UI. Most also feature average sessions that fail the length and depth tests. Other common categories (blogs, marketing sites, etc.) are even easier to disqualify. At most, these categories can stand a dose of progressive enhancement (but not too much!) owing to their shallow sessions.</p>
<p>What's left? Productivity and social apps, mainly.</p>
<p>Of course, there are many sites with bi-modal session types or sub-apps, all of which might involve different tradeoffs. For example, a blogging site is two distinct systems combined by a database/CMS. The first is a long-session, heavy interaction post-writing and editing interface for a small set of users. The other is a short-session interface for a much larger audience who mostly interact by loading a page and then scrolling. As the browser, not developer code, handles scrolling, we omit from interaction counts. For most sessions, this leaves us only a single data update (initial page load) to divide all costs by.</p>
<p>If the denominator of our equation is always close to one, it's nearly impossible to justify extra weight in anticipation of updates that will likely never happen.<sup><a href="#fn-out-with-the-bathwater-11" id="fnref-out-with-the-bathwater-11">[11]</a></sup></p>
<p>To formalise slightly, we can understand average latency as the sum of latencies in a session, divided by the number of interactions. For multi-page architectures, a session's average latency (<math><msub><mi>L</mi><mi>avg</mi></msub></math>) is simply a session's summed <a href="https://web.dev/articles/lcp"><math><mi>LCP</mi></math></a>'s divided by the number of navigations in a session (<math><mi>N</mi></math>):</p>
<math display="block">
  <msub>
    <msup>
      <mi>L</mi>
      <mi>m</mi>
    </msup>
    <mi>avg</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <munderover>
        <mo>∑</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mi>N</mi>
      </munderover>
      <mi>LCP</mi>
      <mo>(</mo>
      <mi>i</mi>
      <mo>)</mo>
    </mrow>
    <mi>N</mi>
  </mfrac>
</math>
<p>SPAs need to add initial navigation latency to the latencies of all other session interactions (<math><mi>I</mi></math>). The total number of interactions in a session <math><mi>N</mi></math> is:</p>
<math display="block">
  <mi>N</mi><mo>=</mo><mn>1</mn><mo>+</mo><mi>I</mi>
</math>
<p>The general form is of SPA average latency is:</p>
<math display="block">
  <msub>
    <mi>L</mi>
    <mi>avg</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>latency</mi>
      <mo>(</mo>
      <mi>navigation</mi>
      <mo>)</mo>
      <mo>+</mo>
      <munderover>
        <mo>∑</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mi>I</mi>
      </munderover>
      <mi>latency</mi>
      <mo>(</mo>
      <mi>i</mi>
      <mo>)</mo>
    </mrow>
    <mi>N</mi>
  </mfrac>
</math>
<p>We can handwave a bit and use <a href="https://web.dev/articles/inp"><math><mi>INP</mi></math></a> <em>for each individual update</em> (via the Performance Timeline) as our measure of in-page update lag. This leaves some room for gamesmanship(the React ecosystem is famous for attempting to duck metrics accountability with scheduling shenanigans), so a real measurement system will need to substitute end-to-end action completion (including server latency) for <math><mi>INP</mi></math>, but this is a reasonable bootstrap.</p>
<p><math><mi>INP</mi></math> also helpfully omits scrolling unless the programmer does something problematic. This is correct for the purposes of metric construction as scrolling gestures are generally handled by the browser, not application code, and our metric should only measure what developers control. SPA average latency simplifies to:</p>
<math display="block">
  <msub>
    <msup>
      <mi>L</mi>
      <mi>s</mi>
    </msup>
    <mi>avg</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>LCP</mi>
      <mo>+</mo>
      <munderover>
        <mo>∑</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mi>I</mi>
      </munderover>
      <mi>INP</mi>
      <mo>(</mo>
      <mi>i</mi>
      <mo>)</mo>
    </mrow>
    <mi>N</mi>
  </mfrac>
</math>
<p>As a metric for architecture, this is simplistic and fails to capture <a href="https://en.wikipedia.org/wiki/Variance">variance</a>, which SPA defenders will argue matters greatly. How might we incorporate it?</p>
<p>Variance (<math><msup><mi>σ</mi><mn>2</mn></msup></math>) across a session is straightforward if we have logs of the latencies of all interactions and an understanding of <a href="https://newrelic.com/blog/best-practices/expected-distributions-website-response-times">latency distributions</a>. Assuming latencies follows the <a href="https://en.wikipedia.org/wiki/Erlang_distribution">Erlang distribution</a>, we might have work to do to assess variance, except that complete logs simplify this to the usual population variance formula. Standard deviation (<math><mi>σ</mi></math>) is then just the square root:</p>
<math display="block">
  <msup>
    <mi>σ</mi>
    <mn>2</mn>
  </msup>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mo>∑</mo>
      <msup>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>-</mo>
          <mi>μ</mi>
          <mo>)</mo>
        </mrow>
        <mn>2</mn>
      </msup>
    </mrow>
    <mi>N</mi>
  </mfrac>
</math>
<p>Where <math><mi>μ</mi></math> is the mean (average) of the population <math><mi>X</mi></math>, the set of measured latencies in a session.</p>
<p>We can use these tools to compare architectures and their outcomes, particularly the effects of larger up-front payloads for SPA architecture for sites with shallow sessions. Suffice to say, the smaller the deonominator (i.e., the shorter the session), the worse average latency will be for JS-oriented designs and the more sensitive variance will be to population-level effects of hardawre and networks.</p>
<p>A fuller exploration will have to wait for a separate post. <a href="#fnref-rubrics-8">⇐</a> <a href="#fnref-rubrics-8:1">⇐</a> <a href="#fnref-rubrics-8:2">⇐</a></p>
</li>
<li id="fn-no-ssr-doesnt-count-9"><p>Certain frameworkists will claim that <em>their</em> framework is fine for use in informational scenarios because their systems do "Server-Side Rendering" (a.k.a., "SSR").</p>
<p>Parking discussion of the linguistic crime that "SSR" represents for the moment, we can reject these claims by substituting a test: does the tool in question send a copy of a library to support SPA navigations down the wire by default?</p>
<p>This test is helpful, as it shows us that React-based tools like Next.js are wholly unsuitable for this class of site, while React-friendly tools like <a href="https://astro.build/">Astro</a> are appropriate.</p>
<p>We lack a name for this test today, and I hope readers will suggest one. <a href="#fnref-no-ssr-doesnt-count-9">⇐</a></p>
</li>
<li id="fn-ignorance-or-malice-10"><p>React's initial claims of good performance <em>because</em> it used a virtual DOM were never true, and the React team was forced to retract them by 2015. But like many retracted, <a href="https://gh.bmj.com/content/5/11/e003719.abstract">zombie ideas</a>, there seems to have been no reduction in the rate of junior engineers regurgitating this long-falsified idea as a reason to continue to choose React.</p>
<p>It matters, then, how such a <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead#How-did-the-meme-start">baldly incorrect claim was offered in the first place.</a> The options are unappetising; either the React team knew their work-doubling machine was not fast but allowed others to think it was, or they didn't know but should have.<sup><a href="#fn-private-views-14" id="fnref-private-views-14">[14]</a></sup></p>
<p>Neither suggest the sort of grounded technical leadership that developers or businesses should invest heavily in. <a href="#fnref-ignorance-or-malice-10">⇐</a></p>
</li>
<li id="fn-out-with-the-bathwater-11"><p>It should go without saying, but sites that aren't SPAs shouldn't use tools that are premised entirely on optimistic updates to client-side data because sites that aren't SPAs shouldn't be paying the cost of creating a (separate, expensive) client-side data store separate from the DOM representation of HTML.</p>
<p>Which is the long way of saying that if there's React or Angular in your blogware, 'ya done fucked up, son. <a href="#fnref-out-with-the-bathwater-11">⇐</a></p>
</li>
<li id="fn-or-whatever-12"><p>When it's pointed out that React is, in fact, <em>not</em> great in these contexts, the excuses come fast and thick. It's generally less than 10 minutes before they're rehashing some variant of how some <em>other</em> site is fast (without traces to prove it, obvs), and <em>it</em> uses React, so React is fine.</p>
<p>Thus begins an infinite regression of easily falsified premises.</p>
<p>The folks dutifully shovelling this bullshit aren't consciously trying to invoke Brandolini's Law in their defence, but that's the net effect. It's exhausting and principally serves to convince the challenged party <em>not</em> that they should try to understand user needs and build to them, but instead that you're an asshole. <a href="#fnref-or-whatever-12">⇐</a></p>
</li>
<li id="fn-reversability-13"><p>Most managers pay lip service to the idea of preferring reversible decisions. Frustratingly, failure to put this into action is in complete alignment with social science research into the <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103113001443">psychology of decision-making biases</a> (<a href="https://pure.uva.nl/ws/files/1746420/130125_06.pdf">open access PDF summary</a>).</p>
<p>The job of managers, then, is to <em>manage</em> these biases. Working against them involves building processes and objective frames of reference to nullify their effects. It isn't particularly challenging, but it is <em>work</em>. Teams that do not build this discipline pay for it dearly, particularly on the front end, where we program the devil's computer.<sup><a href="#fn-the-devils-computer-2" id="fnref-the-devils-computer-2:1">[2:1]</a></sup></p>
<p>But make no mistake: choosing React is a one-way door; an irreversible decision that is costly to relitigate. Teams that buy into React implicitly opt into leaky abstractions like timing quirks of React's (unique; as in, nobody else has one because it's costly and slow) synthentic event system and non-portable concepts like portals. Teams that invest in React are stuck, and the paths out are challenging.</p>
<p>This will seem comforting, but the long-run maintenance costs of being trapped in this decision are excruciatingly high. No wonder, then, that Reactors believe they should command a salary premium. Whatcha gonna do, switch? <a href="#fnref-reversability-13">⇐</a></p>
</li>
<li id="fn-private-views-14"><p>Where do I come down on this?</p>
<p>My interactions with React team members over the years have combined with their confidently incorrect public statements about how browsers work to convince me that honest ignorance about their system's performance sat underneath misleading early claims.</p>
<p>This was likely exascerbated by a competitive landscape in which their customers (web developers) were unable to judge the veracity of the assertions, and a deference to authority; <em>Facebook</em> wouldn't mislead anyone, right? The need for an edge against Google's Angular and other successful systems in the market also likely played a role. It's underappreciated how tenuous the position of frontend and client-side framework teams are within Big Tech companies. The Closure <a href="https://github.com/google/closure-library">library</a> and compiler that powered Google's most successful web apps (Gmail, Docs, Drive, Sheets, Maps, etc.) <em>was not staffed</em> for most of its history. It was literally a 20% project that the entire company depended on. For the React team to justify headcount within Facebook, public success was likely essential.</p>
<p>Understood in context, I don't entirely excuse the React team for their early errors, but they are understandable. What's not forgivable are the material and willful omissions by Facebook's React team once the evidence of terrible performance began to accumulate. The React team took no responsibility, did not explain the constraints that Facebook applied to their JS-based UIs to make them perform as well as they do — particularly on mobile — and benefited greatly from <a href="https://infrequently.org/2023/02/the-market-for-lemons/">pervasive misconceptions</a> that continue to cast React is a better light than hard evidence can support. <a href="#fnref-private-views-14">⇐</a></p>
</li>
</ol>
</section>

    
    
  </article>
      <!-- END_BLOCK_CONTENT -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sol-Ark manufacturer reportedly disables all Deye inverters in the US (224 pts)]]></title>
            <link>https://solarboi.com/2024/11/17/sol-ark-oem-disables-all-deye-inverters-in-the-us/</link>
            <guid>42279010</guid>
            <pubDate>Sat, 30 Nov 2024 02:51:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solarboi.com/2024/11/17/sol-ark-oem-disables-all-deye-inverters-in-the-us/">https://solarboi.com/2024/11/17/sol-ark-oem-disables-all-deye-inverters-in-the-us/</a>, See on <a href="https://news.ycombinator.com/item?id=42279010">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
    </channel>
</rss>