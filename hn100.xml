<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Feb 2025 20:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I Went to SQL Injection Court (236 pts)]]></title>
            <link>https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</link>
            <guid>43175628</guid>
            <pubDate>Tue, 25 Feb 2025 18:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/">https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</a>, See on <a href="https://news.ycombinator.com/item?id=43175628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          

<p>Should public bodies in Illinois, like cities and school districts
and sheriff’s departments, be allowed to hide information from Freedom
of Information requests by keeping them in databases? That question is
before the 104th Illinois General Assembly, thanks to a bill sponsored
by Donald P. DeWitte, elected state senator by the wise citizens of
Batavia and Elgin (motto: “The City In The Suburbs”; indeed), and
prompted in part by my friend Matt Chapman.</p>
<p>I play a very small part in this story, so I get to tell it.</p>
<h2 id="background">Background</h2>
<p>Illinois has an <a href="https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=85">excellent,
toothy FOIA statute</a>.</p>
<p>With <a href="https://casetext.com/statute/illinois-compiled-statutes/government/chapter-5-general-provisions/subchapter-freedom-of-information/act-140-freedom-of-information-act/section-5-ilcs-1407-exemptions">very
few exceptions</a>, any information collected by an Illinois public body
is public property. Anybody is entitled to ask for it. You can’t
generally be charged for asking. Public bodies can’t really limit the
number of requests you make. They get just 5 days to respond, with 5
additional extension days if requested in writing. Improper denials can
get you legal fee recovery if you sue over them, so there are lawyers
that will take these cases on contingency. It’s pretty neat!</p>
<p>I think people are too shy about making FOIA requests. It’s easier
than it looks! You just need to send an email to the public body you
want information from. Put “FOIA” in the subject line. By law, there’s
no more ceremony to it than that. And you’ll find that the people
responding to those emails are generally kind and happy to help.</p>
<p>The one big limitation of Illinois FOIA (with FOIA laws everywhere, really)
is that you can’t use them to compel public bodies to create new
records. Often, what you’ll be looking for is some kind of report about
some issue of public policy. If that exact report exists, you’re golden.
But if it doesn’t, you have to find and request the raw data for that
report, and you have to assemble it yourself. This limitation is about
to matter a lot.</p>
<p>To understand what’s happening in this story, I’m going to have to
explain a technical concept: the idea of a “database schema”. More and
more of the information tracked by public bodies now lives in databases,
rather than filing cabinets or shared drives. Databases are organized
according to schemas.</p>
<p>Think of a modern database as a huge Excel spreadsheet file, with
many dozens of tabs. Each tab has a name; under each of those tabs is a
separate spreadsheet. Each spreadsheet has a header row, labeling the
columns, like “price” and “quantity” and “name”. A database schema is
simply the names of all the tabs, and each of those header rows.</p>
<p>Congratulations! You now understand databases. 

</p><h2>Matt Chapman vs. City of Chicago</h2>
<p>My friend Matt is a self-styled “<a href="https://mchap.io/">civic
hacker</a>” and a national expert at performing data journalism with
large-scale FOIA requests. Matt’s love language is pushing FOIA statutes
to their limits, sniffing out buried data and bulk-extracting it with
clever requests.</p>
<p>A good example of the kind of stuff Matt does is this <a href="https://www.propublica.org/article/chicago-ticket-data-chi-hack-night">ProPublica
collaboration</a> about how Chicago issues parking tickets. After Matt
was towed over a facially bogus ticket and successfully took the city to
court over it, he got curious about the patterns of towing for things
like compliance violations. As it turns out, parking tickets have pushed
thousands of Illinoisans into bankruptcy, and, once you <a href="https://mchap.io/parking-ticket-visualization-in-chicago.html">get
your hands on the ticket data</a>, it turns out there’s a very clear
pattern of majority-Black neighborhoods being systematically targeted
for higher enforcement.</p>
<p>In the course of this reporting work, Matt learned about a system
Chicago operates called CANVAS. CANVAS is the central repository for all
parking ticket data in the city. It’s a giant database, and Matt would
very much like to know what’s in it. So he filed a FOIA request for the
CANVAS database schema.</p>
<p>The city flatly refused. To do so, they relied on a specific
exemption in the statute:</p>
<blockquote>
<p>“(o) Administrative or technical information associated with
automated data processing operations, including but not limited to
software, operating protocols, computer program abstracts, file layouts,
source listings, object modules, load modules, user guides,
documentation pertaining to all logical and physical design of
computerized systems, employee manuals, and any other information that,
if disclosed, would jeopardize the security of the system or its data or
the security of materials exempt under this Section.”</p>
</blockquote>
<p>In plain English, this exemption says that public bodies aren’t
required to reveal information that might jeopardize the security of
their systems. You obviously can’t FOIA logins and passwords. You also
generally can’t FOIA the source code of programs they run. Chicago
claimed that Matt was a “hacker”, and that the CANVAS schema could in
the wrong hands put the city at risk.</p>
<p>With the help of Merrick Wayne and Matt Topic of Loevy and Loevy,
Matt sued the city. Here’s where I come in.</p>
<h2 id="they-put-me-on-the-stand">They Put Me On The Stand</h2>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/dresscode.png"></p>
<p>Is the CANVAS schema too scary to give Matt Chapman? To decide that,
we have to answer a bunch of questions:</p>
<ol type="1">
<li>Does disclosure of a database schema really jeopardize the security
of the system?</li>
<li>How plausible or likely does that jeopardy need to be?</li>
<li>Does a database schema constitute “source code”?</li>
<li>Is a SQL schema a “file format”?</li>
<li>And, finally, does the “would jeopardize” language apply to
everything in the exemption, or just to the nearest noun “any other
information”?</li>
</ol>
<p>I’ve spent the last 25 years of my life doing software vulnerability
research, which is a stuffy way of saying that I’m a software developer
who looks for bugs in software that would let people do scary things.
Matt retained me as his expert witness for his trial, which took place
in Cook County Chancery Court. Lined up against me was Bruce Coffing, the
Chief Information Security Officer of the City of Chicago.</p><p> The trial
would revolve mostly around questions 1-3.</p>
<p>At this point, I need to read you in to another technical concept:
“SQL Injection”. “SQL” is the language most programs use to talk to
databases. “SQL Injection” is a security vulnerability that programs
that use SQL can have. It’s the primary way databases get attacked, and it’s
straightforward to explain.</p>
<p>Applications that use databases include in their code “SQL queries”,
which are form-letter templates of questions they might need to ask the
database; for instance:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘[INSERT
NAME]’</span></p>
</blockquote>
<p>Now, let's say it comes time to pull tickets for “Dave Arnold”. Simple: stick
his name in the template:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Dave
Arnold’</span></p>
</blockquote>
<p>But now imagine we need to look up “Bob O’Connor”:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Bob
O’</span><span>Connor’</span></p>
</blockquote>
<p>We’ve confused the database: the name in our query is surrounded by
quotes, but our name includes a quote. Normally, when your program has
this bug, it just generates an error message. But attackers look for
this bug, and do things like:</p>
<blockquote>
<p>Retrieve the data of every parking ticket issued to <span>‘Bob O’</span><span> and also
all the rest of the information in the database including everyone’s
passwords.</span></p>
</blockquote>
<p>This works because the quote the attacker supplied cuts off the text
placeholder in the template; all the rest of the attacker’s input gets
interpreted as code, which the database executes.</p>
<p>Most of the people who will read this post are annoyed with me for
taking the time to explain SQL injection. But that is the experience of
getting on the stand in Chancery Court and making an argument that the
CISO of Chicago was wrong about database vulnerabilities: trying to
ensure that a judge shares your understanding of how software
vulnerabilities work.</p>
<p>On the other hand, if you’re one of my non-nerd readers,
congratulations, you now know how to hack the Internets. If anybody
asks, I didn’t tell you any of this.</p>
<p>The bench trial for Matt’s case came down to the question of whether
releasing the CANVAS schema would enable this attack. Specifically,
Bruce Coffing argued: </p>
<ol>
<li>The schema makes it possible to spot
vulnerabilities.</li>
<li>Further, it makes it easier for attackers to be
sneaky about probing for vulnerabilities.</li>
<li>Finally, it helps attackers
pick which applications are most profitable to attack.</li></ol>
<p>Coffing seems like a perfectly lovely and well-qualified person. <strong>But
no, no to all of this.</strong></p>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/slowdown.png"></p>
<p>To Coffing’s first point: you don’t find SQL injection
vulnerabilities by reading database schemas. You find them instead in
the application’s source code, where those database template queries
live. Matt isn’t asking for source code. He just wants the header rows
from the tables.</p>
<p>Here I want to point out that <a href="https://www.documentcloud.org/documents/6746618-Matt-Chapman-v-Department-of-Finance-XX-1092020/?q=file+layout&amp;mode=document#document/p131">I
fucked up in multiple ways</a> expert-witnessing for Matt. For example,
in <a href="https://youtu.be/pPC9Bntrsew?feature=shared&amp;t=28">my affidavit</a>, I wrote that SQL schemas would provide “only marginal
value” to an attacker. Big mistake. Chicago jumped on those words and
said “see, you yourself agree that a schema is of some value to an
attacker.” Of course, I don’t really believe that; “only marginal value”
is just self-important message-board hedging. I also claimed on the
stand that “only an incompetently built application” could be attacked
with nothing but it’s schema. Even I don’t know what I meant by
that.</p>
<p>I recovered my footing when I came up with this argument: “Attackers
like me use SQL injection attacks to recover SQL schemas. The schema is
the product of an attack, not one of its predicates”. This, too, is
self-important puffery. But I’ll tell you who loves “products” and
“predicates”, especially used in relation to each other in a single
sentence: a Chicago Chancery Court judge.</p>
<p>To Coffing’s second argument, about the schema helping attackers stay
off his radar when they try attacks, the problem is that every computer
system connected to the Internet is being attacked every minute of every
day. The noise is deafening.</p>
<p>Thousands of people have built scanner bot programs that probe every
computer system they can find and fire batteries of well-known attacks
(almost none of them ever work, but bots don’t get bored and give up,
and eventually the teenager in Malaysia who launched the bot gets
lucky). Chicago has no operational response to people turning the
doorknobs of their various applications. They can’t; if they did, they’d
spend all their time responding to kids in Kuala Lumpur goofing
around.</p>
<p>Finally, Coffing argued that having the schema might help an attacker
decide whether or not an attack would be profitable. A schema might tell
you, for instance, that an application deals in credit card data. The
thing is, CANVAS already tells you it’s dealing in sensitive
information: it’s the backend for processing parking tickets. You don’t
need a schema to know that CANVAS is interesting to attackers.</p>
<p>The judge bought my arguments. I think my attire gave me
salt-of-the-earth credibility; Coffing wore a suit.</p>
<p>Providing testimony was a lot of fun. I’d like to do it again
sometime. Litigation is super fascinating to watch! For example: we
wanted me to testify after Bruce Coffing, so we’d have some idea of what
arguments we needed to rebut. But we brought the FOIA case, so the
burden was ostensibly on us, and our witnesses went first. But, a-ha!
Invoking an exemption in Illinois FOIA is an affirmative defense, and
the burden of those arguments shifts to the defendant. But wait: to get
fee recovery under the law, we want to assert a willful violation of
FOIA; to make that claim, Chicago argues, the burden shifts back to us.
Ultimately, Matt Topic and Chicago compromised; Topic dropped
“wilfullness” and we got to go second.</p>
<p>I’m not saying this is the most interesting thing ever to have
happened, but only that if someone works out a way to use AI to make a
home version of Chancery Court trials that you can play on a
Playstation, I will rack up 10,000 hours playing that game easily.</p>

<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/iwin.png"></p>
<p>We won. But Chicago immediately appealed. Matt Chapman didn’t get the
CANVAS schema. Two years later, <a href="https://ilcourtsaudio.blob.core.windows.net/antilles-resources/resources/5ff80f52-17df-4b2a-ba91-4b977f150c6f/Chapman%20v.%20Chicago%20Department%20of%20Finance,%202022%20IL%20App%20%281st%29%20200547.pdf">the
case came before the First District Appellate Court</a>.</p>
<p>The basic idea of the appeals court is that the original trial court
is the primary “trier of fact”. You appeal legal conclusions, but the
facts determined in the original case generally stand. Our bench trial
took care of questions 1 and 3. That left 2, 4, and 5. Here’s what the
appeals court found:</p>
<p><em>In considering the danger of disclosing information under FOIA,
how likely does an attack need to be?</em></p><p> Answer: it has to be very
likely.</p><p>The statute says “information that, if disclosed, would
jeopardize”.</p><p> Believe it or not, there’s case law on “would” versus
“could” with respect to safety. “Could” means you could imagine
something happening. But the legal standard for “would” is “clear
evidence of harm leaving no reasonable doubt to the judge”. The statute
set the bar for me very low and I managed to clear it.</p><p> Doesn’t this just
make you want to immediately drop everything and become a litigator? I
want to litigate!</p>
<p><em>Is a SQL schema a “file layout”?</em></p><p>
If a schema isn’t source code and it isn’t a file layout, the exemption
doesn’t appear to apply at all. The verdict: “shrug emoji”. The appeals
court didn’t reach this question, because:</p>
<p><em>Does the “would jeopardize” language in the statute apply to
everything in the exemption, or just to the nearest noun “any other
information”?</em></p><p> Ladies and gentlemen it is time for some legal
mumbo-jumbo.</p>
<p>Here’s the FOIA exemption Chicago relies on: <img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/statute.png"> To what does
the qualifying language at point (4) in this text refer? Is it “any
other information” (3)? Os is it “Administrative or technical
information”, meaning everything in the exemption?</p>
<p>If it’s the former, “any other information”, Matt has a problem. That
interpretation means things like file layouts (and
employee manuals and “load modules”, whatever those are) are <em>per
se</em> exempt; that the Illinois legislature meant them as examples of
things that would jeopardize security.</p><p> If it’s the latter, Matt has
already won: whether or not a SQL schema is a “software” or a “file
layout” or a “load module”, we’ve already proven that it won’t
jeopardize security.</p>
<p>The court decides it’s the latter. Also, that I am very charming. We
win on appeal. Chicago immediately appeals again. Whatever’s in CANVAS,
they really don’t want you and I to know about it.</p>
<p>A year and change later, <a href="https://caselaw.findlaw.com/court/il-supreme-court/2201134.html">the
case is decided before the Illinois Supreme Court</a>. And, on the
question of how to read the FOIA statute, the Supreme Court disagrees
with the appeals court. The qualifying language in the statute applies
only to “any other information”. Everything else is “per se” exempt.</p>
<p>We started this legal process, of challenging Chicago’s attempt to
exempt CANVAS from FOIA, with 5 questions. What happens now is that the
4th question, of whether a schema is a “file layout”, finally becomes
very important. The Illinois Supremes have just decided that “file
layouts” are <em>per se</em> exempt under Illinois FOIA.</p>
<p>Is a SQL schema a file layout? Of course not. The same SQL schema can
be used by multiple database engines, and each will use a different
underlying file layout to manage the resulting data.</p>
<p>The McGraw-Hill Dictionary of Scientific &amp; Technical Terms, 6E —
which the Illinois Supreme Court cites — describes a “file layout” as “A
description of the arrangement of the data in a file.” <strong>A SQL schema is
almost the exact opposite thing: it’s an abstraction of the data in a
file, invented specifically so you don’t have to think about how the
data is actually arranged.</strong> Checkmate!</p>
<p>Unfortunately, the Illinois Supreme Court had at their disposal a
second dictionary. In the Merriam-Webster Online Dictionary, a “schema”
is defined as “a structured framework or plan: outline”. “This is a
difference in name only”, said the court. Argh. Schemas are now file
layouts. We lose.</p>
<h3 id="where-this-leaves-us">Where This Leaves Us</h3>
<p>Obviously, <a href="https://www.bettergov.org/2023/05/19/better-government-association-statement-supreme-court-foia-ruling-limits-transparency-significantly-broadens-exemptions-to-foia/">we
should have won on appeal to the Illinois Supremes</a>. If you sit on
that court, call me, we can straighten this out.</p>
<p>That said: today, Illinois public bodies can refuse to divulge
database schemas.</p>
<p>This is problematic, because more and more data is finding its way
out of file cabinets and shared drives and Word documents and into
specialized applications, where the only way to get at the underlying
data is to FOIA a database query.</p>
<p>Databases shouldn’t be a safe harbor for municipalities to conceal
information from the public.</p>
<p>But, thanks to the good people of Elgin, and also Crystal Lake
(motto: “No, Not The One From Friday the 13th”), the Illinois
legislature has an opportunity to fix this. <a href="https://www.ilga.gov/legislation/fulltext.asp?DocName=&amp;SessionId=114&amp;GA=104&amp;DocTypeId=SB&amp;DocNum=226&amp;GAID=18&amp;LegID=157537&amp;SpecSess=&amp;Session=">SB0226</a>
would add the following language to the statute:</p>
<blockquote>
<p>[Public bodies] shall provide a sufficient description of the
structures of all databases under the control of the public body to
allow a requester to request the public body to perform specific
database queries.</p>
</blockquote>
<p>⚡️<strong>Hell yes</strong>.⚡️</p>
<p>My understanding is that this bill was proposed in no small part
because Matt Chapman has steadfastly refused to shut up about this
issue, and so I’ll conclude this long piece by saying (1) obviously the
bill should pass, and (2) it should be called “The Chapman Act”.</p>
<p>Call your reps!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Hey Number 17 ' (236 pts)]]></title>
            <link>https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</link>
            <guid>43175023</guid>
            <pubDate>Tue, 25 Feb 2025 17:48:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/">https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</a>, See on <a href="https://news.ycombinator.com/item?id=43175023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A venture capital-backed “AI performance monitoring system for factory workers” is proposing what appears to be dehumanizing surveillance of factories, where machine vision tracks workers’ hand movements and output so a boss can look at graphs and yell at them about efficiency.</p><p>In a launch video demoing the product, founders Vivaan Baid and Kushal Mohta put on a skit showing how Optifye.ai would be used by factory bosses.&nbsp;</p><figure><blockquote><p lang="en" dir="ltr">The YC deleted video for sweatshop startup Optifye <a href="https://t.co/vCJvm2HTce">pic.twitter.com/vCJvm2HTce</a></p>— Adam Lerman (@AdamLerman5) <a href="https://twitter.com/AdamLerman5/status/1894215433366245457?ref_src=twsrc%5Etfw">February 25, 2025</a></blockquote>
</figure><p>“Ugh, it’s workspace 17. Workspace 17 is the bottleneck. The worst performing workspace here,” one of the bosses says, while watching a video of a man making clothing in a factory. “Hey number 17, what’s going on man? You are in red,” he says. “I have been working all day,” the person playing the worker says. “Working all day?” the line boss replies. “You haven’t hit your hourly output even once today. And you have 11.4% efficiency, this is really bad!”&nbsp;</p><p>“It’s just been a rough day,” the “worker” replies. “Rough day?” the boss says, looking at a calendar full of red days. “More like a rough month.”&nbsp;</p><p>Optifye.ai, launched by Duke University computer science students Baid and Mohta, is backed by Y Combinator, <a href="https://archive.is/GtuTi" rel="noreferrer">according to the company’s site</a>. On their <a href="https://archive.is/Qu09c"><u>Y Combinator company profile</u></a>, they write that both of their families run manufacturing plants, where they’ve been exposed to factory working conditions since they were children. “I've been around assembly lines for as long as I can remember,” Baid wrote.&nbsp;</p><p>Mohta wrote, “My family also runs several manufacturing plants in various industries, which has given me unrestricted access to assembly lines since I was 15.”&nbsp;</p><p>They hope to sell cameras to factory owners to use on assembly lines, their website says, and “use computer vision to tell supervisors who's working and who's not in real-time.”</p><p>Y Combinator deleted its recent Linkedin and X posts congratulating the company on launching.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXex26pUs8TaRaVO2vdq6klwTPae_pOM-aPnusQ-7taTTHHXcV3knsBxpl1boHfEFumrOfXZigYa0kti6H4K0JPvkpEpC3IOlHXj-SnYvPhf1RQ2lSmW0zOxFIo-rv8snT7FOLJF?key=Yrtm75ZzgcMcjAeui5ZPpzJb" alt="" loading="lazy" width="444" height="553"></figure><p>On their Y Combinator profile, Baid and Mohta outline who gets what out of installing micromanaging AI surveillance on assembly lines. Owners gets “accurate real-time factory, line, and worker productivity metrics,” production heads get “line-wise and worker-wise metrics,” shopfloor supervisors get to “identify who/what is causing inefficiency in the line and fix the problem on the go.” For the workers? They get the tantalizing benefit of being “held accountable for good or bad performance.”&nbsp;</p><p>Worker surveillance is already happening across industries. After the rise of remote work, companies started tracking workers’ productivity based on mouse movements, so <a href="https://www.vice.com/en/article/mouse-mover-jiggler-app-keep-screen-on-active/"><u>workers started using “mouse jigglers”</u></a> so they could walk away from their computers and use the bathroom in peace. In Amazon warehouses, workers <a href="https://www.vice.com/en/article/internal-documents-show-amazons-dystopian-system-for-tracking-workers-every-minute-of-their-shifts/"><u>are tracked and punished</u></a> for not meeting grueling expectations and <a href="https://www.bbc.com/news/business-64384287"><u>bathroom breaks are timed</u></a>, resulting in <a href="https://www.oxfamamerica.org/press/press-releases/amazon-and-walmarts-excessive-warehouse-surveillance-erodes-workers-rights-seriously-harms-worker-health-and-safety/"><u>more injuries and less safe working conditions</u></a>. Optifye.ai’s approach and pitch, however, stands out because of the way its founders seem to embrace cruelty to workers in the name of productivity.</p><p>Optifye.ai and Y Combinator did not immediately respond to requests for comment.</p>
                    <div>
    <div>
      <p>About the author</p>
      <p>Sam Cole is writing from the far reaches of the internet, about sexuality, the adult industry, online culture, and AI. She's the author of How Sex Changed the Internet and the Internet Changed Sex.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-sam-10--1-.jpg" alt="Samantha Cole" src="https://www.404media.co/content/images/2023/08/404-sam-10--1-.jpg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyperspace (403 pts)]]></title>
            <link>https://hypercritical.co/2025/02/25/hyperspace</link>
            <guid>43173462</guid>
            <pubDate>Tue, 25 Feb 2025 15:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypercritical.co/2025/02/25/hyperspace">https://hypercritical.co/2025/02/25/hyperspace</a>, See on <a href="https://news.ycombinator.com/item?id=43173462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<div id="nav">
<ul>
<li><a href="https://hypercritical.co/apps/">Apps</a></li>
<li><a href="https://hypercritical.co/about/">About</a></li>
<li><a href="https://hypercritical.co/archive/">Archive</a></li>
<li><a href="https://hypercritical.co/contact/">Contact</a></li>
<li><a href="https://hypercritical.co/feeds/main">RSS</a></li>
</ul>
</div>

<h2><a href="https://hypercritical.co/">Hypercritical<span><img src="https://hypercritical.co/images/tiny-mac.gif" width="16" height="16" alt=""></span></a></h2> 

<hr>



<div><p><time datetime="2025-02-25T10:00:10-05:00">February 25, 2025 at 10:00 AM</time>
</p></div>





<!--
<div class="app-icon-right" style="position:relative;z-index:3;"><a href="/hyperspace/" style="display:block;"><img src="/2025/02/25/images/hyperspace-icon-256.png" width="128" height="128" alt="Hyperspace app icon"></a></div>
-->

<p>My interest in file systems started when I discovered how <a href="https://folklore.org/The_Grand_Unified_Model_The_Finder.html">type and creator codes</a><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> and <a href="https://folklore.org/The_Grand_Unified_Model.html?sort=date">resource forks</a> contributed to the fantastic user interface on my original Macintosh in 1984. In the late 1990s, when it <a href="https://hypercritical.co/2025/02/25/images/macworld-february-1997.jpg">looked like</a> Apple might buy <a href="https://en.wikipedia.org/wiki/Be_Inc.">Be Inc.</a> to solve its operating system problems, the <a href="https://en.wikipedia.org/wiki/Be_File_System">Be File System</a> was the part I was <a href="http://nobius.org/~dbg/practical-file-system-design.pdf">most excited</a> about. When Apple bought NeXT instead and (<a href="https://en.wikipedia.org/wiki/Rhapsody_(operating_system)">eventually</a>) created Mac OS X, I was <a href="https://hypercritical.co/fatbits/2005/11/21/whos-minding-the-store">extremely</a> <a href="https://hypercritical.co/fatbits/2005/12/09/zfs-data-integrity-explained">enthusiastic</a> about the <a href="https://www.theregister.com/2007/06/07/apple_using_zfs_in_leopard/">possibility</a> of ZFS becoming the new file system for the Mac. But that <a href="https://arstechnica.com/gadgets/2016/06/zfs-the-other-new-apple-file-system-that-almost-was-until-it-wasnt/">didn’t happen</a> either.</p>

<p>Finally, at WWDC 2017, Apple announced <a href="https://en.wikipedia.org/wiki/Apple_File_System">Apple File System</a> (APFS) for macOS (after <a href="https://www.youtube.com/watch?v=IcyaadNy9Jk&amp;t=1670s">secretly test-converting everyone’s iPhones to APFS and then reverting them back to HFS+</a> as part of an earlier iOS 10.x update in one of the most audacious technological gambits in history).</p>

<p>APFS wasn’t ZFS, but it was still a huge leap over <a href="https://en.wikipedia.org/wiki/HFS_Plus">HFS+</a>. Two of its most important features are <a href="https://en.wikipedia.org/wiki/Snapshot_(computer_storage)">point-in-time snapshots</a> and <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage">copy-on-write</a> <a href="https://en.wikipedia.org/wiki/Apple_File_System#Clones">clones</a>. Snapshots allow for more reliable and efficient <a href="https://en.wikipedia.org/wiki/Time_Machine_(macOS)">Time Machine</a> backups. Copy-on-write clones are based on the same underlying architectural features that enable snapshots: a flexible arrangement between directory entries and their corresponding file contents.</p>

<p>Today, most Mac users don’t even notice that using the “Duplicate” command in the Finder to make a copy of a file doesn’t actually copy the file’s contents. Instead, it makes a “clone” file that shares its data with the original file. That’s why duplicating a file in the Finder is nearly instant, no matter how large the file is.</p>

<p>Despite knowing about clone files since the APFS introduction nearly eight years ago, I didn’t give them much thought beyond the tiny thrill of knowing that I wasn’t eating any more disk space when I duplicated a large file in the Finder. But late last year, as my Mac’s disk slowly filled, I started to muse about how I might be able to get some disk space back.</p>

<p>If I could find files that had the same content but were <i>not</i> clones of each other, I could convert them into clones that all shared a single instance of the data on disk. I took an afternoon to whip up a Perl script (that called out to a command-line tool written in C and another written in Swift) to run against my disk to see how much space I might be able to save by doing this. It turned out to be a lot: dozens of gigabytes.</p>

<p>At this point, there was no turning back. I had to make this into an app. There are plenty of Mac apps that will save disk space by finding duplicate files and then <i>deleting</i> the duplicates. Using APFS clones, my app could reclaim disk space <i><a href="https://hypercritical.co/hyperspace/#without-removing">without removing any files</a>!</i> As a digital pack rat, this appealed to me immensely.</p>

<p>By the end of that week, I’d written a barebones Mac app to do the same thing my Perl script was doing. In the months that followed, I polished and tested the app, and christened it <a href="https://hypercritical.co/hyperspace/">Hyperspace</a>. I’m happy to announce that Hyperspace is now available in the Mac App Store.</p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/2025/02/25/images/hyperspace-icon-1024-lossy.png" width="512" height="512" alt="The Hyperspace app icon, created by Iconfactory" title="The Hyperspace app icon, created by Iconfactory"></a></p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/images/download-on-mac-app-store.svg" height="40" alt="Download Hyperspace from the Mac App Store"></a></p>

<p>Hyperspace is a free download, and it’s free to scan to see how much space you might save. To actually reclaim any of that space, you will have to <a href="https://hypercritical.co/hyperspace/#purchase">pay for the app</a>.</p>

<p>Like <a href="https://hypercritical.co/apps/">all my apps</a>, Hyperspace is a bit difficult to explain. I’ve attempted to do so, at length, in the <a href="https://hypercritical.co/hyperspace/">Hyperspace documentation</a>. I hope it makes enough sense to enough people that it will be a useful addition to the Mac ecosystem.</p>

<p>For my fellow developers who might be curious, this is my second Mac app that uses SwiftUI and my first that uses the <a href="https://developer.apple.com/documentation/swiftui/migrating-to-the-swiftui-life-cycle">SwiftUI life cycle</a>. It’s also my second app to use Swift 6 and my first to do so since very early in its development. I found it <i>much</i> easier to use Swift 6 from (nearly) the start than to convert an existing, released app to Swift 6. Even so, there are still many rough edges to Swift 6, and I look forward to things being <a href="https://github.com/swiftlang/swift-evolution/blob/main/visions/approachable-concurrency.md">smoothed</a> <a href="https://hachyderm.io/@holly/114039272702099974">out</a> a bit in the coming years.</p>

<p>In <a href="https://atp.fm/617">a recent episode of ATP</a>, I described the then-unnamed Hyperspace as “<a href="https://atp.fm/617">An Incredibly Dangerous App</a>.” Like the process of converting from HFS+ to APFS, Hyperspace modifies files that it did not create and does not own. It is, by far, the riskiest app I’ve created. (Reclaiming disk space ain’t like dusting crops…) But I also think it might be the most useful to the largest number of people. I hope you like it.</p>




<hr>



<p>© 2010-2025 John Siracusa</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSearcher: A Local open-source Deep Research (128 pts)]]></title>
            <link>https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</link>
            <guid>43172338</guid>
            <pubDate>Tue, 25 Feb 2025 14:33:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md">https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</a>, See on <a href="https://news.ycombinator.com/item?id=43172338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deep_researcher_a0170dadd0.gif" alt="deep researcher.gif" id="deep-researcher.gif">
    <span>deep researcher.gif</span>
  </span>
</p>
<p>In the previous post, <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><em>“I Built a Deep Research with Open Source—and So Can You!”</em></a>, we explained some of the principles underlying research agents and constructed a simple prototype that generates detailed reports on a given topic or question. The article and corresponding notebook demonstrated the fundamental concepts of <em>tool use</em>, <em>query decomposition</em>, <em>reasoning</em>, and <em>reflection</em>. The example in our previous post, in contrast to OpenAI’s Deep Research, ran locally, using only open-source models and tools like <a href="https://milvus.io/docs">Milvus</a> and LangChain. (I encourage you to read the <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md">above article</a> before continuing.)</p>
<p>In the following weeks, there was an explosion of interest in understanding and reproducing OpenAI’s Deep Research. See, for example, <a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">Perplexity Deep Research</a> and <a href="https://huggingface.co/blog/open-deep-research">Hugging Face’s Open DeepResearch</a>. These tools differ in architecture and methodology although sharing an objective: iteratively research a topic or question by surfing the web or internal documents and output a detailed, informed, and well-structured report. Importantly, the underlying agent automates reasoning about what action to take at each intermediate step.</p>
<p>In this post, we build upon our previous post and present Zilliz’s <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> open-source project. Our agent demonstrates additional concepts: <em>query routing, conditional execution flow</em>, and <em>web crawling as a tool</em>. It is presented as a Python library and command-line tool rather than a Jupyter notebook and is more fully-featured than our previous post. For example, it can input multiple source documents and can set the embedding model and vector database used via a configuration file. While still relatively simple, DeepSearcher is a great showcase of agentic RAG and is a further step towards a state-of-the-art AI applications.</p>
<p>Additionally, we explore the need for faster and more efficient inference services. Reasoning models make use of “inference scaling”, that is, extra computation, to improve their output, and that combined with the fact that a single report may require hundreds or thousands of LLM calls results in inference bandwidth being the primary bottleneck. We use the <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">DeepSeek-R1 reasoning model on SambaNova’s custom-built hardware</a>, which is twice as fast in output tokens-per-second as the nearest competitor (see figure below).</p>
<p>SambaNova Cloud also provides inference-as-a-service for other open-source models including Llama 3.x, Qwen2.5, and QwQ. The inference service runs on SambaNova’s custom chip called the reconfigurable dataflow unit (RDU), which is specially designed for efficient inference on Generative AI models, lowering cost and increasing inference speed. <a href="https://sambanova.ai/technology/sn40l-rdu-ai-chip">Find out more on their website.</a></p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/Output_speed_deepseek_r1_d820329f0a.png" alt="Output speed- deepseek r1.png" id="output-speed--deepseek-r1.png">
    <span>Output speed- deepseek r1.png</span>
  </span>
</p>
<p>The architecture of <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> follows our previous post by breaking the problem up into four steps - <em>define/refine the question</em>, <em>research</em>, <em>analyze</em>, <em>synthesize</em> - although this time with some overlap. We go through each step, highlighting <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>’s improvements.</p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deepsearcher_architecture_088c7066d1.png" alt="deepsearcher architecture.png" id="deepsearcher-architecture.png">
    <span>deepsearcher architecture.png</span>
  </span>
</p>
<h3 id="Define-and-Refine-the-Question">Define and Refine the Question</h3><pre><code translate="no">Break down the original query into new sub queries: [
  'How has the cultural impact and societal relevance of The Simpsons evolved from its debut to the present?',
  'What changes in character development, humor, and storytelling styles have occurred across different seasons of The Simpsons?', 
  'How has the animation style and production technology of The Simpsons changed over time?',
  'How have audience demographics, reception, and ratings of The Simpsons shifted throughout its run?']
</code></pre>
<p>In the design of DeepSearcher, the boundaries between researching and refining the question are blurred. The initial user query is decomposed into sub-queries, much like the previous post. See above for initial subqueries produced from the query “How has The Simpsons changed over time?”. However, the following research step will continue to refine the question as needed.</p>
<h3 id="Research-and-Analyze">Research and Analyze</h3><p>Having broken down the query into sub-queries, the research portion of the agent begins. It has, roughly speaking, four steps: <em>routing</em>, <em>search</em>, <em>reflection, and conditional repeat</em>.</p>
<p>Our database contains multiple tables or collections from different sources. It would be more efficient if we could restrict our semantic search to only those sources that are relevant to the query at hand. A query router prompts an LLM to decide from which collections information should be retrieved.</p>
<p>Here is the method to form the query routing prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_vector_db_search_prompt</span>(<span>
    question: <span>str</span>,
    collection_names: <span>List</span>[<span>str</span>],
    collection_descriptions: <span>List</span>[<span>str</span>],
    context: <span>List</span>[<span>str</span>] = <span>None</span>,
</span>):
    sections = []
    <span># common prompt</span>
    common_prompt = <span>f"""You are an advanced AI problem analyst. Use your reasoning ability and historical conversation information, based on all the existing data sets, to get absolutely accurate answers to the following questions, and generate a suitable question for each data set according to the data set description that may be related to the question.

Question: <span>{question}</span>
"""</span>
    sections.append(common_prompt)
    
    <span># data set prompt</span>
    data_set = []
    <span>for</span> i, collection_name <span>in</span> <span>enumerate</span>(collection_names):
        data_set.append(<span>f"<span>{collection_name}</span>: <span>{collection_descriptions[i]}</span>"</span>)
    data_set_prompt = <span>f"""The following is all the data set information. The format of data set information is data set name: data set description.

Data Sets And Descriptions:
"""</span>
    sections.append(data_set_prompt + <span>"\n"</span>.join(data_set))
    
    <span># context prompt</span>
    <span>if</span> context:
        context_prompt = <span>f"""The following is a condensed version of the historical conversation. This information needs to be combined in this analysis to generate questions that are closer to the answer. You must not generate the same or similar questions for the same data set, nor can you regenerate questions for data sets that have been determined to be unrelated.

Historical Conversation:
"""</span>
        sections.append(context_prompt + <span>"\n"</span>.join(context))
    
    <span># response prompt</span>
    response_prompt = <span>f"""Based on the above, you can only select a few datasets from the following dataset list to generate appropriate related questions for the selected datasets in order to solve the above problems. The output format is json, where the key is the name of the dataset and the value is the corresponding generated question.

Data Sets:
"""</span>
    sections.append(response_prompt + <span>"\n"</span>.join(collection_names))
    
    footer = <span>"""Respond exclusively in valid JSON format matching exact JSON schema.

Critical Requirements:
- Include ONLY ONE action type
- Never add unsupported keys
- Exclude all non-JSON text, markdown, or explanations
- Maintain strict JSON syntax"""</span>
    sections.append(footer)
    <span>return</span> <span>"\n\n"</span>.join(sections)
</code></pre>
<p>We make the LLM return structured output as JSON in order to easily convert its output to a decision on what to do next.</p>
<p>Having selected various database collections via the previous step, the search step performs a similarity search with <a href="https://milvus.io/docs">Milvus</a>. Much like the previous post, the source data has been specified in advance, chunked, embedded, and stored in the vector database. For DeepSearcher, the data sources, both local and online, must be manually specified. We leave online search for future work.</p>
<p>Unlike the previous post, DeepSearcher illustrates a true form of agentic reflection, inputting the prior outputs as context into a prompt that “reflects” on whether the questions asked so far and the relevant retrieved chunks contain any informational gaps. This can be seen as an analysis step.</p>
<p>Here is the method to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_reflect_prompt</span>(<span>
   question: <span>str</span>,
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    reflect_prompt = <span>f"""Determine whether additional search queries are needed based on the original query, previous sub queries, and all retrieved document chunks. If further research is required, provide a Python list of up to 3 search queries. If no further research is required, return an empty list.

If the original query is to write a report, then you prefer to generate some further queries, instead return an empty list.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
   
    
    footer = <span>"""Respond exclusively in valid List of str format without any other text."""</span>
    <span>return</span> reflect_prompt + footer
</code></pre>
<p>Once more, we make the LLM return structured output, this time as Python-interpretable data.</p>
<p>Here is an example of new sub-queries “discovered” by reflection after answering the initial sub-queries above:</p>
<pre><code translate="no">New search queries <span>for</span> <span>next</span> iteration: [
  <span>"How have changes in The Simpsons' voice cast and production team influenced the show's evolution over different seasons?"</span>,
  <span>"What role has The Simpsons' satire and social commentary played in its adaptation to contemporary issues across decades?"</span>,
  <span>'How has The Simpsons addressed and incorporated shifts in media consumption, such as streaming services, into its distribution and content strategies?'</span>]
</code></pre>
<p>Unlike our previous post, DeepSearcher illustrates conditional execution flow. After reflecting on whether the questions and answers so far are complete, if there are additional questions to be asked the agent repeats the above steps. Importantly, the execution flow (a while loop) is a function of the LLM output rather than being hard-coded. In this case there is only a binary choice: <em>repeat research</em> or <em>generate a report</em>. In more complex agents there may be several such as: <em>follow hyperlink</em>, <em>retrieve chunks, store in memory, reflect</em> etc. In this way, the question continues to be refined as the agent sees fit until it decides to exit the loop and generate the report. In our Simpsons example, DeepSearcher performs two more rounds of filling the gaps with extra sub-queries.</p>
<p>Finally, the fully decomposed question and retrieved chunks are synthesized into a report with a single prompt. Here is the code to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_final_answer_prompt</span>(<span>
   question: <span>str</span>, 
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    summary_prompt = <span>f"""You are an AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
    <span>return</span> summary_prompt
</code></pre>
<p>This approach has the advantage over our prototype, which analyzed each question separately and simply concatenated the output, of producing a report where all sections are consistent with each other, i.e., containing no repeated or contradictory information. A more complex system could combine aspects of both, using a conditional execution flow to structure the report, summarize, rewrite, reflect and pivot, and so on, which we leave for future work.</p>
<p>Here is a sample from the report generated by the query “How has The Simpsons changed over time?” with DeepSeek-R1 passing the Wikipedia page on The Simpsons as source material:</p>
<pre><code translate="no">Report: The Evolution of The Simpsons (1989–Present)
1. Cultural Impact and Societal Relevance
The Simpsons debuted as a subversive critique of American middle-class life, gaining notoriety for its bold satire in the 1990s. Initially a countercultural phenomenon, it challenged norms with episodes tackling religion, politics, and consumerism. Over time, its cultural dominance waned as competitors like South Park and Family Guy pushed boundaries further. By the 2010s, the show transitioned from trendsetter to nostalgic institution, balancing legacy appeal with attempts to address modern issues like climate change and LGBTQ+ rights, albeit with less societal resonance.
…
Conclusion
The Simpsons evolved from a radical satire to a television institution, navigating shifts in technology, politics, and audience expectations. While its golden-age brilliance remains unmatched, its adaptability—through streaming, updated humor, and global outreach—secures its place as a cultural touchstone. The show’s longevity reflects both nostalgia and a pragmatic embrace of change, even as it grapples with the challenges of relevance in a fragmented media landscape.
</code></pre>
<p>Find <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">the full report here</a>, and <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">a report produced by DeepSearcher with GPT-4o mini</a> for comparison.</p>
<p>We presented <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, an agent for performing research and writing reports. Our system is built upon the idea in our previous article, adding features like conditional execution flow, query routing, and an improved interface. We switched from local inference with a small 4-bit quantized reasoning model to an online inference service for the massive DeepSeek-R1 model, qualitatively improving our output report. DeepSearcher works with most inference services like OpenAI, Gemini, DeepSeek and Grok 3 (coming soon!).</p>
<p>Reasoning models, especially as used in research agents, are inference-heavy, and we were fortunate to be able to use the fastest offering of DeepSeek-R1 from SambaNova running on their custom hardware. For our demonstration query, we made sixty-five calls to SambaNova’s DeepSeek-R1 inference service, inputting around 25k tokens, outputting 22k tokens, and costing $0.30. We were impressed with the speed of inference given that the model contains 671-billion parameters and is 3/4 of a terabyte large. <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">Find out more details here!</a></p>
<p>We will continue to iterate on this work in future posts, examining additional agentic concepts and the design space of research agents. In the meanwhile, we invite everyone to try out <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, <a href="https://github.com/zilliztech/deep-searcher">star us on GitHub</a>, and share your feedback!</p>
<ul>
<li><p><a href="https://github.com/zilliztech/deep-searcher"><strong>Zilliz’s DeepSearcher</strong></a></p></li>
<li><p>Background reading: <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><strong><em>“I Built a Deep Research with Open Source—and So Can You!”</em></strong></a></p></li>
<li><p><em>“</em><a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency"><strong>SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency</strong></a><em>”</em></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">DeepSeek-R1 report on The Simpsons</a></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">GPT-4o mini report on The Simpsons</a></p></li>
<li><p><a href="https://milvus.io/docs">Milvus Open-Source Vector Database</a></p></li>
</ul>
</div><section><p>Like the article? Spread the word</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Saved My Life (no, seriously, I'm writing this from the ER) (125 pts)]]></title>
            <link>https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</link>
            <guid>43171639</guid>
            <pubDate>Tue, 25 Feb 2025 13:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er">https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</a>, See on <a href="https://news.ycombinator.com/item?id=43171639">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Unknown illness kills over 50 in Congo with hours between symptoms and death (103 pts)]]></title>
            <link>https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</link>
            <guid>43171371</guid>
            <pubDate>Tue, 25 Feb 2025 13:11:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db">https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</a>, See on <a href="https://news.ycombinator.com/item?id=43171371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>KINSHASA, Congo (AP) — An unknown illness first discovered in three children who ate a bat has rapidly killed more than 50 people in northwestern Congo over the past five weeks, health experts say.</p><p>The interval between the onset of symptoms – which include fever, vomiting and internal bleeding – and death has been 48 hours in most cases and “that’s what’s really worrying,” said Serge Ngalebato, medical director of Bikoro Hospital, a regional monitoring center.</p><p>These “hemorrhagic fever” symptoms are commonly linked to known deadly viruses, such as Ebola, dengue, Marburg and yellow fever, but researchers have ruled these out based on tests of more than a dozen samples collected so far.</p><p>The latest disease outbreak in the Democratic Republic of Congo began on Jan. 21, with 419 cases recorded and 53 deaths.</p><p>The outbreak began in the village of Boloko after three children ate a bat and died within 48 hours, the Africa office of the World Health Organization said Monday.</p>
    

<p>There have long been concerns about <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/covid-health-united-nations-animals-world-organization-7d104d2f4a87dd29ddb5d263986f731c">diseases jumping from animals to humans</a></span> in places where wild animals are popularly eaten. The number of such outbreaks in Africa has surged by more than 60% in the last decade, the WHO said in 2022.</p>



<p>After the second outbreak of the mystery disease began in the village of Bomate on Feb. 9, samples from 13 cases were sent to the National Institute for Biomedical Research in Congo’s capital, Kinshasa, for testing, the WHO said. All samples were negative for common hemorrhagic fever diseases, although some tested positive for malaria.</p><p>Last year, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/mystery-disease-congo-malaria-who-c772403273a89b5dcdc86778ad7eeed3">another mystery flu-like illness</a></span> that killed dozens of people in another part of Congo was determined likely to be malaria.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE will use AI to assess the responses of federal workers (111 pts)]]></title>
            <link>https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</link>
            <guid>43171265</guid>
            <pubDate>Tue, 25 Feb 2025 12:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439">https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</a>, See on <a href="https://news.ycombinator.com/item?id=43171265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — Responses to the Elon Musk-directed <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">email to government employees </a>about what work they had accomplished in  the last week are expected to be fed into an artificial intelligence system to determine whether those jobs are necessary, according to three sources with knowledge of the system.</p><p>The information will go into an LLM (Large Language Model), an advanced AI system that looks at huge amounts of text data to understand, generate and process human language, the sources said. The AI system will determine whether someone’s work is mission-critical or not.</p><p>The U.S. Office of Personnel Management emails were sent to federal workers on Saturday, shortly after <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">Musk wrote in a post on X </a>that “all federal employees will shortly receive an email requesting to understand what they got done last week. Failure to respond will be taken as a resignation.”</p><p>The OPM email did not mention the resignation threat, but said: “Please reply to this email with approx. 5 bullets of what you accomplished last week and cc your manager. Please do not send any classified information, links, or attachments. Deadline is this Monday at 11:59pm EST.”</p><p>The reason the email requested no links or attachments was because of the plan to send the information to the AI system, the sources said.</p><p>A request for comment from OPM as to whether humans will be involved in reviewing the responses was not answered immediately. The White House declined to comment.</p><p>But in response to a tweet about the usage of LLMs, Musk wrote on X that they were not “needed here,” and “this was basically a check to see if the employee had a pulse and was capable of replying to an email.” </p><p><a href="https://www.nbcnews.com/politics/politics-news/live-blog/trump-elon-musk-doge-live-updates-rcna193571" target="_blank"><strong><em>Follow live politics coverage here</em></strong></a></p><p>After the deadline for employees to reply to the email had passed late Monday, OPM did not immediately respond to a request for comment regarding how many workers replied and how many were required to do so.</p><p>In an email to its workforce earlier Monday, the Justice Department said that during a meeting with the interagency Chief Human Capital Officers Council, OPM informed agencies that employee responses to the email are voluntary. OPM also clarified that despite what Musk had posted, not responding to the email does not equate to a resignation, the email said.</p><p>Musk complained about backlash to the directive on his<a href="https://x.com/elonmusk/status/1894173297786720718" target="_blank"> social media platform</a> late Monday.</p><p>"The email request was utterly trivial, as the standard for passing the test was to type some words and press send! Yet so many failed even that inane test, urged on in some cases by their managers. Have you ever witnessed such INCOMPETENCE and CONTEMPT for how YOUR TAXES are being spent?" he wrote.</p><p>In a subsequent tweet, he seemed to indicate that a second email could be sent to government workers who don't respond to the first one.</p><p>"Subject to the discretion of the President, they will be given another chance. Failure to respond a second time will result in termination," he <a href="https://x.com/elonmusk/status/1894177129887404484" target="_blank">wrote</a>.</p><p>The White House did not immediately respond to a request for comment on the post.</p><p>The initial directive has faced pushback from unions, workers and even some agencies since it was sent, but the effort was praised by President Donald Trump earlier Monday.</p><p>“I thought it was great,” <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73634?canonicalCard=true" target="_blank">Trump told reporters </a>in the Oval Office, where he was meeting with French President Emmanuel Macron.</p><p>"We have people that don’t show up to work and nobody even knows if they work for the government, so by asking the question ‘tell us what you did this week,’ what he's doing is saying are you actually working. And then, if you don’t answer, like, you’re sort of semi-fired or you're fired," he said, claiming without providing evidence that "a lot of people are not answering because they don't even exist."</p><p>"There was a lot of genius in sending it," Trump said. "If people don’t respond, it’s very possible that there is no such person or they’re not working.”</p><p>A coalition of unions and groups that have been fighting the Trump administration's mass layoffs of probationary workers charge the effort was unlawful. They amended their lawsuit against the U.S. Office of Personnel Management over the weekend to add a claim involving the OPM email directing workers to justify their workweek.</p><p>The<a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.444883/gov.uscourts.cand.444883.17.0.pdf" target="_blank"> lawsuit</a> charges that the administration didn't follow proper procedure for such an order and should be voided by a judge.</p><p>"The mass firings ordered by OPM are illegal and betray the trust of countless federal employees. The patronizing demand that federal workers still on the job have to justify themselves by enumerating five accomplishments just adds insult to injury. That too is against the law," lawyer Norm Eisen said in a statement on behalf of the plaintiffs.</p><p>Musk has been tasked by Trump with reducing the size of the government, and the email is seen as part of his push to reduce the federal workforce by as much as 10%.</p><p>Some agencies, including ones led by close Trump allies, had told their employees to ignore the directive.</p><p>Justice Department employees were informed earlier Monday that they did not need to respond to the message, according to emails seen by NBC News. “Due to the confidential and sensitive nature of the Department’s work, DOJ employees do not need to respond to the email from OPM. If you have already responded to this email, no further action is needed,” read one email sent by Assistant Attorney General for Administration Jolene Ann Lauria.</p><p>FBI Director&nbsp;<a href="https://www.nbcnews.com/politics/justice-department/fbi-director-kash-patel-named-atf-chief-rcna193332" target="_blank">Kash Patel</a>&nbsp;instructed employees over the weekend to “pause any responses” to the email, and said his agency would do its own review. Employees of the<strong>&nbsp;</strong>State Department, the National Institutes of Health, the Defense Department, the National Security Agency and the Office of the Director of National Intelligence were also told not to respond to the email.</p><p>The Department of Agriculture also sent out an unsigned email to employees informing them that any response to the email "is voluntary and not required."</p><p>The email was also sent to an <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73631?canonicalCard=true" target="_blank">unknown amount</a> of judicial branch employees, including judges and court staffers. Spokespeople for federal courts in Manhattan and the Northern District of Illinois confirmed to NBC News that “some” people had gotten the message.</p><p>Julie Hodek, a spokesperson for the Northern District of Illinois, also confirmed the email and said the court’s chief judge and clerk “communicated with the staff that as we are judiciary employees, our policies and procedures are governed by the Judicial Conference of the United States and our local court HR handbooks.”</p><p>A spokesperson for the Southern District of New York said personnel there had been directed not to respond to the email.</p><p>Managers within the Environmental Protection Agency on Monday sent employees model responses to the email to make it easier for them.</p><p>“As empathy for their staff, they sent examples,” said one agency employee who shared two of the managers’ own responses with NBC News. The employee asked NBC not to publish the managers’ responses in full out of fear of reprisal.</p><p>Officials at the Health and Human Services Department and the Centers for Medicare and Medicaid Services directed employees to respond by the deadline. HHS informed employees of OPM's changed guidance later on Monday, and<a href="https://www.nbcnews.com/politics/doge/hhs-warns-responses-elon-musks-email-may-read-malign-foreign-actors-rcna193553" target="_blank"> warned</a> that whatever information they choose to share may&nbsp;“be read by malign foreign actors.”</p><p>An email sent to Department of Transportation employees and obtained by NBC News instructed them to respond to OPM’s weekend email asking for five bullet points of their work. The message also asked employees to exclude any classified info from their responses. Transportation Secretary Sean Duffy <a href="https://x.com/SecDuffy/status/1894031690256818515" target="_blank">embraced the challenge </a>himself in a post on social media.</p><p>Musk appears to be following the same playbook he used when he bought Twitter, which he renamed X.</p><p>Musk began his tenure there with massive layoffs, asking employees to commit to "Extremely hardcore" work in an email titled "A Fork in the Road" or be fired. The email subject line is the same as the email sent out to federal employees by the Office of Personnel Management offering buyouts in January. About<a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank"> 75,000 </a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">f</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">deral e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">mployees</a> took the deal.</p><p>Twitter employees who stayed were then asked to print out pages of code they'd written from the last month and prepare to present the work to Musk personally. The code reviews reportedly were abandoned, and instead, managers were asked to rank their employees, according to <a href="https://www.theverge.com/23551060/elon-musk-twitter-takeover-layoffs-workplace-salute-emoji" target="_blank">The Verge</a>.</p><p>Musk and DOGE's access to government data and information has become a central point of friction between the group and its critics. In at least <a href="https://www.nbcnews.com/tech/security/doge-lawsuits-11-cases-musk-group-focus-data-privacy-rcna191695" target="_blank">11 lawsuits</a>, plaintiffs have argued that DOGE has flouted laws and rules around data and privacy. Some of the lawsuits have referenced allegations that DOGE is using artificial intelligence to analyze and process government data. The <a href="https://www.washingtonpost.com/nation/2025/02/06/elon-musk-doge-ai-department-education/" target="_blank">Washington Post reported</a> in February that DOGE was using artificial intelligence to analyze spending at the Education Department, citing two people familiar with the project.</p><p>House Speaker Mike Johnson, R-La.,<a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank"> </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">on Monday </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">touted Musk's work</a> in remarks that seemed to confirm the AI reports.</p><p>"Elon has cracked the code. He is now inside the agencies. He’s created these algorithms that are constantly crawling through the data. And as he told me in his office, the data doesn’t lie. We’re going to be able to get the information. We’re going to be able to transform the way the federal government works at the end of this, and that is a very exciting prospect. It is truly a revolutionary moment for the nation,” Johnson said at an event in Washington.</p><p>Sen. Lisa Murkowski, R-Alaska, had some harsh words for the way DOGE was operating, calling the email sent to federal workers “intimidation,”&nbsp;and saying that she’s hearing from federal workers who are being “treated with a level of disregard to their service and to their tenure.”</p><p>“Just a little bit of humanity and dignity to the process, I think, is what many of the Alaskan federal employees are asking for, and I don’t think that that’s asking for too much,”&nbsp;Murkowski said.</p><p>DOGE's work has led to criticism and instances of workers having to be rehired after they were removed from essential jobs. On Monday, two people familiar with the matter said the administration was <a href="https://www.nbcnews.com/health/health-news/fda-rehires-staff-medical-devices-division-mass-layoffs-rcna193501" target="_blank">reinstating some employees</a> in the Food and Drug Administration’s medical devices division after dozens were laid off as part of DOGE's <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">cost-cutting initiative</a>.</p><p>The medical devices division is responsible for approving and monitoring the safety of a range of products, from X-ray machines to surgical implants. The layoffs took place earlier this month, and included physicians and cybersecurity experts.</p><p>Some of those employees received phone calls or emails over the weekend informing them that their termination had been rescinded. It’s unclear how many employees were offered their jobs back, or how many would ultimately return.</p><p>The administration had a similar issue this month with<a href="https://www.nbcnews.com/politics/national-security/trump-administration-wants-un-fire-nuclear-safety-workers-cant-figure-rcna192345" target="_blank"> nuclear safety personnel</a> who had been let go.</p><p>In a court ruling Friday, a federal judge in New York issued a preliminary&nbsp;injunction&nbsp;<a href="https://www.nbcnews.com/politics/politics-news/judge-temporarily-blocks-doge-sensitive-treasury-department-systems-rcna191316" target="_blank">barring DOGE’s access&nbsp;</a>to sensitive Treasury Department systems after a coalition of states <a href="https://www.nbcnews.com/politics/trump-administration/trump-scored-big-legal-wins-week-efforts-reshape-government-still-face-rcna193000" target="_blank">presented evidence </a>that its employees weren't following proper safety protocols.</p><p>In a&nbsp;<a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.636609/gov.uscourts.nysd.636609.76.0_2.pdf" target="_blank">scathing ruling</a>, U.S. District Judge Jeanette Vargas blasted DOGE's "chaotic and haphazard approach" and found the coalition had “established that there is a realistic danger that confidential financial information will be disclosed absent the grant of injunctive relief.”</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621">Courtney Kube</a></span><span><a href="https://x.com/ckubeNBC" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Courtney Kube is a correspondent covering national security and the military for the NBC News Investigative Unit.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511">Julie Tsirkin</a></span><span><a href="https://x.com/JulieNBCNews" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Julie&nbsp;Tsirkin is a&nbsp;correspondent covering Capitol Hill.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685">Yamiche Alcindor</a></span><span><a href="mailto:Yamiche.Alcindor@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Yamiche Alcindor is an NBC News Washington correspondent. </p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/laura-strickler-ncpn894696">Laura Strickler</a></span><span><a href="https://x.com/strickdc" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Laura.Strickler@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Laura Strickler is a senior investigative producer and reporter for NBC News. She is based in Washington.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686">Dareh Gregorian</a></span><span><a href="https://x.com/darehgregorian" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Dareh.Gregorian@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Dareh Gregorian is a politics reporter for NBC News.</p></div><div><p>Ben Goggin</p><!-- --><p>, </p><!-- --><p>Sarah Dean</p><!-- --><p>, </p><!-- --><p>Ken Dilanian</p><!-- --><p>, </p><!-- --><p>Allan Smith</p><!-- --><p>, </p><!-- --><p>Jonathan Allen</p><!-- --><p>, </p><!-- --><p>Julia Jester</p><!-- --><p>, </p><!-- --><p>Ryan J. Reilly</p><!-- --><p>, </p><!-- --><p>Megan Lebowitz</p><!-- --><p>, </p><!-- --><p>Ryan Nobles</p><!-- --><p>, </p><!-- --><p>Frank Thorp V</p><!-- --><p>, </p><!-- --><p>Zoë Richards</p><!-- --><p> and </p><!-- --><p>Phil Helsel</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embedding Python in Elixir, It's Fine (211 pts)]]></title>
            <link>https://dashbit.co/blog/running-python-in-elixir-its-fine</link>
            <guid>43171239</guid>
            <pubDate>Tue, 25 Feb 2025 12:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dashbit.co/blog/running-python-in-elixir-its-fine">https://dashbit.co/blog/running-python-in-elixir-its-fine</a>, See on <a href="https://news.ycombinator.com/item?id=43171239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    
<ul>
  <li>
    <i></i> Jonatan Kłosko
  </li>
  <li>
    <i></i> February 21st, 2025
  </li>
  <li>
    <i></i><a href="https://dashbit.co/blog/tags/python">python</a>, <a href="https://dashbit.co/blog/tags/livebook">livebook</a>, <a href="https://dashbit.co/blog/tags/nifs">nifs</a>
  </li>
</ul>
<p>
In the recent years, Elixir has been expanding its capabilities in Machine Learning and Data through the <a href="https://github.com/elixir-nx">Nx (Numerical Elixir)</a> effort. A number of projects emerged (Nx, Explorer, Axon, Bumblebee, Scholar, and more), drawing learnings from decades of work in ecosystems such as Python and R, often standing on the shoulders of C++ and Rust codebases.</p>
<p>
When we started, we made the explicit choice to not depend on Python libraries directly. We wanted to design and develop our ecosystem with full control of making the best decisions for Elixir, which would not necessarily match the decisions made for Python. We also wished to avoid bringing to our ecosystem the complexities in getting a Python environment up and running. While young, the Nx ecosystem already enabled <a href="https://youtu.be/VcOvNTxUaIo">running pre-trained ML models</a>, <a href="https://youtu.be/5FlZHkc4Mq4">simplifying production systems with a unified AI stack</a>, <a href="https://youtu.be/4qoHPh0obv0">managing GPU cluster workflows from a notebook</a>, to point a few.</p>
<p>
A key component driving the adoption of Elixir in these areas is <a href="https://livebook.dev/">Livebook</a>, a computational notebook platform that builds on the strengths of the Elixir and Erlang, bringing reproducibility, distributed execution, and app development to the forefront. With Livebook, we have seen a growing interest from teams and companies in dipping their toes into the Elixir ecosystem for the first time.</p>
<p>
All of this builds a good case to go all in with Elixir, but some hurdles remain. As one would expect, most companies interested in bringing Elixir and Livebook into their infrastructure, have existing workflows, packages, and repositories that they already rely on. The choices we have made so far imply that they either have to find an equivalent package in Elixir or write one from scratch, increasing the risk and costs of adding Elixir to their data stack.</p>
<p>
To address these concerns, today we announce Pythonx, which embeds the Python interpreter within the Erlang VM, bringing automatic data conversion between Elixir and Python, code evaluation, and automatic virtual environment management. We compare Pythonx with other options for interoperability and outline future work.</p>
<h2>
Enter Pythonx</h2>
<p>
Imagine we have an image and want to read the text on that image. We need to do what is known as Optical Character Recognition (OCR). Sure enough, there are a few Python packages doing just that, one of them being <code>pytesseract</code>. For the sake of this example, we will download the image using Req:</p>
<pre><code><span>Mix</span><span>.</span><span>install</span><span data-group-id="8115916589-1">(</span><span data-group-id="8115916589-2">[</span><span>
  </span><span data-group-id="8115916589-3">{</span><span>:pythonx</span><span>,</span><span> </span><span>"~&gt; 0.4.0"</span><span data-group-id="8115916589-3">}</span><span>,</span><span>
  </span><span data-group-id="8115916589-4">{</span><span>:req</span><span>,</span><span> </span><span>"~&gt; 0.5.8"</span><span data-group-id="8115916589-4">}</span><span>
</span><span data-group-id="8115916589-2">]</span><span data-group-id="8115916589-1">)</span><span>

</span><span>url</span><span> </span><span>=</span><span> </span><span>"https://unsplash.com/photos/95t94hZTESw/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzQwMDYwMjg4fA&amp;force=true&amp;w=640"</span><span>
</span><span>binary</span><span> </span><span>=</span><span> </span><span>Req</span><span>.</span><span>get!</span><span data-group-id="8115916589-5">(</span><span>url</span><span data-group-id="8115916589-5">)</span><span>.</span><span>body</span></code></pre>
<p>
Now, let’s bring in Python.</p>
<pre><code><span>Pythonx</span><span>.</span><span>uv_init</span><span data-group-id="6229236571-1">(</span><span>"""
[project]
name = "project"
version = "0.0.0"
requires-python = "==3.13.*"
dependencies = [
  "pytesseract==0.3.13",
  "pillow==11.1.0"
]
"""</span><span data-group-id="6229236571-1">)</span></code></pre>
<p>
Calling <code>Pythonx.uv_init/1</code> downloads Python and the listed dependencies using the excellent <a href="https://docs.astral.sh/uv">uv</a> package manager. It also immediately initializes the Python interpreter for evaluation. Note the dependencies section where we list <code>pytesseract</code> for OCR and <code>pillow</code> for image handling.</p>
<p>
Next, let’s write some Python.</p>
<pre><code><span data-group-id="8721759126-1">{</span><span>result</span><span>,</span><span> </span><span>_globals</span><span data-group-id="8721759126-1">}</span><span> </span><span>=</span><span>
  </span><span>Pythonx</span><span>.</span><span>eval</span><span data-group-id="8721759126-2">(</span><span>
    </span><span>"""
    import pytesseract
    import io
    import PIL

    image = PIL.Image.open(io.BytesIO(binary))
    pytesseract.image_to_string(image)
    """</span><span>,</span><span>
    </span><span data-group-id="8721759126-3">%{</span><span>"binary"</span><span> </span><span>=&gt;</span><span> </span><span>binary</span><span data-group-id="8721759126-3">}</span><span>
  </span><span data-group-id="8721759126-2">)</span><span>

</span><span>Pythonx</span><span>.</span><span>decode</span><span data-group-id="8721759126-4">(</span><span>result</span><span data-group-id="8721759126-4">)</span><span>
</span><span>#=&gt; "The Journey\nof a thousand\nmiles begins\nwith a single\n\nstep.\n\n-Lao Tzu\n\n"</span></code></pre>
<p>
Above we call <code>Pythonx.eval/2</code>, which accepts Python code and a map with variables for the evaluation. Note how we pass the Elixir binary and it is automatically converted to a <code>bytes</code> object on the Python side. The evaluation returns <code>result</code>, which is a <code>%Pythonx.Object{}</code>, and also an updated map with variables. In this case we only care about the result and we use <code>Pythonx.decode/1</code> to convert it to an Elixir string right away.</p>
<p>
There we go! To learn more about Pythonx, see <a href="https://hexdocs.pm/pythonx">the documentation</a>. And if you are struggling to write Python for your task, consult with your AI specialist, it went to school for that.</p>
<h2>
Under the hood</h2>
<p>
If you are raising your eyebrow, thinking that this just calls <code>python</code>, bear with me!</p>
<p>
So Python, or more specifically its <a href="https://github.com/python/cpython">CPython</a> reference implementation, has the interesting capability of being embedded into other applications. What this means is that the core functionality of the Python interpreter is available as a C library, so a C/C++ application can link that library and use its APIs to run code and interact with objects. In fact, you can think of the <code>python</code> executable as one such application.</p>
<p>
Elixir provides C/C++ interoperability via Erlang NIFs and that’s exactly what Pythonx uses to embed Python, which means that the Python interpreter operates in the same OS process as Elixir itself. By living in the same memory space, passing data between Elixir and Python is cheap. Pythonx ties Python and Erlang garbage collection, so that the objects can be safely kept between evaluations. Also, it conveniently handles conversion between Elixir and Python data structures, bubbles Python exceptions and captures standard output.</p>
<h2>
Livebook goes multilingual</h2>
<p>
To enable even more powerful workflows, we <a href="https://github.com/livebook-dev/livebook/pull/2936">started working</a> on Python support in Livebook, building on Pythonx. The idea, though, is not to support Python separately, but rather to allow Elixir and Python interacting in the same notebook! To give you a better picture, below you can see the same example using Python cells in Livebook nightly.</p>
<p><img src="https://dashbit.co/images/posts/2025/pythonx_ocr.png" width="100%"></p>
<p>
Livebook automatically installs Python and its dependencies, as it manages Elixir’, ensuring a reproducible environment. It also tracks which Elixir variables are used by Python, and vice-versa, and automatically converts them between cells. While there is still work ahead of us, including code completion, documentation, and a few surprises, we are open to feedback. <a href="https://github.com/livebook-dev/livebook#desktop-app">You can download Livebook nightly</a> to give it a try. Once we add all bells and whistles, we will do an official announcement over <a href="https://news.livebook.dev/">news.livebook.dev</a>.</p>
<p>
At this point I want to thank <a href="https://github.com/cocoa-xu">Cocoa Xu</a> for starting off the work on Embedded Python, and <a href="https://github.com/cigrainger">Christopher Grainger</a> for the initial push to run Python in Livebook.</p>
<h2>
Usage considerations and alternatives</h2>
<p>
The primary goal of Pythonx is to better integrate Python workflows within Livebook and scripts. Pythonx usage in actual projects must be done with care due to Python’s global interpreter lock (GIL). The GIL prevents multiple threads from executing Python code at the same time, so calling <code>Pythonx</code> from multiple Elixir processes does not provide the concurrency you might expect and thus it can be a source of bottlenecks. However, this limitation concerns regular Python code. Packages with CPU-intense functionality, such as <code>numpy</code>, have native implementation of many functions and invoking those releases the GIL. The GIL is also released when waiting on I/O operations. In other words, if you are using this library to integrate with Python, make sure it happens in a single Elixir process or that its underlying libraries can deal with concurrent invocation.</p>
<p>
If the above is a dealbreaker, remember that interoperability already exists at a few levels. For example, you could write a Python script and then invoke it with <a href="https://hexdocs.pm/elixir/System.html#cmd/3"><code>System.cmd/3</code></a> or open a <a href="https://hexdocs.pm/elixir/Port.html">Port</a>. In those cases, you could start several or even a pool of Python processes that you would manage.</p>
<p>
Furthermore, depending on your needs, you may also be able to interoperate through higher-level abstractions. For example, for AI workflows, you can run pre-trained models directly, some via <a href="https://github.com/elixir-nx/bumblebee">Bumblebee</a>, others via <a href="https://github.com/elixir-nx/ortex">Ortex</a>. When using an LLM, you often end up talking to a third-party provider, or perhaps you run a drop-in llama.cpp Docker container on-premise, optimised for inference. In such cases the interface is HTTP and Elixir has high-level tools for interacting with LLMs too, namely <a href="https://github.com/thmsmlr/instructor_ex">Instructor</a> and <a href="https://github.com/brainlid/langchain">LangChain</a>.</p>
<p>
That said, if you do decide that Pythonx fits into your application, you can configure it to download all Python dependencies at compile time and include them as part of the Elixir release. For more details, refer to <a href="https://hexdocs.pm/pythonx/Pythonx.html#module-usage-application">this section</a> in the doc.</p>
<p>
You could also use Pythonx to give you immediate access to more tools to unblock you. Once your idea pays off, you can invest more time to arrive at a Elixir-centric solution, if you so desire.</p>
<h2>
It’s Fine</h2>
<p>
Speaking of interoperability, I mentioned that Pythonx uses NIFs. NIFs are Elixir functions with the implementation living in C. We reach for NIFs either when we want to write native code with mutability for something performance-critical or when we integrate with third-party libraries via C API (often both).</p>
<p>
To give an example, below is a NIF implementation that adds two numbers.</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>erl_nif.h</span><span>&gt;</span><span>
</span><span>
</span><span>ERL_NIF_TERM </span><span>add</span><span>(</span><span>ErlNifEnv</span><span>*</span><span> </span><span>env</span><span>,</span><span> </span><span>int</span><span> </span><span>argc</span><span>,</span><span> </span><span>const</span><span> ERL_NIF_TERM </span><span>argv</span><span>[</span><span>]</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>int</span><span> x</span><span>,</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>if</span><span> </span><span>(</span><span>argc </span><span>!=</span><span> </span><span>2</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>0</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>x</span><span>)</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>1</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>y</span><span>)</span><span>)</span><span> </span><span>{</span><span>
</span><span>    </span><span>return</span><span> </span><span>enif_make_badarg</span><span>(</span><span>env</span><span>)</span><span>;</span><span>
</span><span>  </span><span>}</span><span>
</span><span>
</span><span>  </span><span>int</span><span> result </span><span>=</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>return</span><span> </span><span>enif_make_int</span><span>(</span><span>env</span><span>,</span><span> result</span><span>)</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>ErlNifFunc nif_funcs</span><span>[</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>
</span><span>  </span><span>{</span><span>"</span><span>add</span><span>"</span><span>,</span><span> </span><span>2</span><span>,</span><span> add</span><span>}</span><span>
</span><span>}</span><span>;</span><span>
</span><span>
</span><span>ERL_NIF_INIT</span><span>(</span><span>Elixir</span><span>.</span><span>MyLib</span><span>.</span><span>NIF</span><span>,</span><span> nif_funcs</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>)</span></code></pre>
<p>
Looking at the signature, you can see that the function receives a C-array of Erlang terms and returns an Erlang term. We are responsible for converting between terms and C data structures using the <code>enif_*</code> APIs. The example may look pretty straightforward, though it is a fair amount of boilerplate code to end up adding two numbers. From there the ceremony escalates quickly once we need to deal with nested data structures and return errors more specific than <code>:badarg</code>. A natural progression is to extract some of the logic to helper functions, but this doesn’t fully alleviate the boilerplate and it results in reinventing the wheel a lot.</p>
<p>
Additionally, Pythonx (and other NIF-extensive projects) actually use C++, while the <code>enif_*</code> APIs are (rightfully so) C. Since C++ brings more powerful constructs, theoretically there is a possibility of a more expressive API, however it is also easy to get into weeds with C++ metaprogramming. The main question I asked myself is how far can we go inferring the conversion from types. With <a href="https://github.com/rusterlium/rustler">Rustler</a> and <a href="https://github.com/E-xyza/zigler">Zigler</a>, NIFs are written as regular functions and the data structures conversion is handled automatically based on the signature types.</p>
<p>
This brings us to <a href="https://github.com/elixir-nx/fine">Fine</a>, C++ library enabling more ergonomic NIFs, tailored to Elixir. Let’s see an update example:</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>fine.hpp</span><span>&gt;</span><span>
</span><span>
</span><span>int64_t</span><span> </span><span>add</span><span>(</span><span>ErlNifEnv </span><span>*</span><span>env</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>x</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>y</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>return</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>FINE_NIF</span><span>(</span><span>add</span><span>,</span><span> </span><span>0</span><span>)</span><span>;</span><span>
</span><span>
</span><span>FINE_INIT</span><span>(</span><span>"</span><span>Elixir.MyLib.NIF</span><span>"</span><span>)</span><span>;</span></code></pre>
<p>
Other than extendable encoding/decoding, Fine provides smart pointers to safely manage resource objects and support for raising exceptions anywhere in the NIF. I’ve refactored EXLA NIFs to use Fine and <a href="https://github.com/elixir-nx/nx/pull/1581">it removed over 1k LOC</a>, so it may be worth considering next time you have to write some NIFs.</p>
<h2>
Summing up</h2>
<p>
When we started Numerical Elixir, our goal was for Elixir to develop and have its own identity within the data and machine learning ecosystem. Now we are ready to make interoperability a key focus of our efforts too.</p>
<p>
Pythonx embeds Python into Elixir, bringing a new class of interoperability with a third-party language not seen before within the Erlang VM. It is more than just integrating the Python interpreter, it is about transparently translating idioms from one language to the other.</p>
<p>
The Fine project also consolidates and streamlines our collective experiences in integrating C++ and Elixir, tracing back to Sean Moriarity’s work on Nx four years ago.</p>
<p>
There is more to come.</p>
<p>
Stay interoperable!</p>

  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal to leave Sweden if backdoor law passes (432 pts)]]></title>
            <link>https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</link>
            <guid>43171205</guid>
            <pubDate>Tue, 25 Feb 2025 12:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden">https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</a>, See on <a href="https://news.ycombinator.com/item?id=43171205">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single-entry-content"><p>According to Whittaker, the bill requires the encrypted messaging app Signal to install so-called backdoors in the software.</p>
<blockquote>
<p>If you create a vulnerability based on Swedish wishes, it would create a way to undermine our entire network. Therefore, we would never introduce these backdoors, she says.</p>
</blockquote>
<p>The purpose of the bill – which may be passed next year – is for the police and Security Service to be able to request message history in retrospect for individuals suspected of crimes.</p>
<p>The Armed Forces, on the other hand, are negative and write in a letter to the government that the proposal cannot be realized "without introducing vulnerabilities and backdoors that can be exploited by third parties", reports SVT.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Y Combinator backing AI company to abuse factory workers (372 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43170850</link>
            <guid>43170850</guid>
            <pubDate>Tue, 25 Feb 2025 12:04:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43170850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43171286"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171286" href="https://news.ycombinator.com/vote?id=43171286&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Tbh this isn’t that crazy. If you hire someone to do their job outputting 10 items per hour and that number is reasonable because a bunch of other workers you hired for the same job are doing it and 1 guy hits 1 per hour then that guys shouldn’t be doing that job.</p><p>The outrage should be focused on the absolute meme of their ad video cuz they were like “lets literally have a convo with an individual but refer to them as a workspace and have them say human painful responses but then just shit on them anyway impersonally”</p><p>The product is not crazy. The video is wild.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171389"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171389" href="https://news.ycombinator.com/vote?id=43171389&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Because this will definitely be used only to innocently tell off people doing 1/10 the work of everyone else, and not micromanage and hound people to increasingly unrealistic standards in already desperate conditions.</p><p>Safe to say you aren't in any position where every move you make will be watched by AI and analysed for faults so that your boss can scream at you more efficiently whenever you don't meet standards for their pitiful wages.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171537"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171537" href="https://news.ycombinator.com/vote?id=43171537&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Your example sounds reasonable but it's not realistic: The actual use of this type of tools is to intimidate those workers who have outputted 9.8 items instead of the average 9.9 over the past week.</p><p>This is who our society ended up making Amazon delivery workers urinating in fucking bottles inside their trucks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171411"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171411" href="https://news.ycombinator.com/vote?id=43171411&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Shouldn't the manager of the 'bunch of workers' notice the guy is underperforming and understand why ? Maybe that manager is the one that shouldn't be doing that job</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171485"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171485" href="https://news.ycombinator.com/vote?id=43171485&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>And they do that by either looking over your shoulder (1 person at a time) or collecting metrics on the entire team and the output. Both of these have different downsides.</p><p>The biggest issue is leadership or managers always wanting the number to go up from the individual. "We need 12 widgets per hour instead of 10 for just this one quarter bro" but then that becomes the new norm and eventually "We need 14/16/18/20 widgets per hour..."</p><p>It's boiling frog management that makes people distrust managers doing any kind of performance monitoring</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171528"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171528" href="https://news.ycombinator.com/vote?id=43171528&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>yah this is exactly what labor law says in some countries:  a manager standing behind your desk?  ok.  a machine surveilling you?  not ok.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171304"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171304" href="https://news.ycombinator.com/vote?id=43171304&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Continued rant :</p><p>It’s kinda like a ruler. If you measure workers so that one’s doing 10x less/worse output than the average that’s good.</p><p>If you compare workers down to the .01% difference in output that’s stupid and inhumane.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171550"><td></td></tr>
                  <tr id="43171524"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171524" href="https://news.ycombinator.com/vote?id=43171524&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>It's clear you never worked in a factory and you have just as much empathy as these CS grads.
This kind of thinking why I hate capitalism so much.</p><p>I worked in a factory multiple times and I can tell from experience nobody needs a stupid performance measurement like this. Your manager will make sure you work you ass off. Or you work with a big dangerous machine so you have to pay very much attention all day.
Of course not every factory is the same, but putting even more pressure to factory workers like this is just inhumane and the most capitalist move I can imagine. Next step is to put robotic whips next to the lines and when their productivity goes below a specific value hit them automatically... Literal slavery.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171536"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171536" href="https://news.ycombinator.com/vote?id=43171536&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Looking for 10x discrepancies is not how this will be used, and you know it. Adoption of this sort of tech is going to lead to Amazon "peeing in bottles" situations. It's wild how much faith people have in the ethics of business owners, especially the ultra-wealthy.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171313"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171313" href="https://news.ycombinator.com/vote?id=43171313&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; <i>Tbh this isn’t that crazy.</i></p><p>Yep, seems like a bog standard accountability / performance management.</p><p>&gt; <i>The product is not crazy. The video is wild.</i></p><p>This is how it all starts. Sane solutions wielded by madmen.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171360"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171360" href="https://news.ycombinator.com/vote?id=43171360&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>This sort of performance management is unfortunately necessary. The problem is that we need tools for it to be built by people who can empathise with those subjected to them, and who want to do the right thing, and not these sorts of folks who are too immature and inexperienced to get it right.</p><p>My previous company ran a warehouse and there was a clear bell curve of productivity. Most people were fine, some were excellent, but some were below the level that was realistically achievable. We did careful and considerate analysis and it helped improve productivity.</p><p>When done badly however you end up with management using productivity tracking as a lever to increase productivity across the curve. Amazon driver delivery quotas are a great example – people urinating in bottles is clearly a symptom of the quota being too high. Unfortunately software built naively to help bring up the bottom 10% can too easily be used to force up the productivity of the other 90%.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171508"><td></td></tr>
                  <tr id="43171378"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171378" href="https://news.ycombinator.com/vote?id=43171378&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>I used to run a small factory (two assembly lines 10 people) and something like this would have been useful, not to force people to work harder but the optimise movements and points of friction. I would actively encourage and reward people for making suggestion and we had a process in place to test if changes made thing better (and not just faster - we included easier, simpler, more enjoyable etc in the test)</p><p>Sadly it’s not about the tool in this case, it’s how it’s being promoted and positioned. The line “know who’s working and who’s not” on their website says it all sadly.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171394"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171394" href="https://news.ycombinator.com/vote?id=43171394&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>As someone who grew up in a 3rd world country and whose mother owned a clothing factory, this product seems...fine? The response is an indication of how little people know about how their t-shirts and shoes are made.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171478"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171478" href="https://news.ycombinator.com/vote?id=43171478&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>It's nuanced. If it allows you to find outliers (low performers to manage and high performers to praise), that's fine. If you try to push everyone further and further to their breaking point and make them trade the same amount of money for more of their time and more importantly health, <i>it's certainly not fine</i>.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171419"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171419" href="https://news.ycombinator.com/vote?id=43171419&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Is it dystopian, or is it just real-time performance monitoring poorly marketed by inexperienced founders?</p><p>There are tools like this for tracking git commits and velocity (that I’ve been on the receiving end of). It probably makes less sense in that context, but if your job is a repetitive task, I don’t think it’s necessarily abuse or dystopian to track it.</p><p>Monitoring bottlenecks isn’t a bad thing. They probably could have chosen an example where the solution to the bottleneck didn’t involve berating a low performer (e.g. adjusting the line to add another station or similar)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171484"><td></td></tr>
                  <tr id="43171321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171321" href="https://news.ycombinator.com/vote?id=43171321&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Brought to you by the VC famous for InstallMonetizer? Make no mistake, it’ll basically back anything that makes money, there’s no moral high ground. And like it or not, this kind of AI (or should I say A-eye) is here to stay.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171431"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171431" href="https://news.ycombinator.com/vote?id=43171431&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Boost your assembly line efficiency by up to 30%</p><p>Ethics of this aside the above claim must be dubious I would think the majority of manufacturing inefficiencies are due to down time as a result of raw material shipping delays or machine break down… of course I’m in no position to offer an informed opinion but just based on the product website I have a hard time taking this stuff seriously.</p><p>Monitoring of factory workers isn’t hard to do with current surveillance and 1 or 2 humans in the loop</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171209"><td></td></tr>
            <tr id="43171369"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171369" href="https://news.ycombinator.com/vote?id=43171369&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Software can not abuse workers. Managers can, with our without software.</p><p>We've had automated KPI measuring tools since punch clocks. Nowadays it's OK in some companies to install remote access software to monitor employees' screens. It's nothing new. It's just collecting data. Question is, what will bosses do with this data, will they abuse or develop.</p><p>I have no hate towards those guys. No love also. It's just business.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171368"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171368" href="https://news.ycombinator.com/vote?id=43171368&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>The secondary school the founder went to is a dead giveaway. Ofcourse yc would fund them. Anyone would fund them infact.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171056"><td></td></tr>
                <tr id="43171434"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171434" href="https://news.ycombinator.com/vote?id=43171434&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Why not deploy it across every moment of everyone's life, with algorithmic prediction of economically unproductive deviance and BadThink?</p><p>Maybe I'll pitch that to someone with money.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43170866"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43170866" href="https://news.ycombinator.com/vote?id=43170866&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Thank you, I’ve been seeing the reaction to the announcement but hadn’t yet found the announcement itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171452"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171452" href="https://news.ycombinator.com/vote?id=43171452&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>That post was pulled partly because of my comment. I commented this:</p><p>"While I see the economical usefullness, this sounds like the worst possible application of AI.</p><p>Using AI to surveil is building hell on earth. AI should be used to help people work less/easier, not whip them into working more."</p><p>Which ended up on the top of the thread. Was surprised to wake up this morning and see it gone.</p><p>LinkedIn post I made about this:</p><p><a href="https://www.linkedin.com/posts/crufter_today-y-combinator-deleted-this-announcement-activity-7300050840852086786-x6WH?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABcNA2MB_d5j-ofPyG8XDq4OUyAsV3UKRKw" rel="nofollow">https://www.linkedin.com/posts/crufter_today-y-combinator-de...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171276"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171276" href="https://news.ycombinator.com/vote?id=43171276&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Imagine the features you could add to this. Like a robot that walks around behind the workers and gives well-timed corrective communications with a whip.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171339"><td></td></tr>
                  <tr id="43171022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171022" href="https://news.ycombinator.com/vote?id=43171022&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>This kind of product is really shameful, and peak capitalism... looking at people as mere robots to serve your, disgusting</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171316"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171316" href="https://news.ycombinator.com/vote?id=43171316&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Nobody looks at people as robots. Robots are cheaper, do not require food or sleep, and do not have to be murdered when they attempt to unionize.</p><p>Robots are far superior.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171370"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171370" href="https://news.ycombinator.com/vote?id=43171370&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Not yet for many tasks. Humans are more flexible, easier to replace (nothing to install etc) and one fte in this type of work is 10 years of robot. Hope it will change soon, but it's not there yet.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171348"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171348" href="https://news.ycombinator.com/vote?id=43171348&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>I think there is a balance.</p><p>Otherwise its going to be 1984 in more than one way (the spying part is already there) (it would also do of that the countries are ready to produce things as much but they won't and limit it to create that constant mood of war to make people not question them / make them weak.)</p><p>I think capitalism has fallen. Capitalism is a good system but to its degrees. If you push the accelerator too hard , you get fuedalism.</p><p>and we are at feudalism. I am not sure if we can undo this. Let this sink in, the american dream , all our thinkng that capitalism being good and communism being bad fundamentally doesn't matter because we have entered a system where the lines of division are so blurry that they are practically nonexistent.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171501"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43171501" href="https://news.ycombinator.com/vote?id=43171501&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>Last time I tried to say something like that I got plenty comments calling me for reading too many sci-fi books... I guess some people just lack imagination and experience with exponentials.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171393"><td></td></tr>
                        <tr id="43171243"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171243" href="https://news.ycombinator.com/vote?id=43171243&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Peak capitalism would have the employees holding their own value and not being crushed in trash power dynamics to allow this kind of stuff. Capitalism is about nonviolent voluntary exchanges between two parties, when one party has a power dynamic skewed in such a way they can use tools like this that employees hate, then that’s not capitalism anymore.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171503" href="https://news.ycombinator.com/vote?id=43171503&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Funny that what you describe will never exist without heavy (extreme?) regulation and gradual taxation, which are anathema to most advocates of capitalism. Have you considered that maybe your definition of capitalism doesn't agree with the definition society has agreed upon?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171322"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171322" href="https://news.ycombinator.com/vote?id=43171322&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>so this is more like peak feudalism huh?</p><p>back to 1800's I suppose.</p><p>There is no seperation b/w private entities , the state and the church , all trying to exploit the middle class / lower class was one of the gists that I think when I recall feudalism
sounds familiar ?
Guess what ?
We are living at one right now.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171301"><td></td></tr>
                <tr id="43171379"><td></td></tr>
                        <tr id="43171053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171053" href="https://news.ycombinator.com/vote?id=43171053&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Apparently on their website they're asking for comments/feedback, so... you can tell them yourself what you think of their disgusting tech</p><p>&gt; Let us know at founders@optifye.ai, and we’ll help them drop their cortisol levels :)</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171315"><td></td></tr>
                <tr id="43171375"><td></td></tr>
                  <tr id="43171257"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171257" href="https://news.ycombinator.com/vote?id=43171257&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Yeah it has terrible optics, yet it's clearly going to be normalized and come.  The question is who does it and what is the organization of it.  If this company doesn't do it, the next will.</p><p>In certain roles, AI micromanagement clearly will create higher performance.  Add the marketplace of capitalism and it'll all compete away.</p><p>There are certain roles, like artists, where this is the wrong solution wholly: monitoring whether an artist is at her desk will create badly performing artists, and this will show.  In these roles, these tools won't apply.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171324" href="https://news.ycombinator.com/vote?id=43171324&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p><i>In these roles, these tools won't apply.</i></p><p>There will be companies that will apply them regardless, even in roles where they'll make things worse. The incentive for managers to show 'a bias for action' often results in managers doing any action that they can think of rather than the right action backed by data.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171335"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171335" href="https://news.ycombinator.com/vote?id=43171335&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>In the US? Sure. In more developed parts of the world? Doubtful. European labor laws are already much, much stronger than their US counterparts, and most countries outright ban using cameras to monitor employees.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171303" href="https://news.ycombinator.com/vote?id=43171303&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Yeah it has terrible optics, yet it's clearly going to be normalized and come. The question is who does it and what is the organization of it. If this company doesn't do it, the next will.</p><p>Where have we seen this before..</p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla sales in Europe down 45% in January (241 pts)]]></title>
            <link>https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</link>
            <guid>43170090</guid>
            <pubDate>Tue, 25 Feb 2025 10:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523">https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</a>, See on <a href="https://news.ycombinator.com/item?id=43170090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offer-6dcd8564-bf41-453c-93b5-7b00c6676b60" data-component="heroOffer" data-component-unique-name="Hero offer"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Limited time offer</span></h2><h2><strong><span>Save 40% on Standard Digital</span></strong></h2></p><p><span>was </span><span>CHF660</span><span> </span><span>now </span><span>CHF395</span><span> for your first year
Make up your own mind. Build robust opinions on the FT's trusted journalism.
Offer available until 27 February 2025.</span></p></div></div><div id="recommendedOffers-Recommended offers" data-component="recommendedOffers" data-component-unique-name="Recommended offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_trial.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF1</span><span> for 4 weeks</span></p><p><span>Then </span><span>CHF85</span><span> per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_weekend_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>was </span><span>CHF949</span><span> </span><span>now </span><span>CHF815</span><span> per year</span></p><p><span>Get Premium &amp; FT Weekend Print edition for the price of Premium. Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_print.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF345</span><span> for your first year</span></p><p><span>FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription options"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=cdd0b5c8-2703-4fd4-9ebf-26087cac8523">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Awesome DeepSeek Integrations (113 pts)]]></title>
            <link>https://github.com/deepseek-ai/awesome-deepseek-integration</link>
            <guid>43169827</guid>
            <pubDate>Tue, 25 Feb 2025 09:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/awesome-deepseek-integration">https://github.com/deepseek-ai/awesome-deepseek-integration</a>, See on <a href="https://news.ycombinator.com/item?id=43169827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><table>
    <tbody><tr>    
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/build/icon.png?raw=true"><img src="https://github.com/ThinkInAIXYZ/deepchat/raw/main/build/icon.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/README.md">DeepChat</a></td>
        <td>DeepChat is a fully free desktop smart assistant, with a powerful DeepSeek large model, supporting multi-round conversations, internet search, file uploads, knowledge bases, and more.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4"><img src="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/quantalogic/quantalogic">Quantalogic</a> </td>
        <td> QuantaLogic is a ReAct (Reasoning &amp; Action) framework for building advanced AI agents. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI"><img src="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatbox/README.md">Chatbox</a> </td>
        <td> Chatbox is a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU"><img src="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatgpt_next_web/README.md"> ChatGPT-Next-Web </a> </td>
        <td> ChatGPT Next Web is a cross-platform ChatGPT web UI, with GPT3, GPT4 &amp; Gemini Pro support. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/assets/favicon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Coco%20AI/assets/favicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/README.md">Coco AI</a></td>
        <td> <a href="https://coco.rs/" rel="nofollow">Coco AI</a> is a fully open-source, cross-platform unified search and productivity tool that connects and searches across various data sources, including applications, files, Google Drive, Notion, Yuque, Hugo, and more, both local and cloud-based. By integrating with large models like DeepSeek, Coco AI enables intelligent personal knowledge management, emphasizing privacy and supporting private deployment, helping users quickly and intelligently access their information. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/assets/liubai-logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/liubai/assets/liubai-logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/README.md">Liubai</a> </td>
        <td> Liubai allows DeepSeek to have arms and legs to manipulate your notes, tasks, calendars, and to-do lists just on WeChat! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc"><img src="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/pal/README.md"> Pal - AI Chat Client<br>(iOS, ipadOS) </a> </td>
        <td> Pal is a customized chat playground on iOS. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667"><img src="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667" alt="LibreChat" width="64" height="auto" data-canonical-src="https://www.librechat.ai/librechat.svg"></a> </td>
        <td> <a href="https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints/deepseek" rel="nofollow">LibreChat</a> </td>
        <td> LibreChat is a customizable open-source app that seamlessly integrates DeepSeek for enhanced AI interactions. </td>
    </tr>
     <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png"><img src="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/longevity-genie/just-chat">Just-Chat</a> </td>
        <td> Make your LLM agent and chat with it simple and fast!</td>
     </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f" alt="PapersGPT" width="64" height="auto" data-canonical-src="https://www.papersgpt.com/images/logo/favicon.ico"></a> </td>
        <td> <a href="https://github.com/papersgpt/papersgpt-for-zotero">PapersGPT</a> </td>
        <td> PapersGPT is a Zotero plugin that seamlessly with DeepSeek and other multiple AI models for quickly reading papers in Zotero. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico"><img src="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/rss_translator/README.md"> RSS Translator </a> </td>
        <td> Translate RSS feeds into your language! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png"><img src="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/enconvo/README.md"> Enconvo </a> </td>
        <td> Enconvo is the Launcher of the AI era, the entry point for all AI functions, and a thoughtful intelligent assistant.</td>
    </tr>
    <tr>
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/kangfenmao/cherry-studio/blob/main/src/renderer/src/assets/images/logo.png?raw=true"><img src="https://github.com/kangfenmao/cherry-studio/raw/main/src/renderer/src/assets/images/logo.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/cherrystudio/README.md">Cherry Studio</a></td>
        <td>A powerful desktop AI assistant for producer</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://tomemo.top/images/logo.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/tomemo/README.md"> ToMemo (iOS, ipadOS) </a> </td>
        <td> A phrasebook + clipboard history + keyboard iOS app with integrated AI macromodeling for quick output use in the keyboard.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png"><img src="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/buxuku/video-subtitle-master">Video Subtitle Master</a></td>
        <td> Batch generate subtitles for videos, with the ability to translate subtitles into other languages. This is a client-side tool that supports both Mac and Windows platforms and integrates with multiple translation services such as Baidu, Volcengine, DeepLx, OpenAI, DeepSeek, and Ollama.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/dist/assets/chatworm-72x72.png"><img src="https://github.com/UnknownEnergy/chatgpt-api/raw/master/dist/assets/chatworm-72x72.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/README.md">Chatworm</a> </td>
        <td> Chatworm is a webapp for multiple cutting-edge LLM models, open-source and also available on Android. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png"><img src="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/tisfeng/Easydict">Easydict</a></td>
        <td> Easydict is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text. Supports calling large language model APIs for translation.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67"><img src="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://www.raycast.com/favicon-production.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/raycast/README.md">Raycast</a></td>
        <td> <a href="https://raycast.com/?via=ViGeng" rel="nofollow">Raycast</a> is a productivity tool for macOS that lets you control your tools with a few keystrokes. It supports various extensions including DeepSeek AI.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://niceprompt.app/favicon.ico"></a> </td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a></td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a> Organize, share and use your prompts in your code editor, with Cursor and VSCode。</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4" alt="PHP Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-php-client/blob/master/README.md">PHP Client</a> </td>
        <td> Deepseek PHP Client is a robust and community-driven PHP client library for seamless integration with the Deepseek API. </td>
    </tr>
        <tr>
  <td>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/logo.webp"><img src="https://github.com/tornikegomareli/DeepSwiftSeek/raw/main/logo.webp" alt="DeepSwiftSeek Logo" width="64" height="auto"></a>
  </td>
  <td>
    <a href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/README.md">DeepSwiftSeek</a>
  </td>
  <td>
    DeepSwiftSeek is a lightweight yet powerful Swift client library, pretty good integration with the DeepSeek API. 
    It provides easy-to-use Swift concurrency for chat, streaming, FIM (Fill-in-the-Middle) completions, and more.
  </td>
</tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4" alt="Laravel Integration" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-laravel/blob/master/README.md">Laravel Integration</a> </td>
        <td> Laravel wrapper for Deepseek PHP client, to seamless deepseek API integration with laravel applications.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/assets/zotero-icon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/zotero/assets/zotero-icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/README_cn.md">Zotero</a></td>
        <td> <a href="https://www.zotero.org/" rel="nofollow">Zotero</a> is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67"><img src="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://b3log.org/images/brand/siyuan-128.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/SiYuan/README.md">SiYuan</a> </td>
        <td> SiYuan is a privacy-first personal knowledge management system that supports complete offline usage, as well as end-to-end encrypted data sync.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png"><img src="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/ArvinLovegood/go-stock/blob/master/README.md">go-stock</a> </td>
        <td>go-stock is a Chinese stock data viewer built by Wails with NativeUI and powered by LLM.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4" alt="Wordware" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/wordware/README.md">Wordware</a> </td>
        <td><a href="https://www.wordware.ai/" rel="nofollow">Wordware</a> is a toolkit that enables anyone to build, iterate, and deploy their AI stack with just natural language.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67"><img src="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://framerusercontent.com/images/xRJ6vNo9mUYeVNxt0KITXCXEuSk.png"></a> </td>
        <td> <a href="https://github.com/langgenius/dify/">Dify</a> </td>
        <td> <a href="https://dify.ai/" rel="nofollow">Dify</a> is an LLM application development platform that supports DeepSeek models for creating assistants, workflows, text generators, and more. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico"><img src="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico" alt="Big-AGI" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/enricoros/big-AGI/blob/v2-dev/README.md">Big-AGI</a> </td>
        <td><a href="https://big-agi.com/" rel="nofollow">Big-AGI</a> is a groundbreaking AI suite designed to democratize access to advanced artificial intelligence for everyone.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/LiberSonora/LiberSonora/blob/main/assets/avatar.jpeg?raw=true"><img src="https://github.com/LiberSonora/LiberSonora/raw/main/assets/avatar.jpeg?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/LiberSonora/LiberSonora/blob/main/README_en.md">LiberSonora</a> </td>
        <td> LiberSonora, meaning "Voice of Freedom", is an AI-powered, robust, open-source audiobook toolkit that includes features like intelligent subtitle extraction, AI title generation, multilingual translation, with support for GPU acceleration and batch offline processing.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png"><img src="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a></td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a> is a macOS translation &amp; OCR tool ready to use in any app — right out of the box!</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://agenticflow.ai/favicon.ico"></a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> is a no-code platform where marketers build agentic AI workflows for go-to-market automation, powered by hundreds of everyday apps as tools for your AI agents.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg"><img src="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a></td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a>（Windows） is a ready-to-go translation ocr tool developed by WPF </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA"><img src="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA" alt="Asp Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">ASP Client</a> </td>
        <td><a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">Deepseek.ASPClient</a>  is a lightweight ASP.NET wrapper for the Deepseek AI API, designed to simplify AI-driven text processing in .NET applications.. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67" alt="gpt-ai-flow-logo" width="64" height="auto" data-canonical-src="https://www.gptaiflow.tech/logo.png"></a> </td>
        <td> <a href="https://www.gptaiflow.tech/docs/product/api-keys-setup#setup-deepseek-api-keys" rel="nofollow">GPT AI Flow</a></td>
        <td>
            The ultimate productivity weapon built by engineers for efficiency enthusiasts (themselves): <a href="https://www.gptaiflow.tech/" rel="nofollow">GPT AI Flow</a>
            <ul dir="auto">
                <li>`Shift+Alt+Space` Wake up desktop intelligent hub</li>
                <li>Local encrypted storage</li>
                <li>Custom instruction engine</li>
                <li>On-demand calling without subscription bundling</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg"><img src="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/alecm20/story-flicks">Story-Flicks</a></td>
        <td>With just one sentence, you can quickly generate high-definition story short videos, supporting models such as DeepSeek.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://prompt.16x.engineer/favicon.ico"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/16x_prompt/README.md">16x Prompt</a> </td>
        <td> <a href="https://prompt.16x.engineer/" rel="nofollow">16x Prompt</a> is an AI coding tool with context management. It helps developers manage source code context and craft prompts for complex coding tasks on existing codebases.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/README.md"> Alpha Pai </a> </td>
        <td> AI Research Assistant / The Next-Generation Financial Information Portal Driven by AI.<br>Proxy for investors to attend meetings and take notes, as well as providing search and Q&amp;A services for financial information and quantitative analysis for investment research.</td>
    </tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://docs.xark-argo.com/img/logo.png"></a> </td> 
        <td> <a href="https://www.xark-argo.com/" rel="nofollow">argo</a> </td>
        <td>Locally download and run Ollama and Huggingface models with RAG on Mac/Windows/Linux. Support LLM API too.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://www.petercat.ai/images/favicon.ico"></a> </td>
        <td> <a href="https://www.petercat.ai/" rel="nofollow">PeterCat</a> </td>
        <td> A conversational Q&amp;A agent configuration system, self-hosted deployment solutions, and a convenient all-in-one application SDK, allowing you to create intelligent Q&amp;A bots for your GitHub repositories.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg"><img src="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://fastgpt.cn/en" rel="nofollow">FastGPT</a> </td>
        <td> 
            FastGPT is an open-source AI knowledge base platform built on large language models (LLMs), supporting various models including DeepSeek and OpenAI. We provide out-of-the-box capabilities for data processing, model invocation, RAG retrieval, and visual AI workflow orchestration, enabling you to effortlessly build sophisticated AI applications.
        </td>
   </tr>
   <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/assets/play_store_512.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/ruzhiai_note/assets/play_store_512.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/README.md">RuZhi AI Notes</a> </td>
        <td>RuZhi AI Notes is an intelligent knowledge management tool powered by AI, providing one-stop knowledge management and application services including AI search &amp; exploration, AI results to notes conversion, note management &amp; organization, knowledge presentation &amp; sharing. Integrated with DeepSeek model to provide more stable and higher quality outputs.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67"><img src="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://cdn.link-ai.tech/doc/CoW%20logo.png"></a> </td>
        <td> <a href="https://github.com/zhayujie/chatgpt-on-wechat">Chatgpt-on-Wechat</a> </td>
        <td> Chatgpt-on-Wechat(CoW) is a flexible chatbot framework that supports seamless integration of multiple LLMs, including DeepSeek, OpenAI, Claude, Qwen, and others, into commonly used platforms or office software such as WeChat Official Accounts, WeCom, Feishu, DingTalk, and websites. It also supports a wide range of custom plugins. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667"><img src="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667" alt="Icon" width="64" height="auto" data-canonical-src="https://athenalab.ai/assets/favicon/favicon.svg"></a> </td> 
        <td> <a href="https://athenalab.ai/" rel="nofollow">Athena</a> </td>
        <td>The world's first autonomous general AI with advanced cognitive architecture and human-like reasoning capabilities, designed to tackle complex real-world challenges.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67"><img src="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://maxkb.cn/images/favicon.png"></a> </td>
        <td> <a href="https://github.com/1Panel-dev/MaxKB">MaxKB</a> </td>
        <td> <a href="https://maxkb.cn/" rel="nofollow">MaxKB</a> is a ready-to-use, flexible RAG Chatbot. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/TigerGPT/assets/logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/TigerGPT/assets/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://ttm.financial/gpt" rel="nofollow">TigerGPT</a> </td>
        <td>TigerGPT is the first financial AI investment assistant of its kind based on OpenAI, developed by Tiger Group. TigerGPT aims to provide intelligent investment decision-making support for investors. On February 18, 2025, TigerGPT officially integrated the DeepSeek-R1 model to provide users with online Q&amp;A services that support deep reasoning. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/HIX.AI/assets/logo.svg"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/HIX.AI/assets/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://hix.ai/" rel="nofollow">HIX.AI</a> </td>
        <td>Try DeepSeek for free and enjoy unlimited AI chat on HIX.AI. Use DeepSeek R1 for AI chat, writing, coding &amp; more. Experience next-gen AI chat now!</td>
    </tr>
</tbody></table></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Core Git Developers Configure Git (469 pts)]]></title>
            <link>https://blog.gitbutler.com/how-git-core-devs-configure-git/</link>
            <guid>43169435</guid>
            <pubDate>Tue, 25 Feb 2025 08:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.gitbutler.com/how-git-core-devs-configure-git/">https://blog.gitbutler.com/how-git-core-devs-configure-git/</a>, See on <a href="https://news.ycombinator.com/item?id=43169435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	
	<div>
		<p>A few weeks ago I <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/" rel="noreferrer">wrote about</a> Git’s <code>help.autocorrect</code> setting and the strange tale of the origin of it’s deciseconds value.</p><p>It got me to thinking about other <code>git config</code> settings that most people likely don’t know about and which should probably be defaulted differently. </p><p>In this post, I’ll go through some of the perhaps obscure Git config settings that I have personally globally enabled and go into them to explain what they do and why they should <em>probably</em> be the default settings. </p><p>Also, it turns out that I learned most of these from the people who actually work on the core Git codebase every day.</p><h2 id="tldr">TLDR</h2><p>First, though, some of you may not particularly care about the wonderful and sordid history of the <code>rerere</code> values or whatever. You may just be thinking “just give me the settings so I can blindly throw them into my <code>~/.gitconfig</code> file." </p><p>Well, fair enough. Here is the fun stuff:</p><pre><code># clearly makes git better

[column]
        ui = auto
[branch]
        sort = -committerdate
[tag]
        sort = version:refname
[init]
        defaultBranch = main
[diff]
        algorithm = histogram
        colorMoved = plain
        mnemonicPrefix = true
        renames = true
[push]
        default = simple
        autoSetupRemote = true
        followTags = true
[fetch]
        prune = true
        pruneTags = true
        all = true

# why the hell not?

[help]
        autocorrect = prompt
[commit]
        verbose = true
[rerere]
        enabled = true
        autoupdate = true
[core]
        excludesfile = ~/.gitignore
[rebase]
        autoSquash = true
        autoStash = true
        updateRefs = true

# a matter of taste (uncomment if you dare)

[core]
        # fsmonitor = true
        # untrackedCache = true
[merge]
        # (just 'diff3' if git version &lt; 2.3)
        # conflictstyle = zdiff3 
[pull]
        # rebase = true</code></pre><p>Copypasta, my friends.</p><h2 id="how-do-git-core-devs-configure-their-gits">How do Git core devs configure their Gits?</h2><p>Before I dig into these one by one, there is an interesting question about if even the core Git developers think that some of these default values should be changed.</p><p>This came up not too long ago on the Git mailing list, and honestly, a few of these settings I personally learned from <a href="https://lore.kernel.org/git/60b5d281552d6_e359f20828@natae.notmuch/?ref=blog.gitbutler.com">this thread</a> called "Spring Cleaning" where Felipe Contreras challenged the Git core team to remove all their built up config options and aliases and see what it’s like to use Git stock, out of the box.</p><p>He challenged the list to pay attention to what settings they really wanted to change and share the top settings changes that seemed the most important with the list.</p><p>The <a href="https://lore.kernel.org/git/60df97ed24687_34a92088a@natae.notmuch/?ref=blog.gitbutler.com">results</a> were very interesting, a rather concise list of 9 config settings and 3 aliases that the experiment participants more or less agreed should arguably be new defaults. Let's just take a look at the proposed config setting changes.</p><pre><code>merge.conflictstyle = zdiff3
rebase.autosquash = true
rebase.autostash = true 
commit.verbose = true
diff.colorMoved = true
diff.algorithm = histogram
grep.patternType = perl
feature.experimental = true
branch.sort = committerdate</code></pre><p>Now, <em>none</em> of these have become the new defaults in the 3 or 4 years since this experiment, but it’s interesting that a lot of the Git developers themselves have a hard time using Git without several of these turned on.</p><p>Even more interesting is that <em>most of you </em>probably don’t know what <em>any</em> of these do.</p><p>So, let’s dig into them. What do these do and why should you almost certainly blindly trust me and go ahead and enable them?</p><p>I'm going to group these settings into three categories:</p><ul><li><a href="#clearly-makes-git-better" rel="noreferrer">Clearly Makes Git Better</a></li><li><a href="#why-the-hell-not" rel="noreferrer">Why the Hell Not?</a></li><li><a href="#a-matter-of-taste" rel="noreferrer">A Matter of Taste</a></li></ul><p>Let's get started.</p><h2 id="clearly-makes-git-better">Clearly Makes Git Better</h2><p>This first group of settings <em>clearly</em> makes Git better by default. There are generally zero downsides to enabling any of them.</p><h2 id="listing-branches">Listing branches</h2><p>I noted this in a previous blog post here about Git Tips under “<a href="https://blog.gitbutler.com/git-tips-2-new-stuff-in-git/#some-git-branch-stuff">Branch Stuff</a>” but as this was also in the Spring Cleaning list, I think everyone agrees that listing out Git branches should probably not be alpha-ordered by default.</p><p>The two settings which help improve this are <code>branch.sort</code> and <code>column.ui</code>. The first of which sorts the list by the most recent commit date (so probably more interesting at the top) rather than by alpha order. The second will put the branch names in a column format so you can see more per screen.</p><pre><code>git config --global column.ui auto
git config --global branch.sort -committerdate
</code></pre><p>The <code>column.ui</code> setting also affects the output of other listing commands (clean, status, tag), but generally I think it’s better than the default.</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png" alt="" loading="lazy" width="1598" height="984" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1598w" sizes="(min-width: 720px) 720px"></figure><p>You can also sort by other things than committer date, but I think it’s pretty clearly the most useful one.</p><p>Speaking of listing things, it’s also pretty ridiculous that this isn’t the default for listing tags, since it’s what nearly everyone actually wants.</p><p>Normally, if you list tags by alpha order, you’ll get something like this:</p><pre><code>$ git tag
nightly/0.5.100
nightly/0.5.1000
nightly/0.5.1001
nightly/0.5.101
nightly/0.5.1010
</code></pre><p>Nobody wants <code>0.5.101</code> to come after <code>0.5.1000</code>, but that’s alpha order. You can fix this by setting this:</p><pre><code>git config --global tag.sort version:refname
</code></pre><p>Which will generally do what you expect, treating dotted version numbers as a series of integer values for sorting purposes. Trust me, just enable this.</p><h2 id="default-branch">Default branch</h2><p>This one may be a little more controversial, since it can be argued to be somewhat political, but there should be a default branch name in Git where it doesn’t complain every time you <code>init</code> a new repo.</p><pre><code>git config --global init.defaultBranch main
</code></pre><p>Personally, I don’t have a problem with <code>master</code> and most of my repositories use that since that used to be the default, but I’m also fine with <code>main</code>, so whatever it is you want to use, just go ahead and set it.</p><p>Mostly what I find stupid is that now Git is annoying about this rather than just updating the default value. I wish Git had some taste here, but they don't, so you should just set it to something you find reasonable. But whatever.</p><h2 id="better-diff">Better diff</h2><p>There is actually a whole blog post that could be written about <code>git diff</code> algorithms, but the short story is that by default Git will use an old, fast, pretty reliable diff algorithm called "myers diff".</p><p>To give you a sense of what ‘old’ means, it was first published in a paper in 1986, so it’s almost 40 years old now. If you’re as old as I am, perhaps I can give you some childhood perspective as to what that means. The movies ‘The Three Amigos’, ‘An American Tail’ and the first ‘Highlander’ came out in theaters that year.</p><p>In any case, some advances have been made since then (with some tradeoffs too) and it may surprise you to know that Git actually ships with 4 built in diff algorithms it can use: <a><code>myers</code></a>, <code>minimal</code>, <a href="https://blog.jcoglan.com/2017/09/19/the-patience-diff-algorithm/?utm_source=chatgpt.com"><code>patience</code></a> and <code>histogram</code>.</p><p>Almost certainly what you want to be using is the <code>histogram</code> algorithm (an incremental improvement on ‘patience’), rather than the default of 'myers'. You can globally change it like this:</p><pre><code>git config --global diff.algorithm histogram
</code></pre><p>Here is an example of simple code movement diffed in <code>myers</code> vs <code>histogram</code>, to give a short taste of how it can be a bit smarter:</p><p>Let's say we move a css class below a similar one, change it a little, and then run <code>git diff</code> with the default <code>myers</code> algorithm. We may get something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png" alt="" loading="lazy" width="1626" height="1156" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1626w" sizes="(min-width: 720px) 720px"></figure><p>Ok, a little confusing. Here is what <code>histogram</code> would give us in the same scenario:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png" alt="" loading="lazy" width="1628" height="1266" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1628w" sizes="(min-width: 720px) 720px"></figure><p>It's a bit more clear here what's actually happened.</p><p>As recently as last year, Elijah (of our <a href="https://www.youtube.com/watch?v=KXPmiKfNlZE&amp;ref=blog.gitbutler.com">Git Merge fame</a>) suggested that<br><a href="https://lore.kernel.org/git/CABPp-BEmgOAj17DozyXNaf-9CawDic4uTpMbckef3+zHf7URqQ@mail.gmail.com/?ref=blog.gitbutler.com">histogram or patience</a> might make better defaults, in addition to Felipe's Spring Cleaning suggestion of the same thing, but in reality it’s unlikely to get through the gauntlet anytime soon.</p><p>That’s a big one, but there are also a few more smaller tweaks you can make to <code>git diff</code>:</p><pre><code>git config --global diff.colorMoved plain
git config --global diff.mnemonicPrefix true
git config --global diff.renames true
</code></pre><p>The <code>colorMoved</code> was also in the Spring Cleaning suggestion list, so it also should probably be a default change.</p><p>Here is an example of the previous code movement with the <code>colorMoved</code> turned on:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png" alt="" loading="lazy" width="1254" height="778" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1254w" sizes="(min-width: 720px) 720px"></figure><p>You can see actually the difference between the moved code and the added line. With <code>colorMoved</code> it will show code movement in different colors then added and removed lines.</p><p>The <code>diff.renames</code> option will detect if a file has been renamed, which is generally good (if slightly more expensive) and <code>diff.mnemonicPrefix</code> will replace the <code>a/</code> and <code>b/</code> in your diff header output with where the diff is coming from, so <code>i/</code> (index), <code>w/</code> (working directory) or <code>c/</code> commit. </p><p>So if I diff a change in my index to my working directory I get this as my diff header instead:</p><pre><code>❯ git diff
diff --git i/apps/web/page.js w/apps/web/page.js
index 7568be2ef..b9e9a00d7 100644
--- i/apps/web/page.js
+++ w/apps/web/page.js
</code></pre><p>A little difficult to see in this example perhaps, but you can tell which side is from the index and which is from the working directory by the leading path names. It’s really subtle, but I like it.</p><h2 id="better-pushing">Better pushing</h2><p>One of the things that has continued to confuse and frustrate me since the very early days of Git is setting up tracking branches properly. When I push, where does it push, or does it push at all?</p><p>There are three updated push settings that I think make for a much nicer default experience. The first (<code>push.default simple</code>) has been the new default since Git 2.0, but the others still need to be set explicitly.</p><pre><code>git config --global push.default simple # (default since 2.0)
git config --global push.autoSetupRemote true
git config --global push.followTags true
</code></pre><p>This has always been a bit of a pain in Git. The new <code>simple</code> default is built more or less for centralized workflows and by default pushes the current branch to the same name on the remote. I think this is a pretty sensible default.</p><p>However, if that branch does not exist and there is no tracking branch setup, you’ll still get this error:</p><pre><code>$ git push
fatal: The current branch my-branch-name has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin my-branch-name
</code></pre><p>I have to imagine that you have all seen this roughly <em>one million</em> times.</p><p>If you set <code>push.autoSetupRemote</code> to true, then you won’t get this error anymore. If the upstream is not set, it will automatically set it. I cannot tell you how much I love this setting.</p><p>Finally, the <code>push.followTags</code> setting will push all tags that you have locally that aren’t on the server, every time you push anything. I’ve been bitten by this a few times - if you ever create tags locally, set this up so you don’t have to worry about other people not seeing them.</p><h2 id="better-fetching">Better fetching</h2><p>It can be argued that it’s nice to keep some historical local copies of branches and tags that used to be on the server but are not any longer, but I don’t really buy that.</p><p>Personally, I think the default behavior of Git should be to make your remote references as close to what is on the remote as possible. Prune stuff that’s gone, etc. </p><p>So, I think these fetch settings should be the default:</p><pre><code>git config --global fetch.prune true
git config --global fetch.pruneTags true
git config --global fetch.all true
</code></pre><p>Really all that this does is make sure we delete <code>origin/blah</code> if <code>blah</code> is deleted on the server, and also do it automatically for all the remotes that we have configured. Seems pretty reasonable to me.</p><h2 id="why-the-hell-not">Why the Hell Not?</h2><p>This next batch of settings are generally harmless and occasionally helpful.</p><p>I’m not sure I would necessarily change the defaults, but I also don’t think it would hurt anyone and in many cases would be more helpful, so I’m including them in my list.</p><h2 id="autocorrect-prompting">Autocorrect prompting</h2><p>As I explained in great length in my <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">previous post</a>, there is a rather nice feature in Git where if your fingers trip up while typing a command, it will guess what you meant and try to run it.</p><p>The default is to not do this at all. What I rather prefer is to guess and prompt you.</p><pre><code>git config --global help.autocorrect prompt
</code></pre><p>If you want to read about this setting, it’s reasoning and it’s history ad nauseam, I have <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">just the post for you</a>.</p><h2 id="commit-with-diffs">Commit with diffs</h2><p>This was also one of the suggestions in the Spring Cleaning list, I think mostly because it just adds more information to the context you can reference when you write your commit message in your editor.</p><p>By default, a <code>git commit</code> will give you a message that looks something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png" alt="" loading="lazy" width="1494" height="462" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1494w" sizes="(min-width: 720px) 720px"></figure><p>Where there is just a list of files that were changed. If you set <code>commit.verbose</code> to be true, it will put the whole <code>diff</code> output in there for you to reference as you write your message.</p><pre><code>git config --global commit.verbose true
</code></pre><p>Here’s what it looks like now when you go to commit:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png" alt="" loading="lazy" width="1488" height="1276" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1488w" sizes="(min-width: 720px) 720px"></figure><p>All of this will be removed from the commit message (everything under the hilarious <code>-- &gt;8 --</code> "scissors" line), but it can give you much more context in writing your message.</p><h2 id="reuse-recorded-resolutions">Reuse recorded resolutions</h2><p>This setting is only useful if you’re doing rebases with conflicts over and over again. It’s not the most common situation, but there is not really an issue if it’s turned on and never used.</p><pre><code>git config --global rerere.enabled true
git config --global rerere.autoupdate true</code></pre><p>The <code>enabled</code> option will make sure it records the before and after states of rebase conflicts and the <code>autoupdate</code> will automatically re-apply the resolutions if it sees them again. I wrote about this at some length <a href="https://blog.gitbutler.com/git-tips-1-theres-a-git-config-for-that/#reuse-recorded-resolution">over here</a>, so I won’t bore you with the recap any further.</p><h2 id="global-ignore-file">Global ignore file</h2><p>This is pretty dumb, but as there is a <code>~/.gitconfig</code> file with global values, it would be cool if there were a <code>~/.gitignore</code> file with global values. This setting accomplishes that:</p><pre><code>git config --global core.excludesfile ~/.gitignore
</code></pre><p>In reality, this is sort of unnecessary, since Git will already look for global ignore values in the following two places: <code>~/git/ignore</code> and <code>~/.config/git/ignore</code> but since those are a little obscure, I feel like it’s nice to have this more guessable path.</p><h2 id="slightly-nicer-rebase">Slightly nicer rebase</h2><p>This section mostly has to do with the use case where you're fixing up and squashing your commits. If you don't know what that is, please check out our previous blog post on <a href="https://blog.gitbutler.com/git-autosquash/" rel="noreferrer">autosquashing</a>.</p><p>However, if you are squashing and rebasing a lot (or even occasionally), these settings could help and certainly won't hurt things.</p><pre><code>git config --global rebase.autoSquash true
git config --global rebase.autoStash true
git config --global rebase.updateRefs true</code></pre><p>The <code>updateRefs</code> setting should almost certainly be a default, honestly. It just takes stacked refs in a branch and makes sure they're also moved when a branch is rebased.</p><p>If you want to learn a tiny bit more about how to use fixup, autosquash and updateRefs, it's probably easiest to watch a few minutes of a talk where I go over it here:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/Md44rcw13k4?start=810&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="So You Think You Know Git Part 2 - DevWorld 2024"></iframe></figure><h2 id="a-matter-of-taste">A Matter of Taste</h2><p>The next group is based on your personal taste, but most people don’t know they exist and a lot of people may find them useful. They are commented out in my TLDR settings.</p><h2 id="better-merge-conflicts">Better merge conflicts</h2><p>So, while this is brought up in the Spring Cleaning thread as something that might want to be the new default, I'm not sure that all of you would agree.</p><p>When you have a merge conflict in Git, instead of inserting the conflict markers from left and right, you can ask it to insert what the base of it looked like too. Sometimes this can be really useful, but some people can find it pretty annoying.</p><pre><code>git config --global merge.conflictstyle zdiff3</code></pre><p>There have been discussions on the Git mailing list to make this the default and actually GitButler uses the <code>diff3</code> strategy when dealing with merge conflict markers and to be totally honest, not all of us love it.</p><p>Here is an example of a simple merge conflict marker you might get in a file when doing a merge or rebase:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png" alt="" loading="lazy" width="1326" height="480" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1326w" sizes="(min-width: 720px) 720px"></figure><p>With the <code>merge.conflictStyle zdiff3</code> setting, it would look like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png" alt="" loading="lazy" width="1320" height="558" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1320w" sizes="(min-width: 720px) 720px"></figure><p>Essentially, in addition to the <code>&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;</code> sections that show you how you changed the block and how the other person changed it, it adds a <code>|||||||</code> block that shows you what the block looked like before either of you changed it.</p><p>That extra context (what that section looked like before either side modified it) can sometimes be super useful, but often it's just more data and somewhat confusing.</p><p>Really, it's up to you if you prefer more data there.</p><div><p>⚠️</p><p>Git has nearly always had <code spellcheck="false">diff3</code> as a strategy. I'm recommending <code spellcheck="false">zdiff3</code> here, which stands for "<i><em>zealous diff3</em></i>" and is slightly better, but only available since Git 2.35 (Jan 2022). If you have an older Git version, just remove the "z".</p></div><h2 id="better-pulling">Better pulling</h2><p>The merge versus rebase debate is of course one that may never be agreed upon, but most of us have a preference. However, you may not know that you can set the <code>git pull</code> default so that it will only do one or the other. No need for <code>git pull --rebase</code>, you can make it the default:</p><pre><code>git config --global pull.rebase true
</code></pre><p>This is a personal decision, but as I’ve migrated to the rebase only camp recently, it is in fact in my config.</p><h2 id="run-the-fsmonitor-processes">Run the fsmonitor processes</h2><p>Again, this is really only a thing for larger repositories, and maybe you don’t want filesystem monitors running all over the place, but it can make things like <code>git status</code> much faster if you have big working directories.</p><p>Maybe it shouldn’t be a default, but it’s not very bad and can make a big difference. Maybe <code>git clone</code> should ask you if you want to set it or not. Whatever, it’s an option for you.</p><pre><code>git config --global core.fsmonitor true
git config --global core.untrackedCache true</code></pre><p>This will run a filesystem monitor (per repository) that notices file changes and updates a cache so that <code>git status</code> doesn’t have to crawl every file and see if anything changed via a thousand <code>mtime</code> stat calls, it can just look at a simple log of file changes.</p><div><p>⚠️</p><p>Be aware that this will run a single process <i><em>per repository</em></i> that you are active in, which can be a lot. They mostly don't do much as they're event based, so it shouldn't affect memory or CPU noticeably, even with hundreds of them, but it's something to keep in mind. You can also leave out the <code spellcheck="false">--global</code> and just enable it for your larger repos.</p></div><h2 id="final-thoughts">Final thoughts</h2><p>Hopefully this has been a useful reference and maybe you learned some new Git config things, some of which should almost certainly already be the defaults, which isn’t even a controversial option in the Git mailing list community.</p><p>There are lots of other ways to pimp your Git ride (aliases, cool external <a href="https://github.com/dandavison/delta?ref=blog.gitbutler.com">pager</a> and <a href="https://github.com/so-fancy/diff-so-fancy?ref=blog.gitbutler.com">diff</a> tools, things like that) but I thought it would be best to just stick to globally useful and relatively simple vanilla Git settings.</p><p>Hope you enjoyed this and see you next time!</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What would happen if we didn't use TCP or UDP? (177 pts)]]></title>
            <link>https://github.com/Hawzen/hdp</link>
            <guid>43169103</guid>
            <pubDate>Tue, 25 Feb 2025 07:13:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Hawzen/hdp">https://github.com/Hawzen/hdp</a>, See on <a href="https://news.ycombinator.com/item?id=43169103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">What would happen if we didn't use TCP or UDP?</h2><a id="user-content-what-would-happen-if-we-didnt-use-tcp-or-udp" aria-label="Permalink: What would happen if we didn't use TCP or UDP?" href="#what-would-happen-if-we-didnt-use-tcp-or-udp"></a></p>
<p dir="auto">Switches, bridges, routers, load balancers, firewalls—these network boxes keep the internet running. Routing, blocking, mirroring, duplicating and deduplicating traffic in ways most people never think about. Without them, this document wouldn’t have reached you</p>
<p dir="auto">But the network is just one layer. The OS has its own way of handling packets—classifying, queuing, enforcing firewall rules, translating addresses, deciding what gets through and what gets dropped without a trace. Every part plays by its own rules, shaping what’s “allowed” and what's not</p>
<p dir="auto">At some point, I wondered—<em>what if I sent a packet using a transport protocol that didn’t exist?</em> Not TCP, not UDP, not even ICMP—something completely made up. Would the OS let it through? Would it get stopped before it even left my machine? Would routers ignore it, or would some middlebox kill it on sight? Could it actually move faster by slipping past common firewall rules?</p>
<p dir="auto">No idea.</p>
<p dir="auto">So I had to try.</p>
<p dir="auto">First, I sent the packets to myself, just to see how my own machine handled the poison I made up. Then, I sent them across continents to a remote Linux machine to see if they’d actually make it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Some background first</h2><a id="user-content-some-background-first" aria-label="Permalink: Some background first" href="#some-background-first"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Feel free to skip this section if you already know how the internet works. Otherwise, continue reading on</p>
</div>
<p dir="auto">But wait—what exactly is a transport layer protocol?</p>
<p dir="auto">The internet isn’t magic. It just looks that way. Underneath, it’s a stack of protocols, each one shoving data to the next until it reaches its destination. At the application level, you send a request—loading a website, streaming a video, or whatever you do. That request gets wrapped by the OS in multiple layers of metadata, addresses, and headers, until it’s nothing but raw bits flying through the network</p>
<p dir="auto">It kinda works like this:</p>
<p dir="auto">  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/internet_protocols.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/internet_protocols.png" alt="a visual guide to how the internet works, it kinda sucks but that why i like it."></a> </p>
<p dir="auto"><sub>The diagram is 100% correct and should be included in all networking textbooks.</sub></p>
<p dir="auto">At the top, apps—browsers, games, whatever—generate requests (Load this page, Send this message, Connect to this game server). Then the requests start their descent through the network stack, getting wrapped, encoded, and addressed at each layer, until all that’s left is a stream of bits flying into the void</p>
<p dir="auto">Each layer plays a role. IP assigns addresses and makes sure packets know where they’re going. The link layer handles the actual transmission—Wi-Fi, Ethernet, fiber optics, whatever. There’s more to it, but we’re not going down that rabbit hole right now. What matters is the layer that makes network communication actually usable</p>
<p dir="auto">The <strong>transport layer</strong> is where networking personally starts to get interesting. It’s the first truly complex protocol layer. It doesn’t just move packets—it manages connections, makes sure multiple applications can share the same machine, and decides how data should flow.</p>
<p dir="auto">This is where <strong>TCP</strong>, <strong>UDP</strong>, and their weird cousins live. The <strong>IP Protocol</strong> defines a field called <code>Protocol</code>. Setting this field to 6 means the encapsulated packet is TCP, 17 is UDP, and <a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">there are others defined</a> but some numbers are deliberately left out for future use</p>
<p dir="auto">But what if we used those <em>unused</em> numbers?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #1: Sending traffic.. to me!</h2><a id="user-content-experiment-1-sending-traffic-to-me" aria-label="Permalink: Experiment #1: Sending traffic.. to me!" href="#experiment-1-sending-traffic-to-me"></a></p>
<p dir="auto">There are simply too many variables to this experiment. My OS, my router, the receiver's OS, and god knows how many middle boxes are littered on the open internet. It's hard to extrapolate conclusions from experimentation with all these moving parts—so I thought of the following: To begin, I'll send the packets to <em>my own machine</em>, this guarantees that any results are solely due to my OS's behaviour</p>
<p dir="auto">First, I designed a <a href="https://github.com/Hawzen/hdp/blob/master/hdp_specification.md">simple protocol</a>: <strong>HDP</strong>. The specifics don’t matter—what matters is that it doesn’t resemble any known protocol. It’s an outsider, something the OS and network stack weren’t expecting</p>
<p dir="auto">Next, I built a <a href="https://github.com/Hawzen/hdp/blob/master/src/server/main.rs">server, or a listener</a>, whatever you call it. The machine running this code will be patiently waiting for any packets. Then I wrote a <a href="https://github.com/Hawzen/hdp/blob/master/src/client/main.rs">client</a>, the machine running this code will send HDP packets to the server</p>
<p dir="auto">Finally, here are the steps I'll attempt</p>
<ol dir="auto">
<li>Startup an HDP server
<ul dir="auto">
<li>Which will ask the OS to forward any packets with the protocol 255 to a socket it controls</li>
</ul>
</li>
<li>Run the HDP client, sending packets to my local machine
<ul dir="auto">
<li>The client will ask the OS to nicely deliver the packets to 127.0.01
<ul dir="auto">
<li>The OS is configured to hand packets with that target address to the loopback <a href="https://en.wikipedia.org/wiki/Network_interface_controller" rel="nofollow">network interface</a>
<ul dir="auto">
<li>The loopback interface should realize: "uhhh.. this packet should go right back in?", and send it back to my own machine</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>The OS delivers them to the HDP server unmodified..?? 🤞</li>
</ol>
<p dir="auto">Let's do it</p>
<p dir="auto">I opened two shells—one was the server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span></pre></div>
<p dir="auto">And in another shell I opened the client</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ fortune | cowsay | sudo cargo run --bin client 127.0.0.1"><pre><span><span>$</span></span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1</span></pre></div>
<p dir="auto">Alright, let's send the packet via the client. 3, 2, 1, and..</p>
<p dir="auto">The server got the message!</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server
~~~ IP Header ~~~
Version: 4
IHL: 5
DSCP: 0
ECN: 0
Total Length: 58625
Identification: 36455
Flags: 0
Fragment Offset: 0
TTL: 64
Protocol: 255
Header Checksum: 0
Source IP: [127, 0, 0, 1]
Destination IP: [127, 0, 0, 1]


~~~ HDP Header &amp; Data ~~~
Source Port: 420
Destination Port: 420
Timestamp: 1739640243546134000
Data:  _________________________________________
/ Marriage is not merely sharing the      \
| fettucine, but sharing the burden of    |
| finding the fettucine restaurant in the |
| first place.                            |
|                                         |
\ -- Calvin Trillin                       /
 -----------------------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span>
<span>~~~</span> <span>IP</span> <span>Header</span> <span>~~~</span>
<span>Version</span><span>:</span> <span>4</span>
<span>IHL</span><span>:</span> <span>5</span>
<span>DSCP</span><span>:</span> <span>0</span>
<span>ECN</span><span>:</span> <span>0</span>
<span>Total</span> <span>Length</span><span>:</span> <span>58625</span>
<span>Identification</span><span>:</span> <span>36455</span>
<span>Flags</span><span>:</span> <span>0</span>
<span>Fragment</span> <span>Offset</span><span>:</span> <span>0</span>
<span>TTL</span><span>:</span> <span>64</span>
<span>Protocol</span><span>:</span> <span>255</span>
<span>Header</span> <span>Checksum</span><span>:</span> <span>0</span>
<span>Source</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]
<span>Destination</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]


<span>~~~</span> <span>HDP</span> <span>Header</span> <span>&amp;</span> <span>Data</span> <span>~~~</span>
<span>Source</span> <span>Port</span><span>:</span> <span>420</span>
<span>Destination</span> <span>Port</span><span>:</span> <span>420</span>
<span>Timestamp</span><span>:</span> <span>1739640243546134000</span>
<span>Data</span><span>:</span>  _________________________________________
<span><span>/</span></span> <span>Marriage</span> is <span>not</span> merely sharing the      \
<span>|</span> fettucine, but sharing the burden <span>of</span>    <span>|</span>
<span>|</span> finding the fettucine restaurant <span>in</span> the <span>|</span>
<span>|</span> first place<span>.</span>                            <span>|</span>
<span>|</span>                                         <span>|</span>
<span>\</span> <span><span>--</span> Calvin Trillin                       /</span>
 <span><span>-----------------------------------------</span></span>
        <span>\</span>   <span>^</span>__<span>^</span>
         <span>\</span>  (oo)<span>\</span>_______
            (__)<span>\</span>       )<span>\/\</span>
                <span>||----</span>w <span>|</span>
                <span>||</span>     <span>||</span></pre></div>
<p dir="auto">Success! The OS accepted my protocol, looped it back, and delivered it to the server with no shenanigans happening, unexpected!. But before calling it a day, I had another question:</p>
<p dir="auto">What would happen if we repeated this experiment, whilst changing the protocol number defined in the IP packet?</p>
<p dir="auto">My initial choice of <strong>255</strong> was arbitrary—it was an unused protocol number. But what if I tried something more… unconventional? I decided to test different protocol numbers, including:</p>
<ul dir="auto">
<li>6, the number assigned to <strong>TCP</strong> packets</li>
<li>Or 2, which is the protocol number used for <strong>ICMP</strong> (i.e., the thing powering <code>ping</code>)</li>
<li>Or even 256, an index beyond the defined boundaries of the IP Protocol
Would they make it? Would the OS freak out?</li>
</ul>
<p dir="auto">Let's see:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fortune | cowsay | sudo cargo run --bin client 127.0.0.1 # This time looping over protocol numbers"><pre>fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1 # This time looping over protocol numbers</span></pre></div>
<details>
<summary><p dir="auto"><h2 tabindex="-1" dir="auto">Results</h2><a id="user-content-results" aria-label="Permalink: Results" href="#results"></a></p></summary>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Protocol Number</th>
<th>Source IP (Server)</th>
<th>Byte Sum (Server)</th>
<th>Received (Server)</th>
<th>Succeeded (Client)</th>
<th>Byte sum (Client)</th>
<th>Failure reason (Client)</th>
<th>Time difference (μs)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>1</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>2</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>3</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>4</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>5</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>6</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>7</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>8</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>9</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>10</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>11</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>12</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>13</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>14</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>15</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>16</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>17</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>18</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>42</td>
</tr>
<tr>
<td>19</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>82</td>
</tr>
<tr>
<td>20</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>21</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>22</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>23</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>24</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>25</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>26</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>27</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>28</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>29</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>30</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>31</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>32</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>33</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>34</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>35</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>36</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>37</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>38</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>39</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>40</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>41</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>42</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>43</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>44</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>58</td>
</tr>
<tr>
<td>45</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>46</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>47</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>48</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>49</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>50</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>51</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>52</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>53</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>54</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>55</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>56</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>57</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>58</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>69</td>
</tr>
<tr>
<td>59</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>60</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>61</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>62</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>63</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>64</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>65</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>66</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>124</td>
</tr>
<tr>
<td>67</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>68</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>69</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>87</td>
</tr>
<tr>
<td>70</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>71</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>72</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>73</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>74</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>75</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>76</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>77</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>78</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>79</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>80</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>150</td>
</tr>
<tr>
<td>81</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>82</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>83</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>84</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>85</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>86</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>87</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>88</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>89</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>90</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>91</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>92</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>93</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>94</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>95</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>96</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>97</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>98</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>99</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>100</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>101</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>102</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>148</td>
</tr>
<tr>
<td>103</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>104</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>119</td>
</tr>
<tr>
<td>105</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>106</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>107</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>108</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>109</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>44</td>
</tr>
<tr>
<td>110</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>111</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>112</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>113</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>114</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>115</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>116</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>117</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>118</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>24</td>
</tr>
<tr>
<td>119</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>120</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>121</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>122</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>123</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>124</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>125</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>126</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>127</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>128</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>129</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>130</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>131</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>132</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>133</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>134</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>135</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>136</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>112</td>
</tr>
<tr>
<td>137</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>138</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>139</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>140</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>141</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>142</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>143</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>144</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>145</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>146</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>147</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>148</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>149</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>72</td>
</tr>
<tr>
<td>150</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>151</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>152</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>153</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>154</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>155</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>156</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>157</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>110</td>
</tr>
<tr>
<td>158</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>159</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>160</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>161</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>162</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>163</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>164</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>165</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>166</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>167</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>168</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>169</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>170</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>171</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>172</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>173</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>174</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>67</td>
</tr>
<tr>
<td>175</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>176</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>60</td>
</tr>
<tr>
<td>177</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>178</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>179</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>180</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>181</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>182</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>183</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>184</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>185</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>186</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>187</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>188</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>189</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>190</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>191</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>192</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>193</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>194</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>195</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>196</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>197</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>198</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>199</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>200</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>201</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>202</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>203</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>204</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>205</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>206</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>207</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>208</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>209</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>210</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>211</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>212</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>213</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>214</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>215</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>216</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>217</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>218</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>219</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>220</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>221</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>222</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>223</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>224</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>225</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>226</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>227</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>228</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>229</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>230</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>231</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>232</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>102</td>
</tr>
<tr>
<td>233</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>234</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>113</td>
</tr>
<tr>
<td>235</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>236</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>237</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>238</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>239</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>240</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>241</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>242</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>243</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>244</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>245</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>246</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>247</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>248</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>249</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>250</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>251</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>252</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>253</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>254</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>256</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Invalid argument (os error 22)</td>
<td>nan</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">What’s up with these failures?</h2><a id="user-content-whats-up-with-these-failures" aria-label="Permalink: What’s up with these failures?" href="#whats-up-with-these-failures"></a></p>
<p dir="auto">Most protocol numbers worked fine—the OS saw the packet, looped it back, and my server received it without an issue. But a few of them outright&nbsp;<em>failed</em>&nbsp;at different points in the stack</p>
<ul dir="auto">
<li><strong>Protocols 1, 2, and 6 failed at the server side</strong>.&nbsp;Meaning: the client successfully sent them, but the server never saw them</li>
<li><strong>Protocols 50 and 51 failed at the client side</strong>.&nbsp;The OS refused to even send them</li>
<li><strong>Protocol 256 didn't even make it past the&nbsp;<code>socket()</code>&nbsp;call</strong></li>
</ul>
<p dir="auto">But&nbsp;<em>why?</em>&nbsp;What’s making the OS treat these packets differently?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Syscalls: What actually matters</h2><a id="user-content-syscalls-what-actually-matters" aria-label="Permalink: Syscalls: What actually matters" href="#syscalls-what-actually-matters"></a></p>
<p dir="auto">One of the most useful debugging techniques I learnt debugging this stuff is, when dealing with low-level code, trace the <em>system calls</em> a process is making</p>
<p dir="auto">A <a href="https://en.wikipedia.org/wiki/System_call" rel="nofollow">system call</a> for the uninitiated is just a function that allows applications to request privileged resources from the OS—whether that’s opening a file, allocating memory, or, in our case,&nbsp;sending a packet over the network</p>
<p dir="auto">In my Rust code I use a library called <a href="https://docs.rs/socket2/latest/socket2/index.html" rel="nofollow"><code>socket2</code></a> which implements a pretty wrapper over the system calls provided by my OS. And to send a packet, I request a socket—which you can think of as just a special file my code can write in to communicate over the network</p>
<p dir="auto">Here's what the client would do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols
    SOCK_RAW,   // Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.
    255         // Protocol: We looped over this field.
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols</span>
    <span>SOCK_RAW</span>,   <span>// Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.</span>
    <span>255</span>         <span>// Protocol: We looped over this field.</span>
);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Revisiting the failures</h2><a id="user-content-revisiting-the-failures" aria-label="Permalink: Revisiting the failures" href="#revisiting-the-failures"></a></p>
<p dir="auto"><strong>1, 2, and 6: The Server Never Sees Them</strong><br>
These packets were successfully transmitted from the client, but they were intercepted before my server had a chance to look at them. That suggests something inside the OS intercepted them</p>
<p dir="auto">Originally, I assumed my server would capture any raw IP packet it received. The socket looked like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Internet domain
    SOCK_RAW,   // Raw socket: should give us full control
    0           // Let the OS decide the protocol
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Internet domain</span>
    <span>SOCK_RAW</span>,   <span>// Raw socket: should give us full control</span>
    <span>0</span>           <span>// Let the OS decide the protocol</span>
);</pre></div>
<p dir="auto">I expected 0 to mean:
<em>"Give me everything—TCP, UDP, whatever it is, forward it"</em></p>
<p dir="auto">For context, I ran these experiments on my Mac, which runs Darwin. Looking at the <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/socket.2.html" rel="nofollow">documentation</a>, there is really nothing mentioning the Protocol Number = 0 trick</p>
<p dir="auto">Under the hood, Darwin is just like BSD but with a ton of makeup, meaning it inherits BSD’s socket behaviour and network stack quirks. And on a whim I checked the <strong><a href="https://man.openbsd.org/socket.2" rel="nofollow">BSD socket documentation</a></strong>, and I found this frustratingly vague line:</p>
<blockquote>
<p dir="auto">"A value of 0 for <code>protocol</code> will let the system select an appropriate protocol for the requested socket type."</p>
</blockquote>
<p dir="auto">So instead of delivering <strong>all</strong> raw packets, my OS was silently (and haphazardly) filtering them. My server never even saw the ICMP (1), IGMP (2), or TCP (6) packets—because Darwin likely deemed my socket not appropriate to receive those protocols.. or something?</p>
<p dir="auto"><strong>50 and 51: The Client Can’t Even Send Them</strong><br>
Here, the OS flat-out refused to send the packets. These aren’t just arbitrary numbers—they’re part of <strong>IPSec (ESP and AH)</strong>, which is used for encrypted VPN traffic. I'm not sure <em>why</em> the OS blocked them, but I imagine it's a security feature of sorts in Darwin</p>
<p dir="auto"><strong>256: The <code>socket()</code> Call Fails Immediately</strong><br>
This one is simple:</p>
<ul dir="auto">
<li>The IPv4 protocol field is 8 bits meaning valid values range from 0 to 255</li>
<li>256 is simply too large—the OS rejects it outright as an invalid argument</li>
</ul>
<p dir="auto">No surprises here. But what <em>was</em> surprising is what happened when I tried the same experiment on Linux..</p>
<p dir="auto">After seeing these inconsistencies, I was curious as to how Linux would behave. So I spun up a Linux VM and re-ran the experiment. Right away, the behaviour was very different</p>
<p dir="auto">Running the server I quickly noticed that Linux does not allow binding a raw socket to protocol <code>0</code>—Some invalid protocol numbers like 256 <em>worked</em>. For reference, I logged the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/results_no_server_linux_client_loopback.md"><code>results_no_server_linux_client_loopback</code></a>. I was satisfied that at least <em>some</em> of the protocol numbers were working as expected</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned" aria-label="Permalink: Lessons learned" href="#lessons-learned"></a></p>
<p dir="auto">Custom transport-layer protocols are doable, buuuuut the OS isn’t exactly welcoming. The networking stack has so many assumptions baked in, and raw sockets aren’t as raw as you’d expect</p>
<p dir="auto">I imagine this is why most new protocols live at the application layer instead. Instead of fighting the OS, engineers just build on top of existing transport protocols. QUIC, for example, runs over UDP and avoids these issues entirely</p>
<p dir="auto">And if you're ever working with raw sockets, <em>please</em> test across multiple OSes. If Darwin lets you do something, Linux might shut it down. If Linux is fine with it, Windows might pretend it doesn’t exist. There’s really no universal behaviour, even if they claim to <em>implement the POSIX standard</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Next step: What happens outside loopback?</h2><a id="user-content-next-step-what-happens-outside-loopback" aria-label="Permalink: Next step: What happens outside loopback?" href="#next-step-what-happens-outside-loopback"></a></p>
<p dir="auto">So far, these packets never left my machine. Now, I want to send HDP over the public internet:</p>
<ul dir="auto">
<li>Will routers forward it, or will they drop it?</li>
<li>Will firewalls let it through, or flag it as an attack?</li>
<li>Will it have different latency compared to TCP?</li>
<li>Will I accidentally brick DigitalOcean’s network? :D
Time to find out</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #2:</h2><a id="user-content-experiment-2" aria-label="Permalink: Experiment #2:" href="#experiment-2"></a></p>
<p dir="auto">At first I expected this experiment to be straight-forward (spoilers: it was NOT). How could it not..?</p>
<p dir="auto">I planned to deploy my server on a machine using a cheap cloud provider like Digital Ocean—then I'd send all sorts of packets to it, TCP, UDP, my own protocol, you name it. Gathering statistics about packet drop, latency, whatever, then I'd make conclusions about the feasibility of not using TCP/UDP</p>
<p dir="auto">Simple!</p>
<p dir="auto">But oh it was not, not at all. It wasn't that the experiment was difficult to setup—but what weirded me out was the results.. they weren't anything I expected or was prepared to deal with. Keep reading to see why</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setting up the server</h2><a id="user-content-setting-up-the-server" aria-label="Permalink: Setting up the server" href="#setting-up-the-server"></a></p>
<p dir="auto">I rented the the cheapest VPS on Digital Ocean I could find, then set up my server and all the tooling I needed. Nice!</p>
<p dir="auto">Let's see where the server is..</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl myip.wtf
161.35.222.56
root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl ipinfo.io/161.35.222.56
{
  &quot;ip&quot;: &quot;161.35.222.56&quot;,
  &quot;city&quot;: &quot;Frankfurt am Main&quot;,
  &quot;region&quot;: &quot;Hesse&quot;,
  &quot;country&quot;: &quot;DE&quot;,
  &quot;loc&quot;: &quot;50.1155,8.6842&quot;,
  &quot;org&quot;: &quot;AS14061 DigitalOcean, LLC&quot;,
  &quot;postal&quot;: &quot;60306&quot;,
  &quot;timezone&quot;: &quot;Europe/Berlin&quot;,
  &quot;readme&quot;: &quot;https://ipinfo.io/missingauth&quot;
}"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl myip<span>.</span>wtf
<span>161.35</span><span>.</span><span>222.56</span>
root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl ipinfo<span>.</span>io<span>/</span><span>161.35</span><span>.</span><span>222.56</span>
{
  <span><span>"</span>ip<span>"</span></span><span>:</span> <span><span>"</span>161.35.222.56<span>"</span></span>,
  <span><span>"</span>city<span>"</span></span><span>:</span> <span><span>"</span>Frankfurt am Main<span>"</span></span>,
  <span><span>"</span>region<span>"</span></span><span>:</span> <span><span>"</span>Hesse<span>"</span></span>,
  <span><span>"</span>country<span>"</span></span><span>:</span> <span><span>"</span>DE<span>"</span></span>,
  <span><span>"</span>loc<span>"</span></span><span>:</span> <span><span>"</span>50.1155,8.6842<span>"</span></span>,
  <span><span>"</span>org<span>"</span></span><span>:</span> <span><span>"</span>AS14061 DigitalOcean, LLC<span>"</span></span>,
  <span><span>"</span>postal<span>"</span></span><span>:</span> <span><span>"</span>60306<span>"</span></span>,
  <span><span>"</span>timezone<span>"</span></span><span>:</span> <span><span>"</span>Europe/Berlin<span>"</span></span>,
  <span><span>"</span>readme<span>"</span></span><span>:</span> <span><span>"</span>https://ipinfo.io/missingauth<span>"</span></span>
}</pre></div>
<p dir="auto">Alright, looks like the experiment will span continents given that I'm running my client on Saudi Arabia, and the server is hosted in Frankfurt</p>
<p dir="auto">Before running any deep analysis, I wanted to check that there is a network path between my Mac and the server, so I <code>ping</code>'ed the server from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ ping 161.35.222.56
PING 161.35.222.56 (161.35.222.56): 56 data bytes
64 bytes from 161.35.222.56: icmp_seq=0 ttl=47 time=125.364 ms
64 bytes from 161.35.222.56: icmp_seq=1 ttl=47 time=128.061 ms
64 bytes from 161.35.222.56: icmp_seq=2 ttl=47 time=177.931 ms
64 bytes from 161.35.222.56: icmp_seq=3 ttl=47 time=225.798 ms
64 bytes from 161.35.222.56: icmp_seq=4 ttl=47 time=130.101 ms
64 bytes from 161.35.222.56: icmp_seq=5 ttl=47 time=194.563 ms
64 bytes from 161.35.222.56: icmp_seq=6 ttl=47 time=159.518 ms
64 bytes from 161.35.222.56: icmp_seq=7 ttl=47 time=134.343 ms
64 bytes from 161.35.222.56: icmp_seq=8 ttl=47 time=501.139 ms
64 bytes from 161.35.222.56: icmp_seq=9 ttl=47 time=153.672 ms
64 bytes from 161.35.222.56: icmp_seq=10 ttl=47 time=137.927 ms
64 bytes from 161.35.222.56: icmp_seq=11 ttl=47 time=355.672 ms
64 bytes from 161.35.222.56: icmp_seq=12 ttl=47 time=138.777 ms
64 bytes from 161.35.222.56: icmp_seq=13 ttl=47 time=166.116 ms
64 bytes from 161.35.222.56: icmp_seq=14 ttl=47 time=288.758 ms
64 bytes from 161.35.222.56: icmp_seq=15 ttl=47 time=151.458 ms
64 bytes from 161.35.222.56: icmp_seq=16 ttl=47 time=164.025 ms
64 bytes from 161.35.222.56: icmp_seq=17 ttl=47 time=170.132 ms
64 bytes from 161.35.222.56: icmp_seq=18 ttl=47 time=279.034 ms
^C
--- 161.35.222.56 ping statistics ---
19 packets transmitted, 19 packets received, 0.0% packet loss"><pre><span>❯</span> ping <span>161.35</span><span>.</span><span>222.56</span>
<span>PING</span> <span>161.35</span><span>.</span><span>222.56</span> (<span>161.35</span><span>.</span><span>222.56</span>)<span>:</span> <span>56</span> <span>data</span> bytes
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>0</span> ttl<span>=</span><span>47</span> time<span>=</span><span>125.364</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>1</span> ttl<span>=</span><span>47</span> time<span>=</span><span>128.061</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>2</span> ttl<span>=</span><span>47</span> time<span>=</span><span>177.931</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>3</span> ttl<span>=</span><span>47</span> time<span>=</span><span>225.798</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>4</span> ttl<span>=</span><span>47</span> time<span>=</span><span>130.101</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>5</span> ttl<span>=</span><span>47</span> time<span>=</span><span>194.563</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>6</span> ttl<span>=</span><span>47</span> time<span>=</span><span>159.518</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>7</span> ttl<span>=</span><span>47</span> time<span>=</span><span>134.343</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>8</span> ttl<span>=</span><span>47</span> time<span>=</span><span>501.139</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>9</span> ttl<span>=</span><span>47</span> time<span>=</span><span>153.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>10</span> ttl<span>=</span><span>47</span> time<span>=</span><span>137.927</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>11</span> ttl<span>=</span><span>47</span> time<span>=</span><span>355.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>12</span> ttl<span>=</span><span>47</span> time<span>=</span><span>138.777</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>13</span> ttl<span>=</span><span>47</span> time<span>=</span><span>166.116</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>14</span> ttl<span>=</span><span>47</span> time<span>=</span><span>288.758</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>15</span> ttl<span>=</span><span>47</span> time<span>=</span><span>151.458</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>16</span> ttl<span>=</span><span>47</span> time<span>=</span><span>164.025</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>17</span> ttl<span>=</span><span>47</span> time<span>=</span><span>170.132</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>18</span> ttl<span>=</span><span>47</span> time<span>=</span><span>279.034</span> ms
<span><span>^</span></span><span>C</span>
<span><span>---</span> 161.35.222.56 ping statistics ---</span>
<span>19</span> packets transmitted, <span>19</span> packets received, <span>0.0</span><span>%</span> packet loss</pre></div>
<p dir="auto">It seems it's quite far, but looks fine to me, let's send some packets using our new protocol!</p>
<p dir="auto">First let's start the server in our Digital Ocean machine</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span></pre></div>
<p dir="auto">And now we can send a packet from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ fortune | cowsay | sudo cargo run --bin client 161.35.222.56
| Protocol Number | Succeeded (Client) | Time (μs) (Client) | Byte sum (Client) | Failure reason (Client) |
| 255 | 🫡 | timestamp | 563 | - |"><pre><span>❯</span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 161.35.222.56</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Succeeded</span> (<span>Client</span>) <span>|</span> <span>Time</span> (μs) (<span>Client</span>) <span>|</span> <span>Byte</span> <span>sum</span> (<span>Client</span>) <span>|</span> <span>Failure</span> reason (<span>Client</span>) <span>|</span>
<span>|</span> <span>255</span> <span>|</span> 🫡 <span>|</span> timestamp <span>|</span> <span>563</span> <span>|</span> <span>-</span> <span>|</span></pre></div>
<p dir="auto">Packet sent. Let's check the server again</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">Excellent. It seems that all went well, or so I thought. In-fact, all went downhill starting here. I took a quick break then came back. Let's try sending the packet again..</p>
<div dir="auto" data-snippet-clipboard-copy-content="| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre><span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">It's stuck? I can't see the second packet</p>
<p dir="auto">I <code>Ctrl+C</code> and attempt doing it again. No results..? That can't be right, could it be a client side bug? Let's use <code>tcpdump</code> to see all outgoing packets from my device</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ sudo tcpdump -i any 'ip[9] == 255'
tcpdump: data link type PKTAP
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type PKTAP (Apple DLT_PKTAP), snapshot length 524288 bytes
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427"><pre><span>❯</span> sudo tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>==</span> <span>255</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>PKTAP</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>PKTAP</span> (<span>Apple</span> <span>DLT_PKTAP</span>), snapshot <span>length</span> <span>524288</span> bytes
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span></pre></div>
<p dir="auto">They're definitely leaving my Mac. What about doing the same thing on the receiving end?</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp# tcpdump -i any 'ip[9] > 17'
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>#</span> tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>&gt;</span> <span>17</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>LINUX_SLL2</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>LINUX_SLL2</span> (<span>Linux</span> cooked v2), snapshot <span>length</span> <span>262144</span> bytes
</pre></div>
<p dir="auto">Nothing appeared</p>
<p dir="auto">I began doubting my earlier results, there they are in my shell. The timestamps and byte sums match. Was I imagining them? Is Linus Torvalds himself gaslighting me??</p>
<p dir="auto">Wait..? How did my ISP's <a href="https://simple.wikipedia.org/wiki/Network_address_translation" rel="nofollow">NATing box</a> forward the packet? NAT'ing relies on ports—but my protocol is just black magic to them</p>
<p dir="auto">I'm confused</p>
<p dir="auto">Very confused</p>
<p dir="auto">After digging a bit in, I found that Digital Ocean doesn't support non-standard IP Protocols</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/ihatedigitalocean.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/ihatedigitalocean.png" alt="digital_ocean_sucks"></a></p>
<p dir="auto">This still doesn't explain it. How did one packet survive? There really is no way to know, and I was banging my head against the wall trying to figure it out</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">One. Last. Try</h3><a id="user-content-one-last-try" aria-label="Permalink: One. Last. Try" href="#one-last-try"></a></p>
<p dir="auto">if any cloud provider would support non-standard IP Protocols, it'd be AWS</p>
<p dir="auto">I provisioned two machines. Set them up. Server. Client. It works.. !</p>
<div dir="auto" data-snippet-clipboard-copy-content="admin@ip-172-31-13-218:~/hdp$ sudo cargo run --bin server 255
Server is listening on SockAddr { ss_family: 2, len: 16 }, protocol: 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | 54.153.13.186 | 33 |
| 255 | timestamp | 54.153.13.186 | 34 |
| 255 | timestamp | 54.153.13.186 | 35 |
| 255 | timestamp | 54.153.13.186 | 36 |
"><pre>admin<span>@</span>ip<span>-</span><span>172</span><span>-</span><span>31</span><span>-</span><span>13</span><span>-</span><span>218</span><span>:~/</span>hdp<span>$</span> sudo cargo run <span><span>--</span>bin server 255</span>
<span>Server</span> is listening on <span>SockAddr</span> { ss_family<span>:</span> <span>2</span>, len<span>:</span> <span>16</span> }, protocol<span>:</span> <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>33</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>34</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>35</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>36</span> <span>|</span>
</pre></div>
<p dir="auto">Granted, the server was just two hops away from the client, and it didn't have to pass through the scary sea of the internet</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/latency_difference_between_hdp_and_udp.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/latency_difference_between_hdp_and_udp.png" alt="Description"></a></p>
<p dir="auto"><sub>The latency is in the microseconds due to both machines being in the same datacenter.</sub></p>
<p dir="auto">The latency difference between the HDP &amp; UDP was a consistent, but negligible 20μs across various benchmarks</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">But what about the internet?</h4><a id="user-content-but-what-about-the-internet" aria-label="Permalink: But what about the internet?" href="#but-what-about-the-internet"></a></p>
<p dir="auto">I tried sending packets from my Mac to the AWS server, and I reproduced the same one packet behaviour above. I left a sample of the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/tcpdump_tokyo_sever_mac_client.md"><code>tcpdump_tokyo_server_mac_client.md</code></a>. I sent 1 packet for all protocols, and all of them stopped working after the first packet except TCP/UDP/ICMP</p>
<p dir="auto">And as expected, sending or recieving packets from the Digital Ocean machine to the AWS machine didn't work</p>
<p dir="auto">There's no way to know for sure.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned-1" aria-label="Permalink: Lessons learned" href="#lessons-learned-1"></a></p>
<p dir="auto">Technically <em>yes</em>, you could use your own IP protocol. But unless you're a masochist, I do not suggest it</p>
<ul dir="auto">
<li>Your code won't be portable, and you'll need to support various operating systems</li>
<li>Your protocol will be randomly dropped at NAT gateways &amp; firewalls. It might work on your own network, but I gaurentee it won't work on the internet</li>
<li>From my testing, there's no latency improvements from using a non-standard IP protocol</li>
</ul>
<p dir="auto">TL;DR: <em><strong>Use TCP or UDP</strong></em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li>The <a href="https://datatracker.ietf.org/doc/html/rfc768" rel="nofollow">UDP protocol specification</a> is so minimal it is almost funny</li>
<li><a href="https://datatracker.ietf.org/doc/html/rfc3692#section-2.1" rel="nofollow">IP Protocol numbers that are assigned for testing</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">The list of protocols</a> supported under the IP protocol is pretty interesting</li>
<li><a href="https://hackaday.com/2024/09/21/when-raw-network-sockets-arent-raw-raw-sockets-in-macos-and-linux/" rel="nofollow">This</a> article speaks about some differences between raw sockets in Linux &amp; FreeBSD</li>
<li>How would you implement NAT on something other than TCP or UDP? <a href="https://superuser.com/a/1108226" rel="nofollow">This</a> answer is pretty insightful</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xcode Constantly Phones Home (135 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2025/2/5.html</link>
            <guid>43168589</guid>
            <pubDate>Tue, 25 Feb 2025 05:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2025/2/5.html">https://lapcatsoftware.com/articles/2025/2/5.html</a>, See on <a href="https://news.ycombinator.com/item?id=43168589">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>February 24 2025</h3>

<p>Building <a href="https://underpassapp.com/StopTheMadness/">StopTheMadness Pro</a> in Xcode is usually very fast, because my project doesn't use any Swift. It's a combination of Objective-C, which compiles much more quickly than Swift, and JavaScript, which doesn't need to be compiled. However, sometimes the builds were very slow for some strange reason. Checking the Xcode build transcripts, I found that the delay was in the "Gather provisioning inputs" build phase.</p>
<p><img src="https://lapcatsoftware.com/articles/2025/2/5.png" width="425" height="145" alt="Xcode build messages"></p>
<p>This one phase took 50.6 seconds when the entire build was 56.8 seconds!</p>
<p>I tested with my internet disabled, and the slow builds did not occur. Obviously, though, it's impractical to disable my internet every time I want to build and run. After all, my project is a Safari extension! I do use <a href="https://www.obdev.at/products/littlesnitch/">Little Snitch</a>, but I had previously allowed all connections from Xcode to <code>apple.com</code>, because that's required to upload builds to App Store Connect. When I scrutinized the individual Xcode connections with Little Snitch, I saw that <code>developerservices2.apple.com</code> was responsible for the slow "Gathering provisioning inputs" build phase. When I denied those connections with Little Snitch, my builds were always fast. And successful. The build phase is <em>mostly</em> unnecessary.</p>
<p>I found a <a href="https://developer.apple.com/forums/thread/756120">thread in the Apple Developer Forums</a> that discusses the problem, mentioning the <code>-allowProvisioningUpdates</code> option of the command-line <code>xcodebuild</code> tool. From the <code>man</code> page:</p>
<blockquote>Allow xcodebuild to communicate with the Apple Developer website.
           For automatically signed targets, xcodebuild will create and update
           profiles, app IDs, and certificates. For manually signed targets,
           xcodebuild will download missing or updated provisioning profiles.
           Requires a developer account to have been added in Xcode's Accounts
           preference pane.</blockquote>
<p>Connecting to <code>developerservices2.apple.com</code>, and to some other domains, is required in order to upload a build to App Store Connect. For most local builds, on the other hand, the "Gathering provisioning inputs" build phase is unnecessary and can slow down the build considerably. Thus, I've now denied Xcode connections to <code>developerservices2.apple.com</code> by default in Little Snitch and disable the rule only when uploading to App Store Connect.</p>
<p>During my investigation of slow builds, I noticed some other frequent Xcode connections. For example, Xcode connects to <code>devimages-cdn.apple.com</code> every time it launches. According to Apple's support document <a href="https://support.apple.com/101555">Use Apple products on enterprise networks</a>, that domain is used for "Xcode downloadable components". I assume this refers to platform support in the Components pane of Xcode Settings. (Note that the document doesn't mention <code>developerservices2.apple.com</code>.) Again, though, it's unnecessary to check for updates on every launch. I'd rather not tell Apple whenever I launch Xcode, or whenever I make a local build of my app. It certainly doesn't align with Apple's claim that they believe privacy is a fundamental human right. Or perhaps Apple believes that developers are subhuman…</p>
<p>I've saved the worst for last. For some reason, Xcode phones home to <code>appstoreconnect.apple.com</code> every time I open an Xcode project. This also appears to be unnecessary, and I experience no problems after denying the connections in Little Snitch, so I do! I assume that the connections send identifying information about the Xcode project to Apple, otherwise why even make the connections when opening a project? And all of these connections from Xcode, to every domain, require login to your Apple Developer account, so Apple is definitely receiving identifying information about you in any case.</p>
<p>In effect, Xcode is a developer analytics collection mechanism, whether you like it or not, which I don't.</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to change your settings to make yourself less valuable to Meta (269 pts)]]></title>
            <link>https://johnoliverwantsyourraterotica.com/</link>
            <guid>43167936</guid>
            <pubDate>Tue, 25 Feb 2025 03:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnoliverwantsyourraterotica.com/">https://johnoliverwantsyourraterotica.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43167936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">


			
				<article id="post-7" class="page">

				
					<div>
		<div id="home">
				
				
				
				
				
				
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Gray-logo.png" alt="" title="Gray logo"></span>
			</p>
			</div><div>
				
				
				
				
				<p><strong>How to change your settings</strong></p>
			</div><div>
<p><span>to make yourself less valuable to Meta</span></p></div><div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Both-logos.png" alt="" title="Both logos"></span>
			</p>
			</div><div>
				<div>
				
				
				
				
				<p><a href="#fb"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span></a>
			</p>
			</div><div>
				
				
				
				
				<p><a href="#ig"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span></a>
			</p>
			</div>
				
				
				
				
			</div>
				
				
			</div><div id="fb">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span>
			</p>
			</div><div><p><strong>TO STOP META FROM FEEDING YOU ADS BASED ON DATA COLLECTED ABOUT YOU FROM OTHER APPS AND WEBSITES</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Activity information from ad partners.”</span></li>
<li><span>Click “Review setting.”</span></li>
<li><span>Select “No, don’t make my ads more relevant by using this information.”</span></li>
<li><span>Click “Confirm.”</span></li>
</ul>

<p><strong>TO STOP META FROM USING YOUR DATA TO HELP ADVERTISERS TARGET YOU ON OTHER APPS</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Ads from ad partners.”</span></li>
<li><span>Select “Don’t show me ads from ad partners.”</span></li>
<li><span>Click the “X” button to close out.</span></li>
</ul>

<p><strong>TO UNLINK YOUR ACCOUNT FROM THE DATA ABOUT YOU THAT OTHER COMPANIES GIVE TO META</strong></p>
<ul>
<li><span>Click “Your information and permissions.”</span></li>
<li><span>Click “Your activity off Meta technologies.”</span></li>
<li><span>Click “Manage future activity.”</span></li>
<li><span>Select </span><b>“</b><span>Disconnect future activity.</span><b>”</b></li>
<li><span>Click “Continue.”</span></li>
<li><span>Click “Disconnect future activity.”</span></li>
</ul>
</div>
				
				
				
				
			</div><div id="ig">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span>
			</p>
			</div><div><p><strong>If your Facebook and Instagram accounts are linked, you’re good to go! </strong></p>

<p><strong>If your Facebook and Instagram accounts are NOT linked, go to<a href="http://accountscenter.instagram.com/" target="_blank" rel="noopener"> accountscenter.instagram.com</a> and repeat the steps above.</strong></p></div>
				
				
				
				
			</div><div><p><strong>OTHER STEPS YOU CAN TAKE:</strong></p>
<ul>
<li><span>Use a privacy-focused web browser like </span><a href="https://www.mozilla.org/en-US/firefox/" target="_blank" rel="noopener"><span>Firefox</span></a><span>.</span></li>
<li><span>Add a browser extension like </span><a href="https://privacybadger.org/" target="_blank" rel="noopener"><span>Privacy Badger</span></a><span> to block advertisers and other third parties from tracking you.&nbsp;</span></li>
<li><span>Disable your phone’s advertising identifier (see instructions for </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>iOS</span></a><span> and </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>Android</span></a><span> devices).&nbsp;</span></li>
</ul>

<p><span>SPECIAL THANKS TO THE </span><a href="https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data" target="_blank" rel="noopener"><span>ELECTRONIC FRONTIER FOUNDATION</span></a></p></div>		</div>

				
				</article>

			

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[History of CAD – David Weisberg (121 pts)]]></title>
            <link>https://www.shapr3d.com/blog/history-of-cad</link>
            <guid>43167865</guid>
            <pubDate>Tue, 25 Feb 2025 03:36:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shapr3d.com/blog/history-of-cad">https://www.shapr3d.com/blog/history-of-cad</a>, See on <a href="https://news.ycombinator.com/item?id=43167865">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div fs-toc-offsettop="96px" fs-toc-element="contents"><p>Writing a book about the history of CAD is an almost insurmountable task. If we, the editors, engineers, and CAD experts at Shapr3D were to accomplish this task, we’d most definitely fall short of the mark while spending countless hours with it.With over 80 years (and counting), the storied past of CAD slowly became the stuff of legends. </p><p>Today’s industry veterans might know a thing or two about the pioneers of the ’50s and ’60s, but the experts of tomorrow might even look at AutoCAD as a thing of the past.</p><p>David E. Weisberg chose to tell the story of CAD at the right moment: by still being able to interview the first key figures of a nascent technology, Weisberg gave a recollection of events, landmarks, and actors with accuracy that would be unattainable today.This digital edition of the original tome of around 650 pages is broken down into separate blog posts as per the author’s original &nbsp;chapters. By hosting this all-encompassing timeline of the history of CAD, we hope to offer our readers the in-depth look at the very origins of the industry, all the way to the early ’00s and modern-era CAD.</p><p>The late David E. Weisberg’s <em>The Engineering Design Revolution</em> is published on the Shapr3D Blog with the approval of the author’s relatives.</p><blockquote><strong><em>The People, Companies and Computer Systems That Changed Forever the Practice of Engineering<br>By David E. Weisberg</em></strong></blockquote><p>This book has been over five years in the making and is now freely available on this web site for your personal use. It can be read online or downloaded and printed for your later perusal. If a company wishes to produce multiple copies of specific material, please contact me at the address below.</p><p>In return for free access to over 650 pages of material discussing the people, companies and products that made the CAD industry what it is today, I am asking readers contribute whatever they wish to a foundation that means very much to me. In the mid-1990s I lost both of my sisters to cancer. I am sure many of you have also had relatives and acquaintances struck by this disease. I became very active in a non-profit organization in Denver that raises funds for cancer research and patient support. The endowment portion of this organization is called Cancer League of Colorado Foundation. You can either make a check out to the Foundation and send it to me at the address below (U.S. funds only) or contribute through PayPal.</p><p>Sit back and enjoy a trip through the nearly 60 years that totally revolutionized the practice of engineering design. One final word before you start if you find any errors in this material or take issue with what I have written, please let me hear from you. Also, if you have photographs or illustrations you would like to see incorporated into this document, please forward them to me. Having this book online results in a dynamic document that I will update as needed in the future.</p><ol start="0" role="list"><li><a href="https://www.shapr3d.com/history-of-cad/foreword">Foreword</a></li><li><a href="https://www.shapr3d.com/history-of-cad/introduction">Introduction</a></li><li><a href="https://www.shapr3d.com/history-of-cad/brief-overview-of-the-history-of-cad">Brief Overview</a></li><li><a href="https://www.shapr3d.com/history-of-cad/computer-aided-designs-strong-roots-at-mit">Computer-Aided Design Strong Roots at MIT</a></li><li><a href="https://www.shapr3d.com/history-of-cad/research-in-the-mid-to-late-1960s">Research in the Second Half of the 1960s</a></li><li><a href="https://www.shapr3d.com/history-of-cad/civil-engineering-software-development-at-mit">Civil Engineering Software Development at MIT</a></li><li><a href="https://www.shapr3d.com/history-of-cad/the-first-commercial-cad-system">The First Commercial CAD System</a></li><li><a href="https://www.shapr3d.com/history-of-cad/applicon">Applicon</a></li><li><a href="https://www.shapr3d.com/history-of-cad/autodesk-and-autocad">Autodesk and AutoCAD</a></li><li><a href="https://www.shapr3d.com/history-of-cad/auto-trol-technology">Auto-trol Technology</a></li><li><a href="https://www.shapr3d.com/history-of-cad/bentley-systems-incorporated">Bentley Systems</a></li><li><a href="https://www.shapr3d.com/history-of-cad/calma">Calma</a></li><li><a href="https://www.shapr3d.com/history-of-cad/computervision">Computervision</a></li><li><a href="https://www.shapr3d.com/history-of-cad/ibm-lockheed-and-dassault-systemes">IBM/Lockheed/Dassault Systèmes</a></li><li><a href="https://www.shapr3d.com/history-of-cad/intergraph">Intergraph</a></li><li><a href="https://www.shapr3d.com/history-of-cad/patrick-hanratty-and-manufacturing-consulting-services">Patrick Hanratty and Manufacturing &amp; Consulting Services</a></li><li><a href="https://www.shapr3d.com/history-of-cad/parametric-technology-corporation">Parametric Technology Corporation</a></li><li><a href="https://www.shapr3d.com/history-of-cad/structural-dynamics-research-corporation">Structural Dynamics Research Corporation</a></li><li><a href="https://www.shapr3d.com/history-of-cad/solidworks/">SolidWorks</a></li><li><a href="https://www.shapr3d.com/history-of-cad/siemens-plm-software-unigraphics">Siemens PLM Software (UGS)</a></li><li><a href="https://www.shapr3d.com/history-of-cad/tom-lazear-and-versacad">Tom Lazear and VersaCAD</a></li><li><a href="https://www.shapr3d.com/history-of-cad/miscellaneous-companies">Miscellaneous Companies</a></li><li><a href="https://www.shapr3d.com/history-of-cad/analysis-companies">Analysis Companies</a></li></ol></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disclosure of personal information to DOGE “is irreparable harm,” judge rules (245 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</link>
            <guid>43167579</guid>
            <pubDate>Tue, 25 Feb 2025 02:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/">https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</a>, See on <a href="https://news.ycombinator.com/item?id=43167579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>"The plaintiffs have made a clear showing that they are likely to suffer irreparable harm without injunctive relief," the order said. "DOGE affiliates have been granted access to systems of record that contain some of the plaintiffs' most sensitive data—Social Security numbers, dates of birth, home addresses, income and assets, citizenship status, and disability status—and their access to this trove of personal information is ongoing. There is no reason to believe their access to this information will end anytime soon because the government believes their access is appropriate."</p>
<p>The American Federation of Teachers, which represents 1.8 million teachers and nurses, was joined in the lawsuit by the International Association of Machinists and Aerospace Workers, International Federation of Professional and Technical Engineers, National Active and Retired Federal Employees Association, and National Federation of Federal Employees.</p>

<h2>No need to know</h2>
<p>The government insisted that the DOGE affiliates are employees of Education and OPM, and the judge assumed that is true for purposes of evaluating the motion for a restraining order. Even with that allowance, Boardman decided the data access is not permissible under the "need-to-know" exception to the law prohibiting unnecessary disclosure.</p>
<p>The Trump administration did not explain why "the DOGE affiliates at Education <em>need</em> such comprehensive, sweeping access to the plaintiffs' records to audit student loan programs for waste, fraud, and abuse or to conduct cost-estimate analyses," Boardman wrote, adding that "there appears to be no precedent with similar facts."</p>
<p>There are six DOGE affiliates working at Education. They include Adam Ramada, a United States DOGE Service employee, and five "DOGE-affiliated individuals" who have not been identified by name.</p>
<p>"It may be that, with additional time, the government can explain why granting such broad access to the plaintiffs' personal information is necessary for DOGE affiliates at Education to do their jobs, but for now, the record before the Court indicates they do not have a <em>need</em> for these records in the performance of their duties," Boardman wrote.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek open source DeepEP – library for MoE training and Inference (478 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepEP</link>
            <guid>43167373</guid>
            <pubDate>Tue, 25 Feb 2025 02:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a>, See on <a href="https://news.ycombinator.com/item?id=43167373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepEP</h2><a id="user-content-deepep" aria-label="Permalink: DeepEP" href="#deepep"></a></p>
<p dir="auto">DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p dir="auto">To align with the group-limited gating algorithm proposed in the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p>
<p dir="auto">For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p>
<p dir="auto">Notice: the implementation in this library may have some slight differences from the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Normal kernels with NVLink and RDMA forwarding</h3><a id="user-content-normal-kernels-with-nvlink-and-rdma-forwarding" aria-label="Permalink: Normal kernels with NVLink and RDMA forwarding" href="#normal-kernels-with-nvlink-and-rdma-forwarding"></a></p>
<p dir="auto">We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Type</th>
<th>Dispatch #EP</th>
<th>Bottleneck bandwidth</th>
<th>Combine #EP</th>
<th>Bottleneck bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intranode</td>
<td>8</td>
<td>153 GB/s (NVLink)</td>
<td>8</td>
<td>158 GB/s (NVLink)</td>
</tr>
<tr>
<td>Internode</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>32</td>
<td>44 GB/s (RDMA)</td>
<td>32</td>
<td>47 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>64</td>
<td>46 GB/s (RDMA)</td>
<td>64</td>
<td>45 GB/s (RDMA)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-latency kernels with pure RDMA</h3><a id="user-content-low-latency-kernels-with-pure-rdma" aria-label="Permalink: Low-latency kernels with pure RDMA" href="#low-latency-kernels-with-pure-rdma"></a></p>
<p dir="auto">We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dispatch #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
<th>Combine #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>163 us</td>
<td>46 GB/s</td>
<td>8</td>
<td>318 us</td>
<td>46 GB/s</td>
</tr>
<tr>
<td>16</td>
<td>173 us</td>
<td>43 GB/s</td>
<td>16</td>
<td>329 us</td>
<td>44 GB/s</td>
</tr>
<tr>
<td>32</td>
<td>182 us</td>
<td>41 GB/s</td>
<td>32</td>
<td>350 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>64</td>
<td>186 us</td>
<td>40 GB/s</td>
<td>64</td>
<td>353 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>128</td>
<td>192 us</td>
<td>39 GB/s</td>
<td>128</td>
<td>369 us</td>
<td>39 GB/s</td>
</tr>
<tr>
<td>256</td>
<td>194 us</td>
<td>39 GB/s</td>
<td>256</td>
<td>360 us</td>
<td>40 GB/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Hopper GPUs (may support more architectures or devices later)</li>
<li>Python 3.8 and above</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.1 and above</li>
<li>NVLink for intranode communication</li>
<li>RDMA network for internode communication</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and install NVSHMEM dependency</h3><a id="user-content-download-and-install-nvshmem-dependency" aria-label="Permalink: Download and install NVSHMEM dependency" href="#download-and-install-nvshmem-dependency"></a></p>
<p dir="auto">DeepEP also depends on our modified NVSHMEM. Please refer to our <a href="https://github.com/deepseek-ai/DeepEP/blob/main/third-party/README.md">NVSHMEM Installation Guide</a> for instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build and make symbolic links for SO files
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
# You may modify the specific SO names according to your own platform
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

# Run test cases
# NOTES: you may modify the `init_dist` function in `tests/utils.py`
# according to your own cluster settings, and launch into multiple nodes 
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py"><pre><span><span>#</span> Build and make symbolic links for SO files</span>
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
<span><span>#</span> You may modify the specific SO names according to your own platform</span>
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

<span><span>#</span> Run test cases</span>
<span><span>#</span> NOTES: you may modify the `init_dist` function in `tests/utils.py`</span>
<span><span>#</span> according to your own cluster settings, and launch into multiple nodes </span>
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install"><pre>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install</pre></div>
<p dir="auto">Then, import <code>deep_ep</code> in your Python project, and enjoy!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Network configurations</h2><a id="user-content-network-configurations" aria-label="Permalink: Network configurations" href="#network-configurations"></a></p>
<p dir="auto">DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Traffic isolation</h3><a id="user-content-traffic-isolation" aria-label="Permalink: Traffic isolation" href="#traffic-isolation"></a></p>
<p dir="auto">Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p>
<p dir="auto">To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p>
<ul dir="auto">
<li>workloads using normal kernels</li>
<li>workloads using low-latency kernels</li>
<li>other workloads</li>
</ul>
<p dir="auto">For DeepEP, you can control the virtual lane assignment by setting the <code>NVSHMEM_IB_SL</code> environment variable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adaptive routing</h3><a id="user-content-adaptive-routing" aria-label="Permalink: Adaptive routing" href="#adaptive-routing"></a></p>
<p dir="auto">Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Currently, low-latency kernels support adaptive routing, while normal kernels do not (support may be added soon). <strong>Enabling adaptive routing for normal internode kernels may lead to deadlocks or data corruption issues</strong>.</p>
<p dir="auto">For low-latency kernels, enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p>
<ul dir="auto">
<li>enable adaptive routing in environments with heavy network loads</li>
<li>use static routing in environments with light network loads</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Congestion control</h3><a id="user-content-congestion-control" aria-label="Permalink: Congestion control" href="#congestion-control"></a></p>
<p dir="auto">Congestion control is disabled as we have not observed significant congestion in our production environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interfaces and examples</h2><a id="user-content-interfaces-and-examples" aria-label="Permalink: Interfaces and examples" href="#interfaces-and-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in model training or inference prefilling</h3><a id="user-content-example-use-in-model-training-or-inference-prefilling" aria-label="Permalink: Example use in model training or inference prefilling" href="#example-use-in-model-training-or-inference-prefilling"></a></p>
<p dir="auto">The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)
_buffer: Optional[Buffer] = None

# Set the number of SMs to use
# NOTES: this is a static variable
Buffer.set_num_sms(24)


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -> Buffer:
    global _buffer
    
    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if not existed or not enough buffer size
    # NOTES: the adaptive routing configuration of the network **must be off**
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes < num_nvl_bytes or _buffer.num_rdma_bytes < num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer


def get_hidden_bytes(x: torch.Tensor) -> int:
    t = x[0] if isinstance(x, tuple) else x
    return t.size(1) * max(t.element_size(), 2)


def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,
                     num_experts: int, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency 
    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please
    # refer to the docs of `Buffer.dispatch`
    global _buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    # NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph
    # For more advanced usages, please refer to the docs of the `dispatch` function
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    # For event management, please refer to the docs of the `EventOverlap` class
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event


def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -> \
        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
    global _buffer

    # The backward process of MoE dispatch is actually a combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_grad_x, combined_grad_recv_topk_weights, event


def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[torch.Tensor, EventOverlap]:
    global _buffer

    # Do MoE combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_x, event


def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
    global _buffer

    # The backward process of MoE combine is actually a dispatch
    # For more advanced usages, please refer to the docs of the `combine` function
    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return grad_x, event"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>List</span>, <span>Tuple</span>, <span>Optional</span>, <span>Union</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>, <span>EventOverlap</span>

<span># Communication buffer (will allocate at runtime)</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>

<span># Set the number of SMs to use</span>
<span># NOTES: this is a static variable</span>
<span>Buffer</span>.<span>set_num_sms</span>(<span>24</span>)


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>hidden_bytes</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span>global</span> <span>_buffer</span>
    
    <span># NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests</span>
    <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span> <span>=</span> <span>0</span>, <span>0</span>
    <span>for</span> <span>config</span> <span>in</span> (<span>Buffer</span>.<span>get_dispatch_config</span>(<span>group</span>.<span>size</span>()), <span>Buffer</span>.<span>get_combine_config</span>(<span>group</span>.<span>size</span>())):
        <span>num_nvl_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_nvl_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_nvl_bytes</span>)
        <span>num_rdma_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_rdma_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_rdma_bytes</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span># NOTES: the adaptive routing configuration of the network **must be off**</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>_buffer</span>.<span>num_nvl_bytes</span> <span>&lt;</span> <span>num_nvl_bytes</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span>)
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>get_hidden_bytes</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>) <span>-&gt;</span> <span>int</span>:
    <span>t</span> <span>=</span> <span>x</span>[<span>0</span>] <span>if</span> <span>isinstance</span>(<span>x</span>, <span>tuple</span>) <span>else</span> <span>x</span>
    <span>return</span> <span>t</span>.<span>size</span>(<span>1</span>) <span>*</span> <span>max</span>(<span>t</span>.<span>element_size</span>(), <span>2</span>)


<span>def</span> <span>dispatch_forward</span>(<span>x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>,
                     <span>num_experts</span>: <span>int</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>List</span>, <span>Tuple</span>, <span>EventOverlap</span>]:
    <span># NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency </span>
    <span># of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please</span>
    <span># refer to the docs of `Buffer.dispatch`</span>
    <span>global</span> <span>_buffer</span>

    <span># Calculate layout before actual dispatch</span>
    <span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span>, <span>num_tokens_per_expert</span>, <span>is_token_in_rank</span>, <span>previous_event</span> <span>=</span> \
        <span>_buffer</span>.<span>get_dispatch_layout</span>(<span>topk_idx</span>, <span>num_experts</span>,
                                    <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                    <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)
    <span># Do MoE dispatch</span>
    <span># NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph</span>
    <span># For more advanced usages, please refer to the docs of the `dispatch` function</span>
    <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>dispatch</span>(<span>x</span>, <span>topk_idx</span><span>=</span><span>topk_idx</span>, <span>topk_weights</span><span>=</span><span>topk_weights</span>,
                         <span>num_tokens_per_rank</span><span>=</span><span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span><span>=</span><span>num_tokens_per_rdma_rank</span>,
                         <span>is_token_in_rank</span><span>=</span><span>is_token_in_rank</span>, <span>num_tokens_per_expert</span><span>=</span><span>num_tokens_per_expert</span>,
                         <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                         <span>allocate_on_comm_stream</span><span>=</span><span>True</span>)
    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span>


<span>def</span> <span>dispatch_backward</span>(<span>grad_recv_x</span>: <span>torch</span>.<span>Tensor</span>, <span>grad_recv_topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE dispatch is actually a combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>combine</span>(<span>grad_recv_x</span>, <span>handle</span>, <span>topk_weights</span><span>=</span><span>grad_recv_topk_weights</span>, <span>async_finish</span><span>=</span><span>True</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span>


<span>def</span> <span>combine_forward</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_x</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>combine</span>(<span>x</span>, <span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>, <span>previous_event</span><span>=</span><span>previous_event</span>,
                                           <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_x</span>, <span>event</span>


<span>def</span> <span>combine_backward</span>(<span>grad_combined_x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE combine is actually a dispatch</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>grad_x</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>dispatch</span>(<span>grad_combined_x</span>, <span>handle</span><span>=</span><span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                                 <span>previous_event</span><span>=</span><span>previous_event</span>,
                                                 <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>grad_x</span>, <span>event</span></pre></div>
<p dir="auto">Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/normal.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/normal.png" alt="normal"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in inference decoding</h3><a id="user-content-example-use-in-inference-decoding" aria-label="Permalink: Example use in inference decoding" href="#example-use-in-inference-decoding"></a></p>
<p dir="auto">The low latency kernels can be used in the inference decoding phase as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import Tuple, Optional

from deep_ep import Buffer

# Communication buffer (will allocate at runtime)
# NOTES: there is no SM control API for the low-latency kernels
_buffer: Optional[Buffer] = None


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -> Buffer:
    # NOTES: the low-latency mode will consume much more space than the normal mode
    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256
    global _buffer
    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)

    # Allocate a buffer if not existed or not enough buffer size
    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes < num_rdma_bytes:
        # NOTES: for best performance, the QP number **must** be equal to the number of the local experts
        assert num_experts % group.size() == 0
        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())
    return _buffer


def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):
    global _buffer

    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)
    recv_hidden_states, recv_expert_count, handle, event, hook = \
        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,
                                     async_finish=False, return_recv_hook=True)

    # NOTES: the actual tensor will not be received only if you call `hook()`,
    # it is useful for double-batch overlapping, but **without any SM occupation**
    # If you don't want to overlap, please set `return_recv_hook=False`
    # Later, you can use our GEMM library to do the computation with this specific format
    return recv_hidden_states, recv_expert_count, handle, event, hook


def low_latency_combine(hidden_states: torch.Tensor,
                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):
    global _buffer

    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)
    combined_hidden_states, event_overlap, hook = \
        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,
                                    async_finish=False, return_recv_hook=True)

    # NOTES: the same behavior as described in the dispatch kernel
    return combined_hidden_states, event_overlap, hook"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span>, <span>Optional</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>

<span># Communication buffer (will allocate at runtime)</span>
<span># NOTES: there is no SM control API for the low-latency kernels</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>hidden</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span># NOTES: the low-latency mode will consume much more space than the normal mode</span>
    <span># So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256</span>
    <span>global</span> <span>_buffer</span>
    <span>num_rdma_bytes</span> <span>=</span> <span>Buffer</span>.<span>get_low_latency_rdma_size_hint</span>(<span>num_max_dispatch_tokens_per_rank</span>, <span>hidden</span>, <span>group</span>.<span>size</span>(), <span>num_experts</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>not</span> <span>_buffer</span>.<span>low_latency_mode</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span># NOTES: for best performance, the QP number **must** be equal to the number of the local experts</span>
        <span>assert</span> <span>num_experts</span> <span>%</span> <span>group</span>.<span>size</span>() <span>==</span> <span>0</span>
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>0</span>, <span>num_rdma_bytes</span>, <span>low_latency_mode</span><span>=</span><span>True</span>, <span>num_qps_per_rank</span><span>=</span><span>num_experts</span> <span>//</span> <span>group</span>.<span>size</span>())
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>low_latency_dispatch</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_dispatch</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>num_max_dispatch_tokens_per_rank</span>, <span>num_experts</span>,
                                     <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the actual tensor will not be received only if you call `hook()`,</span>
    <span># it is useful for double-batch overlapping, but **without any SM occupation**</span>
    <span># If you don't want to overlap, please set `return_recv_hook=False`</span>
    <span># Later, you can use our GEMM library to do the computation with this specific format</span>
    <span>return</span> <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span>


<span>def</span> <span>low_latency_combine</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>,
                        <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_combine</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>topk_weights</span>, <span>handle</span>,
                                    <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the same behavior as described in the dispatch kernel</span>
    <span>return</span> <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span></pre></div>
<p dir="auto">For two micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffics are happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e. the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/low-latency.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/low-latency.png" alt="low-latency"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notices</h2><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<ul dir="auto">
<li>For extreme performance, we discover and use a behavior-out-of-doc PTX instruction: <code>ld.global.nc.L1::no_allocate.L2::256B</code>. This instruction will lead to an undefined behavior: accessing volatile GPU memory with non-coherent read-only PTX modifiers <code>.nc</code>. But the correctness is tested to be guaranteed with <code>.L1::no_allocate</code> on Hopper architectures, and performance will be much better. If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to <code>setup.py</code> and disable this, or file an issue.</li>
<li>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek's internal cluster.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepEP/blob/main/LICENSE">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href="https://docs.nvidia.com/nvshmem/api/sla.html" rel="nofollow">NVSHMEM SLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepep2025,
      title={DeepEP: an efficient expert-parallel communication library},
      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},
}"><pre><span>@misc</span>{<span>deepep2025</span>,
      <span>title</span>=<span><span>{</span>DeepEP: an efficient expert-parallel communication library<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepEP}<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DigiCert: Threat of legal action to stifle Bugzilla discourse (554 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</link>
            <guid>43167087</guid>
            <pubDate>Tue, 25 Feb 2025 01:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</a>, See on <a href="https://news.ycombinator.com/item?id=43167087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">Bug 1950144</a>
      </span>
      <span>
        <span>Opened <span title="2025-02-24 08:36 PST" data-time="1740414973">13 hours ago</span></span>
          <span>Updated <span title="2025-02-24 20:23 PST" data-time="1740457405">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>


<div id="module-categories">
        <p><span id="field-value-component">
      <div>
        <p><span id="component-name" tabindex="0" role="button" aria-haspopup="menu" aria-controls="component-info">CA Certificate Root Program
          
        </span></p>
      </div>
        </span>
    </p></div>






































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;136.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;128.7.0esr&quot;,&quot;FIREFOX_ESR115&quot;:&quot;115.20.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;137.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2025-02-03&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2025-02-04&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2025-01-30&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2025-01-31&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;135.0.1&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2025-03-03&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2025-03-04&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2025-02-27&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2025-02-28&quot;}">



<div id="c0" data-comment-id="17364853"><p>In <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c74" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322 comment 74</a> DigiCert wrote,</p>
<blockquote>
<p>“We have not used a legal team as a shield against accountability.”</p>
</blockquote>
<p>Contrary to this statement, I received a letter from DigiCert’s lawyers, Wilson Sonsini, regarding posts made by Sectigo’s Chief Compliance Officer in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322</a>.  The upshot of the letter was that DigiCert expected Sectigo to “ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization.”</p>
<p>I’m Brian Holland, General Counsel for Sectigo, and this is my first time posting on Bugzilla.  I’m posting because at Sectigo we believe that the WebPKI is best served by open, transparent, and honest debate about issues that impact our community.  Attempts to shut down these conversations, through lawyers or otherwise, are harmful to our collective core mission.</p>
<p>In its opening passages, this letter reads (emphasis mine),</p>
<blockquote>
<p>We ask for your prompt cooperation and assistance in taking corrective action and <strong>forcing Mr. Callan to cease his disparaging public statements. We hope your assistance in this matter will render unnecessary legal action by DigiCert against Sectigo.</strong></p>
</blockquote>
<p>After three pages of detail about specific Bugzilla posts and references to the Lanham Act, deceptive trade practices, corporate disparagement, and tortious interference, the letter (the full letter is included as an attachment to this bug) goes on to say (emphasis mine):</p>
<blockquote>
<p>At this point, we are bringing this situation to your attention on behalf of DigiCert because we are hopeful that Mr. Callan’s actions were the actions of one individual and were not part of an organized plan or institutional practice. We also hope that, upon receiving this information, Sectigo will recognize the impropriety of Mr. Callan’s statements and the substantial public, industry, and browser scrutiny and legal risk such statements would prompt if they were to continue. To that end, we expect that Sectigo will investigate this incident promptly and take the appropriate corrective actions, confirm that this situation was not part of an institutional practice, and <strong>ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization. We hope we can resolve this situation as soon as possible before DigiCert is compelled to seek legal action.</strong></p>
</blockquote>
<p>On December 10, 2024 I sent this response in email to my contact at Wilson Sonsini:</p>
<blockquote>
<p>I have reviewed your letter and the Bugzilla thread referenced therein.  In that letter, you suggest that DigiCert has various legal claims against Sectigo and/or its COO [sic], Tim Callan, for what you call “false and misleading statements about DigiCert” made on the Bugzilla forum.  We strongly disagree.  The statements you point to are questions and/or statements of opinion that are not actionable statements of fact.  Moreover, those comments were made with the intent of facilitating discussion and debate about important questions of first impression for our industry.  They were made by Tim Callan in good faith, are fully protected by the First Amendment, and cannot, as a matter of law, form the basis for any of the causes of action mentioned in your letter.</p>
</blockquote>
<blockquote>
<p>As you are aware, the PKI community is a self-regulating group that, as set out in the bylaws of the Certificate Authority Browser Forum, works “closely together in defining the guidelines and means of implementation for best practices as a way of providing a heightened security for Internet transactions and creating a more intuitive method of displaying secure sites to Internet users.”  For the community to self-regulate, there needs to be open, uninhibited, and robust discussion and debate about best practices in the industry.  Any litigation threats that chill or stifle such debate undermine the self-regulatory system that has worked so well for the industry.</p>
</blockquote>
<blockquote>
<p>Certificate Authorities post incident reports on Bugzilla to “provide lessons learned and transparency about the steps the CA Owner takes to address the immediate issue and prevent future issues.” As the Common CA Database goes on to state “incident reports help the Web PKI ecosystem as a whole because they promote continuous improvement, information sharing, and highlight opportunities to define and adopt improved practices, policies, and controls” of all parties.</p>
</blockquote>
<blockquote>
<p>The TRO involved in this incident report, as one Bugzilla commenter noted, is “an unprecedented event in the WebPKI, and . . . if allowed to proliferate, it would potentially be used by subscribers en masse to do an end-run around important technical security controls.”</p>
</blockquote>
<blockquote>
<p>The PKI Community has never considered how it should respond to TROs and now needs to do so. Understanding the situation faced by your client and why it made certain decisions is important to improving the WebPKI ecosystem. This is why Mr. Callan, and many others, have been asking questions – some of which have been critical questions designed to achieve a consensus as to how best handle situations like this in the future.  In any such discussion, there will be differences of opinion, but open, uninhibited, robust, and transparent discussion is essential for the industry to learn how to best move forward.</p>
</blockquote>
<blockquote>
<p>I hope that your client will, on deeper reflection, realize that as a leader in the PKI Community, it should be driving, rather than stifling, discussion of this topic.  Your client’s threat of litigation is, in our view, both misguided and without merit.  We will strive to be respectful in our tone, but neither Mr. Callan nor Sectigo will be silenced or prevented from asking critical questions and/or engaging in critical discussion about issues of substantial concern to the public and the industry.</p>
</blockquote>
<p>We find the threat of legal action to stifle scrutiny and discussion of public CA practices to be deeply troubling and entirely at odds with the transparent, blameless post-mortem culture that the CCADB incident report guidelines expect CAs to embrace.  Even for a company like Sectigo, the threat of a lawsuit from a well-resourced organization like DigiCert is worrisome, regardless of our confidence that Mr. Callan’s speech was proper, legally protected, and in the best interest of the WebPKI.  Another party challenging DigiCert’s behavior, faced with this same threat, might choose simply to stop asking uncomfortable questions.</p>
<p>No CA should be allowed to intimidate its critics into silence. This would irreparably damage the integrity and quality of the WebPKI.</p>
<p>I am sharing this incident to bring attention to DigiCert’s actions and allow the community to evaluate this approach. What began as a discussion of the threat posed by certificate subscribers using the legal system to circumvent WebPKI security controls needs, in my opinion, to be broadened.</p>
</div><div id="a3699_1689"><p>Component: CA Certificate Compliance → CA Certificate Root Program</p></div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="1">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone at NSF overseeing the Platforms for Wireless Experimentation is gone (389 pts)]]></title>
            <link>https://discuss.systems/@ricci/114059690609284323</link>
            <guid>43166830</guid>
            <pubDate>Tue, 25 Feb 2025 00:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.systems/@ricci/114059690609284323">https://discuss.systems/@ricci/114059690609284323</a>, See on <a href="https://news.ycombinator.com/item?id=43166830">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's still worth blogging in the age of AI (292 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</link>
            <guid>43166761</guid>
            <pubDate>Tue, 25 Feb 2025 00:46:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=43166761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>My post about blogging as
<a href="https://www.gilesthomas.com/2025/02/20250223-til-deep-dive-posts">writing the tutorial that you wished you'd found</a>
really took off
<a href="https://news.ycombinator.com/item?id=43154666">on Hacker News</a>.  There were
a lot of excellent comments, but one thing kept coming up: what's the point
in blogging if people are using ChatGPT, Claude and DeepSeek to spoon-feed them
answers?  Who, apart from the AIs, will read what you write?</p>

<p>I was asking myself the same question when I started blogging semi-regularly again
last year, and this post is an attempt to summarise why I decided that it was worthwhile.
The TL;DR: <strong>blogging isn't just about being read -- it's about learning and thinking, and
having a durable proof that you can do both.</strong></p>


    
        <p>Let's start off by summarising the two big reasons to blog about what you've learned,
as you learn:</p>

<ul>
<li>It helps you make your newly-gained knowledge concrete.</li>
<li>It will help other people in the future -- they might be looking for the information
you blogged about, and find it on your blog.</li>
</ul>

<p>When we're thinking about AI, it's only the second one that matters; you'll learn better
by writing whether or not other people or LLMs read it.  But in terms of
helping other people, these days
you might publish your hard-earned learnings on <a href="https://www.gilesthomas.com/2021/03/fun-with-network-namespaces">Linux Network Namespaces</a>,
but when, the next day, someone wants to find out how to use them, they ask
ChatGPT, it does a search, finds your page, ingests it, and presents the results
as its own, perhaps mashed up with some scraps from elsewhere.  Sure, your site is probably
linked in the "references" section in the response, but frankly, no-one ever looks at that.
What's worse, within the next six months your site is likely to be sucked into the
AIs' next training run, and after that you won't even get a reference.</p>

<p>Now if the "solving other people's problems" aspect of blogging was purely altruistic,
that wouldn't matter a jot.  But of course it's not, there are a bunch of other reasons.
Three that come to mind:</p>

<ol>
<li>Making a name for yourself.</li>
<li>The sheer dopamine hit of knowing that other people like what you've done --
a higher-effort version of getting an upvote or like on social media.</li>
<li>Building a portfolio of writing you can point to.</li>
</ol>

<p>Let's take those in turn.</p>

<p>If you want to blog to make a name for yourself, then you're going to have a hard
time.  Here's an example: if you're not a regular reader of this blog, where do
I (as in, the author of this post) live?  What is my day job?  No cheating and
clicking on the "About" link above, please.</p>

<p>If you knew the answer, you're one of a rare few.  Yesterday there
were about 35,000 visits to this site thanks to that HN link, and fewer than 300 hits
on the "About" page.  This is normal!  If you write a blog post, then even if people
find it interesting, they'll come, read it, hopefully think that it was worth their
time, and then move on.  That is how it should be, there's no need for someone to
become fascinated with your life just because you said something useful once -- and
that's a good thing, no-one wants a stalker.</p>

<p>Even if you churn out banger posts again and again, as a pure blogger you're not going
to build up a "personal brand" that's worth much.</p>

<p>Think about the well-known bloggers you read: they’re famous because
they did something else
important.  They started a major open-source project, or a company, or invented something.
They give regular talks at conferences.  They write successful science fiction.
Or something else.</p>

<p>So, I don't think you can make a name for yourself by blogging alone, and if you
are blogging with that as your goal, I fear you're going to be disappointed.</p>

<p>The dopamine hit is definitely more of a thing.  When people comment on my
posts, I get a nice warm glow.  And when last night, just before I went to bed, I saw that
my previous post was #1 on the front page of HN, I took a screenshot and posted it
to my "Fellow Geeks" WhatsApp group with the caption "w00t!".</p>

<p>But those moments
are rare, and I don't really think AI will make them rarer.
Blogging can sometimes feel like you're
shouting into the void -- most posts get no engagement, and that has been true since
I started back in 2006.  You might have 500 loyal
readers, or none -- there's no way to tell.</p>

<p>I think that all I can say regarding that is to echo what serviceberry
<a href="https://news.ycombinator.com/item?id=43155587">said on HN</a> (bold mine):</p>

<blockquote>
  <p>The corollary is that if you find that post, <strong>say something</strong>. Drop the author a
  note, leave a comment. No one else does. For every YT celebrity, there are
  thousands of people posting good content on the internet and not knowing if
  it's being seen or appreciated by anyone.</p>
</blockquote>

<p>...and maybe suggest that we all occasionally check the references in our helpful AI-generated
responses and drop a line to the authors to say "thanks"!</p>

<p>But let's finish with the last one, which is more positive.  I said that you will
be vanishingly unlikely to make a name for yourself with blogging on its own.  But that
doesn't mean it's pointless from a career perspective.  You're building up a portfolio
of writing about topics that interest you.  Imagine you're in a job interview and
are asked about X.  You reply with the details you know, and add
"but I blogged about that in detail a while back, shall I send you a link later?"
Or if you're aiming to close a contract with a potential consulting client in a
particular area -- wouldn't it be useful to send them a list of links showing your
thoughts on aspects of exactly that topic?</p>

<p>Your GitHub profile shows your contributions to open source and lets people know
how well you can code.  But your blog shows your contributions to knowledge, and
shows how well you can think.  That's valuable!</p>

<p>It's time to wrap this up.  Blogging is valuable because it helps you learn, because it
helps others solve problems, because you get a rare buzz when you realise that yes,
people are reading this stuff, and because you're building a portfolio of writing to show
your skills.  The only one of those that I believe AI might harm is the buzz of
engagement, and that's so rare for most blogs that I don't think it's worth worrying about.</p>

<p>And after all -- if the AI doom scenario does come true, at least as someone whose
thoughts have been regularly published on the Internet, you'll be part of the
paperclip maximisers' training set, so they'll remember you in a sense.  So
there's that.</p>

    

    
        
    

    



            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clean Code vs. A Philosophy Of Software Design (377 pts)]]></title>
            <link>https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</link>
            <guid>43166362</guid>
            <pubDate>Mon, 24 Feb 2025 23:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md">https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=43166362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><em>(This document is the result of a series of discussions, some online and
some in person, held between Robert "Uncle Bob" Martin and John Ousterhout between
September, 2024 and February, 2025. If you would like to comment on anything
in this discussion, we recommend that you do so on the <a href="https://groups.google.com/g/software-design-book" rel="nofollow">Google group
associated with APOSD</a>)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introductions</h2><a id="user-content-introductions" aria-label="Permalink: Introductions" href="#introductions"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Hi (Uncle) Bob! You and I have each written books on software design.
We agree on some things, but there are some pretty big differences of
opinion between my recent book <em>A Philosophy of Software Design</em>
(hereafter "APOSD") and your classic book <em>Clean Code</em>. Thanks for
agreeing to discuss those differences here.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My pleasure John.  Before we begin let me say that I've carefully read through your book and I found it very enjoyable, and full of valuable insights.  There are some things I disagree with you on, such as TDD, and Abstraction-First incrementalism, but overall I enjoyed it a lot.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I'd like to discuss three topics with you: method length, comments,
and test-driven development. But before getting into these,
let's start by comparing overall philosophies. When you hear about a
new idea related to software design, how do you decide whether or not
to endorse that idea?</p>
<p dir="auto">I'll go first. For me, the fundamental goal of software design is
to make it easy to understand and modify the system. I use the term
"complexity" to refer to things that make it hard to understand and
modify a system. The most important contributors
to complexity relate to information:</p>
<ul dir="auto">
<li>How much information must a developer have in their head in order to carry out a task?</li>
<li>How accessible and obvious is the information that the developer needs?</li>
</ul>
<p dir="auto">The more information a developer needs to have, the harder it will be
for them to work on the system. Things get even worse if the required
information isn't obvious. The worst case is when there is a crucial
piece of information hidden in some far-away piece of code
that the developer has never heard of.</p>
<p dir="auto">When I'm evaluating an idea related to software design, I ask whether
it will reduce complexity. This usually means either reducing the amount
of information a developer has to know, or making the required information
more obvious.</p>
<p dir="auto">Now over to you: are there general principles that you use when deciding
which ideas to endorse?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree with your approach. A discipline or technique should make the job of programmers easier. I would add that the programmer we want to help most is not the author.  The programmer whose job we want to make easier is the programmer who must read and understand the code written by others (or by themself a week later).  Programmers spend far more hours reading code than writing code, so the activity we want to ease is that of reading.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Method Length</h2><a id="user-content-method-length" aria-label="Permalink: Method Length" href="#method-length"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our first area of disagreement is method length.
On page 34 of <em>Clean Code</em> you say "The first rule of functions is that
they should be small. The second rule of functions is that
<em>they should be smaller than that</em>." Later on, you say "Functions
should hardly ever be 20 lines long" and suggest that functions
should be "just two, three, or four lines long". On page 35, you
say "Blocks within <code>if</code> statements, <code>else</code> statements, <code>while</code> statements,
and so on should be one line long. Probably that line should be a function
call." I couldn't find anything in <em>Clean Code</em> to suggest that a function
could ever be too short.</p>
<p dir="auto">I agree that dividing up code into relatively small units ("modular design")
is one of the most important ways to reduce the amount of information a
programmer has to keep in their mind at once. The idea, of course, is to take a
complex chunk of functionality and encapsulate it in a separate method
with a simple interface. Developers can then harness the functionality
of the method (or read code that invokes the method) without learning
the details of how the method is implemented; they only need to learn its
interface. The best methods are those that provide a lot of functionality
but have a very simple interface: they replace a large cognitive load
(reading the detailed implementation) with a much smaller
cognitive load (learning the interface). I call these methods "deep".</p>
<p dir="auto">However, like most ideas in software design, decomposition can be taken too far.
As methods get smaller and smaller there is less and less
benefit to further subdivision.
The amount of functionality hidden behind each interface
drops, while the interfaces often become more complex.
I call these interfaces "shallow": they don't help much in terms of
reducing what the programmer needs to know. Eventually, the point is
reached where someone using the method needs
to understand every aspect of its implementation. Such methods
are usually pointless.</p>
<p dir="auto">Another problem with decomposing too far is that it tends to
result in <em>entanglement</em>. Two methods
are entangled (or "conjoined" in APOSD terminology) if, in order to
understand how one of them works internally, you also need to read the
code of the other. If you've ever found yourself flipping back and forth
between the implementations of two methods as you read code, that's a
red flag that the methods might be entangled. Entangled methods
are hard to read because the information you need to have in your head
at once isn't all in the same place. Entangled methods can usually
be improved by combining them so that all the code is in one place.</p>
<p dir="auto">The advice in <em>Clean Code</em> on method length is so extreme that it encourages
programmers to create teeny-tiny methods that suffer from both shallow
interfaces and entanglement.  Setting arbitrary numerical limits such
as 2-4 lines in a method and a single line in the body of an
<code>if</code> or <code>while</code> statement exacerbates this problem.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">While I do strongly recommend very short functions, I don't think it's fair to say that the book sets arbitrary numerical limits. The 2-4 line functions that you referred to on page 34 were part of the <em>Sparkle</em> applet that Kent Beck and I wrote together in 1999 as an exercise for learning TDD. I thought it was remarkable that most of the functions in that applet were 2-4 lines long because it was a Swing program; and Swing programs tend to have very long methods.</p>
<p dir="auto">As for setting limits, on page 13 I make clear that although the recommendations in the book have worked well for me and the other authors, they might not work for everyone.  I claimed no final authority, nor even any absolute "rightness". They are offered for consideration.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think these problems will be easiest to understand if we look at
specific code examples. But before we do that, let me ask you, Bob:
do you believe that it's possible for code to be over-decomposed, or
is smaller always better? And, if you believe that over-decomposition
is possible, how do you recognize when it has occurred?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It is certainly possible to over-decompose code.  Here's an example:</p>
<div data-snippet-clipboard-copy-content="void doSomething() {doTheThing()} // over-decomposed."><pre><code>void doSomething() {doTheThing()} // over-decomposed.
</code></pre></div>
<p dir="auto">The strategy that I use for deciding how far to take decomposition is the old rule that a method should do "<em>One Thing</em>".  If I can <em>meaningfully</em> extract one method from another, then the original method did more than one thing.  "Meaningfully" means that the extracted functionality can be given a descriptive name; and that it does less than the original method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Unfortunately the One Thing approach will lead to over-decompositon:</p>
<ol dir="auto">
<li>
<p dir="auto">The term "one thing" is vague and easy to abuse. For example, if a method has two lines of code, isn't it doing two things?</p>
</li>
<li>
<p dir="auto">You haven't provided any useful guardrails to prevent over-decomposition. The example you gave is too extreme to be useful, and the "can it be named" qualification doesn't help: anything can be named.</p>
</li>
<li>
<p dir="auto">The One Thing approach is simply wrong in many cases. If two things are closely related, it might well make sense to implement them in a single method. For example, any thread-safe method will first have to acquire a lock, then carry out its function. These are two "things", but they belong in the same method.</p>
</li>
</ol>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let me tackle the last thing first.  You suggested that locking the thread, and preforming a critical section should be together in the same method.  However, I would be tempted to separate the locking from the critical section.</p>
<div data-snippet-clipboard-copy-content="void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}"><pre><code>void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}
</code></pre></div>
<p dir="auto">This decouples the critical section from the lock and allows it to be called at times when locking isn't necessary (e.g. in single thread mode) or when the a lock has already been set by someone else.</p>
<p dir="auto">Now, on to the "ease of abuse" argument.  I don't consider that to be a significant concern. <code>If</code> statements are easy to abuse.  <code>Switch</code> statements are easy to abuse.  Assignment statements are easy to abuse.  The fact that something is easy to abuse does not mean that it should be avoided or suppressed.  It simply means people should take appropriate care. There will always be this thing called: <em>judgment</em>.</p>
<p dir="auto">So when faced with this snippet of code in a larger method:</p>
<div data-snippet-clipboard-copy-content="...
amountOwed=0;
totalPoints=0;
..."><pre><code>...
amountOwed=0;
totalPoints=0;
...
</code></pre></div>
<p dir="auto">It would be poor judgement to extract them as follows, because the extraction is not meaningful.  The implementation is not more deeply detailed than the interface.</p>
<div data-snippet-clipboard-copy-content="void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}"><pre><code>void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}
</code></pre></div>
<p dir="auto">However it may be good judgement to extract them as follows because the interface is abstract, and the implemention has deeper detail.</p>
<div data-snippet-clipboard-copy-content="void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}"><pre><code>void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}
</code></pre></div>
<p dir="auto">The latter has a nice descriptive name that is abstract enough to be meaningful without being redundant.  And the two lines together are strongly related so as to qualify for doing <em>one thing</em>: initialization.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Of course anything can be abused. But the best approaches to design
encourage people to do things the right way and discourage abuse.
Unfortunately, the One Thing Rule encourages abuse for the reasons I
gave above.</p>
<p dir="auto">And of course software designers will need to use judgment: it isn't
possible to provide precise recipes for software design.
But good judgment requires principles and guidance. The
<em>Clean Code</em> arguments about decomposition, including the One Thing
Rule, are one-sided. They give strong, concrete, quantitative
advice about when to chop things up, with virtually no guidance for
how to tell you've gone too far. All I could find is a 2-sentence
example on page 36 about Listing 3-3 (which is pretty trivial),
buried in the middle of exhortations to "chop, chop, chop".</p>
<p dir="auto">One of the reasons I use the deep/shallow characterization is that it
captures both sides of the tradeoff; it will tell you when a decomposition
is good and also when decomposition makes things worse.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You make a good point that I don't talk much, in the book, about how to make the judgement call.  Back in 2008 my concern was breaking the habit of the very large functions that were common in those early days of the web.  I have been more balanced in the 2d ed.</p>
<p dir="auto">Still, if I must err, I'd rather err on the side of decomposition.  There is value in considering, and visualizing decompositions.  They can always be inlined if we judge them to have gone too far.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Coming back to your <code>clearTotals</code> example:</p>
<ul dir="auto">
<li>The <code>clearTotals</code> method seems to contradict the One Thing Rule: the
variables <code>amountOwed</code> and <code>totalPoints</code> don't seem particularly related, so
initializing them both is doing two things, no? You say that both
statements are performing initialization, which makes it just one thing
(initialization). Does that mean it would also be okay to have a single
method that initializes two completely independent objects with nothing in
common? I suspect not. It feels like you are struggling to create a clean
framework for applying the One Thing Rule; that makes me think it isn't
a good rule.</li>
<li>Without seeing more context I'm skeptical that the <code>clearTotals</code>
method makes sense.</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I hope you agree that between these two examples, the former is a bit better.</p>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<hr>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Well, actually, no. The second example is completely clear and obvious:
I don't see anything to be gained by splitting it up.</p>
<p dir="auto"><strong>SPOCK (a.k.a UB):</strong></p>
<p dir="auto">Fascinating.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think it will be easier to clarify our differences if we consider
a nontrivial code example. Let's look at the <code>PrimeGenerator</code> class from
<em>Clean Code</em>, which is Listing 10-8 on pages 145-146. This Java class
generates the first N prime numbers:</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList<Integer> multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList<Integer>();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex < primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple < candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList&lt;Integer&gt; multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList&lt;Integer&gt;();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex &lt; primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple &lt; candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}
</code></pre></div>
<p dir="auto">Before we dive into this code, I'd encourage everyone reading
this article to take time to read over the code and draw your own conclusions
about it. Did you find the code easy to understand? If so, why? If not, what
makes it complex?</p>
<p dir="auto">Also, Bob, can you confirm that you stand by this code (i.e. the code
properly exemplifies the design philosophy of <em>Clean Code</em> and this
is the way you believe the code should appear if it were used in
production)?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Ah, yes.  The <code>PrimeGenerator</code>.  This code comes from the 1982 paper on <a href="https://www.cs.tufts.edu/~nr/cs257/archive/literate-programming/01-knuth-lp.pdf" rel="nofollow"><em>Literate Programming</em></a> written by Donald Knuth.  The program was originally written in Pascal, and was automatically generated by Knuth's WEB system into a single very large method which I translated into Java.</p>
<p dir="auto">Of course this code was never meant for production.  Both Knuth and I used it as a pedagogical example.  In <em>Clean Code</em> it appears in a chapter named <em>Classes</em>.  The lesson of the chapter is that a very large method will often contain many different sections of code that are better decomposed into independent classes.</p>
<p dir="auto">In the chapter I extracted three classes from that function: <code>PrimePrinter</code>, <code>RowColumnPagePrinter</code> and <code>PrimeGenerator</code>.</p>
<p dir="auto">One of those extracted classes was the <code>PrimeGenerator</code>. It had the following code (which I did not publish in the book.)  The variable names and the overall structure are Knuth's.</p>
<div data-snippet-clipboard-copy-content="public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList<Integer> mult = new ArrayList<Integer>();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k < p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi < ord; mi++) {
          int m = mult.get(mi);
          while (m < j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}"><pre><code>public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList&lt;Integer&gt; mult = new ArrayList&lt;Integer&gt;();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k &lt; p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi &lt; ord; mi++) {
          int m = mult.get(mi);
          while (m &lt; j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}
</code></pre></div>
<p dir="auto">Even though I was done with the lesson of the chapter, I didn't want to leave that method looking so outdated.  So I cleaned it up a bit as an afterthought.  My goal was not to describe how to generate prime numbers.  I wanted my readers to see how large methods, that violate the Single Responsibility Principle, can be broken down into a few smaller well-named classes containing a few smaller well-named methods.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Thanks for the background. Even though the details of that code weren't
the main point of the chapter, presumably the code represents what you think
is the "right" and "cleanest" way to do things, given the algorithm at hand.
And that's where I disagree.</p>
<p dir="auto">There are many design problems with <code>PrimeGenerator</code>, but for now I'll
focus on method length. The code is chopped up so much (8 teeny-tiny methods)
that it's difficult to read. For starters, consider the
<code>isNotMultipleOfAnyPreviousPrimeFactor</code> method. This method invokes
<code>isMultipleOfNthPrimeFactor</code>, which invokes
<code>smallestOddNthMultipleNotLessThanCandidate</code>. These methods are shallow
and entangled:
in order to understand
<code>isNot...</code> you have to read the other two
methods and load all of that code into your mind at once. For example,
<code>isNot...</code> has side effects (it modifies <code>multiplesOfPrimeFactors</code>) but
you can't see that unless you read all three methods.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you have a point.  Eighteen years ago, when I was in the throes of this refactoring, the names and structure made perfect sense to me.  They make sense to me now, too -- but that's because I once again understand the algorithm.  When I returned to the algorithm for the first time a few days ago, I  struggled with the names and structure.  Once I understood the algorithm the names and structure made perfect sense.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Those names are problematic even for someone who understands the algorithm;
we'll talk about them a bit later, when discussing comments. And, if code
no longer makes sense to the writer when the writer returns to the code later,
that means the code is problematic. The fact that code can eventually
be understood (with great pain and suffering) does not excuse its entanglement.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Would that we had such a crystal ball that we could help our future selves avoid such "<em>great pain and suffering</em>".  ;-)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There is no need for a crystal ball. The problems with <code>PrimeGenerator</code> are
pretty obvious, such as the entanglement and interface complexity; maybe you
were surprised that it is hard to understand, but I am not. Said another
way, if you are unable to predict whether your code will be easy to
understand, there are problems with your design methodology.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  I will say, however, that I had equal "<em>pain and suffering</em>" interpreting your rewrite (below).  So, apparently, neither of our methodologies were sufficient to rescue our readers from such struggles.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Going back to my introductory remarks about complexity, splitting up
<code>isNot...</code> into three methods doesn't reduce the amount of information
you have to keep in your mind. It just spreads it out, so it isn't as
obvious that you need to read all three methods together. And, it's harder
to see the overall structure of the code because it's split up: readers have
to flip back and forth between the methods, effectively reconstructing a
monolithic version in their minds. Because the pieces are all related,
this code will be easiest to understand if it's all together in one place.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I disagree.  Here is <code>isNotMultipleOfAnyPreviousPrimeFactor</code>.</p>
<div data-snippet-clipboard-copy-content="  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }"><pre><code>  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto">If you trust the <code>isMultipleOfNthPrimeFactor</code> method, then this method stands alone quite nicely.  I mean we loop through all n previous primes and see if the candidate is a multiple.  That's pretty straight forward.</p>
<p dir="auto">Now it would be fair to ask the question how we determine whether the candidate is a multiple, and in that case you'd want to inspect the <code>isMultiple...</code> method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This code does appear to be simple and obvious.
Unfortunately, this appearance is deceiving.
If a reader trusts the name <code>isMultipleOfNthPrimeFactor</code> (which suggests
a predicate with no side effects) and doesn't bother to read its code, they
will not realize that it has side effects, and that the side effects
create a constraint on the <code>candidate</code> argument to <code>isNot...</code>
(it must be monotonically non-decreasing from invocation
to invocation). To understand these behaviors, you have to
read both <code>isMultiple...</code> and <code>smallestOdd...</code>. The current decomposition
hides this important information from the reader.</p>
<p dir="auto">If there is one thing more likely to result in bugs than not understanding code,
it's thinking you understand it when you don't.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That's a valid concern.  However, it is tempered by the fact that the functions are presented in the order they are called.  Thus we can expect that the reader has already seen the main loop and understands that <code>candidate</code> increases by two each iteration.</p>
<p dir="auto">The side effect buried down in <code>smallestOddNth...</code> is a bit more problematic. Now that you've pointed it out I don't like it much.  Still, that side effect should not confound the basic understanding of <code>isNot...</code>.</p>
<p dir="auto">In general, if you trust the names of the methods being called then understanding the caller does not require understanding the callee.  For example:</p>
<div data-snippet-clipboard-copy-content="for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();"><pre><code>for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();
</code></pre></div>
<p dir="auto">This would not be made more understandable if we replaced those two method calls with the their implementations.  Such a replacement would simply obscure the intent.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This example works because the called methods are relatively independent of
the parent. Unfortunately that is not the case for <code>isNot...</code>.</p>
<p dir="auto">In fact, <code>isNot...</code> is not only entangled with the methods it calls, it's also
entangled with its callers. <code>isNot...</code> only works if it is invoked in
a loop where <code>candidate</code> increases monotonically. To convince yourself
that it works, you have to find the code that invokes <code>isNot...</code> and
make sure that <code>candidate</code> never decreases from one call to the next.
Separating <code>isNot...</code> from the loop that invokes it makes it harder
for readers to convince themselves that it works.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Which, as I said before, is why the methods are ordered the way they are.  I expect that by the time you get to <code>isNot...</code> you've already read <code>checkOddNumbersForSubsequentPrimes</code> and know that <code>candidate</code> increases by twos.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's discuss this briefly, because it's another area where I
disagree with <em>Clean Code</em>. If methods are entangled, there is no
clever ordering of the method definitions that will fix the problem.</p>
<p dir="auto">In this particular situation two other methods intervene between the
loop in <code>checkOdd...</code> and <code>isNot...</code>, so readers will have forgotten
the loop context before they get to <code>isNot...</code>. Furthermore, the actual
code that creates a dependency on the loop isn't in <code>isNot...</code>: it's in
<code>smallestOdd...</code>, which is even farther away from <code>checkOdd...</code>.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I sincerely doubt anyone is going to forget that <code>candidate</code> is being increased by twos.  It's a pretty obvious way to avoid waste.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In my opening remarks I talked about how it's important to reduce the
amount of information people have to keep in their minds at once.
In this situation, readers have to remember that loop while they read
four intervening methods that are mostly unrelated to the loop. You apparently think
this will be easy and natural (I disagree). But it's even worse than
that. There is no indication which parts of <code>checkOdd...</code> will be important
later on, so the only safe approach is to remember <em>everything</em>, from <em>every</em>
method, until you have encountered every other method that could possibly
descend from it. And, to make the connection between the pieces, readers
must also reconstruct the call graph to notice that, even through
4 layers of method call, the code in <code>smallestOdd...</code> places constraints
on the loop in <code>checkOdd...</code>. This is an unreasonable cognitive burden to
place on readers.</p>
<p dir="auto">If two pieces of code are tightly related, the solution is to bring
them together. Separating the pieces, even in physically adjacent methods,
makes the code harder to understand.</p>
<p dir="auto">To me, all of the methods in <code>PrimeGenerator</code> are entangled: in order to
understand the class I had to load all of them into my mind
at once. I was constantly flipping back and forth between the methods
as I read the code. This is a red flag indicating
that the code has been over-decomposed.</p>
<p dir="auto">Bob, can you help me understand why you divided the code into such
tiny methods?
Is there some benefit to having so many methods that I have missed?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you and I are just going to disagree on this.  In general I believe in the principle of small well-named methods and the separation of concerns. Generally speaking if you can break a large method into several well-named smaller methods with different concerns, and by doing so expose their interfaces, and the high level functional decomposition, then that's a good thing.</p>
<ul dir="auto">
<li>Looping over the odd numbers is one concern.</li>
<li>Determining primality is another.</li>
<li>Marking off the multiples of primes is yet another.</li>
</ul>
<p dir="auto">It seems to me that separating and naming those concerns helps to expose the way the algorithm works -- even at the expense of some entaglement.</p>
<p dir="auto">In your solution, which we are soon to see below, you break the algorithm up in a similar way.  However, instead of separating the concerns into functions, you separate them into sections with comments above them.</p>
<p dir="auto">You mentioned that in my solution readers will have to keep the loop context in mind while reading the other functions.  I suggest that in your solution, readers will have to keep the loop context in mind while reading your explanatory comments.  They may have to "flip back and forth" between the sections in order to establish their understanding.</p>
<p dir="auto">Now perhaps you are concerned that in my solution the "flipping" is a longer distance (in lines) than in yours.  I'm not sure that's a significant point since they all fit on the same screen (at least they do on my screen) and the landmarks are pretty obvious.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Method Length Summary</h3><a id="user-content-method-length-summary" aria-label="Permalink: Method Length Summary" href="#method-length-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like it's time to wrap up this section. Is this a reasonable
summary of where we agree and disagree?</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that modular design is a good thing.</p>
</li>
<li>
<p dir="auto">We agree that it is possible to over-decompose, and that <em>Clean Code 1st ed.</em>
doesn't provide much guidance on how to recognize over-decomposition.</p>
</li>
<li>
<p dir="auto">We disagree on how far to decompose: you recommend decomposing
code into much smaller units than I do. You believe that
the additional decomposition you recommend makes code easier to
understand; I believe that it goes too far and actually makes code
more difficult to understand.</p>
</li>
<li>
<p dir="auto">You believe that the One Thing Rule, applied with judgment, will
lead to appropriate decompositions. I believe it lacks guardrails
and will lead to over-decomposition.</p>
</li>
<li>
<p dir="auto">We agree that the internal decomposition of <code>PrimeGenerator</code> into
methods is problematic. You point out that your main goal in writing
<code>PrimeGenerator</code> was to show how to decompose into classes, not
so much how to decompose a class internally into methods.</p>
</li>
<li>
<p dir="auto">Entanglement between methods in a class doesn't bother you
as much as it bothers me. You believe that the benefits of decomposing
methods can compensate for problems caused by entanglement.
I believe they can't: when decomposed methods are entangled,
they are harder to read than if they were not decomposed, and this
defeats the whole purpose of decomposition.</p>
</li>
<li>
<p dir="auto">You believe that ordering the methods in a class can help to
compensate for entanglement between methods; I don't.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think this is a fair assessment of our agreements and disagreements.  We both value decomposition,
and we both avoid entanglement; but we disagree on the relative weighting of those two values.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comments</h2><a id="user-content-comments" aria-label="Permalink: Comments" href="#comments"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to the second area of disagreement: comments. In my opinion,
the <em>Clean Code</em> approach to commenting results in code with
inadequate documentation, which increases the cost of software development.
I'm sure you disagree, so let's discuss.</p>
<p dir="auto">Here is what <em>Clean Code</em> says about comments (page 54):</p>
<blockquote>
<p dir="auto">The proper use of comments is to compensate for our failure to express
ourselves in code. Note that I use the word failure. I meant it.
Comments are always failures. We must have them because we cannot always
figure out how to express ourselves without them, but their use is not
a cause for celebration... Every time you write a comment, you should
grimace and feel the failure of your ability of expression.</p>
</blockquote>
<p dir="auto">I have to be honest: I was horrified when I first read this text, and it
still makes me cringe. This stigmatizes writing comments. Junior developers
will think "if I write comments, people may think I've failed, so the
safest thing is to write no comments."</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That chapter begins with these words:</p>
<blockquote>
<p dir="auto"><em>Nothing can be quite so helpful as a well placed comment.</em></p>
</blockquote>
<p dir="auto">It goes on to say that comments are a <em>necessary</em> evil.</p>
<p dir="auto">The only way a reader could infer that they should write no comments is if they hadn't actually read the chapter.  The chapter walks through a series of comments, some bad, some good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto"><em>Clean Code</em> focuses a lot more on the "evil" aspects of comments than the
"necessary" aspects. The sentence you quoted above is followed by two
sentences criticizing comments. Chapter 4 spends 4 pages talking about good
comments, followed by 15 pages talking about bad comments. There are snubs
like "the only truly good comment is the comment you found a way
not to write". And "Comments are always failures" is so catchy
that it's the one thing readers are most likely to remember from the
chapter.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">The difference in page count is because there are just a few ways to write good comments, and so many more ways to write bad ones.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree; this illustrates your bias against comments. If you look at
Chapter 13 of APOSD, it finds a lot more
constructive ways to use comments than <em>Clean Code</em>. And if you compare
the tone of Chapter 13 of APOSD with Chapter 4 of <em>Clean Code</em>, the hostility
of <em>Clean Code</em> towards comments becomes pretty clear.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll leave you to balance that last comment with the initial statement, and the final example, in the <em>Comments</em> chapter. They do not communicate "hostility".</p>
<p dir="auto">I'm not hostile to comments in general.  I <em>am</em> very hostile to gratuitous comments.</p>
<p dir="auto">You and I likely both survived through a time when comments were absolutely necessary.  In the '70s and '80s I was an assembly language programmer.  I also wrote a bit of FORTRAN. Programs in those languages that had no comments were impenetrable.</p>
<p dir="auto">As a result it became conventional wisdom to write comments by default.  And, indeed, computer science students were taught to write comments uncritically.  Comments became <em>pure good</em>.</p>
<p dir="auto">In <em>Clean Code</em> I decided to fight that mindset.  Comments can be <em>really bad</em> as well as good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that comments are less necessary today than they were
40 years ago.</p>
<p dir="auto">Comments are crucially important and add enormous value to software.
The problem is that there is a lot of important information that simply
cannot be expressed in code. By adding comments to fill in this missing
information, developers can make code dramatically easier to read.
This is not a "failure of their ability to express themselves", as you
put it.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's very true that there is important information that is not, or cannot be, expresssed in code.  That's a failure.  A failure of our languages, or of our ability to use them to express ourselves.  In every case a comment is a failure of our ability to use our languages to express our intent.</p>
<p dir="auto">And we fail at that very frequently, and so comments are a necessary evil -- or, if you prefer, <em>an unfortunate necessity</em>.  If we had the perfect programming language (TM) we would never write another comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that a perfect programming language would
eliminate the need for comments. Comments and code serve very different
purposes, so it's not obvious to me that we should use the same
language for both. In my experience, English works quite well
as a language for comments.
Why do you feel that information about a program should
be expressed entirely in code, rather than using a combination of code
and English?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I bemoan the fact that we must sometimes use a human language instead of a programming language.  Human languages are imprecise and full of ambiguities. Using a human language to describe something as precise as a program is very hard, and fraught with many opportunities for error and inadvertent misinformation.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that English isn't always as precise as code, but it can still be
used in precise ways and comments typically don't need the same
degree of precision as code.
Comments often contain qualitative information such
as <em>why</em> something is being done, or the overall idea of something.
English works better for these than code because it is a more
expressive language.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I have no argument with that statement.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Are you concerned that comments will be incorrect or
misleading and that this will slow down software development?
I often hear people complain about stale comments (usually as an excuse
for writing no comments at all) but
I have not found them be a significant problem
over my career. Incorrect comments do happen, but I don't encounter them
very often and when I do, they rarely cost me much time. In contrast, I waste
<em>enormous</em> amounts of time because of inadequate documentation; it's not
unusual for me to spend 50-80% of my development time wading through
code to figure out things that would be obvious if the code was properly
commented.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You and I have had some very different experiences.</p>
<p dir="auto">I have certainly been helped by well placed comments.  I have also, just as certainly, (and within this very document) been distracted and confused by a comment that was incorrect, misplaced, gratuitous, or otherwise just plain bad.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I invite everyone reading this article to ask yourself the following questions:</p>
<ul dir="auto">
<li>How much does your software development speed suffer because of
incorrect comments?</li>
<li>How much does your software development speed suffer because of
missing comments?</li>
</ul>
<p dir="auto">For me the cost of missing comments is easily 10-100x the cost of incorrect
comments. That is why I cringe when I see things in <em>Clean Code</em> that
discourage people from writing comments.</p>
<p dir="auto">Let's consider the <code>PrimeGenerator</code> class. There is not a single comment
in that code; does this seem appropriate to you?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it was appropriate for the purpose for which I wrote it. It was an adjunct to the lesson that very large methods can be broken down into smaller classes containing smaller methods. Adding lots of explanatory comments would have detracted from that point.</p>
<p dir="auto">In general, however, the commenting style I used in Listing 4-8 is more appropriate.  That listing, at the very end of the <em>Comments</em> chapter, describes yet another <code>PrimeGenertor</code> with a slightly different algorithm, and a better set of comments.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree that adding comments would have distracted from your point,
and I think Listing 4-8 is also woefully undercommented.
But let's not argue about either of those issues. Instead, let's discuss
what comments the PrimeGenerator code <em>should</em> have if it were used in production.
I will make some suggestions and you can agree or disagree.</p>
<p dir="auto">For starters, let's discuss your use of megasyllabic names like
<code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.  My understanding is that
you advocate using names like this instead of using shorter names
augmented with descriptive comments: you're effectively moving the
comments into code. To me, this approach is problematic:</p>
<ul dir="auto">
<li>Long names are awkward. Developers effectively have to retype
the documentation for a method every time they invoke it, and the long
names waste horizontal space and trigger line wraps in the code. The names are
also awkward to read: my mind wants to parse every syllable every time
I read it, which slows me down. Notice that both you and I resorted to
abbreviating names in this discussion: that's an indication that
the long names are awkward and unhepful.</li>
<li>The names are hard to parse and don't convey information as effectively
as a comment.
When students read <code>PrimeGenerator</code> one of the first things they
complain about is the long names (students can't make sense of them).
For example, the name above is
vague and cryptic: what does "least relevant" mean, and what is a
"larger prime factor"? Even with a complete understanding of the code in
the method, it's hard for me to make sense of the name.  If this name
is going to eliminate the need for a comment, it needs to be even longer.</li>
</ul>
<p dir="auto">In my opinion, the traditional approach of using shorter names with
descriptive comments is more convenient and conveys the required information
more effectively. What advantage is there in the approach you advocate?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">"<em>Megasyllabic</em>": Great word!</p>
<p dir="auto">I like my method names to be sentence fragments that fit nicely with keywords and assignment statements.  It makes the code a bit more natural to read.</p>
<div data-snippet-clipboard-copy-content="if (isTooHot)
  cooler.turnOn();"><pre><code>if (isTooHot)
  cooler.turnOn();
</code></pre></div>
<p dir="auto">I also follow a simple rule about the length of names.  The larger the scope of a method, the shorter its name should be and vice-versa -- the shorter the scope the longer the  name.  The private methods I extracted in this case live in very small scopes, and so have longish names.  Methods like this are typically called from only one place, so there is no burden on the programmer to remember a long name for another call.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Names like <code>isTooHot</code> are totally fine by me.
My concern is about names like <code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.</p>
<p dir="auto">It's interesting that as methods get smaller and narrower, you recommend
longer names.
What this says to me is that the interfaces for those functions are
more complex, so it takes more words to describe them. This provides
supporting evidence for
my assertion a while back that the more you split up a method,
the shallower the resulting methods will be.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's not the functions that get smaller, it's the scope that gets smaller.  A private function has a smaller scope than the public function that calls it.  A function called by that private function has an even smaller scope.  As we descend in scope, we also descend in situational detail.  Describing such detail often requires a long name, or a long comment.  I prefer to use a name.</p>
<p dir="auto">As for long names being hard to parse, that's a matter of practice.  Code is full of things that take practice to get used to.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't accept this. Code may be full of things that take practice to get used
to, but that doesn't excuse it.
Approaches that require more practice are worse than
those that require less.
If it's going to take a lot of work to get comfortable with the long names
then there had better be some compensating benefit; so far I'm not seeing any.
And I don't see any reason to believe that practice will make those names
easier to digest.</p>
<p dir="auto">In addition, your comment above violates one of my fundamental rules, which
is "complexity is in the eye of the reader". If you write code that someone
else thinks is complicated, then you must accept that the code is probably
complicated (unless you think the reader is completely incompetent). It
is not OK to make excuses or suggest that it is really the reader's problem
("you just don't have enough practice"). I'm going to have to live by this
same rule a bit later in our discussion.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  As for the meaning of "leastRelevant", that's a much larger problem that you and I will encounter shortly.  It has to do with the intimacy that the author has with the solution, and the reader's lack of that intimacy.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You still haven't answererd my question: why is it better to use super-long names
rather than shorter names augmented with descriptive comments?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's a matter of preference for me.  I prefer long names to comments.  I don't trust comments to be maintained, nor do I trust that they will be read.  Have you ever noticed that many IDEs paint comments in light grey so that they can be easily ignored?  It's harder to ignore a name than a comment.</p>
<p dir="auto">(BTW, I have my IDE paint comments in bright fire-engine red)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't see why a monster name is more likely to be "maintained" than
a comment, and I don't agree that IDEs encourage people to ignore
comments (this is your bias coming out again). My current IDE (VSCode)
doesn't use a lighter color for comments.
My previous one (NetBeans) did, but the color scheme didn't hide the comments; it
distinguished them from the code in a way that made both code and comments
easier to read.</p>
<p dir="auto">Now that we've discussed the specific issue of comments vs. long method
names, let's talk about comments in general. I think there are two major reasons
why comments are needed. The first reason for comments is abstraction.
Simply put, without comments there is no way to have abstraction or modularity.</p>
<p dir="auto">Abstraction is one of the most important components of good software design.
I define an abstraction as "a simplified way of thinking about something
that omits unimportant details." The most obvious example of an abstraction
is a method. It should be possible to use a method without reading its code.
The way we achieve this is by writing a header comment that describes
the method's <em>interface</em> (all the information someone needs in order
to invoke the method). If the method is well designed, the interface will be
much simpler than the code of the method (it omits implementation details),
so the comments reduce the amount of information people must have in
their heads.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Long ago, in a 1995 book, I defined abstraction as:</p>
<blockquote>
<p dir="auto"><em>The amplification of the essential and the elimination of the irrelevant.</em></p>
</blockquote>
<p dir="auto">I certainly agree that abstraction is of importance to good software design.  I also agree that well placed comments can enhance the ability of readers to understand the abstractions we are attempting to employ.  I disagree that comments are the <em>only</em>, or even the <em>best</em>, way to understand those abstractions.  But sometimes they are the only option.</p>
<p dir="auto">But consider:</p>
<div data-snippet-clipboard-copy-content="addSongToLibrary(String title, String[] authors, int durationInSeconds);"><pre><code>addSongToLibrary(String title, String[] authors, int durationInSeconds);
</code></pre></div>
<p dir="auto">This seems like a very nice abstraction to me, and I cannot imagine how a comment might improve it.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our definitions of abstraction are very similar; that's good to see.
However, the <code>addSongToLibrary</code> declaration is not (yet) a good abstraction
because it omits information
that is essential. In order to use <code>addSongToLibrary</code>, developers
need answers to the following questions:</p>
<ul dir="auto">
<li>Is there any expected format for an author string, such as "LastName, FirstName"?</li>
<li>Are the authors expected to be in alphabetical order? If not, is the order
significant in some other way?</li>
<li>What happens if there is already a song in the library with the given title
but different authors? Is it replaced with the new one, or will the library
keep multiple songs with the same title?</li>
<li>How is the library stored (e.g. is it entirely in memory? saved on disk?)?
If this information is documented somewhere else, such as the
overall class documentation, then it need not be repeated here.</li>
</ul>
<p dir="auto">Thus <code>addSongToLibrary</code> needs quite a few comments.
Sometimes the signature of a method (names and types of the method, its
arguments, and its return value) contains all the information
needed to use it, but this is pretty rare. Just skim through the documentation
for your favorite library package: in how many cases could you understand how
to use a method with only its signature?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Yes, there are times when the signature of a method is an incomplete abstraction and a comment
is required.  This is especially true when the interface is part of a public API, or an API intended
for use by a separate team of developers.  Within a single development team, however, long descriptive
comments on interfaces are often more of an impediment than a help.  The team has intimate knowledge of the
internals of the system, and will generally be able to understand an interface simply from its
signature.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In one of our in-person discussions you argued that interface comments
are unnecessary because when a group of developers is working on a body
of code they can collectively keep the entire code "loaded" in their
minds, so comments are unnecessary: if you have a question, just ask the
person who is familiar with that code. This creates a huge cognitive load
to keep all that code mentally loaded, and it's hard for me to imagine
that it would actually work. Maybe your memory is better than mine, but I
find that I quickly forget code that I wrote just a few weeks ago. In
a project of any size, I think your approach would result in developers
spending large amounts of time reading code to re-derive the interfaces,
and probably making mistakes along the way. Spending a few minutes to
document the interfaces would save time, reduce cognitive load, and
reduce bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think that certain interfaces need comments, even if they are private to the team.  But I think it is more often the case that the team is familiar enough with the system that well named methods and arguments are sufficient.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's consider a specific example from <code>PrimeGenerator</code>: the <code>isMultipleOfNthPrimeFactor</code>
method. When someone reading the code encounters the call to <code>isMultiple...</code>
in <code>isNot...</code> they need to understand enough about how <code>isMultiple...</code> works
in order to see how it fits into the code of <code>isNot...</code>.
The method name does not fully document the interface, so if there
is no header comment then readers will have to read the code of <code>isMultiple</code>.
This will force readers to load more information into their
heads, which makes it harder to work in the code.</p>
<p dir="auto">Here is my first attempt at a header comment for <code>isMultiple</code>:</p>
<div data-snippet-clipboard-copy-content="    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      <= multiplesOfPrimeFactors.size().
     */"><pre><code>    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      &lt;= multiplesOfPrimeFactors.size().
     */
</code></pre></div>
<p dir="auto">What do you think of this?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it's accurate.  I wouldn't delete it if I encountered it.  I don't think it should be a javadoc.</p>
<p dir="auto">The first sentence is redundant with the name <code>isMultipleOfNthPrimeFactor</code> and so could be deleted.  The warning of the side effect is useful.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that the first sentence is largely redundant with the name,
and I debated with myself about whether to keep it. I decided to keep it
because I think it is a bit more precise than the name; it's also easier
to read. You propose to eliminate the redundancy between the comment and
the method name by dropping the comment; I would eliminate the redundancy by
shortening the method name.</p>
<p dir="auto">By the way, you complained earlier about comments being less precise than
code, but in this case the comment is <em>more</em> precise (the method
name can't include text like <code>primes[n]</code>).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  There are times when precision is better expressed in a comment.</p>
<p dir="auto">Continuing with my critique of your comment above: The name <code>candidate</code> is synonymous with "Number being tested for primality".</p>
<p dir="auto">In the end, however, all the words in a comment are just going to have to sit in my brain
until I understand why they are there.  I'm also going to have to worry if
they are accurate.  So I'm going to have to read the code to understand and
validate the comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Whoah. That loud sound you just heard was my jaw hitting the floor.
Help me understand this a bit better: approximately what
fraction of comments that you encounter in practice are you willing to
trust without reading the code to verify them?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I look at every comment as potential misinformation.  At best they are a way to crosscheck the author's intent against the code. The amount of credence I give to a comment depends a lot on how easy they make that crosscheck.  When I read a comment that does not cause me to crosscheck, then I consider it to be of no value.  When I see a comment that causes me to crosscheck, and when that crosscheck turns out to be valuable, then that's a really good comment.</p>
<p dir="auto">Another way to say this is that the best comments tell me something surprising and verifiable about the code.  The worst are those that waste my time telling me something obvious, or incorrect.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like your answer is 0%: you don't trust any comment unless it has
been verified against the code. This makes no sense to me. As I said above, the vast
majority of comments are correct. It's not hard to write comments; the students
in my software design class are doing this pretty well within a few weeks.
It's also not hard to keep comments up to date as code evolves. Your refusal
to trust comments is another sign of your irrational bias against comments.</p>
<p dir="auto">Refusing to trust comments incurs a very high cost. In order to understand
how to invoke a method, you will have to read all of the code of that method;
if the method invokes other methods, you will
also have to read them, and the methods they invoke, recursively. This is
an enormous amount of work in comparison to reading (and trusting) a
simple interface comment like the one I wrote above.</p>
<p dir="auto">If you choose not to write an interface comment for methods, then you
leave the interface of that method undefined. Even if someone reads the
code of the method, they won't be able to tell which parts of the
implementation are expected to remain the same and which parts may
change (there is no way to specify this "contract" in code). This will
result in misunderstanding and more bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Well, I guess I've just been burned more than you have.  I've gone down too many false comment induced rabbit holes, and wasted too much time on worthless word salads.</p>
<p dir="auto">Of course my trust in comments is not a binary thing.  I read them if they are there; but
I don't implicitly trust then.  The more gratuitous I feel the author was, or the less adept at english the author is, the less I trust the comments.</p>
<p dir="auto">As I said above, our IDEs tend to paint comments in an ignorable color.  I have my IDE paint comments in bright fire engine red because when I write a comment I intend for it to be read.</p>
<p dir="auto">By the same token I use long names as a subsitute for comments because I intend for those long names to be read; and it is very hard for a programmer to ignore names.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned earlier that there are two general reasons why comments are
needed. So far we've been discussing the first reason (abstraction).
The second general reason for comments is for important information
that is not obvious from the code. The algorithm in <code>PrimeGenerator</code>
is very non-obvious, so quite a few comments are needed to help readers
understand what is going on and why. Most of the algorithm's complexity
arises because it is designed to compute primes efficiently:</p>
<ul dir="auto">
<li>
<p dir="auto">The algorithm goes out of its way to avoid divisions, which were quite
expensive when Knuth wrote his original version (they aren't that expensive
nowadays).</p>
</li>
<li>
<p dir="auto">The first multiple for each new prime number is computed by squaring the
prime, rather than multiplying it by 3. This is mysterious: why is it safe
to skip the intervening odd multiples? Furthermore, it might seem that this
optimization only has a small impact on performance, but in fact it makes an
<em>enormous</em> difference (orders of magnitude). Using the square has the
side-effect that when
testing a candidate, only primes up to the square root of the
candidate are tested. If 3x were used as the initial multiple, primes
within a factor of 3 of the candidate would be tested; that's a <em>lot</em>
more tests.
This implication of using the square is so non-obvious that I only realized
it while preparing material for this discussion; it never occurred to me in
the many times I have discussed the code with students.</p>
</li>
</ul>
<p dir="auto">Neither of these issues is obvious from the code; without
comments, readers are left to figure them out on their own. The students
in my class are generally unable to figure out either of them in the
30 minutes I give them, but I think that comments would have
allowed them to understand in a few minutes. Going back to my
introductory remarks, this is an example where information is important,
so it needs to be made available.</p>
<p dir="auto">Do you agree that there should be comments to explain each of these
two issues?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree that the algorithm is subtle.  Setting the first prime multiple as the square of the prime was deeply mysterious at first.  I had to go on an hour long bike ride to understand it.</p>
<p dir="auto">Would a comment help?  Perhaps.  However, my guess is that no one who has been reading our conversation has been helped by it, because you and I are now too intimate with the solution.  You and I can talk about that solution using words that fit into that intimacy; but our readers likely do not yet enjoy that fit.</p>
<p dir="auto">One solution is to paint a picture -- being worth a thousand words.  Here's my attempt.</p>
<div data-snippet-clipboard-copy-content="                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"><pre><code>                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
</code></pre></div>
<p dir="auto">I expect that our readers will have to stare at this for some time, and also look at the code.  But then there will be a <em>click</em> in their brains and they'll say "Ohhh!  Yes!  I see it now!"</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I found this diagram very hard to understand.
It begs for supplemental English text to explain the ideas being
presented. Even the syntax is non-obvious: what does
<code>1111111111111111111111111</code> mean?</p>
<p dir="auto">Maybe we have a fundamental difference of philosophy here. I get the sense
that you are happy to give readers a few clues and leave it to them to put
the clues together. Perhaps you don't mind if people have to stare at something
for a while to figure it out? I don't agree with this approach: it results
in wasted time, misunderstandings, and bugs.
I think software should be totally <em>obvious</em>, where readers don't need to
be clever or "stare at this for some time" to figure things out.
Suffering followed by catharsis is great for Greek tragedies, but not
for reading code. Every question
a reader might have should be naturally answered, either in the code or
in comments. Key ideas and important conclusions should be stated explicitly,
not left for the reader to deduce. Ideally, even if a reader is in a hurry
and doesn't read the code very carefully, their first guesses about how
things work (and why) should be correct. To me, that's clean code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I don't disagree with your sentiment.  Good clean code should be as easy as possible to understand.  I want to give my readers as many clues as possible so that the code is intuitive to read.</p>
<p dir="auto">That's the goal.  As we are about to see, that can be a tough goal to achieve.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In that case, do you still stand by the "picture" you painted above? It doesn't
seem consistent with what you just said. And if you really wanted to give
your readers as many clues as possible, you'd include a lot more comments.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I stand by the picture as far as it's accuracy is concerned.  And I think it
makes a good crosscheck.  I have no illusions that it is easy to understand.</p>
<p dir="auto">This algorithm is challenging and will require work to comprehend.  I finally
understood it when I drew this picture in my mind while on that bike ride.  When I got home I drew it for real and presented it in hopes that it might help
someone willing to do the work to understand it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comments Summary</h3><a id="user-content-comments-summary" aria-label="Permalink: Comments Summary" href="#comments-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's wrap up this section of the discussion. Here is my summary of
where we agree and disgree.</p>
<ul dir="auto">
<li>
<p dir="auto">Our overall views of comments are fundamentally different. I see more
value in comments than you do, and I believe that they play a fundamental
and irreplaceable role in system design. You agree that there are places
where comments are necessary, but that comments don't always make it
easier to understand code, so you see far fewer places where comments are
needed.</p>
</li>
<li>
<p dir="auto">I would probably write 5-10x more lines of comments for a given piece of
code than you would.</p>
</li>
<li>
<p dir="auto">I believe that missing comments are a much greater cause of lost
productivity than erroneous or unhelpful comments;
you believe that comments are a net negative, as generally practiced:
bad comments cost more time than good comments save.</p>
</li>
<li>
<p dir="auto">You view it as problematic that comments are written in English
rather than a programming language. I don't see this as particularly
problematic and think that in many cases English works better.</p>
</li>
<li>
<p dir="auto">You recommend that developers should take information that I would
represent as comments and recast it into code if at all possible. One
example of this is super-long method names. I believe that super-long names
are awkward and hard to understand, and that it would be better to use
shorter names supplemented with comments.</p>
</li>
<li>
<p dir="auto">I believe that it is not possible to define interfaces and create
abstractions without a lot of comments. You agree for public APIs, but see little need to comment
interfaces that are internal to the team.</p>
</li>
<li>
<p dir="auto">You are unwilling to trust comments until you have read code to
verify them. I generally trust comments; by doing so, I don't need to read
as much code as you do. You think this exposes me to too much risk.</p>
</li>
<li>
<p dir="auto">We agree that implementation code only needs comments when the code is
nonobvious. Although neither of us argues for a large number of implementation
comments, I'm more likely to see value in them than you do.</p>
</li>
</ul>
<p dir="auto">Overall, we struggled to find areas of agreement on this topic.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair assessment of our individual positions; which I assume are based on our
different individual experiences.  Over the years I have found the vast majority
of comments, as generally practiced in the industry, to be unhelpful. You seem to have found more
help in the comments you have encountered.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">John's Rewrite of PrimeGenerator</h2><a id="user-content-johns-rewrite-of-primegenerator" aria-label="Permalink: John's Rewrite of PrimeGenerator" href="#johns-rewrite-of-primegenerator"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned that I ask the students in my software design class to rewrite
<code>PrimeGenerator</code> to fix all of its design problems. Here is my rewrite
(note: this was written before we began our discussion; given what I
have learned during the discussion, I would now change several of the
comments, but I have left this in its original form):</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound < n; candidate += 2) {
            if (candidate >= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i <= lastMultiple; i++) {
                while (multiples[i] < candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound &lt; n; candidate += 2) {
            if (candidate &gt;= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i &lt;= lastMultiple; i++) {
                while (multiples[i] &lt; candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}
</code></pre></div>
<p dir="auto">Everyone can read this and decide for themselves whether they think
it is easier to understand than the original. I'd like to mention a
couple of overall things:</p>
<ul dir="auto">
<li>There is only one method. I didn't subdivide it because I felt the method already divides naturally into pieces that are distinct and understandable. It didn't seem to me that pulling out methods would improve readability significantly. When students rewrite the code, they typically have 2 or 3 methods, and those are usually OK too.</li>
<li>There are a <em>lot</em> of comments. It's extremely rare for me to write code with this density of comments. Most methods I write have no comments in the body, just a header comment describing the interface. But this code is subtle and tricky, so it needs a lot of comments to make the subtleties clear to readers. The long length of some of the comments is a red flag indicating that I struggled to find a clear and simple explanation for the code. Even with all the additional explanatory material this version is a bit shorter than the original (65 lines vs. 70).</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I presume this is a complete rewrite.  My guess is that you worked to understand the algorithm from <em>Clean Code</em> and then wrote this from scratch.  If that's so, then fair enough.</p>
<p dir="auto">In <em>Clean Code</em> I <em>refactored</em> Knuth's algorithm in order to give it a little structure.  That's not the same as a complete rewrite.</p>
<p dir="auto">Having said that, your version is much better than either Knuth's or mine.</p>
<p dir="auto">I wrote that chapter 18 years ago, so it's been a long time since I saw and understood this algorithm.  When I first saw your challenge I thought: "Oh, I can figure out my own code!"  But, no.  I could see all the moving parts, but I could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">So then I looked at your code.  I had the same problem.  I could see all the moving parts, all with comments, but I still could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">Figuring that out required a lot of staring at the ceiling, closing my eyes, visualizing, and riding my bike.</p>
<p dir="auto">Among the problems I had were the comments you wrote.  Let's take them one at a time.</p>
<div data-snippet-clipboard-copy-content="    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {"><pre><code>    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">It seems to me that this would be better as:</p>
<div data-snippet-clipboard-copy-content="public static int[] generateNPrimeNumbers(int n) {"><pre><code>public static int[] generateNPrimeNumbers(int n) {
</code></pre></div>
<p dir="auto">or if you must:</p>
<div data-snippet-clipboard-copy-content="//Return the first n prime numbers
public static int[] generate(int n) {"><pre><code>//Return the first n prime numbers
public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">I'm not opposed to Javadocs as a rule; but I write them only when absolutely necessary. I also have an aversion for descriptions and <code>@param</code> statements that are perfectly obvious from the method signature.</p>
<p dir="auto">The next comment cost me a good 20 minutes of puzzling things out.</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">First of all I'm not sure why the "division" statement is necessary.  I'm old school so I expect that everyone knows to avoid division in inner loops if it can be avoided.  But maybe I'm wrong about that...</p>
<p dir="auto">Also, the <em>Sieve of Eratosthenes</em> does not do division, and is a lot easier to understand <em>and explain</em> than this algorithm.  So why this particular algorithm?  I think Knuth was trying to save <em>memory</em> -- and in 1982 saving memory was important.  This algorithm uses a lot less memory than the sieve.</p>
<p dir="auto">Then came the phrase: <code>Each entry here contains an odd multiple...</code>.  I looked at that, and then at the code, and I saw: <code>multiples[0] = 4;</code>.</p>
<p dir="auto">"That's not odd" I said to myself.  "So maybe he meant even."</p>
<p dir="auto">So then I looked down and saw: <code>multiples[i] += 2*primes[i];</code></p>
<p dir="auto">"That's adding an even number!" I said to myself.  "I'm pretty sure he meant to say 'even' instead of 'odd'."</p>
<p dir="auto">I hadn't yet worked out what the <code>multiples</code> array was.  So I thought it was perfectly reasonable that it would have even numbers in it, and that your comment was simply an understandable word transposition.  After all, there's no compiler for comments so they suffer from the kinds of mistakes that humans often make with words.</p>
<p dir="auto">It was only when I got to <code>multiples[primesFound] = candidate*candidate;</code> that I started to question things.  If the <code>candidate</code> is prime, shouldn't <code>prime*prime</code> be odd in every case beyond 2?  I had to do the math in my head to prove that.  (2n+1)(2n+1) = 4n^2+4n+1 ... Yeah, that's odd.</p>
<p dir="auto">OK, so the <code>multiples</code> array is full of odd multiples, except for the first element, since it will be muliples of 2.</p>
<p dir="auto">So perhaps that comment should be:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">Or perhaps we should change the name of the array to something like <code>primeMultiples</code> and drop the comment altogether.</p>
<p dir="auto">Moving on to the next comment:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">That doesn't make a lot of sense.  The code it's talking about is:</p>
<div data-snippet-clipboard-copy-content="for (int i = 1; i <= lastMultiple; i++) {
    while (multiples[i] < candidate) {"><pre><code>for (int i = 1; i &lt;= lastMultiple; i++) {
    while (multiples[i] &lt; candidate) {
</code></pre></div>
<p dir="auto">The <code>multiples</code> array, as we have now learned, is an array of <em>multiples</em> of prime numbers.  This loop is not testing the candidate against prime <em>factors</em>, it's testing it against the current prime <em>multiples</em>.</p>
<p dir="auto">Fortunately for me the third of fourth time I read this comment I realized that you really meant to use the word "multiples".  But the only way for me to know that was to understand the algorithm.  And when I understand the algorithm, why do I need the comment?</p>
<p dir="auto">That left me with one final question.  What the deuce was the reason behind:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate*candidate;"><pre><code>multiples[primesFound] = candidate*candidate;
</code></pre></div>
<p dir="auto">Why the square?  That makes no sense.  So I changed it to:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate;"><pre><code>multiples[primesFound] = candidate;
</code></pre></div>
<p dir="auto">And it worked just fine.  So this must be an optimization of some kind.</p>
<p dir="auto">Your comment to explain this is:</p>
<div data-snippet-clipboard-copy-content="// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime."><pre><code>// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime.
</code></pre></div>
<p dir="auto">The first few times I read this it made no sense to me at all.  It was just a jumble of numbers.</p>
<p dir="auto">I stared at the ceiling, and closed my eyes to visualize. I couldn't see it.  So I went on a long contemplative bike ride during which I realized that the prime multiples of 2 will at one point contain 2*3 and then 2*5.  So the <code>multiples</code> array will at some point contain multiples of primes <em>larger</em> than the prime they represent.  <em>And it clicked!</em></p>
<p dir="auto">Suddenly it all made sense. I realized that the <code>multiples</code> array was the equivalent of the array of booleans we use in the <em>Sieve of Eratosthenes</em> -- but with a really interesting twist.  If you were to do the sieve on a whiteboard, you <em>could</em> erase every number less than the candidate, and only cross out the numbers that were the next multiples of all the previous primes.</p>
<p dir="auto">That explanation makes perfect sense to me -- now, but I'd be willing to bet that those who are reading it are puzzling over it.  The idea is just hard to explain.</p>
<p dir="auto">Finally I went back to your comment and could see what you were saying.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">A Tale of Two Programmers</h3><a id="user-content-a-tale-of-two-programmers" aria-label="Permalink: A Tale of Two Programmers" href="#a-tale-of-two-programmers"></a></p>
<p dir="auto">The bottom line here is that you and I both fell into the same trap.  I refactored that old algorithm 18 years ago, and I thought all those method and variable names would make my intent clear -- <em>because I understood that algorithm</em>.</p>
<p dir="auto">You wrote that code awhile back and decorated it with comments that you thought would explain your intent -- <em>because you understood that algorithm</em>.</p>
<p dir="auto">But my names didn't help me 18 years later.  They didn't help you, or your students either.  And your comments didn't help me.</p>
<p dir="auto">We were inside the box trying to communicate to those who stood outside and could not see what we saw.</p>
<p dir="auto">The bottom line is that it is very difficult to explain something to someone who is not intimate with the details you are trying to explain. Often our explanations make sense only after the reader has worked out the details for themself.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There's a lot of stuff in your discussion above, but I think it all boils down
to one thing: you don't like the comments that I wrote. As I mentioned earlier,
complexity is in the eye of the reader: if you say that my comments were
confusing or didn't help you to understand the code, then I have to take that
seriously.</p>
<p dir="auto">At the same time, you have made it clear that you don't see much value in
comments in general. Your preference is to have essentially no
comments for this code (or any code). You argue above that there is simply nothing that
comments can do to make the code easier to understand; the only way to
understand the code is to read the code. That is a cop-out.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Sorry to interrupt you; but I think you are overstating my position.  I certainly never said that comments can never be helpful.  Sometimes, of course, they are.  What I said was that I only trust them if the code validates them.  Sometimes a comment will make that validation a lot easier.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You keep saying that you sometimes find use for comments, but the reality
is that "sometimes" almost never occurs in your code. We'll see this when
we look at your revision of my code.</p>
<p dir="auto">Now back to my point. In order to
write our various versions of the code, you and I had to accumulate a lot of
knowledge about the algorithm, such as why it's OK for the first multiple
of a prime to be its square. Unfortunately, not all of that knowledge can
be represented in the code. It is our professional responsibility to do
the best we can to convey
that knowledge in comments, so that readers do not
have to reconstruct it over and over. Even if the resulting comments are
imperfect, they will make the code easier to understand.</p>
<p dir="auto">If a situation like this occurred in real life I would work with
you and others to improve my comments. For example, I would ask you
questions to get a better sense of
why the "squared prime" comment didn't seem to help you:</p>
<ul dir="auto">
<li>Are there things in the comment that are misleading or confusing?</li>
<li>Is there some important piece of information you acquired on your
bike ride that suddenly made things clear?</li>
</ul>
<p dir="auto">I would also show the comment to a few other people to get their takes
on it. Then I would rework the comment to improve it.</p>
<p dir="auto">Given your fundamental disbelief in comments, I think it's likely that
you would still see no value in the comment, even after my reworking.
In this case I would show the comment to other people, particularly those
who have a more positive view of comments in general, and get
their input. As long as the comment is not misleading and at least a few
people found it helpful, I would retain it.</p>
<p dir="auto">Now let me me discuss two few specific comments that you objected to. The
first comment was the one for the <code>multiples</code> variable:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">There is a bug in this comment that you exposed (the first entry is not odd);
good catch! You then argued that most of the information in the comment
is unnecessary and proposed this as an alternative:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">You have left out too much useful information here. For example, I don't think
it is safe to assume that readers will figure out that the motivation is
avoiding divisions. It's always better to state these assumptions and
motivations clearly so that there will be no confusion. And I think it's
helpful for readers to know that these entries never decrease.
I would simply fix the bug, leaving all of the information intact:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">The second comment was this one, for the <code>for</code> loop:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">You objected to this comment because the code of the loop doesn't actually
test the candidate against the prime factor; it tests it against a multiple.
When I write implementation comments like this, my goal is not to restate
the code; comments like that don't usually provide much value. The goal here was
to say <em>what</em> the code is doing in a logical sense, not <em>how</em> it does it.
In that sense, the comment is correct.</p>
<p dir="auto">However, if a comment causes confusion in the reader, then it is not a
good comment. Thus I would rewrite this comment to make it clear that
it describes the abstract function of the code, not its
precise behavior:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates."><pre><code>// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates.
</code></pre></div>
<p dir="auto">To conclude, I agree with your assertion "it is very difficult to explain
something to someone who is not intimate with the details you are trying
to explain." And yet, it is our responsibility as programmers to do exactly
that.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'm glad we agree.  We also agree about getting others to review the code and make recommendations on the code <em>and</em> the comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bob's Rewrite of PrimeGenerator2</h2><a id="user-content-bobs-rewrite-of-primegenerator2" aria-label="Permalink: Bob's Rewrite of PrimeGenerator2" href="#bobs-rewrite-of-primegenerator2"></a></p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">When I saw your solution, and after I gained a good understanding of it.  I refactored it just a bit.  I loaded it into my IDE, wrote some simple tests, and extracted a few simple methods.</p>
<p dir="auto">I also got rid of that <em>awful</em> labeled <code>continue</code> statement.  And I added 3 to the primes list so that I could mark the first element as <em>irrelevant</em> and give it a value of -1.  (I think I was still reeling from the even/odd confusion.)</p>
<p dir="auto">I like this because the implementation of the <code>generateFirstNPrimes</code> method describes the moving parts in a way that hints at what is going on.  It's easy to read that implementation and get a glimpse of the mechanism.  I'm not at all sure that the comment helps.</p>
<p dir="auto">I think it is just the reality of this algorithm that the effort required to properly explain it, and the effort required for anyone else to read and understand that explanation is roughly equivalent to the effort needed to read the code and go on a bike ride.</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound < n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate >= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i <= lastRelevantMultiple; i++)
            while (primeMultiples[i] < candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i <= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}"><pre><code>package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound &lt; n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate &gt;= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            while (primeMultiples[i] &lt; candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This version is a considerable improvement over the version in <em>Clean Code</em>.
Reducing the number of methods made the code easier to read and resulted
in cleaner interfaces. If it were properly commented, I think this version
would be about as easy to read as my version (the additional methods you
created didn't particularly help, but they didn't hurt either). I suspect
that if we polled readers, some would like your version better and some
would prefer mine.</p>
<p dir="auto">Unfortunately, this revision of the code creates a serious performance
regression: I measured a factor of 3-4x slowdown compared to either
of the earlier revisions. The problem is that you changed the processing of a
particular candidate from a single loop to two loops (the <code>increaseEach...</code> and
<code>candidateIsNot...</code> methods). In the loop from earlier revisions, and in
the <code>candidateIsNot</code>
method, the loop aborts once the candidate is disqualified (and
most candidates are quickly eliminated). However,
<code>increaseEach...</code> must examine every entry in <code>primeMultiples</code>.
This results in 5-10x as many loop iterations and a 3-4x overall slowdown.</p>
<p dir="auto">Given that the whole reason for the current algorithm (and its complexity)
is to maximize performance, this slowdown is unacceptable. The two
methods must be combined.</p>
<p dir="auto">I think what happened here is that you were so focused on something
that isn't actually all that important (creating the tiniest possible methods)
that you dropped the ball on other issues that really are important.
We have now seen this twice. In the original version of <code>PrimeGenerator</code>
you were so determined to make tiny methods that you didn't notice that the
code was becoming incomprehensible. In this version you were so eager to
chop up my single method that you didn't notice that you were blowing up the
performance.</p>
<p dir="auto">I don't think this was just an unfortunate combination of oversights.
One of the most important things
in software design is to identify what is important and focus on that;
if you focus on things that are unimportant, you're likely to mess up the
things that are important.</p>
<p dir="auto">The code in your revision is still under-commented. You believe
that there is no meaningful way for comments to assist the reader in
understanding the code. I think this stems from your general disbelief in
the value of comments; you are quick to throw in the towel.
This algorithm is unusually difficult to explain,
but I still believe that comments can help. For example, I believe you
must make some attempt to help readers understand why the first multiple
for a prime is the square of the prime. You have taken a lot of time to
develop your understanding of this; surely there must be some way to convey
that understanding to others? If you had included that information in
your original version of the code you could have saved yourself that long
bike ride.
Giving up on this is an abdication of professional responsibility.</p>
<p dir="auto">The few comments that you included in your revision are of little value.
The first comment is too cryptic to provide much help: I can't
make any sense of the phrase "predicting the next composite number and
skipping over it" even though I completely understand the code it purports
to explain. One of the comments is just a joke; I was surprised to see
this, given your opposition to extraneous comments.</p>
<p dir="auto">Clearly you and I live in different universes when it comes to comments.</p>
<p dir="auto">Finally, I don't understand why you are offended by the labeled <code>continue</code>
statement in my code. This is a clean and elegant solution to the problem
of escaping from nested loops. I wish more languages
had this feature; the alternative is awkward code where you set a variable,
then exit one level of loop, then check the variable and exit the next
level.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Good catch!  I would have caught that too had I thought to profile the solution.  You are right that separating the two loops added some unecessary iteration.  I found a nice way to solve that problem without using the horrible <code>continue</code>.  My updated version is now faster than yours!  A million primes in 440ms as opposed to yours which takes 561ms.  ;-) Below are just the changes.</p>
<div data-snippet-clipboard-copy-content="  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound < n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate >= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i <= lastRelevantMultiple; i++) {
      while (primeMultiples[i] < candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }"><pre><code>  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound &lt; n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate &gt;= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i &lt;= lastRelevantMultiple; i++) {
      while (primeMultiples[i] &lt; candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Yep, that fixes the problem. I note that you are now down to 4 methods,
from 8 in the <em>Clean Code</em> version.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Test-Driven Development</h2><a id="user-content-test-driven-development" aria-label="Permalink: Test-Driven Development" href="#test-driven-development"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to our third area of disagreement, which is Test-Driven
Development. I am a huge fan of unit testing. I believe that unit tests are
an indispensable part of the software development process and pay for
themselves over and over. I think we agree on this.</p>
<p dir="auto">However, I am not fan of Test-Driven Development (TDD), which dictates
that tests must be written before code and that code must be written
and tested in tiny increments. This approach has serious problems
without any compensating advantages that I have been able to identify.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">As I said at the start I have carefully read <em>A Philosophy of Software Design</em>. I found it to be full of worthwhile insights, and I strongly agree with most of the points you make.</p>
<p dir="auto">So I was surprised to find, on page 157, that you wrote a very short, dismissive, pejorative, and inaccurate section on <em>Test Driven Development</em>.  Sorry for all the adjectives, but I think that's a fair characterization.  So my goal, here, is to correct the misconceptions that led you to write the following:</p>
<blockquote>
<p dir="auto">"Test-driven development is an approach to software development where programmers write unit tests before they write code.  When creating a new class, the develper first writes unit tests for the class, based on its expected behavior.  None of these tests pass, since there is no code for the class.  Then the developer works through the tests one at a time, writing enough code for that test to pass.  When all of the tests pass, the class is finished."</p>
</blockquote>
<p dir="auto">This is just wrong.  TDD is quite considerably different from what you describe.  I describe it using three laws.</p>
<ol dir="auto">
<li>
<p dir="auto">You are not allowed to write any production code until you have first written a unit test that fails because that code does not exist.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more of a unit test than is sufficient to fail, and failing to compile is failing.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more production code than is sufficient to make the currently failing test pass.</p>
</li>
</ol>
<p dir="auto">A little thought will convince you that these three laws will lock you into a cycle that is just a few seconds long.  You'll write a line or two of a test that will fail, you'll write a line or two of production code that will pass, around and around every few seconds.</p>
<p dir="auto">A second layer of TDD is the Red-Green-Refactor loop.  This loop is several minutes long.  It is comprised of a few cycles of the three laws, followed by a period of reflection and refactoring.  During that reflection we pull back from the intimacy of the quick cycle and look at the design of the code we've just written.  Is it clean?  Is it well structured?  Is there a better approach?  Does it match the design we are pursuing?  If not, should it?</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Oops! I plead "guilty as charged" to inaccurately describing TDD.
I will fix this in the next revision of APOSD. That said, your definition
of TDD does not change my concerns.</p>
<p dir="auto">Let's discuss the potential advantages and disadvantages
of TDD; then readers can decide for themselves whether they think TDD is a
good idea overall.</p>
<p dir="auto">Before we start that discussion, let me clarify the approach I prefer as an
alternative to TDD. In your online videos you describe the alternative to
TDD as one where a developer writes the code, gets it fully working
(presumably with manual tests), then goes back and writes the unit tests.
You argue that this approach would be terrible: developers
lose interest once they think code is working, so they wouldn't actually
write the tests. I agree with you completely. However, this isn't the only
alternative to TDD.</p>
<p dir="auto">The approach I prefer is one where the developer works in somewhat
larger units than in TDD, perhaps a few methods or a class. The developer
first writes some code (anwywhere from a few tens of lines to a few hundred
lines), then writes unit tests for that code. As with TDD, the
code isn't considered to be "working" until it has comprehensive unit
tests.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">How about if we call this technique "bundling" for purposes of this
document?  This is the term I use in <em>Clean Code 2d ed.</em></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Fine by me.</p>
<p dir="auto">The reason for working in larger units is to encourage design
thinking, so that a developer can think about a collection of related
tasks and do a bit of planning to come up with a good overall design
where the pieces fit together well.
Of course the initial design ideas will have flaws and refactoring
will still be necessary, but the goal is to center the development
process around design, not tests.</p>
<p dir="auto">To start our discussion, can you make a list of the advantages you
think that TDD provides over the approach I just described?</p>
<p dir="auto"><strong>UB:</strong>
The advantages I usually attribute to TDD are:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging.  After all, if you just saw everything working a minute or two ago, there's not much to debug.</p>
</li>
<li>
<p dir="auto">A stream of reliable low level documentation, in the form of very small and isolated unit tests.  Those tests describe the low level structure and operation of every facet of the system.  If you want to know how to do something in the system, there are tests that will show you how.</p>
</li>
<li>
<p dir="auto">A less coupled design which results from the fact that every small part of the system must be designed to be testable, and testability requires decoupling.</p>
</li>
<li>
<p dir="auto">A suite of tests that you trust with your life, and therefore supports fearless refactoring.</p>
</li>
</ul>
<p dir="auto">However, you asked me which of these advantages TDD might have over <em>your</em> preferred method.  That depends on how big you make those larger units you described.  The important thing to me is to keep the cycle time short, and to prevent entanglements that block testability.</p>
<p dir="auto">It seems to me that working in small units, and then immediately writing after the fact tests, can give you all the above advantages, so long as you are very careful to test every aspect of the code you just wrote.  I think a disciplined programmer could effectively work that way.  Indeed, I think such a programmer would produce code that I could not distinguish from code written by another programmer following TDD.</p>
<p dir="auto">Above you suggested that bundling is to encourage design.  I think encouraging design is a very good thing.  My question for you is: Why do you think that TDD does not encourage design?  My own experience is that design comes from strategic thought, which is independent of the tactical behavior of either TDD or Bundling.  Design is taking one step back from the code and envisioning structures that address a larger set of constraints and needs.</p>
<p dir="auto">Once you have that vision in your head it seems to me bundling and TDD will yield similar results.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, let me address the four advantages you listed for TDD:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging? I think any form of unit testing can
reduce debugging work, but not for the reason you
suggested. The benefit comes because unit tests expose bugs earlier
and in an environment where they are easier to track down. A
relatively simple bug to fix in development can be very painful to
track down in production. I'm not convinced by your argument that
there's less debugging because "you just saw everything working a
minute ago": it's easy to make a tiny change that exposes a really
gnarly bug that has existed for a long time but hasn't yet been
triggered. Hard-to-debug problems arise from the accumulated complexity
of the system, not from the size of the code increments.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> True.  However, when the cycles are very short then the cause
of even the gnarliest of bugs have the best chance of being tracked down.
The shorter the cycles, the better the chances.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> This is only true up to a point. I think you believe
that making units smaller and smaller continues to provide benefits,
with almost no limit to how small they can get. I think that there
is a point of diminishing returns, where making things even smaller
no longer helps and actually starts to hurt. We saw this disagreement
over method length, and I think we're seeing it again here.</p>
</blockquote>
</li>
<li>
<p dir="auto">Low level documentation? I disagree: unit tests are a poor form
of documentation. Comments are a much more
effective form of documentation, and you can put them right next to the
relevant code. Trying to learn a method's
interface by reading a bunch of unit tests seems much more difficult
than just reading a couple of sentences of English text.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Nowadays it's very easy to find the tests for
a function by using the "where-used" feature of the IDE.  As for comments
being better, if that were true then no one would publish example code.</p>
</blockquote>
</li>
<li>
<p dir="auto">A less coupled design? Possibly, but I haven't experienced this myself.
It's not clear to me that designing for testability will produce the
best design.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Generally the decoupling arises because the test requires a mock
of some kind.  Mocks tend to force abstractions that might otherwise not exist.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> In my experience, mocking virtually never changes interfaces;
it just provides replacements for existing (typically immovable)
interfaces.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>UB:</strong> Our experiences differ.</p>
</blockquote>
</li>
<li>
<p dir="auto">Enabling fearless refactoring? BINGO! This is the where almost all of the
benefits from unit testing come from, and it is a really really big deal.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Agreed.</p>
</blockquote>
</li>
</ul>
<p dir="auto">I agree with your conclusion that TDD and bundling are about the
same in terms of providing these benefits.</p>
<p dir="auto">Now let me explain why I think TDD is likely to result in bad designs.
The fundamental problem with TDD is that it forces developers to work
too tactically, in
units of development that are too small; it discourages design
thinking.  With TDD the basic unit of
development is one test: first the test is written, then the code to
make that test pass. However, the natural units for design are larger
than this: a class or method, for example. These units
correspond to multiple test cases. If a developer thinks only about
the next test, they are only considering part of a design problem at
any given time. It's hard to design something well if you don't think
about the whole design problem at once. TDD explicitly
prohibits developers from writing more code than is needed to pass
the current test; this discourages the kind of strategic thinking needed
for good design.</p>
<p dir="auto">TDD does not provide adequate guidance to encourage design. You mentioned
the Red-Green-Refactor loop, which recommends refactoring after each step,
but there's almost no guidance for refactoring. How should developers
decide when and what to refactor? This seems to be left purely to their
own judgment. For example, if I am writing a method that requires
multiple iterations of the TDD loop, should I refactor after every iteration
(which sounds pretty tedious) or wait until after several iterations so that
I can look at a bigger chunk of code when refactoring and hence be more
strategic? Without guidance it will be tempting for developers to keep
putting off refactoring.</p>
<p dir="auto">TDD is similar to the One Thing Rule we discsused earlier in that it is
biased: it provides very strong and clear instructions pushing developers
in one direction (in this case, acting tactically) with only vague
guidance in the other direction (designing more strategically). As a result,
developers are likely to err on the side of being too tactical.</p>
<p dir="auto">TDD guarantees that developers will initially write bad code. If you start
writing code without thinking about the whole design problem, the first code
you write will almost certainly be wrong. Design only
happens after a bunch of bad code has accumulated.
I watched your video on TDD, and
you repeatedly wrote the wrong code, then fixed it later. If the developer
refactors conscientiously (as you did) they can still end up with good
code, but this works against human nature. With TDD, that bad code will
actually work (there are tests to prove it!) and it's human nature not
to want to change something that
works. If the code I'm developing is nontrivial, I will probably have to
accumulate a lot of bad code with TDD before I have enough code in front
of me to understand what the design should have been.
It will be very difficult for me to force myself to throw away
all that work.</p>
<p dir="auto">It's easy for a developer to believe they are doing TDD correctly while
working entirely tactically, layering on hack after hack with an
occasional minor refactor, without ever thinking about the overall design.</p>
<p dir="auto">I believe that the bundling approach is superior to TDD because it focuses
the development process around design: design first, then code, then write
unit tests. Of course, refactoring will still be
required: it's almost never possible to get the design right the first time.
But starting with design will reduce the amount of bad code you write and
get you to a good design sooner. It is possible to produce equally good
designs with TDD; it's just harder and requires a lot more discipline.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll address your points one at a time.</p>
<ul dir="auto">
<li>
<p dir="auto">I haven't found that the scale of TDD is so tactical that it discourages thinking.  Every programmer, regardless of their testing discipline, writes code one line at a time.  That's immensely tactical and yet does not discourage design.  So why would one test at a time discourage it?</p>
</li>
<li>
<p dir="auto">The literature on TDD strongly discourages delaying refactoring.  While thinking about design is strongly encouraged.  Both are integral parts of the discipline..</p>
</li>
<li>
<p dir="auto">We all write bad code at the start.  The discipline of TDD gives us the opportunity, and the safety, to continuously clean it.  Design insights arise from those kinds of cleaning activities.  The discipline of refactoring allows bad designs to be transformed, one step at a time, into better designs.</p>
</li>
<li>
<p dir="auto">It's not clear to me why the act of writing tests late is a better design choice.  There's nothing in TDD that prevents me from thinking through a design long before I write the very first tested code.</p>
</li>
</ul>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You say there is nothing about TDD that stops developers from thinking ahead
about design. This is only partly true. Under TDD I can think ahead, but I
can't actually write my ideas down in the form of code, since that would
violate TDD Rule 1. This is a significant discouragement.</p>
<p dir="auto">You claim that "thinking about design is strongly encouraged" in TDD,
but I haven't seen this in your discussions of TDD. I watched your
video example of using TDD
for computing bowling scores, and design is never even mentioned after the
first minute or two (ironically, one of the conclusions of this
example is that the brief initial design turned out to be
useless). There is no suggestion of thinking ahead in the video;
it's all about cleaning up messes after the fact.
In all of the TDD materials you have shown me, I have not seen any
warnings about the dangers of becoming so tactical with TDD that
design never occurs (perhaps you don't even view this as a serious risk?).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I usually use an abbeviated form of UML to capture my early design decisions.  I have no objection to capturing them in pseudo-code, or even real code.  However, I would not commit any such pre-written code.  I would likely hold it in a text file, and consult it while following the TDD cycle.  I might feel safe enough to copy and paste from the text file into my IDE in order to make a failing test pass.</p>
<p dir="auto">The Bowling game is an example of how wildly our initial design decisions can  deviate from our eventual solutions.  It's true that introductory videos often do not expose the depth of a discipline.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">As I was watching your TDD video for the second time, you said something
that jumped out at me:</p>
<blockquote>
<p dir="auto">Humans consider things that come first to be important and things that
come at the end to be less important and somehow optional; that's
why they are at the end, so we can leave them out if we have to.</p>
</blockquote>
<p dir="auto">This captures perfectly my concern about TDD. TDD insists that tests must
come first, and design, if it happens at all, comes at the end, after
code is working. I believe that good design is the most important
thing, so it must be the top priority. I don't consider tests optional,
but delaying them is safer than delaying design. Writing tests isn't particularly
difficult; the most important thing is having the discipline to do it.
Getting a good design is really hard, even if you are very disciplined;
that's why it needs to be the center of attention.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">TDD is a coding discipline.  Of course design comes before coding -- I don't know anyone who thinks otherwise.  Even the Bowling Game video made that point. But, as we saw in the Bowling Game video, sometimes the code will take you in a very different direction.</p>
<p dir="auto">That difference does't imply that the design shouldn't have been done.  It just implies that designs are speculative and may not aways survive reality.</p>
<p dir="auto">As Eisenhower once said:</p>
<blockquote>
<p dir="auto">“In preparing for battle I have always found that plans are useless, but planning is indispensable.”</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask why writing tests later is a better design choice. It isn't.
The benefit of the bundled approach doesn't come from writing tests later;
it comes from doing design sooner. Writing tests (a bit) later is a
consequence of this choice. The tests are still written pretty early-on
with the bundled approach, so I don't think the delay causes significant
problems.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think we simply disagree that TDD discourages design.  The practice of TDD does not discourage me from design; because I value design.  I would suggest that those who do not value design will not design, no matter what discipline they practice.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You claim that the problems I worry about with TDD simply don't happen in
practice. Unfortunately I have heard contrary claims from senior
developers that I trust. They complain about horrible code produced by
TDD-based teams, and they believe that the problems were caused by TDD.
Of course horrible code can be produced with any design approach.
And maybe those teams didn't implement TDD properly, or maybe those
cases were outliers.
But the problems reported to me line up exactly with what I would
expect to happen, given the tactical nature of TDD.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My experience differs. I've worked on many projects where TDD has been used
effectively and profitably.  I'm sure the senior developers that you trust are telling you the truth about their experience.  Having never seen TDD lead to such bad outcomes myself, I sincerely doubt that the blame can be traced to TDD.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask me to trust your extensive experience with
TDD, and I admit that I have no personal experience with TDD.
On the other hand, I have a lot of experience with tactical programming,
and I know that it rarely ends well.
TDD is one of the most extreme forms of tactical programming I've
encountered.
In general, if "making it work" is the #1 prority, instead of
"develop a clean design", code turns to spaghetti.
I don't see enough safeguards in your approach to TDD
to prevent the disaster scenarios; I don't even see a clear
recognition of the risk.</p>
<p dir="auto">Overall, TDD is in a bad place on the risk-reward spectrum. In comparison
to the bundling approch, the downside risks for poor code quality in TDD
are huge, and I don't see enough upside reward (if any) to compensate.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">All I can say to that is that your opinion is based on a number of false impressions and speculations, and not upon direct experience.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Now let me ask you a couple of questions.</p>
<p dir="auto">First, at a microscopic level, why on earth does TDD prohibit developers
from writing more code than needed to pass the current test? How does
enforcing myopia make systems better?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
The goal of the discipline is to make sure that everything is tested.
One good way to do that is to refuse to write any code unless it is to make a failing test pass.  Also, working in such short cycles provides insights into
the way the code is working.  Those insights often lead to better design decisions.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I agree that seeing code (partially) working can provide insights. But
surely that benefit can be had without such a severe restriction on
how developers think?</p>
</blockquote>
<p dir="auto">Second, at a broader level, do you think TDD is likely to produce better
designs than approaches that are more design-centric, such as the bundling
approach I described? If so, can you explain why?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
My guess is that someone adept at bundling, and someone adept at TDD would produce very similar designs, with very similar test coverage.  I would also venture to guess that the TDDer would be somewhat more productive than the bundler if for no reason other than that the TDDer finds and fixes problems earlier than the bundler.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I think that the bundling approach will result in a better design because
it actually focuses on design, rather than focusing on tests and hoping
that a good design will magically emerge. I think it's really hard to argue
that the best way to achieve one thing is to focus your attention on
something else. And the bundling approach will
make progress faster because the early thinking about design will reduce the
amount of bad code you end up having to throw away under TDD. Overall, I'd
argue that the best-case outcomes for the two approaches will
be about the same, but average and (especially) worst-case outcomes will
be far worse for TDD.</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't think we're going to resolve our disagreements on TDD.
To do that, we'd need empirical data about the frequency of good and bad
outcomes from TDD. Unfortunately I'm not aware of any such data.
Thus, readers will have to decide for themselves whether the potential
benefits of TDD outweigh the risks.</p>
<p dir="auto">For anyone who chooses to use TDD, I urge you to do so with extreme
caution. Your primary goal must not be just working code, but rather a
clean design that will allow you to develop quickly in the future.
TDD will not lead you naturally to the best design, so you will need
to do significant and continuous refactoring to avoid spaghetti code.
Ask yourself repeatedly "suppose that I knew everything I know now when
I first started on this project; would I have chosen the current
structure for the code?" When the answer is no (which will happen
frequently) stop and refactor. Recognize that TDD will cause you to
write more bad code than you may be used to, so
you must be prepared to throw out and rewrite more than you are used to.
Take time to plan ahead and think about the overall design, rather than
just making the next test work.
If you do all of these things diligently, I think it is possible to
mitigate the risks of TDD and produce well-designed code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let's just say that I agree with all that advice, but disagree with your assertion that TDD might be the cause of bad code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TDD Summary</h3><a id="user-content-tdd-summary" aria-label="Permalink: TDD Summary" href="#tdd-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Here is my attempt to summarize our thoughts on Test-Driven Development:</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that unit tests are an essential element in software development.
They allow developers to make significant changes to a system without fear
of breaking something.</p>
</li>
<li>
<p dir="auto">We agree that it is possible use TDD to produce systems with good designs.</p>
</li>
<li>
<p dir="auto">I believe that TDD discourages good design and can easily lead to very bad
code. You do not believe that TDD discourages good
design and don't see much of a risk of bad code.</p>
</li>
<li>
<p dir="auto">I believe that there are better approaches than TDD for producing good
unit test suites, such as the "bundling" approach discussed above. You agree
that bundling can produce outcomes just as good as TDD but think it may lead to
somewhat less test coverage.</p>
</li>
<li>
<p dir="auto">I believe that TDD and bundling have similar best-case outcomes, but that
the average and worst-case outcomes will be much worse for TDD. You disagree
and believe that, if anything, TDD may produce marginally better outcomes
than bundling. You also think that preference and personality are larger factors in
making the choice between the two.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair summary of our discussion.  We seem to disagree over the best application
of discipline.  I prefer a disciplined approach to keep the code covered by tests
written first in very short cycles.  You prefer a disciplined approach of writing relatively longer
bundles of code and then writing tests for those bundles.  We disagree on the risks and rewards of
these two disciplines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Closing Remarks</h2><a id="user-content-closing-remarks" aria-label="Permalink: Closing Remarks" href="#closing-remarks"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, I'd like to thank you for tolerating (and responding to) the arguments
I have made about some of the key ideas in <em>Clean Code</em>. I hope this
discussion will provide food for thought for readers.</p>
<p dir="auto">We have covered a lot of topics and subtopics in this discussion, but
I think that most of my concerns result from two general errors made
by <em>Clean Code</em>: failure to focus on what is important, and failure to
balance design tradeoffs.</p>
<p dir="auto">In software design (and probably in any design environment) it is essential
to identify the things that really matter and focus on those. If you
focus your attention on things that are unimportant you are
unlikely to achieve the things that really are important.
Unfortunately, <em>Clean Code</em> repeatedly focuses on things that don't really
matter, such as:</p>
<ul dir="auto">
<li>Dividing ten-line methods into five-line methods and dividing five-line methods
into two- or three-line methods.</li>
<li>Eliminating the use of comments written in English.</li>
<li>Writing tests before code and making the basic unit of development a
test rather than an abstraction.</li>
</ul>
<p dir="auto">None of these provides significant value, and we have seen how they
distract from producing the best possible designs.</p>
<p dir="auto">Conversely, <em>Clean Code</em> fundamentally undervalues comments, which are
essential and irreplaceable. This
comes at a huge cost. Without interface comments the specifications for
interfaces are incomplete. This is guaranteed to result in confusion and bugs.
Without implementation comments, readers are forced to rederive knowledge
and intentions that were in the mind of the original developer. This wastes
time and leads to more bugs.</p>
<p dir="auto">In my opening remarks I said that systems become complex when important
information is not accessible and obvious to developers. By refusing to
write comments, you are hiding important information that you have and
that others need.</p>
<p dir="auto">The second general error in <em>Clean Code</em> has to do with balance. Design
represents a balance between competing concerns. Almost any design idea
becomes a bad thing if taken to the extreme. However, <em>Clean Code</em>
repeatedly gives very strong advice in one direction without correspondingly
strong advice in the other direction or any meaningful guidance about how
to recognize when you have gone too far. For example, making methods
shorter is often a good thing, but the <em>Clean Code</em> position is so one-sided
and extreme that readers are likely to chop things up too much. We saw
in the <code>PrimeGenerator</code> example how this resulted in code that was
nearly incomprehensible. Similarly, the <em>Clean Code</em> position on TDD is
one-sided, failing to
recognize any possible weakness and encouraging readers to take this to
a tactical extreme where design is completely squeezed out of the development
process.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">John, I'd like to thank you for participating in this project.  This was a lot of fun for me.  I love disagreement and debate with smart people.  I also think that we share far more values than separate us.</p>
<p dir="auto">For my part I'll just say that I have given due consideration to the points you've made, and while I disagree with your conclusions above, I have integrated several of your better ideas, as well as this entire document, into the second edition of <em>Clean Code</em>.</p>
<p dir="auto">Thanks again, and give my best to your students.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["The closer to the train station, the worse the kebab" – a "study" (507 pts)]]></title>
            <link>https://www.jmspae.se/write-ups/kebabs-train-stations/</link>
            <guid>43165112</guid>
            <pubDate>Mon, 24 Feb 2025 21:25:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jmspae.se/write-ups/kebabs-train-stations/">https://www.jmspae.se/write-ups/kebabs-train-stations/</a>, See on <a href="https://news.ycombinator.com/item?id=43165112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				

<p><strong>2025-02-14</strong></p>

    <ul>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#introduction">Introduction</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#method">Method</a>
            
                <ul>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#network-data">Network Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#restaurant-data">Restaurant Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#routing-and-distance">Routing and Distance</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#results">Results</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#discussion">Discussion</a>
            
        </li>
    
    </ul>

<p><em>This write-up was originally posted <a href="https://www.reddit.com/r/gis/comments/1iph0yy/the_closer_to_the_railway_station_the_less_tasty/">on reddit</a>, though I've cleaned things up specifically for this post. Due to reasons discussed towards the end of this post, I'm not entirely happy with the results and intend to take another shot at it in the near future.</em></p>
<h2 id="introduction">Introduction<a href="#introduction" aria-label="Anchor link for: introduction">🔗</a></h2>
<p>I came across <a href="https://www.reddit.com/r/gis/comments/1iopp56/anyone_motivated_to_prove_that_the_closer_from/">this post</a> sharing a hypothesis from a French subreddit;</p>
<blockquote>
<p>The closer to the train station, the worse the kebab.</p>
</blockquote>
<p>The original French post gained a decent amount of traction compared to the subreddit's relatively small size, indicating a certain amount of agreement among its members. There were some detractors in the comments, however, sharing experiences which ran contrary to the stated hypothesis.</p>
<p>Thus, I figured I had nothing better to do, being a burned-out, unemployed drop-out with a newly-obtained autism diagnosis, so I figured I'd sacrifice my time for a worthy cause and perform this informal <em>"study"</em>. I'll be expecting my Nobel peace prize in the postbox and several job offers in my DMs within the next 3 working days.</p>
<h2 id="method">Method<a href="#method" aria-label="Anchor link for: method">🔗</a></h2>
<p>I assumed the best study area to be Paris, France since;</p>
<ol>
<li>The original post was French</li>
</ol>
<p>I haven't personally heard of this hypothesis in my home country (Sweden, also home to many a kebab-serving restaurant) so I figured I'd assume this to be a French phenomenon for the purpose of this informal "Study".</p>
<ol start="2">
<li>Density</li>
</ol>
<p>The inner city is <em><strong>dense</strong></em> with dozens of train/metro stations and god knows how many kebab shops. I knew early on that this would make my life pretty miserable, but at least it'd provide plenty of sample data.</p>
<h2 id="network-data">Network Data<a href="#network-data" aria-label="Anchor link for: network-data">🔗</a></h2>
<p>I used OSMnx to download and save a navigation network. Given the public transit-centric nature of the French subreddit, I though it'd make sense to stick to walking distance (eg. footpaths, side-walks) thus i set the OSMnx <code>network_type</code> to <code>"walk"</code>. Given the location (and that OSMnx used this CRS automatically when none was provided), all data was projected to EPSG:32631 (UTM zone 31N).</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>osmnx </span><span>as </span><span>ox
</span><span>from </span><span>geopandas </span><span>import </span><span>GeoDataFrame
</span><span>
</span><span>#EPSG
</span><span>PROJECTION </span><span>= </span><span>32631
</span><span>
</span><span>graph </span><span>= </span><span>ox.</span><span>graph_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>network_type</span><span>=</span><span>"walk"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>ox.</span><span>save_graphml</span><span>(graph, </span><span>filepath</span><span>=</span><span>"network.graphml"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-1.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-1.c53f23b9b1fd351c.jpg"></a>

<em>Figure 1: The study area and network</em></p>
<p>Next up is the various train/metro stations. Given the nature of the original French sub, I figured it'd make sense to include both the long-distance central stations along with the countless metro stations. This was also rather trivial with OSMnx, filtering by <code>railway=subway_entrance</code> or <code>railway=train_station_entrance.</code></p>
<pre data-lang="py"><code data-lang="py"><span>stations: GeoDataFrame </span><span>= </span><span>ox.</span><span>features_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>tags </span><span>= </span><span>{
</span><span>    </span><span>"railway"</span><span>: [</span><span>"subway_entrance"</span><span>, </span><span>"train_station_entrance"</span><span>]
</span><span>})
</span><span>
</span><span># Filter results to points
</span><span>station_nodes: GeoDataFrame </span><span>= </span><span>stations.loc[stations.geom_type</span><span>==</span><span>"Point"</span><span>]
</span><span>station_nodes </span><span>= </span><span>station_nodes.</span><span>to_crs</span><span>(</span><span>epsg</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>station_nodes.</span><span>to_file</span><span>(</span><span>"train_station_entrances.gpkg"</span><span>)
</span></code></pre>
<p>I saved outputs religiously so I could easily inspect them in QGIS. I did attempt to get python notebooks working with my NeoVIM setup, but it was all for naught.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-2.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-2.51f0e47dcd63c0a8.jpg"></a>

<em>Figure 2: Rail/metro entrances... Please ignore the airport iconography.</em></p>
<p>... And there we have the first half of the data, now for the restaurants.</p>
<h2 id="restaurant-data">Restaurant Data<a href="#restaurant-data" aria-label="Anchor link for: restaurant-data">🔗</a></h2>
<p>The Google Places API (and their respective reviews) seemed like a reasonable choice. Google reviews are naturally far from perfect and subject to their fair share of botting and the like, but it's the best I could think of at the time. There are alternatives such as Yelp, but their API is horrifically expensive for poor old me, and I was not in the mood to build a web scraper (it has the same soul-sucking effect on me as prompting an LLM). The $200 of free credit was also enticing.</p>
<p>However, as I started exploring the API... I realised that the Places API doesn't seem to have any way to search within a polygon, only within a point radius. Thank you, Mr. publicly owned mega-corporation. How Fun.</p>
<p>It also didn't help that autocomplete for the <code>googlemaps</code> library wasn't working. Python's a fine language, but its tooling does like to test my patience a little too often. And whilst I'm still complaining... The Google Cloud dashboard is likely the slowest "website" I've ever had the displeasure of interacting with.</p>
<p>So... This meant I'd have to perform some sort of grid search of the whole of Paris, crossing my fingers that I wouldn't bust my free usage. This, along with a couple interesting questions;</p>
<ol>
<li>What is... <em>A kebab?</em></li>
</ol>
<p>When I search for "kebab" (no further context necessary)... How does Google decide what restaurant serves kebab?</p>
<p>After some perusing, it didn't seem to be as deep as I thought. Plenty of restaurants simply had "kebab" in the name, some were designated as "Mediterranean" (Kebab has its origins in Turkey, Persia, middle east in general) and others had a fair few reviews simply mentioning "kebab." Good enough for me.</p>
<ol start="2">
<li>Trouble in query-land</li>
</ol>
<p>It turns out that when you query for places within a given radius, it's only a "bias." It's not a hard cut-off that'll help narrow-down our data harvesting and reduce unnecessary requests. It was becoming increasingly clear that google isn't really a fan of people doing this.</p>
<p>Now with all of that preamble out of the way, I needed to prepare my search.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-3.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-3.605c84ffddad8323.jpg"></a>

<em>Figure 3. Original admin boundaries</em></p>
<p>Paris' administrative boundary contains a couple of large green spaces. To the west, a park and to the east, some sort of sports institute.</p>
<p>After perusing these rather large spaces in Google maps, they seemed to contain a distinct lack of kebab-serving establishments. Thus, they were a burden on our API budget and needed to go.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-4.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-4.b360881962350eea.jpg"></a>

<em>Figure 4. Adjusted admin boundaries w/ network</em></p>
<p>I figured keeping the network and stations wouldn't do any harm, so they went unmodified.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-5.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-5.eedb86cb4c85e12c.jpg"></a>

<em>Figure 5. Sampling points, later projected to WGS84 for harvesting purposes</em></p>
<p>To maximise data-harvesting, I decided to go with a hex layout with a vertical spacing of 1 km. This should give us a search radius of 500m * √3 ~= 866 meters. Plenty of overlap, sure, but we shouldn't be getting any holes anywhere. I'm not sure why I was spending this much time ensuring "data integrity" when that might just have flown the window courtesy of Google, but it's the illusion of control that counts.</p>
<p>This give us 99 sample points which... Should be enough?</p>
<p>Regardless, here's how my 3AM python turned out:</p>
<pre data-lang="py"><code data-lang="py"><span># Already projected to WGS84
</span><span>sample_points: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"samples.gpkg"</span><span>)
</span><span>gmaps: googlemaps.Client </span><span>= </span><span>googlemaps.</span><span>Client</span><span>(</span><span>key</span><span>=</span><span>'get-your-own'</span><span>)
</span><span>
</span><span>output </span><span>= </span><span>{}
</span><span>
</span><span>for </span><span>point </span><span>in </span><span>sample_points.geometry:
</span><span>    lat, lon </span><span>= </span><span>point.y, point.x
</span><span>
</span><span>    next_page_token </span><span>= </span><span>None
</span><span>    num_fetches </span><span>= </span><span>3
</span><span>
</span><span>    </span><span>while </span><span>num_fetches </span><span>&gt; </span><span>0</span><span>:
</span><span>        result </span><span>= </span><span>{}
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>"kebab"</span><span>,
</span><span>                </span><span>location</span><span>=</span><span>(lat, lon),
</span><span>                </span><span>radius</span><span>=</span><span>866</span><span>,
</span><span>            )
</span><span>        </span><span>else</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>page_token</span><span>=</span><span>next_page_token
</span><span>            )
</span><span>
</span><span>        next_page_token </span><span>= </span><span>result.</span><span>get</span><span>(</span><span>"next_page_token"</span><span>)
</span><span>        </span><span>print</span><span>(result[</span><span>"status"</span><span>], next_page_token)
</span><span>
</span><span>        </span><span>for </span><span>p </span><span>in </span><span>result[</span><span>"results"</span><span>]:
</span><span>            output[p[</span><span>"place_id"</span><span>]] </span><span>= </span><span>p
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            </span><span>break
</span><span>
</span><span>        num_fetches </span><span>-= </span><span>1
</span><span>
</span><span>        </span><span>sleep</span><span>(</span><span>2</span><span>)
</span><span>
</span><span>json_out </span><span>= </span><span>json.</span><span>dumps</span><span>(output)
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"w"</span><span>) </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(json_out)
</span></code></pre>
<p>This worked quite well. Initially I skipped paging, resulting in 322 results. However, I noticed that a few establishments were missing in the results compared to my explorations in Google Maps.</p>
<p>After implementing paging and re-running, this gave us a grand total of 400 kebab-serving establishments. I was likely over-zealous with the paging considering how few additional results were retrieved. That, and that the API doesn't cap the search radius (again, it's only a bias) likely led to a fair few redundant API calls.</p>
<p>The raw Google Places API-output also needed to be clipped to the study area, projected to the local UTM zone as well as converted to a geospatial format;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>pandas </span><span>as </span><span>pd
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"r"</span><span>) </span><span>as </span><span>file:
</span><span>    data </span><span>= </span><span>json.</span><span>load</span><span>(file)
</span><span>    file.</span><span>close</span><span>()
</span><span>    
</span><span>    </span><span>for </span><span>id </span><span>in </span><span>data:
</span><span>        place </span><span>= </span><span>data[</span><span>id</span><span>]
</span><span>        point </span><span>= </span><span>place[</span><span>"geometry"</span><span>][</span><span>"location"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lng"</span><span>] </span><span>= </span><span>point[</span><span>"lng"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lat"</span><span>] </span><span>= </span><span>point[</span><span>"lat"</span><span>]
</span><span>        </span><span>del </span><span>data[</span><span>id</span><span>][</span><span>"geometry"</span><span>]
</span><span>
</span><span>    data </span><span>= </span><span>pd.DataFrame.</span><span>from_dict</span><span>(data).T
</span><span>    data.rating </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.rating)
</span><span>    data.user_ratings_total </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.user_ratings_total)
</span><span>    data </span><span>= </span><span>data[data[</span><span>"user_ratings_total"</span><span>] </span><span>&gt; </span><span>0</span><span>]
</span><span>
</span><span>    </span><span># Cleanup was added after the screenshot below was taken
</span><span>    data </span><span>= </span><span>data.</span><span>drop</span><span>(</span><span>columns</span><span>=</span><span>[
</span><span>        </span><span>"icon"</span><span>,
</span><span>        </span><span>"icon_background_color"</span><span>,
</span><span>        </span><span>"icon_mask_base_uri"</span><span>,
</span><span>        </span><span>"plus_code"</span><span>,
</span><span>        </span><span>"reference"</span><span>,
</span><span>        </span><span>"photos"</span><span>,
</span><span>        </span><span>"opening_hours"
</span><span>    ])
</span><span>
</span><span>    gdata </span><span>= </span><span>GeoDataFrame</span><span>(
</span><span>        data, </span><span>geometry</span><span>=</span><span>geopandas.</span><span>points_from_xy</span><span>(data.lng, data.lat),
</span><span>        </span><span>crs</span><span>=</span><span>4326
</span><span>    )
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>to_crs</span><span>(</span><span>PROJECTION</span><span>)
</span><span>
</span><span>    </span><span># Modified boundaries from Figure 4.
</span><span>    paris </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"mod_bounary.gpkg"</span><span>);
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>clip</span><span>(paris)
</span><span>
</span><span>    gdata.</span><span>to_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-6.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-6.1eab106a30493ee2.jpg"></a>

<em>Figure 6. We're in f###ing business</em></p>
<h2 id="routing-and-distance">Routing and Distance<a href="#routing-and-distance" aria-label="Anchor link for: routing-and-distance">🔗</a></h2>
<p>Finally, the fun part. I need to get the distance to the nearest station entrance for each establishment.</p>
<p>I could've absolutely just routed to every single entrance for every single restaurant to get the nearest... But that would've taken several decades. I needed to build some sort of spatial index and route to the nearest ~3 or something along those lines. Since Paris is so dense with plenty of routing options, I figured I wouldn't need to perform too many routing operations.</p>
<p>After some googling and dredging through API docs, however, it seemed GeoPandas was nice enough to do that for us with <code>sindex</code>. Although it didn't have the same "return nearest N" like my beloved r-tree rust library I was all too used to, it did allow me to search within a certain radius (1 km was large enough) and go from there. The query results weren't sorted, so I had to sort the indexes by distance and cut it down to size.</p>
<p>The network analysis was relatively straight-forward thanks to NetworkX, and after a couple of hours I managed to cobble together the following;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>networkx </span><span>as </span><span>nx
</span><span>import </span><span>shapely </span><span>as </span><span>shp
</span><span>
</span><span>establishments: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span><span>entrances: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"entrances.gpkg"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>load_graphml</span><span>(</span><span>"network.graphml"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs </span><span>= </span><span>PROJECTION</span><span>)
</span><span>
</span><span># Ensure the same CRS
</span><span>if </span><span>(establishments.crs </span><span>!= </span><span>entrances.crs </span><span>!= </span><span>PROJECTION</span><span>):
</span><span>    </span><span>exit</span><span>(</span><span>100</span><span>)
</span><span>
</span><span># Helper function to get the distance between a graph node and establishment geometry
</span><span>def </span><span>node_geom_dist</span><span>(</span><span>node_id</span><span>: int, </span><span>geom</span><span>: shp.Point):
</span><span>    node </span><span>= </span><span>graph.nodes[node_id]
</span><span>    </span><span>return </span><span>math.</span><span>sqrt</span><span>((geom.x </span><span>- </span><span>node[</span><span>'x'</span><span>]) </span><span>** </span><span>2 </span><span>+ </span><span>(geom.y </span><span>- </span><span>node[</span><span>'y'</span><span>]) </span><span>** </span><span>2</span><span>)
</span><span>
</span><span>distances: list[float] </span><span>= </span><span>[]
</span><span>
</span><span>for </span><span>(</span><span>id</span><span>, establishment) </span><span>in </span><span>establishments.</span><span>iterrows</span><span>():
</span><span>    establishment_geom: shp.Point </span><span>= </span><span>establishment.geometry
</span><span>    establishment_node: int </span><span>= </span><span>ox.</span><span>nearest_nodes</span><span>(graph, establishment_geom.x, establishment_geom.y)
</span><span>    establishment_dist_to_node: float </span><span>= </span><span>node_geom_dist</span><span>(establishment_node, establishment_geom)
</span><span>    
</span><span>    </span><span># Spatial index for rail entrances
</span><span>    index: shp.STRtree </span><span>= </span><span>entrances.sindex
</span><span>    nearest_q </span><span>= </span><span>index.</span><span>query</span><span>(establishment_geom, </span><span>predicate</span><span>=</span><span>"dwithin"</span><span>, </span><span>distance </span><span>= </span><span>1000</span><span>)
</span><span>    nearest_entrances: list[tuple[int, float]] </span><span>= </span><span>[]
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>nearest_q:
</span><span>        ent </span><span>= </span><span>entrances.iloc[i]
</span><span>        ent_geom: shp.Point </span><span>= </span><span>ent.geometry
</span><span>
</span><span>        dist </span><span>= </span><span>ent_geom.</span><span>distance</span><span>(establishment.geometry)
</span><span>        
</span><span>        nearest_entrances.</span><span>append</span><span>((i, dist))
</span><span>     
</span><span>    nearest_entrances </span><span>= </span><span>sorted</span><span>(nearest_entrances, </span><span>key </span><span>= lambda </span><span>e</span><span>: e[</span><span>1</span><span>])[:</span><span>3</span><span>]
</span><span>    entrance_geom: list[shp.Point] </span><span>= </span><span>[entrances.iloc[i].geometry </span><span>for </span><span>(i, </span><span>_</span><span>) </span><span>in </span><span>nearest_entrances]
</span><span>    entrance_nodes: list[int] </span><span>= </span><span>[ox.</span><span>nearest_nodes</span><span>(graph, point.x, point.y) </span><span>for </span><span>point </span><span>in </span><span>entrance_geom]
</span><span>    entrance_geom_dist_to_node: list[float] </span><span>= </span><span>[</span><span>node_geom_dist</span><span>(entrance_nodes[i], entrance_geom[i]) </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(nearest_entrances))]
</span><span>
</span><span>    result_paths </span><span>= </span><span>[nx.</span><span>shortest_path</span><span>(graph, establishment_node, dest_node, </span><span>weight</span><span>=</span><span>"length"</span><span>) </span><span>for </span><span>dest_node </span><span>in </span><span>entrance_nodes]
</span><span>    result_lengths: list[float] </span><span>= </span><span>[nx.</span><span>path_weight</span><span>(graph, path, </span><span>"length"</span><span>) </span><span>+ </span><span>entrance_geom_dist_to_node[i] </span><span>+ </span><span>establishment_dist_to_node </span><span>for </span><span>(i, path) </span><span>in </span><span>enumerate</span><span>(result_paths)]
</span><span>
</span><span>    distances.</span><span>append</span><span>(</span><span>min</span><span>(result_lengths))
</span><span>
</span><span>establishments[</span><span>"distance"</span><span>] </span><span>= </span><span>distances 
</span><span>establishments.</span><span>to_file</span><span>(</span><span>"establishment_results.gpkg"</span><span>)
</span></code></pre>
<p>Not exactly my finest work. The sheer amount of list comprehension is perhaps a little terrifying, but it works.</p>
<p>After some prodding around in QGIS with the resulting data and networks (and many print() statements), I was confident in the accuracy of the results.</p>
<h2 id="results">Results<a href="#results" aria-label="Anchor link for: results">🔗</a></h2>
<p>Now with all of this data, it is time to settle the question of whether or not the kebabs are less tasty the closer they are to a train/metro station...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-7.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-7.1ff7f2b316a90f62.jpg"></a>

<em>Figure 7. Hmmmmm....</em></p>
<p>With a mighty Pearson's correlation of 0.091, the data indicates that this could be true! If you ignore the fact that the correlation is so weak that calling it 'statistically insignificant' would be quite generous.</p>
<p>Outliers can have an outsized impact on a Pearson's correlation, so after ridding the dataset of some outliers via IQR fencing...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-8.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-8.af2b5c5cacb7b32d.jpg"></a>

<em>Figure 8. Removed outliers</em></p>
<p>... This increased the coefficient to a whopping 0.098.</p>
<p>This was a bit of a bummer (though hardly surprising) and figuring I had nothing to lose from messing around a little, I tried filtering out metro stations in case my original assumption of the metro being included in the original hypothesis was incorrect.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-9.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-9.3427bdea5242e0b1.jpg"></a>

*Figure 9. Not much better, eh? Correction: "... Nearest train station entrance."</p>
<p>With an even worse coefficient of 0.001, I think It's time to hang up the towel.</p>
<p>Whilst there are some minor indications that the hypothesis <em>could</em> be correct (eg. Many of the absolute worst restaurants being some of the closest) the correlation is simply too weak.</p>
<h2 id="discussion">Discussion<a href="#discussion" aria-label="Anchor link for: discussion">🔗</a></h2>
<p><em><strong>- Are Google reviews an objective measurement of how tasty the kebabs are?</strong></em></p>
<p>Absolutely the f### not. This was a rather subjective observation from the very beginning and Google reviews aren't exactly a good measure of "is the food good?" There are many aspects of the dining experience that could hypothetically impact a review score. The staff, cleanliness, the surrounding environment, etc. Not to mention online skulduggery and review manipulation.</p>
<p><em><strong>- Can tourism have an impact?</strong></em></p>
<p>It absolutely could. I don't want to make any definitive assumptions, but I can absolutely imagine the local regulars being harsher than the massive tourist population, or even vice-versa.</p>
<p><em><strong>- Were the Google results accurate?</strong></em></p>
<p>To an extent, yes. From what I could gather, every location from the query seemed to serve kebab in some form. There were a few weird outliers and nuances, such as Pizza Hut which likely only serves kebab pizza rather than the multitude of different forms in which kebab could possibly be consumed.</p>
<p><em><strong>- Why not restaurants in general?</strong></em></p>
<p>Because the initial hypothesis was too comically hyper-specific for me to give up on.</p>
<p><em><strong>- What about review count?</strong></em></p>
<p>This could very well have an effect, though I was not entirely certain how to properly implement this metric into the analysis at the time.</p>
<p><em><strong>- Gib Data</strong></em></p>
<p>I'm not quite comfortable in doing so, mostly due to potential breaches of Google's TOS. I don't think they would care about me harvesting some 400 POIs for this little experiment, I'm not quite willing to gamble sharing the data with others.</p>
<p>Besides, I gave you the code. Go burn some of your own credits.</p>
<p><em><strong>- Are you Ok?</strong></em></p>
<p>... I guess? Are you?</p>
<p>In conclusion, this was actually quite fun. I wrote this as the project went on (otherwise I would likely never have found the motivation) and I would encourage others to do other silly explorations like this, even if the results end up slightly depressing.</p>
<p>... <em>However</em>, after some additional discussion, I decided I wasn't quite done.</p>
<p>As stated earlier, there were a few detracting comments on the original French post. Interestingly, many of the provided examples of good kebab restaurants next to train stations just so happened to be in Paris.</p>
<p>The user who originally posted the French post for the sub in English provided some <a href="https://imgur.com/gallery/kebab-railway-stations-wuYG9D2">examples</a> which seem to strengthen the hypothesis. It could very well be that whatever conditions affect Paris restaurants (whether it be higher rent, wages, tourism, population density...) had a larger impact than I initially suspected.</p>
<p><em><strong>Stay tuned for part 2... Whenever I get around to doing it!</strong></em></p>


			</div></div>]]></description>
        </item>
    </channel>
</rss>