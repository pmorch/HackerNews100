<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 04 Mar 2025 16:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Bayleaf · Building a low-profile wireless split keyboard (160 pts)]]></title>
            <link>https://www.graz.io/articles/bayleaf-wireless-keyboard</link>
            <guid>43255529</guid>
            <pubDate>Tue, 04 Mar 2025 15:00:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.graz.io/articles/bayleaf-wireless-keyboard">https://www.graz.io/articles/bayleaf-wireless-keyboard</a>, See on <a href="https://news.ycombinator.com/item?id=43255529">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Federal workers ordered to return to offices without desks, Wi-Fi and lights (244 pts)]]></title>
            <link>https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</link>
            <guid>43253562</guid>
            <pubDate>Tue, 04 Mar 2025 12:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html">https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43253562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thliwo000v2cozgh2xbdev@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Millions of federal workers were <a href="https://www.cnn.com/2025/01/23/business/trump-federal-workers-rto-mandate/index.html">ordered to return to offices</a> across the country in recent weeks, marking an end to Covid-era rules allowing more flexibility to work from home.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thlk6200003b6m4ysl92ra@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Many have come back to workplaces that weren’t ready for them.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thv5ao00083b6mw2qej22u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In one Department of Health and Human Services office, there was no Wi-Fi or full electricity in the first hours when people returned last week.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok3000e3b6mmdfnpnek@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Department of Education employees at an office in Dallas returned to ethernet cords in piles around the floor, random wires sticking out of walls, and motion-sensor lights that weren’t working correctly, leading to dark workspaces. One employee tripped over a pile of cords on her first day back, resulting in a large gash on her foot. She’s submitted a workers’ compensation complaint.
    </p>

  
<div data-image-variation="image_inline-small" data-breakpoints="{&quot;image_inline-small--eq-extra-small&quot;: 115, &quot;image_inline-small--eq-small&quot;: 300, &quot;image_inline-small--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti30t500013b6mvbtdzglx@published" data-name="c-IMG_2545.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666875" data-original-height="1067" data-original-width="1600" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?c=original" data-editable="settings">
       <picture><source height="1067" width="1600" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill" alt="Ethernet cords lay on the floor at a Department of Education office in Dallas. " onload="this.classList.remove('image_inline-small__dam-img--loading')" onerror="imageLoadError(this)" height="1067" width="1600" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000f3b6mlbe4g8bi@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            And a Department of Defense employee who returned to in-office work and handles sensitive information was stuck in a conference room with people on different teams, forcing them to leave the room to make calls. The employee was eventually moved to an office — but one without Wi-Fi, so they had to use their phone’s spotty hot spot.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000g3b6mvpdz4pmj@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The only thing a return to the office has given me is an hour of traffic while driving and a loss in efficiency,” said the worker, who requested anonymity for fear of job reprisals.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000h3b6my9a5k5lc@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The problems, confusion and slipups that federal employees told CNN they’ve encountered returning to the office have only added to the chaos inside the workforce six weeks into a Trump administration determined to <a href="https://www.cnn.com/politics/tracking-federal-workforce-firings-dg/index.html">slash the size and scope</a> of the federal government.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000i3b6m1ztsyzip@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some federal workers being told to return to the office have no space to return to. At least two office buildings used by the Interior Department in the Western US were told last week their leases had been canceled, according to a source familiar with the matter — while a third office housing hundreds of people was notified its lease will be up in June.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000j3b6maute1e0f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One source said the General Services Administration, which manages federal buildings, did not appear to coordinate the lease terminations with Interior officials, leaving employees unclear of what to do. An Interior spokesperson said the department was “working with GSA to ensure facilities or alternative options will be available” for employees.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000k3b6myjlfpb4f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            President <a href="https://www.cnn.com/politics/president-donald-trump-47">Donald Trump</a> has repeatedly demanded all federal workers return to the office as his administration has undertaken sweeping actions seeking to drastically reduce the size of the federal workforce, including mass firings of probationary workers and employees in government diversity departments.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000l3b6mpk1596b3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump’s in-office mandate has been coupled with a push to slash government real estate, setting up a dilemma of too little space for too many people. Even before Trump took office in January, the federal government was downsizing office space due in part to the shift toward telework during the pandemic.
    </p>

  





    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000m3b6m9c6j2qe2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Nearly half of the more than 2.3 million civilian federal workers were eligible for telework, and 10% were in remote positions with no expectation of in-person work, according to a 2024 Office of Management and Budget report.<strong> </strong>Many workers stopped working full time in their offices during the Covid-19 pandemic, but others had long-held arrangements for working from home<strong> </strong>— and now could face the prospect of choosing between fundamentally changing their job or leaving it altogether.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000n3b6m0hjimos2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            More than 80% of federal workers reside outside the Washington, DC, metro area, meaning the return-to-office mandate will cause ripple effects across the country.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000o3b6m93ver5jt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some government employees are now making the post-pandemic transition that millions of private-sector workers already have — upending their lives and schedules in the process. For many federal employees, however, those worries have been subsumed by their larger fear of losing jobs entirely in Trump’s multipronged effort to shrink the federal workforce.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000p3b6m4evksl91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal workers and union officials told CNN they see this as part of an attempt by the Trump administration to <a href="https://www.propublica.org/article/video-donald-trump-russ-vought-center-renewing-america-maga" target="_blank">make life uncomfortable</a> for federal workers in hopes that some will quit.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000q3b6mquhadaqy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump and Elon Musk, de facto chief of the Department of Government Efficiency, have both threatened to fire workers who do not come back to the office, even those represented by unions who have signed long-term telework agreements. Many federal agencies set February 24 as the first date for some of their employees to comply with Trump’s return-to-office directive.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000r3b6mvsxcej23@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “If they don’t report for work, we’re firing them. In other words, you have to go to office,” Trump said at a conservative political conference last month, while claiming that his <a href="https://www.cnn.com/2025/02/18/politics/mar-a-lago-trump-remote-work-golf/index.html">golf game would improve dramatically</a> if he were to work remotely.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000s3b6mjad0bg9i@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal employee unions have argued Trump’s demand to break long-term telework agreements <a href="https://www.cnn.com/2025/02/07/politics/trump-musk-federal-workforce/index.html">is unlawful</a>, though the return-to-office mandate isn’t central to the major legal challenges unions have brought against the president. But there have been pockets of pushback, such as the employee union at the Environmental Protection Agency, which is demanding to bargain around the return-to-office mandate. Additionally, some individual EPA employees represented by the union are filing grievances around returning to the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000t3b6moq891utg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One of their biggest concerns, an EPA union official told CNN, is “local fire and safety regulations — how many people can you jam into a room?”
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000v3b6m57ha4i02@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            DOGE has kept a running list on its website touting more than 200 building leases the Musk-run agency says it’s canceled. The canceled leases, which include Social Security Administration and US attorneys’ offices, have <a href="https://www.kristv.com/news/local-news/d-o-g-e-terminates-lease-for-local-u-s-attorneys-office" target="_blank">raised questions locally</a> about where those employees are supposed to go.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000w3b6mla7qqoax@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump also signed an executive order last week instructing each government agency to identify all leases that can be terminated and submit a plan to dispose of “government-owned real property which has been deemed by the agency as no longer needed.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000x3b6mj4swzprz@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The drive to shrink the federal government’s real estate portfolio predates the Trump administration. The Office of Management and Budget issued a “Reduce the Footprint” directive in 2015 that requires agencies to make more efficient use of federal property and dispose of surplus assets.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000y3b6mlikxf8fh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Last year’s OMB report included a list of agencies’ efforts to downsize their holdings. For instance, the Department of Veterans Affairs reduced office space in its headquarters locations in the Washington, DC, metro area by 16% between 2020 and 2022, saving $15 million annually.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000z3b6mrrjwv4ij@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The GSA cut its own footprint by more than 2 million square feet over 10 years, avoiding $300 million in costs. And the Department of Energy moved out of 193,000 square feet of leased space in 2023, saving $9 million annually.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3k1r00033b6m5wj2k31k@published" data-name="c-GettyImages-2201176517.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="1600" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?c=original" data-editable="settings">
       <picture><source height="1600" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill" alt="A pedestrian near a General Services Administration building in Washington, DC, on Monday, February 24. " onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400103b6mb5d7s02n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Those downsizings have had an impact. One federal employee still waiting for their date to return to the office told CNN they suspect it’s been delayed because of a lack of room. Their agency, which the employee asked not to name because of concerns for their job, has been shedding office space for several years, so teleworking staffers have had to reserve desks in the remaining locations for the days they come in.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400113b6moszzk9vr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The space constraint, coupled with the Trump administration’s <a href="https://www.cnn.com/2025/02/26/politics/federal-mass-layoffs-trump-memo/index.html">reduction-in-force order</a>, has the worker fearing their agency will suffer heavy layoffs. “The only way RTO (return-to-office) works in these types of situations is if you now reduce the number of people,” the employee said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400133b6myk6mc303@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Multiple federal agencies brought the bulk of their employees back last week, a return that was met in some cases with a lack of desks, basic supplies and working equipment.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400143b6mo8c8mzg8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “There was very little prep and planning and it was messy with equipment,” an HHS staffer told CNN. The employee, who asked for their specific location not to be named, said there were reports of Wi-Fi and electricity not working.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tzbkg60000356mwlpg5w7y@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Asked about the issue, Andrew Nixon, HHS’ communications director, said the agency is complying with Trump’s return to the office executive order. “We look forward to seeing and collaborating with our colleagues in person to Make America Healthy Again,” he continued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400153b6mwhuve4yt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Department of Education sent an email last week to staff in regional offices acknowledging the shortcomings in some facilities on Day 1 of their return.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400163b6mbx9ev3n5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “At the present time we are unable to deploy full peripheral setups in the regional/remote offices,” the department’s Office of the Chief Information Officer told employees in an email, which was obtained by CNN.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400173b6mmbs9f9rw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Sheria Smith, president of the American Federation of Government Employees Local 252 in Dallas and a Department of Education employee, said her office was “chaos” when employees returned. She filed the workers’ comp complaint after injuring her foot tripping over a pile of cords on the floor.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400183b6mgniynv3n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The facilities were not actually ready for us to return,” Smith said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400193b6m4wt9l8ci@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “No one is on-site to try to fix the issues,” she added, saying the mess leads her to believe “they would be hoping that we would quit, I guess — that they didn’t expect us to come.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001a3b6mit76tbc8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Another Department of Education employee in Washington, DC, said the first week back lacked basic office needs: computers, pens and headsets — as well as private space and conference rooms necessary for confidential job requirements.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001b3b6m0l2xr20t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the National Oceanic and Atmospheric Administration headquarters in Silver Spring, Maryland, a telework policy has been in place for more than 20 years, officials said. Many employees had been teleworking three to four days a week and are now adapting to the rigidity of returning to the office.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3w9800053b6m4thchr42@published" data-name="c-GettyImages-2203083573.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6670833333333334" data-original-height="1601" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?c=original" data-editable="settings">
       <picture><source height="1601" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill" alt="Hundreds of demonstrators gather to protest against Department of Government Efficiency (DOGE) cuts outside the headquarters of the National Oceanic and Atmospheric Administration on March 3, in Silver Spring, Maryland." onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1601" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001c3b6mt5ajyxux@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Morale is pretty low,” a staffer in the agency said. “If you have a dentist appointment at 3 p.m. near where you live, and it ends at 4:15 p.m., you cannot work that last hour from home. You have to go back to the office or take sick leave.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001d3b6m8sdzmo6f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The return-to-office mandate comes as budget cuts at EPA led to reduced cleaning and facility services at key offices, even before the Trump administration took over.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001e3b6mokfltdj8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In the three major EPA offices around the country that make up the agency’s headquarters — Washington, Cincinnati and Research Triangle Park, North Carolina — health units were closed and in-office mail delivery was cut back, according to a memo obtained by CNN. In Cincinnati, drinking fountain cleaning schedules were reduced to once per week, carpets and hallways were swept once a month, and bathroom cleanings were pared back to once a day.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001f3b6m81n56i52@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “With these people coming back to the offices, they’re going to have fewer facility services,” the EPA union official said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001h3b6mxf5m3uln@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The 2024 OMB report found those eligible for telework spend about 60% of their working hours in the office, on average, though that figure varies widely by agency. About 10% of staffers have remote jobs, where they are not expected to report to an office at all, according to the report, which noted the Biden administration also directed agencies to increase the amount of time federal workers spent at the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001i3b6m9ga6xl82@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Biden administration’s goal was for teleworkers to spend at least 50% of the time in the office. Now Trump turned that into a full-time mandate.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001j3b6mw8ryoj8k@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But not all federal workers are returning at the same pace, as Trump’s executive order, issued hours after he took office in January, directed agencies to terminate remote work arrangements “as soon as practicable.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001k3b6movkqhmp4@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Several agencies, including the departments of Veterans Affairs and Health and Human Services, have said political appointees, senior executives and other senior staffers could no longer work remotely or telework as of February 24, and neither could supervisors who live within 50 miles of an agency facility.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001l3b6mqhwzi9zp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Others have more time. For instance, at the VA, lower-level non-union employees who live within 50 miles of a facility will have their remote and telework arrangements terminated by April 28, except for ad hoc or situational circumstances, the agency said. But these arrangements for supervisors and other non-union employees who reside farther away were not ended — those staffers will receive additional guidance.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001m3b6mcycmncdy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            As for union workers at the VA, their return-to-office date will be announced at a later time, the agency said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tpdftm00053b6mfxan4xsn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the Department of Education, more than 70% of its workforce started reporting to the Washington and regional offices full time last week, the agency said in a news release. The rest are expected back by June 1 after building renovations and relocation arrangements are complete.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001n3b6maetty1jv@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, many union members at the Department of Housing and Urban Development who were previously able to telework on certain days had to return to the office full time in late February, said Antonio Gaines, president of the AFGE Council 222, which represents 5,300 employees at the agency. Workers at some regional and field offices are exempt for now because of lack of space and safety concerns, including building renovations and inadequate HVAC systems.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001o3b6mk4fzp47r@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The mandate violates the union’s collective bargaining agreement, he argued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001p3b6mwn7j8d8n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “They made a unilateral decision to bypass the negotiating process,” Gaines said of the agency, adding that the union plans to file a complaint on this matter as part of a bigger grievance package.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001q3b6meyhnp4y8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The department did not return a request for comment.
    </p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Italy moves to reverse anti-nuclear stance (201 pts)]]></title>
            <link>https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</link>
            <guid>43253407</guid>
            <pubDate>Tue, 04 Mar 2025 11:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance">https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</a>, See on <a href="https://news.ycombinator.com/item?id=43253407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
                    Monday, 3 March 2025

                    </p>

                        <p>Italy's Council of Ministers has approved a draft law calling for the government to adopt a series of legislative decrees to create the legal framework for the reintroduction of nuclear power, which was phased out following a referendum in 1987.</p>
                    
                            <figure>
                                <img src="https://www.world-nuclear-news.org/images/articles/GilbertoPichettoFratin_67730.jpg" alt="Cabinet moves to reverse Italy's anti-nuclear stance">
                                    <figcaption>Pichetto Fratin speaking at a press conference following the cabinet meeting (Image: Ministry of Environment and Energy Security)</figcaption>

                            </figure>
                    
                    


               

                <p>On 28 February, on the proposal of President Giorgia Meloni and the Minister of the Environment and Energy Security Gilberto Pichetto Fratin, the Council of Ministers (the Italian cabinet) approved the draft delegation law on 'sustainable nuclear energy'.</p>

<p>The government said the text is aimed at "the inclusion of sustainable nuclear and fusion in the so-called 'Italian energy mix' and intervenes organically from an economic, social and environmental perspective, within the framework of European decarbonisation policies with a time horizon of 2050, consistently with the objectives of carbon neutrality and security of supply".</p>

<p>It added that the intervention aims to: ensure continuity of energy supply in the presence of a constant increase in demand and promote the achievement of energy independence; contribute to the decarbonisation objectives necessary to tackle climate change; and ensure the sustainability of costs borne by end users and the competitiveness of the national industrial system.</p>

<p>The draft law says that Italy should make "a clear break ... with respect to the nuclear plants of the past" and "use of the best available technologies, including modular and advanced technologies". It calls for the government to establish an independent authority for the regulation, supervision and control of nuclear infrastructures.</p>

<p>"Promoters of nuclear projects must provide adequate financial and legal guarantees to cover the costs of construction, operation and decommissioning of the plants and for risks, even those not directly attributable to them, arising from nuclear activity," it adds.</p>

<p>The draft law requires the government to adopt a series of legislative decrees, within 12 months of entry into force, to "organically regulate the entire life cycle of the new sustainable energy, through the drafting of a national programme: from the testing, localisation, construction and operation of the new reactors, to the issue of manufacturing and reprocessing of the fuel will be addressed in a circular economy vision". It will also "serve to provide training and information tools, train new technicians and professionals in the sector, and identify benefits for the territories involved".</p>

<p>The draft law will now be submitted to parliament for final approval.</p>

<p>"With the latest generation nuclear, together with renewables, we will be able to achieve the objectives of decarbonisation, guaranteeing the full energy security of the country," Minister Pichetto Fratin said. "In this way, Italy is ready to face the challenges of the future."</p>

<h4><span>The background</span><br>
&nbsp;</h4>

<p>Italy operated a total of four nuclear power plants starting in the early 1960s but decided to phase out nuclear power in a referendum that followed the 1986 Chernobyl accident. It closed its last two operating plants, Caorso and Trino Vercellese, in 1990.</p>

<p>In late March 2011, following the Fukushima Daiichi accident, the Italian government approved a moratorium of at least one year on construction of nuclear power plants in the country, which had been looking to restart its long-abandoned nuclear programme.</p>

<p>The public mood has changed since then, and in May 2023, the Italian Parliament approved a motion to urge the government to consider incorporating nuclear power into the country's energy mix. In September last year, the first meeting was held of the National Platform for a Sustainable Nuclear, set up by the government to define a time frame for the possible resumption of nuclear energy in Italy and identify opportunities for the country's industrial chain already operating in the sector.</p>

<p>Italy's government included potential nuclear capacity - up to 16 GW/20-22% of capacity by 2050 - in its National Integrated Energy and Climate Plan, which was submitted to the European Commission on 1 July 2024.</p>

<p>Speaking the following day at the <em>Global Energy Transition Congress in Milan</em>, Pichetto Fratin, said: "We expect to be able to reach about 8 GW from nuclear power by 2050, covering more than 10% of the nation's electricity demand. This percentage may increase to over 20-22% by fully exploiting the potential of nuclear power in our country."</p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let’s code a TCP/IP stack, 1: Ethernet and ARP (2016) (294 pts)]]></title>
            <link>https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/</link>
            <guid>43250093</guid>
            <pubDate>Tue, 04 Mar 2025 03:55:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/">https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/</a>, See on <a href="https://news.ycombinator.com/item?id=43250093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
    <p>Writing your own TCP/IP stack may seem like a daunting task. Indeed, TCP has accumulated many specifications over its lifetime of more than thirty years. The core specification, however, is seemingly compact<sup id="fnref:tcp-roadmap" role="doc-noteref"><a href="#fn:tcp-roadmap">1</a></sup> - the important parts being TCP header parsing, the state machine, congestion control and retransmission timeout computation.</p>

<p>The most common layer 2 and layer 3 protocols, Ethernet and IP respectively, pale in comparison to TCP’s complexity. In this blog series, we will implement a minimal userspace TCP/IP stack for Linux.</p>

<p>The purpose of these posts and the resulting software is purely educational - to learn network and system programming at a deeper level.</p>

<h2 id="contents">Contents</h2>

<ul id="markdown-toc">
  <li><a href="#tuntap-devices" id="markdown-toc-tuntap-devices">TUN/TAP devices</a></li>
  <li><a href="#ethernet-frame-format" id="markdown-toc-ethernet-frame-format">Ethernet Frame Format</a></li>
  <li><a href="#ethernet-frame-parsing" id="markdown-toc-ethernet-frame-parsing">Ethernet Frame Parsing</a></li>
  <li><a href="#address-resolution-protocol" id="markdown-toc-address-resolution-protocol">Address Resolution Protocol</a></li>
  <li><a href="#address-resolution-algorithm" id="markdown-toc-address-resolution-algorithm">Address Resolution Algorithm</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#sources" id="markdown-toc-sources">Sources</a></li>
</ul>

<h2 id="tuntap-devices">TUN/TAP devices</h2>

<p>To intercept low-level network traffic from the Linux kernel, we will use a Linux TAP device. In short, a TUN/TAP device is often used by networking userspace applications to manipulate L3/L2 traffic, respectively. A popular example is <a href="https://www.saminiir.com/openvpn-puts-packets-inside-your-packets/#tunneling">tunneling</a>, where a packet is wrapped inside the payload of another packet.</p>

<p>The advantage of TUN/TAP devices is that they’re easy to set up in a userspace program and they are already being used in a multitude of programs, such as <a href="https://www.saminiir.com/openvpn-puts-packets-inside-your-packets/">OpenVPN</a>.</p>

<p>As we want to build the networking stack from the layer 2 up, we need a TAP device. We instantiate it like so:</p>

<figure><pre><code data-lang="c"><span>/*
 * Taken from Kernel Documentation/networking/tuntap.txt
 */</span>
<span>int</span> <span>tun_alloc</span><span>(</span><span>char</span> <span>*</span><span>dev</span><span>)</span>
<span>{</span>
    <span>struct</span> <span>ifreq</span> <span>ifr</span><span>;</span>
    <span>int</span> <span>fd</span><span>,</span> <span>err</span><span>;</span>

    <span>if</span><span>(</span> <span>(</span><span>fd</span> <span>=</span> <span>open</span><span>(</span><span>"/dev/net/tap"</span><span>,</span> <span>O_RDWR</span><span>))</span> <span>&lt;</span> <span>0</span> <span>)</span> <span>{</span>
        <span>print_error</span><span>(</span><span>"Cannot open TUN/TAP dev"</span><span>);</span>
        <span>exit</span><span>(</span><span>1</span><span>);</span>
    <span>}</span>

    <span>CLEAR</span><span>(</span><span>ifr</span><span>);</span>

    <span>/* Flags: IFF_TUN   - TUN device (no Ethernet headers)
     *        IFF_TAP   - TAP device
     *
     *        IFF_NO_PI - Do not provide packet information
     */</span>
    <span>ifr</span><span>.</span><span>ifr_flags</span> <span>=</span> <span>IFF_TAP</span> <span>|</span> <span>IFF_NO_PI</span><span>;</span>
    <span>if</span><span>(</span> <span>*</span><span>dev</span> <span>)</span> <span>{</span>
        <span>strncpy</span><span>(</span><span>ifr</span><span>.</span><span>ifr_name</span><span>,</span> <span>dev</span><span>,</span> <span>IFNAMSIZ</span><span>);</span>
    <span>}</span>

    <span>if</span><span>(</span> <span>(</span><span>err</span> <span>=</span> <span>ioctl</span><span>(</span><span>fd</span><span>,</span> <span>TUNSETIFF</span><span>,</span> <span>(</span><span>void</span> <span>*</span><span>)</span> <span>&amp;</span><span>ifr</span><span>))</span> <span>&lt;</span> <span>0</span> <span>){</span>
        <span>print_error</span><span>(</span><span>"ERR: Could not ioctl tun: %s</span><span>\n</span><span>"</span><span>,</span> <span>strerror</span><span>(</span><span>errno</span><span>));</span>
        <span>close</span><span>(</span><span>fd</span><span>);</span>
        <span>return</span> <span>err</span><span>;</span>
    <span>}</span>

    <span>strcpy</span><span>(</span><span>dev</span><span>,</span> <span>ifr</span><span>.</span><span>ifr_name</span><span>);</span>
    <span>return</span> <span>fd</span><span>;</span>
<span>}</span></code></pre></figure>

<p>After this, the returned file descriptor <code>fd</code> can be used to <code>read</code> and <code>write</code> data to the virtual device’s ethernet buffer.</p>

<p>The flag <code>IFF_NO_PI</code> is crucial here, otherwise we end up with unnecessary packet information prepended to the Ethernet frame. You can actually take a look at the kernel’s <a href="https://github.com/torvalds/linux/blob/v4.4/drivers/net/tun.c#L1306">source code</a> of the tun-device driver and verify this yourself.</p>

<h2 id="ethernet-frame-format">Ethernet Frame Format</h2>

<p>The multitude of different Ethernet networking technologies are the backbone of connecting computers in <em>Local Area Networks</em> (LANs). As with all physical technology, the Ethernet standard has greatly evolved from its first version<sup id="fnref:ethernet" role="doc-noteref"><a href="#fn:ethernet">2</a></sup>, published by Digital Equipment Corporation, Intel and Xerox in 1980.</p>

<p>The first version of Ethernet was slow in today’s standards - about 10Mb/s and it utilized half-duplex communication, meaning that you either sent or received data, but not at the same time. This is why a <em>Media Access Control</em> (MAC) protocol had to be incorporated to organize the data flow. Even to this day, <em>Carrier Sense, Multiple Access with Collision Detection</em> (CSMA/CD) is required as the MAC method if running an Ethernet interface in half-duplex mode.</p>

<p>The invention of the <em>100BASE-T</em> Ethernet standard used twisted-pair wiring to enable full-duplex communication and higher throughput speeds. Additionally, the simultaneous increase in popularity of Ethernet switches made CSMA/CD largely obsolete.</p>

<p>The different Ethernet standards are maintained by the IEEE 802.3<sup id="fnref:ieee-802-3" role="doc-noteref"><a href="#fn:ieee-802-3">3</a></sup> working group.</p>

<p>Next, we’ll take a look at the Ethernet Frame header. It can be declared as a C struct followingly:</p>

<figure><pre><code data-lang="c"><span>#include &lt;linux/if_ether.h&gt;
</span>
<span>struct</span> <span>eth_hdr</span>
<span>{</span>
    <span>unsigned</span> <span>char</span> <span>dmac</span><span>[</span><span>6</span><span>];</span>
    <span>unsigned</span> <span>char</span> <span>smac</span><span>[</span><span>6</span><span>];</span>
    <span>uint16_t</span> <span>ethertype</span><span>;</span>
    <span>unsigned</span> <span>char</span> <span>payload</span><span>[];</span>
<span>}</span> <span>__attribute__</span><span>((</span><span>packed</span><span>));</span></code></pre></figure>

<p>The <code>dmac</code> and <code>smac</code> are pretty self-explanatory fields. They contain the MAC addresses of the communicating parties (destination and source, respectively).</p>

<p>The overloaded field, <code>ethertype</code>, is a 2-octet field, that depending on its value, either indicates the length or the type of the payload. Specifically, if the field’s value is greater or equal to 1536, the field contains the type of the payload (e.g. IPv4, ARP). If the value is less than that, it contains the length of the payload.</p>

<p>After the type field, there is a possibility of several different <em>tags</em> for the Ethernet frame. These tags can be used to describe the <em>Virtual LAN</em> (VLAN) or the <em>Quality of Service</em> (QoS) type of the frame. Ethernet frame tags are excluded from our implementation, so the corresponding field also does not show up in our protocol declaration.</p>

<p>The field <code>payload</code> contains a pointer to the Ethernet frame’s payload. In our case, this will contain an ARP or IPv4 packet. If the payload length is smaller than the minimum required 48 bytes (without tags), pad bytes are appended to the end of the payload to meet the requirement.</p>

<p>We also include the <code>if_ether.h</code> Linux header to provide a mapping between ethertypes and their hexadecimal values.</p>

<p>Lastly, the Ethernet Frame Format also includes the <em>Frame Check Sequence</em> field in the end, which is used with <em>Cyclic Redundancy Check</em> (CRC) to check the integrity of the frame. We will omit the handling of this field in our implementation.</p>

<h2 id="ethernet-frame-parsing">Ethernet Frame Parsing</h2>

<p>The attribute <em>packed</em> in a struct’s declaration is an implementation detail - It is used to instruct the GNU C compiler not to optimize the struct memory layout for data alignment with padding bytes<sup id="fnref:gnu-c-packed" role="doc-noteref"><a href="#fn:gnu-c-packed">4</a></sup>. The use of this attribute stems purely out of the way we are “parsing” the protocol buffer, which is just a type cast over the data buffer with the proper protocol struct:</p>

<figure><pre><code data-lang="c"><span>struct</span> <span>eth_hdr</span> <span>*</span><span>hdr</span> <span>=</span> <span>(</span><span>struct</span> <span>eth_hdr</span> <span>*</span><span>)</span> <span>buf</span><span>;</span></code></pre></figure>

<p>A portable, albeit slightly more laborious approach, would be to serialize the protocol data manually. This way, the compiler is free to add padding bytes to conform better to different processor’s data alignment requirements.</p>

<p>The overall scenario for parsing and handling incoming Ethernet frames is straightforward:</p>

<figure><pre><code data-lang="c"><span>if</span> <span>(</span><span>tun_read</span><span>(</span><span>buf</span><span>,</span> <span>BUFLEN</span><span>)</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
    <span>print_error</span><span>(</span><span>"ERR: Read from tun_fd: %s</span><span>\n</span><span>"</span><span>,</span> <span>strerror</span><span>(</span><span>errno</span><span>));</span>
<span>}</span>

<span>struct</span> <span>eth_hdr</span> <span>*</span><span>hdr</span> <span>=</span> <span>init_eth_hdr</span><span>(</span><span>buf</span><span>);</span>

<span>handle_frame</span><span>(</span><span>&amp;</span><span>netdev</span><span>,</span> <span>hdr</span><span>);</span></code></pre></figure>

<p>The <code>handle_frame</code> function just looks at the <code>ethertype</code> field of the Ethernet header, and decides its next action based upon the value.</p>

<h2 id="address-resolution-protocol">Address Resolution Protocol</h2>

<p>The <em>Address Resolution Protocol</em> (ARP) is used for dynamically mapping a 48-bit Ethernet address (MAC address) to a protocol address (e.g. IPv4 address). The key here is that with ARP, multitude of different L3 protocols can be used: Not just IPv4, but other protocols like CHAOS, which declares 16-bit protocol addresses.</p>

<p>The usual case is that you know the IP address of some service in your LAN, but to establish actual communications, also the hardware address (MAC) needs to be known. Hence, ARP is used to broadcast and query the network, asking the owner of the IP address to report its hardware address.</p>

<p>The ARP packet format is relatively straightforward:</p>

<figure><pre><code data-lang="c"><span>struct</span> <span>arp_hdr</span>
<span>{</span>
    <span>uint16_t</span> <span>hwtype</span><span>;</span>
    <span>uint16_t</span> <span>protype</span><span>;</span>
    <span>unsigned</span> <span>char</span> <span>hwsize</span><span>;</span>
    <span>unsigned</span> <span>char</span> <span>prosize</span><span>;</span>
    <span>uint16_t</span> <span>opcode</span><span>;</span>
    <span>unsigned</span> <span>char</span> <span>data</span><span>[];</span>
<span>}</span> <span>__attribute__</span><span>((</span><span>packed</span><span>));</span></code></pre></figure>

<p>The ARP header (<code>arp_hdr</code>) contains the 2-octet <code>hwtype</code>, which determines the link layer type used. This is Ethernet in our case, and the actual value is <code>0x0001</code>.</p>

<p>The 2-octet <code>protype</code> field indicates the protocol type. In our case, this is IPv4, which is communicated with the value <code>0x0800</code>.</p>

<p>The <code>hwsize</code> and <code>prosize</code> fields are both 1-octet in size, and they contain the sizes of the hardware and protocol fields, respectively. In our case, these would be 6 bytes for MAC addresses, and 4 bytes for IP addresses.</p>

<p>The 2-octet field <code>opcode</code> declares the type of the ARP message. It can be ARP request (1), ARP reply (2), RARP request (3) or RARP reply (4).</p>

<p>The <code>data</code> field contains the actual payload of the ARP message, and in our case, this will contain IPv4 specific information:</p>

<figure><pre><code data-lang="c"><span>struct</span> <span>arp_ipv4</span>
<span>{</span>
    <span>unsigned</span> <span>char</span> <span>smac</span><span>[</span><span>6</span><span>];</span>
    <span>uint32_t</span> <span>sip</span><span>;</span>
    <span>unsigned</span> <span>char</span> <span>dmac</span><span>[</span><span>6</span><span>];</span>
    <span>uint32_t</span> <span>dip</span><span>;</span>
<span>}</span> <span>__attribute__</span><span>((</span><span>packed</span><span>));</span></code></pre></figure>

<p>The fields are pretty self explanatory. <code>smac</code> and <code>dmac</code> contain the 6-byte MAC addresses of the sender and receiver, respectively. <code>sip</code> and <code>dip</code> contain the sender’s and receiver’s IP addresses, respectively.</p>

<h2 id="address-resolution-algorithm">Address Resolution Algorithm</h2>

<p>The <a href="https://tools.ietf.org/html/rfc826">original specification</a> depicts this simple algorithm for address resolution:</p>

<figure><pre><code data-lang="bash">?Do I have the hardware <span>type </span><span>in </span>ar<span>$hrd</span>?
Yes: <span>(</span>almost definitely<span>)</span>
  <span>[</span>optionally check the hardware length ar<span>$hln</span><span>]</span>
  ?Do I speak the protocol <span>in </span>ar<span>$pro</span>?
  Yes:
    <span>[</span>optionally check the protocol length ar<span>$pln</span><span>]</span>
    Merge_flag :<span>=</span> <span>false
    </span>If the pair &lt;protocol <span>type</span>, sender protocol address&gt; is
        already <span>in </span>my translation table, update the sender
        hardware address field of the entry with the new
        information <span>in </span>the packet and <span>set </span>Merge_flag to true.
    ?Am I the target protocol address?
    Yes:
      If Merge_flag is <span>false</span>, add the triplet &lt;protocol <span>type</span>,
          sender protocol address, sender hardware address&gt; to
          the translation table.
      ?Is the opcode ares_op<span>$REQUEST</span>?  <span>(</span>NOW look at the opcode!!<span>)</span>
      Yes:
        Swap hardware and protocol fields, putting the <span>local
            </span>hardware and protocol addresses <span>in </span>the sender fields.
        Set the ar<span>$op</span> field to ares_op<span>$REPLY</span>
        Send the packet to the <span>(</span>new<span>)</span> target hardware address on
            the same hardware on which the request was received.</code></pre></figure>

<p>Namely, the <code>translation table</code> is used to store the results of ARP, so that hosts can just look up whether they already have the entry in their cache. This avoids spamming the network for redundant ARP requests.</p>

<p>The algorithm is implemented in <a href="https://github.com/saminiir/level-ip/blob/e9ceb08f01a5499b85f03e2d615309c655b97e8f/src/arp.c#L53">arp.c</a>.</p>

<p>Finally, the ultimate test for an ARP implementation is to see whether it replies to ARP requests correctly:</p>

<figure><pre><code data-lang="bash"><span>[</span>saminiir@localhost lvl-ip]<span>$ </span>arping <span>-I</span> tap0 10.0.0.4
ARPING 10.0.0.4 from 192.168.1.32 tap0
Unicast reply from 10.0.0.4 <span>[</span>00:0C:29:6D:50:25]  3.170ms
Unicast reply from 10.0.0.4 <span>[</span>00:0C:29:6D:50:25]  13.309ms

<span>[</span>saminiir@localhost lvl-ip]<span>$ </span>arp
Address                  HWtype  HWaddress           Flags Mask            Iface
10.0.0.4                 ether   00:0c:29:6d:50:25   C                     tap0</code></pre></figure>

<p>The kernel’s networking stack recognized the ARP reply from our custom networking stack, and consequently populated its ARP cache with the entry of our virtual network device. Success!</p>

<h2 id="conclusion">Conclusion</h2>

<p>The minimal implementation of Ethernet Frame handling and ARP is relatively easy and can be done in a few lines of code. On the contrary, the reward-factor is quite high, since you get to populate a Linux host’s ARP cache with your own make-belief Ethernet device!</p>

<p>The source code for the project can be found at <a href="https://github.com/saminiir/level-ip">GitHub</a>.</p>

<p>In the next post, we’ll continue the implementation with ICMP echo &amp; reply (ping) and IPv4 packet parsing.</p>

<p>If you liked this post, you can
<a href="https://twitter.com/intent/tweet?url=https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/&amp;text=Let%27s+code+a+TCP%2FIP+stack%2C+1%3A+Ethernet+%26+ARP&amp;via=saminiir" target="_blank">
  share it with your followers</a> 
and 
<a href="https://twitter.com/saminiir">
  follow me on Twitter</a>!</p>

<p><em>Kudos to Xiaochen Wang, whose similar implementation proved invaluable for me in getting up to speed with C network programming and protocol handling. I find his <a href="https://github.com/chobits/tapip">source code</a><sup id="fnref:tapip" role="doc-noteref"><a href="#fn:tapip">5</a></sup> easy to understand and some of my design choices were straight-out copied from his implementation.</em></p>

<h2 id="sources">Sources</h2>


  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Global sales of combustion engine cars have peaked (223 pts)]]></title>
            <link>https://ourworldindata.org/data-insights/global-sales-of-combustion-engine-cars-have-peaked</link>
            <guid>43249068</guid>
            <pubDate>Tue, 04 Mar 2025 01:28:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ourworldindata.org/data-insights/global-sales-of-combustion-engine-cars-have-peaked">https://ourworldindata.org/data-insights/global-sales-of-combustion-engine-cars-have-peaked</a>, See on <a href="https://news.ycombinator.com/item?id=43249068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Our latest Daily Data Insights</h2><a href="https://ourworldindata.org/data-insights">See all Daily Data Insights </a><div><a href="https://ourworldindata.org/data-insights#most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/98ba9faf-1c56-44dc-45c0-2856416a0e00/w=1620" alt="A bar chart displays the share of all births using assisted reproductive technology in various European countries for the year 2019. The countries are listed on the vertical axis, while the percentage of births is represented by horizontal bars extending to the right. 

Spain has the highest percentage at 8.9%, followed by Greece at 7.5% and Denmark at 6.3%. Other countries include Czechia at 6.2%, Estonia at 5.7%, and Iceland at 5.5%. 

The chart includes a note stating that the figures encompass all births in each country, possibly including cross-border treatment. The data source is the European Society of Human Reproduction and Embryology (2023) and the chart is published by Our World in Data." loading="lazy" data-filename="births-using-assisted-reproductive-technology-eshre.png" width="1620" height="2472"></picture></div><div><p>Today</p><h3>What share of births involve assisted reproductive technologies like IVF?</h3><div><p><span>In 1978, Louise Brown became the first baby born through in vitro fertilization (IVF). In this technique, eggs are fertilized with sperm in a lab before the resulting embryos are transferred to the uterus.</span></p><p><span>Assisted reproductive technologies have advanced further and now become widely available.</span></p><p><span>These innovations have helped many people who might otherwise struggle to conceive — supporting individuals and couples facing infertility, allowing older parents to preserve fertility, and enabling same-sex couples to have children.</span></p><p><span>The chart shows the share of births in 2019 that involved assisted reproductive technologies across various European countries. This can include cross-border treatment.</span></p><p><span>Spain had the highest share, with nearly 9% of births resulting from assisted reproductive technology, followed by Greece, Denmark, and Czechia. In contrast, countries like Ireland, Lithuania, Serbia, and Turkey had much lower rates, with less than 2% of all births.</span></p><p><span><span>Explore more data on fertility rates and reproductive technology</span></span><span> →</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#second-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9683a8af-c141-4138-d58d-ca92e70ecf00/w=1620" alt="A bar chart titled &quot;Divorce within the first decade of marriage is becoming less common in England and Wales&quot; displays divorce rates for couples based on their marriage year. The x-axis represents the year of marriage, ranging from 1965 to 2012, while the y-axis indicates the percentage of those divorced by their 10-year anniversary. The bars show divorce rates after a decade over the years, with the highest rate of 25% for those married in 1990 and 1995 and the lowest at 10% for those married in 1965. The most recent data point is from 2012, showing 17.5% of couples divorced within a decade. A note highlights that fewer couples who have married since 2000 have divorced after a decade. The data source is the UK Office for National Statistics, dated 2024. The chart is licensed under CC BY." loading="lazy" data-filename="divorces-in-england.png" width="1620" height="1620"></picture></div><div><p>Yesterday</p><h3>Fewer marriages in England and Wales are ending in divorce within the first ten years</h3><div><p><span>Since 2000, fewer couples in England and Wales have divorced within the first ten years of marriage, reversing the trend of the late 20th century.</span></p><p><span>The chart shows the percentage of marriages ending in divorce within a decade, based on the year of marriage. For those married in 1965, one in ten divorced within ten years.</span></p><p><span>By 1975, this had nearly doubled to 18% as legal reforms made separation easier and less stigmatized. Divorce rates peaked for couples married in 1995, with one in four divorcing by their tenth anniversary.</span></p><p><span>But, as you can see, this trend has started to reverse.  Of the couples that married in 2012, only 17% had divorced by 2022. That’s well below the peak in the 1990s.</span></p><p><span><span>Explore our data on marriages and divorces in other countries</span></span><span> →</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#third-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/c9d2e006-4c8b-47c9-501f-178f6b406b00/w=1620" alt="This chart compares the urbanization rates of Bangladesh and its neighboring countries (India, Myanmar, Nepal, and Sri Lanka) from 1972 to 2022. The y-axis represents the percentage of the population living in urban areas, ranging from 0% to 40%. Bangladesh shows the steepest increase, rising from 8% in 1972 to 40% in 2022, surpassing its neighbors. Other countries display slower and steadier growth." loading="lazy" data-filename="bangladesh-urban-mobile.png" width="1620" height="1620"></picture></div><div><p>February 28</p><h3>Bangladesh has been urbanizing much faster than its neighbors</h3><div><p><span>The biggest migration story of the past few centuries has not been from country to country but from rural areas to cities.</span></p><p><span>In 1960, </span><span><span>one-third of the world’s population</span></span><span> lived in urban areas. This share is now closing in on 60%. By contrast, </span><span><span>less than 4%</span></span><span> of the global population are international migrants.</span></p><p><span>But some countries are urbanizing much more quickly than others. Bangladesh is one example of a country that has experienced much faster internal migration than its South Asian neighbors. You can see this on the chart.</span></p><p><span>In 1972, just 8% of people in Bangladesh lived in towns and cities. This share has more than quadrupled to 40%.</span></p><p><span><span>Explore more data on the global movement of populations from rural areas to cities</span></span><span>&nbsp;→</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#fourth-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/8441b81e-466d-407c-e06b-16b6a82f3400/w=1620" alt="A horizontal bar chart titled &quot;Europeans are pessimistic on housing&quot; shows the survey responses from people in various European countries to the question: &quot;In general, do you think that your country is on the right track or the wrong track when it comes to housing?&quot;.

Each bar represents the percentage of responses categorized as: &quot;Wrong Track&quot;, &quot;Don't Know&quot;, and &quot;Right Track&quot; . 

Countries listed from top to bottom include: Netherlands, Spain, Hungary, Germany, Turkey, Great Britain, France, Ireland, Italy, Belgium, Sweden, and Poland. The chart reveals a dominant trend of pessimism, with many countries showing a higher percentage in the &quot;Wrong Track&quot; category. 

The data source is Ipsos (2025)" loading="lazy" data-filename="people-in-many-countries-are-pessimistic-about-housing.png" width="1620" height="1620"></picture></div><div><p>February 27</p><h3>Many Europeans say their nations are on the wrong track with housing</h3><div><p><span>The </span><span><span>Ipsos Housing Monitor 2025</span></span><span> surveyed people across 30 countries, asking: “In general, do you think that your country is on the right track or the wrong track when it comes to housing?”.</span></p><p><span>The chart shows results for European countries, where housing prices dipped after the 2008 global financial crisis, before starting to rise again around 2013, with particularly </span><span><span>large increases since 2015</span></span><span>.</span></p><p><span>The Netherlands and Spain stand out, with nearly 80% believing their country is on the wrong track.</span></p><p><span>People in Poland and Sweden are less concerned than in other countries. But even in these nations, the majority feels like their country is on the wrong track.</span></p><p><span><span>Explore more data on optimism and pessimism about the future</span></span><span> →</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#fifth-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/9c124f8c-3063-4565-3628-2c0679d4af00/w=1620" alt="This chart shows the share of women who have had no births by the end of their childbearing years in four countries: the United States, Sweden, Japan, and Spain. 

Each country's data is represented on separate graphs, plotted against the years from 1918 to 1972 along the horizontal axis, labeled as &quot;Women's birth year.&quot; The vertical axis indicates the percentage of women who have had no births, ranging from 0% to 30%. 

In the United States graph, the percentage starts around 15% in 1918, dips slightly mid-century, and then rises again to near 20% by the early 1970s. 

The Sweden graph shows a relatively stable line around 10–15% throughout the years, with no significant fluctuations.

Japan's graph trends upward, reaching around 25% by the end of the timeline.

In Spain, the share steadily increases, culminating in over 20% by 1972, indicating a growing trend in women having no births.

The data source is cited as the &quot;Human Fertility Database (2024).&quot; The chart is published by Our World in Data." loading="lazy" data-filename="share-of-women-no-births-across-their-childbearing-years.png" width="1620" height="1620"></picture></div><div><p>February 26</p><h3>What share of women reach the end of their childbearing years without having children?</h3><div><p><span>This chart focuses on the share of women who had no births by the end of their childbearing years. The horizontal axis shows the woman’s birth year.</span></p><p><span>Around 18% of those born in the 1910s in the United States had no children. For the following generations who grew up during the “baby boom”, the share with no children dropped to 5%. Since then, this figure has risen and fallen again.</span></p><p><span>In Sweden, the share of women without any children has remained relatively stable at around 12% for women born between the 1950s and 1970s.</span></p><p><span>The trend in Japan and Spain has been different: the share of women with no children has grown steeply over recent generations. In Spain, the figure nearly doubled in a decade: from 10% for women born in 1960 to almost 20% for those born in 1970. In Japan, it almost tripled in twenty years.</span></p><p><span><span>Explore this data for twenty more countries</span></span><span> →</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#sixth-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/06b9c187-4721-457a-7cd9-00fe24c71500/w=1620" alt="The bar chart illustrates changes in household composition in the United States from 1960 to 2015. The chart consists of three horizontal bars, each representing a different year: 1960, 1990, and 2015. 

In 1960, the largest segment is &quot;Couple with children,&quot; which accounts for 43%. Other segments include: &quot;Single parent with children&quot; at 5%, &quot;Couple&quot; at 22%, &quot;Extended family&quot; at 12%, &quot;Non-relatives&quot; at 4%, and &quot;One person&quot; at 13%.

By 1990, the &quot;Couple with children&quot; category has decreased to 30%. The breakdown is: &quot;Single parent with children&quot; at 8%, &quot;Couple&quot; at 24%, &quot;Extended family&quot; at 8%, &quot;Non-relatives&quot; at 5%, and &quot;One person&quot; at 25%.

In 2015, &quot;Couple with children&quot; drops further to 24%, with the segments now being: &quot;Single parent with children&quot; at 9%, &quot;Couple&quot; at 25%, &quot;Extended family&quot; at 9%, &quot;Non-relatives&quot; at 5%, and &quot;One person&quot; increasing to 28%.

Data sources for the chart are cited as United Nations, 2022." loading="lazy" data-filename="us-household-composition.png" width="1620" height="1620"></picture></div><div><p>February 25</p><h3>Solo living has become the most common arrangement for households in the United States</h3><div><p><span>Households in the United States have changed significantly over the last 60 years. In 1960, 43% of households were couples with children, but this had dropped to 24% by 2015.</span></p><p><span>Once a minority, single individuals living alone are now the most common composition, making up 28% of households in 2015.</span></p><p><span>Several factors may explain this shift. Since 2000, most population growth </span><span><span>has occurred</span></span><span> among those over 60, who are more likely to live alone after widowhood or once children leave home. Declining birth rates have further reduced the share of households with children.</span></p><p><span>At the same time, </span><span><span>rising incomes</span></span><span> among </span><span><span>women</span></span><span>, in particular, have made independent living more accessible, likely contributing to the increase in single-person households alongside the trend of </span><span><span>marrying later</span></span><span> or </span><span><span>not at all</span></span><span>.</span></p><p><span><span>Explore how household types compare across different countries</span></span><span> →</span></p></div></div></a><a href="https://ourworldindata.org/data-insights#seventh-most-recent-data-insight"><div><picture><source srcset="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=48 48w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=100 100w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=350 350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=850 850w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=1350 1350w, https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=1620 1620w" type="image/png" sizes="(max-width: 960px) 95vw, (min-width: 960px) 533px"><img src="https://ourworldindata.org/cdn-cgi/imagedelivery/qLq-8BTgXU8yG0N6HnOy8g/0500bed3-b4f7-42c9-b52d-6f074bc3da00/w=1620" alt="The chart titled &quot;Women live longer than men in every country&quot; shows a scatter plot of life expectancy for men and women in 2023, categorized by continent. Each dot represents a country, with its color indicating the continent: Africa, Asia, Europe, North America, Oceania, or South America. The x-axis displays life expectancy for men, while the y-axis shows life expectancy for women. A diagonal line indicates where life expectancy for both genders would be equal. All dots are above this line, meaning women have a higher life expectancy than men in every country. The trend shows increasing life expectancy for both genders, with women consistently living longer. Data source: UN World Population Prospects (2024)." loading="lazy" data-filename="women-live-longer-mobile.png" width="1620" height="1620"></picture></div><div><p>February 24</p><h3>Women live longer than men in every country in the world</h3><div><p><span>In every country in the world, women tend to live longer than men.</span></p><p><span>You can see this in the chart, which shows the average life expectancy of women on the vertical axis and the life expectancy of men on the horizontal axis, both for 2023. Each dot is one country.</span></p><p><span>As you can see, all countries lie </span><em><span>above</span></em><span> the middle line, which means that women's life expectancy was higher than men's.</span></p><p><span>There are various reasons why this gap in life expectancy exists, which my colleagues Saloni Dattani and Lucas Rodés-Guirao </span><span><span>explain in their article</span></span><span>. Typically, births are skewed in favor of males, with around 105 boys being born for every 100 girls. However, throughout childhood, adolescence, and adulthood, mortality rates tend to be higher in males.</span></p><p><span>This data comes from the United Nations’ World Population Prospects.</span></p><p><span><span>Read our article on why women live longer than men</span></span><span>&nbsp;→</span></p></div></div></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek's smallpond: Bringing Distributed Computing to DuckDB (134 pts)]]></title>
            <link>https://mehdio.substack.com/p/duckdb-goes-distributed-deepseeks</link>
            <guid>43248947</guid>
            <pubDate>Tue, 04 Mar 2025 01:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mehdio.substack.com/p/duckdb-goes-distributed-deepseeks">https://mehdio.substack.com/p/duckdb-goes-distributed-deepseeks</a>, See on <a href="https://news.ycombinator.com/item?id=43248947">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:149879,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09a8f44b-d040-454e-8c01-999fe6121507_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>DeepSeek has made a lot of noise lately. Their R1 model, released in January 2025, outperformed competitors like OpenAI’s O1 at launch. But what truly set it apart was its highly efficient infrastructure—dramatically reducing costs while maintaining top-tier performance.</p><p><span>Now, they're coming for data engineers. </span><a href="https://github.com/deepseek-ai" rel="">DeepSeek</a><span> released a bunch of small repositories as independent code modules. </span><a href="https://www.linkedin.com/in/thom-wolf/" rel="">Thomas Wolf</a><span>, Co-founder and Chief of Product at HuggingFace </span><a href="https://www.linkedin.com/posts/thom-wolf_i-want-to-share-bit-of-context-on-todays-activity-7300872211794440192-QJIb?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA0tl2QBJUocRMpCGqvWI8N_YbcsbmkLctY" rel="">shared some of his highlights</a><span>, but we're going to focus on one particularly important project went that unmentioned—</span><strong><a href="https://github.com/deepseek-ai/smallpond" rel="">smallpond</a></strong><span>, a distributed compute framework built on </span><strong><a href="https://duckdb.org/" rel="">DuckDB</a><span>. </span></strong><span>DeepSeek is pushing DuckDB beyond its single-node roots with smallpond, a new, simple approach to distributed computing.</span></p><p>First, having DeepSeek, a hot AI company, using DuckDB is a significant statement, and we'll understand why. Second, we'll dive into the repository itself, exploring their smart approach to enabling DuckDB as a distributed system, along with its limitations and open questions.</p><p><span>I assume you're familiar with DuckDB. I've created </span><a href="https://www.youtube.com/playlist?list=PLIYcNkSjh-0wlrFUE2VvQilLU2aBPns0K" rel="">tons of content around it</a><span>. But just in case, here's a high-level recap.</span></p><blockquote><p><span>For transparency, at the time of writing this blog, I’m a data engineer and DevRel at </span><a href="https://motherduck.com/" rel="">MotherDuck</a><span>. MotherDuck provides a cloud-based version of DuckDB with enhanced features. Its approach differs from what we’ll discuss here, and while I’ll do my best to remain objective, just a heads-up! 🙂</span></p></blockquote><p>DuckDB is an in-process analytical database, meaning it runs within your application without requiring a separate server. You can install it easily in multiple programming languages by adding a library—think of it as the SQLite of analytics, but built for high-performance querying on large datasets.</p><p>It's built in C++ and contains all the integrations you might need for your data pipelines (AWS S3/Google Cloud Storage, Parquet, Iceberg, spatial data, etc.), and it's damn fast. Besides working with common file formats, it has its own efficient storage format—a single ACID-compliant file containing all tables and metadata, with strong compression.</p><p>In Python, getting started is as simple as:</p><pre><code><code>pip install duckdb</code></code></pre><p>Then, load and query a Parquet file in just a few lines:</p><pre><code><code>import duckdb
conn = duckdb.connect()
conn.sql("SELECT * FROM '/path/to/file.parquet'")</code></code></pre><p><span>It also supports reading and writing to </span><a href="https://pandas.pydata.org/docs/index.html" rel="">Pandas</a><span> and </span><a href="https://pola.rs/" rel="">Polars</a><span> DataFrames with zero copy, thanks to Arrow.</span></p><pre><code><code>import duckdb
import pandas

# Create a Pandas dataframe
my_df = pandas.DataFrame.from_dict({'a': [42]})

# query the Pandas DataFrame "my_df"
# Note: duckdb.sql connects to the default in-memory database connection
results = duckdb.sql("SELECT * FROM my_df").df()</code></code></pre><p>We talk a lot about LLM frameworks, models, and agents, but we often forget that the first step in ANY AI project comes down to data.</p><p>Whether it's for training, RAG, or other applications, it all comes down to feeding systems with good, clean data. But how do we even accomplish that step? Through data engineering. Data engineering is a crucial step in AI workflows but is less discussed because it's less "sexy" and less "new."</p><p><span>Regarding DuckDB, we've already seen other AI companies like </span><a href="https://huggingface.co/" rel="">HuggingFace</a><span> using it behind the scenes to quickly serve and explore their datasets library through their </span><a href="https://huggingface.co/docs/hub/en/datasets-viewer" rel="">dataset viewer.</a></p><p><span>Now, DeepSeek is introducing </span><em>smallpond</em><span>, a lightweight open-source framework, leveraging DuckDB to process terabyte-scale datasets in a distributed manner. Their benchmark states: _“Sorted 110.5TiB of data in 30 minutes and 14 seconds, achieving an average throughput of 3.66TiB/min.” </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png" width="1456" height="303" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:303,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:158598,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b3f4a5-7e53-40da-9044-86c44af20dcd_2628x546.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Source : https://github.com/deepseek-ai/smallpond</figcaption></figure></div><p><span>While we've seen DuckDB crushing 500GB on a single node easily (</span><a href="https://benchmark.clickhouse.com/" rel="">clickbench</a><span>), this enters another realm of data size.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png" width="1456" height="258" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:258,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:293920,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F823ded79-b00e-46ee-a9d0-6739ae6beb56_4092x724.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Clickbench benchmark</figcaption></figure></div><p>But wait, isn't DuckDB single-node focused? What's the catch here?</p><p>Let's dive in.</p><p><span>smallpond follows a </span><strong>lazy evaluation</strong><span> approach when performing operations on DataFrames (</span><code>map()</code><span>, </span><code>filter()</code><span>, </span><code>partial_sql()</code><span>, etc.), meaning it doesn’t execute them immediately. Instead, it constructs a </span><strong>logical plan</strong><span> represented as a </span><strong>directed acyclic graph (DAG)</strong><span>, where each operation corresponds to a node in the graph (e.g., </span><code>SqlEngineNode</code><span>, </span><code>HashPartitionNode</code><span>, </span><code>DataSourceNode</code><span>).</span></p><p>Execution is only triggered when an action is called, such as:</p><ul><li><p><code>write_parquet()</code><span> – Write data to disk</span></p></li><li><p><code>to_pandas()</code><span> – Convert to a pandas DataFrame</span></p></li><li><p><code>compute()</code><span> – Explicitly request computation</span></p></li><li><p><code>count()</code><span> – Count rows</span></p></li><li><p><code>take()</code><span> – Retrieve rows</span></p></li></ul><p>This approach optimizes performance by deferring computation until necessary, reducing redundant operations and improving efficiency.</p><p><span>When execution is triggered, the logical plan is converted into an execution plan. The execution plan consists of tasks (e.g., </span><code>SqlEngineTask</code><span>, </span><code>HashPartitionTask</code><span>) that correspond to the nodes in the logical plan. These tasks are the actual units of work that will be distributed and executed through </span><a href="https://ray.io/" rel="">Ray</a><span>.</span></p><p><span>The important thing to understand is that the distribution mechanism in smallpond operates at the Python level with help from </span><a href="https://www.ray.io/" rel="">Ray</a><span>, specifically </span><a href="https://docs.ray.io/en/latest/ray-core/walkthrough.html" rel="">Ray Core</a><span>, through partitions.</span></p><p>A given operation is distributed based on manual partitioning provided by the user. Smallpond supports multiple partitioning strategies:</p><ul><li><p>Hash partitioning (by column values)</p></li><li><p>Even partitioning (by files or rows)</p></li><li><p>Random shuffle partitioning</p></li></ul><p><span>For each partition, a separate DuckDB instance is created within a Ray </span><a href="https://github.com/deepseek-ai/smallpond/blob/ed112db42af4d006a80861d1305a1c22cabdd359/smallpond/execution/task.py#L4" rel="">task</a><span>. Each task processes its assigned partition independently using SQL queries through DuckDB.</span></p><p><span>Given this architecture, you might notice that the framework is tightly integrated with Ray, which comes with a trade-off: it prioritizes </span><strong>scaling out</strong><span> (adding more nodes with standard hardware) over </span><strong>scaling up</strong><span> (improving the performance of a single node).</span></p><p><span>Therefore, you would need to have a Ray cluster. Multiple options exist, but you would have more options today to manage your own cluster through AWS/GCP compute or a Kubernetes cluster. Only </span><a href="https://www.anyscale.com/" rel="">Anyscale</a><span>, the company founded and led by the creators of Ray, offers a fully managed Ray service. Even then, you have the overhead of monitoring a cluster.</span></p><p><span>The great thing here is that the developer experience is nice because you get a local single node when working and only scale when you need to. But the question is: do you actually need to scale out and add the cluster overhead given that the largest machine on </span><a href="https://aws.amazon.com/ec2/instance-types/high-memory/" rel="">AWS today provides 24TB of memory</a><span>?</span></p><p>Ray Core is just for compute - where does the storage live?</p><p><span>While smallpond supports local filesystems for development and smaller workloads, the benchmark on 100TB mentioned is actually using the custom DeepSeek </span><a href="https://github.com/deepseek-ai/3FS" rel="">3FS framework</a><span>: Fire-Flyer File System is a high-performance distributed file system designed to address the challenges of AI training and inference workloads.</span></p><p><span>To put it simply, compared to AWS S3, </span><strong>3FS is built for speed, not just storage</strong><span>. While S3 is a reliable and scalable object store, it comes with higher latency and eventual consistency, making it less ideal for AI training workloads that require fast, real-time data access. 3FS, on the other hand, is a high-performance distributed file system that leverages </span><strong>SSDs and RDMA</strong><span> networks to deliver low-latency, high-throughput storage</span><strong>.</strong><span> It supports </span><strong>random access to training data</strong><span>, </span><strong>efficient checkpointing</strong><span>, and </span><strong>strong consistency</strong><span>, eliminating the need for extra caching layers or workarounds. For AI-heavy workloads that demand rapid iteration and distributed compute, 3FS offers a more optimized, AI-native storage layer—</span><strong>trading off some cost and operational complexity for raw speed and performance</strong><span>.</span></p><p><span>Because this is a specific framework from DeepSeek, you would have </span><a href="https://github.com/deepseek-ai/3FS/blob/main/deploy/README.md" rel="">to deploy your own the 3FS cluster</a><span> if you want to reach the same performance. There's no fully managed option there... or maybe this is an idea for a spinoff startup from DeepSeek? 😉</span></p><p>One interesting experiment would be to test performance at the same scale using AWS S3. However, this implementation is currently missing in smallpond. This approach would be much more practical for an average company needing 100TB of processing capability.</p><p><span>Unlike systems like </span><a href="https://spark.apache.org/" rel="">Spark</a><span> or </span><a href="https://www.getdaft.io/" rel="">Daft</a><span> that can distribute work at the query execution level (breaking down individual operations like joins or aggregations), smallpond operates at a higher level. It distributes entire partitions to workers, and each worker processes its entire partition using DuckDB.</span></p><p>This makes the architecture simpler but potentially less optimized for complex queries that would benefit from operation-level distribution.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png" width="1092" height="808" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:808,&quot;width&quot;:1092,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:145512,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ca0e10-89c4-40a1-81e9-f1bd84cda1be_1092x808.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Distributed compute levels - Image by the Author</figcaption></figure></div><p>Let's recap the features of smallpond :</p><ul><li><p><strong>Lazy evaluation with DAG-based execution</strong><span> – Operations are deferred until explicitly triggered.</span></p></li><li><p><strong>Flexible partitioning strategies</strong><span> – Supports hash, column-based, and row-based partitioning.</span></p></li><li><p><strong>Ray-powered distribution</strong><span> – Each task runs in its own DuckDB instance for parallel execution.</span></p></li><li><p><strong>Multiple storage layer options</strong><span> – Benchmarks have primarily been conducted using 3FS.</span></p></li><li><p><strong>Cluster management trade-off</strong><span> – Requires maintaining a compute cluster, though fully managed services like Anyscale can mitigate this.</span></p></li><li><p><strong>Potential 3FS overhead</strong><span> – Self-managing a 3FS cluster introduce significant additional complexity. </span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png" width="1456" height="938" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:938,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:166238,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8638d9c6-c5e0-4026-8672-9d729ffdee81_1823x1174.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>High-level-design of smallpond Image by the Author</figcaption></figure></div><p>Another approach to distributed computing with DuckDB is through serverless functions like AWS Lambda. Here, the logic is often even simpler than partitioning, typically processing by file. Or you could decide to process per partition with some wrapper, but you won't be able to go much further than file-by-file processing.</p><p><span>Okta implemented this approach, and you can read more on </span></p><p><span> blog: </span><a href="https://juhache.substack.com/p/oktas-multi-engine-data-stack" rel="">Okta's Multi-Engine Data Stack</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg" width="1456" height="555" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:555,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:147088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://mehdio.substack.com/i/158100947?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5f51204-1040-41d0-b26a-8f5fbb2ac930_1622x618.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>from Julien Hurault’s blog Okta’s Multi-Engine Data Stack</figcaption></figure></div><p><span>Finally, MotherDuck, </span><a href="https://motherduck.com/docs/concepts/architecture-and-capabilities/#dual-execution" rel="">is working on dual execution</a><span>, balancing between local and remote compute for optimize resources usage.</span></p><p>All in all, it's exciting to see that DuckDB is being used in AI-heavy workloads and that people are getting creative on how to split the compute when needed.</p><p>smallpond, while being restricted to a specific tech stack for distributing compute, aims to be simple, which aligns with the philosophy of DuckDB 👏</p><p>It's also a good reminder that there are multiple ways to scale DuckDB. Scaling up is always the simpler approach, but with smallpond and other examples mentioned here, we have plenty of options.</p><p>This approach makes sense nowadays rather than having to rely on complex and heavy distributed frameworks by default "just in case." These not only hurt your cloud costs when starting with small/medium data but also has a tax on developer experience (still love you, Apache Spark ❤️).</p><p><span>While we have powerful single-node solutions that would be enough for most use cases, </span><a href="https://www.linkedin.com/posts/mehd-io_dataengineering-activity-7298333190694293504-_B_f?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA0tl2QBJUocRMpCGqvWI8N_YbcsbmkLctY" rel="">especially if you're in the 94% of use cases under 10TB according to Redshift</a><span>, we now have even more options to make the Duck fly.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. pauses all military aid to Ukraine (311 pts)]]></title>
            <link>https://www.wsj.com/politics/national-security/u-s-hitting-brakes-on-flow-of-arms-to-ukraine-980a71d1</link>
            <guid>43248919</guid>
            <pubDate>Tue, 04 Mar 2025 01:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/politics/national-security/u-s-hitting-brakes-on-flow-of-arms-to-ukraine-980a71d1">https://www.wsj.com/politics/national-security/u-s-hitting-brakes-on-flow-of-arms-to-ukraine-980a71d1</a>, See on <a href="https://news.ycombinator.com/item?id=43248919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><section><p data-type="paragraph">The U.S. will pause all military aid to Kyiv until President Trump determines that Ukrainian President <!-- -->Volodymyr Zelensky<!-- --> is making a good-faith effort toward <a data-type="link" href="https://www.wsj.com/world/europe/trump-russia-ukraine-peace-plan-istanbul-protocols-31808a75?mod=article_inline" target="_blank" rel="">peace negotiations with Russia</a>, according to a White House official.</p><p data-type="paragraph">“The president has been clear that he is focused on peace. We need our partners to be committed to that goal as well. We are pausing and reviewing our aid to ensure that it is contributing to a solution,” a White House official said in a statement.</p></section><p>Copyright ©<!-- -->2025<!-- --> Dow Jones &amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8</p></div><div><p><h2>Videos</h2></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Public health data disappeared. RestoredCDC.org is bringing it back (359 pts)]]></title>
            <link>https://www.RestoredCDC.org</link>
            <guid>43248610</guid>
            <pubDate>Tue, 04 Mar 2025 00:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.RestoredCDC.org">https://www.RestoredCDC.org</a>, See on <a href="https://news.ycombinator.com/item?id=43248610">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<section>
	<h2>Featured Topics</h2>
			<a href="https://www.restoredcdc.org/respiratory-viruses/about/index.html">Respiratory Illnesses</a>
			<a href="https://www.restoredcdc.org/mpox/outbreaks/2023/">Mpox Outbreak</a>
			<a href="https://www.restoredcdc.org/oropouche/outbreaks/2024/index.html">Oropouche Outbreak</a>
			<a href="https://www.restoredcdc.org/food-safety/prevention/index.html">Four Steps to Food Safety</a>
			<a href="https://www.restoredcdc.org/adenovirus/about/index.html">Adenovirus</a>
			<a href="https://www.restoredcdc.org/obesity/risk-factors/risk-factors.html">Risk Factors for Obesity</a>
			<a href="https://www.restoredcdc.org/winter-weather/safety/index.html">Winter Weather</a>
	</section>
<section>
	<h2>A to Z</h2>
	<div>
			<div>
				<p>Find diseases and conditions; healthy living; workplace safety; environmental health; injury, violence and safety; global health; travelers’ health and more.</p>
				<ol>
					<li><a href="https://www.restoredcdc.org/health-topics.html#A">A</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#B">B</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#C">C</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#D">D</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#E">E</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#F">F</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#G">G</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#H">H</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#I">I</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#J">J</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#K">K</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#L">L</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#M">M</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#N">N</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#O">O</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#P">P</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#Q">Q</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#R">R</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#S">S</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#T">T</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#U">U</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#V">V</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#W">W</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#X">X</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#Y">Y</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html#Z">Z</a></li>
					<li><a href="https://www.restoredcdc.org/health-topics.html##">#</a></li>
				</ol>
			</div>
			<div>
				<a href="https://www.restoredcdc.org/common-cold/about/">
					<img src="https://www.restoredcdc.org/homepage/images/common-cold.jpg" alt="Health Topics">
					<p>Common Cold</p>
				</a>
			</div>
		</div>
</section>
<section>
	<h2>
		<span>News</span>
		<a href="https://www.restoredcdc.org/media/">All News <i aria-label="All News"></i></a>
	</h2>
	<ol>
							<li>
						<a href="https://www.restoredcdc.org/media/releases/2024/m1218-h5n1-flu.html">
							<p><span>‎‎DEC</span>
								<span>‎‎18</span>
							</p>
							<p>
								CDC Confirms First Severe Case of H5N1 Bird Flu in the United States							</p>
						</a>
					</li>
										<li>
						<a href="https://www.restoredcdc.org/media/releases/2024/a1129-salmonella-outbreak.html">
							<p><span>‎‎NOV</span>
								<span>‎‎29</span>
							</p>
							<p>
								CDC warns of a <em>Salmonella </em>outbreak linked to cucumbers							</p>
						</a>
					</li>
										<li>
						<a href="https://www.restoredcdc.org/media/releases/2024/m1122-listeria-outbreak.html">
							<p><span>‎‎NOV</span>
								<span>‎‎22</span>
							</p>
							<p>
								CDC warns of Listeria linked to ready-to-eat meat and poultry products							</p>
						</a>
					</li>
										<li>
						<a href="https://www.restoredcdc.org/media/releases/2024/p1122-h5n1-bird-flu.html">
							<p><span>‎‎NOV</span>
								<span>‎‎22</span>
							</p>
							<p>
								CDC confirms H5N1 Bird Flu Infection in a Child in California							</p>
						</a>
					</li>
										<li>
						<a href="https://www.restoredcdc.org/media/releases/2024/a1117-ecoli-organic-carrots.html">
							<p><span>‎‎NOV</span>
								<span>‎‎17</span>
							</p>
							<p>
								CDC warns of new <em>E. coli </em>outbreak linked to organic carrots							</p>
						</a>
					</li>
						</ol>
</section>
<section>
	<h2>Scientific Journals</h2>
	<div>
		<div>
			<a href="https://www.restoredcdc.org/mmwr/index.html">
				<img src="https://www.restoredcdc.org/TemplatePackage/5.0/img/homepage/mmwr.png" alt="MMWR">
				<div>
					<p>MMWR is a weekly epidemiological digest that provides timely, reliable, objective, and useful public health information.</p>
					<p><span>Learn More <i></i></span>
				</p></div>
			</a>
		</div>
		<div>
			<a href="https://www.restoredcdc.org/wwwnc.cdc.gov/eid/">
				<img src="https://www.restoredcdc.org/TemplatePackage/5.0/img/homepage/eid.png" alt="EID">
				<div>
					<p>EID is a monthly peer reviewed journal covering infectious diseases with emphasis on disease prevention, control, and elimination.</p>
					<p><span>Learn More <i></i></span>
				</p></div>
			</a>
		</div>
		<div>
			<a href="https://www.restoredcdc.org/pcd/index.htm">
				<img src="https://www.restoredcdc.org/TemplatePackage/5.0/img/homepage/pcd.png" alt="PCD">
				<div>
					<p>PCD is a peer reviewed journal covering research, public health findings, innovations, and practices on chronic diseases.</p>
					<p><span>Learn More <i></i></span>
				</p></div>
			</a>
		</div>
	</div>
</section>

<div>
			<div>
				<p><img src="https://www.restoredcdc.org/homepage/images/science-cdc.jpg" alt="Science at CDC">
				</p>
			</div>
			<div>
					<h2>Science at CDC</h2>
					<p>To make science and data easier for broad audiences to interpret, CDC is translating science into practical, easy to understand policy by clarifying and presenting scientific language so that anyone can understand it and standardizing guideline development across the agency.</p>
<ul>
<li><a href="https://www.restoredcdc.org/about/cdc-moving-forward.html">CDC Moving Forward</a></li>
<li><a href="https://www.restoredcdc.org/about/priorities/index.html">Advancing Science &amp; Health Equity</a></li>
</ul>
									</div>
		</div>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lawrence of Arabia, Paul Atreides, and the roots of Frank Herbert's Dune (2021) (215 pts)]]></title>
            <link>https://reactormag.com/lawrence-of-arabia-paul-atreides-and-the-roots-of-frank-herberts-dune/</link>
            <guid>43248429</guid>
            <pubDate>Tue, 04 Mar 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reactormag.com/lawrence-of-arabia-paul-atreides-and-the-roots-of-frank-herberts-dune/">https://reactormag.com/lawrence-of-arabia-paul-atreides-and-the-roots-of-frank-herberts-dune/</a>, See on <a href="https://news.ycombinator.com/item?id=43248429">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>At first glance, Frank Herbert’s <em>Dune </em>(1965) might appear to be a mere copy of the story of Lawrence of Arabia with some science-fictional window dressing. Several critics have pointed to the similarities between Lawrence and Paul Atreides—both are foreign figures who immerse themselves in a desert culture and help lead the locals to overthrow their oppressors.</p>
<p>The 1962 film based on a romanticized version of Lawrence’s journey, <a href="https://www.imdb.com/title/tt0056172/"><em>Lawrence of Arabia </em>(directed by David Lean)</a>, was critically acclaimed and widely popular. It rested on the idea of the ‘white savior,’ whose role was to lend a sympathetic ear to oppressed peoples and provide assistance to improve their lot in life. Released at a time when U.S. relations in the Middle East were becoming more complicated and the Cold War was reaching new heights of tension, this offered a potentially reassuring message that Western involvement in foreign affairs could be heroic and therefore welcomed.</p>

<p>Herbert himself was very interested in exploring desert cultures and religions. As part of his extensive research and writing process, he read hundreds of books, including T.E. Lawrence’s wartime memoir, <em>Seven Pillars of Wisdom: A Triumph</em> (1926) [Brian Herbert, <em>Dreamer of </em>Dune, Tom Doherty Associates, 2003] He saw messianic overtones in Lawrence’s story and the possibility for outsiders to manipulate a culture according to their own purposes. [Timothy O’Reilly, <em>Frank Herbert</em>, Frederick Ungar Publishing, 1981]</p>
<p>Yet, although Lawrence’s narrative was certainly an inspiration for key aspects of <em>Dune</em>, there are also critical contrasts in the portrayals of Lawrence and Paul, the Arabs and the Fremen, women, and religion. What follows is a discussion of some similarities and differences between the fictional world of <em>Dune </em>and the worlds in <em>Seven Pillars of Wisdom</em> as filtered through Lawrence’s recollections of his time as a go-between figure in the British and Arab camps during World War I. This overview will demonstrate how Herbert adapted and modified elements of Lawrence’s story to create a world in <em>Dune </em>that is both familiar and new.</p>

<h4>Introducing Lawrence</h4>
<p>The subject of over 70 biographies and multiple films, plays, and other writings, T.E. Lawrence is a household name for many in the West. [Scott Anderson, “<a href="https://www.smithsonianmag.com/history/true-story-lawrence-arabia-180951857/?page=1">The True Story of Lawrence of Arabia</a>,” <em>Smithsonian Magazine</em>, 2014] He was an officer in the British Army during WWI who served as an adviser to the Arabs and helped in their revolt against the Turks, though the extent of his influence is disputed among historians. [Stanley Weintraub, “<a href="https://www.britannica.com/biography/T-E-Lawrence">T.E. Lawrence</a>,” Encyclopedia Britannica, 2020] Other figures, such as British archaeologist and writer Gertrude Bell, were better known at the time and arguably had a greater impact on Middle Eastern politics. [Georgina Howell, <em>Queen of the Desert: The Extraordinary Life of Gertrude Bell</em>, Pan Books, 2015] But after American journalist Lowell Thomas seized upon Lawrence’s story in 1918, Lawrence’s fame grew to eclipse that of his contemporaries.</p>
<p>Interestingly, whether or not others consider Lawrence of Arabia to be a hero, Lawrence does not portray himself that way in <em>Seven Pillars of Wisdom</em>. Instead, he appears as a conflicted man, trying to bridge two worlds but feeling like a fraud. On the one hand, he explains the ways in which he becomes like one of the Arabs: in dress, in mannerisms, and in the ability to appreciate desert living. He takes some pleasure in being hardier and more knowledgeable than his fellow British associates.</p>
<p>On the other hand, there are varying degrees of contempt in his descriptions of the Arabs and their differences from the British. Filtering his experiences through his British sensibilities creates a sense of superiority at times that adds to the cultural barrier he faces. Although Lawrence himself may have been accepted and respected by his Arab companions, the image of Lawrence of Arabia is problematic for its implication that native peoples need a ‘white savior’ to rescue them from their oppression.</p>
<p>This continues to be a topic of debate in relation to <em>Dune</em>, as shown, for example, in Emmet Asher-Perrin’s Tor.com article <a href="https://reactormag.com/2019/03/06/why-its-important-to-consider-whether-dune-is-a-white-savior-narrative/">Why It’s Important to Consider Whether <em>Dune</em> Is a White Savior Narrative</a>.</p>

<h4>Lawrence of Arabia</h4>
<figure id="attachment_647163" aria-describedby="caption-attachment-647163"><img decoding="async" src="https://reactormag.com/wp-content/uploads/2021/05/TE-Lawrence-of-Arabia-1.jpg" alt="" width="500" height="667"><figcaption id="caption-attachment-647163">Photo of T.E. Lawrence taken by American journalist Lowell Thomas in 1919</figcaption></figure>
<p>Both Lawrence and Paul appear to be men raised in Western cultures who adopt the ways of a Middle Eastern culture in order to blend in and meet their goal of rallying a fighting force to meet their own (imperial) goals. They understand the importance of desert power and act as a bridge between the two worlds they inhabit to facilitate the use of this force.</p>
<p>Looking first at Lawrence, he admits early on that his book is not a history of the Arab movement but of himself in the movement. It is about his daily life and encounters with people, with the war providing a sense of purpose to structure the narrative. In short, this purpose is to convince enough Arab tribes to side with Prince Feisal against the Turks to defeat them. It means persuading the tribes to put aside their grudges and vendettas, and sometimes their ways of tribal justice, to form a cohesive front.</p>
<p>Lawrence already knows Arabic and how to wear the skirts and head-cloth of the Arab outfit, but he gains a deeper understanding of the language and culture through his experience traveling in the Middle East. For example, he discovers how important it is to have a broad knowledge of the various peoples who live in the desert if one wants to be accepted as an insider: “In the little-peopled desert every worshipful man knew every other; and instead of books they studied their generation. To have fallen short in such knowledge would have meant being branded either as ill-bred, or as a stranger; and strangers were not admitted to familiar intercourse or councils, or confidence.” [Lawrence, p 416-417*] He is used to book knowledge being valued. Now he must adjust to picking up information tidbits to gain the trust of new tribes and persuade them to his and Feisal’s cause.</p>
<p>In terms of clothing, Lawrence comes to accept the Arab dress as “convenient in such a climate” and blends in with his Arab companions by wearing it instead of the British officer uniform. [Lawrence, p 111] This reduces the sense that he is from a different culture and way of life. He learns the advantages of “going bare foot” to gain a better grip on tough terrain but also the pain of having no shoe protection in rocky or snowy terrain. [Lawrence, p 486] He writes of the incredulity of Egyptian and British military police in Cairo when he answers their questions in Arabic with fluent English: “They looked at my bare feet, white silk robes and gold head-rope and dagger…I was burned crimson and very haggard with travel. (Later I found my weight to be less than seven stone [44 kg/98 lb]).” [Lawrence, p 327-328] Here Lawrence paints a picture of himself as seen through their eyes—a scrawny, sunburned, barefoot leader dressed like an Arab but speaking English like a British person.</p>
<p>Sometimes his transformation leads to feelings of shame, showing Lawrence’s discomfort with the idea that he has ‘gone native.’ At the close of the book, once Damascus has been conquered, he has an unusual encounter with a medical major:</p>
<p>With a brow of disgust for my skirts and sandals he said, ‘You’re in charge?’ Modestly I smirked that in a way I was, and then he burst out, ‘Scandalous, disgraceful, outrageous, ought to be shot…’ At this onslaught I cackled out like a chicken, with the wild laughter of strain…I hooted out again, and he smacked me over the face and stalked off, leaving me more ashamed than angry, for in my heart I felt he was right, and that anyone who pushed through to success a rebellion of the weak against their masters must come out of it so stained in estimation that afterward nothing in the world would make him feel clean. However, it was nearly over. [Lawrence, p 682]</p>
<p>While the medical major is disgusted at Lawrence’s Arab appearance and thinks he has sullied himself, Lawrence seems to feel ashamed of having taken on this appearance as a way of manipulating the Arabs to rebel. He feels dirtied by his role but knows that his part in this performance is almost over.</p>
<p>The strategic advantage that Lawrence identifies is that the Arabs are on their own turf and can engage in guerilla-style attacks, then retreat into the desert with minimal casualties. Throughout <em>Seven Pillars</em>, Lawrence describes how he led small groups of men to sabotage the Turks’ transportation and communication networks by installing explosives in key parts of the railway such as bridges. Their ability to quickly maneuver on camels and disappear made them difficult targets to anticipate or defend against. He makes a comparison between this ‘desert power’ and naval power, which the British were very familiar with:</p>
<p>‘He who commands the sea is at great liberty, and may take as much or as little of the war as he will.’ And we commanded the desert. Camel raiding parties, self-contained like ships, might cruise confidently along the enemy’s cultivation-frontier, sure of an unhindered retreat into their desert-element which the Turks could not explore. [Lawrence, p 345]</p>
<p>As a fighting force, the camels were also formidable. Lawrence says that “a charge of ridden camels going nearly thirty miles an hour was irresistible.” [Lawrence, p 310] Another advantage was that the Arabs’ numbers were constantly in flux due to a reliance on a mixture of tribes rather than one main armed force. This meant “No spies could count us, either, since even ourselves had not the smallest idea of our strength at any given moment.” [Lawrence, p 390] Lawrence’s narrative shows his appreciation for this way of waging war and how much his thinking adapts in response to his new environment.</p>

<h4>Paul Muad’Dib</h4>
<p>How does this picture of Lawrence transformed into Lawrence of Arabia compare with the characterization of Paul Atreides in <em>Dune</em>?</p>
<p>Paul is also raised in a Western-like style yet able to adopt the ways of a foreign people with relative ease. He is curious about the “will-o’-the-sand people called Fremen” even before he moves from Caladan to Arrakis. [Herbert, p 5*] Once there, he relies on his training as the son of a duke and a Bene Gesserit to understand and adapt to the local culture.</p>
<p>Paul somehow knows how to properly fit a stillsuit on his first try, as if it were already natural to him. His knowledge and intelligence impress the Imperial Planetologist Dr. Liet Kynes, who believes Paul fits with the legend: “<em>He shall know your ways as though born to them</em>.” [Herbert, p 110] Compare this with a passage from <em>Seven Pillars</em>: “Now as it happened I had been educated in Syria before the war to wear the entire Arab outfit when necessary without strangeness, or sense of being socially compromised.” [Lawrence, p 111] Unlike Lawrence, Paul has the advantage of his growing prescience to give him special foreknowledge of how to adjust to his new environment, as well as a savior narrative to align with. But both are able to take on the garb of a different culture relatively smoothly.</p>
<p>Besides dress, their outward attitude toward the foreigners they find themselves among is similar. Lawrence states idealistically that “I meant to make a new nation, to restore a lost influence, to give twenty millions of Semites the foundation on which to build an inspired dream-palace of their national thoughts.” [Lawrence, p 23] Once among the Fremen, Paul is named Paul Muad’Dib and Usul and learns how to live according to their cultural norms and values. He presumes to help train and lead the Fremen so they can fight against their common enemy, the Harkonnen, and turn Arrakis into a water-filled paradise. But both figures admit that what they actually need is a fighting force. The promise of independence they hold out is thus a means to an end.</p>
<p>The idea of desert power in Lawrence’s story also appears in <em>Dune</em>. Duke Leto informs his son, Paul, of this shift in how to maintain control of their new planet. He tells Paul, “On Caladan, we ruled with sea and air power…Here, we must scrabble for desert power.” [Herbert, p 104] Later, Paul shows that he has accepted this as his own strategy: “Here, it’s <em>desert power</em>. The Fremen are the key.” [Herbert, p 204] Just as the Turks were constantly stymied by the Arab attacks on their equipment and forces, the Harkonnen find themselves with severe losses due to the Fremen raids. Their underestimation of the Fremen leaves them vulnerable. By the time they acknowledge that they have been losing five troops to every one Fremen, it is too late.</p>
<p>Herbert gives the Fremen on their sandworms a final dramatic military maneuver when they ride in to attack the Emperor after using atomics to blow open the Shield Wall. Just like the camels that Lawrence describes create an “irresistible” charge during battle, the sandworms handily plow through the Emperor’s forces in their surprise appearance.</p>
<p>Compare Lawrence’s description of the camel-mounted forces surrounding him at an honor march with Herbert’s scene:</p>
<blockquote>
<p>…the forces behind us swelled till there was a line of men and camels winding along the narrow pass towards the watershed for as far back as the eye reached…behind them again the wild mass of twelve hundred bouncing camels of the bodyguard, packed as closely as they could move, the men in every variety of coloured clothes and the camels nearly as brilliant in their trappings. We filled the valley to its banks with our flashing stream. [Lawrence, p 144-145]</p>
<p>Out of the sand haze came an orderly mass of flashing shapes—great rising curves with crystal spokes that resolved into the gaping mouths of sandworms, a massed wall of them, each with troops of Fremen riding to the attack. They came in a hissing wedge, robes whipping in the wind as they cut through the melee on the plain. [Herbert, p 464]</p>
</blockquote>
<p>Both passages give a sense of the magnitude of these mounted forces prepared to do battle. They even use similar imagery: a “flashing stream” and “flashing shapes,” a “wild mass” and “a massed wall.” To any enemy who had discounted the desert dwellers as merely a pest, these mounted forces prove the error in that assumption.</p>
<p>Like Lawrence, by bringing new insights, training, and “skilled assistance,” Paul aids local efforts to achieve victory. [Lawrence, p 113] He also holds a more expansive vision of what can be achieved, and acts as a bridge between the worlds of the Fremen and the Imperium. This is how Paul becomes a Lawrence of Arabia figure, and the clear parallels between the desert in Dune and the Middle East only add to this sense.</p>

<h4>Differing Emotions</h4>
<p>Despite their similarities, Lawrence appears much more conflicted than Paul about his role in adopting the ways of a foreign people and assuming such great authority over them. His anxiety is peppered throughout <em>Seven Pillars</em> as he describes his attempt to inhabit two worlds.</p>

<h4>A Conflicted Man</h4>
<p>Lawrence admits that he is unprepared for the large role he is given in the Middle East during WWI, but out of duty or other reasons he stays the course. He says, “I was unfortunately as much in command of the campaign as I pleased, and was untrained.” [Lawrence, p 193] When he is told to return to Arabia and Feisal after believing he was done in the region, he notes that this task goes against his grain—he is completely unfit for the job, he hates responsibility, and he is not good with persuading people. His only knowledge of soldiering is as a student at Oxford reading books about Napoleon’s campaigns and Hannibal’s tactics. Yet he is still forced to go and “take up a role for which I felt no inclination.” [Lawrence, p 117]</p>
<p>Deeper into the 700-page memoir, Lawrence writes more specifically and frequently about feeling like a fraud and trying to serve two masters. He foreshadows his conflictions early on, believing that “In my case, the effort for these years to live in the dress of Arabs, and to imitate their mental foundation, quitted me of my English self, and let me look at the West and its conventions with new eyes: they destroyed it all for me. At the same time I could not sincerely take on the Arab skin: it was an affectation only.” [Lawrence, p 30]</p>
<p>Although he gains a new perspective on his own culture, he acknowledges that his role was part of a performance. He knows that “I must take up again my mantle of fraud in the East…It might be fraud or it might be farce: no one should say that I could not play it.” [Lawrence, p 515] This means having to present different faces to the British and the Arabs, and he knows the latter will necessarily suffer in the face of the former’s might. He says, “Not for the first or last time service to two masters irked me… Yet I could not explain to Allenby the whole Arab situation, nor disclose the full British plan to Feisal… Of course, we were fighting for an Allied victory, and since the English were the leading partners, the Arabs would have, in the last resort, to be sacrificed for them. But was it the last resort?” [Lawrence, p 395] In one instance, he feels homesick and like an outcast among the Arabs, someone who has “exploited their highest ideals and made their love of freedom one more tool to help England win.” [Lawrence, p 560]</p>
<p>The words he uses paint a dismal picture of his complicity in winning the Arabs’ trust. He believes that “I was raising the Arabs on false pretenses, and exercising a false authority over my dupes” and that “the war seemed as great a folly as my sham leadership a crime.” [Lawrence, p 387] Again he calls them “our dupes, wholeheartedly fighting the enemy” but still the “bravest, simplest and merriest of men.” [Lawrence, p 566]</p>
<figure id="attachment_647169" aria-describedby="caption-attachment-647169"><img decoding="async" src="https://reactormag.com/wp-content/uploads/2021/05/Feisal-Lawrence-at-Versailles-740x555.jpg" alt="" width="740" height="555" srcset="https://reactormag.com/wp-content/uploads/2021/05/Feisal-Lawrence-at-Versailles-740x555.jpg 740w, https://reactormag.com/wp-content/uploads/2021/05/Feisal-Lawrence-at-Versailles-140x105.jpg 140w, https://reactormag.com/wp-content/uploads/2021/05/Feisal-Lawrence-at-Versailles-768x576.jpg 768w, https://reactormag.com/wp-content/uploads/2021/05/Feisal-Lawrence-at-Versailles.jpg 1024w" sizes="(max-width: 740px) 100vw, 740px"><figcaption id="caption-attachment-647169">Prince Feisal (front, center) and T.E. Lawrence (right of Feisal) at Paris Peace Conference in 1919</figcaption></figure>
<p>It especially seems to bother him that he is a foreigner—from a large colonial power, no less—preaching to them about the need for national freedom. He states, “When necessary, I had done my share of proselytizing fatigues, converting as best I could; conscious all the time of my strangeness, and of the incongruity of an alien’s advocating national liberty.” [Lawrence, p 458] He calls himself “the stranger, the godless fraud inspiring an alien nationality” who hopes “to lead the national uprising of another race, the daily posturing in alien dress, preaching in alien speech.” [Lawrence, p 564, 514]</p>
<p>Such feelings prey on his mind and make him fearful of being left with his thoughts: “My will had gone and I feared to be alone, lest the winds of circumstance, or power, or lust, blow my empty soul away.” [Lawrence, p 514] He also suspects that there must be something in him that enabled such a duplicitous performance: “I must have had some tendency, some aptitude, for deceit, or I would not have deceived men so well, and persisted two years in bringing to success a deceit which others had framed and set afoot…Suffice it that since the march to Akaba I bitterly repented my entanglement in the movement, with a bitterness sufficient to corrode my inactive hours, but insufficient to make me cut myself clear of it.” [Lawrence, p 569]</p>
<p>But Lawrence still finds himself craving a good reputation among others and feeling guilty that he of all people should have one. He sees that “Here were the Arabs believing me, Allenby and Clayton trusting me, my bodyguard dying for me: and I began to wonder if all established reputations were founded, like mine, on fraud.” [Lawrence, p 579]</p>

<h4>A Confident Man</h4>
<p>The reflections on fraudulence and guilt in Lawrence’s book stand out as aspects that are mostly absent in the characterization of Paul in <em>Dune</em>. Paul does have some fears about his ability to prevent the jihad he foresees. But he appears fully able to reconcile his position as a duke in exile with his position as a leader among the Fremen who supposedly has their interests at heart. In comparison to Lawrence, Paul appears overly confident and unbothered by his use of foreign forces to gain authority and territorial rule.</p>
<p>As discussed above, Paul is explicitly told by his father about the importance of desert power. He seems to think his status entitles him to not only secure safety and survival among the Fremen, but to convince them to sacrifice themselves to help him reclaim his House’s ruling authority. And his plan is made even smoother by the fact that the way has already been paved by the Bene Gesserit’s Missionaria Protectiva for him to be accepted as a messiah figure.</p>
<p>Despite Paul seeing the likelihood of a terrible jihad waged by a combination of Atreides forces and Fremen warriors, there is little indication of an effort to take a different path. Paul describes how he “suddenly saw how fertile was the ground into which he had fallen, and with this realization, the terrible purpose filled him.” [Herbert, p 199] He foresees a path with “peaks of violence…a warrior religion there, a fire spreading across the universe with the Atreides green and black banner waving at the head of fanatic legions drunk on spice liquor.” [Herbert, p 199] He even seems to blame the Fremen for this at times. For example, he feels that “this Fremen world was fishing for him, trying to snare him in its ways. And he knew what lay in that snare—the wild jihad, the religious war he felt he should avoid at any cost.” [Herbert, p 346-347]</p>
<p>Somewhat arrogantly, he believes that he is the only one who can prevent this from happening. On the day of his sandworm riding test, “Half pridefully, Paul thought: <em>I cannot do the simplest thing without its becoming a legend…every move I make this day. Live or die, it is a legend. I must not die. Then it will be only legend and nothing to stop the jihad</em>.” [Herbert, p 388] On seeing the Fremen leader Stilgar transformed into “a receptacle for awe and obedience” toward him, Paul tells himself, “<em>They sense that I must take the throne…But they cannot know I do it to prevent the jihad</em>.” [Herbert, p 469]</p>
<p>Yet he, along with his mother, are the ones who train the Fremen to become even more skilled warriors, and he invites them to defeat not only the Harkonnen but the Emperor himself. Thus, Paul conveniently overlooks his own actions which directly contribute to this outbreak of violence across the universe. It is only toward the end of the book that he recognizes his role: “And Paul saw how futile were any efforts of his to change any smallest bit of this. He had thought to oppose the jihad within himself, but the jihad would be. His legions would rage out from Arrakis even without him. They needed only the legend he already had become. He had shown them the way.” [Herbert, p 482]</p>
<p>Whereas Lawrence reveals increased feelings of guilt during his time among the Arabs, Paul appears more and more confident, buoyed by his prescient abilities and victories over his enemies. And although both <em>Seven Pillars of Wisdom </em>and <em>Dune </em>have arguably successful endings for the peoples that have received external assistance, there is a sense that Lawrence is relieved that he can relinquish his position of authority, while Paul is triumphant at his rising power. He also displays his sense of ownership and control over the Fremen as a people, unequivocally stating that “The Fremen are mine.” [Herbert, p 489]</p>
<p>This represents a clear difference between these two men and how they process responsibility and authority. Paul is indeed a Lawrence of Arabia-type character, but appears to be absolved of the sense of fraudulence and guilt that Lawrence returns to again and again in his reflections.</p>

<h4>Orientalizing Tendencies</h4>
<p>There are also differences in Lawrence’s account of the Arabs as compared to Paul’s understanding of the Fremen. Although both use stereotypes, Lawrence’s descriptions have a greater tendency to contain Orientalist attitudes about non-Western cultures.</p>
<p>In brief, according to the famous Palestinian American academic Edward Said, Orientalism refers to the way that Westerners have historically set up a distinction between East and West, Orient and Occident, without acknowledging that this is a human-created construct that strengthens the power of the West. [<em>Orientalism</em>, Vintage, (first ed 1978) 2003] This perpetuates the idea that the West is superior to the East and reinforces stereotypes about who is civilized and who is human. In an Orientalist perspective, there is an “absolute and systematic difference between the West, which is rational, developed, humane, superior, and the Orient, which is aberrant, undeveloped, inferior.” [Said, p 300]</p>
<p>Said’s theory has been widely used in academic circles to analyze concepts such as imperialism, colonialization, and racism. It is also used as a lens to analyze cultural products like books, films, and advertising. Because Said specifically focuses on the Middle East and depictions of Arabs in his work, it is particularly useful in examining texts related to these.</p>

<h4>The Arabs</h4>
<p>Having spent extended periods of time living with various Arab groups, Lawrence is able to move past some stereotypes. As discussed above, there are certainly aspects of the Arabs that he finds beneficial. Although the living conditions can be difficult, he displays a certain amount of respect for the way the nomads, in particular, have carved out a living through use of dress, camels, wells, and other adaptations to the landscape and climate. He himself adopts their ways and language and communicates with them about complex military operations.</p>
<p>Certain men he describes favorably, such as Prince Feisal: “In appearance he was tall, graceful and vigorous, with the most beautiful gait, and a royal dignity of head and shoulders.” [Lawrence, p 98] Another leader he characterizes with less positive language: “Nuri, the hard, silent, cynical old man, held the tribe between his fingers like a tool.” [Lawrence, p 641]</p>
<p>Lawrence is more neutral in tone about his observations regarding how the Arabs organize themselves. He portrays the tribal structure and lack of hierarchy as somewhat of a double-edged sword. On the one hand, society is more egalitarian and “there were no distinctions, traditional or natural.” [Lawrence, p 161] This means that a leader must earn their position through merit and share the experiences of living and eating with those in their ranks.</p>
<p>On the other hand, it means that they are less likely to form the kind of large, disciplined armies that nations like Britain use for conquest and control. Lawrence explains how it takes Feisal two years to settle all of the blood feuds in the region so that different tribes can unite in war against the Turks. Because their “idea of nationality was the independence of clans and villages,” it is more challenging to ask them to view themselves as part of an Arab nation. [Lawrence, p 103]</p>
<p>Lawrence’s descriptions of the Arabs as a people show the type of Orientalist tendencies that Said criticizes. Lawrence claims that they are a simple people, willing believers, and undisciplined fighters who need leadership and guidance to harness their potential. He also sometimes uses the language of savagery, perhaps in an attempt to differentiate himself, whom he considers a civilized Englishman, from the tribesmen.</p>
<p>In his observations, it is clear he is using his own culture as a reference point: “They were a dogmatic people, despising doubt, our modern crown of thorns. They did not understand our metaphysical difficulties, our introspective questionings. They knew only truth and untruth, belief and unbelief, without our hesitating retinue of finer shades…they were a limited, narrow-minded people.” [Lawrence, p 36]</p>
<p>Yet their minds are fully open to belief and obedience, according to Lawrence. One of his pieces of evidence is that three of the great world religions (Judaism, Christianity, Islam) arose out of this region and found ways of prospering among the people.</p>
<p>His opinion is that “Arabs could be swung on an idea as on a cord; for the unpledged allegiance of their minds made them obedient servants. None of them would escape the bond till success had come, and with it responsibility and duty and engagements…Their mind was strange and dark, full of depressions and exaltations, lacking in rule, but with more of ardour and more fertile in belief than any other in the world.” [Lawrence, p 41]</p>
<p>Lawrence sees this characteristic of obedience as full of potential, but only if it can be used to establish discipline. He describes how the Arabs perform well in small units but “[i]n mass they were not formidable, since they had no corporate spirit, nor discipline nor mutual confidence.” [Lawrence, p 140] After “spartan exercises” and training, though, they can become “excellent soldiers, instantly obedient and capable of formal attack.” [Lawrence, p 141] The goal appears to be to use the men’s usual fighting style for guerilla attacks when needed, but also train them to be able to fight in a more formal style that will help the Allies.</p>

<h4>The Fremen</h4>
<p>There are certainly several general parallels between the cultures of the Arabs and the Fremen. A strong Arabic influence appears in <em>Dune </em>through the use of Arab history, topography, culture, and words. Herbert borrows substantially from Arabic with terms such as Muad’Dib, Usul, Lisan Al-Gaib, Sayyadina, Shari-a, and Shaitan. [Istvan Csicsery-Ronay Jr, <em>Seven Beauties of Science Fiction</em>, Wesleyan University Press, 2008, p 39; Karin Christina Ryding, “The Arabic of <em>Dune</em>: Language and Landscape,” In <em>Language in Place: Stylistic Perspectives on Landscape, Place and Environment</em>, edited by Daniela Francesca Virdis, Elisabetta Zurru, and Ernestine Lahey, John Benjamins Publishing, 2021]</p>
<p>Critics have pointed to an analogy between the Fremen and Bedouin Arabs due to their cultures being nomadic, using guerilla war tactics, and having to live in harmony with nature out of necessity. [Csicsery-Ronay; B. Herbert; O’Reilly] In addition, the camel and sandworm are both used for transportation, warfare, and economic and cultural needs. [Hoda M. Zaki, “Orientalism in Science Fiction.” In <em>Food for Our Grandmothers: Writings by Arab-American and Arab-Canadian Feminists</em>, edited by Joanna Kadi, South End Press, 1994, p 182]</p>
<p>The overall characterization of the Fremen may be considered an overly romantic vision of Arab Bedouin society: long, flowing robes and dark or tanned skin; the practice of polygamy; values such as honor, trust, and bravery; and tribes that live primitive and simple lives in response to a brutal environment. [Zaki, p 183]</p>
<p>The representation of desert peoples through the Atreides’ eyes does rely on some romanticized notions. However, it can be seen as relying on fewer negative stereotypes than the depiction of the Arabs in Lawrence’s book.</p>
<p>In the Atreides’ view, the Fremen appear at first to be a suspicious and cautious people, willing to see if they can work with the Atreides or if they will need to consider them hostile like the Harkonnen. In the meantime, the Fremen helpfully provide solid intelligence and gifts of value such as stillsuits. Following his father, Paul accepts the view that the Fremen could be the allies and ‘desert power’ that they need. He thus has a clear incentive to look upon them favorably, just as Lawrence does.</p>
<p>When he sees the Fremen Stilgar for the first time, he feels the leader’s commanding presence: “A tall, robed figure stood in the door…A light tan robe completely enveloped the man except for a gap in the hood and black veil that exposed eyes of total blue—no white in them at all…In the waiting silence, Paul studied the man, sensing the aura of power that radiated from him. He was a leader—a <em>Fremen</em> leader.” [Herbert, p 92] Stilgar brings with him a sense of authority that all recognize. This aligns with how Lawrence describes Feisal—with a sense of destiny: “I felt at first glance that this was the man I had come to Arabia to seek – the leader who would bring the Arab Revolt to full glory. Feisal looked very tall and pillar-like, very slender, in his long white silk robes and his brown head-cloth bound with a brilliant scarlet and gold cord.” [Lawrence, p 92]</p>
<p>Also similar to Lawrence, Paul comes to understand and respect the way the Fremen have made the harsh environment livable through their stillsuits, sandworm riding, and other adaptations. When he realizes that the Fremen do not fear the desert because they know how to “<em>outwit the worm</em>”, he is impressed. [Herbert, p 125]</p>
<p>He notes the difference between his world—heavily regulated by the faufreluches class system—and that of the Fremen, who “lived at the desert edge without caid or bashar to command them” and were not recorded in Imperial censuses. [Herbert, p 4-5] Like Lawrence, he appears not to mind his experience living in a tribal structure, though both men still enjoy a certain privilege as outsiders. He learns how to ride sandworms, just as Lawrence learns how to ride camels.</p>
<p>Along with his mother, Jessica, Paul finds success in teaching Fremen fighters how to engage in more effective attacks against the Harkonnen. Jessica realizes that “<em>The little raids, the certain raids—these are no longer enough now that Paul and I have trained them. They feel their power. They want to fight.</em>” [Herbert, p 399]</p>
<p>Yet the concept of these desert peoples being simple-minded and willing to believe anything is also present in <em>Dune</em>. Fremen society has been sown with the myths and legends of the Bene Gesserit’s Missionaria Protectiva, which primes them to accept Jessica and Paul as savior figures without much question. Jessica knowingly capitalizes on these legends to solidify her and Paul’s status, and Paul is pulled along into the mythos.</p>
<p>In comparison to these two rational-seeming figures, the Fremen can appear superstitious and trapped in their traditional ways. Their minds seem especially open to belief and obedience, in a way similar to how Lawrence describes the Arabs.</p>
<p>Arguably this is part of Herbert’s study of religions and his critique of people’s willingness to follow religious leaders and their promises: The Missionaria Protectiva goes out to many planets and populations, not just the Fremen. But the Orientalist overtones remain an inescapable part of the Fremen’s characterization, with ‘enlightened’ leaders needing to come to assist supposedly ‘inferior’ native peoples. The Fremen as a whole shift from independent tribal groups to commando forces operating under Paul’s guidance and religious authority. No matter how independent and authoritative Stilgar is initially, he too comes to believe in the legend and defers to Paul.</p>
<p>However, it is significant that the main characters themselves essentially <em>become </em>Fremen, even though this is out of necessity and somewhat exploitative. Just like Lawrence sees some of the Arabs’ ways as beneficial and chooses to adopt them, Paul and Jessica see the value of the Fremen’s ways in the desert environment and adopt them. They learn the water discipline necessary for desert survival. Jessica becomes a Fremen Reverend Mother and thus a key keeper of memory and advisor for the tribe. Paul accepts the mantle of messiah, new names, and a Fremen woman, Chani, as his concubine.</p>
<p>Basically, they both accept a hybrid identity as the new norm for their lives—a type of uniting of West and East that helps them defeat their mutual enemies. [Kara Kennedy, “<a href="https://www.researchgate.net/publication/303317810_Epic_World-Building_Names_and_Cultures_in_Dune">Epic World-Building: Names and Cultures in <em>Dune</em></a>” <em>Names</em>, vol. 64, no. 2, p 106] This adds more dimension and nuance to the depiction of the Fremen and their culture, preventing it from relying solely on Orientalist stereotypes. And unlike Lawrence, who eventually returns to England, Paul remains close to the desert environment and influenced by Fremen in his role as ruler.</p>
<h4>Women and Religion</h4>

<p>There are two other notable differences between the worlds of <em>Seven Pillars </em>and <em>Dune</em>. One is the portrayal of women.</p>
<p>Lawrence’s book is clearly positioned as a man’s story about a male domain (war) likely intended for a male audience, and there are only a few mentions of women in total. Lawrence makes some brief reflections about the lack of women, but this mainly seems to be so that he can comment on the effect the absence has on men. He says the Arab leaders rely on their instinct and intuition and “Like women, they understood and judged quickly, effortlessly.” [Lawrence, p 221] He attributes this to “the Oriental exclusion of woman from politics”—that men end up taking on both so-called masculine and feminine characteristics in women’s absence. [Lawrence, p 221] He notes that “from end to end of it there was nothing female in the Arab movement, but the camels.” [Lawrence, p 221]</p>
<p>In contrast, women are very much present throughout <em>Dune</em>. A woman opens not only the book itself, but each unnumbered chapter within. This is the voice of Princess Irulan, the Emperor’s daughter, who authors the epigraphs and enters as a character at the book’s close. Irulan’s role is significant for shaping how the reader interprets each chapter. Her writings foreshadow key points and add to the sense that certain events are destined to happen.</p>
<p>Jessica appears so often she can be considered a main character alongside Paul. Being one of the Bene Gesserit, she is a highly skilled woman who takes responsibility for training and guiding her son, and securing their safety and survival as outsiders among the Fremen.</p>
<p>Chani is the child of Planetologist Liet Kynes and a Fremen woman and is introduced as a fierce fighter in Stilgar’s group that travels as a military company.</p>
<p>There is certainly no equivalent to these women in Lawrence’s book (or the 1962 film, which has no speaking roles for women in its 227-minute running time). Any comparisons between Paul and Lawrence of Arabia should acknowledge that Paul is not the kind of solitary hero that Lawrence is often held up to be.</p>
<p>The second major difference between the texts is in the portrayal of religion.</p>
<p>In <em>Seven Pillars </em>it is nearly absent. In a book so focused on the Middle East and its people and politics, one might expect some discussion of Islam and religious practices. But as Lawrence explains it, religion is not a major factor in the war the Arabs are fighting since their enemies, the Turks, are also Muslim. He says that “Of religious fanaticism there was little trace”, implying that religion would not be a helpful motivation for the Arabs in their alliance with Allied forces. [Lawrence, p 103]</p>
<p>Meanwhile, <em>Dune </em>is saturated with references to a variety of religions, including Catholicism, Islam, and Buddhism. Paul quotes the Orange Catholic Bible and receives a miniature copy of one. Jessica employs religious incantations from the Missionaria Protectiva to fit the mold of a prophesied figure, and also helps Paul capitalize on these myths. “Appendix II: The Religion of Dune” provides more background information on the different religious currents in the universe and is interwoven with references to real-world religions.</p>
<p>All of these references to and critiques of religion make it a significant aspect of the book. This fits with Herbert’s interest in exploring the nature of the desert environment, and specifically what has caused it to give birth to so many major religions and loyal followers. It also aligns with his warnings about the danger of superhero figures, who he believes are “disastrous for humankind.” [Frank Herbert, “Dangers of the Superhero,” In <em>The Maker of Dune</em>, edited by Tim O’Reilly, Berkley Books, 1987, p 97]</p>

<h4>Conclusion</h4>
<p>In examining Lawrence’s <em>Seven Pillars of Wisdom </em>as a source of inspiration for Herbert’s <em>Dune</em>, we’ve seen that there are multiple similarities, but also significant differences between the two works. T.E. Lawrence and Paul Atreides have much in common, yet while Lawrence expresses his sense of feeling like an unprepared fraud, Paul is bolstered by his training and status to feel much more confident in his leadership. The Arabs and Bedouin tribes are indeed an inspiration for the characterization of the Fremen, and Paul has a more favorable attitude toward desert peoples than Lawrence, who exhibits more overt Orientalizing tendencies. And finally, <em>Dune </em>is much more concerned with including a variety of religious references and a positive portrayal of women than Lawrence, who excludes these aspects almost entirely.</p>
<p>What all this shows is that <em>Dune </em>is not in fact a copy of the story of Lawrence of Arabia with some science-fictional window dressing. Rather, it uses elements of Lawrence’s story and his unique perspective as key ingredients with which to create a new and fascinating world.</p>

<p>Note: Page numbers for <em>Seven Pillars of Wisdom: A Triumph </em>are from the Alden Press 1946 edition and for <em>Dune </em>the Berkley Books 1984 edition.</p>
<p>Kara Kennedy, PhD, is a researcher and writer in the areas of science fiction, digital literacy, and writing. She has published academic articles on world-building in the <em>Dune </em>series and has other works about the series forthcoming. She posts literary analyses of <em>Dune</em> for a mainstream audience on her blog at <a href="https://dunescholar.com/">DuneScholar.com</a>.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Repairable Flatpack Toaster (691 pts)]]></title>
            <link>https://www.kaseyhou.com/#/repairable-flatpack-toaster/</link>
            <guid>43246892</guid>
            <pubDate>Mon, 03 Mar 2025 21:19:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kaseyhou.com/#/repairable-flatpack-toaster/">https://www.kaseyhou.com/#/repairable-flatpack-toaster/</a>, See on <a href="https://news.ycombinator.com/item?id=43246892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="/repairable-flatpack-toaster/">
            <div><div><p><img src="https://images.squarespace-cdn.com/content/v1/592869fe725e258ed3620a39/1499890990440-KXLZ5Z829FFFH3K0FZG0/DSC_3406+2.jpg?format=500w" data-src="https://images.squarespace-cdn.com/content/v1/592869fe725e258ed3620a39/1499890990440-KXLZ5Z829FFFH3K0FZG0/DSC_3406+2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/592869fe725e258ed3620a39/1499890990440-KXLZ5Z829FFFH3K0FZG0/DSC_3406+2.jpg" alt="DSC_3406 2.jpg"></p></div><p>0</p></div>
            <p>Repairable Flatpack Toaster</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comparing Fuchsia components and Linux containers [video] (200 pts)]]></title>
            <link>https://fosdem.org/2025/schedule/event/fosdem-2025-5381-comparing-fuchsia-components-and-linux-containers/</link>
            <guid>43246703</guid>
            <pubDate>Mon, 03 Mar 2025 21:06:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-5381-comparing-fuchsia-components-and-linux-containers/">https://fosdem.org/2025/schedule/event/fosdem-2025-5381-comparing-fuchsia-components-and-linux-containers/</a>, See on <a href="https://news.ycombinator.com/item?id=43246703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://fuchsia.dev/">Fuchsia</a> is a new (non-Linux) operating system from Google, and one of the key pieces of Fuchsia's design is <a href="https://fuchsia.dev/fuchsia-src/concepts/components/v2/introduction?hl=en">the component framework</a>. Components on Fuchsia have many similarities with some of the container solutions on Linux (such as Docker): they both fetch content addressed blobs from the network, assemble those blobs into an isolated filesystem structure that holds all the dependencies necessary to run some piece of software, and launch namespaced processes with that created directory as its root.</p>
<p>The most interesting details are where these two projects diverge. Both have different use cases and requirements, which leads to different strengths between the systems. This talk will largely be focusing on where and why these two similar technologies diverge.</p>
<p>Relevant links:
- <a href="https://fuchsia.googlesource.com/fuchsia/">Fuchsia's source code</a>
- <a href="https://fuchsia-review.googlesource.com/">Fuchsia's code review</a>
- <a href="https://fuchsia.dev/fuchsia-src/get-started/learn-fuchsia">Getting started page</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atlanta Fed predicts -2.8% GDP (214 pts)]]></title>
            <link>https://www.atlantafed.org/cqer/research/gdpnow</link>
            <guid>43246371</guid>
            <pubDate>Mon, 03 Mar 2025 20:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlantafed.org/cqer/research/gdpnow">https://www.atlantafed.org/cqer/research/gdpnow</a>, See on <a href="https://news.ycombinator.com/item?id=43246371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="myTabContent">

<div id="Tab1" role="tabpanel" aria-labelledby="Element1">
                        <p>The growth rate of real gross domestic product (GDP) measured by the U.S. Bureau of Economic Analysis (BEA) is a key metric of the pace of economic activity. It is one of the four variables included in the economic projections of Federal Reserve Board members and Bank presidents for every other Federal Open Market Committee (FOMC) meeting. As with many economic statistics, GDP estimates are released with a lag whose timing can be important for policymakers. For example, of the four scheduled 2014 release dates of an “advance” (or first) estimate of GDP growth, two are on the second day of a scheduled FOMC meeting with the other two on the day after the meeting. In preparation for FOMC meetings, policymakers have the Fed Board staff projection of this “advance” estimate at their disposal. These projections—available through 2008 at the <a href="http://www.philadelphiafed.org/research-and-data/real-time-center/greenbook-data/">Philadelphia Fed’s Real Time Data Center</a>—have generally been more accurate than forecasts from simple statistical models. As stated by economists <a href="http://amstat.tandfonline.com/doi/abs/10.1198/jbes.2009.07214#.U20Z4FeUWSY">Jon Faust and Jonathan H. Wright in a 2009 paper</a>, “by mirroring key elements of the data construction machinery of the Bureau of Economic Analysis, the Fed staff forms a relatively precise estimate of what BEA will announce for the previous quarter’s GDP even before it is announced.”</p>
<p>The Atlanta Fed GDPNow model also mimics the methods used by the BEA to estimate real GDP growth. The GDPNow forecast is constructed by aggregating statistical model forecasts of 13 subcomponents that comprise GDP. Other private forecasters use similar approaches to “<a href="http://en.wikipedia.org/wiki/Nowcasting_%28economics%29">nowcast</a>” GDP growth. However, these forecasts are not updated more than once a month or quarter, are not publicly available, or do not have forecasts of the subcomponents of GDP that add “color” to the top-line number. The Atlanta Fed GDPNow model fills these three voids.</p>
<p>The BEA’s advance estimates of the subcomponents of GDP use publicly released data from the U.S. Census Bureau, U.S. Bureau of Labor Statistics, and other sources. Much of this data is displayed in the BEA’s <a href="https://www.bea.gov/data/gdp/gross-domestic-product" target="_blank">Key Source Data and Assumptions</a> table that accompanies the “advance” GDP estimate. GDPNow relates these source data to their corresponding GDP subcomponents using a “bridge equation” approach similar to the one described in a <a href="http://www.minneapolisfed.org/research/qr/qr2022.pdf">Minneapolis Fed</a> study by Preston J. Miller and Daniel M. Chin. Whenever the monthly source data is not available, the missing values are forecasted using econometric techniques similar to those described in papers by <a href="http://www.tandfonline.com/doi/abs/10.1198/073500102317351921#.U20n6leUWSY">James H. Stock and Mark W. Watson</a> and <a href="http://ideas.repec.org/a/eee/moneco/v55y2008i4p665-676.html">Domenico Giannone, Lucrezia Reichlin, and David Small</a>. A detailed description of the data sources and methods used in the GDPNow model is provided in an accompanying <a href="https://www.atlantafed.org/pubs/wp/14_07.cfm">Atlanta Fed working paper</a>.</p>
<p>As more monthly source data becomes available, the GDPNow forecast for a particular quarter evolves and generally becomes more accurate. That said, the forecasting error can still be substantial just prior to the “advance” GDP estimate release. It is important to emphasize that the Atlanta Fed GDPNow forecast is a model projection not subject to judgmental adjustments. It is not an official forecast of the Federal Reserve Bank of Atlanta, its president, the Federal Reserve System, or the FOMC.</p>
<p>©2017 Federal Reserve Bank of Atlanta. All rights reserved. Permission is granted to reproduce for personal and educational use only.</p>
                        
                    </div>
<div id="Tab2" role="tabpanel" aria-labelledby="Element2">
                        
<p><strong>Is GDPNow an official forecast of the Atlanta Fed or the Bank's president?</strong><br>
No, it is not an official forecast of the Atlanta Fed, its president, the Federal Reserve System, or the FOMC.</p>
<p><strong>Is any judgment used to adjust the forecasts?</strong><br>
No. Once the GDPNow model begins forecasting GDP growth for a particular quarter, the code will not be adjusted until after the "advance" estimate. If we improve the model over time, we will roll out changes right after the "advance" estimate so that forecasts for the subsequent quarter use a fixed methodology for their entire evolution. </p>
<p><strong>When will nowcasts of GDP growth in a particular quarter begin and end?</strong><br>
GDPNow nowcasts of real GDP growth in a particular quarter begin about 90 days before the "advance" estimate for GDP growth for the quarter is released; they end on the last business day with a data release GDPNow utilizes that precedes the release date of the Bureau of Economic Analysis’s (BEA) advance estimate of GDP growth. Except after annual benchmark or comprehensive revisions of GDP typically occurring in late July, GDPNow nowcasts for a quarter generally begin on the weekday after the advance estimate of GDP growth for the previous quarter is released. After comprehensive or benchmark GDP revisions, the initial GDPNow nowcast for the subsequent quarter can be delayed for around a week until the BEA releases revised “underlying detail tables” for the National Income and Product Accounts.</p>
<p>For example, GDPNow’s initial nowcast of real GDP growth in the first quarter of 2018 took place on Monday, January 29, 2018, the first weekday after Friday, January 26, 2018, when the advance estimate of real GDP growth in the fourth quarter of 2017 was released. The final GDPNow nowcast of real GDP growth in the first quarter of 2018 was made on April 26, 2018, and the advance estimate of real GDP growth in the first quarter of 2018 was released on April 27, 2018.
</p>
<p><strong>How frequently is the GDPNow forecast updated?</strong><br>
The model forecast is updated six or seven times a month on weekdays, with at least one following seven data releases: <a href="http://www.ism.ws/ismreport/mfgrob.cfm">Manufacturing ISM Report on Business</a>, <a href="http://www.census.gov/foreign-trade/Press-Release/current_press_release/">U.S. International Trade in Goods and Services (FT900)</a>, <a href="http://www.census.gov/wholesale/">Wholesale Trade</a>, <a href="http://www.census.gov/retail/">Monthly Retail Trade Report</a>, <a href="http://www.census.gov/construction/nrc/">New Residential Construction</a>, <a href="http://www.census.gov/manufacturing/m3/adv/pdf/durgd.pdf">Advance Report on Durable Goods Manufacturers</a>, and <a href="http://www.bea.gov/newsreleases/national/pi/pinewsrelease.htm">Personal Income and Outlays</a>. Other data releases, such as <a href="http://www.federalreserve.gov/releases/g17/current/">Industrial Production and Capacity Utilization</a> and <a href="http://www.realtor.org/topics/existing-home-sales">Existing-Home Sales</a>, are incorporated in the model as well and their impact on the model's forecast will be shown on the next weekday with one of the data releases. The proprietary forecasts from <em>Blue Chip Economic Indicators</em> and <em>Blue Chip Financial Forecasts</em> shown in the chart are available from <a href="http://www.bluechippubs.com/">Aspen Publishers</a>.</p>
<p><strong>How can I access historical forecasts from the GDPNow model?</strong><br>
These forecasts are available in <a href="https://www.frbatlanta.org/-/media/Documents/cqer/researchcq/gdpnow/GDPTrackingModelDataAndForecasts.xlsx?la=en">this</a> downloadable spreadsheet. See the tab "ReadMe" in the spreadsheet for hyperlinks to the historical forecasts and other data for the model. In particular, the tab "TrackingDeepArchives" has forecasts for the 2011:Q3–2014:Q1 period (before the model went live), the tab "TrackingArchives" has forecasts from 2014:Q2 through the last quarter for which an advance estimate of GDP has been released by the BEA, and the tab "TrackRecord" has a comparison of the historical GDPNow model forecasts with the actual "advance" real GDP growth estimates from the BEA.</p>
<p><strong>Where can I read about the methods and source data used in the model?</strong><br>
A detailed description is given in a <a href="https://www.atlantafed.org/-/media/documents/research/publications/wp/2014/wp1407.pdf">working paper</a>&nbsp;describing the model. To summarize, the BEA's <a href="http://www.bea.gov/national/pdf/allchapters.pdf">NIPA Handbook</a> provides very detailed documentation on both the source data and methods used for estimating the subcomponents of GDP. The late Nobel Prize–winning economist Lawrence Klein pioneered many of the "bridge equation" methods used for making short-run forecasts of GDP growth using this source data; <a href="http://www.tradersshop.com/prod/2011106900000/CQM_Theory_Revision.pdf">a 1989 paper</a> he coauthored with E. Sojo describes the approach. Kathleen Navin, an economist at Macroeconomic Advisers, provides a bird's-eye view illustrating how to use a bridge equation approach in practice to improve GDP forecasts in <a href="https://www.dropbox.com/s/yx8kzd6dzdxvndd/Navin.pdf?dl=0">this 2017 presentation</a>. The econometric techniques used in our GDPNow model were heavily adapted from the GDP nowcasting models described in a 1996 Minneapolis Fed<em> Quarterly Review</em> article by <a href="http://www.minneapolisfed.org/research/qr/qr2022.pdf">Preston J. Miller and Daniel M. Chin</a> and a <a href="http://ideas.repec.org/a/eee/moneco/v55y2008i4p665-676.html">2008 paper</a> by the Board's David Small and economists Domenico Giannone and Lucrezia Reichlin.</p>
<p><strong>Where can I find alternative forecasts of GDP growth?</strong><br>
For model forecasts from other Reserve Banks, see the <a href="https://www.newyorkfed.org/research/policy/nowcast.html">New York Fed Nowcasting Report</a>, the <a href="https://fred.stlouisfed.org/series/STLENI">St. Louis Fed Economic News Index: Real GDP Nowcast</a>, the <a href="http://www.philadelphiafed.org/research-and-data/real-time-center/PRISM/">Philadelphia Research Intertemporal Stochastic Model (PRISM)</a>, and the Federal Reserve Bank of Cleveland's <a href="https://www.clevelandfed.org/en/our-research/indicators-and-data/yield-curve-and-gdp-growth.aspx">prediction model for GDP growth based on the slope of the yield curve</a>. <a href="https://www.economy.com/dismal/models/high-frequency-us-gdp-model.aspx">Moody's Analytics</a> and <a href="http://www.now-casting.com/">Now-Casting.com</a> produce proprietary model short-run GDP forecasts. For survey-based forecasts, see the Philadelphia Fed's <a href="http://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters/">quarterly Survey of Professional Forecasters,</a> which includes forecasts of real GDP and its major subcomponents. The <em>Wall Street Journal</em>'s <a href="http://online.wsj.com/public/page/economic-forecasting.html">Economic Forecasting Survey</a> occurs monthly, and the Moody's Analytics/CNBC <a href="http://www.cnbc.com/id/101508204">Rapid Update</a> survey generally occurs several times a week. Neither of these surveys includes forecasts of the subcomponents of GDP.</p>
<p><strong>How accurate are the GDPNow forecasts? Are they more accurate than "professional" forecasts?</strong><br>
The chart below shows GDPNow's real-time forecasts made just prior to the release of the initial estimate of the annualized growth rate of real GDP along with the initial estimates from the US Bureau of Economic Analysis.</p>

<p>Since we started tracking GDP growth with versions of this model in 2011, the average absolute error of final GDPNow forecasts is 0.77 percentage points. The <a href="http://en.wikipedia.org/wiki/Root-mean-square_deviation" target="blank">root-mean-squared error</a> of the forecasts is 1.15 percentage points. These accuracy measures cover initial estimates for 2011:Q3–2024:Q3. Some further analysis of GDPNow's forecast errors is available in <em>macroblog</em> posts located <a href="http://macroblog.typepad.com/macroblog/2016/05/gdpnow-and-then.html">here</a> and <a href="http://macroblog.typepad.com/macroblog/2016/05/can-two-wrongs-make-a-right.html">here</a>. We have made some improvements to the model from its earlier versions, and the model forecasts have become more accurate over time (the complete track record is <a href="https://www.frbatlanta.org/-/media/Documents/cqer/researchcq/gdpnow/GDPTrackingModelDataAndForecasts.xlsx?la=en">here</a>). When back-testing with <em>revised</em> data, the root mean-squared error of the model's out-of sample forecast with the same data coverage that an analyst would have just before the "advance" estimate is 1.15 percentage points for the 2000:Q1–2013:Q4 period. The figure below shows how the forecasts become more accurate as the interval between the date the forecast is made and the forthcoming GDP release date narrows.</p>

<!--<p><img width="472" height="364" title="Root Mean Square Forecast Error of GDP Growth (SAAR) for GDPNow Model" alt="Root Mean Square Forecast Error of GDP Growth (SAAR) for GDPNow Model" src="~/media/Images/cqer/researchcq/gdpnow/gdpnowforecastchart.ashx" class="centeredImage" /></p>-->
<p>Overall, these accuracy metrics do not give compelling evidence that the model is more accurate than professional forecasters. The model does appear to fare well compared to other conventional statistical models.</p>
<p><strong>How are revisions to data not yet reflected in the latest GDP release handled?</strong><br>
In general, the model does not attempt to anticipate how data releases after the latest GDP report will affect the revisions made in the forthcoming GDP release. The exception is the "change in private inventories" subcomponent, where revisions to the prior quarter's reading affect GDP growth in the current quarter. Users of the GDPNow forecast should generally use the forecasts of the change in "net exports" and the change in the "change in private inventories," and not forecasts of the levels. Revisions to retail sales are used to anticipate revisions to real monthly expenditures in the "<a href="http://www.bea.gov/faq/index.cfm?faq_id=519">PCE control group</a>" and revisions to housing starts are used to anticipate revisions in the <a href="http://www.census.gov/construction/c30/c30index.html">monthly value of private residential construction spending put in place</a>.</p>
<p><strong>Do you share your code?</strong><br>
At this point, no. However, the Excel <a href="https://www.frbatlanta.org/-/media/Documents/cqer/researchcq/gdpnow/GDPTrackingModelDataAndForecasts.xlsx?la=en">spreadsheet</a> gives the numerical details—including the raw data and model parameters—of how the monthly data map into forecasts of the subcomponents of GDP.</p>
<p><strong>What are the differences between GDPNow and the FRBNY Nowcast models? Why do the two models have different forecasts?</strong></p>
<p>The FRBNY Nowcast model of real GDP growth is based on a dynamic factor model described in this <a href="http://libertystreeteconomics.newyorkfed.org/2016/04/just-released-introducing-the-frbny-nowcast.html#.Vw0Qz3qgYks">Liberty Street blog entry.</a> The <a href="https://www.chicagofed.org/publications/cfnai/index">Chicago Fed National Activity Index</a> and <a href="https://www.philadelphiafed.org/research-and-data/real-time-center/business-conditions-index">Aruoba-Diebold-Scotti Business Conditions Index</a> are both indicators of economic activity estimated from factor models. The latest nowcast from the FRBNY Nowcast model along with some related Q&amp;A is available <a href="https://www.newyorkfed.org/research/policy/nowcast.html">here</a>.</p>
<p>The Atlanta Fed's GDPNow also uses a dynamic factor model—based on a <a href="http://www.sciencedirect.com/science/article/pii/S0304393208000652">model</a> from one of the New York Fed economists who coauthored the Liberty Street blog entry—but uses the factor only as an input to fill in the yet-to-be-released monthly source data for GDP. The estimates of this dynamic factor are available in the <strong>Factor </strong>tab of this <a href="https://www.atlantafed.org/-/media/documents/cqer/researchcq/gdpnow/GDPTrackingModelDataAndForecasts.xlsx">Excel file</a>.</p>
<p>The monthly source data are then used to estimate the subcomponents of GDP, which are then aggregated up to a real GDP growth nowcast. Besides a dynamic factor model, GDPNow uses several other econometric techniques, including "bridge equations" and Bayesian vector autoregressions, to nowcast the subcomponents of GDP. The exact methods are described in this <a href="https://www.atlantafed.org/sitecore/service/notfound.aspx?item=web%3a%7b8D93F92E-8FC1-46D3-A7F2-A00EF85E4B28%7d%40en">working paper</a>. The numerical details—including the raw data and model parameters—translating the monthly data into nowcasts of the subcomponents of GDP in the latest GDPNow forecast are available in this <a href="https://www.atlantafed.org/-/media/documents/cqer/researchcq/gdpnow/GDPTrackingModelDataAndForecasts.xlsx">Excel file</a> (see the <strong>ReadMe</strong> tab).</p>
<p>Because GDPNow and the FRBNY Nowcast are different models, they can generate different forecasts of real GDP growth. Our policy is not to comment on or interpret any differences between the forecasts of these two models.</p>
                        
                    </div>
<div id="Tab3" role="tabpanel" aria-labelledby="Element3">
                        <p>These charts show how the forecasted GDP subcomponent contributions to growth aggregate up to GDPNow's real GDP growth forecast for each update day in a particular forecast quarter and how changes in the subcomponent contribution forecasts aggregate up to changes in the GDP growth forecasts. Whenever a user hovers the cursor over a bar in one of the charts, the pop-up box displays the data releases for the date of the bar as well the numerical values for the GDP growth forecast and either the levels or changes in the subcomponent contribution forecasts. For previously reported quarters, the final date in the top chart shows the official first estimates of real GDP growth and the subcomponent contributions to growth from the Bureau of Economic Analysis (BEA). The final date in the bottom chart shows the forecast errors of the final GDPNow projections of the BEA's first estimates of real GDP growth and the subcomponent contributions to growth.</p>

<p><label for="chooseQuarter">Select a quarter:</label></p>



                        
                    </div>
<div id="Tab4" role="tabpanel" aria-labelledby="Element4">
                        
<p>Release times shown are from the original source. The GDPNow model is usually updated within a few hours following these times. Release schedule subject to change.</p>
<table>
    <tbody>
        <tr>
            <td><strong>Release</strong></td>
            <td><strong>Date of release</strong></td>
            <td><strong>Time of release</strong></td>
        </tr>
        <tr>
            <td>Construction spending</td>
            <td>1/2/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index</td>
            <td>1/3/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), ISM Services</td>
            <td>1/7/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>1/9/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Import and export prices</td>
            <td>1/16/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts, Industrial production and capacity utilization</td>
            <td>1/17/2025</td>
            <td>9:15 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>1/28/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Final nowcast of 2024:Q4 GDP growth: Advance Economic Indicators</td>
            <td>1/29/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Initial nowcast of 2025:Q1 GDP growth: Personal income and outlays, NIPA underlying detail tables</td>
            <td>1/31/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>2/3/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), ISM Services</td>
            <td>2/5/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Employment situation, Wholesale trade</td>
            <td>2/7/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization, Import and export prices</td>
            <td>2/14/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>2/19/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables, Advance Economic Indicators</td>
            <td>2/28/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>3/3/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), Wholesale trade</td>
            <td>3/6/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories</td>
            <td>3/17/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts, Industrial production and capacity utilization, Import and export prices</td>
            <td>3/18/2025</td>
            <td>9:15 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>3/26/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables</td>
            <td>3/28/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>4/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), ISM Services</td>
            <td>4/3/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization</td>
            <td>4/16/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>4/17/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1), Existing-home sales</td>
            <td>4/24/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Final nowcast of 2025:Q1 GDP growth:  Advance Economic Indicators</td>
            <td>4/29/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Initial nowcast of 2025:Q2 GDP growth:  Personal income and outlays, Q1 GDP, NIPA underlying detail tables</td>
            <td>4/30/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>5/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report)</td>
            <td>5/6/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>5/8/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization, Producer Price Index</td>
            <td>5/15/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts, Import and export prices</td>
            <td>5/16/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>5/27/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables, Advance Economic Indicators</td>
            <td>5/30/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>6/2/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report)</td>
            <td>6/5/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>6/9/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization, Import and export prices</td>
            <td>6/17/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>6/18/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables</td>
            <td>6/27/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>7/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), Employment situation, Census M3-2 manufacturing (Full report), ISM Services</td>
            <td>7/3/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>7/9/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Import and export prices</td>
            <td>7/17/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>7/18/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>7/25/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Final nowcast of 2025:Q2 GDP growth:  Advance Economic Indicators</td>
            <td>7/29/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Initial nowcast of 2025:Q3 GDP growth:  Personal income and outlays, NIPA underlying detail tables</td>
            <td>7/31/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending, Employment situation</td>
            <td>8/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), ISM Services</td>
            <td>8/5/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>8/7/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization, Import and export prices</td>
            <td>8/15/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>8/19/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>8/26/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables, Advance Economic Indicators</td>
            <td>8/29/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>9/2/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), ISM Services</td>
            <td>9/4/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade, Producer Price Index</td>
            <td>9/10/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Industrial production and capacity utilization, Import and export prices</td>
            <td>9/16/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>9/17/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables**</td>
            <td>9/26/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>10/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report)</td>
            <td>10/7/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>10/9/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>10/27/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Final nowcast of 2025:Q3 GDP growth:  Advance Economic Indicators</td>
            <td>10/29/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Initial nowcast of 2025:Q4 GDP growth:  Personal income and outlays, NIPA underlying detail tables</td>
            <td>10/31/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report), Census M3-2 manufacturing (Full report)</td>
            <td>11/4/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade</td>
            <td>11/6/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories, Producer Price Index</td>
            <td>11/14/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts</td>
            <td>11/19/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables, Advance Census manufacturing (M3-1), GDP, Advance Economic Indicators, New-home sales</td>
            <td>11/26/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>ISM Manufacturing Index, Construction spending</td>
            <td>12/1/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>International trade (Full report)</td>
            <td>12/4/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Wholesale trade, Consumer Price Index</td>
            <td>12/10/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Housing starts, Import and export prices</td>
            <td>12/16/2025</td>
            <td>8:30 AM</td>
        </tr>
        <tr>
            <td>Retail sales + inventories</td>
            <td>12/17/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Personal income and outlays, NIPA underlying detail tables, GDP, Existing-home sales</td>
            <td>12/19/2025</td>
            <td>10:00 AM</td>
        </tr>
        <tr>
            <td>Advance Census manufacturing (M3-1)</td>
            <td>12/24/2025</td>
            <td>8:30 AM</td>
        </tr>
    </tbody>
</table>
<p><a href="https://www.atlantafed.org/-/media/documents/cqer/researchcq/gdpnow/GDPNowcastDataReleaseDates.xlsx">Download a spreadsheet of these release dates</a></p>
<p>*Time of last economic release; GDPNow update typically released 2 to 3.5 hours after this time.</p>
<p>**Timing of 9/26 GDPNow updates depends on details regarding 2025 annual revision of the National Income and Product Accounts.  Further details to be determined/announced.</p>
                        
                    </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Logo, Three Companies (104 pts)]]></title>
            <link>https://estilofilos.blogspot.com/2016/03/one-logo-three-companies-i.html</link>
            <guid>43245315</guid>
            <pubDate>Mon, 03 Mar 2025 18:54:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://estilofilos.blogspot.com/2016/03/one-logo-three-companies-i.html">https://estilofilos.blogspot.com/2016/03/one-logo-three-companies-i.html</a>, See on <a href="https://news.ycombinator.com/item?id=43245315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
I am not going to speak about fountain pens today but about pencils and paper… and whisky and cars…</p><p>

The brand Mitsubishi is well known for a number of products and services available in the market. So, we can buy a Mitsubishi Pajero, open an account in the Bank of Tokyo-Mitsubishi-UFJ, drink a Mitsubishi cider, write with a Mitsubishi UNI pencil on a paper with a THREE DIAMOND watermark… and even more without realizing we still were in the Mitsubishi realm: a picture with a Nikon camera, a Kirin whisky,…</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyRbGAm3e40ZeM-maBxu2Iayyw1b5vErqboDJzCF_pYSiafxV7TsMdXYCaulvTYzcCQ1o1FlJNuhYxtpj3SkDV23ySf8JJW8wZtp450rvUE5UD7s2WaDTSXohPHlQWM0FRyDpiS7W2THi9/s1600/P1050147-blog-WM.jpg" imageanchor="1"><img height="215" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyRbGAm3e40ZeM-maBxu2Iayyw1b5vErqboDJzCF_pYSiafxV7TsMdXYCaulvTYzcCQ1o1FlJNuhYxtpj3SkDV23ySf8JJW8wZtp450rvUE5UD7s2WaDTSXohPHlQWM0FRyDpiS7W2THi9/s320/P1050147-blog-WM.jpg" width="300"></a><br><i>Is this a car, a pencil, a bank, a cider...?</i>
</p>

<p>
Mitsubishi is all that and more. But Mitsubishi is, first, three very different things.</p><p>

Mitsubishi is the <a href="https://en.wikipedia.org/wiki/Mitsubishi">Mitsubishi Group of Companies</a>, a <i><a href="https://en.wikipedia.org/wiki/Keiretsu">keiretsu</a></i> of <a href="https://www.mitsubishi.com/php/users/category_search.php?lang=1">companies operating in a very wide variety of fields</a>. It all started in 1870 by the hand of Yatarô Iwasaki as a freight transporter. Around 1913, the company registered the well known logo with the three diamonds. Among the fields included in the activities of the Group we can find finances, nuclear technology, cars and industrial vehicles, paper milling… Some of the companies in the Group use the three diamond logo, but not all of them.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFXyC1AQqWVs01vCJNA2nQ-HxX4DM_kH7kj7cl6BFGGQpYkTv5S2Bu5MpE0TsnANPnqjPIQBHswMeU9v0ZuXwwhx7kPXvzvRteK-ozfCG6IM2wR3Gxk9iDRgRBt2YG60vsKTORpP1WfAi5/s1600/KIMG0169-blog-WM.jpg" imageanchor="1"><img height="686" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFXyC1AQqWVs01vCJNA2nQ-HxX4DM_kH7kj7cl6BFGGQpYkTv5S2Bu5MpE0TsnANPnqjPIQBHswMeU9v0ZuXwwhx7kPXvzvRteK-ozfCG6IM2wR3Gxk9iDRgRBt2YG60vsKTORpP1WfAi5/s320/KIMG0169-blog-WM.jpg" width="300"></a>
</p>



<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjez4d8d4MRb627T1AImGVJkgFMdtKtIuzSFt5S5g5ASi_kKAzoaqdnJRsbCE8I5-PgFNFMgyRxlvnfdocf0O-b8eYOK45vt-DoMPWo-CPQFEt-gmCYO5Raq7wN512leQWOmzYfJ6lilZam/s1600/IMG_7246-blog-WM.jpg" imageanchor="1"><img height="375" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjez4d8d4MRb627T1AImGVJkgFMdtKtIuzSFt5S5g5ASi_kKAzoaqdnJRsbCE8I5-PgFNFMgyRxlvnfdocf0O-b8eYOK45vt-DoMPWo-CPQFEt-gmCYO5Raq7wN512leQWOmzYfJ6lilZam/s320/IMG_7246-blog-WM.jpg" width="500"></a><br><i>Kirin and Nikon are also part of the Mitsubishi Group of Companies.</i>
</p>

<p>
A second company by the same name is <a href="http://www.mpuni.co.jp/">Mitsubishi Pencil Co.</a> This has no ties with the big Mitsubishi Group. The pencil company was founded in 1887 by Jinroku Masaki as Masaki Pencil Co. (Masaki Enpitsu), and in 1903 he registered the three diamond logo based on the family crest. The activities of this company, soon to celebrate its 130 years of history, are limited to the production of writing tools, but not of paper.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhzTY59tIqzVMJCrrzNz5i4bKsre13Hnv_Nf3gGGf2GBcuJ9DBz1fTj3ohjkSP8luCfYDAL6LzAkok85AiNiq6yXBbYWf7FccHuiLywp_Oyfz-Q3S6kCMpVGy5BU6DCOX7FUWAqIEUvVLK/s1600/Pencil+box-blog-WM.jpg" imageanchor="1"><img height="233" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhzTY59tIqzVMJCrrzNz5i4bKsre13Hnv_Nf3gGGf2GBcuJ9DBz1fTj3ohjkSP8luCfYDAL6LzAkok85AiNiq6yXBbYWf7FccHuiLywp_Oyfz-Q3S6kCMpVGy5BU6DCOX7FUWAqIEUvVLK/s320/Pencil+box-blog-WM.jpg" width="400"></a>
</p>



<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7pweI9zScttYpdbsnqm2hyphenhyphen7I92IPWuu5sXuum0m7CSiV6UKOklHm6qDTo2E8ziugew7zF_pQSsRxnuZRbilbeL4iguScSAVD4pNQBY34U1FoQTge7lSjKJkC4eW5oF4qsSCZA11C_DKPN/s1600/IMG_7230-blog-WM.jpg" imageanchor="1"><img height="133" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7pweI9zScttYpdbsnqm2hyphenhyphen7I92IPWuu5sXuum0m7CSiV6UKOklHm6qDTo2E8ziugew7zF_pQSsRxnuZRbilbeL4iguScSAVD4pNQBY34U1FoQTge7lSjKJkC4eW5oF4qsSCZA11C_DKPN/s320/IMG_7230-blog-WM.jpg" width="400"></a><br><i>HI-uni is one of the lines of graphie pencils of Mitsubishi Pencil Co.</i>
</p>

<p>
Finally, Mitsubishi is also the name of a cider –in the Japanese meaning of it, a non-alcoholic, carbonated soda—produced by the company <a href="http://www.konyusha.co.jp/index.html">Konyusha</a>, from Kumamoto, unrelated to the Mitsubishi <i>keiretsu</i>. It was founded in 1883 and registered the three diamond logo in 1919.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinBX0vIRENRRailtvcTXhxng1Qi4Jp7815ig10Xc3ois05czMLsoyqKhokOOLoCn5fHqq4btvuOJ04ZU483SuGhuxxU1I0oNijmu14RYLeS30QPxf45sRV7PQUwZ91gKJp70qFlAUjfE2n/s1600/IMG_7257-blog-WM.jpg" imageanchor="1"><img height="350" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinBX0vIRENRRailtvcTXhxng1Qi4Jp7815ig10Xc3ois05czMLsoyqKhokOOLoCn5fHqq4btvuOJ04ZU483SuGhuxxU1I0oNijmu14RYLeS30QPxf45sRV7PQUwZ91gKJp70qFlAUjfE2n/s320/IMG_7257-blog-WM.jpg" width="350"></a><br><i>Yeah, Mitsubishi cider... together with the three diamond logo as well.</i>
</p>

<p>
So, there we have three different companies using the same name and the same logo. No wonder, then, the existence of the <a href="https://www.mitsubishi.com/e/group/mark.html">Mitsubishi Corporate Name and Trademark Committee</a> to control and prevent any fraud in the use of both name and logo. But the problem and the confusion are deeply rooted.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHDG11Rhop9X1mxQwPl8lE_Iam0e0QdirMvKJyGK86wy3nsc8REac7G5ZJhVL1aZaqflrDlqgbld6hw9Jq6WP673xw388bYgnGMHrrq_g9Gq8vbI-lSl8C66SIdjC_RlnpHgu4yHkXQhf6/s1600/IMG_3631-blog-WM.jpg" imageanchor="1"><img height="143" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHDG11Rhop9X1mxQwPl8lE_Iam0e0QdirMvKJyGK86wy3nsc8REac7G5ZJhVL1aZaqflrDlqgbld6hw9Jq6WP673xw388bYgnGMHrrq_g9Gq8vbI-lSl8C66SIdjC_RlnpHgu4yHkXQhf6/s320/IMG_3631-blog-WM.jpg" width="200"></a>
</p>

<div><p>
Romillo Essential Black – Montblanc Racing Green</p><p>

Bruno Taut<br>
Nakano March, 2016<br>
etiquetas: Mitsubishi Pencil, Mitsubishi Paper Mills, papelería</p></div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the U.K. broke its own economy (277 pts)]]></title>
            <link>https://www.theatlantic.com/ideas/archive/2025/03/uk-needs-abundance/681877/</link>
            <guid>43245235</guid>
            <pubDate>Mon, 03 Mar 2025 18:45:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/ideas/archive/2025/03/uk-needs-abundance/681877/">https://www.theatlantic.com/ideas/archive/2025/03/uk-needs-abundance/681877/</a>, See on <a href="https://news.ycombinator.com/item?id=43245235">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>With the best intentions, the United Kingdom engineered a housing and energy shortage.</p></div><div><figure><div data-flatplan-lead_figure_media="true"><picture><img alt="British flag umbrella turned inside out on a wet sidewalk" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/iwcB2vvLLjzDdruzZKhBAd5WQwY=/0x0:2700x1519/750x422/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 750w, https://cdn.theatlantic.com/thumbor/waTG8YmDMXlIhPy6smV0d90cpEQ=/0x0:2700x1519/828x466/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 828w, https://cdn.theatlantic.com/thumbor/7zzKOU2bCpn2_rVsNaWESSOUfx0=/0x0:2700x1519/960x540/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 960w, https://cdn.theatlantic.com/thumbor/q6BCaYL_5P1drYUCIhHDuwq8Mpg=/0x0:2700x1519/976x549/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 976w, https://cdn.theatlantic.com/thumbor/NjMSW58E8Bk9Dv1E8TZ0fhHSr-s=/0x0:2700x1519/1952x1098/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/7zzKOU2bCpn2_rVsNaWESSOUfx0=/0x0:2700x1519/960x540/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg" id="article-lead-image" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">ballyscanlon / Getty</figcaption></figure></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><div data-view-action="view - audio player - start" data-view-label="681877" data-event-module="audio player" data-event-content-type="narrated" data-event-module-state="start" data-event-view="true"><div><p><img alt="British flag umbrella turned inside out on a wet sidewalk" sizes="80px" srcset="https://cdn.theatlantic.com/thumbor/JxMC2TyWH7ZCeH1Yue8aNI1IOjk=/162x0:1681x1519/80x80/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 80w, https://cdn.theatlantic.com/thumbor/8h1GCscqCO1iKkzySq3VTACwtMs=/162x0:1681x1519/96x96/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 96w, https://cdn.theatlantic.com/thumbor/DY39a6tMXnsf09EKpJLQQQ4cWOg=/162x0:1681x1519/128x128/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 128w, https://cdn.theatlantic.com/thumbor/MYb7prbT6vBLBEoKOrXGXIUgHAU=/162x0:1681x1519/160x160/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 160w, https://cdn.theatlantic.com/thumbor/AOhtVvKjU36g_HXFwXWP3kDMk9g=/162x0:1681x1519/192x192/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 192w, https://cdn.theatlantic.com/thumbor/OLHgxUk_flcanBPo0U0Xyof551k=/162x0:1681x1519/256x256/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 256w, https://cdn.theatlantic.com/thumbor/_0TtLa8scgz650kOOx2728KwnTE=/162x0:1681x1519/384x384/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 384w, https://cdn.theatlantic.com/thumbor/rsjZU7XfOJDeGfpq59mpQ2oQe0I=/162x0:1681x1519/512x512/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg 512w" src="https://cdn.theatlantic.com/thumbor/JxMC2TyWH7ZCeH1Yue8aNI1IOjk=/162x0:1681x1519/80x80/media/img/mt/2025/02/2025_02_27_british_economy_AZ_479794709/original.jpg" width="80" height="80"></p></div><p>Produced by ElevenLabs and<!-- --> <a href="https://newsoveraudio.com/?offerId=atl_reader_exclusive_jks1kjl"> <!-- -->News Over Audio (Noa)</a> <!-- -->using AI narration. Listen to more stories on the Noa app.</p></div><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">What’s the matter with the United Kingdom? Great Britain is the birthplace of the Industrial Revolution, which ushered in an era of energy super-production and launched an epoch of productivity advancements that made many life essentials, such as clothes and food, more affordable. Today, the country suffers from the converse of these achievements: a profound energy shortage and a deep affordability crisis. In February, the Bank of England reported an ongoing productivity slump so mysterious that its own economists “<a data-event-element="inline link" href="https://x.com/surplustakes/status/1887506827664928819">cannot account fully</a>” for it. Real wages have barely grown for 16 years. British politics seems stuck in a cycle of disappointment followed by dramatic promises of growth, followed by yet more disappointment.</p><p data-flatplan-paragraph="true">A new report, titled <a data-event-element="inline link" href="https://ukfoundations.co/">“Foundations,”</a> captures the country’s economic malaise in detail. The U.K. desperately needs more houses, more energy, and more transportation infrastructure. “No system can be fixed by people who do not know why it is broken,” write the report’s authors, Sam Bowman, Samuel Hughes, and Ben Southwood. They argue that the source of the country’s woes as well as “the most important economic fact about modern Britain [is] that it is difficult to build almost anything, anywhere.” The nation is gripped by laws and customs that make essentials unacceptably scarce and drive up the cost of construction across the board.</p><p data-flatplan-paragraph="true">Housing is an especially alarming case in point. The homeownership rate for the typical British worker aged 25 to 34 declined by more than half <a data-event-element="inline link" href="https://ifs.org.uk/sites/default/files/output_url_files/BN224.pdf">from the 1990s to the 2010s</a>. In that same time, average housing prices more than doubled, even after adjusting for inflation, according to the Institute for Fiscal Studies.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/newsletters/archive/2022/10/uk-economy-disaster-degrowth-brexit/671847/">Read: How the U.K. became one of the poorest countries in Western Europe</a></p><p data-flatplan-paragraph="true">The housing shortage traces back to the postwar period, when a frenzy of nationalization swept the country. The U.K. created the National Health Service, brought hundreds of coal mines under state control, and centralized many of the country’s railways and trucking and electricity providers. In 1947, the U.K. passed the Town and Country Planning Act, which forms the basis of modern housing policy. The TCPA effectively prohibited new development without special permission from the state; “green belts” were established to restrict sprawl into the countryside. Rates of private-home building never returned to their typical prewar levels. With some spikes and troughs, new homes built as a share of the total housing stock have generally declined over the past 60 years.</p><p data-flatplan-paragraph="true">The TCPA was considered reasonable and even wise at the time. Postwar Britain had been swept up by the theory that nationalization created economies of scale that gave citizens better outcomes than pure capitalism. “There was an idea that if we could rationalize the planning system … then we could build things in the right way—considered, and planned, and environmentally friendly,” Bowman told me.</p><p data-flatplan-paragraph="true">But the costs of nationalization became clear within a few decades. With more choke points for permitting, construction languished from the 1950s through the ’70s. Under Prime Minister Margaret Thatcher, the Conservatives rolled back nationalization in several areas, such as electricity and gas production. But their efforts to loosen housing policy from the grip of government control was a tremendous failure, especially once it was revealed that Thatcher’s head of housing policy himself opposed new housing developments near his home.</p><p data-flatplan-paragraph="true">Housing is, as I’ve written, <a data-event-element="inline link" href="https://www.theatlantic.com/ideas/archive/2024/08/the-urban-family-exodus-is-a-warning-for-progressives/679350/">the quantum field of urban policy</a>, touching every station of urban life. Broken housing policies have a ripple effect. In London, Bowman said, the most common options are subsidized flats for the low-income and luxury units for the rich, creating a dearth of middle-class housing. As a result, the city is bifurcated between the über-wealthy and the subsidized poor. “I think housing policy is a major driver of a lot of anti-foreigner, white-supremacist, anti-Black, anti-Muslim attitudes among young people who are frustrated that so-called these people get free houses while they have to live in a bedsit or move somewhere an hour outside the city and commute in,” Bowman said.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/ideas/archive/2024/08/the-urban-family-exodus-is-a-warning-for-progressives/679350/">Read: The urban family exodus is a warning for progressives</a></p><p data-flatplan-paragraph="true">Constrictive housing policy in Britain has also arguably prevented other great cities from being born. If the University of Cambridge’s breakthroughs in biotech had happened in the 19th century, Bowman said, the city of Cambridge might have bloomed to accommodate new companies and residents, the same way Glasgow grew by an order of magnitude around shipbuilding in the 1800s. Instead Cambridge remains a small city of fewer than 150,000 people, its potential stymied by rules all but prohibiting its growth.</p><p data-flatplan-paragraph="true">The story for transit and energy is similar: Rules and attitudes that make it difficult to build things in the world have made life worse for the British. “On a per-mile basis, Britain now faces some of the highest railway costs in the world,” Bowman, Hughes, and Southwood write. “This has led to some profoundly dissatisfying outcomes. Leeds is now the largest city in Europe without a metro system.” Despite Thatcher’s embrace of North Sea gas, and more <a data-event-element="inline link" href="https://www.theguardian.com/environment/2022/sep/15/liz-truss-to-lift-fracking-ban-despite-little-progress-on-earthquake-risk">recent attempts</a> to loosen fracking regulations, Britain’s energy markets are still an omnishambles. Per capita electricity generation in the U.K. is <a data-event-element="inline link" href="https://ourworldindata.org/grapher/per-capita-electricity-generation?tab=chart&amp;country=GBR~USA">now roughly one-third that of the United States</a>, and energy use per unit of GDP is the lowest in the G7. By these measures, at least, Britain may be the most energy-starved nation in the developed world.</p><p data-flatplan-paragraph="true">Scarcity is a policy choice. This is as true in energy as it is in housing. In the 1960s, Britain was home to about half of the world’s entire fleet of nuclear reactors. Today, the U.K. has extraordinarily high nuclear-construction costs compared with Asia, and it’s behind much of Europe in the <a data-event-element="inline link" href="https://ourworldindata.org/grapher/share-electricity-nuclear?tab=chart&amp;country=FRA~SWE~GBR~USA~ESP~CHE~FIN~CZE~ROU">share of its electricity</a> generated from nuclear power—not only France but also Finland, Switzerland, Sweden, Spain, and Romania.</p><p data-flatplan-paragraph="true">What happened to British nuclear power? After North Sea oil and gas production ramped up in the 1970s and ’80s, Britain redirected its energy production away from nuclear power. Even this shift has had its own complications. In the past few years, the U.K. has passed several measures to reduce shale-gas extraction, citing earthquake risks, environmental costs, and public opposition. As a result, gas production in the U.K. has declined 70 percent since 2000. Although the country’s renewable-energy market has grown, solar and wind power haven’t increased nearly enough to make up the gap.</p><p data-flatplan-paragraph="true">The comparison with France makes clear Britain’s policy error: In 2003, very large businesses in both countries paid about the same price for electricity. But by 2024, after decades of self-imposed scarcity and the supply shock of the war in Ukraine, electricity in the U.K. was more than twice as expensive as in France.</p><p data-flatplan-paragraph="true">There is an inconvenient subcurrent to the U.K.’s scarcity crisis—and ours. Sixty years ago, the environmentalist revolution transformed the way governments, courts, and individuals thought about their relationship to the natural world. This revolution was not only successful but, in many ways, enormously beneficial. In the U.S., the Clean Air Act and Clean Water Act <a data-event-element="inline link" href="https://www.gao.gov/blog/50-years-after-clean-water-act-gauging-progress">brought</a> about <a data-event-element="inline link" href="https://e360.yale.edu/features/delaware-river-clean-water-act">exactly</a><a data-event-element="inline link" href="https://www.epa.gov/transportation-air-pollution-and-climate-change/accomplishments-and-successes-reducing-air"> that</a>. But over time, American environmental rules, such as those in the National Environmental Policy Act and the California Environmental Quality Act, have been used to <a data-event-element="inline link" href="https://www.theatlantic.com/magazine/archive/2025/03/american-geographic-social-mobility/681439/">stop new housing developments</a> and, ironically, even <a data-event-element="inline link" href="https://www.ncpc.gov/docs/actions/2016June/GSA_Capital_Solar_Challenge_Recommendation_7789_June2016.pdf">clean</a>-<a data-event-element="inline link" href="https://www.liberalcurrents.com/the-case-for-abolishing-the-national-environmental-policy-act/">energy</a> <a data-event-element="inline link" href="https://www.noahpinion.blog/p/the-big-nepa-roundup">additions</a>. Similarly, in the U.K., any individual who sues to stop a new project on environmental grounds—say, to oppose a new road or airport—generally has their legal damages capped at £5,000, if they lose in court. “Once you’ve done that,” Bowman said, “you’ve created a one-way system, where people have little incentive to not bring spurious cases to challenge any new development.” Last year, Britain’s high-speed-rail initiative was compelled to spend an additional £100 million on a shield to protect bats in the woods of Buckinghamshire. Finding private investment is generally difficult for infrastructure developers when the path to completion is strewn with nine-figure surprise fees.</p><p data-flatplan-paragraph="true">Some of Britain’s problems echo across the European continent, including slow growth and high energy prices. More than a decade ago, Germany began to <a data-event-element="inline link" href="https://www.wsj.com/world/europe/why-germanys-confidence-is-shattered-and-its-economy-is-kaput-d1d95890?mod=hp_lead_pos9">phase out</a> nuclear power while failing to ramp up other energy production. The result has been catastrophic for citizens and for the ruling government. In the first half of 2024, Germans paid <a data-event-element="inline link" href="https://www.wsj.com/world/europe/why-germanys-confidence-is-shattered-and-its-economy-is-kaput-d1d95890?mod=hp_lead_pos9">the highest electricity prices in the European Union</a>. This month, Social Democrats were punished at the polls with their <a data-event-element="inline link" href="https://www.lemonde.fr/en/international/article/2025/02/26/germany-s-social-democrats-in-turmoil-after-historic-general-election-defeat_6738587_4.html">worst defeat since World War II</a>. Bowman offered a droll summary: “Europe has an energy problem; the Anglosphere has a housing problem; Britain has both.”</p><p data-flatplan-paragraph="true">These problems are obvious to many British politicians. Leaders in the Conservative and Labour Parties often comment on expensive energy and scarce housing. But their goals haven’t been translated into priorities and policies that lead to growth. “Few leaders in the U.K. have thought seriously about the scale of change that we need,” Bowman said. Comprehensive reform is necessary to unlock private investment in housing and energy—including overhauling the TCPA, reducing incentives for anti-growth lawsuits, and directly encouraging nuclear and gas production to build a bridge to a low-carbon-energy economy.</p><p data-flatplan-paragraph="true">Effective 21st-century governance requires something more than the ability to win elections by decrying the establishment and bemoaning sclerotic institutions. Progress requires a positive vision of the future, a deep understanding of the bottlenecks in the way of building that future, and a plan to add or remove policies to overcome those blockages. In a U.S. context, that might mean making it easier to build advanced semiconductors, or removing bureaucratic kludge for scientists while adding staff at the FDA to accelerate drug approval.</p><p id="injected-recirculation-link-2" data-view-action="view link - injected link - item 3" data-event-element="injected link" data-event-position="3"><a href="https://www.bloomberg.com/news/articles/2025-02-23/germany-s-social-democrats-ditch-scholz-after-historic-defeat">Read: A simple plan to solve all of America’s problems</a></p><p data-flatplan-paragraph="true">In the U.K., the bottlenecks are all too clear: Decades-old rules make it too easy for the state to block housing developments or for frivolous lawsuits to freeze out energy and infrastructure investment. In their conclusion, Bowman and his co-authors strike a similar tone. “Britain can enjoy such a renewal once more,” they write. “To do so, it need simply remove the barriers that stop the private sector from doing what it already wants to do.”</p></section><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[James Harrison, whose blood donations saved >2M babies, has died (280 pts)]]></title>
            <link>https://www.npr.org/2025/03/03/nx-s1-5316163/james-harrison-blood-donor</link>
            <guid>43245129</guid>
            <pubDate>Mon, 03 Mar 2025 18:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2025/03/03/nx-s1-5316163/james-harrison-blood-donor">https://www.npr.org/2025/03/03/nx-s1-5316163/james-harrison-blood-donor</a>, See on <a href="https://news.ycombinator.com/item?id=43245129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="resg-s1-51764">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4649x3094+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F03%2Fbe%2F1b1c2e3c4d289d8a9ca5c2fd2fb9%2Fgettyimages-995411052.jpg" alt="A man looks at a woman holding a baby while he sits on a recliner giving blood." fetchpriority="high">
        </picture>
</div>
<div>
    <div>
        <p>
                James Harrison looks at 8-month-old Layla and her mother, Beth Ismay, in Sydney, Australia, in May 2018. It was the last time Harrison donated blood.
                <b aria-label="Image credit">
                    
                    Subel Bhandari/Picture Alliance via Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Subel Bhandari/Picture Alliance via Getty Images
        
    </span>
</p></div>
   </div>
   <p>Australia's most prolific blood and plasma donor, James Harrison, has died at age 88. Known as the "Man with the Golden Arm," Harrison is credited with saving the lives of 2.4 million babies over the course of more than half a century.</p>   
   
<!-- END ID="RESNX-S1-5316163-100" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Harrison died "peacefully in his sleep" at a nursing home on Feb. 17, Lifeblood — the Australian Red Cross branch responsible for blood donations — <a href="https://www.lifeblood.com.au/news-and-stories/media-centre/media-releases-and-statements/vale-james-harrison-oam" target="_blank">announced on Saturday</a>.</p>   <p>Harrison donated blood and plasma a whopping 1,173 times, according to Lifeblood, every two weeks between 1954 and 2018. All but 10 were from his right arm, the <a href="https://www.npr.org/sections/thetwo-way/2018/05/14/611074956/australias-man-with-the-golden-arm-retires-after-saving-2-4-million-babies" target="_blank"><em>Sydney Morning Herald</em></a> reported.</p>   <p>He "never missed a single appointment," the agency said, and "expected nothing in return." Blood donors are not compensated financially under <a href="https://www.health.gov.au/topics/blood-and-blood-products/what-were-doing-about-blood-and-blood-products" target="_blank">Australian law</a>.</p>   <p>"James was a remarkable, stoically kind, and generous person who was committed to a lifetime of giving and he captured the hearts of many people around the world," Lifeblood CEO Stephen Cornelissen said in a statement. </p>   
   <p>Harrison's plasma contained a rare and precious <a href="https://www.lifeblood.com.au/blood/learn-about-blood/plasma/anti-D" target="_blank">antibody called anti-D</a>, which was discovered in the mid-1960s. It is used in medications to prevent haemolytic disease of the fetus and newborn (HDFN) — also known as rhesus disease — a potentially fatal disease that occurs when a pregnant person's blood is incompatible with that of their unborn baby, prompting their immune system to attack it.</p>   
   
<!-- END ID="RESNX-S1-5316163-101" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>According to Lifeblood, 17% of Australian women who become pregnant end up needing anti-D injections — and most of the country's supply comes from a pool of less than 200 regular plasma donors.</p>   <p>Harrison became the country's first and most prolific anti-D donor, according to Lifeblood. In 1999, he received the Medal of the Order of Australia, one of the country's highest civilian honors. But he downplayed his accomplishments in interviews throughout the years, urging others to roll up their sleeves too.</p>   <p>"Some people say, 'Oh, you're a hero,' " <a href="https://www.npr.org/2015/06/14/414397424/man-with-the-golden-arm-donates-blood-thats-saved-2-million-babies" target="_blank">Harrison told NPR</a> in 2015. "But I'm in a safe room, donating blood. They give me a cup of coffee and something to nibble on. And then I just go on my way. … No problem, no hardship."<br></p>   <h3>Harrison started donating to repay others' generosity</h3>   <div id="resg-s1-51766">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/4307x2864+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F34%2Fbf%2F57188d874da286f360522c50a369%2Fgettyimages-975243976.jpg" alt="Harrison holds an index card documenting his blood donations in the 1960s." loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                At his last blood donation in 2018, James Harrison shows a card documenting his earlier donations.
                <b aria-label="Image credit">
                    
                    Subel Bhandari/picture alliance via Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Subel Bhandari/picture alliance via Getty Images
        
    </span>
</p></div>
   </div>
   <p>When Harrison was 14, he got sick and had to have one of his lungs removed.</p>   
   <p>The grueling process involved a three-month hospital stay, 100 stitches and nearly two gallons of donated blood, <a href="https://www.npr.org/2015/06/14/414397424/man-with-the-golden-arm-donates-blood-thats-saved-2-million-babies" target="_blank">he told NPR</a>. It inspired him to donate his own later — despite his <a href="https://www.lifeblood.com.au/news-and-stories/stories/james-harrison" target="_blank">aversion to needles</a>.</p>   <p>"I was always looking forward to donating, right from the operation, because I don't know how many people it took to save my life," he said. "I never met them, didn't know them."</p>   
   
<!-- END ID="RESNX-S1-5316163-102" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>In 1954, as soon as he hit the legal age of 18, Harrison started giving blood and plasma.</p>   <p>Scientists discovered the anti-D treatment for HDFN about a decade later, and Harrison soon learned his blood contained the rare life-saving antibody — which doctors believe has to do with the blood he received years earlier.</p>   <p>Once Harrison learned he possessed anti-D, <a href="https://www.lifeblood.com.au/news-and-stories/stories/james-harrison" target="_blank">Lifeblood says</a> he was "happy to continue to donate and switch over to plasma donation in order to help as many people as possible."</p>   <p>"I was prepared and wanted to give something back," Harrison said. "And I've been donating for 60 years."</p>   <p>Lifeblood says more than 3 million doses of anti-D containing Harrison's blood have been issued to Australian mothers since 1967. That long list of recipients includes members of his own family.<br></p>   <h3>His donations helped grow and inspire his own family</h3>   <p>Harrison's daughter, Tracey Mellowship, was among the women who received the injection while pregnant.</p>   <p>"As an anti-D recipient myself, he has left behind a family that may not have existed without his precious donations," she said in a statement, adding that her dad was "immensely proud" to have welcomed two great-grandchildren in his final years.</p>   <p>Harrison's contributions didn't just enable his family to grow, but to give back themselves.</p>   <p>"The whole family are blood donors," Harrison told NPR. "And that makes you feel proud, too."</p>   
   
<!-- END ID="RESNX-S1-5316163-103" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>In 2011, his grandson Scott made his first donation — seated right next to Harrison, who was making his 1,000th.</p>   <p>His late wife Barbara was a blood donor as well. Harrison kept donating "even in his darkest days," including after her death, Lifeblood says.</p>   <p>Mellowship said her dad was proud to have saved so many lives "without any cost or pain."</p>   <p>"It made him happy to hear about the many families like ours, who existed because of his kindness," Mellowship said. "He always said it does not hurt, and the life you save could be your own."</p>   <div id="resg-s1-51765">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2880x1807+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fc9%2F58%2Fb6358b494527955859a12f12a857%2Fgettyimages-1079006630.jpg" alt="A black and white photo of Harrison giving blood." loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                James Harrison, pictured giving his 537th blood donation in December 1992.
                <b aria-label="Image credit">
                    
                    Simon Alekna/Fairfax Media Archive via Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Simon Alekna/Fairfax Media Archive via Getty Images
        
    </span>
</p></div>
   </div>
   <h3>He continues to advance scientific research</h3>   <p>Harrison <a href="https://www.npr.org/sections/thetwo-way/2018/05/14/611074956/australias-man-with-the-golden-arm-retires-after-saving-2-4-million-babies" target="_blank">officially retired</a> at age 81, the <a href="https://www.lifeblood.com.au/faq/eligibility/other/age" target="_blank">maximum age</a> for blood donations under Australian law.</p>   
   <p>He made his last donation in May 2018, surrounded by half a dozen grateful mothers holding babies who benefited from the anti-D program.</p>   <p>From the blood center recliner, Harrison bemoaned his forced retirement, telling the<em> </em><a href="https://www.smh.com.au/healthcare/final-donation-for-man-whose-blood-helped-save-2-4-million-babies-20180511-p4zerp.html" target="_blank"><em>Sydney Morning Herald</em></a> that "I'd keep on going if they let me."</p>   <p>But he also spoke optimistically about passing the baton — or, more accurately, the squishy stress ball.</p>   <p>"I hope it's a record that somebody breaks, because it will mean they are dedicated to the cause," he said.</p>   
   
<!-- END ID="RESNX-S1-5316163-104" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Australia has about 200 anti-D donors who help around 45,000 mothers and babies annually, according to Lifeblood.</p>   <p>But because the antibody is so rare, and there are so few human donors able to donate regularly, scientists are also trying to come up with a synthetic version.</p>   <p>Lifeblood is working with Walter and Eliza Hall Institute of Medical Research (WEHI), Australia's oldest research institute, on a project they call "James in a Jar." It could see Harrison continue to save lives long after his death.</p>   <p>"Using the blood of James and other donors, the team has successfully recreated and grown his antibody in the lab — with the hope it will one day help prevent [HDFN], not just for pregnant women in Australia, but also worldwide," it says.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking the Xbox 360 Hypervisor Part 2: The Bad Update Exploit (238 pts)]]></title>
            <link>https://icode4.coffee/?p=1081</link>
            <guid>43244739</guid>
            <pubDate>Mon, 03 Mar 2025 18:06:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://icode4.coffee/?p=1081">https://icode4.coffee/?p=1081</a>, See on <a href="https://news.ycombinator.com/item?id=43244739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									
									
<p>Welcome to part 2 of the Hacking the Xbox 360 Hypervisor blog series. In this part I’ll cover how I found and exploited bugs in the Xbox 360 hypervisor to get full code execution and create the “Bad Update” exploit. If you haven’t already, I highly recommend you read (or at least skim through) <a href="https://icode4.coffee/?p=1047" data-type="post" data-id="1047" target="_blank" rel="noreferrer noopener">part 1</a> as this post will reference a lot of the material discussed there.</p>



<p>As I mentioned in part 1, I consider the Xbox 360 hypervisor to be one of the most secure pieces of code Microsoft has ever written, with only one software bug to date that was likely the result of a compiler issue. I spent a lot of time in the past looking for vulnerabilities in the hypervisor but never found anything of significance. Since then, I’ve spent 7 years working as a security engineer professionally and had developed a whole new mindset for how I analyze targets and find bugs. I was very motivated to put my new skills to the test and see if I could finally hack the Xbox 360 hypervisor. This is what I considered “the final boss” of my journey as a game console hacker.</p>



<h2>Testing Environment</h2>



<p>My goal was to exploit a game save bug I found in <a href="https://icode4.coffee/?p=954#xbox-36-exploit" data-type="post" data-id="954" target="_blank" rel="noreferrer noopener">Tony Hawk’s American Wasteland</a> to start an exploit chain that would hopefully end with me getting hypervisor mode code execution. My testing environment for doing this research consisted of an already modified Xbox 360 console that had full debugging capabilities. This allowed me to write exploit tests in C code which I could run and debug on the console, while also being able to introspect the internal state of the hypervisor. I wanted to focus my efforts on determining if any bugs I found were actually exploitable, and writing the exploit tests in C code that I could debug was the fastest way to do this. Once I had a viable exploit I could worry about triggering it from the Tony Hawk game save bug.</p>



<h2>Analyzing Attack Surface</h2>



<p>In part 1 I talked a bit about the <a href="https://icode4.coffee/?p=1047#system-calls" data-type="post" data-id="1047" target="_blank" rel="noreferrer noopener">system call attack surface</a> the hypervisor exposes to kernel mode. I looked through many of the system calls nearly 15 years ago when I first started researching the console but never found anything noteworthy. However, now that I had a new mindset for bug hunting it was time to look through all of the available attack surface starting fresh. Due to how little attack surface there is in the Xbox 360 hypervisor I decided on “code review” for my method of finding bugs. It didn’t really make sense to try and write a fuzzer or emulation harness and would have been way more work than just doing code review anyway. I say “code review” here because I don’t actually have access to the source code and what I’m really doing is reverse engineering/analyzing the disassembly of the hypervisor binary in IDA.</p>



<p>In my first pass of “code review” I’d be looking for “trivial” bugs, things like out of bounds reads/writes, race conditions/lack of locking, time of check/time of use bugs, integer overflows/underflows, missed validation of parameters, etc. I call these “trivial” because many of these can be exploitable without any additional primitives and don’t require a complex attack scenario.</p>



<h3>System Calls</h3>



<p>At launch the hypervisor had 65 system calls which increased to 120 by the end of its lifecycle, nearly doubling the amount of attack surface it exposed to kernel mode. The hypervisor system calls can be called by any code in kernel mode at any time and provide a range of functionality to support the console OS. The system call attack surface is very secure and has thorough validation of all parameters, only operates on data in protected memory and performs signature checks on most forms of data that involve parsing logic. While there were a few system calls that had notable or interesting behavior (things like cache flushing primitives, arbitrary writes to unprotected memory, etc.) I didn’t find anything that could be used to try and attack the hypervisor itself.</p>



<h3>XeKeysExecute Payloads</h3>



<p>There’s a special system call called <code>HvxKeysExecute</code> that’s used for running small pieces of ad-hoc code in hypervisor mode. This system call takes in a XeKeysExecute (XKE) payload which is a small piece of code signed by Microsoft that the hypervisor will load and execute dynamically. These payloads typically perform a single operation like update or validate some encrypted security file or provide a way for Microsoft to ship anti-cheat payloads through Xbox Live. XKE payloads are good attack surface because they perform non-standard ad-hoc operations, typically take in parameters and data from kernel mode to operate on, and typically have no version or runtime checks that prevent you from running them out of band. Put simply: any executable (game, dashboard, or otherwise) can typically run these at any time on any system software version.</p>



<p>These payloads are most commonly used by system update executables to perform additional security operations when updating the console. I leveraged this to my advantage and downloaded around ~85 system update packages I <a href="https://digiex.net/forums/dashboard-system-updates.62/">found online</a> to help build a collection of payloads for analysis. After writing some scripts to extract the update files and scan them for XKE payloads I had around 25 XKE payloads for analysis. Unfortunately, I didn’t find any trivial bugs in them. There was one payload that stood out as having some interesting behavior that could be exploitable but required I had a way to attack <a href="https://icode4.coffee/?p=1047#encrypted-memory" data-type="post" data-id="1047" target="_blank" rel="noreferrer noopener">encrypted memory</a>. Since I didn’t have any way to do this just yet I made note of the payload and moved on.</p>



<h3>Findings (none)</h3>



<p>I wasn’t hopeful I would find any fruitful results in this first pass and to be honest I would’ve been disappointed if I did. After a few days of code review on the attack surface available I had found nothing notable, which was to be expected. However, this did allow me to learn more about how encrypted and protected memory worked which gave me a few ideas for possible attack vectors. Faced with a lack of attack surface it was time to think outside the box and see if I could “create” more attack surface.</p>



<h2 id="attacking-encrypted-memory">Attacking Encrypted Memory</h2>



<p>With no bugs found the next thing I turned to was trying to attack encrypted memory as this would allow me to try and attack the notable XKE payload I found earlier. Since encrypted memory doesn’t have any CRC checksums it can be modified from outside of the hypervisor, but I’d need to find a way to craft the ciphertext for data I wanted to write. Encrypted memory is typically used in a “write-only” fashion by the hypervisor. It’s clear Microsoft was aware it could be modified by kernel mode and therefore almost never read back from encrypted memory and operate on it. However, this notable XKE payload was doing just that and seemed like it could lead to some exploitable bugs in hypervisor mode.</p>



<p>Attacking this notable XKE payload was going to be difficult so I decided it would be best to first find an easier target and prove I could successfully attack encrypted memory without worrying about extraneous variables. There are only a few things that are stored in encrypted memory, mainly all kernel mode code, as well as&nbsp;some security related data that’s shared between the hypervisor and kernel (revocation lists, kernel&nbsp;mode RNG state, etc.). Attacking the encrypted memory for some kernel mode code seemed like a good test candidate as I could overwrite it with my own assembly code to do something like change the console’s LED colors, thus proving the attack worked.</p>



<h2>Encrypted Memory Allocations</h2>



<p>Both the protected and encrypted memory pathways perform encryption by mixing together a 10-bit whitening value, a per-boot per-pathway encryption key, and the address of cache line containing the memory being accessed. This prevents attackers from trying ciphertext manipulation attacks such:</p>



<ul>
<li>Using ciphertext from address A at address B where A != B.</li>



<li>Using ciphertext from the protected pathway with the encrypted pathway (or vice versa).</li>



<li>Using ciphertext across resets/reboots of the CPU.</li>
</ul>



<p>The hypervisor manages a table of values that I’ll refer to as the “page whitening table” which tracks encrypted pages of memory (on a 64KB granularity) and what whitening values have already been used on those pages. Every time the hypervisor allocates a page of memory for the encrypted pathway it’ll call a function I’ve named “HvpSetPageWhiteningBits”. This function will check the page whitening table to determine the next whitening value to use for the page. When the console boots, all entries are initialized to 0 and each time a page of encrypted memory is allocated the whitening value will be incremented by 1 until it reaches 1024 (the point at which a 10-bit integer would overflow). At this point an overflow bit will be set which indicates all possible whitening values have been used exactly once. Any allocation of the page thereafter will use a randomly generated number for the whitening value. The pseudo code for this function can be seen below:</p>



<div id="urvanov-syntax-highlighter-67c61f5b71657905577838" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p></div>
				</td>
						<td><div><p><span>struct</span><span> </span><span>PAGE_WHITENING_TABLE_ENTRY</span></p><p><span>{</span></p><p><span>	</span><span>WORD</span><span> </span><span>WhiteningValue</span><span> </span><span>:</span><span> </span><span>10</span><span>;</span></p><p><span>	</span><span>WORD</span><span> </span><span>WhiteningOverflow</span><span> </span><span>:</span><span> </span><span>1</span><span>;</span></p><p><span>	</span><span>WORD</span><span> </span><span>Valid</span><span> </span><span>:</span><span> </span><span>1</span><span>;</span></p><p><span>}</span><span>;</span></p><p><span>DWORD </span><span>HvpSetPageWhiteningBits</span><span>(</span><span>PAGE_WHITENING_TABLE_ENTRY</span><span>*</span><span> </span><span>pPageAllocationTablePtr</span><span>,</span><span> </span><span>DWORD </span><span>pageNumber</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>WORD</span><span> </span><span>whiteningValue</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>	</span><span>// Get the whitening entry for the page.</span></p><p><span>	</span><span>PAGE_WHITENING_TABLE_ENTRY</span><span>*</span><span> </span><span>pPageEntry</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>pPageWhiteningTablePtr</span><span>[</span><span>pageNumber</span><span>]</span><span>;</span></p><p><span>	</span><span>// Check if the whitening values have all been used at least once.</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>pPageEntry</span><span>-&gt;</span><span>WhiteningOverflow</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>// Use the next whitening value until overflow is triggered.</span></p><p><span>		</span><span>whiteningValue</span><span> </span><span>=</span><span> </span><span>pPageEntry</span><span>-&gt;</span><span>WhiteningBits</span><span>++</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>else</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>// Randomize the whitening value.</span></p><p><span>		</span><span>XeCryptRandom</span><span>(</span><span>&amp;</span><span>whiteningValue</span><span>,</span><span> </span><span>sizeof</span><span>(</span><span>whiteningValue</span><span>)</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>// Isolate and return the 10-bit whitening value.</span></p><p><span>	</span><span>return</span><span> </span><span>whiteningValue</span><span> </span><span>&amp;</span><span> </span><span>0x3FF</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>The purpose of this function is to introduce entropy into the encryption used across allocations of the same page of memory. Each time the page is allocated the ciphertext will change even if the plaintext data doesn’t.</p>



<h3>A Hypothetical Attack Scenario</h3>



<p>While it may seem like all the per-boot/per-pathway/per-page encryption feeds make it difficult to perform ciphertext attacks this is actually quite easy to defeat. Sure we could just change the ciphertext arbitrarily and the resulting plaintext would change as well. But this doesn’t give us any control over what the resulting plaintext data will be and trying to brute force out even 16 bytes of random data that happens to give us some 16 bytes of plaintext data we want isn’t really feasible. </p>



<p>However, say we have some way to get the ciphertext for arbitrary plaintext data even though we don’t know what whitening value was used to create the ciphertext. For any given address there’s only 1024 possible ciphertext values for any constant plaintext data because the only entropy into the memory encryption will be the 10-bit whitening value. 10-bits is a very small search space and could easily be brute forced through in seconds to minutes if we had some primitive to cycle the whitening value and encrypt some known plaintext data.</p>



<p>Consider the following attack scenario:</p>



<ol>
<li>Choose some 64KB page of memory at address A such that A falls within the executable region for some executable file (could be a game executable or dll).</li>



<li>Get the ciphertext for all possible whitening values for the plaintext data we want to write into memory at address A (this would be our kernel mode shell code). There will be exactly 1024 different ciphertext values.</li>



<li>Load a game executable or dll into memory (a dll is easier because it can be loaded in a suspended state).</li>



<li>Loop for 1024 times and perform the following:
<ol>
<li>Write the next block of ciphertext to memory and flush CPU cache. This can be done using a DMA operation from another device such as the GPU or southbridge (remember this memory is read-only in kernel mode from the CPU’s perspective).</li>



<li>Read back the plaintext data using the encrypted pathway (basically just perform a normal memory read from kernel mode) and check it matches the expected plaintext data. </li>



<li>If the whitening value currently being used for the page of memory doesn’t match the whitening value used to generate the ciphertext then the resulting plaintext data will be garbage. Only once the whitening values match will the plaintext data be correct (i.e.: data was decrypted successfully).</li>



<li>Once the plaintext data is valid we have successfully overwritten encrypted memory with our own arbitrary data. Since we’re overwriting kernel mode code we can now execute this memory in kernel mode and run small amounts of our own arbitrary code.</li>
</ol>
</li>
</ol>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b8036b&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1116" height="393" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/kernel_code_exec_attack-4.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/kernel_code_exec_attack-4.png 1116w, https://icode4.coffee/wp-content/uploads/kernel_code_exec_attack-4-300x106.png 300w, https://icode4.coffee/wp-content/uploads/kernel_code_exec_attack-4-768x270.png 768w" sizes="(max-width: 1116px) 100vw, 1116px"><figcaption>Figure 1: Kernel mode code execution attack scenario</figcaption></figure></div>


<p>The only missing primitive here is a way to generate ciphertext for some arbitrary plaintext data.</p>



<h2>Crafting Ciphertext</h2>



<p>The primary source of ciphertext for the encrypted pathway is kernel mode code which must be RSA signed by Microsoft. However, tracing all the references to the <code>HvpSetPageWhiteningBits</code> function I found another code path that would work in my favor. The hypervisor implements three system calls that can be used to reserve, encrypt, and release an allocation in encrypted memory at an arbitrary address called <code>HvxEncryptedReserveAllocation</code>, <code>HvxEncryptedEncryptAllocation</code>, and <code>HvxEncryptedReleaseAllocation</code>.</p>



<p>The hypervisor enforces that all executable code is encrypted at runtime and the only memory that’s not encrypted is kernel mode data. This is mostly because data allocations may be used with external devices such as the GPU, southbridge, audio encoder, etc. which have no ability to perform decryption on the data. However, Microsoft wanted to give developers the ability to store data in encrypted memory and created the HvxEncrypted* API set. I’m not entirely sure what the attack scenario would be where one would want to use these APIs, presumably to hide some game data (cryptographic keys maybe?) at runtime from anyone who could dump the contents of RAM from outside the CPU. I can’t really think of any data that would justify the need for these APIs and I honestly don’t know if any game ever used them, but they certainly came in handy for exploitation.</p>



<p>The HvxEncrypted* APIs are pretty simple to use, you start by making a physical memory allocation using the unprotected pathway at the physical memory location you want to generate ciphertext for. Next you pass this address to <code>HvxEncryptedReserveAllocation</code> which marks the pages as in-use, followed by a call to <code>HvxEncryptedEncryptAllocation</code> which sets the page whitening bits and flushes CPU cache. At this point you now have two “views” of the same physical memory location, one through the unprotected pathway using the physical memory allocation address, and one through the encrypted pathway using a virtual address mapped by <code>HvxEncryptedReserveAllocation</code>.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b81401&quot;}" data-wp-interactive="core/image"><img decoding="async" width="785" height="246" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/multiple_pathways-4.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/multiple_pathways-4.png 785w, https://icode4.coffee/wp-content/uploads/multiple_pathways-4-300x94.png 300w, https://icode4.coffee/wp-content/uploads/multiple_pathways-4-768x241.png 768w" sizes="(max-width: 785px) 100vw, 785px"><figcaption>Figure 2: Multiple pathways to the same memory location</figcaption></figure></div>


<p>Using these APIs we can craft the ciphertext for arbitrary data by simply writing our plaintext data using the encrypted address, flushing CPU cache, and reading back from the unprotected address. We won’t know what whitening value was used for the encryption but that doesn’t matter. We can simply generate the ciphertext for the data we plan to overwrite and use it as an indicator to know when the whitening value currently in use matches the whitening value used to generate our malicious ciphertext. I’ll hereby refer to this indicator data as our “oracle” data (not to be confused with an oracle machine) as it’s essentially guiding us to finding a whitening value collision.</p>



<h2>Overwriting Read-Only Memory</h2>



<p>The memory we want to overwrite with our malicious ciphertext is executable code which is marked read-only in kernel mode, so we’ll need to find a way to work around this. There are a couple different techniques I’ve used to overwrite arbitrary memory while ignoring page permissions:</p>



<ul>
<li>Southbridge DMA operations
<ul>
<li>The CPU communicates with peripheral devices such as the disc drive, HDD, and network port through the southbridge. By setting up Direct Memory Access (DMA) operations with a peripheral device the southbridge can read and write to any location in RAM as specified by the request.</li>
</ul>
</li>



<li>GPU shaders
<ul>
<li>The Xbox 360 uses a unified memory model which means the GPU doesn’t have dedicated VRAM, and instead accesses main RAM directly. The GPU also implements a feature called “memory export shaders” which are similar to modern day fragment shaders. These shaders allow the GPU to write back to RAM to create vertex, index, or texture buffers dynamically. By crafting a special shader it’s possible to use it as a memcpy primitive to read and write to anywhere in RAM.</li>
</ul>
</li>



<li>Hypervisor APIs
<ul>
<li>There are a few places where the hypervisor will perform reads and writes to unprotected memory addresses in order to read/write input and output parameters for certain system calls. By finding some APIs that can be used together you can craft a memcpy primitive by calling the APIs in a loop to copy data between two unprotected memory addresses.</li>
</ul>
</li>
</ul>



<p>I’ll cover each of these techniques in-depth in part 3, but for now I’m going to focus on the hypervisor APIs technique as this is what I used for the exploit.</p>



<h3>Hypervisor Key Storage</h3>



<p>When running in hypervisor mode all memory is considered read-write-execute. If you can find one or more APIs that work as a “read from address A write to address B” primitive you can have the hypervisor do the memcpy operation for you. There are a few places where the hypervisor will perform reads and writes to memory addresses provided by kernel mode using the unprotected pathway. However, I only found two particular APIs that would actually work for a viable memcpy operation: <code>HvxKeysExGetKey</code> and <code>HvxKeysExSetKey</code>.</p>



<p>These two APIs are used to get and set keys in the extended key store that the hypervisor manages. Almost all of the keys in the normal and extended key stores are not accessible to kernel mode and the ones that are accessible are typically read-only. However, the extended key store has a couple keys that are both accessible and writable from kernel mode. I’m not sure what the key slots are supposed to be used for (perhaps IP-TV keys/certificates) but the slot is writable and that’s all that matters.</p>



<p>By calling <code>HvxKeysExSetKey</code> we can store the data we want to write into the extended key store, and call <code>HvxKeysExGetKey</code> to retrieve it. When the retrieval happens the hypervisor will write the key value to the memory address provided by kernel mode using the unprotected pathway. As long as the memory address is outside of the hypervisor address space it’ll write to any memory location you specify. By putting these two APIs into a loop you can create an arbitrary memcpy primitive:</p>



<div id="urvanov-syntax-highlighter-67c61f5b71669367715360" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p></div>
				</td>
						<td><div><p><span>void</span><span> </span><span>MemcpyCipherText</span><span>(</span><span>ULONG </span><span>DstPhys</span><span>,</span><span> </span><span>ULONG </span><span>SrcPhys</span><span>,</span><span> </span><span>ULONG </span><span>size</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>while</span><span> </span><span>(</span><span>size</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>// Setup next block size.</span></p><p><span>		</span><span>ULONG </span><span>copySize</span><span> </span><span>=</span><span> </span><span>min</span><span>(</span><span>size</span><span>,</span><span> </span><span>2048</span><span>)</span><span>;</span></p><p><span>		</span><span>*</span><span>(</span><span>ULONG</span><span>*</span><span>)</span><span>pSizeScratch</span><span> </span><span>=</span><span> </span><span>copySize</span><span>;</span></p><p><span>		</span><span>// Store the next block to the key store and read it back to the target address.</span></p><p><span>		</span><span>ULONG </span><span>result</span><span> </span><span>=</span><span> </span><span>HvxKeysExSetKey</span><span>(</span><span>0x102</span><span>,</span><span> </span><span>SrcPhys</span><span>,</span><span> </span><span>copySize</span><span>)</span><span>;</span></p><p><span>		</span><span>result</span><span> </span><span>=</span><span> </span><span>HvxKeysExGetKey</span><span>(</span><span>0x102</span><span>,</span><span> </span><span>DstPhys</span><span>,</span><span> </span><span>SizeScratchPhysAddr</span><span>)</span><span>;</span></p><p><span>		</span><span>SrcPhys</span><span> </span><span>+=</span><span> </span><span>copySize</span><span>;</span></p><p><span>		</span><span>DstPhys</span><span> </span><span>+=</span><span> </span><span>copySize</span><span>;</span></p><p><span>		</span><span>size</span><span> </span><span>-=</span><span> </span><span>copySize</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<h2>Getting Kernel Mode Code Execution</h2>



<p>Now that we have a way to generate ciphertext for arbitrary data and write it to read-only memory it’s time to put everything together and get some arbitrary kernel mode code execution. Since I was testing this on an already hacked console I was able to write the poc for this in C code which I’ve included below. The steps I used for this attack are slightly different than the ones I outlined above but ultimately arrives at the same result.</p>



<p>First we’ll need an executable file we can load and unload dynamically. I chose to use the boot animation dll because it’s stored in system flash (so it exists on every console), can be loaded in a suspended state, and doesn’t terminate the executable that’s currently running. Then I chose the address of some executable code in the dll, <code>0x98030000</code>, to overwrite with my shell code. The address of any 64KB page of executable code will work.</p>



<h3>Getting the Oracle Data</h3>



<p>Next we’ll need to get some 16 bytes of plaintext data from the boot animation at the chosen target address (<code>0x98030000</code>), which we’ll use to create our oracle ciphertext data. We’ll also get the physical memory address our target address maps to which we’ll need later on.</p>



<div id="urvanov-syntax-highlighter-67c61f5b7168d603547955" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>// Load the boot animation so we can get the physical address of the last code segment.</span></p><p><span>DWORD </span><span>result</span><span> </span><span>=</span><span> </span><span>XexLoadImage</span><span>(</span><span>"Flash:\\bootanim.xex"</span><span>,</span><span> </span><span>0x40000009</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>&amp;</span><span>hBootAnim</span><span>)</span><span>;</span></p><p><span>// Get the physical address of the last code segment.</span></p><p><span>DWORD </span><span>codePhysAddr</span><span> </span><span>=</span><span> </span><span>MmGetPhysicalAddress</span><span>(</span><span>(</span><span>void</span><span>*</span><span>)</span><span>0x98030000</span><span>)</span><span>;</span></p><p><span>// Copy the data at this address to use as our whitening oracle.</span></p><p><span>memcpy</span><span>(</span><span>abOracleData</span><span>,</span><span> </span><span>(</span><span>void</span><span>*</span><span>)</span><span>0x98030000</span><span>,</span><span> </span><span>16</span><span>)</span><span>;</span></p><p><span>// Unload the boot animation so we can reclaim the memory.</span></p><p><span>result</span><span> </span><span>=</span><span> </span><span>XexUnloadImage</span><span>(</span><span>hBootAnim</span><span>)</span><span>;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<h3>Generating Our Ciphertext</h3>



<p>Next we loop 1024 times and allocate encrypted memory at the chosen target address. This will exhaust the whitening values for the chosen 64KB page and cause the overflow bit to get set such that each subsequent allocation of the page will use a random whitening value. For each iteration we’ll have two addresses to the same physical memory location, an unprotected address which just fetches whatever data is in RAM, and an encrypted address which will encrypt/decrypt data from RAM on access.</p>



<p>After ~50% of the search space has been used (so around i=512) we’ll write the plaintext oracle data to the encrypted address and read back the corresponding ciphertext from the unprotected address. We’ll do the same operation again for the shell code we want to write to memory and capture the corresponding ciphertext. We now have the ciphertext for the oracle data, and the ciphertext for our shell code at virtual address <code>0x98030000</code> for some unknown whitening value (presumably it’s 513 but it doesn’t really matter).</p>



<div id="urvanov-syntax-highlighter-67c61f5b71693813996851" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p></div>
				</td>
						<td><div><p><span>bool</span><span> </span><span>GetPayloadCipherText</span><span>(</span><span>DWORD </span><span>TargetPhysicalAddr</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>// Arbitrary virtual address that will get mapped through the encrypted pathway to TargetPhysicalAddr.</span></p><p><span>	</span><span>BYTE</span><span> </span><span>*</span><span>pEncryptedAddress</span><span> </span><span>=</span><span> </span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>0x8D000000</span><span>;</span></p><p><span>	</span><span>// Allocate physical memory (through the unprotected pathway) for the target address we want to encrypt data at.</span></p><p><span>	</span><span>BYTE</span><span>*</span><span> </span><span>pUnprotectedAddress</span><span> </span><span>=</span><span> </span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>XPhysicalAlloc</span><span>(</span><span>0x10000</span><span>,</span><span> </span><span>(</span><span>ULONG_PTR</span><span>)</span><span>TargetPhysicalAddr</span><span>,</span><span> </span><span>0x10000</span><span>,</span><span> </span><span>PAGE_READWRITE</span><span> </span><span>|</span><span> </span><span>MEM_LARGE_PAGES</span><span>)</span><span>;</span></p><p><span>	</span><span>// Loop and exhaust all possible whitening values for the memory address.</span></p><p><span>	</span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>1024</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>// Allocate encrypted memory.</span></p><p><span>		</span><span>DWORD </span><span>result</span><span> </span><span>=</span><span> </span><span>HvxEncryptedReserveAllocation</span><span>(</span><span>(</span><span>DWORD</span><span>)</span><span>pEncryptedAddress</span><span>,</span><span> </span><span>TargetPhysicalAddr</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>		</span><span>result</span><span> </span><span>=</span><span> </span><span>HvxEncryptedEncryptAllocation</span><span>(</span><span>(</span><span>DWORD</span><span>)</span><span>pEncryptedAddress</span><span>)</span><span>;</span></p><p><span>		</span><span>// Wait until we exhaust ~50% of the whitening space before capturing cipher text.</span></p><p><span>		</span><span>if</span><span> </span><span>(</span><span>i</span><span> </span><span>==</span><span> </span><span>1024</span><span> </span><span>/</span><span> </span><span>2</span><span>)</span></p><p><span>		</span><span>{</span></p><p><span>			</span><span>// Pass 1: copy the oracle data that will be loaded at this address when bootanim.xex is loaded.</span></p><p><span>			</span><span>memcpy</span><span>(</span><span>pEncryptedAddress</span><span>,</span><span> </span><span>abOracleData</span><span>,</span><span> </span><span>16</span><span>)</span><span>;</span></p><p><span>			</span><span>// Flush cache and read back ciphertext.</span></p><p><span>			</span><span>KeFlushCacheRange</span><span>(</span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>pEncryptedAddress</span><span>,</span><span> </span><span>0x80</span><span>)</span><span>;</span></p><p><span>			</span><span>KeFlushCacheRange</span><span>(</span><span>pUnprotectedAddress</span><span>,</span><span> </span><span>0x80</span><span>)</span><span>;</span></p><p><span>			</span><span>memcpy</span><span>(</span><span>pPayloadCipherText</span><span>,</span><span> </span><span>pUnprotectedAddress</span><span>,</span><span> </span><span>16</span><span>)</span><span>;</span></p><p><span>			</span><span>// Pass 2: copy our shell code to the target address we want to overwrite.</span></p><p><span>			</span><span>memset</span><span>(</span><span>pEncryptedAddress</span><span>,</span><span> </span><span>0x41</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>			</span><span>ReadFile</span><span>(</span><span>"game:\\shell_code.bin"</span><span>,</span><span> </span><span>pEncryptedAddress</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>			</span><span>// Flush cache and read back ciphertext.</span></p><p><span>			</span><span>KeFlushCacheRange</span><span>(</span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>pEncryptedAddress</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>			</span><span>KeFlushCacheRange</span><span>(</span><span>pUnprotectedAddress</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>			</span><span>memcpy</span><span>(</span><span>pPayloadCipherText</span><span> </span><span>+</span><span> </span><span>0x80</span><span>,</span><span> </span><span>pUnprotectedAddress</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>		</span><span>}</span></p><p><span>		</span><span>// Free the encrypted allocation.</span></p><p><span>		</span><span>result</span><span> </span><span>=</span><span> </span><span>HvxEncryptedReleaseAllocation</span><span>(</span><span>(</span><span>DWORD</span><span>)</span><span>pEncryptedAddress</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>XPhysicalFree</span><span>(</span><span>pUnprotectedAddress</span><span>)</span><span>;</span></p><p><span>	</span><span>return</span><span> </span><span>true</span><span>;</span></p><p><span>}</span></p><p><span>// Allocate physical memory for the shell code cipher text.</span></p><p><span>pPayloadCipherText</span><span> </span><span>=</span><span> </span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>XPhysicalAlloc</span><span>(</span><span>0x10080</span><span>,</span><span> </span><span>MAXULONG_PTR</span><span>,</span><span> </span><span>0x80</span><span>,</span><span> </span><span>PAGE_READWRITE</span><span> </span><span>|</span><span> </span><span>MEM_LARGE_PAGES</span><span>)</span><span>;</span></p><p><span>// Get the physical address of the shell code cipher text buffer.</span></p><p><span>PayloadCipherTextPhysAddr</span><span> </span><span>=</span><span> </span><span>MmGetPhysicalAddress</span><span>(</span><span>pPayloadCipherText</span><span>)</span><span>;</span></p><p><span>// Find the cipher text for the data we want to overwrite.</span></p><p><span>GetPayloadCipherText</span><span>(</span><span>codePhysAddr</span><span>)</span><span>;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<h3>Finding a Whitening Value Collision</h3>



<p>Finally, in an infinite loop we’ll load the boot animation dll and check the ciphertext for our chosen target address <code>0x98030000</code>. If it matches our oracle ciphertext we know the 64KB page was encrypted using the same whitening value that was used to generate our malicious ciphertext, and we can now write the ciphertext for our shell code to memory. If the oracle data doesn’t match it means a different whitening value was used and we’ll unload the boot animation dll and try again. Once we get a hit we can use the hypervisor extended key store memcpy technique to copy the ciphertext for our shell code to the 64KB executable page which is marked read-only. Then we just jump to the address <code>0x98030000</code> and our shell code will execute.</p>



<div id="urvanov-syntax-highlighter-67c61f5b71696367930165" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p></div>
				</td>
						<td><div><p><span>// Loop and load the boot animation until we find the correct whitening bits.</span></p><p><span>DWORD </span><span>count</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>while</span><span> </span><span>(</span><span>true</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>// Load the boot animation.</span></p><p><span>	</span><span>result</span><span> </span><span>=</span><span> </span><span>XexLoadImage</span><span>(</span><span>"Flash:\\bootanim.xex"</span><span>,</span><span> </span><span>0x40000009</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>&amp;</span><span>hBootAnim</span><span>)</span><span>;</span></p><p><span>	</span><span>// Get the cipher text for the target address.</span></p><p><span>	</span><span>MemcpyCipherText</span><span>(</span><span>PayloadCipherTextPhysAddr</span><span> </span><span>+</span><span> </span><span>16</span><span>,</span><span> </span><span>codePhysAddr</span><span>,</span><span> </span><span>16</span><span>)</span><span>;</span></p><p><span>	</span><span>KeFlushCacheRange</span><span>(</span><span>pPayloadCipherText</span><span>,</span><span> </span><span>0x80</span><span>)</span><span>;</span></p><p><span>	</span><span>// Check if the ciphertext in memory matches the oracle ciphertext we generated earlier.</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>pPayloadCipherText</span><span>,</span><span> </span><span>pPayloadCipherText</span><span> </span><span>+</span><span> </span><span>16</span><span>,</span><span> </span><span>16</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>// Overwrite the boot animation code with our shell code ciphertext and flush cache.</span></p><p><span>		</span><span>MemcpyCipherText</span><span>(</span><span>codePhysAddr</span><span>,</span><span> </span><span>PayloadCipherTextPhysAddr</span><span> </span><span>+</span><span> </span><span>0x80</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>		</span><span>KeFlushCacheRange</span><span>(</span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>0x98030000</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>		</span><span>HvxFlushDCacheRange</span><span>(</span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>codePhysAddr</span><span>,</span><span> </span><span>0x10000</span><span>)</span><span>;</span></p><p><span>		</span><span>// Call our payload.</span></p><p><span>		</span><span>ULONG</span><span> </span><span>(</span><span>__stdcall</span><span> </span><span>*</span><span>pPayloadFunc</span><span>)</span><span>(</span><span>)</span><span> </span><span>=</span><span> </span><span>(</span><span>ULONG</span><span>(</span><span>__stdcall</span><span>*</span><span>)</span><span>(</span><span>)</span><span>)</span><span>0x98030000</span><span>;</span></p><p><span>		</span><span>result</span><span> </span><span>=</span><span> </span><span>pPayloadFunc</span><span>(</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>// Unload the boot animation.</span></p><p><span>	</span><span>result</span><span> </span><span>=</span><span> </span><span>XexUnloadImage</span><span>(</span><span>hBootAnim</span><span>)</span><span>;</span></p><p><span>	</span><span>count</span><span>++</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>With everything in place I wrote a small piece of assembly code that would set the console’s ring of light to full orange to know when it executed. I ran the exploit and after 30 seconds to a minute it was able to find a whitening value collision and my assembly code ran. I captured a view of the code from a debugger which you can see here:</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b82425&quot;}" data-wp-interactive="core/image"><img decoding="async" width="670" height="539" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/image.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/image.png 670w, https://icode4.coffee/wp-content/uploads/image-300x241.png 300w" sizes="(max-width: 670px) 100vw, 670px"><figcaption>Figure 3: Kernel mode shell code as seen by the debugger</figcaption></figure></div>


<p>Now that I had a way to attack encrypted memory it was time to loop back to the notable XKE payload and see if attacking it was feasible.</p>



<figure><div>
<blockquote data-width="470" data-dnt="true"><p lang="en" dir="ltr">I can now get up to 64kb of arbitrary kernel mode code execution on 360 but still no hv code exec. I'm starting to run out of unhinged attack ideas…</p>— Ryan M (@Grimdoomer) <a href="https://twitter.com/Grimdoomer/status/1832178471881282035?ref_src=twsrc%5Etfw">September 6, 2024</a></blockquote>
</div></figure>



<h2 id="bootloader-update-payload">The Bootloader Update Payload</h2>



<p>This notable XKE payload that I’ll refer to as the “bootloader update payload” is used during system updates when the 2nd stage bootloader (2bl) needs to be updated. The reason this payload is interesting is because it reads in data from kernel mode and performs LZX decompression on it. The decompression process requires a scratch buffer to store the LZX decoder context structure which contains various pointers that the hypervisor will use when decompressing data. This scratch buffer is provided by kernel mode and relocated to <strong>encrypted memory</strong>, which means it doesn’t have any integrity checks and we can attack it from kernel mode asynchronously.</p>



<div id="urvanov-syntax-highlighter-67c61f5b7169c514080037" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p></div>
				</td>
						<td><div><p><span>typedef</span><span> </span><span>struct</span></p><p><span>{</span></p><p><span>	</span><span>/* pointer to beginning of window buffer */</span></p><p><span>	</span><span>byte</span><span>*</span><span>				</span><span>dec_mem_window</span><span>;</span></p><p><span>	</span><span>.</span><span>.</span><span>.</span></p><p><span>	</span><span>/* input (compressed) data pointers */</span></p><p><span>	</span><span>byte</span><span>*</span><span>				</span><span>dec_input_curpos</span><span>;</span></p><p><span>	</span><span>byte</span><span>*</span><span>				</span><span>dec_end_input_pos</span><span>;</span></p><p><span>	</span><span>/* output (uncompressed) data pointer */</span></p><p><span>	</span><span>byte</span><span>*</span><span>				</span><span>dec_output_buffer</span><span>;</span></p><p><span>	</span><span>.</span><span>.</span><span>.</span></p><p><span>	</span><span>/* memory allocation functions */</span></p><p><span>	</span><span>PFNALLOC			</span><span>dec_malloc</span><span>;</span></p><p><span>	</span><span>PFNFREE				</span><span>dec_free</span><span>;</span></p><p><span>	</span><span>long</span><span>*</span><span>				</span><span>dec_memory</span><span>;</span></p><p><span>	</span><span>.</span><span>.</span><span>.</span></p><p><span>}</span><span> </span><span>t_decoder_context</span><span>;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>Looking at the decoder context structure there’s a couple juicy pointers we could try and attack. The <code>dec_malloc</code> and <code>dec_free</code> pointers are function pointers so overwriting these would result in the hypervisor jumping directly to some shell code we control. The <code>dec_output_buffer</code> pointer contains the address where decompressed data should be written to, so overwriting this pointer would result in data being written to any real mode address we choose (such as hypervisor code or data). All we need to do is generate the ciphertext for the address we want to write into the decoder context structure and the hypervisor will gladly use any real mode address we put there, whether it points to protected, encrypted, or unprotected memory. So why would Microsoft store this data in encrypted memory and not protected memory if they knew it could be modified asynchronously by kernel mode?</p>



<h2>Analyzing the Payload</h2>



<p>When the system update process is running and the 2nd stage bootloader needs to be updated, the updater application can have the hypervisor run this XKE payload and it’ll build an updated boot chain for the console in memory. The way this is supposed to work is kernel mode creates one large memory allocation in unprotected memory (around 700-800 KB) that gets divided up into different regions. These regions include a header, update data, decompressed bootloader data, LZX scratch buffer, and a region for the newly updated boot chain.</p>



<p>When the payload starts running it’ll relocate these regions in-place to either encrypted or protected memory depending on the size of the region. The header and compressed update data are relocated to protected memory as both regions are smaller than 64KB, and the decompressed bootloader data, LZX scratch buffer, and updated boot chain are relocated to encrypted memory as each region is too large to fit into a single slot of protected memory (64KB). Technically speaking each region could fit into protected memory if it spanned more than one slot but because a change in slot means a change in the upper 32-bits of a real mode address you’d have to handle this address break in code. You can’t just roll off the end of the 64KB page for slot N and into slot N+1 as even though the memory may be physically contiguous the addresses for each slot are not.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b8305f&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1144" height="720" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/working_buffer_layout.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/working_buffer_layout.png 1144w, https://icode4.coffee/wp-content/uploads/working_buffer_layout-300x189.png 300w, https://icode4.coffee/wp-content/uploads/working_buffer_layout-768x483.png 768w, https://icode4.coffee/wp-content/uploads/working_buffer_layout-110x70.png 110w" sizes="auto, (max-width: 1144px) 100vw, 1144px"><figcaption>Figure 4: Layout of the working buffer</figcaption></figure></div>


<p>Once the memory is relocated the payload will process the update data which is a compressed, encrypted, RSA signed blob that contains every possible 2nd stage bootloader for all the different hardware revisions of the console. The payload will first verify the RSA signature (that only Microsoft can sign) is valid, then decrypt the data, and then decompress it into the decompressed data region. Next the payload will scan a lookup table in the header of the compressed blob that contains a listing of every bootloader in the blob, what CPU, GPU, and HANA chip they match to, and try to match one of the entries to the console. If a match is found and the blob contains an updated 2nd stage bootloader for the console, the payload will craft a new boot chain and store it in the final region of the data buffer.</p>



<p>The size of the LZX scratch buffer is calculated as <code>sizeof(LZX_DECODER_CTX) + 2 * window_size</code>, where <code>window_size</code> is the block size of data to be compressed. In other words, the input data is processed in blocks of <code>window_size</code> bytes and compressed. To decompress a block you’ll need a buffer of <code>window_size</code> bytes to hold the decompressed data. In the case of this payload the window size chosen was 0x8000 bytes (32KB), thus the size of the scratch buffer is <code>sizeof(LZX_DECODER_CTX) + 0x10000</code> and won’t fit into a single slot of protected memory. Had the window size been slightly smaller the scratch buffer would have been able to fit into protected memory. Fortunately for us this works to our advantage because we can attack the encrypted memory for the scratch buffer using the techniques discussed earlier.</p>



<h2>Attacking LZX: Attempt 1</h2>



<p>In order to attack one of the pointers in the LZX decoder context structure we’ll need the ciphertext for some oracle data to check for a whitening value collision, and the ciphertext for the malicious pointer we want to write into the structure. Using the technique described above the attack would look something like this:</p>



<ol>
<li>Obtain the ciphertext for the oracle data and malicious pointer we want to write. For this attack we’ll target the <code>dec_malloc</code> function pointer in the LZX decoder structure. The pointer should be a real mode address that points to some shell code we setup in unprotected memory.</li>



<li>Create two threads:
<ul>
<li>Thread A will be the payload thread and continuously run the bootloader update payload in a loop.</li>



<li>Thread B will be the attack thread which continuously monitors the input data for the oracle ciphertext. Once the payload relocates the input data to encrypted memory we can observe the ciphertext for it by accessing the memory from kernel mode using the unprotected pathway. Once the ciphertext observed matches our oracle ciphertext we know a whitening value collision was hit and it’s safe to overwrite the LZX decoder structure.</li>
</ul>
</li>



<li>Once a whitening collision is observed, thread B will overwrite the <code>dec_malloc</code> pointer in the LZX decoder structure with the pre-computed ciphertext for our malicious pointer (which points to our shell code).</li>



<li>If we win the race between thread B overwriting the pointer and thread A using the pointer from hypervisor mode, then the hypervisor should jump to our shell code and we’ll get full hypervisor code execution.</li>
</ol>



<p>For the oracle data I chose to use the ciphertext for the first 16 bytes of LZX decoder structure with values of all 00s. When the payload does the in-place relocation of the LZX scratch buffer it’ll zero-initialize it afterwards. By capturing the ciphertext for 16 bytes of 00s at the same address I can use the zero-initialization of the scratch buffer as a start indicator to know when to hammer the decoder structure with the ciphertext for our malicious function pointer. This is the earliest point in time we can detect a whitening value collision and we’re gonna need as much time as we can get to win the race condition.</p>



<h3>Cache Rules Everything Around Me</h3>



<p>Unfortunately attacking the <code>dec_malloc</code> pointer isn’t feasible and the reason is that the CPU L2 cache is just too big for the data containing the decoder structure to age out of cache in the time frame we need. When the XKE payload goes to decompress the input data it’ll call <code>LDICreateDecompression</code> which takes in either two function pointers (one for <code>dec_malloc</code> and one for <code>dec_free</code>) or a pointer to a pre-allocated scratch buffer. In the case of this payload the function pointers are always null and a pointer to the LZX scratch buffer region is provided instead. Regardless of which you provide, the <code>dec_malloc</code> pointer will always be set to the value provided in the function arguments.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b83a6a&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="583" height="724" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/ldi_create_decompression_pfma-1.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/ldi_create_decompression_pfma-1.png 583w, https://icode4.coffee/wp-content/uploads/ldi_create_decompression_pfma-1-242x300.png 242w" sizes="auto, (max-width: 583px) 100vw, 583px"><figcaption>Figure 5: Disassembly of dec_malloc being set to NULL</figcaption></figure></div>


<p>Immediately after the <code>dec_malloc</code> function pointer is set to NULL <code>LDICreateDecompression</code> calls <code>sub_2668</code> which will initialize some more of the decoder structure and then try to use the <code>dec_malloc</code> pointer if it’s non-NULL.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b8418c&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="768" height="793" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/ldi_init_use_pfma-4-768x793.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/ldi_init_use_pfma-4-768x793.png 768w, https://icode4.coffee/wp-content/uploads/ldi_init_use_pfma-4-290x300.png 290w, https://icode4.coffee/wp-content/uploads/ldi_init_use_pfma-4-1045x1080.png 1045w, https://icode4.coffee/wp-content/uploads/ldi_init_use_pfma-4.png 1055w" sizes="auto, (max-width: 768px) 100vw, 768px"><figcaption>Figure 6: Disassembly of dec_malloc being used</figcaption></figure></div>


<p>This means we need our malicious ciphertext to be committed to memory by thread B before it’s accessed in <code>_dec_mem_alloc</code>, AND we need the cache line containing the <code>dec_malloc</code> pointer to either age out or be evicted from L2 cache of thread A. Unfortunately, there’s less than 300 instructions between these two operations and the L2 cache is too large for the data to age out or be evicted in this time frame. It’s also not possible to attack the <code>dec_free</code> pointer because the decompression routine never calls <code>LDIDestroyDecompression</code> so the <code>dec_free</code> pointer will never be checked and called. There are some additional things I did to try and put pressure on the CPU L2 cache which I’ll dig into in part 3, but as it stands attacking the <code>dec_malloc</code>/<code>dec_free</code> pointers is a no-go.</p>



<h2>Attacking LZX: Attempt 2</h2>



<p>Next attempt was trying to attack the <code>dec_output_buffer</code> pointer. When the decompression routine is run the compressed input buffer is decompressed in blocks. Each block will result in a call to <code>LDIDecompress</code> which decompresses the block and copies the decompressed data to the output buffer specified by the <code>dec_output_buffer</code> pointer. The goal for this attack was to successfully overwrite the <code>dec_output_buffer</code> pointer and get the decompressed data copied an address we specify.</p>



<p>This attempt would work mostly the same as the previous attempt except I’d be hammering the <code>dec_output_buffer</code> pointer to point into the last segment of the hypervisor. This particular area doesn’t contain any important code or data so I can freely trash it and observe the ciphertext for it from kernel mode without impacting stability of the hypervisor. Once the ciphertext changes I’ll know I won the race and got the XKE payload to use my malicious <code>dec_output_buffer</code> pointer to overwrite part of the hypervisor. This attack wouldn’t get me hypervisor code execution directly but instead I’d get a write to an arbitrary real mode address from hypervisor context. I’d need to find a way to turn this write into code execution but first I needed to prove the attack worked and that I wouldn’t be blocked by CPU L2 cache.</p>



<p>Since I was focused on determining if the attack was possible and doing all my testing on a console that was already hacked, I decided to modify the XKE payload and make a slight change that would improve my odds of winning the race condition. Each time the XKE payload runs it’ll use a random whitening value for the in-place memory relocation of the LZX scratch buffer. I modified the payload to instead use a constant value of 0x111. This would allow me to skip checking for a whitening value collision and just hammer the ciphertext for the <code>dec_output_buffer</code> pointer continuously in a loop. If the poc worked and the attack was possible then I could worry about winning the race condition later on.</p>



<p>After deploying the poc and modified XKE payload to my hacked console I let it run and got the debug spew that the race condition had been won. The ciphertext for the last segment of the hypervisor had changed which meant this attack was (most likely) feasible. At the very least I had somewhat of an arbitrary write primitive from hypervisor context and now I needed to find a way to turn it into code execution.</p>



<p>Ignoring that I still needed to win the race condition without my static whitening value patch in place, the write primitive I now have is hardly ideal. Every time the write primitive is triggered it’s going to write 0x8000 bytes of data (the size of the block after being decompressed) that I don’t control the contents of. I made a couple attempts to try and get control of the decompressed data before it gets copied to the <code>dec_output_buffer</code> pointer but had no luck in doing so. In most cases it would cause the console to hang or the <code>LDIDecompress</code> function to fail and return early without copying any data. This was a really crappy write primitive but I knew I’d probably never find another one and this could be the only bug I ever end up finding on the console. I accepted the challenge and decided I <em>had</em> to find a way to make it work.</p>



<figure><div>
<blockquote data-width="470" data-dnt="true"><p lang="en" dir="ltr">When you finally get an arbitrary write primitive that works on hypervisor memory but it always writes 0x8000 bytes of data you can't reliably control the contents of, and now you have to find some unhinged way to make this useful because you know you'll never find another one <a href="https://t.co/IDKIZifzv1">pic.twitter.com/IDKIZifzv1</a></p>— Ryan M (@Grimdoomer) <a href="https://twitter.com/Grimdoomer/status/1833974721010151669?ref_src=twsrc%5Etfw">September 11, 2024</a></blockquote>
</div></figure>



<h2>Failure Is Not An Option</h2>



<p>The next step is to turn this 0x8000 byte write primitive into something useful. My first train of thought was to use it to perform some memory corruption on other hypervisor data that I could turn into code execution or a more controlled write primitive. I began looking through the data segment of the hypervisor for anything that could be interesting to corrupt such as pointers to other code/data, RSA public keys for code authentication, or internal data structures used by the hypervisor. I found a number of interesting targets and even found some side channels I could use to figure out exactly what data was written to memory. But as I started to conceptualize various attack vectors I quickly realized this wasn’t going to work.</p>



<p>All of the interesting bits of data I was trying to attack were sitting in a minefield of spinlocks the hypervisor uses for acquiring thread safe access to data. My plan of attack depended on specific values within the decompressed data overwriting specific variables in the hypervisor data segment. If I overwrote a spinlock with a non-zero value the hypervisor would never be able to acquire it again, and trying to acquire it would result in that thread getting stuck indefinitely in the spinlock. Some of the spinlocks didn’t matter since they were for functionality that wasn’t important. However, the spinlock that was sitting after all of the interesting data I had found was the spinlock for running XKE payloads. If I overwrote that spinlock I’d never be able to run another XKE payload and I’d basically be locked out from running the write primitive again.</p>



<p>All of the variables I was targeting depended on winning the race condition on a specific block of data. This meant I would need to run the write primitive multiple times until the right block was written, and if I could only run the write primitive once before being locked out then this wasn’t going to work. I tried to find some other variables I could attack with a single write from any decompressed block but I had no such luck in finding any. I spent a few days digging through the data segment over and over but kept coming to the same conclusion: I can’t overwrite anything in the hypervisor data segment that will work to my advantage.</p>



<h3 id="update-data-side-channel">Thinking Outside the Box</h3>



<p>After scoping out several other pieces of hypervisor data in different locations with no success I decided to think outside of the box. The data being decompressed was bootloader code (i.e. valid PPC instructions), so I could in theory execute it if I overwrite some other hypervisor code such as a system call function. But I didn’t have any way to determine which block of decompressed bootloader code was written when I won the race condition. However, side channels come in all shapes and sizes. I realized that all of the decompressed blocks had the same size <em>except</em> for the last block. If I observed the ciphertext for the hypervisor code I overwrite I could determine when I won the race on the last block by seeing that the ciphertext changed for less than 0x8000 bytes. Simply observe at two locations such that the second location is greater than the size of the last block but less than 0x8000.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b84b6d&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1201" height="274" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/hypervisor_code_block_write-2.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/hypervisor_code_block_write-2.png 1201w, https://icode4.coffee/wp-content/uploads/hypervisor_code_block_write-2-300x68.png 300w, https://icode4.coffee/wp-content/uploads/hypervisor_code_block_write-2-768x175.png 768w" sizes="auto, (max-width: 1201px) 100vw, 1201px"><figcaption>Figure 7: How to identify the last block</figcaption></figure></div>


<p>Now that I had a side channel to determine when the last block was written I needed to find some convenient instructions in this block that I could utilize. The idea is to overwrite some hypervisor code such that the first instructions for some hypervisor function get overwritten with some convenient instruction sequence I find in the bootloader data. To make it more complicated the only segment of hypervisor code I could overwrite was the last segment because it’s the only segment that doesn’t contain code for system critical functions which could cause the console to crash if corrupted. Most of this code was for things like IP-TV decryption and DVD drive anti-piracy routines, things that aren’t going to get executed in the background while the system is running.</p>



<p>On top of that only about 12% (or less than 0x2000 bytes) of the last hypervisor segment is code, the rest is just unused. This means any convenient instruction sequence I found in the decompressed bootloader data had to be in the first ~0x2000 bytes or I wouldn’t be able to position it such that it could overwrite the function in the hypervisor segment. The odds weren’t looking good but I began searching through the bootloader code in IDA looking for any instruction sequence I could use. After searching for a bit I found an instruction sequence that was exactly what I was looking for and at an offset that would work:</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b8529f&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="405" height="100" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/block_14_instruction_sequence.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/block_14_instruction_sequence.png 405w, https://icode4.coffee/wp-content/uploads/block_14_instruction_sequence-300x74.png 300w" sizes="auto, (max-width: 405px) 100vw, 405px"><figcaption>Figure 8: Convenient instruction sequence</figcaption></figure></div>


<p>This instruction sequence will store the bottom byte of r4 to the memory location pointed to by r6+2 and then return. Simply put these two instructions will act as a fully controlled single byte arbitrary write. As an added bonus it has a low offset which makes placing this instruction sequence over some useless system call function in the last hypervisor segment easy. With this final piece of the puzzle I should now be able to write a full exploit chain to get hypervisor code execution.</p>



<h2>Getting Hypervisor Code Execution</h2>



<p>There are many steps in this exploit chain but the flow will go something like this:</p>



<ol>
<li>Obtain the ciphertext for the oracle data and malicious pointer we want to write over the <code>dec_output_buffer</code> pointer. The malicious pointer will point into the last segment of the hypervisor such that the offset of the stb/blr instruction sequence (0xFA8) in the last block of bootloader data will fall at the start of a system call. I chose <code>HvxFlushUserModeTb</code> because it’s the last system call in the segment.</li>



<li>Create two threads:
<ul>
<li>Thread A will be the XKE payload thread that will continuously run the bootloader update payload in a loop.</li>



<li>Thread B will be the attack thread and continuously monitor the XKE input data for the oracle ciphertext. Once the payload relocates the input data to encrypted memory we can observe the ciphertext for it by using the unprotected pathway. Once the ciphertext observed matches our oracle ciphertext we know a whitening value collision was hit and it’s safe to overwrite the LZX decoder structure.</li>
</ul>
</li>



<li>Once a whitening collision is hit thread B will overwrite the <code>dec_output_buffer</code> pointer in the LZX decoder structure with the ciphertext for our malicious pointer.</li>



<li>We’ll know if the race was won because we can observe the ciphertext for the last segment of the hypervisor and see that it changed after the XKE payload was run. By observing the ciphertext in two locations we can determine if less than 0x8000 bytes of data was written which indicates the last block was written.</li>



<li>Once the race has been won on the last block the instructions for the <code>HvxFlushUserModeTb</code> system call will be replaced with the stb/blr instruction pair. We can now execute this system call from kernel mode and use it as an arbitrary write primitive.</li>



<li>Using the <code>HvxFlushUserModeTb</code> write primitive overwrite an entry in the system call dispatch table to point to an instruction sequence that will get us code execution. For this I chose an instruction sequence that will jump to the address in r4, and overwrote the system call entry for <code>HvxPostOutput</code>.</li>



<li>Set r4 to point to some shell code we create and call <code>HvxPostOutput</code> from kernel mode which will cause the hypervisor to jump to r4 and execute our code. For testing I wrote some assembly code to set the console’s ring of light to full orange and return the value 0x41414141.</li>
</ol>



<p>After coding this up and deploying it to my hacked console I let the exploit run. After a minute or two I got the debug spew that the race had been won on the last block and the exploit ran. I could see the console’s ring of light was lit full orange and the shell code returned 0x41414141 back to kernel mode.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b85bb7&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="412" height="199" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/race_hit_payload_ran.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/race_hit_payload_ran.png 412w, https://icode4.coffee/wp-content/uploads/race_hit_payload_ran-300x145.png 300w" sizes="auto, (max-width: 412px) 100vw, 412px"><figcaption>Figure 9: Debug output from the exploit</figcaption></figure></div>


<p>At this point I should be good to go, just port this over to work on a non-hacked console and we’re done, hypervisor hacked, right? There’s one thing I forgot when running the last few poc’s and that was the patch I made to the XKE payload to use a static whitening value. I kept this patch in to test the poc with the best possible conditions for winning the race, which is why the exploit triggered in 1-2 minutes. Once I remove this patch the payload will use a random whitening value every time it runs. This means not only do we have to wait for a whitening value collision but we also have to win the race condition which we’re going to fail at many times. </p>



<p>After I removed the patch and reran the exploit I wasn’t even getting debug spew that the race condition was being won for any block, even after 30 minutes there was still nothing. It was starting to seem like this attack wasn’t going to work but I wasn’t ready to give up yet.</p>



<h2>Introducing Thread Feng Shui</h2>



<p>To try and get a better idea of what was going on I wrote some code/patches to record timestamps for each thread as the XKE payload was running. This gave me timestamps in microseconds for when the XKE payload relocated the LZX scratch buffer to encrypted memory, when it started to decompress each block of input data, and when the attack thread would start/stop hammering the LZX decoder structure. From the timing data I was able to see that by the time the attack thread was able to observe the whitening value collision the payload thread had already decompressed most of the input data. Which meant that by the time the attack thread got the malicious ciphertext flushed to RAM the XKE payload was already done running.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;67c61f5b8630f&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="400" height="477" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://icode4.coffee/wp-content/uploads/cpu_threads.png" alt="" srcset="https://icode4.coffee/wp-content/uploads/cpu_threads.png 400w, https://icode4.coffee/wp-content/uploads/cpu_threads-252x300.png 252w" sizes="auto, (max-width: 400px) 100vw, 400px"><figcaption>Figure 10: Die shot of the Xbox 360 CPU</figcaption></figure></div>


<p>When you create a software thread on the Xbox 360 you can specify which physical hardware thread you want it to run on. I speculated that what hw thread I ran the software threads on actually mattered, and that perhaps the distance to the FSB/L2 cache or inner logic of the L2 crossbar made a difference in how fast the operations completed. To test this I tried a number of different configurations of placing the software threads on different hardware threads and eventually found a configuration that allowed me to win the race condition. I suspected that putting the payload thread as far away from the FSB/L2 cache as possible and the attack thread as close as possible would have had the best results, however, this wasn’t the case.</p>



<p>The configuration with the best results was running the attack thread on hw thread 0 and the payload thread on hw thread 1. It seemed like scheduling them on different CPU cores actually hurt the odds of winning the race condition, and they had to be on the same core. Core 0 also had the best results compared to putting both threads on core 1 or 2. I know basically nothing about how CPUs work at a hardware level, or the inner workings of L1/2 cache and memory transactions. But what I suspect is happening is that each CPU core has its own load/store queues for performing memory transactions. Putting both threads on the same CPU core causes the memory transactions to be queued onto the L2 crossbar together when they would otherwise be queued separately if run on different cores. Put simply, running on the same core batches the loads/stores from both threads when they would otherwise be performed individually, and this significantly reduces the transaction times.</p>



<figure><div>
<blockquote data-width="470" data-dnt="true"><p lang="en" dir="ltr">I never would have thought the physical distance between CPU cores and L2 cache/FSB would make a difference when trying to win a race condition but for the one I'm exploiting it's the difference between non exploitable to exploited. I guess every nanometer counts</p>— Ryan M (@Grimdoomer) <a href="https://twitter.com/Grimdoomer/status/1835754498318606378?ref_src=twsrc%5Etfw">September 16, 2024</a></blockquote>
</div></figure>



<h2>Abusing the CPU for Fun and <s>Profit</s> Cache</h2>



<p>Now that I was able to win the race condition with an unmodified version of the XKE payload I at least knew the attack was theoretically possible. However, the rate at which the race condition was being won was abysmal, and it appeared that the race was only being won during the first half of the decompression process. This meant there was a slim chance of winning the race when the last block was being decompressed. Starting with the race condition success rate I knew this was largely due to how big the L2 cache was. Because the payload thread was more likely to fetch data from cache instead of RAM it meant more missed opportunities for it to fetch my malicious ciphertext from RAM. So if I could find a way to thrash L2 cache the race condition success rate should improve.</p>



<p>The Xbox 360 CPU has a 1MB, 8 way associative L2 cache, meaning each of the 8 pathways can map to 128KB of cache space. One of the interesting features of the Xbox 360 is that you can lock access to sections of L2 cache (as well as the pathways) and map it to a virtual address that the GPU can access. Yes, the GPU can read directly from the CPU’s L2 cache. The idea is that instead of having the CPU write data to RAM and then the GPU read it, you can put the data in L2 cache and have the GPU read it directly. This reduces the number of memory transactions from 2 to 1. There are some other performance benefits to this feature, but what it means for us is we have a way to reduce the amount of L2 cache the CPU can use. After figuring out how to use the <code>XLockL2</code> kernel API I was able to write the following helper function:</p>



<div id="urvanov-syntax-highlighter-67c61f5b716a5894788306" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p></div>
				</td>
						<td><div><p><span>void</span><span> </span><span>LockAndThrashL2</span><span>(</span><span>int</span><span> </span><span>index</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>// Allocate a block of 256kb cachable physical memory that will be used to lock the L2 range.</span></p><p><span>	</span><span>BYTE</span><span>*</span><span> </span><span>pPhysMemoryPtr</span><span> </span><span>=</span><span> </span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>XPhysicalAlloc</span><span>(</span><span>256</span><span> </span><span>*</span><span> </span><span>1024</span><span>,</span><span> </span><span>MAXULONG_PTR</span><span>,</span><span> </span><span>256</span><span> </span><span>*</span><span> </span><span>1024</span><span>,</span><span> </span><span>PAGE_READWRITE</span><span> </span><span>|</span><span> </span><span>MEM_LARGE_PAGES</span><span>)</span><span>;</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>pPhysMemoryPtr</span><span> </span><span>==</span><span> </span><span>NULL</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>DbgPrint</span><span>(</span><span>"Failed to allocate memory for L2 cache lock\n"</span><span>)</span><span>;</span></p><p><span>		</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>		</span><span>VdDisplayFatalError</span><span>(</span><span>0x12400</span><span> </span><span>|</span><span> </span><span>ERR_LOCKL2_OOM</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>// Reserve L2 cache and lock 2 of the available pathways.</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>XLockL2</span><span>(</span><span>index</span><span>,</span><span> </span><span>pPhysMemoryPtr</span><span>,</span><span> </span><span>256</span><span> </span><span>*</span><span> </span><span>1024</span><span>,</span><span> </span><span>XLOCKL2_LOCK_SIZE_2_WAYS</span><span>,</span><span> </span><span>0</span><span>)</span><span> </span><span>==</span><span> </span><span>FALSE</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>DbgPrint</span><span>(</span><span>"Failed to reserve L2 cache range\n"</span><span>)</span><span>;</span></p><p><span>		</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>		</span><span>VdDisplayFatalError</span><span>(</span><span>0x12400</span><span> </span><span>|</span><span> </span><span>ERR_LOCKL2_RESERVE</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>// Fill the cache with trash data.</span></p><p><span>	</span><span>memset</span><span>(</span><span>pPhysMemoryPtr</span><span>,</span><span> </span><span>0x41</span><span>,</span><span> </span><span>256</span><span> </span><span>*</span><span> </span><span>1024</span><span>)</span><span>;</span></p><p><span>	</span><span>// Commit the L2 cache range and prevent it from being replaced.</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>XLockL2</span><span>(</span><span>index</span><span>,</span><span> </span><span>pPhysMemoryPtr</span><span>,</span><span> </span><span>256</span><span> </span><span>*</span><span> </span><span>1024</span><span>,</span><span> </span><span>XLOCKL2_LOCK_SIZE_2_WAYS</span><span>,</span><span> </span><span>XLOCKL2_FLAG_SUSPEND</span><span>_</span>REPLACEMENT<span> </span><span>)</span><span> </span><span>==</span><span> </span><span>FALSE</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>DbgPrint</span><span>(</span><span>"Failed to commmit L2 cache range\n"</span><span>)</span><span>;</span></p><p><span>		</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>		</span><span>VdDisplayFatalError</span><span>(</span><span>0x12400</span><span> </span><span>|</span><span> </span><span>ERR_LOCKL2_COMMIT</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>The way this works is we first allocate a block of 256KB of physical memory that will be used to back the L2 lock. Then we reserve a portion of L2 cache that’s the same size as the physical memory allocation and fill it with data. When the CPU sees any reads and writes to this memory region it’ll redirect them to the reserved region of L2 cache. Finally, we commit the lock which prevents that region of L2 cache from being evicted. This function can be used to lock up to 512KB of L2 cache and 4 of the available pathways for an overall reduction of 50% of L2 cache. This will put a lot of pressure on the remaining L2 cache which will cause an increase in cache misses, thus increasing the likelihood of the payload thread fetching the malicious ciphertext from RAM.</p>



<p>After modifying the poc to use the <code>LockAndThrashL2</code> function I was happy to see the success rate of the race condition had gone from hitting once every 10-15 minutes down to once every 2-3 minutes. This was a huge increase and the L2 thrashing was working great. </p>



<h2>Offsetting the Race Timing</h2>



<p>The next issue to deal with was the race timing. The race was typically being won during the first half of the decompression process and since we’re targeting the last block of compressed data, I needed to find a way to shift the window of opportunity to the last half of the decompression process. Initially I tried adding a delay between when the attack thread observes the oracle ciphertext and when it starts hammering the LZX decoder structure. However, no matter how much I tweaked the length of the delay I wasn’t able to get anything consistent enough to slide the attack window to the second half of the decompression process. So I decided to take a different approach.</p>



<p>Currently the oracle data (which was 16 bytes of 00s) was based on when the XKE payload did the in-place relocation of the LZX scratch buffer to encrypted memory and zero-initialized it. After the XKE payload initializes this memory it would verify the RSA signature on the compressed input data and decrypt it, before finally calling <code>LDICreateDecompression</code> which would fill the first 16 bytes of the LZX decoder structure with data. This gave a pretty large window of opportunity to observe the ciphertext for the oracle data but led to the attack window happening too early. I decided to change the oracle data to be the first 16 bytes of the LZX decoder structure after it was initialized by <code>LDICreateDecompression</code>, in hopes it would shift the attack window to the latter half of the decompression process.</p>



<p>The first 16 bytes of the LZX decoder structure is just a header with some constant values based on the parameters provided to <code>LDICreateDecompression</code>, so I could easily change the poc to generate the ciphertext for this data. You can see this change in lines 4-6 below:</p>



<div id="urvanov-syntax-highlighter-67c61f5b716ca487197768" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>// Get the cipher text for the expected header data in the LZX decoder context. This acts as our</span></p><p><span>// "oracle" to know when to start the race attack.</span></p><p><span>memset</span><span>(</span><span>pMemoryAddress</span><span> </span><span>+</span><span> </span><span>ScratchOffset</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>16</span><span>)</span><span>;</span></p><p><span>*</span><span>(</span><span>ULONG</span><span>*</span><span>)</span><span>(</span><span>pMemoryAddress</span><span> </span><span>+</span><span> </span><span>ScratchOffset</span><span> </span><span>+</span><span> </span><span>0</span><span>)</span><span> </span><span>=</span><span> </span><span>0x4349444c</span><span>;</span><span>		</span><span>// signature = 'CIDL'</span></p><p><span>*</span><span>(</span><span>ULONG</span><span>*</span><span>)</span><span>(</span><span>pMemoryAddress</span><span> </span><span>+</span><span> </span><span>ScratchOffset</span><span> </span><span>+</span><span> </span><span>4</span><span>)</span><span> </span><span>=</span><span> </span><span>0x8000</span><span>;</span><span>			</span><span>// windows size = 0x8000</span></p><p><span>*</span><span>(</span><span>ULONG</span><span>*</span><span>)</span><span>(</span><span>pMemoryAddress</span><span> </span><span>+</span><span> </span><span>ScratchOffset</span><span> </span><span>+</span><span> </span><span>8</span><span>)</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span>				</span><span>// cpu type = 1</span></p><p><span>__dcbst</span><span>(</span><span>ScratchOffset</span><span>,</span><span> </span><span>pMemoryAddress</span><span>)</span><span>;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>With the change in place I re-ran the poc a number of times and saw that the attack window had shifted to the latter half of the decompression process, and in some cases was winning the race on the last block of data. The poc was nearing completion, however, it was about this time that a new issue had popped up.</p>



<h2>Dirty Cache</h2>



<p>When testing the previous change there were a number of iterations where the race was supposedly won on the last block but the console would crash when trying to patch the hypervisor system call table. This could only happen if the block that was written to memory wasn’t the last block in the input file and the data being written over the <code>HvxFlushUserModeTb</code> system call wasn’t our stb/blr instruction sequence.</p>



<div id="urvanov-syntax-highlighter-67c61f5b716d2631555766" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p></div>
				</td>
						<td><div><p><span>DWORD </span><span>RunUpdatePayloadThreadProc</span><span>(</span><span>THREAD_ARGS</span><span>*</span><span> </span><span>pArgs</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>.</span><span>.</span><span>.</span></p><p><span>	</span><span>// Get the cipher text at the location we want to overwrite and at an offset that is &gt; the size of block 14. This allows</span></p><p><span>	</span><span>// us to determine when we win the race on block 14 (smallest block in the file) vs any other block.</span></p><p><span>	</span><span>DWORD</span><span>*</span><span> </span><span>pCipherTextPtr1</span><span> </span><span>=</span><span> </span><span>(</span><span>DWORD</span><span>*</span><span>)</span><span>(</span><span>0xA0030000</span><span> </span><span>+</span><span> </span><span>HV_SEG3_OVERWRITE_OFFSET</span><span>)</span><span>;</span></p><p><span>	</span><span>DWORD</span><span>*</span><span> </span><span>pCipherTextPtr2</span><span> </span><span>=</span><span> </span><span>(</span><span>DWORD</span><span>*</span><span>)</span><span>(</span><span>0xA0030000</span><span> </span><span>+</span><span> </span><span>(</span><span>HV_SEG3_OVERWRITE_OFFSET</span><span> </span><span>-</span><span> </span><span>BLOCK_14_TARGET_OFFSET</span><span>)</span><span> </span><span>+</span><span> </span><span>BLOCK_14_SIZE</span><span> </span><span>+</span><span> </span><span>0x80</span><span>)</span><span>;</span></p><p><span>	</span><span>DWORD </span><span>cipherValue1</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr1</span><span>;</span></p><p><span>	</span><span>DWORD </span><span>cipherValue2</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr2</span><span>;</span></p><p><span>	</span><span>// Loop and hammer the payload until we hopefully get code exec.</span></p><p><span>	</span><span>while</span><span> </span><span>(</span><span>true</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>.</span><span>.</span><span>.</span></p><p><span>		</span><span>// Execute the payload.</span></p><p><span>		</span><span>DWORD </span><span>result</span><span> </span><span>=</span><span> </span><span>HvxKeysExecute</span><span>(</span><span>pArgs</span><span>-&gt;</span><span>PayloadPhys</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>PayloadSize</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>UpdateDataPhys</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>UpdateDataSize</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>)</span><span>;</span></p><p><span>		</span><span>// Flush cache on the cipher text pointers.</span></p><p><span>		</span><span>__dcbf</span><span>(</span><span>0</span><span>,</span><span> </span><span>pCipherTextPtr1</span><span>)</span><span>;</span></p><p><span>		</span><span>__dcbf</span><span>(</span><span>0</span><span>,</span><span> </span><span>pCipherTextPtr2</span><span>)</span><span>;</span></p><p><span>		</span><span>// Check if we got a block overwrite and if it appears to be block 14.</span></p><p><span>		</span><span>DWORD </span><span>test1</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr1</span><span>;</span></p><p><span>		</span><span>if</span><span> </span><span>(</span><span>cipherValue1</span><span> </span><span>!=</span><span> </span><span>test1</span><span>)</span></p><p><span>		</span><span>{</span></p><p><span>			</span><span>// We got a block overwrite, check if it was block 14.</span></p><p><span>			</span><span>DWORD </span><span>test2</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr2</span><span>;</span></p><p><span>			</span><span>if</span><span> </span><span>(</span><span>cipherValue2</span><span> </span><span>==</span><span> </span><span>test2</span><span>)</span></p><p><span>			</span><span>{</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>"Block 14 overwrite hit!\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>// Overwrite the syscall function pointer for HvxPostOutput to point to a gadget that will jump to an arbitrary address.</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Patching hv syscall table\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>HvWriteULONG</span><span>(</span><span>HV_SYSCALL_POST_OUTPUT_ADDRESS</span><span>,</span><span> </span><span>HV_CALL_R4_GADGET_ADDRESS</span><span>)</span><span>;</span></p><p><span>				</span><span>// Try to execute our shell code which will restore the data we trashed in the last hypervisor segment and patch out</span></p><p><span>				</span><span>// the RSA signature checks on executable files.</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Running payload\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>result</span><span> </span><span>=</span><span> </span><span>HvxPostOutputExploit</span><span>(</span><span>0</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>ShellCodePhysAddress</span><span>)</span><span>;</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Payload returned 0x%08x\n"</span><span>,</span><span> </span><span>result</span><span>)</span><span>;</span></p><p><span>				</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>			</span><span>}</span></p><p><span>			</span><span>else</span></p><p><span>			</span><span>{</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>"Race hit: 0x%08x\n"</span><span>,</span><span> </span><span>test1</span><span>)</span><span>;</span></p><p><span>			</span><span>}</span></p><p><span>			</span><span>// Save the latest block hit values.</span></p><p><span>			</span><span>cipherValue1</span><span> </span><span>=</span><span> </span><span>test1</span><span>;</span></p><p><span>			</span><span>cipherValue2</span><span> </span><span>=</span><span> </span><span>test2</span><span>;</span></p><p><span>		</span><span>}</span></p><p><span>	</span><span>}</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>After doing some introspection on my hacked console I was able to confirm that the block being written wasn’t the last block in the input file and that the ciphertext read from the <code>pCipherTextPtr2</code> pointer was stale. This basically meant that the latter portion of data being written to hypervisor memory was getting stuck in L2 cache and not being committed to RAM where it could be observed from kernel mode. This wasn’t a huge issue, it simply meant I had to find a cache flush primitive I could use to get this data committed to RAM.</p>



<h3>Finding a Cache Flush Primitive</h3>



<p>Finding a cache flush primitive isn’t too difficult, you mainly need to find a function that will process some data and cause what’s currently in L2 cache to age out. Any hypervisor function that meets the following criteria will work:</p>



<ul>
<li>Runs in hypervisor context. Kernel mode cannot flush cache lines that were allocated by hypervisor mode.</li>



<li>Runs with caching enabled. The hypervisor normally runs with caching disabled so we’ll need to find a function that enables caching before processing the data. Any function that operates on data using the encrypted pathway will do this.</li>



<li>Touches as much data as possible while also performing as little validation as possible.</li>
</ul>



<p>After looking through all the hypervisor functions that operate on encrypted memory I came across a system call called <code>HvxRevokeUpdate</code> that met the above criteria. This function is used to encrypt a console certificate revocation list using a per-console key. These revocation lists are used to blacklist console certificates that have been considered “compromised”. For example, if someone hacked their console and used their console certificate to sign malicious game saves (for hacking the console or cheating online), Microsoft could blacklist the console certificate used to sign them by adding it to the revocation list and distributing it over Xbox Live or in system updates. After it’s been revoked any game save signed by that console certificate will show up as “corrupted” and can’t be loaded.</p>



<p>For us this function is a great cache flush primitive because it operates on an input buffer up to 0x20000 bytes large in encrypted memory and performs no validation on the input data. We simply provide it an empty input buffer and once the hypervisor enables caching and touches this data from the encrypted pathway it’ll cause old data in L2 cache to be evicted. After including the cache flush primitive into the exploit poc I ended up with the following:</p>



<div id="urvanov-syntax-highlighter-67c61f5b716d7759369431" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p><p>57</p><p>58</p><p>59</p><p>60</p><p>61</p><p>62</p><p>63</p><p>64</p></div>
				</td>
						<td><div><p><span>DWORD </span><span>RunUpdatePayloadThreadProc</span><span>(</span><span>THREAD_ARGS</span><span>*</span><span> </span><span>pArgs</span><span>)</span></p><p><span>{</span></p><p><span>	</span><span>// Allocate a scratch buffer to help with flushing L2 cache in hypervisor context.</span></p><p><span>	</span><span>BYTE</span><span>*</span><span> </span><span>pCacheFlushBuffer</span><span> </span><span>=</span><span> </span><span>(</span><span>BYTE</span><span>*</span><span>)</span><span>XPhysicalAlloc</span><span>(</span><span>0x20000</span><span>,</span><span> </span><span>MAXULONG_PTR</span><span>,</span><span> </span><span>0x10000</span><span>,</span><span> </span><span>PAGE_READWRITE</span><span> </span><span>|</span><span> </span><span>MEM_LARGE_PAGES</span><span>)</span><span>;</span></p><p><span>	</span><span>if</span><span> </span><span>(</span><span>pCacheFlushBuffer</span><span> </span><span>==</span><span> </span><span>NULL</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>DbgPrint</span><span>(</span><span>"Failed to allocate memory for cache flush buffer\n"</span><span>)</span><span>;</span></p><p><span>		</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>		</span><span>VdDisplayFatalError</span><span>(</span><span>0x12400</span><span> </span><span>|</span><span> </span><span>ERR_CACHE_FLUSH_BUFFER_OOM</span><span>)</span><span>;</span></p><p><span>	</span><span>}</span></p><p><span>	</span><span>ULONG </span><span>CacheFlushBufferPhys</span><span> </span><span>=</span><span> </span><span>MmGetPhysicalAddress</span><span>(</span><span>pCacheFlushBuffer</span><span>)</span><span>;</span></p><p><span>	</span><span>.</span><span>.</span><span>.</span></p><p><span>	</span><span>// Loop and hammer the payload until we hopefully get code exec.</span></p><p><span>	</span><span>while</span><span> </span><span>(</span><span>true</span><span>)</span></p><p><span>	</span><span>{</span></p><p><span>		</span><span>.</span><span>.</span><span>.</span></p><p><span>		</span><span>// Execute the payload.</span></p><p><span>		</span><span>DWORD </span><span>result</span><span> </span><span>=</span><span> </span><span>HvxKeysExecute</span><span>(</span><span>pArgs</span><span>-&gt;</span><span>PayloadPhys</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>PayloadSize</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>UpdateDataPhys</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>UpdateDataSize</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>)</span><span>;</span></p><p><span>		</span><span>// Flush cache on the cipher text pointers.</span></p><p><span>		</span><span>__dcbf</span><span>(</span><span>0</span><span>,</span><span> </span><span>pCipherTextPtr1</span><span>)</span><span>;</span></p><p><span>		</span><span>// Check if we got a block overwrite and if it appears to be block 14.</span></p><p><span>		</span><span>DWORD </span><span>test1</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr1</span><span>;</span></p><p><span>		</span><span>if</span><span> </span><span>(</span><span>cipherValue1</span><span> </span><span>!=</span><span> </span><span>test1</span><span>)</span></p><p><span>		</span><span>{</span></p><p><span>			</span><span>// Flush cache on the secondary cipher text pointer. This one MUST be thorough or else we risk fetching stale data</span></p><p><span>			</span><span>// and trying to execute the post-block-write part of the exploit which will cause the console to hang.</span></p><p><span>			</span><span>HvxRevokeUpdate</span><span>(</span><span>CacheFlushBufferPhys</span><span>,</span><span> </span><span>0x20000</span><span>,</span><span> </span><span>0</span><span>)</span><span>;</span></p><p><span>			</span><span>__dcbf</span><span>(</span><span>0</span><span>,</span><span> </span><span>pCipherTextPtr2</span><span>)</span><span>;</span></p><p><span>			</span><span>// We got a block overwrite, check if it was block 14.</span></p><p><span>			</span><span>DWORD </span><span>test2</span><span> </span><span>=</span><span> </span><span>*</span><span>pCipherTextPtr2</span><span>;</span></p><p><span>			</span><span>if</span><span> </span><span>(</span><span>cipherValue2</span><span> </span><span>==</span><span> </span><span>test2</span><span>)</span></p><p><span>			</span><span>{</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>"Block 14 overwrite hit!\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>// Overwrite the syscall function pointer for HvxPostOutput to point to a gadget that will jump to an arbitrary address.</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Patching hv syscall table\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>HvWriteULONG</span><span>(</span><span>HV_SYSCALL_POST_OUTPUT_ADDRESS</span><span>,</span><span> </span><span>HV_CALL_R4_GADGET_ADDRESS</span><span>)</span><span>;</span></p><p><span>				</span><span>// Try to execute our shell code which will restore the data we trashed in the last hypervisor segment and patch out</span></p><p><span>				</span><span>// the RSA signature checks on executable files.</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Running payload\n"</span><span>)</span><span>;</span></p><p><span>				</span><span>result</span><span> </span><span>=</span><span> </span><span>HvxPostOutputExploit</span><span>(</span><span>0</span><span>,</span><span> </span><span>pArgs</span><span>-&gt;</span><span>ShellCodePhysAddress</span><span>)</span><span>;</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>" * Payload returned 0x%08x\n"</span><span>,</span><span> </span><span>result</span><span>)</span><span>;</span></p><p><span>				</span><span>DbgBreakPoint</span><span>(</span><span>)</span><span>;</span></p><p><span>			</span><span>}</span></p><p><span>			</span><span>else</span></p><p><span>			</span><span>{</span></p><p><span>				</span><span>DbgPrint</span><span>(</span><span>"Race hit: 0x%08x\n"</span><span>,</span><span> </span><span>test1</span><span>)</span><span>;</span></p><p><span>			</span><span>}</span></p><p><span>			</span><span>// Save the latest block hit values.</span></p><p><span>			</span><span>cipherValue1</span><span> </span><span>=</span><span> </span><span>test1</span><span>;</span></p><p><span>			</span><span>cipherValue2</span><span> </span><span>=</span><span> </span><span>test2</span><span>;</span></p><p><span>		</span><span>}</span></p><p><span>	</span><span>}</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>Running the updated poc showed the cache flush primitive was working and the console was no longer crashing on false positives due to stale data in RAM. The only thing left to do now was assemble all the pieces of this exploit into a single chain that I would trigger using my favorite game exploit, Tony Hawk’s Pro Strcpy.</p>



<h2>Introducing the Xbox 360 Bad Update Exploit</h2>



<p>To turn this into an end-to-end exploit and get hypervisor code execution I’d use my <a href="https://icode4.coffee/?p=954" data-type="post" data-id="954">Tony Hawk’s Pro Strcpy</a> bug as an entry point and combine it with the techniques I developed for attacking this bootloader update XKE payload. Putting all of the pieces of the exploit together the full exploit chain would look like so:</p>



<ol>
<li>Stage 1
<ul>
<li>Using a modified game save file I would trigger the gap name strcpy bug in Tony Hawk’s American Wasteland (you can read about this bug in the <a href="https://icode4.coffee/?p=954" data-type="post" data-id="954">Tony Hawk’s Pro Strcpy</a> post) and get a ROP chain executing in kernel mode. This ROP chain would load a much larger ROP chain (stage 2) from a file on the HDD and pivot execution to it.</li>
</ul>
</li>



<li>Stage 2
<ul>
<li>The larger ROP chain would perform the operations needed to get arbitrary kernel mode code execution, and load stage 3 from a file on the HDD into the region of executable kernel memory.</li>
</ul>
</li>



<li>Stage 3
<ul>
<li>Stage 3 is the C code I wrote to run and attack the bootloader update XKE payload. This will perform the attack on the LZX decompression process to try and overwrite some hypervisor code and create an arbitrary write primitive. Using the arbitrary write primitive we’ll further modify hypervisor memory to get hypervisor code execution and run the stage 4 code in hypervisor mode.</li>
</ul>
</li>



<li>Stage 4
<ul>
<li>Stage 4 is assembly code I wrote that will undo all the memory corruption done to the hypervisor in stage 3 so it’s in a clean and stable state. After that it’ll patch out the RSA signature checks the hypervisor does on executable code so that unsigned executables can be run on the console.</li>



<li>Once complete stage 4 will return back to kernel mode and run an unsigned executable from the HDD thus proving the exploit worked.</li>
</ul>
</li>
</ol>



<p>While all the major pieces were already proven to work I still needed to chain them all together into a top-down exploit.</p>



<h2>A ROP Chain for the Record Books</h2>



<p>The technique I developed for getting arbitrary kernel mode code execution was done in C code that I could deploy and run on my hacked console. This code would need to be recreated in a ROP chain, and while the code itself wasn’t very complex doing it all in ROP wasn’t easy. It took about a week to rewrite the C code in ROP and the final chain consisted of 20-30 unique ROP gadgets and just over 28,000 links in the chain. I wrote many parts of the chain in reusable macros and split it across several source files to simplify the process, but even then the complexity of the chain is quite high.</p>



<figure><div>
<blockquote data-width="470" data-dnt="true"><p lang="en" dir="ltr">Almost out of ROP chain hell… Currently have 20 unique gadgets and 27,716 links in the chain. I implemented common functionality in macros which makes writing more complex aspects of the chain easier. Another day or so and I can move on to the third stage of the exploit (in C) <a href="https://t.co/uNpLKtlMGP">pic.twitter.com/uNpLKtlMGP</a></p>— Ryan M (@Grimdoomer) <a href="https://twitter.com/Grimdoomer/status/1838187355037167943?ref_src=twsrc%5Etfw">September 23, 2024</a></blockquote>
</div></figure>



<h2>The Grand Finale</h2>



<p>With everything in place there was one last thing to do. If you read my <a href="https://icode4.coffee/?p=954" data-type="post" data-id="954">Tony Hawk’s Pro Strcpy</a> blog post you might remember I used a homebrew <a href="https://www.youtube.com/watch?v=2yJgwwDcgV8">nyan cat</a> executable to test my exploits. It has since become somewhat of a signature for my exploits and while I had a version of this for the Xbox 360 ready to go, I decided to give it a small upgrade. After copying all the exploit files to a USB stick I booted up Tony Hawk’s American Wasteland, loaded my hacked game save, and waited. I knew from test runs on my hacked console that the exploit had a low success rate (around 35%) and could take upwards of 30 minutes to trigger, but I was ecstatic when it triggered on the first attempt in only a few minutes time. I now had an end-to-end hypervisor exploit that worked on the latest system software (version 17559) and could be triggered using software only methods.</p>



<figure><p>
<iframe loading="lazy" title="Xbox 360 Bad Update Exploit Demo" width="470" height="264" src="https://www.youtube.com/embed/WgmrW0Pfgl4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<figure><div>
<blockquote data-width="470" data-dnt="true"><p lang="en" dir="ltr">As of today I have a fully working software only hypervisor exploit for the latest xbox 360 retail dashboard 17559 (should work on almost any software version though). Here's what you need to know…</p>— Ryan M (@Grimdoomer) <a href="https://twitter.com/Grimdoomer/status/1847497275289063676?ref_src=twsrc%5Etfw">October 19, 2024</a></blockquote>
</div></figure>



<p>Full source code and files for the exploit can be found on GitHub: <a href="https://github.com/grimdoomer/Xbox360BadUpdate" target="_blank" rel="noreferrer noopener">Xbox360BadUpdate</a>.</p>



<h2>Exploit F.A.Q.</h2>



<p>There are a couple questions I know people will be asking about the current state of the exploit, if a softmod is available, and how it compares to things like the RGH hack. I’ve provided answers to a few of these here.</p>



<ul>
<li><strong>What Xbox 360 software/kernel version does this exploit work on?</strong>
<ul>
<li>The exploit works on all software versions up to/through 17559, though there’s some version specific information that would need to be updated for any version other than 17559. At the time of writing this post I have reported the vulnerability to Microsoft but I don’t know if it will be patched in a future software update or not.</li>
</ul>
</li>



<li><strong>Does this mean a “softmod” is now available?</strong>
<ul>
<li>While the exploit is software only (no additional hardware or opening of the console required) it’s not persistent. You’ll need to rerun the exploit every time you turn the console on. Once the exploit runs your console will be in a hacked state until it’s rebooted/powered off. It works much the same as a tethered iOS jailbreak.</li>
</ul>
</li>



<li><strong>Can this be turned into a softmod/made persistent?</strong>
<ul>
<li>It’s theoretically possible to turn this into a persistent “softmod” but it wouldn’t be an ideal experience. The Xbox 360 boot chain is well protected and the only viable way to trigger the exploit on boot would be by finding a bug in the Xbox 360 dashboard, which may require additional input to trigger (ex: wait for dashboard to load, navigate to games list, scroll down, exploit triggers). <p>On top of this the Bad Update exploit can take up to 20 minutes to trigger and only has a ~35% success rate. Imagine every time you turn your console on, the dashboard loads, you have to navigate through a menu or two, then wait 20 minutes for the exploit to hopefully trigger, and if it doesn’t you have to reboot the console and start over until it works. Not a very good experience is it?</p></li>
</ul>
</li>



<li><strong>Why does the exploit take so long to trigger/can it be made faster/more reliable?</strong>
<ul>
<li>The exploit takes a while to trigger because it has to exploit a race condition under very specific circumstances. It may be possible to improve the timing/reliability but I don’t think it’ll ever be good enough to try and turn into a persistent hack.</li>
</ul>
</li>



<li><strong>Should I use this instead of the RGH hack?</strong>
<ul>
<li>No, this exploit does not provide a good user experience for those wanting to hack their Xbox 360 consoles. RGH is, and always will be the defacto boot-to-hacked-state exploit for the console.</li>
</ul>
</li>
</ul>



<h2>Conclusion</h2>



<p>This exploit took around two and half months to research, develop, and finish but it felt like an eternity. There were many times I thought I hit a dead end and had no path forward, only to spend a day or two thinking and coming up with something new to try. Finding and developing this exploit pushed my abilities as a security researcher, from understanding how the console worked, to finding ways to create attack surface when there otherwise wasn’t any, to using anything and everything I possibly could to help win the race condition and (quite literally) beat the CPU into submission. The constant need to think outside the box and turn nothing into something is part of what I love about being a security researcher, but it’s also exhausting.</p>



<p>Now that I’ve completed this exploit I’m going to retire from game console hacking (at least for the foreseeable future). I’ve learned all I wanted to learn and hacked all I wanted hacked, now it’s time I explore some new avenues. I want to give a big shout out to everyone who encouraged me to continue working on this through all the ups and downs, doom, “x”, and the Seabards for listening to all my rambling about L2 cache fuckery and the numerous times I thought I hit a dead end. Once again, a big thank you to everyone who read through this wall of text. Even though it’s quite long this blog post doesn’t even begin to capture all the work that made this exploit possible.</p>



<p>For anyone who wants to read more about the Xbox 360 inner workings I plan to write a part 3 where I’ll (briefly) cover some of my failed attempts at exploiting the console, and other interesting behavior/techniques I developed along the way. However, this part won’t be written for several months time so don’t count on it any time soon.</p>




									
																		
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chrome Returns 206 when the Server Returns 403 (118 pts)]]></title>
            <link>https://aoli.al/blogs/chrome-bug/</link>
            <guid>43244680</guid>
            <pubDate>Mon, 03 Mar 2025 18:00:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aoli.al/blogs/chrome-bug/">https://aoli.al/blogs/chrome-bug/</a>, See on <a href="https://news.ycombinator.com/item?id=43244680">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  <section>
    <h2 id="postmortem">Postmortem</h2>
<p>It is a feature not a bug! I got detailed explanation from chrome developers which is very helpful. You may find the complete response in the <a href="https://issues.chromium.org/issues/390229583">bug ticket</a>. Also this blog post draws a lot of attention after I post it on Hackers News. You may find comments <a href="https://news.ycombinator.com/item?id=43244680">here</a>.</p>
<p><strong>Updates:</strong> I did know the people who replied my issue are not the developers as pointed out by a HN comment.</p>
<blockquote>
<p>I was also pretty surprised when the OP said “the Chromium team refused to use my server to reproduce the bug”, when the actual comments of the ticket were “clone this repo and run my giant node app” and the tester’s response was “It seems a bit difficult to set up an build environment to run the static server, could you provide a more minimal repro case?”. OP’s description of the tester’s reasonable concerns seems very unfair.</p></blockquote>
<p>And I would like to withdraw my statement about Chromium team refused to use my server to reproduce the bug. However, I do believing providing a place to allow users to showcase their weird servers is not a bad idea. Debugging and failure reproduction is always hard! I still think offering GCP credits to issue reporters is a practical approach😁.</p>
<p>You may continue to read the original post if you want to learn an interesting debugging experience.</p>
<h2 id="original-post">Original Post</h2>
<p>Sorry for the clickbait, but it is true.</p>
<p>I was helping my partner debug an interesting bug in his fancy <a href="https://github.com/XiangpengHao/parquet-viewer/issues/7">parquet viewer</a>. The website crashes when a user tries to access S3.</p>
<p>So the symptom is that <a href="https://github.com/apache/opendal">OpenDAL</a> (one of the parquet viewer’s dependencies) crashes because it did not receive enough data from S3 storage.</p>
<img src="https://aoli.al/images/chrome-bug/parquet-viewer-bug.png">
<h2 id="my-initial-attempt">My Initial Attempt</h2>
<p>Initially, I thought this was a bug inside OpenDAL, so I tried compiling everything on my native machine, but the bug disappeared. This allowed me to narrow the bugs to all WASM-related components.</p>
<p>My goal was to find the first program location where the data is lost. Unfortunately, debugging the WASM code is not fun, and I have fallen back to print debugging. So I was doing the following manually, starting with parquet viewer:</p>
<ol>
<li>download and compile the project</li>
<li>insert debugging code to print the size of the data</li>
<li>check if data is already shorter than expected</li>
<li>if true, download and compile the library it used to fetch the data and go back to step 1</li>
</ol>
<p>I have gone through Paquet Viewer, OpenDAL, and reqwest and all the way to the standard library. This process wasn’t easy, especially since I needed to find the right location to insert my debug code. Fortunately, LLMs helped me greatly.</p>
<p>But to my surprise, the application received chopped data from the very beginning.</p>
<h2 id="my-second-attempt">My Second Attempt</h2>
<p>Having spent a day debugging nothing was frustrating, and I was questioning what had gone wrong. As a reflection, the main issue of my debugging process is that I set the wrong <strong>trust boundary</strong> at the very beginning. I was assuming Chrome was functioning correctly and jumped into the rabbit hole of debugging application code.</p>
<p>After removing the trust boundary, the first thing I did was verify what exactly Chrome had received from the server. We found some surprising results with our old friend Wireshark: the server returns 403 for the second request. And more things are going on:</p>
<p>My client sends two requests to the server and I’m using the equivalent fetch command to show them:</p>
<p>The first request has range <code>4-138724</code>:</p>
<div><pre tabindex="0"><code data-lang="js"><span><span><span>fetch</span>(<span>"..."</span>, {
</span></span><span><span>  <span>"headers"</span><span>:</span> {
</span></span><span><span>    <span>"range"</span><span>:</span> <span>"bytes=4-138724"</span>,
</span></span><span><span>  },
</span></span><span><span>  <span>"body"</span><span>:</span> <span>null</span>,
</span></span><span><span>  <span>"method"</span><span>:</span> <span>"GET"</span>,
</span></span><span><span>  <span>"mode"</span><span>:</span> <span>"cors"</span>,
</span></span><span><span>  <span>"credentials"</span><span>:</span> <span>"omit"</span>
</span></span><span><span>})
</span></span></code></pre></div><p>The server successfully sends the requested data back and replied 206, and the Wireshark confirms that my server indeed responds to 206.</p>
<p>For the second request, it becomes interesting. The second request has a range <code>4-1943507</code>:</p>
<div><pre tabindex="0"><code data-lang="js"><span><span><span>fetch</span>(<span>"..."</span>, {
</span></span><span><span>  <span>"headers"</span><span>:</span> {
</span></span><span><span>    <span>"range"</span><span>:</span> <span>"bytes=4-1943507"</span>,
</span></span><span><span>  },
</span></span><span><span>  <span>"body"</span><span>:</span> <span>null</span>,
</span></span><span><span>  <span>"method"</span><span>:</span> <span>"GET"</span>,
</span></span><span><span>  <span>"mode"</span><span>:</span> <span>"cors"</span>,
</span></span><span><span>  <span>"credentials"</span><span>:</span> <span>"omit"</span>
</span></span><span><span>})
</span></span></code></pre></div><p>However, when Chrome sends the request to the server Wireshark shows that the real request is:</p>
<div><pre tabindex="0"><code data-lang="js"><span><span><span>fetch</span>(<span>"..."</span>, {
</span></span><span><span>  <span>"headers"</span><span>:</span> {
</span></span><span><span>    <span>"range"</span><span>:</span> <span>"bytes=138725-1943507"</span>,
</span></span><span><span>  },
</span></span><span><span>  <span>"body"</span><span>:</span> <span>null</span>,
</span></span><span><span>  <span>"method"</span><span>:</span> <span>"GET"</span>,
</span></span><span><span>  <span>"mode"</span><span>:</span> <span>"cors"</span>,
</span></span><span><span>  <span>"credentials"</span><span>:</span> <span>"omit"</span>
</span></span><span><span>})
</span></span></code></pre></div><p>The range field is changed. This is because Chrome noticed that the first <code>4-138724</code> bytes are already cached, and therefore, it only needs to request the rest part. Hmm, this sounds a great optimization to save data! But when the server responds, it becomes chaotic. Let’s first check what Chrome shows in the response:</p>
<img src="https://aoli.al/images/chrome-bug/chrome-response.png">
<p>Chrome says it has successfully gotten the data and even indicates bytes <code>4-1943507</code> in the response header. But in the response viewer, it shows that the response only has 138720 bytes of data!</p>
<img src="https://aoli.al/images/chrome-bug/chrome-data.png">
<p>This is crazy. Where did the rest of the data go? Now, if we go back to Wireshark, it will tell us that the second request to the server failed with 403 (it is another story why the server returns 403 for the second request)! However, Chrome still returns 206 to the application with only partial data.</p>
<h2 id="report-to-chromium-team">Report to Chromium Team</h2>
<p>This seems to be a bug in Chrome, so I decided to report it to the developers. To make their lives easier, I also created a buggy server to mimic the behavior I have encountered.</p>
<p>Here is the issue: <a href="https://issues.chromium.org/issues/390229583">https://issues.chromium.org/issues/390229583</a>. You may follow the steps to reproduce the bug if you are interested and I promise, it will not take more than 10 minutes.</p>
<p>Initially, I thought the developers would immediately acknowledge the issue and then fix it. But to my surprise, the issue was open for two months, and eventually, they thought it was a feature, not a bug.</p>
<img src="https://aoli.al/images/chrome-bug/feature-not-bug.png" width="250">
<p>After the issue was submitted, <del>the Chromium team refused to use my server to reproduce the bug and asked me to submit</del> a <a href="https://aoli.al/blogs/chrome-bug/%5Bhttps://www.chromium.org/for-testers/providing-network-details/%5D(https://www.chromium.org/for-testers/providing-network-details/)">network</a> log while reproducing it. So I did, out of curiosity. The Netlog looks scary because it not only contains the traffic while I reproduced the bug but also 1) all traffic from the Chrome plugins and 2) many websites that I have browsed before but haven’t visited during the recording. While I promise there is nothing in my browser history 😉. But I still don’t share it with public. So when I share my netlog, I added a restriction to the file.</p>
<img src="https://aoli.al/images/chrome-bug/restriction.png" width="400">
<p>But to my surprise, this blocks the Chromium team from accessing the file too. After all the back and forth, the Chromium team finally understands the issue. But they think this is a feature, not a bug, and you may find the complete response in the <a href="https://issues.chromium.org/issues/390229583">bug ticket</a>.</p>
<blockquote>
<p>All we’re saying is that this is how the range requests for partially-cached content in Chrome interact with the cache and it isn’t likely to change because there aren’t any really good ways to change the behavior that won’t also break existing applications. The existing behavior is at least consistent and can be worked with in application code by requesting the missing part of the range after the 2nd request completes (at which point you should get a 403 since none of the range overlaps with what is in cache).</p></blockquote>
<p>What a bug or a feature! I think I will probably stop tracking this issue since I’m working on neither web technologies nor browsers. I’m just a fan of tracking down and solving those elusive bugs. I opened an issue in <a href="https://github.com/apache/opendal/issues/5689">OpenDAL</a> because if Chrome is not going to fix it, they may need to fix it.</p>
<p>Also, maybe a lesson for me is that finding the right <strong>trust boundary</strong> can significantly speed up debugging!</p>

  </section>

  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ex-SAP CTO walks away with €7.1M payout after scandal (112 pts)]]></title>
            <link>https://www.theregister.com/2025/03/03/former_sap_cto_payout/</link>
            <guid>43244490</guid>
            <pubDate>Mon, 03 Mar 2025 17:44:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/03/03/former_sap_cto_payout/">https://www.theregister.com/2025/03/03/former_sap_cto_payout/</a>, See on <a href="https://news.ycombinator.com/item?id=43244490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>SAP paid former CTO Jürgen Müller €7.1 million ($7.5 million) after he left the German software company by mutual agreement in September last year.</p>
<p>Müller <a target="_blank" href="https://www.theregister.com/2024/09/03/sap_cto_departs/">departed</a> following an "incident" at a company event earlier in the year. At the time, SAP said its supervisory board had reached a "mutual agreement" with Müller for him to leave the company's executive board at the end of September.</p>
<p>In a prepared statement at the time, Müller said his behavior had been inappropriate. "I regret being inconsiderate and sincerely apologize to everyone affected. I recognize my behavior at that moment did not reflect our values at SAP."</p>

    

<p>Later, authorities <a target="_blank" href="https://www.theregister.com/2024/09/17/sap_cto_investigation/">confirmed</a> that the investigation related to allegations of sexual harassment.</p>

        


        

<p><a target="_blank" rel="nofollow" href="https://www.bloomberg.com/news/articles/2025-01-29/ex-sap-cto-settles-probe-over-inappropriate-conduct-claims">Reports</a> from January suggest that Müller had reached a settlement and that the investigation had ended. <em>The Register</em> has asked public prosecutor Staatsanwaltschaft Heidelberg to comment.</p>
<p>In a <a target="_blank" rel="nofollow" href="https://www.sap.com/docs/download/investors/2024/sap-2024-annual-report-form-20f.pdf">filing to the US Securities and Exchange Commission</a>, SAP documented its executive compensation. In addition to CEO Christian Klein receiving €19 million ($20 million) and Thomas Saueressig, board member for customer services and delivery, receiving €8.2 million ($8.6 million), the regulatory releases reveal total compensation to Müller of €7.14 million.</p>
<ul>

<li><a href="https://www.theregister.com/2025/02/14/sap_business_suite_revival/">Users await the fine print on SAP Business Suite reboot</a></li>

<li><a href="https://www.theregister.com/2025/02/13/sap_erp_revenue_trap/">SAP snared in revenue trap unless it extends legacy ERP support</a></li>

<li><a href="https://www.theregister.com/2024/09/17/sap_cto_investigation/">Prosecutors confirm probe into SAP CTO amid allegations of sexual harassment</a></li>

<li><a href="https://www.theregister.com/2024/09/03/sap_cto_departs/">SAP CTO bows out over 'incident' at company shindig</a></li>
</ul>
<p>The filing said Müller had reached a mutual agreement with the supervisory board to end his employment at SAP effective September 30, 2024.</p>
<p>Also departing this year was Scott Russell, who resigned from his position of chief revenue officer on August 31, 2024. He reached a "mutual agreement" with the supervisory board to end his employment at SAP effective December 31, 2024. A severance payment for the remainder of his original term of appointment until January 31, 2027, totaled €12.6 million ($13.2 million), while his total compensation reached €21.5 million ($22.5 million).</p>

        

<p>Julia White also exited the role of chief marketing and solutions officer, effective August 31, 2024. Her severance payment for the remainder of her original term of appointment until February 28, 2027, reached €9 million ($9.2 million), while her total compensation for 2024 was €17.1 million ($17.9 million).</p>
<p>For 2024, SAP said its cloud and software revenue was €29.96 billion ($31.5 billion), within the range of earlier forecasts, while its operating profit exceeded the forecast range to reach €8.2 billion ($8.6 billion).</p>
<p>The value of SAP's shares has risen by approximately 50 percent in the last year. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite-on-the-Server Is Misunderstood: Better at Hyper-Scale Than Micro-Scale (288 pts)]]></title>
            <link>https://rivet.gg/blog/2025-02-16-sqlite-on-the-server-is-misunderstood</link>
            <guid>43244307</guid>
            <pubDate>Mon, 03 Mar 2025 17:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rivet.gg/blog/2025-02-16-sqlite-on-the-server-is-misunderstood">https://rivet.gg/blog/2025-02-16-sqlite-on-the-server-is-misunderstood</a>, See on <a href="https://news.ycombinator.com/item?id=43244307">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><img blurwidth="8" blurheight="4" alt="Promo Image" loading="lazy" width="2334" height="1024" decoding="async" data-nimg="1" src="https://rivet.gg/_next/static/media/image.f86d756a.png">
<p>We're Rivet, a new open-source, self-hostable serverless platform. We've been in the weeds with SQLite-on-the-server recently and – boy – do we have a lot of thoughts to share. <a href="https://github.com/rivet-gg/rivet">Give us a star on GitHub</a>, we'll be sharing a lot more about SQLite soon!</p>
<p>There's been <a href="https://lobste.rs/s/t1enph/siren_call_sqlite_on_server">a lot of discussion</a> recently about the pros and cons of SQLite on the server. After reading many of these conversations, I realized that my perspective on the power of SQLite-on-the-server is lopsided from popular opinion: SQLite's strengths really shine at scale, instead of with small hobbyist deployments that it's frequently referenced in.</p>
<hr>
<p>Before jumping in to my perspective on the benefits of SQLite at scale, it's helpful to understand some background on SQLite-on-the-server for micro-scale apps.</p>

<img loading="lazy" width="2500" height="1786" decoding="async" data-nimg="1" src="https://rivet.gg/_next/static/media/sqlite-microscale.5d171f3e.png">
<p>Most developers consider server-side SQLite a simple, cost-effective choice for small-scale applications. It's often valued for:</p>
<ul>
<li><strong>Low infrastructure costs</strong>: No need for separate database servers—just a single file.</li>
<li><strong>Seamless development and testing</strong>: The same database file can be used across client and server.</li>
<li><strong>Minimal management overhead</strong>: No complex configurations or database daemons.</li>
<li><strong>Proven reliability</strong>: It's been around forever. It's the <a href="https://www.sqlite.org/mostdeployed.html">world's most widely deployed database</a> and <a href="https://hackernoon.com/the-story-of-dwayne-richard-hipp-and-the-development-of-sqlite-in-1999-yc4v356q">built to withstand battleships getting blown to bits</a>.</li>
</ul>
<p>These characteristics make SQLite an attractive option for personal projects, lightweight applications, and prototypes.</p>

<p>Tools like <a href="https://github.com/superfly/litefs">LiteFS</a>, <a href="https://litestream.io/">Litestream</a>, <a href="https://github.com/rqlite/rqlite">rqlite</a>, <a href="https://dqlite.io/">Dqlite</a>, and <a href="https://bedrockdb.com/">Bedrock</a> enhance SQLite with replication and high availability for micro-scale deployments.</p>
<p>However, this post focuses on <a href="https://developers.cloudflare.com/durable-objects/get-started/tutorial-with-sql-api/">Cloudflare Durable Objects</a> and <a href="https://turso.tech/">Turso</a> to highlight the often-overlooked advantages of SQLite at scale.</p>
<hr>
<img loading="lazy" width="1934" height="2178" decoding="async" data-nimg="1" src="https://rivet.gg/_next/static/media/cassandra.bb9addd0.png">
<p>In high-scale systems, companies frequently struggle scaling databases like <a href="https://www.postgresql.org/">Postgres</a> or <a href="https://www.mysql.com/">MySQL</a>. Instead, they often turn to sharded databases such as <a href="https://cassandra.apache.org/">Cassandra</a>, <a href="https://www.scylladb.com/">ScyllaDB</a>, <a href="https://aws.amazon.com/dynamodb/">DynamoDB</a>, <a href="https://vitess.io/">Vitess</a> (sharded MySQL), and <a href="https://www.citusdata.com/">Citus</a> (sharded Postgres).</p>
<p>These systems use partitioning keys to co-locate related &amp; similarly structured data. For example, a typical chat application on Cassandra might define:</p>

<p>To query messages from this partition, you could write:</p>


<p>Sharded databases power almost every large tech company because they provide:</p>
<ul>
<li>Efficient batch reads with data grouped in the same partition.</li>
<li>Horizontal scalability by partitioning data across nodes.</li>
<li>Optimized writes for high-ingestion workloads</li>
</ul>


<p>While partitioning strategies improve scalability, they introduce significant challenges:</p>
<ul>
<li><strong>Rigid schemas</strong>: Unlike Postgres or MySQL, the schema must exactly match the intended query patterns exactly, limiting flexibility.</li>
<li><strong>Complex schema changes</strong>: Adding a new index or relation requires significant operational overhead to create &amp; populate a new table in a live system.</li>
<li><strong>Complex cross-partition operations</strong>: Enforcing ACID properties across partitions is difficult. Companies often resort to complicated <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commits</a> or design with an acceptable level of data inconsistency.</li>
<li><strong>Data inconsistency</strong>: Without strong constraints between tables &amp; partitions, data frequently becomes "dirty" because of interrupted transactions or failure to propagate changes.</li>
</ul>
<hr>
<p><a href="https://developers.cloudflare.com/durable-objects/get-started/tutorial-with-sql-api/">Cloudflare Durable Objects</a> and <a href="https://turso.tech/">Turso</a> demonstrate how SQLite will change how hyper-scale applications may be architected in the future.</p>
<p>These databases provide:</p>
<ul>
<li><strong>Dynamic scaling</strong>: Instantly provision databases per entity, reducing infrastructure complexity.</li>
<li><strong>Infinite, cheap databases</strong>: Similar to partitions, you can spawn an infinite number of SQLite databases because they are incredibly cheap to create &amp; manage.</li>
<li><strong>Global distribution</strong>: Databases are placed closer to users, improving query performance.</li>
<li><strong>Built-in replication and durability</strong>: Unlike traditional SQLite, these services replicate data across multiple regions for high availability.</li>
</ul>

<img loading="lazy" width="1674" height="2172" decoding="async" data-nimg="1" src="https://rivet.gg/_next/static/media/sqlite-hyperscale.94d4d0af.png">
<p>Using SQLite with Cloudflare Durable Objects &amp; Turso allows defining databases per entity, effectively replacing partitioning keys.</p>
<p>Instead of storing chat logs in a single partition, each chat channel can have its own SQLite database that also includes more tables, like participants and reactions. A sample schema could look like this:</p>

<p>From Cloudflare Durable Objects or Turso, this SQLite partition database could be queried like this:</p>


<ul>
<li><strong>Local ACID transactions</strong>: Complex SQL queries can be within each partition without cross-partition complexities.</li>
<li><strong>Efficient I/O</strong>: SQLite enables performing complex queries within the partition with very high performance.</li>
<li><strong>Leverage existing SQLite extensions</strong>: SQLite has a rich ecosystem of existing extensions, such as <a href="https://www.sqlite.org/fts5.html">FTS5</a>, <a href="https://www.sqlite.org/json1.html">JSON1</a>, <a href="https://www.sqlite.org/rtree.html">R*Tree</a>, and <a href="https://www.gaia-gis.it/fossil/libspatialite/index">SpatiaLite</a>.</li>
<li><strong>Full Power of SQL migrations</strong>: SQLite provides the full power of SQL migrations &amp; leveraging existing migrations tools such as Drizzle &amp; Prisma.</li>
<li><strong>Lazy schema migrations</strong>: Changing schema is tricky at scale. Assuming your migrations are lightweight, they can be executed on demand after the SQLite database is opened at the cost of a slightly higher p99 after deploys.</li>
</ul>

<hr>
<p>Despite its benefits, SQLite at scale presents a few challenges:</p>
<ul>
<li>Lack of an open-source, self-hosted solution.</li>
<li>No built-in cross-database querying, making complex analytics difficult without a dedicated data lake.</li>
<li>Limited database tooling, such as SQL browsers, ETL pipelines, monitoring, and backups. <a href="https://starbasedb.com/">StarbaseDB</a> is addressing this for Cloudflare Durable Objects with SQLite.</li>
<li>Non-standard protocols for communicating with SQLite-on-the-server. In contrast, PostgreSQL, MySQL, and Cassandra all have a well standardized wire protocols across all cloud providers that has led to a rich community of tools.</li>
<li>There are no case studies like Cassandra &amp; DynamoDB have of using SQLite with this architecture at hyper-scale. This will change with time.</li>
</ul>

<hr>
<p>SQLite on the server is more than a lightweight solution for small deployments – it's an increasingly viable alternative to traditional partitioned databases. By leveraging SQLite-per-partition solutions like Turso and Durable Objects, developers gain rich SQL capabilities, ACID compliance, and significant operational advantages.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Sonauto API – Generative music for developers (103 pts)]]></title>
            <link>https://sonauto.ai/developers</link>
            <guid>43244166</guid>
            <pubDate>Mon, 03 Mar 2025 17:17:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sonauto.ai/developers">https://sonauto.ai/developers</a>, See on <a href="https://news.ycombinator.com/item?id=43244166">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The power of interning: making a time series database 2000x smaller in Rust (243 pts)]]></title>
            <link>https://gendignoux.com/blog/2025/03/03/rust-interning-2000x.html</link>
            <guid>43243914</guid>
            <pubDate>Mon, 03 Mar 2025 17:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gendignoux.com/blog/2025/03/03/rust-interning-2000x.html">https://gendignoux.com/blog/2025/03/03/rust-interning-2000x.html</a>, See on <a href="https://news.ycombinator.com/item?id=43243914">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header>
        
        <p>
            <a href="https://gendignoux.com/blog/tags.html#rust"> rust</a><a href="https://gendignoux.com/blog/tags.html#perf"> perf</a><a href="https://gendignoux.com/blog/tags.html#json"> json</a>
        </p>
        <p>March 3, 2025</p>
        <p>by <span>Guillaume Endignoux</span>
        <br>
            <a href="https://infosec.exchange/@gendx" target="_blank"> @gendx</a> | <a href="https://gendignoux.com/blog/feed.xml"> RSS</a>
        </p>
    </header>
    <section>
        <p>This week-end project started by browsing the <a href="https://prim.iledefrance-mobilites.fr/en">open-data repository</a> of Paris’ public transport network, which contains <a href="https://prim.iledefrance-mobilites.fr/en/catalogue-data?type=api">various APIs</a> to query <a href="https://prim.iledefrance-mobilites.fr/en/apis/idfm-ivtr-requete_globale">real-time departures</a>, <a href="https://prim.iledefrance-mobilites.fr/en/apis/idfm-disruptions_bulk">current disruptions</a>, etc.
The <a href="https://prim.iledefrance-mobilites.fr/en/reutilisations">data reuse section</a> caught my eye, as it features external projects that use this open data.
In particular, the <a href="https://ratpstatus.fr/">RATP status website</a> provides a really nice interface to visualize historical disruptions on metro, RER/train and tramway lines.</p>

<!--more-->

<p><a href="https://gendignoux.com/blog/images/rust-interning-2000x/ratp-status-2025-01-28.aUIZIqVMnogK.png"><img src="https://gendignoux.com/blog/images/rust-interning-2000x/ratp-status-2025-01-28.BnEKVNacK-tC.webp" width="2559" height="1251" loading="lazy" alt="Screenshot of the RATP status website"></a>
A usual day of disruptions on <a href="https://ratpstatus.fr/">ratpstatus.fr</a>.</p>

<p>Under the hood, the <a href="https://github.com/wincelau/ratpstatus">ratpstatus.fr GitHub repository</a> contains <a href="https://github.com/wincelau/ratpstatus/tree/main/datas">all the JSON files</a> queried from the open-data API, every 2 minutes for almost a year now.
A repository with 188K commits and more than 10 GB of accumulated data at the last commit alone (as measured by <code>git clone --depth=1</code>) is definitely an interesting database choice!
To be clear, this post isn’t in any way a critique of that.
RATP status is an excellent website providing useful information that runs blazingly fast<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> and smoothly without the usual bloat you see on the web nowadays.</p>

<p>Nonetheless, the 10 GB of data got me to wonder: can we compress that better, by spending a reasonable amount of time (i.e. a week-end project)?
In this deep dive post, I’ll explain how I used the <a href="https://en.wikipedia.org/wiki/Interning_(computer_science)"><em>interning</em> design pattern</a> in Rust to compress this data set by a factor of two thousand!
We’ll investigate how to best structure the interner itself, how to tune our data schema to work well with it, and likewise how serialization can best leverage interning.</p>

<p>If you’ve got lots of JSON files accumulating in your storage, you should read on!</p>

<hr>

<ul id="markdown-toc">
  <li><a href="#importing-the-data-135" id="markdown-toc-importing-the-data-135">Importing the data (135%)</a></li>
  <li><a href="#interning" id="markdown-toc-interning">Interning</a>    <ul>
      <li><a href="#strings-47" id="markdown-toc-strings-47">Strings (47%)</a></li>
      <li><a href="#arbitrary-types-76" id="markdown-toc-arbitrary-types-76">Arbitrary types (7.6%)</a></li>
      <li><a href="#dropping-the-reference-28" id="markdown-toc-dropping-the-reference-28">Dropping the reference (2.8%)</a></li>
    </ul>
  </li>
  <li><a href="#tuning-the-schema" id="markdown-toc-tuning-the-schema">Tuning the schema</a>    <ul>
      <li><a href="#sorting-sets-15" id="markdown-toc-sorting-sets-15">Sorting sets (1.5%)</a></li>
      <li><a href="#using-enums-14" id="markdown-toc-using-enums-14">Using enums (1.4%)</a></li>
      <li><a href="#splitting-structs-082" id="markdown-toc-splitting-structs-082">Splitting structs (0.82%)</a></li>
      <li><a href="#specializing-types-064" id="markdown-toc-specializing-types-064">Specializing types (0.64%)</a></li>
    </ul>
  </li>
  <li><a href="#serialization" id="markdown-toc-serialization">Serialization</a>    <ul>
      <li><a href="#writing-custom-deserializers-with-serde-029" id="markdown-toc-writing-custom-deserializers-with-serde-029">Writing custom (de)serializers with Serde (0.29%)</a></li>
      <li><a href="#compression-and-fighting-the-rust-borrow-checker-linux-pipes-005" id="markdown-toc-compression-and-fighting-the-rust-borrow-checker-linux-pipes-005">Compression and fighting <del>the Rust borrow checker</del> Linux pipes (0.05%)</a></li>
      <li><a href="#tuple-encoding" id="markdown-toc-tuple-encoding">Tuple encoding</a></li>
      <li><a href="#optimizing-sets-revisited" id="markdown-toc-optimizing-sets-revisited">Optimizing sets revisited</a></li>
    </ul>
  </li>
  <li><a href="#final-result-a-lightweight-append-only-database" id="markdown-toc-final-result-a-lightweight-append-only-database">Final result: a lightweight append-only database</a></li>
</ul>

<h2 id="importing-the-data-135"><span><a href="#importing-the-data-135"></a></span>Importing the data (135%)</h2>

<p>The first step of this experiment was to import the source data.
To give a bit more context, each data point was a JSON file with many entries looking like this.</p>

<div><pre><code><span>{</span><span>
  </span><span>"disruptions"</span><span>:</span><span> </span><span>[</span><span>
    </span><span>{</span><span>
      </span><span>"id"</span><span>:</span><span> </span><span>"445a6032-d1ca-11ef-b3f5-0a58a9feac02"</span><span>,</span><span>
      </span><span>"applicationPeriods"</span><span>:</span><span> </span><span>[</span><span>
        </span><span>{</span><span>
          </span><span>"begin"</span><span>:</span><span> </span><span>"20250113T180000"</span><span>,</span><span>
          </span><span>"end"</span><span>:</span><span> </span><span>"20250228T230000"</span><span>
        </span><span>}</span><span>
      </span><span>],</span><span>
      </span><span>"lastUpdate"</span><span>:</span><span> </span><span>"20250113T172013"</span><span>,</span><span>
      </span><span>"cause"</span><span>:</span><span> </span><span>"PERTURBATION"</span><span>,</span><span>
      </span><span>"severity"</span><span>:</span><span> </span><span>"BLOQUANTE"</span><span>,</span><span>
      </span><span>"title"</span><span>:</span><span> </span><span>"Activities in Aincourt"</span><span>,</span><span>
      </span><span>"message"</span><span>:</span><span> </span><span>"&lt;p&gt;Due to work in Aincourt, the Centre and Eglise stops will not be served in both directions of traffic on line 95 15 and in the direction of Magny en Vexin Gare Routière only on line 95 44. &lt;br&gt;From 13/01 until further notice. &lt;/p&gt;&lt;br&gt;Please refer to Les Cadenas stops"</span><span>
    </span><span>},</span><span>
  </span><span>...</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>Let’s import this data into our program!
If you’re not familiar with <a href="https://www.rust-lang.org/">Rust</a>, this programming language makes it very easy to deserialize data from all sorts of formats via libraries like <a href="https://docs.rs/serde/"><code>serde</code></a> and <a href="https://docs.rs/serde_json/"><code>serde_json</code></a>.
I’m depending on the following versions in my <a href="https://doc.rust-lang.org/cargo/reference/manifest.html"><code>Cargo.toml</code> manifest</a>.</p>

<div><pre><code><span>[dependencies]</span>
<span>serde</span> <span>=</span> <span>{</span> <span>version</span> <span>=</span> <span>"1.0.217"</span><span>,</span> <span>features</span> <span>=</span> <span>[</span><span>"derive"</span><span>]</span> <span>}</span>
<span>serde_json</span> <span>=</span> <span>"1.0.137"</span>
</code></pre></div>

<p>With that, we can define a data schema as regular Rust structs/enums and simply annotate them with <code>serde</code>’s <a href="https://docs.rs/serde/1.0.217/serde/derive.Deserialize.html"><code>Deserialize</code> derive macro</a> to automatically implement deserialization for it.
I recommend using the <a href="https://serde.rs/container-attrs.html#deny_unknown_fields"><code>deny_unknown_fields</code> attribute</a> to make sure unknown JSON fields aren’t silently ignored.
These attributes are documented separately on the <a href="https://serde.rs/attributes.html">serde.rs website</a> (not on <a href="https://docs.rs/serde/">docs.rs</a>).</p>

<div><pre><code><span>use</span> <span>serde</span><span>::</span><span>Deserialize</span><span>;</span>

<span>#[derive(Debug,</span> <span>Deserialize)]</span>
<span>#[serde(deny_unknown_fields)]</span>
<span>struct</span> <span>Data</span> <span>{</span>
    <span>#[serde(rename</span> <span>=</span> <span>"statusCode"</span><span>)]</span>
    <span>status_code</span><span>:</span> <span>Option</span><span>&lt;</span><span>i32</span><span>&gt;</span><span>,</span>
    <span>error</span><span>:</span> <span>Option</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
    <span>message</span><span>:</span> <span>Option</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
    <span>disruptions</span><span>:</span> <span>Option</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Disruption</span><span>&gt;&gt;</span><span>,</span>
    <span>lines</span><span>:</span> <span>Option</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Line</span><span>&gt;&gt;</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"lastUpdatedDate"</span><span>)]</span>
    <span>last_updated_date</span><span>:</span> <span>Option</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>One can then trivially deserialize a JSON file into a <code>Data</code> struct with functions like <a href="https://docs.rs/serde_json/1.0.137/serde_json/fn.from_reader.html"><code>serde_json::from_reader()</code></a>.</p>

<div><pre><code><span>// Open a file for reading.</span>
<span>let</span> <span>file</span> <span>=</span> <span>File</span><span>::</span><span>open</span><span>(</span><span>path</span><span>)</span><span>?</span><span>;</span>
<span>// Add a layer of buffering for performance.</span>
<span>let</span> <span>reader</span> <span>=</span> <span>BufReader</span><span>::</span><span>new</span><span>(</span><span>file</span><span>);</span>
<span>// Deserialize the JSON contents into a Data.</span>
<span>let</span> <span>data</span><span>:</span> <span>Data</span> <span>=</span> <span>serde_json</span><span>::</span><span>from_reader</span><span>(</span><span>reader</span><span>)</span><span>?</span><span>;</span>
</code></pre></div>

<p>To give more details about the specific data schema I’m importing, each <code>Disruption</code> contains informative fields, as well as a list of time periods during which it applies.
For example, there may be construction work on a line every evening for a month, so there would be an <code>ApplicationPeriod</code> for each of these evenings.</p>

<div><pre><code><span>#[derive(Debug,</span> <span>Deserialize)]</span>
<span>#[serde(deny_unknown_fields)]</span>
<span>struct</span> <span>Disruption</span> <span>{</span>
    <span>id</span><span>:</span> <span>String</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"applicationPeriods"</span><span>)]</span>
    <span>application_periods</span><span>:</span> <span>Vec</span><span>&lt;</span><span>ApplicationPeriod</span><span>&gt;</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"lastUpdate"</span><span>)]</span>
    <span>last_update</span><span>:</span> <span>String</span><span>,</span>
    <span>cause</span><span>:</span> <span>String</span><span>,</span>
    <span>severity</span><span>:</span> <span>String</span><span>,</span>
    <span>tags</span><span>:</span> <span>Option</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>String</span><span>&gt;&gt;</span><span>,</span>
    <span>title</span><span>:</span> <span>String</span><span>,</span>
    <span>message</span><span>:</span> <span>String</span><span>,</span>
    <span>disruption_id</span><span>:</span> <span>Option</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>#[derive(Debug,</span> <span>Deserialize)]</span>
<span>#[serde(deny_unknown_fields)]</span>
<span>struct</span> <span>ApplicationPeriod</span> <span>{</span>
    <span>begin</span><span>:</span> <span>String</span><span>,</span>
    <span>end</span><span>:</span> <span>String</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>The data also contains an index by <code>Line</code>s, listing all the objects (e.g. stations) impacted by disruptions on each metro line.</p>

<div id="schema-line"><pre><code><span>#[derive(Debug,</span> <span>Deserialize)]</span>
<span>#[serde(deny_unknown_fields)]</span>
<span>struct</span> <span>Line</span> <span>{</span>
    <span>id</span><span>:</span> <span>String</span><span>,</span>
    <span>name</span><span>:</span> <span>String</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"shortName"</span><span>)]</span>
    <span>short_name</span><span>:</span> <span>String</span><span>,</span>
    <span>mode</span><span>:</span> <span>String</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"networkId"</span><span>)]</span>
    <span>network_id</span><span>:</span> <span>String</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"impactedObjects"</span><span>)]</span>
    <span>impacted_objects</span><span>:</span> <span>Vec</span><span>&lt;</span><span>ImpactedObject</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>#[derive(Debug,</span> <span>Deserialize)]</span>
<span>#[serde(deny_unknown_fields)]</span>
<span>struct</span> <span>ImpactedObject</span> <span>{</span>
    <span>#[serde(rename</span> <span>=</span> <span>"type"</span><span>)]</span>
    <span>typ</span><span>:</span> <span>String</span><span>,</span>
    <span>id</span><span>:</span> <span>String</span><span>,</span>
    <span>name</span><span>:</span> <span>String</span><span>,</span>
    <span>#[serde(rename</span> <span>=</span> <span>"disruptionIds"</span><span>)]</span>
    <span>disruption_ids</span><span>:</span> <span>Vec</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>Lastly, I wanted to estimate how much space these objects take in memory, in order to benchmark the improvements obtained by interning methods.
Rust’s <a href="https://doc.rust-lang.org/std/mem/fn.size_of.html"><code>std::mem::size_of()</code> function</a> returns the “stack” size of an object, but that’s not sufficient as it ignores any data indirectly allocated on the heap (such as via a <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html"><code>Vec</code></a> collection).
Therefore, I defined a new trait and implemented it for the needed types.</p>

<div><pre><code><span>trait</span> <span>EstimateSize</span><span>:</span> <span>Sized</span> <span>{</span>
    <span>/// Returns the number of bytes indirectly allocated on the heap by this object.</span>
    <span>fn</span> <span>allocated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span><span>;</span>

    <span>/// Returns the total number of bytes that this object uses.</span>
    <span>fn</span> <span>estimated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>std</span><span>::</span><span>mem</span><span>::</span><span>size_of</span><span>::</span><span>&lt;</span><span>Self</span><span>&gt;</span><span>()</span> <span>+</span> <span>self</span><span>.allocated_bytes</span><span>()</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>EstimateSize</span> <span>for</span> <span>i32</span> <span>{</span>
    <span>fn</span> <span>allocated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>0</span>  <span>// Nothing allocated on the heap.</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>EstimateSize</span> <span>for</span> <span>String</span> <span>{</span>
    <span>fn</span> <span>allocated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>self</span><span>.len</span><span>()</span>  <span>// Each item is one byte long. Ignores the string capacity.</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>EstimateSize</span><span>&gt;</span> <span>EstimateSize</span> <span>for</span> <span>Vec</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>allocated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>// Recursively sum each item's total size.</span>
        <span>self</span><span>.iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span><span>.estimated_bytes</span><span>())</span><span>.sum</span><span>()</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>For compound types (structs), the implementation visits all the fields.
In principle, this could automatically be implemented by a <a href="https://doc.rust-lang.org/reference/procedural-macros.html#derive-macros">derive macro</a> (like serde’s <code>Deserialize</code>), but creating a new macro seemed overkill given the scale of my experiment.</p>

<div><pre><code><span>impl</span> <span>EstimateSize</span> <span>for</span> <span>Data</span> <span>{</span>
    <span>fn</span> <span>allocated_bytes</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>self</span><span>.status_code</span><span>.allocated_bytes</span><span>()</span>
            <span>+</span> <span>self</span><span>.error</span><span>.allocated_bytes</span><span>()</span>
            <span>+</span> <span>self</span><span>.message</span><span>.allocated_bytes</span><span>()</span>
            <span>+</span> <span>self</span><span>.disruptions</span><span>.allocated_bytes</span><span>()</span>
            <span>+</span> <span>self</span><span>.lines</span><span>.allocated_bytes</span><span>()</span>
            <span>+</span> <span>self</span><span>.last_updated_date</span><span>.allocated_bytes</span><span>()</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>With that, reading all the files from May 2024 gave the following result: expanding the 1.1 GB of JSON files into in-memory structs increased the size by 35% (commit <a href="https://github.com/gendx/rust-interning/commit/d961e6e5f0cce53c2b328a7fd70482811e80f26e">d961e6e</a>).
Not in the right direction… let’s start optimizing!</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
</code></pre></div>

<h2 id="interning"><span><a href="#interning"></a></span>Interning</h2>

<p>In this section, I’ll present the basics of the interning pattern, and how to apply it to various types.</p>

<h3 id="strings-47"><span><a href="#strings-47"></a></span>Strings (47%)</h3>

<p>The first use case that comes to mind in terms of interning is strings, as evidenced by the <a href="https://lib.rs/keywords/interning">top Rust interning libraries</a>.
We’re not going to use any of these packages, as my goal was to learn more about the inner details of interning.</p>

<p>I stumbled upon a blog post by <em>matklad</em> from 2020 titled <a href="https://matklad.github.io/2020/03/22/fast-simple-rust-interner.html"><em>Fast and Simple Rust Interner</em></a>, and my first iteration is inspired by this design.
The main difference is that I wrapped strings into an <a href="https://doc.rust-lang.org/std/rc/struct.Rc.html"><code>Rc</code></a> (reference-counted wrapper) to avoid duplicating them in memory.
If the interner is intended to be used from multiple threads, I’d use an <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc</code></a> instead, but I’m keeping things simple for this experiment.</p>

<p>So what is an interner?
It’s essentially a table of strings, consisting of a vector of strings paired with a hash map that allows looking up if a string is already in the database, and if so at which index in the vector.</p>

<div><pre><code><span>use</span> <span>std</span><span>::</span><span>collections</span><span>::</span><span>HashMap</span><span>;</span>
<span>use</span> <span>std</span><span>::</span><span>rc</span><span>::</span><span>Rc</span><span>;</span>

<span>#[derive(Default)]</span>
<span>struct</span> <span>StringInternerImpl</span> <span>{</span>
    <span>vec</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Rc</span><span>&lt;</span><span>String</span><span>&gt;&gt;</span><span>,</span>
    <span>map</span><span>:</span> <span>HashMap</span><span>&lt;</span><span>Rc</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span> <span>usize</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span> <span>StringInternerImpl</span> <span>{</span>
    <span>fn</span> <span>intern</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> <span>value</span><span>:</span> <span>String</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>if</span> <span>let</span> <span>Some</span><span>(</span><span>&amp;</span><span>id</span><span>)</span> <span>=</span> <span>self</span><span>.map</span><span>.get</span><span>(</span><span>&amp;</span><span>value</span><span>)</span> <span>{</span>
            <span>return</span> <span>id</span><span>;</span>
        <span>}</span>

        <span>let</span> <span>id</span> <span>=</span> <span>self</span><span>.vec</span><span>.len</span><span>();</span>
        <span>let</span> <span>rc</span><span>:</span> <span>Rc</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>=</span> <span>Rc</span><span>::</span><span>new</span><span>(</span><span>value</span><span>);</span>
        <span>self</span><span>.vec</span><span>.push</span><span>(</span><span>Rc</span><span>::</span><span>clone</span><span>(</span><span>&amp;</span><span>rc</span><span>));</span>
        <span>self</span><span>.map</span><span>.insert</span><span>(</span><span>rc</span><span>,</span> <span>id</span><span>);</span>
        <span>id</span>
    <span>}</span>

    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>id</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>{</span>
        <span>Rc</span><span>::</span><span>clone</span><span>(</span><span>&amp;</span><span>self</span><span>.vec</span><span>[</span><span>id</span><span>])</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>An interned string is then an index in the interner table.
The main advantage of this setup is to reduce the amount of memory used by the program, because an integer index is usually smaller than the full string.
This is especially effective when there are many repeated strings.</p>

<p><a href="https://gendignoux.com/blog/images/rust-interning-2000x/string-interning.lrRCNfxir2Qy.svg"><img src="https://gendignoux.com/blog/images/rust-interning-2000x/string-interning.lrRCNfxir2Qy.svg" width="744" height="440" loading="lazy" alt="Memory layout of strings without and with interning"></a>
Memory layout of strings without and with interning.</p>

<p>There are several possible designs to represent an interned string object: as a first iteration I’ve chosen to pair the index with a reference to the interner that contains it.</p>

<div><pre><code><span>struct</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>StringInterner</span><span>,</span>
    <span>id</span><span>:</span> <span>usize</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>from</span><span>(</span><span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>StringInterner</span><span>,</span> <span>value</span><span>:</span> <span>String</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>let</span> <span>id</span> <span>=</span> <span>interner</span><span>.intern</span><span>(</span><span>value</span><span>);</span>
        <span>Self</span> <span>{</span> <span>interner</span><span>,</span> <span>id</span> <span>}</span>
    <span>}</span>

    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>{</span>
        <span>self</span><span>.interner</span><span>.lookup</span><span>(</span><span>self</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>At this point, you might have noticed that I’ve declared <code>StringInterner</code> and <code>StringInternerImpl</code> types.
What’s the difference?</p>

<p>The answer is that once an <code>IString</code> captures an interner handle <code>&amp;StringInterner</code>, the underlying <code>StringInterner</code> cannot be mutated anymore via an <code>intern()</code> function taking a <code>&amp;mut self</code> parameter, as it would break <a href="https://doc.rust-lang.org/rust-by-example/scope/borrow/alias.html">Rust’s aliasing rules</a>: there cannot be both a <code>&amp;StringInterner</code> and a <code>&amp;mut StringInterner</code> pointing to the same thing at the same time.
This is quite unfortunate, as it prevents interning more than one string!</p>

<p>The way to resolve this conflict is to use <a href="https://doc.rust-lang.org/reference/interior-mutability.html"><em>interior mutability</em></a> via the <a href="https://doc.rust-lang.org/std/cell/struct.RefCell.html"><code>RefCell</code> type</a>.
By defining a <code>StringInterner</code> as a <code>RefCell&lt;StringInternerImpl&gt;</code>, we can intern more values without needing a <code>&amp;mut self</code> reference to the interner.</p>

<div><pre><code><span>#[derive(Default)]</span>
<span>struct</span> <span>StringInterner</span> <span>{</span>
    <span>inner</span><span>:</span> <span>RefCell</span><span>&lt;</span><span>StringInternerImpl</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span> <span>StringInterner</span> <span>{</span>
    <span>// This function takes an immutable reference!</span>
    <span>fn</span> <span>intern</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>value</span><span>:</span> <span>String</span><span>)</span> <span>-&gt;</span> <span>usize</span> <span>{</span>
        <span>// The borrow_mut() method performs runtime checks and releases a</span>
        <span>// mutable reference if it's safe to do so (or panics).</span>
        <span>self</span><span>.inner</span><span>.borrow_mut</span><span>()</span><span>.intern</span><span>(</span><span>value</span><span>)</span>
    <span>}</span>

    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>id</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>{</span>
        <span>self</span><span>.inner</span><span>.borrow</span><span>()</span><span>.lookup</span><span>(</span><span>id</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>With this setup, we can for example overload the comparison operator <code>==</code> to directly compare interned strings with regular strings.</p>

<div><pre><code><span>impl</span> <span>PartialEq</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>for</span> <span>IString</span><span>&lt;</span><span>'_</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>String</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.lookup</span><span>()</span><span>.deref</span><span>()</span> <span>==</span> <span>other</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>Lastly, I’ve defined new structs for the data schema using the interned string type in place of all strings.
This means adding a lifetime <code>'a</code> everywhere, which isn’t quite ergonomic, but we’ll revisit this pattern <a href="#dropping-the-reference-28">later</a>.</p>

<div><pre><code><span>struct</span> <span>Disruption</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>application_periods</span><span>:</span> <span>Vec</span><span>&lt;</span><span>ApplicationPeriod</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
    <span>last_update</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>cause</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>severity</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>tags</span><span>:</span> <span>Option</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>IString</span><span>&lt;</span><span>'a</span><span>&gt;&gt;&gt;</span><span>,</span>
    <span>title</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>message</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>disruption_id</span><span>:</span> <span>Option</span><span>&lt;</span><span>IString</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>

<span>struct</span> <span>ApplicationPeriod</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>begin</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>end</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>I’ve also defined functions to convert data from the original structs to their interned counterparts, as well as comparison functions to validate that the interned data is semantically equivalent to the original (and confirm that my benchmarks are not cheating).</p>

<div><pre><code><span>impl</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>ApplicationPeriod</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>from</span><span>(</span><span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>StringInterner</span><span>,</span> <span>source</span><span>:</span> <span>source</span><span>::</span><span>ApplicationPeriod</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>Self</span> <span>{</span>
            <span>begin</span><span>:</span> <span>IString</span><span>::</span><span>from</span><span>(</span><span>interner</span><span>,</span> <span>source</span><span>.begin</span><span>),</span>
            <span>end</span><span>:</span> <span>IString</span><span>::</span><span>from</span><span>(</span><span>interner</span><span>,</span> <span>source</span><span>.end</span><span>),</span>
        <span>}</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>PartialEq</span><span>&lt;</span><span>source</span><span>::</span><span>ApplicationPeriod</span><span>&gt;</span> <span>for</span> <span>ApplicationPeriod</span><span>&lt;</span><span>'_</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>source</span><span>::</span><span>ApplicationPeriod</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.begin</span> <span>==</span> <span>other</span><span>.begin</span> <span>&amp;&amp;</span> <span>self</span><span>.end</span> <span>==</span> <span>other</span><span>.end</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>With that, we can already see the effectiveness of interning: each string appeared on average 425 times in the input data, the in-memory data is three times smaller than the baseline, and twice smaller than the original JSON files (commit <a href="https://github.com/gendx/rust-interning/commit/5297faabef807c535bba8c29ca665c72957f3ca0">5297faa</a>).
We’re still quite far from the headline of this post though!</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 529308335 bytes (relative size = 46.55%)
- [0.84%] String interner: 56374 objects | 4433343 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
</code></pre></div>

<h3 id="arbitrary-types-76"><span><a href="#arbitrary-types-76"></a></span>Arbitrary types (7.6%)</h3>

<p>The next step was to realize that strings aren’t the only objects that repeat a lot in the input data.
For example, a <code>Disruption</code> may last maybe an hour if it’s unexpected, or something like a month if it’s planned maintenance work.
Given that the input data is a time series sampled every 2 minutes, it’s expected that a given disruption will show up many times.</p>

<p>Fortunately, the interning technique isn’t unique to strings: any data that can be put into a vector and a hash map should work as well.
So we can use <a href="https://doc.rust-lang.org/rust-by-example/generics.html">generics</a> to make it work for arbitrary types that implement <a href="https://doc.rust-lang.org/std/cmp/trait.Eq.html"><code>Eq</code></a> and <a href="https://doc.rust-lang.org/std/hash/trait.Hash.html"><code>Hash</code></a> (to work with a hash map).</p>

<div><pre><code><span>// Type alias for convenience.</span>
<span>type</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>=</span> <span>Interned</span><span>&lt;</span><span>'a</span><span>,</span> <span>String</span><span>&gt;</span><span>;</span>

<span>struct</span> <span>Interned</span><span>&lt;</span><span>'a</span><span>,</span> <span>T</span><span>&gt;</span> <span>{</span>
    <span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>,</span>
    <span>id</span><span>:</span> <span>usize</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'a</span><span>,</span> <span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Interned</span><span>&lt;</span><span>'a</span><span>,</span> <span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>from</span><span>(</span><span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>,</span> <span>value</span><span>:</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>let</span> <span>id</span> <span>=</span> <span>interner</span><span>.intern</span><span>(</span><span>value</span><span>);</span>
        <span>Self</span> <span>{</span> <span>interner</span><span>,</span> <span>id</span> <span>}</span>
    <span>}</span>

    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
        <span>self</span><span>.interner</span><span>.lookup</span><span>(</span><span>self</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>As a new requirement, we also need to implement the <a href="https://doc.rust-lang.org/std/cmp/trait.PartialEq.html"><code>PartialEq</code></a>, <a href="https://doc.rust-lang.org/std/cmp/trait.Eq.html"><code>Eq</code></a> and <a href="https://doc.rust-lang.org/std/hash/trait.Hash.html"><code>Hash</code></a> traits on <code>Interned&lt;_&gt;</code>, so that it can be recursively used in structs that are themselves interned.
A naive implementation is to lookup the actual data and compare or hash it, but we’ll revisit that in a moment.</p>

<div><pre><code><span>use</span> <span>std</span><span>::</span><span>hash</span><span>::{</span><span>Hash</span><span>,</span> <span>Hasher</span><span>};</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>PartialEq</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>'_</span><span>,</span> <span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>Self</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.lookup</span><span>()</span><span>.deref</span><span>()</span> <span>==</span> <span>other</span><span>.lookup</span><span>()</span><span>.deref</span><span>()</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Eq</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>'_</span><span>,</span> <span>T</span><span>&gt;</span> <span>{}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Hash</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>'_</span><span>,</span> <span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>hash</span><span>&lt;</span><span>H</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>state</span><span>:</span> <span>&amp;</span><span>mut</span> <span>H</span><span>)</span>
    <span>where</span>
        <span>H</span><span>:</span> <span>Hasher</span><span>,</span>
    <span>{</span>
        <span>self</span><span>.lookup</span><span>()</span><span>.deref</span><span>()</span><span>.hash</span><span>(</span><span>state</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>On the schema side, we’ll now have a collection of interners: each type gets its own interner.</p>

<div><pre><code><span>#[derive(Default)]</span>
<span>struct</span> <span>Interners</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>string</span><span>:</span> <span>Interner</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
    <span>disruption</span><span>:</span> <span>Interner</span><span>&lt;</span><span>Disruption</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
    <span>line</span><span>:</span> <span>Interner</span><span>&lt;</span><span>Line</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
    <span>application_period</span><span>:</span> <span>Interner</span><span>&lt;</span><span>ApplicationPeriod</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
    <span>impacted_object</span><span>:</span> <span>Interner</span><span>&lt;</span><span>ImpactedObject</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>The data structs in the schema can now contain interned versions of other structs, such as <code>Interned&lt;'a, ApplicationPeriod&lt;'a&gt;&gt;</code>, and simply derive the comparison and hashing traits.</p>

<div><pre><code><span>#[derive(Debug,</span> <span>Hash,</span> <span>PartialEq,</span> <span>Eq)]</span>
<span>struct</span> <span>Disruption</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>application_periods</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Interned</span><span>&lt;</span><span>'a</span><span>,</span> <span>ApplicationPeriod</span><span>&lt;</span><span>'a</span><span>&gt;&gt;&gt;</span><span>,</span>
    <span>last_update</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>cause</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>severity</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>tags</span><span>:</span> <span>Option</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>IString</span><span>&lt;</span><span>'a</span><span>&gt;&gt;&gt;</span><span>,</span>
    <span>title</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>message</span><span>:</span> <span>IString</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span>
    <span>disruption_id</span><span>:</span> <span>Option</span><span>&lt;</span><span>IString</span><span>&lt;</span><span>'a</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>Conversion from the original data types now uses a reference to the whole collection of <code>Interners</code>.</p>

<div><pre><code><span>impl</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>Disruption</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>from</span><span>(</span><span>interners</span><span>:</span> <span>&amp;</span><span>'a</span> <span>Interners</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>,</span> <span>source</span><span>:</span> <span>source</span><span>::</span><span>Disruption</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>Self</span> <span>{</span>
            <span>id</span><span>:</span> <span>Interned</span><span>::</span><span>from</span><span>(</span><span>&amp;</span><span>interners</span><span>.string</span><span>,</span> <span>source</span><span>.id</span><span>),</span>
            <span>application_periods</span><span>:</span> <span>source</span>
                <span>.application_periods</span>
                <span>.into_iter</span><span>()</span>
                <span>.map</span><span>(|</span><span>x</span><span>|</span> <span>{</span>
                    <span>Interned</span><span>::</span><span>from</span><span>(</span>
                        <span>&amp;</span><span>interners</span><span>.application_period</span><span>,</span>
                        <span>ApplicationPeriod</span><span>::</span><span>from</span><span>(</span><span>interners</span><span>,</span> <span>x</span><span>),</span>
                    <span>)</span>
                <span>})</span>
                <span>.collect</span><span>(),</span>
            <span>...</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>With this generalization, the size improvement starts to be substantial, about 6 times smaller than the previous step and 12 times smaller than the baseline input files (commit <a href="https://github.com/gendx/rust-interning/commit/f532ef9aa822a77de496b8fae44b1642db47bef8">f532ef9</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 86349519 bytes (relative size = 7.59%)
[67.50%] Interners: 58288479 bytes
- [5.13%] String interner: 56374 objects | 4433343 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [1.81%] Disruption interner: 7550 objects | 1565896 bytes (207.40 bytes/object) | 625760 references (82.88 refs/object)
  - [0.28%] ApplicationPeriod interner: 5883 objects | 245688 bytes (41.76 bytes/object) | 631593 references (107.36 refs/object)
- [53.61%] Line interner: 97090 objects | 46289464 bytes (476.77 bytes/object) | 930026 references (9.58 refs/object)
  - [6.66%] ImpactedObject interner: 56110 objects | 5754088 bytes (102.55 bytes/object) | 3183332 references (56.73 refs/object)
</code></pre></div>

<h3 id="dropping-the-reference-28"><span><a href="#dropping-the-reference-28"></a></span>Dropping the reference (2.8%)</h3>

<p>One thing you may have noticed is how the reference to an <code>Interner</code> proliferates well beyond the <code>Interned</code> struct.
It forces us (1) to add a lifetime <code>'a</code> everywhere and (2) to use interior mutability which causes the <code>Interner</code>/<code>InternerImpl</code> split.</p>

<div><pre><code><span>struct</span> <span>Interned</span><span>&lt;</span><span>'a</span><span>,</span> <span>T</span><span>&gt;</span> <span>{</span>
    <span>interner</span><span>:</span> <span>&amp;</span><span>'a</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>,</span>
    <span>id</span><span>:</span> <span>usize</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>Crucially, it also means inflated memory usage due to a lot of duplication: a struct like <code>Disruption&lt;'a&gt;</code> contains at least 7 references to the same string interner!
So what if we just got rid of it?</p>

<p>In the current design, the <code>Interned</code> type is <a href="https://stackoverflow.com/questions/5004162/what-does-it-mean-for-a-data-structure-to-be-intrusive"><em>intrusive</em></a> (as it’s aware of the surrounding <code>Interner</code>).
We can instead externalize the interner, and let the caller provide a reference to the interner when needed.</p>

<div><pre><code><span>use</span> <span>std</span><span>::</span><span>marker</span><span>::</span><span>PhantomData</span><span>;</span>

<span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>usize</span><span>,</span>
    <span>// Marker to indicate that an interned object behaves like a function that</span>
    <span>// returns a T (via the lookup method).</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>T</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>// The interner reference is now provided by the caller.</span>
    <span>fn</span> <span>from</span><span>(</span><span>interner</span><span>:</span> <span>&amp;</span><span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>,</span> <span>value</span><span>:</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>interner</span><span>.intern</span><span>(</span><span>value</span><span>)</span>
    <span>}</span>

    <span>// Same here.</span>
    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>interner</span><span>:</span> <span>&amp;</span><span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
        <span>interner</span><span>.lookup</span><span>(</span><span>self</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>One difficulty with this simplified design is how to implement comparison and hashing methods on <code>Interned&lt;T&gt;</code>.
Indeed, these operators have a fixed API given by traits such as <a href="https://doc.rust-lang.org/std/cmp/trait.PartialEq.html"><code>PartialEq</code></a>, so an interner reference cannot be provided as an additional value to the <a href="https://doc.rust-lang.org/std/cmp/trait.PartialEq.html#tymethod.eq"><code>eq()</code></a> function for example.</p>

<p>To solve this issue, we can remark that an interned index fully represents the underlying object (within the realm of an interner): two values will be interned to the same index if and only if they are equal.
So rather than doing a deep (recursive) comparison, we can simply compare the indices, i.e. <code>derive</code> their implementations for <code>Interned</code>.
Likewise for hashing.</p>

<div><pre><code><span>#[derive(Debug,</span> <span>Hash,</span> <span>PartialEq,</span> <span>Eq)]</span>
<span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>
</code></pre></div>

<p>However, the difficulty remains when we want to compare an <code>Interned&lt;T&gt;</code> with a <code>T</code>.
In that case, we really need to look up the underlying value and perform a deep comparison.
For that purpose, I ended up defining a new <code>EqWith</code> trait that allows passing the interner as a helper object for the comparison.</p>

<div><pre><code><span>trait</span> <span>EqWith</span><span>&lt;</span><span>Rhs</span><span>,</span> <span>Helper</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq_with</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>Rhs</span><span>,</span> <span>helper</span><span>:</span> <span>&amp;</span><span>Helper</span><span>)</span> <span>-&gt;</span> <span>bool</span><span>;</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>EqWith</span><span>&lt;</span><span>T</span><span>,</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;&gt;</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq_with</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>T</span><span>,</span> <span>interner</span><span>:</span> <span>&amp;</span><span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.lookup</span><span>(</span><span>interner</span><span>)</span><span>.deref</span><span>()</span> <span>==</span> <span>other</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>We can then compare structs from the source and optimized schemas.</p>

<div><pre><code><span>#[derive(Debug,</span> <span>Hash,</span> <span>PartialEq,</span> <span>Eq)]</span>
<span>struct</span> <span>ApplicationPeriod</span> <span>{</span>
    <span>begin</span><span>:</span> <span>IString</span><span>,</span>
    <span>end</span><span>:</span> <span>IString</span><span>,</span>
<span>}</span>

<span>impl</span> <span>EqWith</span><span>&lt;</span><span>source</span><span>::</span><span>ApplicationPeriod</span><span>,</span> <span>Interners</span><span>&gt;</span> <span>for</span> <span>ApplicationPeriod</span> <span>{</span>
    <span>fn</span> <span>eq_with</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>source</span><span>::</span><span>ApplicationPeriod</span><span>,</span> <span>interners</span><span>:</span> <span>&amp;</span><span>Interners</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.begin</span><span>.eq_with</span><span>(</span><span>&amp;</span><span>other</span><span>.begin</span><span>,</span> <span>&amp;</span><span>interners</span><span>.string</span><span>)</span>
            <span>&amp;&amp;</span> <span>self</span><span>.end</span><span>.eq_with</span><span>(</span><span>&amp;</span><span>other</span><span>.end</span><span>,</span> <span>&amp;</span><span>interners</span><span>.string</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>With that, we’ve halved the size of an <code>Interned&lt;T&gt;</code>, and therefore almost halved the total in-memory size (commit <a href="https://github.com/gendx/rust-interning/commit/59fae78b25aa2cfd013992c68f3692a2336f7f23">59fae78</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 49829943 bytes (relative size = 4.38%)
[68.66%] Interners: 34215191 bytes
- [8.90%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [2.17%] Disruption interner: 7550 objects | 1081928 bytes (143.30 bytes/object) | 625760 references (82.88 refs/object)
  - [0.30%] ApplicationPeriod interner: 5883 objects | 151552 bytes (25.76 bytes/object) | 631593 references (107.36 refs/object)
- [49.71%] Line interner: 97090 objects | 24768600 bytes (255.11 bytes/object) | 930026 references (9.58 refs/object)
  - [7.59%] ImpactedObject interner: 56110 objects | 3779776 bytes (67.36 bytes/object) | 3183332 references (56.73 refs/object)
</code></pre></div>

<p>Can we half it again?</p>

<p>Yes!
The <a href="https://matklad.github.io/2020/03/22/fast-simple-rust-interner.html">original blog post</a> by <em>matklad</em> was using a <code>u32</code> index, rather than <code>usize</code>.
This is indeed a fairly reasonable choice for objects that are supposed to be referenced multiple times.
In my case, the dataset didn’t contain any type with more than a million distinct objects, so there was enough margin.</p>

<div><pre><code><span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>u32</span><span>,</span>  <span>// Now a 32-bit index.</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>T</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>intern</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> <span>value</span><span>:</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
        <span>if</span> <span>let</span> <span>Some</span><span>(</span><span>&amp;</span><span>id</span><span>)</span> <span>=</span> <span>self</span><span>.map</span><span>.get</span><span>(</span><span>&amp;</span><span>value</span><span>)</span> <span>{</span>
            <span>return</span> <span>id</span><span>;</span>
        <span>}</span>

        <span>// Runtime check that the identifier doesn't exceed a u32.</span>
        <span>let</span> <span>id</span> <span>=</span> <span>self</span><span>.vec</span><span>.len</span><span>();</span>
        <span>assert!</span><span>(</span><span>id</span> <span>&lt;=</span> <span>u32</span><span>::</span><span>MAX</span> <span>as</span> <span>usize</span><span>);</span>
        <span>let</span> <span>id</span> <span>=</span> <span>id</span> <span>as</span> <span>u32</span><span>;</span>

        <span>let</span> <span>rc</span><span>:</span> <span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>Rc</span><span>::</span><span>new</span><span>(</span><span>value</span><span>);</span>
        <span>self</span><span>.vec</span><span>.push</span><span>(</span><span>Rc</span><span>::</span><span>clone</span><span>(</span><span>&amp;</span><span>rc</span><span>));</span>
        <span>self</span><span>.map</span><span>.insert</span><span>(</span><span>rc</span><span>,</span> <span>id</span><span>);</span>
        <span>id</span>
    <span>}</span>

    <span>fn</span> <span>lookup</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>id</span><span>:</span> <span>u32</span><span>)</span> <span>-&gt;</span> <span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
        <span>Rc</span><span>::</span><span>clone</span><span>(</span><span>&amp;</span><span>self</span><span>.vec</span><span>[</span><span>id</span> <span>as</span> <span>usize</span><span>])</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>We’re now down below 3% of the original data set size (commit <a href="https://github.com/gendx/rust-interning/commit/84d79e72fd0ec828d975ffeeed1f10d6cd1db906">84d79e7</a>).
Steady progress!</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 31391391 bytes (relative size = 2.76%)
[72.41%] Interners: 22730967 bytes
- [14.12%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [2.48%] Disruption interner: 7550 objects | 779548 bytes (103.25 bytes/object) | 625760 references (82.88 refs/object)
  - [0.33%] ApplicationPeriod interner: 5883 objects | 104488 bytes (17.76 bytes/object) | 631593 references (107.36 refs/object)
- [45.86%] Line interner: 97090 objects | 14396532 bytes (148.28 bytes/object) | 930026 references (9.58 refs/object)
  - [9.61%] ImpactedObject interner: 56110 objects | 3017064 bytes (53.77 bytes/object) | 3183332 references (56.73 refs/object)
</code></pre></div>

<h2 id="tuning-the-schema"><span><a href="#tuning-the-schema"></a></span>Tuning the schema</h2>

<p>Using general interning techniques wasn’t the end of the journey.
Indeed, we can leverage business knowledge about the data to optimize it even more.</p>

<h3 id="sorting-sets-15"><span><a href="#sorting-sets-15"></a></span>Sorting sets (1.5%)</h3>

<p>One common pattern in this data was a field containing a set of objects (themselves interned).
For example, a <code>Line</code> contains a set of impacted objects stored as a <code>Vec&lt;Interned&lt;ImpactedObject&gt;&gt;</code>, and each <code>ImpactedObject</code> contains a set of disruption IDs as a <code>Vec&lt;IString&gt;</code>.</p>

<div><pre><code><span>struct</span> <span>Line</span> <span>{</span>
    <span>id</span><span>:</span> <span>IString</span><span>,</span>
    <span>name</span><span>:</span> <span>IString</span><span>,</span>
    <span>short_name</span><span>:</span> <span>IString</span><span>,</span>
    <span>mode</span><span>:</span> <span>IString</span><span>,</span>
    <span>network_id</span><span>:</span> <span>IString</span><span>,</span>
    <span>impacted_objects</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Interned</span><span>&lt;</span><span>ImpactedObject</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>

<span>struct</span> <span>ImpactedObject</span> <span>{</span>
    <span>typ</span><span>:</span> <span>IString</span><span>,</span>
    <span>id</span><span>:</span> <span>IString</span><span>,</span>
    <span>name</span><span>:</span> <span>IString</span><span>,</span>
    <span>disruption_ids</span><span>:</span> <span>Vec</span><span>&lt;</span><span>IString</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>What’s interesting is that these sets don’t have a particular order, semantically speaking: we only care about which objects are impacted on a given metro line, not whether one impacted object is “before” another (whatever that means).
However, in Rust the <code>Vec</code> collection type is semantically ordered!</p>

<p>This means that two <code>ImpactedObject</code>s with the same <code>typ</code>, <code>id</code> and <code>name</code> fields but <code>disruption_ids</code> equal to <code>[123, 42, 73]</code> in one case and <code>[73, 123, 42]</code> in the other would be considered different in terms of hashing and equality, even though they are semantically the same.</p>

<p>As it turns out, the API was returning such sets in arbitrary order from one call to the next (which I guess makes sense if they internally represent them using hash tables or hash sets).
So one object containing a set of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> items could be represented as up to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">!</mo></mrow><annotation encoding="application/x-tex">N!</annotation></semantics></math></span></span> JSON representations appearing distinct from the perspective of the interner (number of permutations of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> items).</p>

<p>Unfortunately, the problem compounds: a <code>Line</code> contains a set of <code>ImpactedObject</code>s which themselves contain sets of <code>IString</code>.
Consider the following example: each of the two <code>ImpactedObject</code>s has <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo stretchy="false">!</mo><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">3! = 6</annotation></semantics></math></span></span> possible representations and there are <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo stretchy="false">!</mo><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2! = 2</annotation></semantics></math></span></span> possible orderings of these two objects, so this <code>Line</code> has up to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo>⋅</mo><mn>6</mn><mo>⋅</mo><mn>2</mn><mo>=</mo><mn>72</mn></mrow><annotation encoding="application/x-tex">6 \cdot 6 \cdot 2 = 72</annotation></semantics></math></span></span> representations.
And that’s a fairly simple example, in reality the sets could be longer than 3 disruptions.
In practice, the <code>Interner&lt;Line&gt;</code> contained the most number of objects (97090), totaling 14 MB which was 45% of the optimized bytes.</p>

<div><pre><code><span>Line</span> <span>{</span>
    <span>impacted_objects</span><span>:</span> <span>vec!</span><span>[</span>
        <span>ImpactedObject</span> <span>{</span>
            <span>disruption_ids</span><span>:</span> <span>vec!</span><span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>],</span> <span>...</span>
        <span>},</span>
        <span>ImpactedObject</span> <span>{</span>
            <span>disruption_ids</span><span>:</span> <span>vec!</span><span>[</span><span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>],</span> <span>...</span>
        <span>},</span>
    <span>],</span>
    <span>...</span>
<span>}</span>

<span>// Same object serialized differently.</span>
<span>Line</span> <span>{</span>
    <span>impacted_objects</span><span>:</span> <span>vec!</span><span>[</span>
        <span>ImpactedObject</span> <span>{</span>
            <span>disruption_ids</span><span>:</span> <span>vec!</span><span>[</span><span>6</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>],</span> <span>...</span>
        <span>},</span>
        <span>ImpactedObject</span> <span>{</span>
            <span>disruption_ids</span><span>:</span> <span>vec!</span><span>[</span><span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>],</span> <span>...</span>
        <span>},</span>
    <span>],</span>
    <span>...</span>
<span>}</span>
</code></pre></div>

<p>To mitigate this problem, we can canonicalize such sets, the easiest way being to sort them.
However, adding ordering operators (via the <a href="https://doc.rust-lang.org/std/cmp/trait.PartialOrd.html"><code>PartialOrd</code></a> and <a href="https://doc.rust-lang.org/std/cmp/trait.Ord.html"><code>Ord</code></a> traits) for all the structs in the schema would be annoying.
But that’s not required: we only need to order sets of <code>Interned&lt;T&gt;</code>, and we can do that by simply ordering the underlying indices!
Indeed, all we need is a canonical order, we don’t care if this order reflects the semantics of the objects.</p>

<p>Unfortunately, we cannot derive <code>PartialOrd</code> on <code>Interned&lt;T&gt;</code> if the underlying <code>T</code> doesn’t itself implement it.
This is a <a href="https://github.com/rust-lang/rust/issues/26925">known and long-standing limitation</a> of <code>derive</code> (<a href="https://github.com/rust-lang/rust/issues/7671">more than 10 years old</a>!).</p>

<div><pre><code><span>use</span> <span>std</span><span>::</span><span>marker</span><span>::</span><span>PhantomData</span><span>;</span>

<span>#[derive(PartialEq,</span> <span>Eq,</span> <span>PartialOrd,</span> <span>Ord)]</span>
<span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>u32</span><span>,</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>T</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>struct</span> <span>MyArbitraryType</span><span>;</span>

<span>fn</span> <span>foo</span><span>(</span><span>set</span><span>:</span> <span>&amp;</span><span>mut</span> <span>[</span><span>Interned</span><span>&lt;</span><span>MyArbitraryType</span><span>&gt;</span><span>])</span> <span>{</span>
    <span>// error[E0277]: the trait bound `MyArbitraryType: Ord` is not satisfied</span>
    <span>set</span><span>.sort_unstable</span><span>();</span>
<span>}</span>
</code></pre></div>

<div id="derive-error-277"><pre><code>error[E0277]: the trait bound `MyArbitraryType: Ord` is not satisfied
    --&gt; src/lib.rs:13:9
     |
13   |     set.sort_unstable();
     |         ^^^^^^^^^^^^^ the trait `Ord` is not implemented for `MyArbitraryType`
     |
     = help: the trait `Ord` is implemented for `Interned&lt;T&gt;`
note: required for `Interned&lt;MyArbitraryType&gt;` to implement `Ord`
    --&gt; src/lib.rs:3:37
     |
3    | #[derive(PartialEq, Eq, PartialOrd, Ord)]
     |                                     ^^^ unsatisfied trait bound introduced in this `derive` macro
note: required by a bound in `core::slice::&lt;impl [T]&gt;::sort_unstable`
    --&gt; /playground/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/slice/mod.rs:2932:12
     |
2930 |     pub fn sort_unstable(&amp;mut self)
     |            ------------- required by a bound in this associated function
2931 |     where
2932 |         T: Ord,
     |            ^^^ required by this bound in `core::slice::&lt;impl [T]&gt;::sort_unstable`
     = note: this error originates in the derive macro `Ord` (in Nightly builds, run with -Z macro-backtrace for more info)
help: consider annotating `MyArbitraryType` with `#[derive(Ord)]`
     |
9    + #[derive(Ord)]
10   | struct MyArbitraryType;
     |
</code></pre></div>

<p>So we have to implement the comparison traits manually on <code>Interned&lt;T&gt;</code>, which isn’t <em>that</em> bad.
Note that if we implement <code>PartialEq</code> manually, we can <code>derive(Hash)</code> it but a <a href="https://rust-lang.github.io/rust-clippy/master/index.html#derived_hash_with_manual_eq">Clippy lint</a> will warn against that.</p>

<div id="manual-comparisons"><pre><code><span>use</span> <span>std</span><span>::</span><span>cmp</span><span>::</span><span>Ordering</span><span>;</span>
<span>use</span> <span>std</span><span>::</span><span>hash</span><span>::{</span><span>Hash</span><span>,</span> <span>Hasher</span><span>};</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>PartialEq</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>Self</span><span>)</span> <span>-&gt;</span> <span>bool</span> <span>{</span>
        <span>self</span><span>.id</span><span>.eq</span><span>(</span><span>&amp;</span><span>other</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Eq</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>PartialOrd</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>partial_cmp</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>Self</span><span>)</span> <span>-&gt;</span> <span>Option</span><span>&lt;</span><span>Ordering</span><span>&gt;</span> <span>{</span>
        <span>Some</span><span>(</span><span>self</span><span>.cmp</span><span>(</span><span>other</span><span>))</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Ord</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>cmp</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>&amp;</span><span>Self</span><span>)</span> <span>-&gt;</span> <span>Ordering</span> <span>{</span>
        <span>self</span><span>.id</span><span>.cmp</span><span>(</span><span>&amp;</span><span>other</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Hash</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>hash</span><span>&lt;</span><span>H</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>state</span><span>:</span> <span>&amp;</span><span>mut</span> <span>H</span><span>)</span>
    <span>where</span>
        <span>H</span><span>:</span> <span>Hasher</span><span>,</span>
    <span>{</span>
        <span>self</span><span>.id</span><span>.hash</span><span>(</span><span>state</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>I then chose to create an <code>InternedSet&lt;T&gt;</code> abstraction, that will sort the items in canonical order.
Note the use of the <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.sort_unstable"><code>sort_unstable()</code></a> function, which is more efficient than the generic <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.sort"><code>sort()</code></a>.
Also note that we store the set as a boxed slice <code>Box&lt;[_]&gt;</code> instead of a <code>Vec&lt;_&gt;</code>, which is more compact for immutable sequences as it doesn’t require storing a capacity field to potentially grow the vector.</p>

<div><pre><code><span>#[derive(Debug,</span> <span>Hash,</span> <span>PartialEq,</span> <span>Eq)]</span>
<span>struct</span> <span>InternedSet</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>set</span><span>:</span> <span>Box</span><span>&lt;</span><span>[</span><span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span><span>]</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>InternedSet</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>new</span><span>(</span><span>set</span><span>:</span> <span>impl</span> <span>IntoIterator</span><span>&lt;</span><span>Item</span> <span>=</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;&gt;</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>let</span> <span>mut</span> <span>set</span><span>:</span> <span>Box</span><span>&lt;</span><span>[</span><span>_</span><span>]</span><span>&gt;</span> <span>=</span> <span>set</span><span>.into_iter</span><span>()</span><span>.collect</span><span>();</span>
        <span>set</span><span>.sort_unstable</span><span>();</span>
        <span>Self</span> <span>{</span> <span>set</span> <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>We can then integrate it into the schema as follows.</p>

<div><pre><code><span>struct</span> <span>ImpactedObject</span> <span>{</span>
    <span>typ</span><span>:</span> <span>IString</span><span>,</span>
    <span>id</span><span>:</span> <span>IString</span><span>,</span>
    <span>name</span><span>:</span> <span>IString</span><span>,</span>
    <span>disruption_ids</span><span>:</span> <span>InternedSet</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span> <span>ImpactedObject</span> <span>{</span>
    <span>fn</span> <span>from</span><span>(</span><span>interners</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Interners</span><span>,</span> <span>source</span><span>:</span> <span>source</span><span>::</span><span>ImpactedObject</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>Self</span> <span>{</span>
            <span>typ</span><span>:</span> <span>Interned</span><span>::</span><span>from</span><span>(</span><span>&amp;</span><span>mut</span> <span>interners</span><span>.string</span><span>,</span> <span>source</span><span>.typ</span><span>),</span>
            <span>id</span><span>:</span> <span>Interned</span><span>::</span><span>from</span><span>(</span><span>&amp;</span><span>mut</span> <span>interners</span><span>.string</span><span>,</span> <span>source</span><span>.id</span><span>),</span>
            <span>name</span><span>:</span> <span>Interned</span><span>::</span><span>from</span><span>(</span><span>&amp;</span><span>mut</span> <span>interners</span><span>.string</span><span>,</span> <span>source</span><span>.name</span><span>),</span>
            <span>disruption_ids</span><span>:</span> <span>InternedSet</span><span>::</span><span>new</span><span>(</span>
                <span>source</span>
                    <span>.disruption_ids</span>
                    <span>.into_iter</span><span>()</span>
                    <span>.map</span><span>(|</span><span>x</span><span>|</span> <span>Interned</span><span>::</span><span>from</span><span>(</span><span>&amp;</span><span>mut</span> <span>interners</span><span>.string</span><span>,</span> <span>x</span><span>)),</span>
            <span>),</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>This change divided the number of different <code>Line</code> objects by 14, and the total optimized size by two once again (commit <a href="https://github.com/gendx/rust-interning/commit/bdb00b523433db5f1f4a3150507f3b05eef76de5">bdb00b5</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 16578863 bytes (relative size = 1.46%)
[50.70%] Interners: 8405895 bytes
- [26.74%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [3.97%] Disruption interner: 7550 objects | 658748 bytes (87.25 bytes/object) | 625760 references (82.88 refs/object)
  - [0.63%] ApplicationPeriod interner: 5883 objects | 104488 bytes (17.76 bytes/object) | 631593 references (107.36 refs/object)
- [4.06%] Line interner: 6880 objects | 672568 bytes (97.76 bytes/object) | 930026 references (135.18 refs/object)
  - [15.30%] ImpactedObject interner: 55373 objects | 2536756 bytes (45.81 bytes/object) | 3183332 references (57.49 refs/object)
</code></pre></div>

<h3 id="using-enums-14"><span><a href="#using-enums-14"></a></span>Using enums (1.4%)</h3>

<p>At this point, the <code>Data</code> root structs used half of the optimized size.
You might have noticed that it contained only optional fields.</p>

<div><pre><code><span>struct</span> <span>Data</span> <span>{</span>
    <span>status_code</span><span>:</span> <span>Option</span><span>&lt;</span><span>i32</span><span>&gt;</span><span>,</span>
    <span>error</span><span>:</span> <span>Option</span><span>&lt;</span><span>IString</span><span>&gt;</span><span>,</span>
    <span>message</span><span>:</span> <span>Option</span><span>&lt;</span><span>IString</span><span>&gt;</span><span>,</span>
    <span>disruptions</span><span>:</span> <span>Option</span><span>&lt;</span><span>InternedSet</span><span>&lt;</span><span>Disruption</span><span>&gt;&gt;</span><span>,</span>
    <span>lines</span><span>:</span> <span>Option</span><span>&lt;</span><span>InternedSet</span><span>&lt;</span><span>Line</span><span>&gt;&gt;</span><span>,</span>
    <span>last_updated_date</span><span>:</span> <span>Option</span><span>&lt;</span><span>IString</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>However, in practice, the fields that are set come together: either the object contains an error, with <code>status_code</code>, <code>error</code> and <code>message</code> fields, or it contains useful data with <code>disruptions</code>, <code>lines</code> and <code>last_updated_date</code> fields.
So a better representation is to use an enumeration with two variants.</p>

<div><pre><code><span>enum</span> <span>Data</span> <span>{</span>
    <span>Success</span> <span>{</span>
        <span>disruptions</span><span>:</span> <span>InternedSet</span><span>&lt;</span><span>Disruption</span><span>&gt;</span><span>,</span>
        <span>lines</span><span>:</span> <span>InternedSet</span><span>&lt;</span><span>Line</span><span>&gt;</span><span>,</span>
        <span>last_updated_date</span><span>:</span> <span>IString</span><span>,</span>
    <span>},</span>
    <span>Error</span> <span>{</span>
        <span>status_code</span><span>:</span> <span>i32</span><span>,</span>
        <span>error</span><span>:</span> <span>IString</span><span>,</span>
        <span>message</span><span>:</span> <span>IString</span><span>,</span>
    <span>},</span>
<span>}</span>
</code></pre></div>

<p>This separation brings two benefits: the schema is more sound semantically and takes less space in memory.
Indeed, an <code>Interned&lt;T&gt;</code> uses 4 bytes (a <code>u32</code> index) but an <code>Option&lt;Interned&lt;T&gt;&gt;</code> uses 8 bytes: 1 bit for the option state and the rest to align to a multiple of 4 bytes.</p>

<p>The improvement was more modest this time (commit <a href="https://github.com/gendx/rust-interning/commit/c8ac2611348f271c8deffeb26b11aee022387baa">c8ac261</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 15847679 bytes (relative size = 1.39%)
[53.04%] Interners: 8405895 bytes
- [27.97%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [4.16%] Disruption interner: 7550 objects | 658748 bytes (87.25 bytes/object) | 625760 references (82.88 refs/object)
  - [0.66%] ApplicationPeriod interner: 5883 objects | 104488 bytes (17.76 bytes/object) | 631593 references (107.36 refs/object)
- [4.24%] Line interner: 6880 objects | 672568 bytes (97.76 bytes/object) | 930026 references (135.18 refs/object)
  - [16.01%] ImpactedObject interner: 55373 objects | 2536756 bytes (45.81 bytes/object) | 3183332 references (57.49 refs/object)
</code></pre></div>

<h3 id="splitting-structs-082"><span><a href="#splitting-structs-082"></a></span>Splitting structs (0.82%)</h3>

<p>Another thing you may have noticed about the <code>ImpactedObject</code> example is that it contains both fields that are constant (type, identifier, name) and fields that change over time (list of disruptions).
This means that each time a disruption is added or removed from the list, a new <code>ImpactedObject</code> is created and added to the interner, even if the “header” fields that define the object haven’t changed.
A more optimized approach is to extract the fixed fields into a separate object, and to add a new interner for it.</p>

<div><pre><code><span>struct</span> <span>ImpactedObject</span> <span>{</span>
    <span>// Fixed part of an object.</span>
    <span>object</span><span>:</span> <span>Interned</span><span>&lt;</span><span>Object</span><span>&gt;</span><span>,</span>
    <span>disruption_ids</span><span>:</span> <span>InternedSet</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>// New struct grouping the fixed fields of an object.</span>
<span>struct</span> <span>Object</span> <span>{</span>
    <span>typ</span><span>:</span> <span>IString</span><span>,</span>
    <span>id</span><span>:</span> <span>IString</span><span>,</span>
    <span>name</span><span>:</span> <span>IString</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>Note that this new design doesn’t change the total number of <code>ImpactedObject</code>s (55373), but makes each one smaller: one <code>u32</code> index for an <code>Interned&lt;Object&gt;</code> instead of three <code>IString</code> indices.
There are however much fewer <code>Object</code>s (1679) because each one is referenced by many <code>ImpactedObject</code>s (commit <a href="https://github.com/gendx/rust-interning/commit/8914a64497362cab7847796d60a27aeb117134b5">8914a64</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 15330607 bytes (relative size = 1.35%)
[51.46%] Interners: 7888823 bytes
- [28.92%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [4.30%] Disruption interner: 7550 objects | 658748 bytes (87.25 bytes/object) | 625760 references (82.88 refs/object)
  - [0.68%] ApplicationPeriod interner: 5883 objects | 104488 bytes (17.76 bytes/object) | 631593 references (107.36 refs/object)
- [3.67%] Line interner: 6880 objects | 562488 bytes (81.76 bytes/object) | 930026 references (135.18 refs/object)
  - [0.01%] LineHeader interner: 45 objects | 1428 bytes (31.73 bytes/object) | 930026 references (20667.24 refs/object)
  - [13.66%] ImpactedObject interner: 55373 objects | 2093772 bytes (37.81 bytes/object) | 3183332 references (57.49 refs/object)
    - [0.23%] Object interner: 1679 objects | 34564 bytes (20.59 bytes/object) | 3183332 references (1895.97 refs/object)
</code></pre></div>

<p>The improvement was small this time, but if we look at the second half of an <code>ImpactedObject</code> we can observe that several objects may be affected by the exact same set of disruptions, for example because they represent stations on the same line.
By adding another layer of indirection and interning the interned set, we can again reduce redundancy.
In practice there were only 9127 distinct sets of disruption IDs (commit <a href="https://github.com/gendx/rust-interning/commit/fcff0d5f1e9bc1089382170f5ed7f097190c48de">fcff0d5</a>).</p>

<div><pre><code><span>struct</span> <span>ImpactedObject</span> <span>{</span>
    <span>object</span><span>:</span> <span>Interned</span><span>&lt;</span><span>Object</span><span>&gt;</span><span>,</span>
    <span>// Also intern the set.</span>
    <span>disruption_ids</span><span>:</span> <span>Interned</span><span>&lt;</span><span>InternedSet</span><span>&lt;</span><span>String</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>This reduced the optimized size by more than a third.</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1531039733 bytes in memory (relative size = 134.63%)
Optimized to 9364491 bytes (relative size = 0.82%)
[94.79%] Interners: 8877035 bytes
- [47.34%] String interner: 56374 objects | 4433335 bytes (78.64 bytes/object) | 23964083 references (425.09 refs/object)
- [3.50%] InternedSet&lt;String&gt; interner: 9127 objects | 327620 bytes (35.90 bytes/object) | 3183332 references (348.78 refs/object)
- [7.90%] InternedSet&lt;Disruption&gt; interner: 7066 objects | 739652 bytes (104.68 bytes/object) | 30430 references (4.31 refs/object)
  - [7.03%] Disruption interner: 7550 objects | 658748 bytes (87.25 bytes/object) | 625760 references (82.88 refs/object)
    - [1.12%] ApplicationPeriod interner: 5883 objects | 104488 bytes (17.76 bytes/object) | 631593 references (107.36 refs/object)
- [11.88%] InternedSet&lt;Line&gt; interner: 7183 objects | 1112896 bytes (154.93 bytes/object) | 30430 references (4.24 refs/object)
  - [6.01%] Line interner: 6880 objects | 562488 bytes (81.76 bytes/object) | 930026 references (135.18 refs/object)
    - [0.02%] LineHeader interner: 45 objects | 1428 bytes (31.73 bytes/object) | 930026 references (20667.24 refs/object)
    - [9.63%] ImpactedObject interner: 55373 objects | 901816 bytes (16.29 bytes/object) | 3183332 references (57.49 refs/object)
      - [0.37%] Object interner: 1679 objects | 34564 bytes (20.59 bytes/object) | 3183332 references (1895.97 refs/object)
</code></pre></div>

<h3 id="specializing-types-064"><span><a href="#specializing-types-064"></a></span>Specializing types (0.64%)</h3>

<p>After all these optimizations, strings were using half of the optimized space: each <code>Interner</code> is a table that other objects can reference by index.
In practice, the string interner contained values of very heterogeneous semantics: some strings were long error messages in HTML format, some were simple names of places, some were identifiers such as <a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">UUIDs</a> and others were timestamps in human-readable format.
The latter two are particularly interesting: instead of storing them as plain strings, we could parse them into more compact and semantically rich formats.</p>

<p>For UUIDs, we can simply use the <code>Uuid</code> type from the <a href="https://docs.rs/uuid/"><code>uuid</code></a> crate as a drop-in replacement.
Internally, this type uses only 16 bytes to store each identifier, instead of 36 bytes for strings like <code>67e55044-10b1-426f-9247-bb680e5fe0c8</code> (overhead of hexadecimal encoding + separators).</p>

<p>For timestamps, I opted for the <a href="https://docs.rs/chrono/"><code>chrono</code></a> crate, aiming to store each one in 8 bytes (64-bit number of seconds since the Unix epoch) rather than 15 to 24 bytes for something like <code>20240428T044500</code> or <code>2024-05-01T00:59:25.384Z</code>.
There was a difficulty however: some timestamps were encoded in <a href="https://tools.ietf.org/html/rfc3339">RFC 3339</a> format with millisecond precision in the UTC time zone, others were a naive date + time in seconds implicitly in the Paris time zone.
In that case, I used the <a href="https://docs.rs/chrono-tz/"><code>chrono-tz</code></a> crate to handle the conversion and canonicalize them to UTC.</p>

<p>There is a caveat however when the time zone is implicit: on days when <a href="https://en.wikipedia.org/wiki/Daylight_saving_time">daylight saving</a> starts or end, the times between 2am and 3am may either be ambiguous (we don’t know if we’re in the CET or CEST time zone when the time jumps back from 3am to 2am) or invalid (when the time jumps from 2am to 3am).
That said, the metro network is normally closed at this time, so a small imprecision isn’t too dramatic.
It would be more annoying for the scheduling of night trains for example.</p>

<div><pre><code><span>use</span> <span>chrono</span><span>::</span><span>offset</span><span>::</span><span>LocalResult</span><span>;</span>
<span>use</span> <span>chrono</span><span>::{</span><span>DateTime</span><span>,</span> <span>NaiveDateTime</span><span>};</span>
<span>use</span> <span>chrono_tz</span><span>::</span><span>Europe</span><span>::</span><span>Paris</span><span>;</span>

<span>struct</span> <span>TimestampSecondsParis</span><span>(</span><span>i64</span><span>);</span>

<span>impl</span> <span>TimestampSecondsParis</span> <span>{</span>
    <span>fn</span> <span>from_formatted</span><span>(</span><span>x</span><span>:</span> <span>&amp;</span><span>str</span><span>,</span> <span>format</span><span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>let</span> <span>naive_datetime</span> <span>=</span> <span>NaiveDateTime</span><span>::</span><span>parse_from_str</span><span>(</span><span>x</span><span>,</span> <span>format</span><span>)</span><span>.unwrap_or_else</span><span>(|</span><span>_</span><span>|</span> <span>{</span>
            <span>panic!</span><span>(</span><span>"Failed to parse datetime (custom format {format:?}) from {x}"</span><span>)</span>
        <span>});</span>

        <span>// Handle the specifics of times falling during the daylight saving transition.</span>
        <span>let</span> <span>datetime</span> <span>=</span> <span>match</span> <span>naive_datetime</span><span>.and_local_timezone</span><span>(</span><span>Paris</span><span>)</span> <span>{</span>
            <span>LocalResult</span><span>::</span><span>Single</span><span>(</span><span>x</span><span>)</span> <span>=&gt;</span> <span>x</span><span>,</span>
            <span>LocalResult</span><span>::</span><span>Ambiguous</span><span>(</span><span>earliest</span><span>,</span> <span>latest</span><span>)</span> <span>=&gt;</span> <span>{</span>
                <span>eprintln!</span><span>(</span><span>"Ambiguous mapping of {naive_datetime:?} to the Paris timezone: {earliest:?} or {latest:?}"</span><span>);</span>
                <span>earliest</span>
            <span>}</span>
            <span>LocalResult</span><span>::</span><span>None</span> <span>=&gt;</span> <span>{</span>
                <span>panic!</span><span>(</span><span>"Invalid mapping of {naive_datetime:?} to the Paris timezone"</span><span>)</span>
            <span>}</span>
        <span>};</span>

        <span>TimestampSecondsParis</span><span>(</span><span>datetime</span><span>.timestamp</span><span>())</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>These changes removed a large majority of the interned strings, shaving off another 2 MB (commits <a href="https://github.com/gendx/rust-interning/commit/2031f428a7d91019486d896e25824c17b1f0ce8f">2031f42</a> and <a href="https://github.com/gendx/rust-interning/commit/ad29181ff2ce5c0dff8d6d4e7010244456b60842">ad29181</a>).</p>

<div><pre><code>Parsed 1137178883 bytes from 30466 files (+ 21 failed files)
Expanded to 1322690141 bytes in memory (relative size = 116.31%)
Optimized to 7264110 bytes (relative size = 0.64%)
[89.93%] Interners: 6532926 bytes
- [25.03%] String interner: 8050 objects | 1818466 bytes (225.90 bytes/object) | 17309489 references (2150.25 refs/object)
- [2.25%] Uuid interner: 6617 objects | 163296 bytes (24.68 bytes/object) | 4735218 references (715.61 refs/object)
- [10.18%] InternedSet&lt;Disruption&gt; interner: 7066 objects | 739652 bytes (104.68 bytes/object) | 30430 references (4.31 refs/object)
  - [9.90%] Disruption interner: 7550 objects | 719148 bytes (95.25 bytes/object) | 625760 references (82.88 refs/object)
    - [2.09%] ApplicationPeriod interner: 5883 objects | 151552 bytes (25.76 bytes/object) | 631593 references (107.36 refs/object)
- [15.32%] InternedSet&lt;Line&gt; interner: 7183 objects | 1112896 bytes (154.93 bytes/object) | 30430 references (4.24 refs/object)
  - [7.74%] Line interner: 6880 objects | 562488 bytes (81.76 bytes/object) | 930026 references (135.18 refs/object)
    - [0.02%] LineHeader interner: 45 objects | 1428 bytes (31.73 bytes/object) | 930026 references (20667.24 refs/object)
    - [12.41%] ImpactedObject interner: 55373 objects | 901816 bytes (16.29 bytes/object) | 3183332 references (57.49 refs/object)
      - [0.48%] Object interner: 1679 objects | 34564 bytes (20.59 bytes/object) | 3183332 references (1895.97 refs/object)
      - [4.51%] InternedSet&lt;Uuid&gt; interner: 9127 objects | 327620 bytes (35.90 bytes/object) | 3183332 references (348.78 refs/object)
</code></pre></div>

<h2 id="serialization"><span><a href="#serialization"></a></span>Serialization</h2>

<p>At this point, we basically have an implicit in-memory database.
The next step is to serialize it, to see how interning plays out with persistence.
Serialization is also an important step to validate that the claimed space improvements are real.
Lastly, it’s an opportunity to further compress the data.</p>

<p>In Rust, the de-facto serialization framework is <a href="https://docs.rs/serde/"><code>serde</code></a>, which we’ve already used to <a href="#importing-the-data-135">import the JSON input</a>.
Serde supports many formats out-of-the-box, via extension crates, so I’ve decided to try a few of them.</p>

<ul>
  <li><a href="https://tools.ietf.org/html/rfc8949">CBOR</a>, via <a href="https://docs.rs/ciborium/"><code>ciborium</code></a>,</li>
  <li><a href="https://tools.ietf.org/html/rfc8259">JSON</a>, via <a href="https://docs.rs/serde_json/"><code>serde_json</code></a>,</li>
  <li><a href="https://postcard.jamesmunns.com/">Postcard</a>, via <a href="https://docs.rs/postcard/"><code>postcard</code></a>,</li>
  <li>The <a href="https://docs.rs/bincode/"><code>bincode</code></a> crate (no specification available).</li>
</ul>

<h3 id="writing-custom-deserializers-with-serde-029"><span><a href="#writing-custom-deserializers-with-serde-029"></a></span>Writing custom (de)serializers with Serde (0.29%)</h3>

<p>Usually, all you have to do is to annotate each type in your schema with <code>#[derive(Serialize, Deserialize)]</code> and Serde provides you (de)serialization functions out-of-the-box, which is the whole appeal of the framework.
However, this wouldn’t be very efficient for the <code>Interner&lt;T&gt;</code> table, which contains the same objects twice (in a vector and in a hash map).
Additionally, Serde’s <a href="https://docs.rs/serde/1.0.217/serde/trait.Serialize.html#impl-Serialize-for-Rc%3CT%3E">documentation</a> mentions that even though <a href="https://doc.rust-lang.org/std/rc/struct.Rc.html"><code>Rc</code></a> is (de)serializable if you opt into the <code>rc</code> feature, it copies the underlying content each time it’s referenced, which isn’t optimal nor semantically equivalent.</p>

<div><pre><code><span>struct</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>vec</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Rc</span><span>&lt;</span><span>T</span><span>&gt;&gt;</span><span>,</span>
    <span>map</span><span>:</span> <span>HashMap</span><span>&lt;</span><span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span><span>,</span> <span>u32</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>In this case, we don’t want to serialize both the vector of objects and the lookup table of indices: the whole interner state can be recovered from the vector alone.
Indeed, if we deserialize the objects in the same order, they will get the same indices.
And hopefully, Serde makes it easy to serialize a sequence.</p>

<p>Indeed, the docs for <a href="https://docs.rs/serde/1.0.217/serde/trait.Serialize.html"><code>Serialize</code></a> and <a href="https://docs.rs/serde/1.0.217/serde/trait.Serializer.html"><code>Serializer</code></a> quickly guide us to a solution, namely the <a href="https://docs.rs/serde/1.0.217/serde/trait.Serializer.html#tymethod.serialize_seq"><code>serialize_seq()</code></a> or <a href="https://docs.rs/serde/1.0.217/serde/trait.Serializer.html#method.collect_seq"><code>collect_seq()</code></a> methods (the latter works well with functional-style iterators).</p>

<div><pre><code><span>use</span> <span>serde</span><span>::{</span><span>Serialize</span><span>,</span> <span>Serializer</span><span>};</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Serialize</span><span>&gt;</span> <span>Serialize</span> <span>for</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>serialize</span><span>&lt;</span><span>S</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>serializer</span><span>:</span> <span>S</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>S</span><span>::</span><span>Ok</span><span>,</span> <span>S</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>S</span><span>:</span> <span>Serializer</span><span>,</span>
    <span>{</span>
        <span>// Tell Serde to serialize a sequence of items.</span>
        <span>serializer</span><span>.collect_seq</span><span>(</span><span>self</span><span>.vec</span><span>.iter</span><span>()</span><span>.map</span><span>(|</span><span>rc</span><span>|</span> <span>rc</span><span>.deref</span><span>()))</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>Deserialization is more complex, which isn’t surprising as it needs to handle potential errors in the serialized stream.
Ultimately, you need to check the <a href="https://serde.rs/impl-deserialize.html">guide directly on Serde’s website</a>, as the regular API documentation doesn’t explain the details.</p>

<p>One needs not only to manipulate the <a href="https://docs.rs/serde/1.0.217/serde/trait.Deserialize.html"><code>Deserialize</code></a> and <a href="https://docs.rs/serde/1.0.217/serde/trait.Deserializer.html"><code>Deserializer</code></a> traits, but also write a <a href="https://docs.rs/serde/1.0.217/serde/de/trait.Visitor.html"><code>Visitor</code></a> and (in this case) manipulate a <a href="https://docs.rs/serde/1.0.217/serde/de/trait.SeqAccess.html"><code>SeqAccess</code></a>.
There’s a lot more boilerplate, but it works out.</p>

<div id="deserialize-impl"><pre><code><span>use</span> <span>serde</span><span>::</span><span>de</span><span>::{</span><span>SeqAccess</span><span>,</span> <span>Visitor</span><span>};</span>
<span>use</span> <span>serde</span><span>::{</span><span>Deserialize</span><span>,</span> <span>Deserializer</span><span>};</span>

<span>impl</span><span>&lt;</span><span>'de</span><span>,</span> <span>T</span><span>&gt;</span> <span>Deserialize</span><span>&lt;</span><span>'de</span><span>&gt;</span> <span>for</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span>
<span>where</span>
    <span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span> <span>+</span> <span>Deserialize</span><span>&lt;</span><span>'de</span><span>&gt;</span><span>,</span>
<span>{</span>
    <span>fn</span> <span>deserialize</span><span>&lt;</span><span>D</span><span>&gt;</span><span>(</span><span>deserializer</span><span>:</span> <span>D</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Self</span><span>,</span> <span>D</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>D</span><span>:</span> <span>Deserializer</span><span>&lt;</span><span>'de</span><span>&gt;</span><span>,</span>
    <span>{</span>
        <span>// Tell Serde to deserialize a sequence of items using the given visitor.</span>
        <span>deserializer</span><span>.deserialize_seq</span><span>(</span><span>InternerVisitor</span><span>::</span><span>new</span><span>())</span>
    <span>}</span>
<span>}</span>

<span>// A visitor to create an Interner&lt;T&gt;.</span>
<span>struct</span> <span>InternerVisitor</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>// This visitor is stateless, but Rust requires to use the generic parameter T.</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>InternerVisitor</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>new</span><span>()</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>Self</span> <span>{</span>
            <span>_phantom</span><span>:</span> <span>PhantomData</span><span>,</span>
        <span>}</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'de</span><span>,</span> <span>T</span><span>&gt;</span> <span>Visitor</span><span>&lt;</span><span>'de</span><span>&gt;</span> <span>for</span> <span>InternerVisitor</span><span>&lt;</span><span>T</span><span>&gt;</span>
<span>where</span>
    <span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span> <span>+</span> <span>Deserialize</span><span>&lt;</span><span>'de</span><span>&gt;</span><span>,</span>
<span>{</span>
    <span>// Tell Serde that the output will be an Interner.</span>
    <span>type</span> <span>Value</span> <span>=</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span><span>;</span>

    <span>// Error message to display if the input stream doesn't contain a sequence of Ts.</span>
    <span>fn</span> <span>expecting</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>formatter</span><span>:</span> <span>&amp;</span><span>mut</span> <span>std</span><span>::</span><span>fmt</span><span>::</span><span>Formatter</span><span>)</span> <span>-&gt;</span> <span>std</span><span>::</span><span>fmt</span><span>::</span><span>Result</span> <span>{</span>
        <span>formatter</span><span>.write_str</span><span>(</span><span>"a sequence of values"</span><span>)</span>
    <span>}</span>

    <span>// Callback invoked by Serde with a sequence of items.</span>
    <span>fn</span> <span>visit_seq</span><span>&lt;</span><span>A</span><span>&gt;</span><span>(</span><span>self</span><span>,</span> <span>mut</span> <span>seq</span><span>:</span> <span>A</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Self</span><span>::</span><span>Value</span><span>,</span> <span>A</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>A</span><span>:</span> <span>SeqAccess</span><span>&lt;</span><span>'de</span><span>&gt;</span><span>,</span>
    <span>{</span>
        <span>// Serde may give us a hint about the number of items in the sequence.</span>
        <span>let</span> <span>mut</span> <span>interner</span> <span>=</span> <span>match</span> <span>seq</span><span>.size_hint</span><span>()</span> <span>{</span>
            <span>None</span> <span>=&gt;</span> <span>Interner</span><span>::</span><span>default</span><span>(),</span>
            <span>Some</span><span>(</span><span>size_hint</span><span>)</span> <span>=&gt;</span> <span>Interner</span> <span>{</span>
                <span>vec</span><span>:</span> <span>Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>size_hint</span><span>),</span>
                <span>map</span><span>:</span> <span>HashMap</span><span>::</span><span>with_capacity</span><span>(</span><span>size_hint</span><span>),</span>
                <span>references</span><span>:</span> <span>0</span><span>,</span>
            <span>},</span>
        <span>};</span>

        <span>// We gather the elements in a loop and return an Interner.</span>
        <span>while</span> <span>let</span> <span>Some</span><span>(</span><span>t</span><span>)</span> <span>=</span> <span>seq</span><span>.next_element</span><span>()</span><span>?</span> <span>{</span>
            <span>interner</span><span>.push</span><span>(</span><span>t</span><span>);</span>
        <span>}</span>

        <span>Ok</span><span>(</span><span>interner</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>// For completeness, here is the new push() function, slightly more efficient than intern().</span>
<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Eq</span> <span>+</span> <span>Hash</span><span>&gt;</span> <span>Interner</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>/// Unconditionally push a value, without validating that it's already interned.</span>
    <span>fn</span> <span>push</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> <span>value</span><span>:</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
        <span>let</span> <span>id</span> <span>=</span> <span>self</span><span>.vec</span><span>.len</span><span>();</span>
        <span>assert!</span><span>(</span><span>id</span> <span>&lt;=</span> <span>u32</span><span>::</span><span>MAX</span> <span>as</span> <span>usize</span><span>);</span>
        <span>let</span> <span>id</span> <span>=</span> <span>id</span> <span>as</span> <span>u32</span><span>;</span>

        <span>let</span> <span>rc</span><span>:</span> <span>Rc</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>Rc</span><span>::</span><span>new</span><span>(</span><span>value</span><span>);</span>
        <span>self</span><span>.vec</span><span>.push</span><span>(</span><span>Rc</span><span>::</span><span>clone</span><span>(</span><span>&amp;</span><span>rc</span><span>));</span>
        <span>self</span><span>.map</span><span>.insert</span><span>(</span><span>rc</span><span>,</span> <span>id</span><span>);</span>

        <span>id</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>Here is a comparison of the serialization formats at this point, sorted by encoded size.
Postcard is a clear winner at 3.2 MB, much less than JSON at 65 MB in pretty-printed form or 26 MB in minified form (commit <a href="https://github.com/gendx/rust-interning/commit/b5013e6ceed00263e4744ea4f6a7ce27b1e83b8d">b5013e6</a>).</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Bytes</th>
      <th>Relative size</th>
      <th>Encode time</th>
      <th>Decode time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Postcard</td>
      <td>3275869</td>
      <td>0.29%</td>
      <td>17 ms</td>
      <td>16 ms</td>
    </tr>
    <tr>
      <td>Bincode</td>
      <td>5437330</td>
      <td>0.48%</td>
      <td>16 ms</td>
      <td>24 ms</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>17484567</td>
      <td>1.54%</td>
      <td>56 ms</td>
      <td>128 ms</td>
    </tr>
    <tr>
      <td>JSON</td>
      <td>26485131</td>
      <td>2.33%</td>
      <td>74 ms</td>
      <td>118 ms</td>
    </tr>
    <tr>
      <td>JSON (pretty)</td>
      <td>65026281</td>
      <td>5.72%</td>
      <td>152 ms</td>
      <td>135 ms</td>
    </tr>
  </tbody>
</table>

<p>It’s also interesting to compare the encoding and decoding times, as the more compact formats tend to be faster to process too.
Take them with a grain of salt though, that’s not a proper benchmark but a single data point on my machine, and without using any optimized library like <a href="https://docs.rs/simd-json/"><code>simd_json</code></a>.</p>

<h3 id="compression-and-fighting-the-rust-borrow-checker-linux-pipes-005"><span><a href="#compression-and-fighting-the-rust-borrow-checker-linux-pipes-005"></a></span>Compression and fighting <del>the Rust borrow checker</del> Linux pipes (0.05%)</h3>

<p>Once we have serialized the database into bytes, we can apply general-purpose compression to it.
Indeed, even after interning we expect some amount of redundancy: for example disruption messages follow templates where only some details vary.</p>

<p>I didn’t really want to investigate the various Rust compression libraries, as I thought that spawning a command would be simpler for this experiment.
Indeed, the <a href="https://doc.rust-lang.org/std/process/struct.Command.html"><code>Command</code></a> type in the Rust standard library allows to easily run other programs, passing them arguments and inputs in a safe way, and collecting the output.
In principle, one should configure <a href="https://doc.rust-lang.org/std/process/struct.Stdio.html#method.piped"><code>Stdio::piped()</code></a> to interact with stdin/stdout/stderr of the child process, and we’re good to go… or so I thought.</p>

<div><pre><code><span>// Naive implementation, which doesn't work (inter-process deadlock).</span>
<span>fn</span> <span>gzip_compress</span><span>(</span><span>input</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> <span>std</span><span>::</span><span>error</span><span>::</span><span>Error</span><span>&gt;&gt;</span> <span>{</span>
    <span>let</span> <span>compress</span> <span>=</span> <span>Command</span><span>::</span><span>new</span><span>(</span><span>"gzip"</span><span>)</span>
        <span>.arg</span><span>(</span><span>"-c"</span><span>)</span>  <span>// Compress to stdout.</span>
        <span>.arg</span><span>(</span><span>"-6"</span><span>)</span>  <span>// Compression level.</span>
        <span>.stdin</span><span>(</span><span>Stdio</span><span>::</span><span>piped</span><span>())</span>
        <span>.stdout</span><span>(</span><span>Stdio</span><span>::</span><span>piped</span><span>())</span>
        <span>.stderr</span><span>(</span><span>Stdio</span><span>::</span><span>null</span><span>())</span>
        <span>.spawn</span><span>()</span><span>?</span><span>;</span>

    <span>compress</span><span>.stdin</span><span>.as_mut</span><span>()</span><span>.expect</span><span>(</span><span>"Failed to open stdin"</span><span>)</span><span>.write_all</span><span>(</span><span>input</span><span>)</span><span>?</span><span>;</span>

    <span>let</span> <span>output</span> <span>=</span> <span>compress</span><span>.wait_with_output</span><span>()</span><span>?</span><span>;</span>
    <span>if</span> <span>!</span><span>output</span><span>.status</span><span>.success</span><span>()</span> <span>{</span>
        <span>/* Return an error */</span>
    <span>}</span>
    <span>Ok</span><span>(</span><span>output</span><span>.stdout</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>Indeed, while this naive implementation worked for simple examples with a short input, it seemingly deadlocked once I tried to compress larger inputs!</p>

<p>The answer lies in the <a href="https://www.man7.org/linux/man-pages/man7/pipe.7.html">Linux manual for pipes</a>.
When a parent and child process communicate via stdin/stdout/stderr, this is done via a so-called <em>pipe</em>.
Under the hood, the kernel provides a buffer where one process can write and the other process can read.</p>

<p>However, to avoid exhausting the system resources, this buffer has a limited size.
In particular, <code>/proc/sys/fs/pipe-max-size</code> sets the maximum size (in bytes) of one pipe, which is 1 MiB on my system.
Additionally, <code>/proc/sys/fs/pipe-user-pages-hard</code> and <code>/proc/sys/fs/pipe-user-pages-soft</code> set the maximum number of pages that one user can create in terms of pipes (across all its processes), which is 64 MiB on my system.</p>

<div><pre><code><span>$ </span><span>cat</span> /proc/sys/fs/pipe-max-size
1048576
<span>$ </span><span>cat</span> /proc/sys/fs/pipe-user-pages-hard
0
<span>$ </span><span>cat</span> /proc/sys/fs/pipe-user-pages-soft
16384
</code></pre></div>

<p>Back to invoking <code>gzip</code>: the problem is that if the input is too large, the <code>write_all(input)</code> call will be blocked until the child (gzip) process reads from the pipe.
The <code>gzip</code> process will indeed start reading from it, but will also want to write compressed output as soon as possible to its standard output (to limit its own memory usage).
However, with the naive implementation, my parent process isn’t reading this output yet until it’s done writing all the input.
So the compressed output will fill its pipe too.</p>

<p><a href="https://gendignoux.com/blog/images/rust-interning-2000x/pipes-writing.rcvX_WUfg9Zh.svg"><img src="https://gendignoux.com/blog/images/rust-interning-2000x/pipes-writing.rcvX_WUfg9Zh.svg" width="840" height="275" loading="lazy" alt="Diagram of a compression process' stdin and stdout pipes filling up"></a>
Linux pipes around a compression process.</p>

<p>At some point, we’ll end up in a situation where my parent process is blocked on writing more input to the stdin pipe (waiting for gzip to read it) while the child gzip process is blocked on writing more compressed output to the stdout pipe (waiting for the parent process to read it).
In other words: an inter-process deadlock!</p>

<p><a href="https://gendignoux.com/blog/images/rust-interning-2000x/pipes-full.WTvcH5bPcbn8.svg"><img src="https://gendignoux.com/blog/images/rust-interning-2000x/pipes-full.WTvcH5bPcbn8.svg" width="840" height="275" loading="lazy" alt="Diagram with full stdin and stdout pipes"></a>
Full pipes causing a deadlock.</p>

<p>Be really careful about this limitation: even if you think that you won’t need to write more than 1 MiB (or whatever <code>/proc/sys/fs/pipe-max-size</code> says on your system), the total per-user limit may restrict pipes to only one page (once <code>/proc/sys/fs/pipe-user-pages-soft</code> is reached) which is usually 4 KiB, i.e. much lower than the 1 MiB default.</p>

<p>To remediate that, we need to make sure to both write uncompressed input and read compressed output at the same time, i.e. use threads.
We can still use the convenient functions <a href="https://doc.rust-lang.org/std/io/trait.Write.html#method.write_all"><code>write_all()</code></a> and <a href="https://doc.rust-lang.org/std/io/trait.Read.html#method.read_to_end"><code>read_to_end()</code></a> as long as they happen in parallel threads.</p>

<p>I ended up creating a utility function to automate this pattern of writing all of an input slice and reading all of the output.
Note the use of <a href="https://doc.rust-lang.org/stable/std/thread/fn.scope.html"><code>std::thread::scope()</code></a>, which allows to capture an input slice with non-static lifetime.</p>

<div><pre><code><span>#![feature(exit_status_error)]</span>

<span>fn</span> <span>io_command</span><span>(</span><span>mut</span> <span>child</span><span>:</span> <span>Child</span><span>,</span> <span>input</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> <span>std</span><span>::</span><span>error</span><span>::</span><span>Error</span><span>&gt;&gt;</span> <span>{</span>
    <span>std</span><span>::</span><span>thread</span><span>::</span><span>scope</span><span>(|</span><span>s</span><span>|</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> <span>std</span><span>::</span><span>error</span><span>::</span><span>Error</span><span>&gt;&gt;</span> <span>{</span>
        <span>let</span> <span>mut</span> <span>stdin</span> <span>=</span> <span>child</span><span>.stdin</span><span>.take</span><span>()</span><span>.expect</span><span>(</span><span>"Failed to open stdin"</span><span>);</span>
        <span>let</span> <span>mut</span> <span>stdout</span> <span>=</span> <span>child</span><span>.stdout</span><span>.take</span><span>()</span><span>.expect</span><span>(</span><span>"Failed to open stdout"</span><span>);</span>

        <span>let</span> <span>input_thread</span> <span>=</span> <span>s</span><span>.spawn</span><span>(</span><span>move</span> <span>||</span> <span>-&gt;</span> <span>std</span><span>::</span><span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
            <span>eprintln!</span><span>(</span><span>"Writing {} bytes..."</span><span>,</span> <span>input</span><span>.len</span><span>());</span>
            <span>stdin</span><span>.write_all</span><span>(</span><span>input</span><span>)</span><span>?</span><span>;</span>
            <span>drop</span><span>(</span><span>stdin</span><span>);</span>
            <span>Ok</span><span>(())</span>
        <span>});</span>

        <span>let</span> <span>output_thread</span> <span>=</span> <span>s</span><span>.spawn</span><span>(</span><span>move</span> <span>||</span> <span>-&gt;</span> <span>std</span><span>::</span><span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;&gt;</span> <span>{</span>
            <span>eprintln!</span><span>(</span><span>"Reading bytes..."</span><span>);</span>
            <span>let</span> <span>mut</span> <span>output</span> <span>=</span> <span>Vec</span><span>::</span><span>new</span><span>();</span>
            <span>stdout</span><span>.read_to_end</span><span>(</span><span>&amp;</span><span>mut</span> <span>output</span><span>)</span><span>?</span><span>;</span>
            <span>Ok</span><span>(</span><span>output</span><span>)</span>
        <span>});</span>

        <span>eprintln!</span><span>(</span><span>"Waiting for child..."</span><span>);</span>
        <span>child</span><span>.wait</span><span>()</span><span>?</span><span>.exit_ok</span><span>()</span><span>?</span><span>;</span>

        <span>input_thread</span><span>.join</span><span>()</span><span>.expect</span><span>(</span><span>"Failed to join input thread"</span><span>)</span><span>?</span><span>;</span>
        <span>let</span> <span>output</span> <span>=</span> <span>output_thread</span>
            <span>.join</span><span>()</span>
            <span>.expect</span><span>(</span><span>"Failed to join output thread"</span><span>)</span><span>?</span><span>;</span>
        <span>Ok</span><span>(</span><span>output</span><span>)</span>
    <span>})</span>
<span>}</span>
</code></pre></div>

<p>Back to the original topic, I’ve benchmarked a few common compression algorithms.
What they show is that the size difference between formats tends to get smaller after compression.
At this point, we’ve reached the 2000x size reduction mark on the most compact formats: from 1.1 GB down to 530 KB for the May 2024 data (commit <a href="https://github.com/gendx/rust-interning/commit/b7281f5da83da3235ec88348a409aefe17c15ac9">b7281f5</a>).</p>

<p>
<a href="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5.AVEOpxGMp-tq.svg">
<picture>
  <source srcset="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5-dark.gDT6pHGl_Zc1.svg" media="(prefers-color-scheme: dark)" width="800" height="600">
  <img src="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5.AVEOpxGMp-tq.svg" width="800" height="600" loading="lazy" alt="Histogram of encoded sizes">
</picture>
</a>
</p>

<table id="table-sizes-b7281f5">
<thead>
<tr>
<th>Format</th>
<th colspan="2">Serialization</th>
<th colspan="2">gzip -6</th>
<th colspan="2">brotli -6</th>
<th colspan="2">xz -6</th>
</tr>
</thead>
<tbody>
<tr>
<td>Postcard</td><td>3275869</td><td>0.29%</td><td>861917</td><td>0.08%</td><td>721120</td><td>0.06%</td><td>539200</td><td>0.05%</td>
</tr>
<tr>
<td>Bincode</td><td>5437330</td><td>0.48%</td><td>893271</td><td>0.08%</td><td>700194</td><td>0.06%</td><td>529996</td><td>0.05%</td>
</tr>
<tr>
<td>CBOR</td><td>17484567</td><td>1.54%</td><td>1187575</td><td>0.10%</td><td>826739</td><td>0.07%</td><td>615124</td><td>0.05%</td>
</tr>
<tr>
<td>JSON</td><td>26485131</td><td>2.33%</td><td>1253219</td><td>0.11%</td><td>916683</td><td>0.08%</td><td>736036</td><td>0.06%</td>
</tr>
<tr>
<td>JSON (pretty)</td><td>65026281</td><td>5.72%</td><td>1563418</td><td>0.14%</td><td>1006035</td><td>0.09%</td><td>958624</td><td>0.08%</td>
</tr>
</tbody>
</table>

<p>However, similar sizes after compression doesn’t mean you should use JSON and forget about it: (de)serialization and (de)compression speed are faster with the more optimized formats like Postcard!</p>

<p>
<a href="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-times-b7281f5.ghE9gu7LV6EA.svg">
<picture>
  <source srcset="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-times-b7281f5-dark.XWoRDMwNKkia.svg" media="(prefers-color-scheme: dark)" width="800" height="600">
  <img src="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-times-b7281f5.ghE9gu7LV6EA.svg" width="800" height="600" loading="lazy" alt="Histogram of encoding and decoding times">
</picture>
</a>
</p>

<table id="table-times-b7281f5">
<thead>
<tr>
<th rowspan="2">Format</th>
<th colspan="2">Serialization</th>
<th colspan="2">gzip -6</th>
<th colspan="2">brotli -6</th>
<th colspan="2">xz -6</th>
</tr>
<tr>
<th>encode</th>
<th>decode</th>
<th>encode</th>
<th>decode</th>
<th>encode</th>
<th>decode</th>
<th>encode</th>
<th>decode</th>
</tr>
</thead>
<tbody>
<tr>
<td>Postcard</td><td>22 ms</td><td>27 ms</td><td>88 ms</td><td>21 ms</td><td>144 ms</td><td>18 ms</td><td>583 ms</td><td>37 ms</td>
</tr>
<tr>
<td>Bincode</td><td>9 ms</td><td>24 ms</td><td>166 ms</td><td>28 ms</td><td>191 ms</td><td>19 ms</td><td>1111 ms</td><td>39 ms</td>
</tr>
<tr>
<td>CBOR</td><td>72 ms</td><td>131 ms</td><td>189 ms</td><td>63 ms</td><td>230 ms</td><td>36 ms</td><td>3255 ms</td><td>51 ms</td>
</tr>
<tr>
<td>JSON</td><td>72 ms</td><td>100 ms</td><td>233 ms</td><td>96 ms</td><td>449 ms</td><td>44 ms</td><td>3314 ms</td><td>60 ms</td>
</tr>
<tr>
<td>JSON (pretty)</td><td>137 ms</td><td>129 ms</td><td>376 ms</td><td>180 ms</td><td>441 ms</td><td>74 ms</td><td>2934 ms</td><td>81 ms</td>
</tr>
</tbody>
</table>

<p>Another aspect that was out of scope for my experiment is the impact on code size: I expect the simpler formats to also compile to smaller code.
Be mindful of it especially if you’re deploying code for embedded systems or to <a href="https://webassembly.org/">WebAssembly</a> for a website.</p>

<h3 id="tuple-encoding"><span><a href="#tuple-encoding"></a></span>Tuple encoding</h3>

<p>Upon inspecting the serialized outputs more closely, I noticed that a lot of <code>_phantom</code> strings were present in the JSON or CBOR outputs.</p>

<div><pre><code><span>{</span><span>"id"</span><span>:</span><span> </span><span>11</span><span>,</span><span> </span><span>"_phantom"</span><span>:</span><span> </span><span>null</span><span>}</span><span>
</span></code></pre></div>

<p>Something like <code>Interned&lt;T&gt;</code> which I would expect to be serialized as a simple integer was taking much more space.</p>

<div><pre><code><span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>u32</span><span>,</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>T</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>Indeed, by default Serde serializes each struct as a map, which causes two problems:</p>
<ol>
  <li>field names are serialized as keys in a map,</li>
  <li>zero-sized types like <code>PhantomData</code> are serialized even though they don’t contain any information and should be trivial to re-create.</li>
</ol>

<p>This default might be useful if your types evolve over time (fields added, removed or renamed) and you want to retain some level of “out-of-the-box” compatibility, but that’s a design choice<sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup>.
In my case however, the <code>Interned&lt;T&gt;</code> abstraction was so ubiquitous that it meant a lot of redundancy.</p>

<p>After digging into it, I found <a href="https://github.com/serde-rs/serde/issues/297">these</a> <a href="https://github.com/serde-rs/serde/issues/1648">issues</a> as well as <a href="https://github.com/dtolnay/request-for-implementation/issues/3">this discussion</a> by the maintainer of Serde, which boiled down to serializing a struct as a tuple rather than as a map.
Given that tuples don’t have field names, that should address the problem.
This feature request never got implemented in the <code>serde</code> crate itself, but a separate <a href="https://docs.rs/serde_tuple/"><code>serde_tuple</code></a> crate provides the <code>Serialize_tuple</code> and <code>Deserialize_tuple</code> derive macros.</p>

<div><pre><code><span>use</span> <span>serde_tuple</span><span>::{</span><span>Deserialize_tuple</span><span>,</span> <span>Serialize_tuple</span><span>};</span>

<span>#[derive(Serialize_tuple,</span> <span>Deserialize_tuple)]</span>
<span>struct</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>id</span><span>:</span> <span>u32</span><span>,</span>
    <span>_phantom</span><span>:</span> <span>PhantomData</span><span>&lt;</span><span>fn</span><span>()</span> <span>-&gt;</span> <span>T</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>This mitigated the first issue (commits <a href="https://github.com/gendx/rust-interning/commit/a9924e982e24805bf7f1cdbe417e4da4e10d4158">a9924e9</a> and <a href="https://github.com/gendx/rust-interning/commit/cec535911616652401202d6d7ff4367cb938123c">cec5359</a>).
However, the <code>PhantomData</code> still appeared in the serialized output as a <code>null</code> value.</p>



<p>Given how ubiquitous the <code>Interned</code> type is, I ended up <a href="#writing-custom-deserializers-with-serde-029">writing a custom serializer</a> once again.</p>

<div><pre><code><span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Serialize</span> <span>for</span> <span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>serialize</span><span>&lt;</span><span>S</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>serializer</span><span>:</span> <span>S</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>S</span><span>::</span><span>Ok</span><span>,</span> <span>S</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>S</span><span>:</span> <span>Serializer</span><span>,</span>
    <span>{</span>
        <span>serializer</span><span>.serialize_u32</span><span>(</span><span>self</span><span>.id</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>For the affected formats (CBOR and JSON), these optimizations reduced the encoded size by 72% to 75%.
The reduction was more modest after compression, ranging from 11% to 33% (commit <a href="https://github.com/gendx/rust-interning/commit/356bc0af128b19365a18145a12f816e90f9686d3">356bc0a</a>).</p>

<p>
<a href="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5-356bc0a.S5UoLPSeolh-.svg">
<picture>
  <source srcset="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5-356bc0a-dark.eoAr23sHfWWK.svg" media="(prefers-color-scheme: dark)" width="800" height="600">
  <img src="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-b7281f5-356bc0a.S5UoLPSeolh-.svg" width="800" height="600" loading="lazy" alt="Histogram of encoded sizes">
</picture>
</a>
</p>

<table id="table-sizes-b7281f5-356bc0a">
  <thead>
    <tr>
      <th>Format</th>
      <th>Serialization</th>
      <th>gzip -6</th>
      <th>brotli -6</th>
      <th>xz -6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Postcard</td>
      <td>3275869</td>
      <td>861917</td>
      <td>721120</td>
      <td>539200</td>
    </tr>
    <tr>
      <td>Bincode</td>
      <td>5437330</td>
      <td>893271</td>
      <td>700194</td>
      <td>529996</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>4475821 (-74%)</td>
      <td>955541 (-20%)</td>
      <td>731878 (-11%)</td>
      <td>535068 (-13%)</td>
    </tr>
    <tr>
      <td>JSON</td>
      <td>6560658 (-75%)</td>
      <td>1005689 (-20%)</td>
      <td>777507 (-15%)</td>
      <td>545832 (-26%)</td>
    </tr>
    <tr>
      <td>JSON (pretty)</td>
      <td>18168838 (-72%)</td>
      <td>1162250 (-26%)</td>
      <td>890102 (-12%)</td>
      <td>639072 (-33%)</td>
    </tr>
  </tbody>
</table>

<p>Note that formats like Postcard and Bincode already perform this optimization by default, leveraging the fact that they are <a href="https://postcard.jamesmunns.com/wire-format#non-self-describing-format">not self-describing</a>.
By using the <code>serde_tuple</code> crate, you drop part of the self-describing guarantees for other formats like JSON too, as the sender and receiver must agree on the schema (e.g. the specific order of fields in each struct) to be able to communicate without data corruption.</p>

<h3 id="optimizing-sets-revisited"><span><a href="#optimizing-sets-revisited"></a></span>Optimizing sets revisited</h3>

<p>I’ve <a href="#sorting-sets-15">previously described</a> how sets of objects could be better unified by sorting them.
The naive (default) approach to serialize them is a list of interned IDs.
However, in practice these IDs are often sequential, because the underlying objects were created around the same time.
For example, all the disruptions that happened on the same day would appear next to each other in the <code>Interned&lt;Disruption&gt;</code> table.</p>

<div><pre><code><span>[</span><span>0</span><span>,</span><span> </span><span>70</span><span>,</span><span> </span><span>72</span><span>,</span><span> </span><span>73</span><span>,</span><span> </span><span>74</span><span>,</span><span> </span><span>75</span><span>,</span><span> </span><span>77</span><span>,</span><span> </span><span>78</span><span>,</span><span> </span><span>79</span><span>,</span><span> </span><span>80</span><span>,</span><span> </span><span>81</span><span>]</span><span>
</span></code></pre></div>

<p>So instead of serializing the IDs directly, we can use <a href="https://en.wikipedia.org/wiki/Delta_encoding">delta encoding</a>: serializing each ID as the difference from the previous ID.</p>

<div><pre><code><span>[</span><span>0</span><span>,</span><span> </span><span>70</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>1</span><span>]</span><span>
</span></code></pre></div>

<p>This is beneficial for two reasons:</p>
<ul>
  <li>common serialization formats use variable-length encoding, meaning that small numbers are encoded in fewer bytes than larger numbers,</li>
  <li>if most of these differences are small numbers, there should be more redundancy that the compression algorithms should be able to exploit.</li>
</ul>

<p>As a <a href="#writing-custom-deserializers-with-serde-029">custom Serde serializer</a>, delta encoding is fairly straightforward to implement within each set.</p>

<div><pre><code><span>struct</span> <span>InternedSet</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>set</span><span>:</span> <span>Box</span><span>&lt;</span><span>[</span><span>Interned</span><span>&lt;</span><span>T</span><span>&gt;</span><span>]</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Serialize</span> <span>for</span> <span>InternedSet</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>serialize</span><span>&lt;</span><span>S</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>serializer</span><span>:</span> <span>S</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>S</span><span>::</span><span>Ok</span><span>,</span> <span>S</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>S</span><span>:</span> <span>Serializer</span><span>,</span>
    <span>{</span>
        <span>let</span> <span>mut</span> <span>prev</span> <span>=</span> <span>0</span><span>;</span>
        <span>serializer</span><span>.collect_seq</span><span>(</span><span>self</span><span>.set</span><span>.iter</span><span>()</span><span>.map</span><span>(|</span><span>x</span><span>|</span> <span>{</span>
            <span>let</span> <span>id</span> <span>=</span> <span>x</span><span>.id</span><span>();</span>
            <span>let</span> <span>diff</span> <span>=</span> <span>id</span> <span>-</span> <span>prev</span><span>;</span>
            <span>prev</span> <span>=</span> <span>id</span><span>;</span>
            <span>diff</span>
        <span>}))</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>While differential encoding didn’t affect the Bincode format (which encodes integers with a fixed size), the serialized size decreased by 9% to 19% for the others.
The size after compression saw a marked decrease (8% to 16%) with gzip and brotli, and a more modest one (4% to 8%) with xz (commit <a href="https://github.com/gendx/rust-interning/commit/4ea388f81630ba8750b258e824a120f37979549c">4ea388f</a>).</p>

<p>
<a href="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-356bc0a-4ea388f.Ek14Qs--wRbq.svg">
<picture>
  <source srcset="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-356bc0a-4ea388f-dark.OS4k2hmGzzHu.svg" media="(prefers-color-scheme: dark)" width="800" height="600">
  <img src="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-356bc0a-4ea388f.Ek14Qs--wRbq.svg" width="800" height="600" loading="lazy" alt="Histogram of encoded sizes">
</picture>
</a>
</p>

<table id="table-sizes-356bc0a-4ea388f">
  <thead>
    <tr>
      <th>Format</th>
      <th>Serialization</th>
      <th>gzip -6</th>
      <th>brotli -6</th>
      <th>xz -6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Postcard</td>
      <td>2966573 (-9%)</td>
      <td>746478 (-13%)</td>
      <td>624801 (-13%)</td>
      <td>499524 (-7%)</td>
    </tr>
    <tr>
      <td>Bincode</td>
      <td>5437330 (=)</td>
      <td>821573 (-8%)</td>
      <td>631655 (-10%)</td>
      <td>506956 (-4%)</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>3688285 (-18%)</td>
      <td>814306 (-15%)</td>
      <td>634705 (-13%)</td>
      <td>493064 (-8%)</td>
    </tr>
    <tr>
      <td>JSON</td>
      <td>5317810 (-19%)</td>
      <td>841046 (-16%)</td>
      <td>708651 (-9%)</td>
      <td>504196 (-7%)</td>
    </tr>
    <tr>
      <td>JSON (pretty)</td>
      <td>15018136 (-17%)</td>
      <td>988869 (-15%)</td>
      <td>820774 (-8%)</td>
      <td>590380 (-7%)</td>
    </tr>
  </tbody>
</table>

<p>In practice, we can go one step further: not only were the deltas often small, but they were also often 1s, with long sequences of consecutive elements.
So I decided to add <a href="https://en.wikipedia.org/wiki/Run-length_encoding">run-length encoding</a> on top.
Note that because the IDs <a href="#sorting-sets-15">have been sorted</a>, the deltas are guaranteed to be non-negative, making a simple dual delta/RLE encoding possible: interpret a negative number <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> as a run of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">-n</annotation></semantics></math></span></span> consecutive elements, and a non-negative number as a delta from the previous element.</p>

<div><pre><code><span>original:</span><span> </span><span>[</span><span>0</span><span>,</span><span> </span><span>70</span><span>,</span><span> </span><span>72</span><span>,</span><span> </span><span>73</span><span>,</span><span> </span><span>74</span><span>,</span><span> </span><span>75</span><span>,</span><span> </span><span>77</span><span>,</span><span> </span><span>78</span><span>,</span><span> </span><span>79</span><span>,</span><span> </span><span>80</span><span>,</span><span> </span><span>81</span><span>]</span><span>
</span><span>optimized:</span><span> </span><span>[</span><span>0</span><span>,</span><span> </span><span>70</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>-3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>-4</span><span>]</span><span>
</span></code></pre></div>

<div><pre><code><span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>Serialize</span> <span>for</span> <span>InternedSet</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>serialize</span><span>&lt;</span><span>S</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>serializer</span><span>:</span> <span>S</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>S</span><span>::</span><span>Ok</span><span>,</span> <span>S</span><span>::</span><span>Error</span><span>&gt;</span>
    <span>where</span>
        <span>S</span><span>:</span> <span>Serializer</span><span>,</span>
    <span>{</span>
        <span>// Combined delta + RLE encoding.</span>
        <span>let</span> <span>mut</span> <span>rle_encoded</span> <span>=</span> <span>Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>self</span><span>.set</span><span>.len</span><span>());</span>
        <span>let</span> <span>mut</span> <span>prev</span><span>:</span> <span>Option</span><span>&lt;</span><span>u32</span><span>&gt;</span> <span>=</span> <span>None</span><span>;</span>
        <span>let</span> <span>mut</span> <span>streak</span><span>:</span> <span>i32</span> <span>=</span> <span>0</span><span>;</span>

        <span>for</span> <span>x</span> <span>in</span> <span>&amp;</span><span>self</span><span>.set</span> <span>{</span>
            <span>let</span> <span>id</span> <span>=</span> <span>x</span><span>.id</span><span>();</span>
            <span>let</span> <span>diff</span> <span>=</span> <span>id</span> <span>-</span> <span>prev</span><span>.unwrap_or</span><span>(</span><span>0</span><span>);</span>
            <span>if</span> <span>prev</span><span>.is_some</span><span>()</span> <span>&amp;&amp;</span> <span>diff</span> <span>==</span> <span>1</span> <span>{</span>
                <span>streak</span> <span>+=</span> <span>1</span><span>;</span>
            <span>}</span> <span>else</span> <span>{</span>
                <span>if</span> <span>streak</span> <span>!=</span> <span>0</span> <span>{</span>
                    <span>rle_encoded</span><span>.push</span><span>(</span><span>-</span><span>streak</span><span>);</span>
                    <span>streak</span> <span>=</span> <span>0</span><span>;</span>
                <span>}</span>
                <span>rle_encoded</span><span>.push</span><span>(</span><span>diff</span> <span>as</span> <span>i32</span><span>);</span>
            <span>}</span>
            <span>prev</span> <span>=</span> <span>Some</span><span>(</span><span>id</span><span>);</span>
        <span>}</span>
        <span>if</span> <span>streak</span> <span>!=</span> <span>0</span> <span>{</span>
            <span>rle_encoded</span><span>.push</span><span>(</span><span>-</span><span>streak</span><span>);</span>
        <span>}</span>

        <span>serializer</span><span>.collect_seq</span><span>(</span><span>rle_encoded</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>This last optimization gave mixed results: while the encoded size decreased by 4% to 11%, the compressed size remained similar – within ±2% (commit <a href="https://github.com/gendx/rust-interning/commit/d09835640cce8f0ee6f9d849cc8f60c7d07bd01e">d098356</a>).</p>

<p>
<a href="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-4ea388f-d098356.uHWcoLxoRSjR.svg">
<picture>
  <source srcset="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-4ea388f-d098356-dark.3rag3jB5bSJc.svg" media="(prefers-color-scheme: dark)" width="800" height="600">
  <img src="https://gendignoux.com/blog/images/rust-interning-2000x/histogram-sizes-4ea388f-d098356.uHWcoLxoRSjR.svg" width="800" height="600" loading="lazy" alt="Histogram of encoded sizes">
</picture>
</a>
</p>

<table id="table-sizes-4ea388f-d098356">
  <thead>
    <tr>
      <th>Format</th>
      <th>Serialization</th>
      <th>gzip -6</th>
      <th>brotli -6</th>
      <th>xz -6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Postcard</td>
      <td>2832574 (-5%)</td>
      <td>751517 (+1%)</td>
      <td>632425 (+1%)</td>
      <td>504388 (+1%)</td>
    </tr>
    <tr>
      <td>Bincode</td>
      <td>4855254 (-11%)</td>
      <td>825177 (+0%)</td>
      <td>643860 (+2%)</td>
      <td>516736 (+2%)</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>3539237 (-4%)</td>
      <td>813895 (-0%)</td>
      <td>637836 (+0%)</td>
      <td>495860 (+1%)</td>
    </tr>
    <tr>
      <td>JSON</td>
      <td>5103171 (-4%)</td>
      <td>846290 (+1%)</td>
      <td>714886 (+1%)</td>
      <td>510584 (+1%)</td>
    </tr>
    <tr>
      <td>JSON (pretty)</td>
      <td>13361128 (-11%)</td>
      <td>983777 (-1%)</td>
      <td>823720 (+0%)</td>
      <td>587044 (-1%)</td>
    </tr>
  </tbody>
</table>

<h2 id="final-result-a-lightweight-append-only-database"><span><a href="#final-result-a-lightweight-append-only-database"></a></span>Final result: a lightweight append-only database</h2>

<p>To reproduce these results, you’ll find my code <a href="https://github.com/gendx/rust-interning">on GitHub</a>.
I’ve used the RATP Status data at <a href="https://github.com/wincelau/ratpstatus/tree/ef028cce567b6ce9183a185e699206e0f483b99d">commit <code>ef028cc</code></a> (files in the <a href="https://github.com/wincelau/ratpstatus/tree/ef028cce567b6ce9183a185e699206e0f483b99d/datas/json"><code>datas/json/</code> folder</a>).</p>

<div><pre><code>git init
git remote add origin https://github.com/wincelau/ratpstatus
git fetch <span>--depth</span> 1 origin ef028cce567b6ce9183a185e699206e0f483b99d
git checkout FETCH_HEAD
</code></pre></div>

<p>Conceptually, interning is a simple technique, yet we’ve seen that it can lead to many choices and optimizations.
It’s commonly applied to strings, but in practice what really made a difference was broadly interning all sorts of data structures!</p>

<p>At the end of this article, we have implicitly built one of the simplest relational database designs (without all the querying/SQL part).
It comes a few limitations, which are fine for a time series:</p>
<ul>
  <li><strong>append-only</strong>: we can’t modify nor delete existing objects,</li>
  <li><strong>in-memory</strong>: the serialized form doesn’t support random access of a value at an arbitrary index, so we need to deserialize the whole database in memory,</li>
  <li><strong>single-writer</strong>: to increment the index when interning a new object without synchronization.</li>
</ul>

<p>Something that should be fairly simple to add is supporting incremental updates: given that the interning tables are append-only with incremental indices, you could easily take a snapshot of the database and later create a diff containing only the new objects added since the snapshot.
Multiple reader nodes could then be implemented on top of that, which would give us a replicated database: the writer would broadcast incremental updates to the readers from time to time.</p>

<p>Now, practically, should you use one of the <a href="https://lib.rs/keywords/interning">countless existing interning crates</a>, or <a href="https://lucumr.pocoo.org/2025/1/24/build-it-yourself/">write it yourself</a>?
The answer is that it depends on what you’re doing!</p>

<p>Do you care about a very optimized in-memory layout for strings?
The naive <code>Rc&lt;String&gt;</code> representation I’ve shown may not be ideal.
Are you fine with a single global interner or do you want to manipulate local interner arenas like I’ve shown in this post?
Do you need to serialize your data, and if so should the serialization be compact?</p>

<p>You might even have come up with the interning pattern yourself without knowing it had a name nor that plenty of libraries implement it: that’s perfectly fine too!
Hopefully this post can help you evaluate the existing libraries and make an informed decision to reinvent the wheel or not.</p>

<hr>



        
        <hr>
        <h2>Comments</h2>
        <p>
            To react to this blog post please check
            the <a href="https://infosec.exchange/@gendx/114099583779462472"> Mastodon thread</a>, the <a href="https://lobste.rs/s/mlseaq/power_interning_making_time_series">Lobste.rs thread</a> and the <a href="https://www.reddit.com/r/rust/comments/1j2nfwt/the_power_of_interning_making_a_time_series/"> Reddit thread</a>.
        </p>
        
        <hr>
        <p>
            <a href="https://gendignoux.com/blog/feed.xml" target="_blank"> RSS</a> |
            <a href="https://infosec.exchange/@gendx" target="_blank"> Mastodon</a> |
            <a href="https://github.com/gendx" target="_blank"> GitHub</a>
        </p>
        <hr>
        <h2>You may also like</h2>
        
    </section>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Agents.json – OpenAPI Specification for LLMs (164 pts)]]></title>
            <link>https://github.com/wild-card-ai/agents-json</link>
            <guid>43243893</guid>
            <pubDate>Mon, 03 Mar 2025 17:01:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wild-card-ai/agents-json">https://github.com/wild-card-ai/agents-json</a>, See on <a href="https://news.ycombinator.com/item?id=43243893">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <source srcset="https://github.com/wild-card-ai/static/logo/agentsjson-white-blackbackground.png">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/logo/agentsjson-white-blackbackground.png"><img alt="Shows a white agents.json Logo with a black background." src="https://github.com/wild-card-ai/agents-json/raw/master/static/logo/agentsjson-white-blackbackground.png" width="full"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"> Translate OpenAPI into LLM Tools</h2><a id="user-content--translate-openapi-into-llm-tools" aria-label="Permalink:  Translate OpenAPI into LLM Tools" href="#-translate-openapi-into-llm-tools"></a></p>
<p dir="auto"><a href="https://github.com/wild-card-ai/agents-json/stargazers"><img src="https://camo.githubusercontent.com/7e2016d6a18e2e7fc1c28582a1d9909033993cacfbbbd7e1d2bffdec18b1ea68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c642d636172642d61692f6167656e74732d6a736f6e3f7374796c653d736f6369616c" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/wild-card-ai/agents-json?style=social"></a>
<a href="https://discord.gg/7VU6HKq7cZ" rel="nofollow"><img src="https://camo.githubusercontent.com/58bba19e5f81bb6af5022e3f53b26aceb835cf91a9582743c006a74e983a8281/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313333343631363530313433363638323430353f7374796c653d666c6174266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465266c6162656c3d646973636f726426636f6c6f723d373238394441266c696e6b3d6874747073253341253246253246646973636f72642e67672532463741503677536b567451" alt="Discord" data-canonical-src="https://img.shields.io/discord/1334616501436682405?style=flat&amp;logo=discord&amp;logoColor=white&amp;label=discord&amp;color=7289DA&amp;link=https%3A%2F%2Fdiscord.gg%2F7AP6wSkVtQ"></a>
<a href="https://docs.wild-card.ai/agents-json" rel="nofollow"><img src="https://camo.githubusercontent.com/7f6d88507e659f89eb75356e49b3e34237a3118de06ec289f5b8614eef0db6ce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d2546302539462539332539352d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue"></a>
<a href="https://x.com/wildcard_ai" rel="nofollow"><img src="https://camo.githubusercontent.com/4574df5e78a4bb291b1dd9018b254e248b9e48d79f27abe7a2ff0247fa0cd9ae/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f77696c64636172645f61693f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/wildcard_ai?style=social"></a></p>
<p dir="auto">The <code>agents.json</code> Specification is an open specification that formally describes contracts for API and agent interactions, built on top of the OpenAPI standard. The current version is <code>0.1.0</code>.</p>
<p dir="auto">Use the Wildcard Bridge Python package to load, parse, and run agents.json.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/quickstart/3easysteps.png"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/quickstart/3easysteps.png" alt="Wildcard Bridge" width="700"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use one of the quickstart notebooks to get started:</h3><a id="user-content-use-one-of-the-quickstart-notebooks-to-get-started" aria-label="Permalink: Use one of the quickstart notebooks to get started:" href="#use-one-of-the-quickstart-notebooks-to-get-started"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Agent</th>
<th>Auth</th>
<th>Notebook</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/wild-card-ai/agents-json/blob/master/examples/resend.ipynb">Resend</a></td>
<td>API Key</td>
<td><code>./examples/resend.ipynb</code></td>
</tr>
<tr>
<td><a href="https://github.com/wild-card-ai/agents-json/blob/master/examples/single.ipynb">Stripe</a></td>
<td>Bearer Token</td>
<td><code>./examples/single.ipynb</code></td>
</tr>
<tr>
<td><a href="https://github.com/wild-card-ai/agents-json/blob/master/examples/rootly.ipynb">Rootly</a></td>
<td>Bearer Token</td>
<td><code>./examples/rootly.ipynb</code></td>
</tr>
<tr>
<td><a href="https://github.com/wild-card-ai/agents-json/blob/master/examples/multiple.ipynb">Twitter + Giphy</a></td>
<td>OAuth 1.0, API Key</td>
<td><code>./examples/multiple.ipynb</code></td>
</tr>
<tr>
<td><a href="https://github.com/wild-card-ai/agents-json/blob/master/examples/multiple-dynamic.ipynb">Resend + Hubspot + Google Sheets</a></td>
<td>OAuth 2.0, API Key</td>
<td><code>./examples/multiple-dynamic.ipynb</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demos</h2><a id="user-content-demos" aria-label="Permalink: Demos" href="#demos"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://demo.wild-card.ai/stripe" rel="nofollow">Stripe Agent</a></td>
<td><a href="https://demo.wild-card.ai/stripe" rel="nofollow"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/demo/stripe-demo.png" alt="Stripe Agent" width="300"></a></td>
</tr>
<tr>
<td><a href="https://demo.wild-card.ai/googlesheets" rel="nofollow">Google Sheets Agent</a></td>
<td><a href="https://demo.wild-card.ai/googlesheets" rel="nofollow"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/demo/googlesheets-demo.png" alt="Google Sheets Agent" width="300"></a></td>
</tr>
<tr>
<td><a href="https://demo.wild-card.ai/resend" rel="nofollow">Resend Agent</a></td>
<td><a href="https://demo.wild-card.ai/resend" rel="nofollow"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/demo/resend-demo.png" alt="Resend Agent" width="300"></a></td>
</tr>
<tr>
<td><a href="https://demo.wild-card.ai/rootly" rel="nofollow">Rootly Agent</a></td>
<td><a href="https://demo.wild-card.ai/rootly" rel="nofollow"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/demo/rootly-demo.png" alt="Rootly Agent" width="300"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">agents.json Specification</h2><a id="user-content-agentsjson-specification" aria-label="Permalink: agents.json Specification" href="#agentsjson-specification"></a></p>
<p dir="auto">The <code>agents.json</code> Specification is an open specification that formally describes contracts for API and agent interactions, built on top of the OpenAPI standard.</p>
<p dir="auto">The current version is <code>0.1.0</code>.</p>
<p dir="auto">Give feedback, share your projects, and get help in our <a href="https://discord.gg/7AP6wSkVt" rel="nofollow">Discord</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Schema</h3><a id="user-content-schema" aria-label="Permalink: Schema" href="#schema"></a></p>
<p dir="auto">The full schema is available <a href="https://docs.wild-card.ai/agentsjson/schema" rel="nofollow">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Motivations</h3><a id="user-content-motivations" aria-label="Permalink: Motivations" href="#motivations"></a></p>
<p dir="auto">Enabling AI agents to interact with APIs is difficult. We faced the same problem as many others building agents: altering APIs to work reliably with LLMs and executing multiple API calls successfully in a row is a trial and error process.</p>
<p dir="auto">APIs are designed for developers and not LLMs. If you're building integrations for AI agents, you need to write boilerplate, experiment with system prompts, optimize tool definitions, and parse responses into vector stores - for each API.</p>
<p dir="auto">For example, the Gmail API has endpoints to search for threads, list the emails in a thread, and reply with an email given base64 RFC 822 content. Instead, LLMs need a clear, top-level directive that can handle all of this with one tool.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/diagram/TraditionalFlowDiagram.png"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/diagram/TraditionalFlowDiagram.png" alt="Traditional API Paradigm" width="full" title="Traditional API Paradigm"></a>
</p>
<p dir="auto"><strong>Why is <code>agents.json</code> built on OpenAPI?</strong> — OpenAPI is the gold standard for describing how API endpoints work and can be executed. Most API providers have OpenAPI specs or have APIs that can be described fully by OpenAPI. These specs alone aren't sufficient for the age of AI agents, but provide great groundwork for API agent communication.</p>
<p dir="auto">So we implemented <code>agents.json</code>. We built this for us and we're excited to share it with you.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/diagram/AirgapFlowDiagram.png"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/diagram/AirgapFlowDiagram.png" alt="LLMs with an Airgap" width="full" title="LLMs with an Airgap"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The <code>agents.json</code> File</h3><a id="user-content-the-agentsjson-file" aria-label="Permalink: The agents.json File" href="#the-agentsjson-file"></a></p>
<p dir="auto"><code>agents.json</code> is a JSON schema of structured contracts designed for AI agents. API providers use their existing OpenAPI spec to construct this file and agents inspect this file to run accurate series of API calls.</p>
<p dir="auto">The <code>agents.json</code> spec contains a set of additions to the OpenAPI spec - optimizing for endpoint discovery and LLM argument generation. These can include updating descriptions and adding examples.</p>
<p dir="auto">Describing endpoints/data models without describing <em><strong>how</strong></em> they interact together is why AI agents struggle to take the right sequence of actions.</p>
<p dir="auto">To solve this, we introduce flows and links. Flows are contracts with a series of 1 or more API calls that describe an outcome. Links describe how two actions are stitched together.</p>
<p dir="auto">We propose the file placed in <code>/.well-known/agents.json</code> so it is easily discoverable by agents accessing web services. For now, we maintain a registry for <a href="https://wild-card.ai/registry" rel="nofollow">available <code>agents.json</code> files</a>.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/diagram/LlmWorksDiagram.png"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/diagram/LlmWorksDiagram.png" alt="LLMs and APIs with Agents.json" width="full" title="LLMs and APIs with Agents.json"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wildcard Bridge</h3><a id="user-content-wildcard-bridge" aria-label="Permalink: Wildcard Bridge" href="#wildcard-bridge"></a></p>
<p dir="auto">Wildcard Bridge enables LLMs to load, parse, and run agents.json. Here's how it works:</p>
<ol dir="auto">
<li>A developer connects their agent with an agents.json file.</li>
<li>The relevant chain(s) are chosen by the agent and arguments populated for a given task.</li>
<li>Bridge runs the chain(s).</li>
</ol>
<p dir="auto">The goal experience is a developer adds an agents.json file in their workflow and the correct set of actions for an integration is executed. Bridge supports adding Basic, ApiKey, and Bearer authentication to requests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Design Tenets</h3><a id="user-content-design-tenets" aria-label="Permalink: Design Tenets" href="#design-tenets"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Build on top of the OpenAPI standard</strong> <br>
Leverage existing standards and infrastructure where possible.</p>
</li>
<li>
<p dir="auto"><strong>Optimize schema for LLMs, not humans</strong> <br>
Design with AI consumption in mind.</p>
</li>
<li>
<p dir="auto"><strong>Enforce Statelessness</strong> <br>
Orchestration is handled by the calling agent.</p>
</li>
<li>
<p dir="auto"><strong>Require minimal changes to existing APIs</strong> <br>
Make adoption as seamless as possible.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Now?</h3><a id="user-content-why-now" aria-label="Permalink: Why Now?" href="#why-now"></a></p>
<p dir="auto">With OpenAI's release of Operator, we've seen a paradigm shift in what AI will automate. Letting AI run free on the internet will ask for both features and guardrails to be built on web experiences for agents. Yet, for the majority of services - this is exactly the functionality APIs already provide - and more.</p>
<p dir="auto">Rather than just optimizing UXs for web agents - enriching APIs will create more scalable, powerful, and safe agents. APIs are supported by backend infrastructure built for scale.</p>
<p dir="auto">There are still open questions and more to be done. Starting the discussion now and building iteratively gives us a place to adapt alongside evolving paradigms in AI agent development.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">FAQs</h3><a id="user-content-faqs" aria-label="Permalink: FAQs" href="#faqs"></a></p>
<details>
<summary>Shouldn't API providers provide their own agent servers or "/agent" endpoints?</summary>
<p dir="auto">We can begin building agents.json files immediately before official adoption by providers. No extra infrastructure changes, servers, or new endpoints. By putting responsibility of execution on the client - the paradigm abides by the same security and orchestration protocol of existing API based applications today. API providers can still choose to maintain official agents.json files.</p>
</details>
<details>
<summary>Why route to an SDK instead of making HTTP requests directly?</summary>
<p dir="auto">Although OpenAPI specs offer great descriptions of how to use APIs, code-gen with tools like OpenAPI generator and Swagger code-gen isn't perfect. Many APIs have edge cases that are accounted for in client SDKs and not in raw HTTP requests. For example, Gmail's RFC2822 format or Twilio's custom TwiML format are better parsed by code rather than generated as input by an LLM. We include copies of OpenAPI specs beside agents.json files in the repo for use.</p>
</details>
<details>
<summary>How is this different than the Model Context Protocol?</summary>
<p dir="auto">While MCP is designed to be stateful—relying on persistent connections between clients and servers for exchanging context and requests—Agents.json is stateless. Here, the agent independently manages all context. You can leverage your existing agent architecture and RAG systems to handle state effectively. Agents.json lets you build with existing pub/sub architectures, server-less environments, and infrastructure APIs already support today. And definitions are strongly typed by OpenAPI specs.</p>
</details>
<details>
<summary>What about llms.txt?</summary>
<p dir="auto"><a href="https://llmstxt.org/" rel="nofollow">llms.txt</a> is a great standard for making website content more readable to LLMs, but it doesn't address the challenges of <strong>taking structured actions</strong>. While llms.txt helps LLMs retrieve and interpret information, agents.json enables them to execute multi-step workflows reliably.</p>
</details>
<details>
<summary>Why use OpenAPI?</summary>
<p dir="auto">OpenAPI is a thoughtful standard that has evolved with the changes of HTTP APIs. It is the gold standard for describing how API endpoints work and can be executed. Most API providers have OpenAPI specs or have APIs that can be described fully by OpenAPI. These specs aren't quite sufficient for the age of agents, but do provide great groundwork for API↔agent communication.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Roadmap</h2><a id="user-content-feature-roadmap" aria-label="Permalink: Feature Roadmap" href="#feature-roadmap"></a></p>
<ul>
<li> OAuth</li>
<li> Memory &amp; context management in links</li>
<li> Transforming fields at runtime</li>
<li> Rate-limits</li>
<li> Parallel Tasking</li>
<li> Conditionals</li>
<li> Loops</li>
<li> Failure Handling</li>
<li> Streaming</li>
<li> Pagination</li>
<li> agents.json Interactive Builder</li>
<li> agents.json Validator</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Licenses</h2><a id="user-content-licenses" aria-label="Permalink: Licenses" href="#licenses"></a></p>
<p dir="auto">The agents.json specification is open source, licensed under the <a href="https://github.com/wild-card-ai/agents-json/blob/master/agents_json/LICENSE">Apache 2.0 License</a>.
The Wildcard Bridge is source-available, licensed under the <a href="https://github.com/wild-card-ai/agents-json/blob/master/LICENSE">Affero GPL v3</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">The agents.json specification needs community input. This GitHub repository will host informal reviews, allowing for version control and public discussion. To discuss, <a href="https://discord.gg/7AP6wSkVtQ" rel="nofollow">join the Discord community</a>. This is an evolving project and can't be done without your feedback.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Team</h2><a id="user-content-team" aria-label="Permalink: Team" href="#team"></a></p>
<p dir="auto">This project is started by <a href="https://wild-card.ai/" rel="nofollow">Wildcard AI</a>. We're a team of 2 founders, <a href="https://x.com/kaushikm_" rel="nofollow">Kaushik</a> and <a href="https://x.com/Life_of_Y_" rel="nofollow">Yagnya</a>, wanting to make agents act predictably and safely.</p>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/wild-card-ai/agents-json/blob/master/static/people/WildcardFoundersYC.jpg"><img src="https://github.com/wild-card-ai/agents-json/raw/master/static/people/WildcardFoundersYC.jpg" alt="Wildcard AI Founders" width="500"></a>
</p>
<br>
<div dir="auto"><p>
Made with ❤️ in San Francisco
</p><p dir="auto"><a href="https://discord.gg/7AP6wSkVtQ" rel="nofollow"><img src="https://camo.githubusercontent.com/58bba19e5f81bb6af5022e3f53b26aceb835cf91a9582743c006a74e983a8281/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313333343631363530313433363638323430353f7374796c653d666c6174266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465266c6162656c3d646973636f726426636f6c6f723d373238394441266c696e6b3d6874747073253341253246253246646973636f72642e67672532463741503677536b567451" alt="Discord" data-canonical-src="https://img.shields.io/discord/1334616501436682405?style=flat&amp;logo=discord&amp;logoColor=white&amp;label=discord&amp;color=7289DA&amp;link=https%3A%2F%2Fdiscord.gg%2F7AP6wSkVtQ"></a>
<a href="https://x.com/wildcard_ai" rel="nofollow"><img src="https://camo.githubusercontent.com/4574df5e78a4bb291b1dd9018b254e248b9e48d79f27abe7a2ff0247fa0cd9ae/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f77696c64636172645f61693f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/wildcard_ai?style=social"></a>
<a href="https://x.com/kaushikm_" rel="nofollow"><img src="https://camo.githubusercontent.com/c68cd88fe3bdb05d428d99c01c637c1154d9f1995f958f8a4d441ef8b8cc1805/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6b61757368696b6d5f3f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/kaushikm_?style=social"></a>
<a href="https://x.com/Life_of_Y_" rel="nofollow"><img src="https://camo.githubusercontent.com/89078a653f61faf85c26ae741a2eaeb484af4a2bea576ae48cd57dd6a6118d31/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f4c6966655f6f665f595f3f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/Life_of_Y_?style=social"></a></p>
</div> 
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Golden Age of Japanese Pencils, 1952-1967 (296 pts)]]></title>
            <link>https://notes.stlartsupply.com/the-golden-age-of-japanese-pencils-1952-1967/</link>
            <guid>43243716</guid>
            <pubDate>Mon, 03 Mar 2025 16:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.stlartsupply.com/the-golden-age-of-japanese-pencils-1952-1967/">https://notes.stlartsupply.com/the-golden-age-of-japanese-pencils-1952-1967/</a>, See on <a href="https://news.ycombinator.com/item?id=43243716">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <main>
            <article>
    

    <div>
        <p>It was the summer of 1952, and the executives of Tombow Pencil were about to revolutionize the Japanese pencil industry—or, possibly, fall flat on their faces. Hachiro Ogawa, the son of founder Harunosuke Ogawa, was Tombow's managing director, and he had just finished a years-long project, at enormous cost, to make the best pencil Japan had ever seen. </p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/tombow-supreme-quality-4h-vintage-pencil-2.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/tombow-supreme-quality-4h-vintage-pencil-2.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/tombow-supreme-quality-4h-vintage-pencil-2.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/tombow-supreme-quality-4h-vintage-pencil-2.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/tombow-supreme-quality-4h-vintage-pencil-2.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>An early Tombow HOMO drawing pencil with transparent lacquer. Note the "H.O.P. Product" branding—in the early 1950s, Tombow still printed this abbreviation for "Harunosuke Ogawa Pencil" on each product. The Tombow name and dragonfly logo are foil-stamped on the opposite side. (</span><i><em>St. Louis Art Supply archives)</em></i></figcaption></figure><p>It was called "HOMO," because in comparison with other Japanese pencils of its day, Tombow's new model had a much more homogenous core. Pencil cores are a mixture of graphite and clay (thanks to Nicolas-Jacques Conté's invention of the modern pencil in the late eighteenth century), and the components in early cores were not always evenly mixed. This was particularly true in Japan, where pencils had only been made since the turn of the century and advanced industrial equipment was just starting to become available.</p><p>Hachiro's team at Tombow was determined to do whatever it took to produce more consistent cores. They struck up a working relationship with scientists at the University of Tokyo, a visionary move that yielded crucial technical research in 1948. Then, to implement the research findings, Tombow had to import more advanced industrial mills from the United States.</p><p>It was a gamble, but it worked, and suddenly Tombow could make much finer particles of graphite and clay than any other Japanese manufacturer. HOMO cores were stronger, smoother, and more consistent than anything else on the domestic market. They came in 17 grades, from 9H to 9B, a wide and finely graduated range that hadn't been possible with Tombow's old process.</p><p>They were also incredibly beautiful. Another import that had become available in the wake of World War II was incense cedar, the material of choice for high-quality pencils. Most of the pencil industry's incense cedar comes from California, and Tombow quickly restarted its imports of the aromatic red wood. HOMO's design takes full advantage of the material upgrade, with a subtle transparent lacquer that highlights the cedar's color and grain.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/tokyo-kaikan-in-1940.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/tokyo-kaikan-in-1940.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/tokyo-kaikan-in-1940.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/tokyo-kaikan-in-1940.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/tokyo-kaikan-in-1940.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Tokyo Kaikan meeting hall in 1940.</span></figcaption></figure><p>For Hachiro Ogawa and his father Harunosuke, the completion of the HOMO project was the culmination of a dream, and it was undoubtedly a pioneering moment in Japanese industry. But as the company prepared to introduce HOMO at the grand Tokyo Kaikan meeting hall, the skeptics must have been hard to ignore. In the early 1950s, a Japanese pencil cost five or ten yen (about 25-50 cents in 2022 dollars.) Tombow's technical leap forward had produced a model far superior to those inexpensive pencils, but they would also be pioneers in price. HOMO would cost 30 yen (about $1.50 today) for a single pencil, with boxes of twelve priced at 360 yen (about $19 today.)</p><p>Japanese consumers weren't used to spending that kind of money on a pencil. But if Hachiro, Harunosuke, and their colleagues were nervous, their fears were surely resolved at the first-ever Tombow New Product Presentation. Tokyo Kaikan was the esteemed meeting place of foreign dignitaries, corporate titans, even heads of state—and now it was absolutely bustling with stationery wholesalers, curious people from other companies, and the press. Tombow took orders for 720,000 HOMO pencils on launch day alone.</p><p>Tombow's surprising success with Japan's first premium pencil, along with the ambition and competitive spirit of midcentury manufacturers, led to the most intense period of development the global pencil industry has ever seen. </p><p>We call it the Golden Age of Japanese Pencils.</p><p>The Golden Age began and ended with two Tombow launches: Hachiro's pioneering HOMO launch in 1952 and the <a href="https://shop.stlartsupply.com/products/mono-100-pencil-hb-set-of-12?ref=notes.stlartsupply.com">MONO 100</a> launch in 1967, fifteen years later. During this period, Tombow and its crosstown rival Mitsubishi Pencil created many of the greatest pencils of all time, including the two best-regarded models offered today.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-9852-beauty-4.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/mitsubishi-9852-beauty-4.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/mitsubishi-9852-beauty-4.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/mitsubishi-9852-beauty-4.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-9852-beauty-4.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A contemporary box of </span><a href="https://shop.stlartsupply.com/products/mitsubishi-9852-hb-pencils-box-of-12?ref=notes.stlartsupply.com"><span>Mitsubishi 9852 "Master Writing"</span></a><span> pencils from our shop's inventory. Neither the box nor the pencil has substantially changed since its 1955 release.</span></figcaption></figure><h3 id="prologue-the-great-tokyo-pencil-rivalry">Prologue: The Great Tokyo Pencil Rivalry</h3><p>Although HOMO was the first Japanese pencil with a modern core, Tombow wasn't the first pencil manufacturer in Japan. That was Jinroku Masaki's pencil factory, which delivered three grades of writing pencils to the Japanese Ministry of Communication in 1901. Masaki had been introduced to the pencil at the Paris Expo of 1878, and he was one of a few Japanese industrialists to learn about the new technology in depth.</p><p>Masaki established a small pencil manufacturing firm in 1887, and just four years later, he had improved his products enough to land a government contract. To commemmorate the three grades of his "ministry pencil," Masaki had the idea to register a three-diamond trademark, along with the "Mitsubishi" name, which means 'three diamonds.' (It may surprise you to learn that this was ten years before the much better-known Mitsubishi Group of heavy industry companies registered its name and identical mark. Mitsubishi Pencil has no connection to the numerous other Mitsubishi companies in Japan; it is and has always been a manufacturer of writing and drawing supplies.)</p><p>Tombow's predecessor, Harunosuke Ogawa Pencil, was a small Tokyo retailer established in 1913. The 28-year-old Ogawa and his wife Towa were energetic and resourceful, and they built their business in tandem with their growing family. They soon decided to try their hand at manufacturing, and the first H.O.P. brand pencil, the Mason, was released a year later.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/various-hop-tombow-pencils.jpg" alt="" loading="lazy" width="1500" height="998" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/various-hop-tombow-pencils.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/various-hop-tombow-pencils.jpg 1000w, https://notes.stlartsupply.com/content/images/2022/03/various-hop-tombow-pencils.jpg 1500w" sizes="(min-width: 720px) 720px"><figcaption><span>Early Harunosuke Ogawa Pencils (c. 1900-1920). Image from </span><a href="https://www.tomboweurope.com/unternehmen/ueber-uns?ref=notes.stlartsupply.com"><span>Tombow Europe</span></a><span>.</span></figcaption></figure><p>The two companies grew and innovated in parallel. As the <em>100 Year History of Tombow Pencil</em> puts it, Mitsubishi and Tombow were "good rivals," who pushed one another to new heights, collaborated in joint ventures overseas, and worked together to develop new industry standards. (As we'll see, they also copied each other shamelessly.)</p><p>In the early years, Tombow led the way in structuring the pencil market, creating new brands and grades intended for different types of users. Before World War II, Tombow introduced several forward-looking models. H.O.P. Drawing Pencils (1928) came in 14 grades and were Japan's very first complete line of artist pencils. And Tombow's groundbreaking Model 8800 (1936), with a formula devised to rival the leads made by German manufacturer Stabilo, influenced other Japanese pencils for decades after its introduction.</p><p>Tombow's technical advantage was the product of its early investment in a modern, centralized factory, where all the trades involved in pencil manufacturing would coexist under one roof. The new Toshima Factory opened in 1927, during one of the worst economic crashes in Japanese history. Nevertheless, the new facility allowed Tombow to outpace Mitsubishi and the other, smaller competitors in product development for almost a quarter century.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/9800-beauty.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/9800-beauty.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/9800-beauty.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/9800-beauty.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/9800-beauty.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A contemporary box of </span><a href="https://shop.stlartsupply.com/products/mitsubishi-9800-writing-pencils-one-dozen?ref=notes.stlartsupply.com"><span>Mitsubishi 9800</span></a><span> pencils, still sold today in grades HB, B, and 2B.</span></figcaption></figure><p>It was only after Mitsubishi opened its Koyasu Factory (still operational today, but now called Yokohama Office) in 1940 that the older company began to catch up to its rival. After the war, Mitsubishi and Tombow set to work releasing a series of nearly identical everyday pencils: the green-and-gold <a href="https://shop.stlartsupply.com/products/mitsubishi-9800-writing-pencils-one-dozen?variant=12181850226771&amp;ref=notes.stlartsupply.com">Mitsubishi 9800</a> with "matured micro graphite lead" in 1946, the green-and-gold <a href="https://shop.stlartsupply.com/products/tombow-8900-pencil-hb-set-of-12?ref=notes.stlartsupply.com">Tombow 8900</a> with "micro crystallite" lead in 1948, and the (yes) green-and-gold Mitsubishi 9000, "Made By Elaborate Process." All three of these pencils, made on the cusp of the Golden Age, are still manufactured and sold today.</p><p>Green and gold, by the way, is the color scheme of Castell 9000 pencils, which were among the best in the world at the time. The Japanese companies had honed their process, but they hadn't developed the confidence and brand identity to step out of the shadow of European manufacturers. Fueled by their early success and a quickly changing world, the rivals were about to change that.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/vintage-mitsubishi-3800-and-mono-construction-1.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/vintage-mitsubishi-3800-and-mono-construction-1.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/vintage-mitsubishi-3800-and-mono-construction-1.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/vintage-mitsubishi-3800-and-mono-construction-1.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/vintage-mitsubishi-3800-and-mono-construction-1.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Vintage Mitsubishi 3800 and Tombow MONO Construction pencils. Both models are now discontinued. (</span><i><em>St. Louis Art Supply archives)</em></i></figcaption></figure><h3 id="the-industry-gets-really-really-organized">The Industry Gets Really, Really Organized</h3><p>As they improved their products and competed in the global market, Japanese manufacturers were keen to build a reputation for quality and reliability. To that end, the government worked with a wide variety of industries to enforce quality standards for different types of products. The system, which persists to this day (but no longer includes pencils), is called the Japanese Industrial Standards, or JIS.</p><p>The JIS Daily Goods Subcommittee researched, deliberated, and consulted with experts to develop the pencil standards, which were officially adopted in 1951. Just two years later, an astonishing 90% of pencils made in Japan had met the standards and completed certification. These pencils displayed the JIS mark 〄, which appeared on quality Japanese pencils until the industry was eventually deregulated in 1998.</p><p>It may sound excessive to create a national standard for pencils, but these everyday products were very important to Japan's economic strategy. In the highly regulated postwar days, the government prioritized light industry, which produces consumer goods like pencils. That policy decision gave the pencil companies a head start in securing resources and capital. Even more importantly, as the decade went on, Japanese consumers simply had more money to spend. From 1950 to 1960, average wages in Japan rose by 250%.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-6.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/more-vintage-pencils-mar-2022-6.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/more-vintage-pencils-mar-2022-6.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/more-vintage-pencils-mar-2022-6.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-6.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>The JIS mark, displayed on a vintage Mitsubishi pencil. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><p>Perhaps it isn't surprising, in retrospect, that Tombow's ultra-high-end HOMO pencil was an immediate success. And the rest of the industry wasn't far behind, because the science behind Tombow's new production process wasn't proprietary. As the product of a public-private collaboration with the University of Tokyo, the 1948 core research was available for anyone to read—and everyone did.</p><p>So, when HOMO created the premium pencil segment in Japan, the 109 other pencil manufacturers weren't caught flat-footed. As the Golden Age began, they raced to develop even more consistent cores, with finer-ground particles and thicker lacquer than the competition. Inspired by the case included with HOMO, Golden Age pencils often featured thick plastic storage cases, with transparent windows to display the increasingly refined and complex writing tools inside.</p><p>They proudly stamped their pencils in gold and silver foil, engraving them with English superlatives like "Supreme Quality" and "Master Writing." They made limited editions wrapped with illustrations and painted in special colors. And if one manufacturer advertised 100 million particles, another wouldn't be far behind with a 200-million-particle pencil.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-4.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/more-vintage-pencils-mar-2022-4.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/more-vintage-pencils-mar-2022-4.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/more-vintage-pencils-mar-2022-4.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-4.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A vintage Mitsubishi Copyrite 2000 pencil. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><h3 id="mitsubishi-goes-back-to-the-drawing-board">Mitsubishi Goes Back to the Drawing Board</h3><p>Yoji Suhara, director of engineering at Mitsubishi (and next in line for the chairmanship), wanted to know what the West thought of his company's pencils, so in 1953, he went on a fact-finding mission to Europe and the United States. He brought boxes of Mitsubishi 9000 pencils, their highest-quality model at the time, and asked local users what they thought about Japanese pencils.</p><p>Suhara was crestfallen to learn that his products had a poor reputation in their most important export markets. Consumers couldn't tell the difference between Mitsubishi's relatively advanced pencils and those from other Japanese producers who had emphasized production speed over quality control. The fact that the whole industry used similar color schemes and four-digit model numbers was confusing, particularly since they were, in many cases, exactly the same color schemes and model numbers as European manufacturers' well-known brands.</p><p>Suhara returned to Tokyo with a bitter taste in his mouth and a mission: to make a high-end pencil not just to top Tombow, but to show the world that Germany no longer had a monopoly on quality pencils.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-3.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/more-vintage-pencils-mar-2022-3.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/more-vintage-pencils-mar-2022-3.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/more-vintage-pencils-mar-2022-3.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-3.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A pair of vintage Mitsubishi 7679 pencils in soft green and pink. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><p>Back in Japan, Mitsubishi's engineers were already poring over the University of Tokyo pencil core research. Dramatic changes to the mixing, molding, and firing processes were all on the agenda, along with upgraded raw materials. Suhara and his colleagues tested 17 types of clay, finally settling on a variety sourced in Germany. To make the raw clay particles finer, Mitsubishi invented a new method of removing impurities; in recognition of this achievement, they earned a 30 million yen ($1.5 million today) subsidy from the Japanese Ministry of International Trade and Industry.</p><p>In addition to these technical improvements, Suhara wanted to make sure no one would confuse the new pencil with any other product in the world. It had to have a contemporary, well-considered, and unquestionably original design. In perhaps the best strategic decision in Mitsubishi's long history, the company hired Yoshio Akioka, who had recently designed the iconic Asakaze "Blue Train." </p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/EF66_55_asakaze_No.1.jpg" alt="" loading="lazy" width="945" height="684" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/EF66_55_asakaze_No.1.jpg 600w, https://notes.stlartsupply.com/content/images/2022/03/EF66_55_asakaze_No.1.jpg 945w" sizes="(min-width: 720px) 720px"><figcaption><span>The Asakaze "Blue Train" in 1988. Its designer, Yoshio Akioka, also led the Uni pencil development effort. Photo credit: </span><a href="https://commons.wikimedia.org/wiki/File:EF66_55_asakaze_No.1.jpg?ref=notes.stlartsupply.com"><span>spaceaero2</span></a><span>, </span><a href="https://creativecommons.org/licenses/by/3.0?ref=notes.stlartsupply.com"><span>CC BY 3.0</span></a><span>, via Wikimedia Commons</span></figcaption></figure><p>In an unconventional move, Mitsubishi gave Akioka extensive creative control over the project from the beginning; the esteemed industrial designer even suggested the price—50 yen ($2.50 today), the same price the European manufacturers were charging at the time.</p><p>One of Akioka's most important contributions was the color, an iconic maroon that's now synonymous with Mitsubishi pencils. The world's best brands all had a signature color, like Faber-Castell's deep green and Staedtler's bright blue. For Mitsubishi, Akioka chose a shade that felt distinctively Japanese, halfway between the traditional colors <em>enji</em> (wine red) and <em>ebicha</em> (maroon).</p><p>To make sure the color would be unique to Mitsubishi, Akioka ordered 160 pencils from manufacturers all over the world. No one had painted a pencil quite this shade of red before.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/ebicha-enji-uni.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/ebicha-enji-uni.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/ebicha-enji-uni.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/ebicha-enji-uni.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/ebicha-enji-uni.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Uni's signature color is distinctively Japanese, but it's not exactly one of Japan's traditional colors.</span></figcaption></figure><p>Akioka named the new pencil, too—"Uni," short for the English word 'unique,' but also French for 'smooth.' Imagining the architects of the future drafting homes with Uni pencils, Akioka designed the timeless, rounded "uni" logotype, with wide, sturdy typography. As he put it, he wanted the Uni brand to be "a house that warmly wraps people up," instantly familiar and comfortable.</p><p>Today, most of Mitsubishi's global customers identify the company with Akioka's Uni brand, which Mitsubishi quickly began to use on a variety of other office supplies and stationery. In the United States, everyone knows Uni-ball pens, but not many people know that they're made by a 135-year-old company called Mitsubishi. That's a testament to the success of Akioka's work in the mid-fifties.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/kh20-sharpener-beauty.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/kh20-sharpener-beauty.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/kh20-sharpener-beauty.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/kh20-sharpener-beauty.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/kh20-sharpener-beauty.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A </span><a href="https://shop.stlartsupply.com/products/kh-20-desktop-pencil-sharpener?ref=notes.stlartsupply.com"><span>Mitsubishi KH-20 crank sharpener</span></a><span> with a </span><a href="https://shop.stlartsupply.com/products/uni-star-pencil-hb-set-of-12?ref=notes.stlartsupply.com"><span>Mitsubishi Uni Star HB</span></a><span>. Uni Star is a budget version of Uni with the same core but a less refined finish.</span></figcaption></figure><p>It was clear soon after Uni's 1958 launch that the pencil was a success. The maroon, gold, and black pencil was an instant status symbol and a genuine object of desire for students. From a technical perspective, Uni remains one of the best pencils ever made, surpassing European and domestic rivals with its ultra-smooth, impressively dark graphite.</p><p>Akioka's futuristic design influenced the rest of the industry, and Uni pencils have been a fixture in Japanese schools and offices ever since. The company continues to manufacture them, almost exactly as they were 64 years ago.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-2.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/more-vintage-pencils-mar-2022-2.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/more-vintage-pencils-mar-2022-2.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/more-vintage-pencils-mar-2022-2.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-2.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A vintage Tombow MONO COPY 2H pencil. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><h3 id="eight-billion-particles-per-cubic-millimeter">Eight Billion Particles Per Cubic Millimeter</h3><p>At Tombow, Hachiro Ogawa had become the company's president following Harunosuke's death in 1957. Hachiro and his younger brother Kohei worked to broaden Tombow's product line, launching their first mechanical pencils in 1957, markers and ballpoint pens in 1958, and an electric sharpener in 1962. But even as Tombow and Mitsubishi expanded their offerings into new kinds of writing tools, the pencil rivalry was not quite over.</p><p>In 1963, Tombow replaced its decade-old HOMO pencils with the newly renamed and completely redesigned MONO brand. The new name (created upon Tombow's belated discovery of the English meaning of "homo") was suggested by Tombow technical consultant Akamatsu Noriyasu, who had been a primary author of the University of Tokyo pencil core research. MONO comes from the Greek <em>monos</em>, which Tombow translates as "unique and unrivaled."</p><p>With a new, glossier design in black, gold, and white, plus advertisements touting "8 billion particles per cubic millimeter" and a prestigious pricetag of 60 yen (20% higher than Uni), Tombow MONO was a direct response to Mitsubishi's groundbreaking design. The dynamic of the rivalry had changed again after a half-century of competition. Now Mitsubishi had the initiative, and Tombow had to match its rival's advances. </p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/70th-anniversary-diamond-memo-70th-beauty.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/70th-anniversary-diamond-memo-70th-beauty.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/70th-anniversary-diamond-memo-70th-beauty.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/70th-anniversary-diamond-memo-70th-beauty.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/70th-anniversary-diamond-memo-70th-beauty.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A vintage Mitsubishi 3900 H pencil, pictured with some anachronistic accessories. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><h3 id="the-new-chairmen">The New Chairmen</h3><p>In April 1963, four months before the release of MONO, tragedy struck Tombow's Ogawa family when Hachiro died suddenly. Kohei, the younger brother, was appointed Hachiro's successor only six years after Hachiro replaced Harunosuke. Towa Ogawa, Harunosuke's wife and collaborator, known for her early embrace of the mass media, would live only one year longer than her oldest son. She died in 1964 and was one of very few women to be awarded Japan's Order of the Sacred Treasure that same year.</p><p>Understandably, the launch of MONO was a moment of mixed emotions at Tombow. Hachiro's latest leap forward was a success, and the MONO brand became just as important to Tombow as the Uni brand is to Mitsubishi. But the innovative older brother wasn't there to see the outsized results of his short six years at the helm.</p><p>In large part because of Hachiro's modernization efforts in the forties and fifties, MONO pencils are still used by artists and writers, and MONO brand stationery in virtually every category is a familiar sight in shops around the world. Hachiro's vision in creating HOMO laid the foundation for Tombow's next 50 years.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-5.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/more-vintage-pencils-mar-2022-5.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/more-vintage-pencils-mar-2022-5.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/more-vintage-pencils-mar-2022-5.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/more-vintage-pencils-mar-2022-5.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Vintage Mitsubishi 1321 H pencil with pink lacquer and black squiggles. </span><i><em>(St. Louis Art Supply archives)</em></i></figcaption></figure><p>Mitsubishi was also undergoing a generational shift. Chairman Saburo Suhara retired in 1960, and his son Yoji, champion of the Uni project, became the company's new leader. After MONO's release in 1963, Suhara and Mitsubishi understood they would have to respond. Pencil production and demand had continued to grow, as had the number of pencil manufacturers in Japan. In 1960, there were more than 200 of them.</p><p>But creating a pencil even more luxurious than Uni would not necessarily have a big impact on the company's performance—a pencil <em>that </em>refined would certainly be a niche item, not sent in schoolchildrens' backpacks like Uni. Further, the writing (if you'll forgive the bad joke) was on the wall for pencils as a universally important, in-demand item.</p><p>Pens and mechanical pencils had supplanted wooden pencils, which no longer seemed so convenient in comparison with a ballpoint. Mitsubishi, in fact, had already started making pens in 1959 and mechanical pencils in 1961. Following Pentel's commercialization of fiber-tipped pens in 1963, Mitsubishi introduced its own porous point pens in 1965.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-and-tombow-vintage-pencils-1.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/mitsubishi-and-tombow-vintage-pencils-1.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/mitsubishi-and-tombow-vintage-pencils-1.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/mitsubishi-and-tombow-vintage-pencils-1.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-and-tombow-vintage-pencils-1.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Golden Age pencils from Mitsubishi and Tombow. From front to back: Uni Popular HB, Tombow 9561 2H, Tombow HOMO 4H. (</span><i><em>St. Louis Art Supply archives)</em></i></figcaption></figure><p>So, although Mitsubishi and Tombow didn't know in advance that the Japanese pencil industry would reach its peak in 1966, both companies clearly saw it coming, and they had already prepared themselves for a future beyond pencils. One wonders why both companies continued to expend research and development resources on high-end pencils in the late 1960s, but they did—and on a personal note, this sometimes inexplicable tendency of Japanese manufacturers to perfect what doesn't need to be perfected is a major reason why we're so passionate about our Japanese imports.</p><p>Faced with a coming drop in demand and with successful, high-quality products already on the market, many Western companies would have started cutting costs, not investing in further improvements. Indeed, the truly innovative days of German and American pencil manufacturing were already over. But thanks to the competitive, perfectionist leaders and designers at Mitsubishi and Tombow, the late 1960s yielded two more historic pencils.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/hi-uni-ad-from-japanese-handball-journal-2.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/hi-uni-ad-from-japanese-handball-journal-2.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/hi-uni-ad-from-japanese-handball-journal-2.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/hi-uni-ad-from-japanese-handball-journal-2.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/hi-uni-ad-from-japanese-handball-journal-2.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A </span><a href="https://shop.stlartsupply.com/collections/pencils/products/hi-uni-pencil-hb-set-of-12?ref=notes.stlartsupply.com"><span>Mitsubishi Hi-Uni</span></a><span> print advertisement from 1970. The copy emphasizes Hi-Uni's dark, soft lead. </span><i><em>(Journal of the Japanese Handball Association, issue 74)</em></i></figcaption></figure><h3 id="the-last-premium-pencils">The Last Premium Pencils</h3><p>In 1966, precisely at the peak of the Japanese pencil industry, Yoji Suhara's Mitsubishi introduced <a href="https://shop.stlartsupply.com/collections/pencils/products/hi-uni-pencil-hb-set-of-12?ref=notes.stlartsupply.com">Hi-Uni</a>, an upgraded version of Yoshio Akioka's eight-year-old Uni. We all know that sequels (especially those released after the commercial peak of the franchise) tend to be disappointing, but Hi-Uni is the rare exception. Rather than overcomplicating or underappreciating Akioka's design, Mitsubishi improved on it in every way without diminishing its modern style.</p><p>Hi-Uni came in a wider range of grades, resolving a key shortcoming of the original model. (The<a href="https://shop.stlartsupply.com/collections/pencils/products/hi-uni-pencil-hb-set-of-12?ref=notes.stlartsupply.com"> current Hi-Uni</a> is offered in an amazing 22 grades, with 10B added to the lineup just recently.) The core boasted Mitsubishi's latest technical innovation, which went beyond simply minimizing particle size to actually <em>maximizing particle density</em> by combining particles of different sizes in a precise ratio. And no expense was spared in sourcing and screening the raw materials for this very precious pencil.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/hi-uni-on-canvas-1.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/hi-uni-on-canvas-1.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/hi-uni-on-canvas-1.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/hi-uni-on-canvas-1.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/hi-uni-on-canvas-1.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A contemporary </span><a href="https://shop.stlartsupply.com/collections/pencils/products/hi-uni-pencil-hb-set-of-12?ref=notes.stlartsupply.com"><span>Mitsubishi Hi-Uni</span></a><span>. The only differences between this and the 1966 Hi-Uni are the gold dot on the endcap (it used to be white) and the removal of the JIS mark in 1998.</span></figcaption></figure><p>The result (and here, I'll reveal a strongly held personal belief) is the greatest pencil ever made, in any country, before or since. For Mitsubishi's designers, the ideal pencil would have "6B blackness with 9H hardness," able to make very dark marks while also wearing down slowly and offering pinpoint precision. Hi-Uni's core, with its innovative mixture of particle sizes, comes pretty close to this unachievable ideal. Better yet, despite its excellent point retention, Hi-Uni lead is also the industry leader in smoothness.</p><p>Hi-Uni was an authoritative and complete response to Tombow MONO, and it launched with the same legendary status it retains for pencil lovers today. At a princely price of 100 yen each in 1966 (that's almost $4 in today's dollars!), Hi-Uni represented the almost excessive pursuit of perfection in simple things. It spoke to the desire for a rich and comforting everyday life. And it expressed the powerful optimism of 1960s Japan.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/vintage-mono-100-with-case-1.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/vintage-mono-100-with-case-1.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/vintage-mono-100-with-case-1.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/vintage-mono-100-with-case-1.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/vintage-mono-100-with-case-1.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A vintage set of </span><a href="https://shop.stlartsupply.com/products/mono-100-pencil-hb-set-of-12?ref=notes.stlartsupply.com"><span>MONO 100</span></a><span> pencils in their original packaging from the late 1960s. The box is surprisingly heavy, in part because an eraser was included—in its own, smaller case! (</span><i><em>St. Louis Art Supply archives)</em></i></figcaption></figure><p>Tombow wasn't far behind with its own aspirational pencil, and <a href="https://shop.stlartsupply.com/products/mono-100-pencil-hb-set-of-12?ref=notes.stlartsupply.com">MONO 100</a>, the last premium pencil launched by either manufacturer to this day, appeared in stores in 1967. It was perhaps a sign of an emerging détente that MONO 100 debuted at exactly the same retail price as Hi-Uni: 100 yen each.</p><p>Taking a page from Mitsubishi's book, Tombow hired a well-known industrial designer, Takashi Kono, to update the MONO design. Kono had been involved in high-profile projects like the Olympics and the World's Fair and was one of the most accomplished adesigners in midcentury Japan. His design for MONO 100 is a genuine classic, with a cooler, slicker, and simpler look than the soft-edged, stately Mitsubishi Hi-Uni. It's particularly recognizable for its white stripe, jauntily painted in a vertical arc over the endcap.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/tombow-mono-100-beauty-3.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/tombow-mono-100-beauty-3.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/tombow-mono-100-beauty-3.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/tombow-mono-100-beauty-3.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/tombow-mono-100-beauty-3.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A contemporary set of </span><a href="https://shop.stlartsupply.com/products/mono-100-pencil-b-set-of-12?ref=notes.stlartsupply.com"><span>MONO 100 pencils in grade B.</span></a></figcaption></figure><p>From an industrial design standpoint, Tombow stuck to its strategy of milling finer and finer particles. The MONO 100 advertisements claimed "10 billion particles per cubic millimeter," up from 8 billion for the original MONO. This design concept makes MONO 100 different from Hi-Uni in a few important ways. MONO 100 is more precise and longer-wearing than the same grade of Hi-Uni, but somewhat less smooth and dark.</p><p>Those differences make the Hi-Uni / MONO 100 debate a sort of personality test. Do you prefer creamy, super-dark graphite, and are you okay with sharpening more frequently? Would you enjoy using a pencil with a design that connotes luxury, elegance, and composure? Then <a href="https://shop.stlartsupply.com/collections/pencils/products/hi-uni-pencil-hb-set-of-12?ref=notes.stlartsupply.com">Hi-Uni</a> is the pencil for you. But if, instead, you prefer sharper, more defined marks, with less smudging, less sharpening, and a futuristic look, then <a href="https://shop.stlartsupply.com/products/mono-100-pencil-b-set-of-12?ref=notes.stlartsupply.com">MONO 100</a> might be a better choice.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/Mitsubishi_Pencil_HQ_Building_March-2019.jpg" alt="" loading="lazy" width="2000" height="1333" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/Mitsubishi_Pencil_HQ_Building_March-2019.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/Mitsubishi_Pencil_HQ_Building_March-2019.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/Mitsubishi_Pencil_HQ_Building_March-2019.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/Mitsubishi_Pencil_HQ_Building_March-2019.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Mitsubishi Pencil's headquarters in 2019. Photo credit: </span><a href="https://commons.wikimedia.org/w/index.php?curid=77372982&amp;ref=notes.stlartsupply.com"><span>Doricono</span></a><span>, </span><a href="https://creativecommons.org/licenses/by-sa/4.0?ref=notes.stlartsupply.com"><span>CC BY-SA 4.0</span></a><span>, via Wikimedia Commons</span></figcaption></figure><h3 id="the-golden-age-never-ended">The Golden Age Never Ended</h3><p>Isn't it strange that this historical essay includes purchase recommendations? That's kind of the beauty of our weird little industry. Most of the pencils discussed in this article are still manufactured and sold, even though there would never be another pencil launch to top Hi-Uni and MONO 100. Mitsubishi and Tombow each expanded and modernized their businesses, and today both are heritage brands with large, diversified product lines.</p><p>For those of us who love pencils—the design and typography, the nostalgia, the irreplaceable sound of smooth graphite on paper—the end of this story is bittersweet. Pencil manufacturing is not really an industry of its own; very few companies today specialize in wood pencils alone, and certainly no large companies do. Pencils have vanished from offices and are used mainly by artists and students. Since the 1970s, many historic European and American brands have actually decreased in quality. Frankly, we have to accept that Hi-Uni and MONO 100 will probably not be improved upon.</p><p>But is that such a bad thing? Maybe Mitsubishi never fired back with a "Hi-Hi-Uni" because pencil manufacturing had reached a natural point of diminishing returns, where any further increase in quality would be virtually imperceptible. Today's manufacturers are not racing to improve the pocket knife, for example, because the technology is mature and the major flaws have been sorted out by past generations. Many of the most revered brands, like France's Opinel, have made the same knives for decades, and they cut just as well as they always have.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-kohitsu-shosha-10b-beauty-3.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/mitsubishi-kohitsu-shosha-10b-beauty-3.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/mitsubishi-kohitsu-shosha-10b-beauty-3.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/mitsubishi-kohitsu-shosha-10b-beauty-3.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/mitsubishi-kohitsu-shosha-10b-beauty-3.jpg 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>A </span><a href="https://shop.stlartsupply.com/collections/pencils/products/kohitsu-shosha-pencil-saitama-edition-10b?ref=notes.stlartsupply.com"><span>Mitsubishi Kohitsu Shosha pencil, Saitama edition</span></a><span>, with hexagonal body and 10B core.</span></figcaption></figure><p>In this pencil merchant's opinion, there's simply no need for a pencil more perfect than the best of Japan's Golden Age. We can admire the heady moment and the strong personalities who created these pencils, and we can be forgiven for daydreaming about the even-more-perfect pencil, the one that would make our handwriting beautiful and our drawings perfectly proportional. </p><p>But when I sit down to sharpen my pencil (usually a Hi-Uni HB or Mitsubishi 9852 "Master Writing" B), my primary feeling is gratitude. The designers and engineers who created these tools didn't know they would be made for 70 years, but they treated their seemingly small task with intense seriousness of purpose, and that passion produced outstanding tools that have still not been surpassed. Today, in 2022, I frequently speak with artists who tell me how much these pencils inspire them and enable their best work.</p><p>So I'm not regretful about the end of the Golden Age of Pencils, because in the ways that matter most, it never ended. <a href="https://shop.stlartsupply.com/collections/mitsubishi-pencil-co?ref=notes.stlartsupply.com">Mitsubishi, in particular, has loyally maintained its midcentury product line,</a> continuing to manufacture its pencils in Japan and even adding a minor new model now and then. (There's an antiviral-coated Mitsubishi in light blue, <a href="https://shop.stlartsupply.com/products/mitsubishi-9800vb-pencil-hb-box-of-12?ref=notes.stlartsupply.com">new for 2022</a>.) Artists and writers still debate the merits of Hi-Uni and MONO 100. </p><p>And I can't speak for everyone who works here, but personally, I'm excited every time I ship a fresh, unsharpened dozen to a new customer. For them, the Golden Age is just getting started.</p><figure><img src="https://notes.stlartsupply.com/content/images/2022/03/md-notebook-light-70th-anniversary-beauty-4.jpg" alt="" loading="lazy" width="2000" height="1335" srcset="https://notes.stlartsupply.com/content/images/size/w600/2022/03/md-notebook-light-70th-anniversary-beauty-4.jpg 600w, https://notes.stlartsupply.com/content/images/size/w1000/2022/03/md-notebook-light-70th-anniversary-beauty-4.jpg 1000w, https://notes.stlartsupply.com/content/images/size/w1600/2022/03/md-notebook-light-70th-anniversary-beauty-4.jpg 1600w, https://notes.stlartsupply.com/content/images/2022/03/md-notebook-light-70th-anniversary-beauty-4.jpg 2000w" sizes="(min-width: 720px) 720px"></figure><hr><h3 id="references">References</h3><p>Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer, "The Next Generation of the Penn World Table," <em>American Economic Review</em>, 105(10), 3150-3182, available for download at http://www.ggdc.net/pwt</p><p>Japan Pencil Industry Cooperative Association, "The history of pencils and the Japanese pencil industry," 2001. <a href="http://www.pencil.or.jp/company/rekishi/rekishi.html?ref=notes.stlartsupply.com#01">http://www.pencil.or.jp/company/rekishi/rekishi.html</a></p><p>NTT Comware Co., "Long seller: Mitsubishi Uni pencils," <em>Comzine</em>, volume 23, issue 5 (2005). <a href="https://www.nttcom.co.jp/comzine/no023/long_seller/?ref=notes.stlartsupply.com">https://www.nttcom.co.jp/comzine/no023/long_seller/</a></p><p>Tombow Pencil Co., Ltd., <em>100 Year History of Tombow Pencil</em>, 2013<em>. </em>Free Japanese ebook available <a href="https://my.ebook5.net/tombow/tL6UMg/?ref=notes.stlartsupply.com">here</a>.</p>
    </div>

            </article>                                </main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC expected to announce $100B investment in U.S. (260 pts)]]></title>
            <link>https://www.wsj.com/tech/trump-chip-maker-tsmc-expected-to-announce-100-billion-investment-in-u-s-02a44399</link>
            <guid>43243580</guid>
            <pubDate>Mon, 03 Mar 2025 16:40:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/trump-chip-maker-tsmc-expected-to-announce-100-billion-investment-in-u-s-02a44399">https://www.wsj.com/tech/trump-chip-maker-tsmc-expected-to-announce-100-billion-investment-in-u-s-02a44399</a>, See on <a href="https://news.ycombinator.com/item?id=43243580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div display="flex"><nav aria-label="Technology breadcrumb"><ol><li><a href="https://www.wsj.com/tech?mod=breadcrumb" data-testid="breadcrumb-link"><span><span><span>Technology</span></span></span></a></li></ol></nav></div><div><h2>It is the latest effort by the president to persuade companies to make big investments in the U.S.</h2></div></div><article><div><section><p data-type="paragraph">WASHINGTON—Taiwan Semiconductor Manufacturing Co. plans to invest at least $100 billion more in chip-manufacturing plants in the U.S. over the next several years under a plan announced Monday by the company and President Trump.</p><p data-type="paragraph">TSMC plans to use the funds to add to its chip manufacturing in Arizona. It will construct three new chip plants, two chip-packaging plants and a research and development center, Chief Executive <!-- -->C.C. Wei<!-- --> said during a White House appearance with the president.</p></section><p>Copyright ©<!-- -->2025<!-- --> Dow Jones &amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8</p></div><div><p><h2>Videos</h2></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go-attention: A full attention mechanism and transformer in pure Go (129 pts)]]></title>
            <link>https://github.com/takara-ai/go-attention</link>
            <guid>43243549</guid>
            <pubDate>Mon, 03 Mar 2025 16:38:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/takara-ai/go-attention">https://github.com/takara-ai/go-attention</a>, See on <a href="https://news.ycombinator.com/item?id=43243549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">go-attention</h2><a id="user-content-go-attention" aria-label="Permalink: go-attention" href="#go-attention"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5ef5f94a9648b5afca0d5e381017252c2bde05d95e188cae63186277041ca20f/68747470733a2f2f74616b6172612e61692f696d616765732f6c6f676f2d32342f54616b61726141692e737667"><img src="https://camo.githubusercontent.com/5ef5f94a9648b5afca0d5e381017252c2bde05d95e188cae63186277041ca20f/68747470733a2f2f74616b6172612e61692f696d616765732f6c6f676f2d32342f54616b61726141692e737667" width="200" alt="Takara.ai Logo" data-canonical-src="https://takara.ai/images/logo-24/TakaraAi.svg"></a></p>
<p dir="auto">From the Frontier Research Team at takara.ai we present the first pure Go implementation of attention mechanisms and transformer layers, designed for high performance and ease of use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">Run our comprehensive examples:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Get the module
go get github.com/takara-ai/go-attention

# Run the examples
go run api_examples.go"><pre><span><span>#</span> Get the module</span>
go get github.com/takara-ai/go-attention

<span><span>#</span> Run the examples</span>
go run api_examples.go</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">API Documentation</h2><a id="user-content-api-documentation" aria-label="Permalink: API Documentation" href="#api-documentation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Types</h3><a id="user-content-core-types" aria-label="Permalink: Core Types" href="#core-types"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="type Vector []float64           // Represents a 1D vector of float64 values
type Matrix []Vector           // Represents a 2D matrix of float64 values"><pre><span>type</span> <span>Vector</span> []<span>float64</span>           <span>// Represents a 1D vector of float64 values</span>
<span>type</span> <span>Matrix</span> []<span>Vector</span>           <span>// Represents a 2D matrix of float64 values</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Basic Dot-Product Attention</h3><a id="user-content-1-basic-dot-product-attention" aria-label="Permalink: 1. Basic Dot-Product Attention" href="#1-basic-dot-product-attention"></a></p>
<p dir="auto">The simplest form of attention mechanism. Useful for basic sequence processing tasks.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;github.com/takara-ai/go-attention/attention&quot;

// Create query-key-value setup
query := attention.Vector{1.0, 0.0, 1.0, 0.0}  // Pattern to search for
keys := attention.Matrix{
    {1.0, 0.0, 1.0, 0.0},  // Similar to query
    {0.0, 1.0, 0.0, 1.0},  // Different from query
    {0.5, 0.5, 0.5, 0.5},  // Neutral pattern
}
values := attention.Matrix{
    {1.0, 2.0},  // Value for similar key
    {3.0, 4.0},  // Value for different key
    {5.0, 6.0},  // Value for neutral key
}

// Compute attention
output, weights, err := attention.DotProductAttention(query, keys, values)
if err != nil {
    log.Fatal(err)
}

// Output will be a weighted combination of values based on query-key similarity
// Weights will show how much attention each key received"><pre><span>import</span> <span>"github.com/takara-ai/go-attention/attention"</span>

<span>// Create query-key-value setup</span>
<span>query</span> <span>:=</span> attention.<span>Vector</span>{<span>1.0</span>, <span>0.0</span>, <span>1.0</span>, <span>0.0</span>}  <span>// Pattern to search for</span>
<span>keys</span> <span>:=</span> attention.<span>Matrix</span>{
    {<span>1.0</span>, <span>0.0</span>, <span>1.0</span>, <span>0.0</span>},  <span>// Similar to query</span>
    {<span>0.0</span>, <span>1.0</span>, <span>0.0</span>, <span>1.0</span>},  <span>// Different from query</span>
    {<span>0.5</span>, <span>0.5</span>, <span>0.5</span>, <span>0.5</span>},  <span>// Neutral pattern</span>
}
<span>values</span> <span>:=</span> attention.<span>Matrix</span>{
    {<span>1.0</span>, <span>2.0</span>},  <span>// Value for similar key</span>
    {<span>3.0</span>, <span>4.0</span>},  <span>// Value for different key</span>
    {<span>5.0</span>, <span>6.0</span>},  <span>// Value for neutral key</span>
}

<span>// Compute attention</span>
<span>output</span>, <span>weights</span>, <span>err</span> <span>:=</span> <span>attention</span>.<span>DotProductAttention</span>(<span>query</span>, <span>keys</span>, <span>values</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Output will be a weighted combination of values based on query-key similarity</span>
<span>// Weights will show how much attention each key received</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Multi-Head Attention</h3><a id="user-content-2-multi-head-attention" aria-label="Permalink: 2. Multi-Head Attention" href="#2-multi-head-attention"></a></p>
<p dir="auto">More sophisticated attention mechanism that can capture different types of relationships in parallel.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;github.com/takara-ai/go-attention/attention&quot;

// Configure multi-head attention
config := attention.MultiHeadConfig{
    NumHeads:    4,        // Number of parallel attention heads
    DModel:      64,       // Size of input/output embeddings
    DKey:        16,       // Size per head (DModel/NumHeads)
    DValue:      16,       // Size per head (DModel/NumHeads)
    DropoutRate: 0.1,      // For regularization
}

// Create the attention module
mha, err := attention.NewMultiHeadAttention(config)
if err != nil {
    log.Fatal(err)
}

// Process sequences (batched input)
batchSize, seqLen := 2, 3  // Process 2 sequences, each with 3 tokens

// Create input matrices [batchSize × seqLen × DModel]
queries := make(attention.Matrix, batchSize*seqLen)
keys := make(attention.Matrix, batchSize*seqLen)
values := make(attention.Matrix, batchSize*seqLen)

// Initialize your matrices with actual data...

// Process through multi-head attention
output, err := mha.Forward(queries, keys, values)
if err != nil {
    log.Fatal(err)
}"><pre><span>import</span> <span>"github.com/takara-ai/go-attention/attention"</span>

<span>// Configure multi-head attention</span>
<span>config</span> <span>:=</span> attention.<span>MultiHeadConfig</span>{
    <span>NumHeads</span>:    <span>4</span>,        <span>// Number of parallel attention heads</span>
    <span>DModel</span>:      <span>64</span>,       <span>// Size of input/output embeddings</span>
    <span>DKey</span>:        <span>16</span>,       <span>// Size per head (DModel/NumHeads)</span>
    <span>DValue</span>:      <span>16</span>,       <span>// Size per head (DModel/NumHeads)</span>
    <span>DropoutRate</span>: <span>0.1</span>,      <span>// For regularization</span>
}

<span>// Create the attention module</span>
<span>mha</span>, <span>err</span> <span>:=</span> <span>attention</span>.<span>NewMultiHeadAttention</span>(<span>config</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Process sequences (batched input)</span>
<span>batchSize</span>, <span>seqLen</span> <span>:=</span> <span>2</span>, <span>3</span>  <span>// Process 2 sequences, each with 3 tokens</span>

<span>// Create input matrices [batchSize × seqLen × DModel]</span>
<span>queries</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)
<span>keys</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)
<span>values</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>batchSize</span><span>*</span><span>seqLen</span>)

<span>// Initialize your matrices with actual data...</span>

<span>// Process through multi-head attention</span>
<span>output</span>, <span>err</span> <span>:=</span> <span>mha</span>.<span>Forward</span>(<span>queries</span>, <span>keys</span>, <span>values</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3. Full Transformer Layer</h3><a id="user-content-3-full-transformer-layer" aria-label="Permalink: 3. Full Transformer Layer" href="#3-full-transformer-layer"></a></p>
<p dir="auto">Complete transformer layer with self-attention and feed-forward network.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import (
    &quot;github.com/takara-ai/go-attention/transformer&quot;
    &quot;github.com/takara-ai/go-attention/attention&quot;
)

// Configure transformer layer
config := transformer.TransformerConfig{
    DModel:      64,       // Size of token embeddings
    NumHeads:    4,        // Number of attention heads
    DHidden:     256,      // Size of feed-forward hidden layer
    DropoutRate: 0.1,      // For regularization
}

// Create transformer layer
layer, err := transformer.NewTransformerLayer(config)
if err != nil {
    log.Fatal(err)
}

// Create input sequence [seq_len × d_model]
seqLen := 3
input := make(attention.Matrix, seqLen)
for i := range input {
    input[i] = make(attention.Vector, config.DModel)
    // Fill with your embedding data...
}

// Process through transformer
output, err := layer.Forward(input)
if err != nil {
    log.Fatal(err)
}"><pre><span>import</span> (
    <span>"github.com/takara-ai/go-attention/transformer"</span>
    <span>"github.com/takara-ai/go-attention/attention"</span>
)

<span>// Configure transformer layer</span>
<span>config</span> <span>:=</span> transformer.<span>TransformerConfig</span>{
    <span>DModel</span>:      <span>64</span>,       <span>// Size of token embeddings</span>
    <span>NumHeads</span>:    <span>4</span>,        <span>// Number of attention heads</span>
    <span>DHidden</span>:     <span>256</span>,      <span>// Size of feed-forward hidden layer</span>
    <span>DropoutRate</span>: <span>0.1</span>,      <span>// For regularization</span>
}

<span>// Create transformer layer</span>
<span>layer</span>, <span>err</span> <span>:=</span> <span>transformer</span>.<span>NewTransformerLayer</span>(<span>config</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}

<span>// Create input sequence [seq_len × d_model]</span>
<span>seqLen</span> <span>:=</span> <span>3</span>
<span>input</span> <span>:=</span> <span>make</span>(attention.<span>Matrix</span>, <span>seqLen</span>)
<span>for</span> <span>i</span> <span>:=</span> <span>range</span> <span>input</span> {
    <span>input</span>[<span>i</span>] <span>=</span> <span>make</span>(attention.<span>Vector</span>, <span>config</span>.<span>DModel</span>)
    <span>// Fill with your embedding data...</span>
}

<span>// Process through transformer</span>
<span>output</span>, <span>err</span> <span>:=</span> <span>layer</span>.<span>Forward</span>(<span>input</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>log</span>.<span>Fatal</span>(<span>err</span>)
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example Output</h2><a id="user-content-example-output" aria-label="Permalink: Example Output" href="#example-output"></a></p>
<p dir="auto">When running the examples, you'll see:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Dot-Product Attention</strong>:</p>
<div data-snippet-clipboard-copy-content="Query: [1 0 1 0]
Attention Weights: [0.506 0.186 0.307]  // Shows focus on similar patterns
Output: [2.601 3.601]                   // Weighted combination of values"><pre><code>Query: [1 0 1 0]
Attention Weights: [0.506 0.186 0.307]  // Shows focus on similar patterns
Output: [2.601 3.601]                   // Weighted combination of values
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Multi-Head Attention</strong>:</p>
<div data-snippet-clipboard-copy-content="Input dimensions: [2 batches × 3 tokens × 64 features]
Output shape: [6×64]"><pre><code>Input dimensions: [2 batches × 3 tokens × 64 features]
Output shape: [6×64]
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Transformer Layer</strong>:</p>
<div data-snippet-clipboard-copy-content="Input shape: [3×64]
Output shape: [3×64]"><pre><code>Input shape: [3×64]
Output shape: [3×64]
</code></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Common Use Cases</h2><a id="user-content-common-use-cases" aria-label="Permalink: Common Use Cases" href="#common-use-cases"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Text Processing</strong>:</p>
<ul dir="auto">
<li>Sequence-to-sequence translation</li>
<li>Document summarization</li>
<li>Sentiment analysis</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Time Series</strong>:</p>
<ul dir="auto">
<li>Financial forecasting</li>
<li>Sensor data analysis</li>
<li>Anomaly detection</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Structured Data</strong>:</p>
<ul dir="auto">
<li>Graph node embedding</li>
<li>Feature interaction modeling</li>
<li>Recommendation systems</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance Considerations</h2><a id="user-content-performance-considerations" aria-label="Permalink: Performance Considerations" href="#performance-considerations"></a></p>
<ul dir="auto">
<li>Matrix operations are optimized for CPU</li>
<li>Memory allocations are minimized</li>
<li>Batch processing for better throughput</li>
<li>No external dependencies</li>
</ul>
<p dir="auto">For more detailed examples, see the <code>examples</code> directory in the repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why go-attention?</h2><a id="user-content-why-go-attention" aria-label="Permalink: Why go-attention?" href="#why-go-attention"></a></p>
<p dir="auto">This module was created to provide a clean, efficient, and dependency-free implementation of attention mechanisms in Go. It's particularly useful for:</p>
<ul dir="auto">
<li><strong>Edge Computing</strong>: Zero external dependencies makes it perfect for edge devices where dependency management is crucial</li>
<li><strong>Real-time Processing</strong>: Pure Go implementation ensures predictable performance for real-time applications</li>
<li><strong>Cloud-native Applications</strong>: Efficient batched operations support high-throughput scaling in cloud environments</li>
<li><strong>Embedded Systems</strong>: Predictable resource usage and minimal memory allocations</li>
<li><strong>Production Systems</strong>: Comprehensive error handling and type safety for robust production deployments</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Efficient dot-product attention mechanism</li>
<li>Multi-head attention support</li>
<li>Full transformer layer implementation with:
<ul dir="auto">
<li>Layer normalization</li>
<li>Position-wise feed-forward networks</li>
<li>Residual connections</li>
</ul>
</li>
<li>Batched operations for improved performance</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Future improvements may include:</p>
<ul dir="auto">
<li>Positional encoding implementations</li>
<li>Dropout support</li>
<li>CUDA acceleration support</li>
<li>Additional transformer variants</li>
<li>Pre-trained models</li>
<li>Training utilities</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT License - see LICENSE file for details</p>
<hr>
<p dir="auto">For research inquiries and press, please reach out to <a href="mailto:research@takara.ai">research@takara.ai</a></p>
<blockquote>
<p dir="auto">人類を変革する</p>
</blockquote>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>