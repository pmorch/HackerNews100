<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Nov 2024 11:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Study Reveals Blood Sugar Control is a Key Factor in Slowing Brain Aging (124 pts)]]></title>
            <link>https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/</link>
            <guid>42049418</guid>
            <pubDate>Tue, 05 Nov 2024 07:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/">https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/</a>, See on <a href="https://news.ycombinator.com/item?id=42049418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Age-related brain atrophy, the gradual loss of neurons and shrinkage of brain tissue, is a natural part of aging, which can lead to cognitive decline and other neurological issues. While so far aging cannot be prevented, recent research from an 18-month dietary intervention offers hope that lifestyle and dietary changes can slow brain aging. A new international study, led by Ben-Gurion University of the Negev as part of the DIRECT PLUS Brain MRI trial, has brought to light how blood sugar control can significantly impact brain health.</p><p>Brain age, as evaluated by MRI measurements of the hippocampus and lateral ventricles, reflects the biological aging of the brain, which can differ from a person's chronological age. Chronological age is the number of years lived, while brain age indicates the brain's actual health. Typically, as we age, the hippocampus shrinks and the lateral ventricles expand, serving as markers of brain aging. Some individuals have a brain age younger or older than their chronological age. A younger brain age suggests better cognitive health, while an older brain age may indicate accelerated aging and increased risk of cognitive decline.</p><p>The study, which was published recently in <a rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0002916524007457" target="_blank"><em>The American Journal of Clinical Nutrition 2024</em></a>, was conducted by an international team of brain and nutrition experts, including researchers from Ben-Gurion University, Harvard University, Leipzig University, and more. The research was primarily carried out by PhD student Dafna Pachter and overseen by Prof. Iris Shai, along with several international collaborators.</p><p>A previous study published two years ago (<a rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/35021194/" target="_blank"><em>American Journal of Clinical Nutrition 2022 </em></a><em>)</em>, reported that Mediterranean (MED) and green-MED diets significantly attenuated age-related brain atrophy by ‚àº50% within 18 months.</p><p>In the current study, the researchers aimed to understand the mechanism by which the slowing of brain atrophy occurs.</p><p>The study found that a decline in HbA1c, and key markers of long-term blood sugar levels, are associated with significant positive changes in specific brain regions commonly affected by age-related atrophy. Brain MRI results showed that lower HbA1c levels corresponded to greater deviations in the thalamus, caudate nucleus, and cerebellum ‚Äì areas crucial for cognitive function, motor control, and sensory processing. The study suggests that improved blood sugar control could be one of the most important factors in slowing down age-related brain changes.</p><p><strong>The Green Mediterranean Diet Shows Promise</strong></p><p>Earlier research has highlighted the benefits of the Green Mediterranean (Green-Med) diet, including better blood sugar control. The Green-Med diet is rich in polyphenols from plant-based sources like Mankai (a high-protein aquatic plant) and green tea, while being low in red and processed meats. The current study further strengthens this connection by suggesting that the Green-Med diet may not only support metabolic health but also exert protective effects on brain structure and function.</p><p><strong>DIRECT PLUS Trial ‚Äì One of the Largest Brain MRI intervention Studies in the World</strong></p><p>The DIRECT PLUS trial, one of the longest and largest brain MRI studies conducted to date, involved approximately 300 participants who were divided into three dietary groups. Whole-brain MRI measurements were taken before and after the 18-month trial to track changes in brain health. The researchers used Hippocampal Occupancy (HOC), as a proxy for brain age which predicts future risk of dementia. HOC typically decreases with age. Interestingly, some participants exhibited a brain age either younger or older than their chronological age.</p><p>Using NeuroQuant, an FDA-authorized fully automated tool, the research team quantified and segmented the brain MRI-derived data. The study aimed to examine whether improved glycemic control and specific dietary components could slow down brain aging. The results indicated that participants who managed to improve their blood sugar levels and achieve normal glucose status experienced a more pronounced attenuation of brain aging. Notably, those who consumed higher amounts of green tea and Mankai duckweed shakes demonstrated the most significant improvements in both blood sugar levels and brain health.</p><figure><img src="https://www.bgu.ac.il/media/raeewflp/iris_shai_ip.jpg" alt="" data-caption="Prof. Iris Shai | Photo: Dani Machlis"><figcaption>Prof. Iris Shai | Photo: Dani Machlis</figcaption></figure><p><strong>Glycemic Control and Polyphenols: The Key to a</strong><strong> Younger Brain Age?</strong></p><p>The study‚Äôs lead researcher, Prof. Iris Shai, from Ben-Gurion University, an adjunct professor at Harvard University, and an Honorary Professor at Leipzig University, explains, ‚ÄúMaintaining low blood sugar levels, even within the normal range, shows promise for preserving a younger brain, especially when combined with a healthy diet and regular physical activity. Specifically, polyphenols found in plant-based foods may cross the blood-brain barrier and help reduce brain inflammation, which is crucial for memory‚Äù.</p><figure><img src="https://www.bgu.ac.il/media/jkiddbja/dafna-pachter-ip.jpg" alt="" data-caption="Dafna Pachter | Photo: Courtesy"><figcaption>Dafna Pachter | Photo: Courtesy</figcaption></figure><p>Dafna Pachter, a PhD student and the first author of the paper, adds, "This trial offers a safe approach to potentially slow down our brain aging‚Äîby adopting the components of a green-Mediterranean diet."</p><p><strong>A Pathway to Reducing Age-Related Cognitive Decline</strong></p><p>This study is one of the first large-scale trials to directly link dietary changes, particularly those associated with the Green-Med diet, to improved glycemic control and slower brain aging. While further research is needed to fully understand the mechanisms at play, these results suggest a potential avenue for reducing the risk of age-related cognitive decline through relatively simple dietary adjustments.</p><p>The DIRECT PLUS trial was funded by grants from the German Research Foundation (DFG), Israel Ministry of Health, Israel Ministry of Science and Technology, and the California Walnuts Commission. None of the funding providers were involved in any stage of the design, conduct, or analysis of the study, nor did they have access to the study results before publication.</p><p>The researchers: Dafna&nbsp;Pachter,&nbsp;Alon&nbsp;Kaplan,&nbsp;Gal&nbsp;Tsaban,&nbsp;Hila&nbsp;Zelicha,&nbsp;Anat Yaskolka&nbsp;Meir,&nbsp;Ehud&nbsp;Rinott,&nbsp;Gidon&nbsp;Levakov,&nbsp;Moti&nbsp;Salti,&nbsp;Yoram&nbsp;Yovell,&nbsp;Sebastian&nbsp;Huhn,&nbsp;Frauke&nbsp;Beyer,&nbsp;Veronica&nbsp;Witte,&nbsp;Peter&nbsp;Kovacs,&nbsp;Martin&nbsp;von Bergen,&nbsp;Uta&nbsp;Ceglarek,&nbsp;Matthias&nbsp;Bl√ºher,&nbsp;Michael&nbsp;Stumvoll,&nbsp;Frank B.&nbsp;Hu,&nbsp;Meir J.&nbsp;Stampfer,&nbsp;Alon&nbsp;Friedman,&nbsp;Ilan&nbsp;Shelef,&nbsp;Galia&nbsp;Avidan,&nbsp;and Iris&nbsp;Shai.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia and its partners built a system to bypass U.S. export restrictions (304 pts)]]></title>
            <link>https://twitter.com/kakashiii111/status/1853433531260649532</link>
            <guid>42048065</guid>
            <pubDate>Tue, 05 Nov 2024 02:20:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/kakashiii111/status/1853433531260649532">https://twitter.com/kakashiii111/status/1853433531260649532</a>, See on <a href="https://news.ycombinator.com/item?id=42048065">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Permits Its A.I. Models to Be Used for U.S. Military Purposes (104 pts)]]></title>
            <link>https://www.nytimes.com/2024/11/04/technology/meta-ai-military.html</link>
            <guid>42048009</guid>
            <pubDate>Tue, 05 Nov 2024 02:06:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/11/04/technology/meta-ai-military.html">https://www.nytimes.com/2024/11/04/technology/meta-ai-military.html</a>, See on <a href="https://news.ycombinator.com/item?id=42048009">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/11/04/technology/meta-ai-military.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Pagination widows, or, why I'm embarrassed about my eBook (2023) (161 pts)]]></title>
            <link>https://clagnut.com/blog/2426</link>
            <guid>42047677</guid>
            <pubDate>Tue, 05 Nov 2024 00:58:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clagnut.com/blog/2426">https://clagnut.com/blog/2426</a>, See on <a href="https://news.ycombinator.com/item?id=42047677">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>The physical copies of <a href="https://book.webtypography.net/">my book on Web Typography</a> sold out quickly. I self-published, and print runs are expensive when you‚Äôre funding them yourself, so numbers were limited. However it was always my plan to publish an ebook at the same time, and that has out-sold the hard copy by an order of magnitude.</p>

<p>I set myself some pretty stiff criteria for the ebook‚Äâ‚Äì‚Äâit needed to replicate the design of print edition as far as possible, adapting to the medium when required. To this day I‚Äôm proud of the result. I completely hand-coded the <a href="https://www.w3.org/TR/epub-33/">ePub</a> (meaning it‚Äôs mostly <abbr>HTML</abbr> and <abbr>CSS</abbr> under the hood), and I believe the effort paid off. If you‚Äôll forgive the rather un-British boasting, I still think it‚Äôs one of the more advanced ebooks out there: with embedded fonts, <abbr>SVG</abbr> images, alt text, bold typographic heirarchy, Javascript-driven syntax highlighting and what I hope is a nuanced, highly readable overall design. Not bad for an ebook anyway, although I‚Äôll grant you the bar is not set high (notable exceptions include <a href="https://abookapart.com/">A Book Apart</a> publications).</p>

<p>All hubris aside, I am still frequently embarrassed by how the ebook renders, particularly in Apple Books. Like a well structured webpage, my book uses a lot of headings and subheadings‚Äâ‚Äì‚ÄâI wrote it to be referenced as much as to be read, so this helps the scanability of the text. However Apple Books, and other WebKit, Gecko, or old Blink-powered ebook readers will happily do this to headings:</p>

<figure><img loading="lazy" src="https://clagnut.com/images/2426/ebook-widows.png" alt="Screen shot of Apple Books rendering an ePub"></figure>

<p>Notice the orphaned heading ‚ÄúLean on six centuries of typesetting&nbsp;experience‚Äù with its following paragraph out of sight on the next page. This is a typographic no-no, and has been for‚Äâ‚Äì‚Äâum‚Äâ‚Äì‚Äâsix centuries. Far better for the reader to have the heading attached to its paragraph on the next page, even if that means leaving some redundant whitespace in its place.</p>

<p>Since 1997(!) and the early drafts of <a href="https://www.w3.org/TR/WD-CSS2-971104/page.html#h-12.2">CSS2</a>, there has been an easy way to tell browsers not insert a page break directly after, or in the middle of, a heading:</p>

<figure><div data-element="code-block"><pre><code>h2 {
    page-break-after: avoid;
    page-break-inside: avoid;
}
</code></pre></div></figure>



<figure><div data-element="code-block"><pre><code>h2 {
    break-after: avoid;
    break-inside: avoid;
}
</code></pre></div></figure>

<p>However 26 years later, <code>break-after:avoid</code> is still not supported by either Safari or Firefox, and was only <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=223068#c54">introduced to Chrome 108</a> in December 2022. I‚Äôve put together a <a href="https://codepen.io/clagnut/pen/MWZRBaM">test for support of <code>break-after</code> and <code>break-inside</code></a> in multi-column layout. Have a play with it in Chrome‚Äâ‚Äì‚Äâtry removing <code>break-inside:avoid</code> and then <code>break-after:avoid</code> from the <code>h2</code> rule in the <abbr>CSS</abbr> and you should see how the subheadings end up at the bottom of a column, or worse still, split over two columns.</p>

<p>Browser support for <abbr>CSS</abbr> properties tends to follow demand from web developers. Unlike in 1997‚Äâ‚Äì‚Äâor indeed 2017‚Äâ‚Äì‚Äâthere is now an annual Interop arrangement between browser rendering engine makers in which they agree a common list of priorities for <abbr>CSS</abbr> and other web technologies. Interop 2024 has just closed for new proposals. Unfortunately I didn‚Äôt manage to submit a request in time for breaking controls to be universally implemented. Thankfully Scott Kellum of <a href="https://typetura.com/">Typetura</a> did put in a <a href="https://github.com/orgs/web-platform-tests/projects/3/views/1?pane=issue&amp;itemId=40510069">proposal for advanced multi-column layouts</a> to be improved, and this included support for <code>break-</code> properties. Sadly there‚Äôs little to no clamour for it from other developers‚Äâ‚Äì‚Äâthe blog post you‚Äôre reading probably doubles the published demand, and that‚Äôs just for within columns.</p>

<p><ins><strong>Update:</strong> Annoyingly the proposal <a href="https://github.com/web-platform-tests/interop/issues/520#issuecomment-1921897115">was not selected</a> for Interop 2024.  I'll just have to keep prodding the bug reports and keep my fingers crossed they are fixed soon‚Äâ‚Äì‚Äâ<strong>these bugs are older than some of my colleagues!</strong></ins></p>

<p>Paged media is very much a forgotten aspect, and it‚Äôs probably true that web pages are rarely printed in the grand scheme of things, however ebooks are definitely a popular form of paged media and deserve attention. I‚Äôd certainly like to read ebooks without failed typographic fundamentals.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Forest Service Decision to Halt Prescribed Burns in CA Is History Repeating (279 pts)]]></title>
            <link>https://cepr.net/us-forest-service-decision-to-halt-prescribed-burns-in-california-is-history-repeating/</link>
            <guid>42046596</guid>
            <pubDate>Mon, 04 Nov 2024 22:19:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cepr.net/us-forest-service-decision-to-halt-prescribed-burns-in-california-is-history-repeating/">https://cepr.net/us-forest-service-decision-to-halt-prescribed-burns-in-california-is-history-repeating/</a>, See on <a href="https://news.ycombinator.com/item?id=42046596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-wrap">
    <!-- <ul id="article-breadcrumb">
        <li>Our Work<span>‚Ä∫</span></li>
        <li>US<span>‚Ä∫</span></li>
        <li>Workers</li>
    </ul> -->
    <div id="article-body">
            <div id="article-body-top-wrap">
                    <!-- <img src="/images/articles/article-headshot.jpg"> -->                        
                                            <h4>October 28, 2024</h4>    
                        
                                            
                        
                </div>
            <div id="article-body-inner">
                <div id="art-body-desc">
                        <p>Last week, the US Forest Service announced it would stop prescribed burning in California ‚Äú<a href="https://www.kqed.org/science/1994972/forest-service-halts-prescribed-burns-california-worth-risk">for the foreseeable future,</a>‚Äù stating that the decision was made as a precautionary measure to ensure the availability of staff and equipment in case of potential wildfires. But temps are falling across California, and state, tribal authorities, and prescribed burn associations have commenced with their prescribed burns. If the federal agency doesn‚Äôt hold up its end of the work, all that mitigation work can be undone.</p>
<p>To grasp the impact of the Forest Service‚Äôs decision on California, it‚Äôs essential to understand the history of the state and the intricate mosaic of private, state, and federal land that constitutes the forests. Over the past 100 years, the state and federal governments relied on a ‚Äú<a href="https://www.ucpress.edu/books/tending-the-wild/paper">paramilitary-like program</a>‚Äù that focused on fire suppression by rapidly mobilizing firefighters and equipment. Very little was done regarding fire prevention besides creating the famous Smokey Bear ad campaign. One of the problems was that colonialist attitudes of fire officials constantly disregarded the valuable knowledge of forest management practices held by California‚Äôs Indigenous communities. One such practice is prescribed burning, which involves intentionally setting controlled fires to remove dry vegetation that could serve as ‚Äúladder fuel,‚Äù allowing wildfires to spread to taller vegetation. Without this mitigation work, the buildup of vegetation and increasing average global temperatures has created the conditions for the mega-wildfires we see in the West today.</p>
<p>These destructive fires have over time shifted state and federal policies toward wildfire mitigation. California‚Äôs recently enacted 2024-25 budget includes <a href="https://ebudget.ca.gov/2024-25/pdf/Enacted/BudgetSummary/NaturalResourcesandEnvironmentalProtection.pdf">$4.2 billion and 12,512 positions</a> for CAL FIRE, which provides resource management and fire protection services across 31 million acres of state forests. However, California relies on the federal government to maintain areas outside state and local control, mainly the 20 million acres of National Forests managed by the Forest Service. The Forest Service, like many other federal agencies, lacks stable long-term funding because the continuing resolution process constantly threatens its programs. In the past, these issues have had dire consequences for Californians.</p>
<p>In 2021, the Caldor Fire started in the El Dorado National Forest, and the Forest Service knew this region was at risk. The agency had created a model two decades before the fire predicting the danger and proposed the Trestle Project in 2013 to manage federal land near the town of Grizzly Flats. The project was expected to be completed by 2020. However, an <a href="https://www.capradio.org/articles/2022/08/16/how-we-measured-us-forest-service-wildfire-prevention-work/">investigation</a> by CapRadio and the California Newsroom discovered only 2,137 acres of the planned 15,000, or 14 percent of the project, had been completed before August 14, 2021. That was the day the Caldor Fire started, and 48 hours later, the fire had leveled Grizzly Flats.</p>
<p>According to<a href="https://www.npr.org/2022/09/26/1125172945/forest-service-fell-short-of-executing-plan-to-protect-town-from-fire-probe-find"> NPR</a>, the Trestle Project faced ‚Äústaffing shortages, pushback from environmental groups, too many days when prescribed burns would be dangerous due to hotter, drier conditions caused by climate change,‚Äù and, most importantly, a lack of funding. Unfortunately, the Trestle Project‚Äôs setbacks left Grizzly Flats and other El Dorado County communities vulnerable to the scenario the Forest Service had modeled.&nbsp; Tellingly, as the fire moved east toward Lake Tahoe, the community of Kirkwood was <a href="https://snowbrains.com/kirkwood-mountain-resort-no-fire-damage-caldor/">spared</a> thanks to an <a href="https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fseprd741953.pdf">escaped prescribed burn</a> in 2019.</p>
<p>This scenario shows what happens when Congress is less committed than California to tackling forest management. With wildfire management funding constantly tied up in unpredictable budget debates, the current state-federal partnership is fragile and based on the whims of the legislative and executive branches, which can withhold funding based on which political party is currently in power. The Forest Service‚Äôs latest decision is the consequence of these issues.</p>
<p>Congress must commit to forest management by providing stable and reliable funding for wildfire prevention. Forest management is not a partisan issue ‚Äì it is a matter of public safety, environmental protection, and economic sustainability. It‚Äôs well past time to put politics aside and cooperate with California and other fire-prone states to ensure that we provide the resources needed to manage forests, reduce the danger of wildfires, and safeguard communities.</p>
<p>EDIT: The original version of this article referred to Smokey Bear as Smokey the Bear. Officially, the mascot is named Smokey Bear. In popular culture, the ‚Äúthe‚Äù was added by a song in 1952 by Steve Nelson and Jack Rollins and stuck with the general public.</p>
                </div>
                 
                <div id="article-authors-wrap">
                            <p><a href="https://cepr.net/staff-member/matt-sedlar/"><img src="https://cepr.net/wp-content/uploads/2019/11/matt-thumb.jpg"></a></p><div>
                                                                    <h2><span><a href="https://cepr.net/staff-member/matt-sedlar/">Matt Sedlar</a></span> | Climate Analyst</h2>
                                                                
                                                                    <div>
                                        <p>Matt Sedlar is an analyst on topics such as climate change, disaster, and housing.</p>

                                    </div>
                                 
                                    
                                <p><a href="https://cepr.net/staff-member/matt-sedlar/">
                                                                            Read Full Bio
                                                                    </a></p>
                            </div>
                        </div>  
            </div>
        </div>
             <div>
        <p>
            <h2>Support Cepr</h2>
            <h2>APOYAR A CEPR</h2>
            <h4>If you value CEPR's work, support us by making a financial contribution.</h4>
            <h4>Si valora el trabajo de CEPR, ap√≥yenos haciendo una contribuci√≥n financiera.</h4>
        </p>
        <p>
            <a href="https://cepr.net/donation">
            	<span>Donate</span>
            	<span>Ap√≥yanos</span>
            </a>
        </p> 
        <p><img src="https://cepr.net/wp-content/themes/cepr/images/press-releases/heart.png">   
    </p></div>
    <div id="article-signup-wrap">
            
            <h2>Keep up with our latest news</h2>
            
        
            
        <p><img src="https://cepr.net/wp-content/themes/cepr/images/articles/mailbox.png">
    </p></div> 
    <div id="related-article-sec">
                             
                    <h2>Related Articles</h2>
                    
                <!-- English -->
                                                        <!---->
                                                    
                            
                                        <!---->
                                                    
                            
                                        <!---->
                                                    
                            
                        
                                    
                <!-- Spanish -->
                            </div> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Albertsons kills rural grocers with land use restrictions (246 pts)]]></title>
            <link>https://www.thebignewsletter.com/p/how-albertsons-kills-rural-grocers</link>
            <guid>42046196</guid>
            <pubDate>Mon, 04 Nov 2024 21:32:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thebignewsletter.com/p/how-albertsons-kills-rural-grocers">https://www.thebignewsletter.com/p/how-albertsons-kills-rural-grocers</a>, See on <a href="https://news.ycombinator.com/item?id=42046196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>One of the best parts of antitrust trials is how much information comes into the public domain about corporations that usually keep details about their industry private. The Kroger-Albertsons supermarket merger case is no different. One interesting nugget is that supermarket executives sees rural markets as particularly easy to monopolize, because there is often just one store. They even have a name, ‚Äúno-comp[etition] or low-comp[etition] zones,‚Äù according to one executive on the stand.</p><p>Of course that makes sense, we‚Äôd expect firms to maximize profits where they can. One might be tempted to say, well, there are some towns that can‚Äôt support more than one store. And that might be true, except that there are several examples of supermarket chains using tactics in such towns to thwart the opening of competition. How? Well, they find a way to dominate the existing plots of land and building suitable for such a store.</p><p><span>In June, for instance, Washington state Attorney General Bob Ferguson, who is also litigating against the merger, </span><a href="https://www.supermarketnews.com/legislation-regulatory-news/albertsons-lifts-land-use-restriction-in-washington" rel="">fined</a><span> Albertsons $25,000 for imposing a land use restriction on a store it sold in 2018 in a low-income section of Bellingham, Washington. As part of the sale, the supermarket giant put a requirement on the deed that no grocery store could open there until 2038. Ferguson found this provision was a violation of the state antitrust law.</span></p><p>These kinds of land use restrictions are likely common. A few months ago, I got an email from an economist explaining how Albertsons abuses its monopoly power in a small skiing town in rural California using a similar strategy. </p><blockquote><p>I‚Äôm in Mammoth, CA. The town needs more grocery, but they‚Äôre limited on land. There‚Äôs a vacant Kmart that is desirable, BUT the local Vons grocery (which is owned by Albertsons) is leasing the old Kmart of keep it vacant (to the tune of $750k annually). The community is outraged, but feels they cant do anything‚Ä¶</p><p>It‚Äôs so apparent walking in the Vons in that region that customer service is dismissed. Workers aren‚Äôt happy, store shelves aren‚Äôt stocked, and the general messiness is simply undignified.‚Äù</p></blockquote><p><span>In Mammoth Lakes, there are two old K-Mart lots that could easily welcome competitive grocery stores, but both are foreclosed by Albertsons. According to </span><a href="https://www.easternsierranow.com/whats-bugging-you-eastern-sierra-oh-tenants-where-art-thou/" rel="">the mayor</a><span>, there are deed restrictions that ‚Äúdeter big box retailers such as Target/Walmart, and basically completely block grocery retailers.‚Äù According to a different media outlet, the prohibition is not in the deed, but is a direct leasing arrangement. Vons </span><a href="https://thesheetnews.com/2019/11/15/bishop-kmart-to-close/" rel="">started</a><span> leasing one plot in 2019 when K-Mart went under, ‚Äúholding the space hostage.‚Äù It now leases the other as well. It doesn‚Äôt actually matter which strategy is involved, both are intended to restrict new grocery stores so there‚Äôs no competition.</span></p><p>And it looks as if Albertson‚Äôs monopolization strategy has worked. Here are the first several Google reviews about the local Vons, along with pictures.</p><blockquote><p>‚ÄúThe most expensive Vons in California is also in the worst physical condition,‚Äù wrote one reviewer. ‚ÄúWorn and missing floor tiles, empty shelves, rude employees etc‚Ä¶‚Äù</p><p>‚ÄúThis is effectively the only grocery store in town and seems to have a hard time keeping items in stock‚Ä¶‚Äù</p><p>‚ÄúThere were a lot of bare shelves and the vegetables looked well picked over. Got the 6 items that I wanted and tried to pay. Self check-out counters were all closed. There was only one cash register light on and NOBODY was there‚Ä¶‚Äù</p><p>‚ÄúDefinitely the worst and dirtiest Vons I've ever seen‚Ä¶ It's clear the owners operate on the "only game in town" ethos. Losers...‚Äù</p><p>‚ÄúAlways a terrible experience at this store. Limited selection of items and saying the older female checker is surly is an understatement. She doesn't understand Vons and Safeway are the same, yelled at me for adding items to our order, refused to honor the digital coupon prices and told us to shop at Grocery Outlet‚Ä¶‚Äù</p><p>‚ÄúConsensus among locals is that everything purchased here basically rots within a day or so of purchase‚Ä¶‚Äù</p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png" width="728" height="769" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:769,&quot;width&quot;:728,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:857321,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2df8c1e-b62d-4294-a964-9e573780b8de_728x769.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png" width="536" height="778" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:778,&quot;width&quot;:536,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:819965,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86bd5276-890f-4b41-94c7-9bccad131e9d_536x778.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Land use restrictions are common in the supermarket industry, because it costs about $10 million and requires a good plot of land to open a new store. It‚Äôs much easier to open a store in a building already suitable for a grocery store than to build something from scratch. Conversely, if you have the only store in town, it‚Äôs much easier to make cash by paying to keep an empty lot from being filled by a rival, than to lower your prices and improve service.</p><p>Some things are obvious, like land use restrictions to thwart competition. But they require a high-profile merger trial to come to light, nonetheless.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insomnia, but not lack of sleep, may hasten brain shrinkage: study (103 pts)]]></title>
            <link>https://www.ucsf.edu/news/2024/10/428701/poor-sleep-midlife-linked-faster-brain-atrophy</link>
            <guid>42045658</guid>
            <pubDate>Mon, 04 Nov 2024 20:23:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ucsf.edu/news/2024/10/428701/poor-sleep-midlife-linked-faster-brain-atrophy">https://www.ucsf.edu/news/2024/10/428701/poor-sleep-midlife-linked-faster-brain-atrophy</a>, See on <a href="https://news.ycombinator.com/item?id=42045658">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For adults in midlife, difficulty getting to sleep and waking up too early may accelerate brain atrophy that is associated with dementia.</p>

<p>The brain naturally begins to atrophy beginning in one‚Äôs 30s and 40s. But this can become much more pronounced with dementia and disorders like traumatic brain injury, stroke and, in some cases, multiple sclerosis.</p>

<blockquote>
<p>Even if the cause of dementia is unrelated to sleep, it‚Äôs possible that poor sleep may advance or exacerbate cognitive symptoms.‚Äù</p>


</blockquote>

<p>To estimate the effects of sleep quality on the brain, the researchers surveyed approximately 600 adults on how well they slept. The participants were asked the same questions five years later and underwent brain scans 10 years after this.</p>

<p>The researchers used machine learning, leveraging data from the scans, to estimate the brain age of each participant based on the actual degree of brain shrinkage. They found that even after adjusting for age, sex, education, health and lifestyle factors, the brains of those participants who slept poorly were more atrophied than those who slept well.</p>

<p>Compared to the 70% of the sample who reported having little trouble sleeping, those with moderate difficulty (22%) had brains that were 1.6 years older, while those with the most difficulty (8%) had brains that were 2.6 years older.</p>

<p>The study publishes in <em><a href="https://www.neurology.org/doi/10.1212/WNL.0000000000209988">Neurology</a></em> on Oct. 23.</p>

<p>‚ÄúWhile we can‚Äôt say that poor sleep causes dementia, earlier research has established an association,‚Äù said first author <a href="https://profiles.ucsf.edu/clemence.cavailles">Cl√©mence Cavaill√®s</a>, PhD, of the UCSF Department of Psychiatry and Behavioral Sciences. ‚ÄúEven if the cause of dementia is unrelated to sleep, it‚Äôs possible that poor sleep may advance or exacerbate cognitive symptoms.‚Äù</p>

<p>The participants, who were 40 years old on average when they enrolled, were part of the Coronary Artery Risk Development in Young Adults (CARDIA) study, which investigates the factors that influence cardiovascular disease, as well as how they may relate to dementia. They were asked about short sleep duration, poor quality of sleep, difficulty getting to sleep, difficulty staying asleep, early morning awakening and daytime sleepiness.</p>

<h2>Lack of sleep not a factor ‚Ä¶ or is it?</h2>

<p>Surprisingly, lack of sleep was not one of the characteristics relating to brain aging. ‚ÄúSleep problems frequently emerge in midlife,‚Äù said Cavaill√®s. ‚ÄúThey‚Äôre likely due to work stress, family caregiving and menopause,‚Äù she said. ‚ÄúAlthough the study found inadequate sleep duration was not an issue in brain atrophy in this study, we cannot say there is no association,‚Äù she said, noting that a previous CARDIA study showed that shorter sleep was associated with worse white matter integrity, indicating lower cognitive functioning.</p>

<blockquote>
<p>Future research should investigate the life-course effect of sleep on brain health in young populations.‚Äù</p>


</blockquote>

<p>Poor sleep that persisted over five years, especially when it related to difficulty getting to sleep and early morning awakening, was found to be highly relevant to brain aging.</p>

<p>‚ÄúThe study shows that poor sleep could be a target for early interventions to prevent possible cognitive decline,‚Äù said senior author <a href="https://profiles.ucsf.edu/kristine.yaffe">Kristine Yaffe</a>, MD, of the UCSF departments of Psychiatry and Behavioral Sciences, Neurology, and Epidemiology and Biostatistics.</p>

<p>‚ÄúPublic health initiatives could include emphasizing sleep quality over quantity in midlife ‚Äì and recognizing the long-term impact of poor sleep quality on the brain,‚Äù added Yaffe, who is a member of the first team of experts to determine that 30% of dementia risk is preventable. ‚ÄúFuture research should investigate the life-course effect of sleep on brain health in young populations.‚Äù</p>

<p><strong>Co-Authors:</strong> <a href="https://profiles.ucsf.edu/christina.dintica">Cristina Dintica</a>, PhD, and <a href="https://profiles.ucsf.edu/yue.leng">Yue Leng</a>, PhD, of UCSF; Mercedes R. Carnethon, PhD, of Northwestern University; and Mohamad Habes, PhD, of the Glenn Biggs Institute for Neurodegenerative Disorders and the University of Texas Health Science Center.</p>

<p><strong>Funding:</strong> The CARDIA study is conducted and supported by the National Heart, Lung, and Blood Institute in collaboration with the University of Alabama at Birmingham (75N92023D00002 &amp; 75N92023D00005), Northwestern University (75N92023D00004), University of Minnesota (75N92023D00006) and Kaiser Foundation Research Institute (75N92023D00003). Also supported in part by (NIA) R35AG071916 and (NIA) R01AG063887. The authors report no relevant disclosures.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machines of Loving Grace (149 pts)]]></title>
            <link>https://www.clunyjournal.com/p/machines-of-loving-grace</link>
            <guid>42045509</guid>
            <pubDate>Mon, 04 Nov 2024 20:05:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.clunyjournal.com/p/machines-of-loving-grace">https://www.clunyjournal.com/p/machines-of-loving-grace</a>, See on <a href="https://news.ycombinator.com/item?id=42045509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg" width="1456" height="2053" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2053,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16961853,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f73bae3-4bb1-4da8-a3a0-1bf68ef71239_8334x11750.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Henry Alan Dragon, May 23, 2024, 1:26 pm.</figcaption></figure></div><p>Last year, at a Panera-catered ‚Äúnew faculty appreciation‚Äù luncheon, the Dean of my college told me‚Äîas I folded my hands across my seven-months-pregnant belly‚Äîthat I should look for long-term academic employment elsewhere. My research and practice was not centered enough on ‚ÄúAI‚Äù and ‚Äúemerging technology‚Äù to fit within the institution; they wanted someone with more of an explicit emphasis on tech. My work in publishing, writing, web-design‚Äîand even my reproductive state‚Äîcouldn‚Äôt possibly be separated from these things, but, more than any other effects AI may have on humanity in the future, this is one of the main effects now: people feel compelled to talk about it, and it‚Äôs a good way for sclerotic institutions to feel like they‚Äôre keeping up with the times. I smiled, thanked him, and left with an M&amp;M cookie.&nbsp;</p><p><span>Earlier this month, at the recommendation of a colleague, I registered to attend a Zoom seminar that boasted a discussion on ‚Äúthe future of design education and academia.‚Äù As the Zoom window expanded, I was met with a looping Pornhub threesome montage next to a side-screen grid of six or so designers front-facing the camera. The hosts clicked around, grunting and gasping as the camera cut to new angles and positions. I opened my department group chat and texted my colleagues </span><em>OH NO. DO NOT JOIN THE CALL.</em><span> The colleague who recommended the talk echoed me: </span><em>DO NOT LOG IN! SOMEONE Hacked their Zoom.</em></p><p>The hosts spoke to each other saying things like:</p><p><em>I think it‚Äôs you.&nbsp;</em></p><p><em>Wait. Oh wait. No.&nbsp;</em></p><p><em>This type of thing happened all the time during Covid. So annoying.</em></p><p><em>Who is *bot-generated-username*?&nbsp;</em></p><p>It was a long two minutes before someone finally had the sense to end the call.&nbsp;</p><p><span>A few hours later, a nearby university, who employs one of the guest speakers, posted a carousel of screenshots on their Instagram page from the talk, which had apparently reconvened. The description read </span><em>Thanks to our Program Director ************** for an amazing panel discussion on the future of design education &amp; academia today!</em><span> üöÄ</span></p><p>I tried to follow up with the panel about what happened, and received no reply.&nbsp;</p><p>The way this talk was handled reminded me that technology, for all its promise of connection and innovation, can have real and tangible repercussions if we‚Äôre not careful‚Äîit illuminates our fundamental responsibility to one another.</p><p><span>In ‚ÄúA Rant about Technology,‚Äù Ursula K. Le Guin says that technology is </span><em>how a society copes with physical reality</em><span>. That it </span><em>is the active human interface with the material world.</em><span> Technology is also a spoon. Technology is also language. Yet, I am shocked when AI does not stop Outlook Messenger from bursting confetti across my desktop in response to the </span><em>Congratulations</em><span> reply I receive when HR misreads my email about both adding and removing our late son from my health insurance.&nbsp;&nbsp;</span></p><p>As a 6th grader in a Dallas suburb, I attended a school assembly where a traveling speaker talked to us about sex. He said that sex was like fire. It was a life force. The discovery of fire sparked a massive advance of civilization. He told us that if we were not careful with fire, if we did not watch it closely and respect the power that it held, we could burn down a whole village. He told us to start a fire someday, but only with someone who you trusted to tend to it while you slept. I believe now that this same sort of diligence must also be extended to our use of technology.&nbsp;</p><p><span>This year I experienced many technological interventions to my body, as many do when they become pregnant. Of all the new ways to be measured and monitored, my favorite was when my sister-in-law, due just a few weeks before me, guessed my midsection's exact circumference with a piece of string. My least favorite was the transvaginal ultrasound. My OB in Virginia had a sort of nonchalance and slapstick humor to him that initially made me feel comfortable in his office. The temperature changed during my nine week appointment when he, quite unexpectedly, shook the ultrasound wand inside of me (with vigor!) in an attempt to wake the baby for the pictures. He mutter-yelled </span><em>Move little bugger. Move! </em><span>I looked to the nurse witness, who looked forward at the door. We received a letter in December that his practice would be closing in April. We received another at the end of summer telling us that he had died.</span></p><p><span>During this same pregnancy, another doctor does imaging on me, externally. With the wand in his hand in the almost pitch black room he tells us that there are two small and one moderate-sized holes in our son‚Äôs heart and that he will need to be rushed to surgery as soon as he is born. I am still fully reclined and covered in gel. He tells us to come back in two weeks. On his walk out of the room he hands us an information pamphlet where he drew on the back two versions of the human heart‚Äîone </span><em>normal</em><span>, one </span><em>our situation</em><span>.&nbsp;</span></p><p><span>Because of this experience, we transfer care to Massachusetts, where my husband‚Äôs family lives and works in the medical field. We navigate this with someone who is known as ‚ÄúThe worldwide head of second opinion.‚Äù At the new hospital the gel is cleared off my belly each time. I cry in the ultrasound room as the video of the baby in my belly is labeled ‚Äúpractice breathing.‚Äù The sonographer walks us to a conference room where we‚Äôre met with cardiac specialists, social workers and obstetrics. Our meetings are prefaced with </span><em>This is going to be a hard conversation</em><span> and when we stand up to leave the room, we are encouraged to take the entire box of tissues with us. Someone even goes to the cupboard to get us a new box, of superior quality and softness.&nbsp;</span></p><p><span>We are told that we may not get to meet him, but we do. The windowless OR looks perfectly encapsulated in pure even daylight. We have a team of thirty people and everyone knows the choreography. The anesthesiologist tells me that I may feel a shock of sorts before she administers a liquid into my spine. I am beaming. I feel strong and I wait for the shock. I squeal. The staff echos my laughter when I tell them that it feels like I sat on a firecracker. They had not heard that one before. The team yells out </span><em>Happy Birthday Henry!</em><span> as he is lifted from my abdominal cavity, screaming and alive. The anesthesiologist tells us with urgency to touch him and she guides our hands to meet his head through the dividing screen. I keep my fingers at the place he fogged the plastic as he is carried away to receive care.&nbsp;</span></p><p>While my layers are being stitched up, my husband notices that the TVs near the ceiling are screening the surgery from above. When a nurse sees him watching, they switch off the monitors. The birth was photographed by our social worker, who printed and laminated the images. A whole stack is waiting for us in our room on the meal tray when we return.</p><p>When I see him next, there is a whole room dedicated to beating his heart. I am wheeled to his side and told not to worry about the beeping, it is just an indication of a new switch to flip. That it is not as time sensitive as it sounds. Henry wraps his hand around my finger, then around the beak of his woodpecker Jellycat. He is on a high platform and adorned in wires and tubing like some sort of cybernetic king. One part of the room-machine spins constantly. It whirls and re-oxygenates the donor blood that has exited him in a slightly less vibrant shade of red than what entered.&nbsp;</p><p>He fights for two days. On his third, when he chooses peace, it is the quietest room I have ever endured. The cardiologist encloses us with the curtain and lets us stay as long as we can.&nbsp;&nbsp;</p><p>We have a mass in Henry‚Äôs name. Afterward, family comes over. They bring food and two new pink babies. I hold my niece and keep checking that my hair does not wrap around her fingers. Our dog screams in the face of the two-year-old cousin, who seems completely unaffected. He says ‚Äúdog‚Äù and pats him. He points to the angel on the mantle and says ‚ÄúAmen.‚Äù We set up a small box in the living room with photographs printed out from the hospital. As his mother, I feel protective of his image. I do not want people to look at him on their phones. I want to be in control of how he is seen in his corporeal form. I want people to sit on the couch and hold him, looking together at the same time and passing him around as they would any other baby. I become upset when I log in to Facebook for the first time in years, to sell a lamp, and see that some of our pictures from the hospital were posted to the family‚Äôs facebook group. I stew on this for a while, then release it when my husband reminds me about when they were posted, his first good day on the ECMO machine. We were two proud normal parents. They were asking the family for prayers.&nbsp;</p><p>During the next three months, I look at his pictures for fifteen minutes every three hours to trigger a milk letdown. I imagine kissing his head and can still taste his baptismal holy water on my lips. Smell the perfumed oil. I nurse a small machine all summer and a refrigerated car comes by to pick up the milk that will be processed, divided, and sent off to babies I will never know. My husband wakes up in the middle of the night to wash the flanges, label the bags, and place them in the freezer. At home, we keep a white noise machine of the ocean waves playing in the bedroom at all times. We have done this for years, but I only just now am paying attention to it. When my dog unplugs it, I notice its absence.</p><p>Earlier this spring my department chair threw me a baby shower. My students ate cupcakes and giggled as we failed to answer baby related pop-culture trivia from the 80‚Äôs. At the end of summer I send them a sad email to thank them for their patience last year and I ask for it again. This fall I sit in class with them, smiling as laughter bubbles from the corners of their mouths in response to the recording I play of Richard Brautigan reciting this poem:&nbsp;&nbsp;</p><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre><em>I like to think (and
the sooner the better!)
of a cybernetic meadow
where mammals and computers
live together in mutually
programming harmony
like pure water
touching clear sky.

I like to think
(right now, please!)
of a cybernetic forest
filled with pines and electronics
where deer stroll peacefully
past computers
as if they were flowers
with spinning blossoms.

I like to think
(it has to be!)
of a cybernetic ecology
where we are free of our labors
and joined back to nature,
returned to our mammal
brothers and sisters,
and all watched over
by machines of loving grace.</em></pre></div><p>I took part in another Zoom call this week. Henry‚Äôs cord blood was collected for research and after five months, the results came back. They were able to find the exact single gene mutation that caused his condition, and explained everything. They assured us that it was not our well water, or the time I had a fever for 15 minutes, or leptospirosis from cat-sitting, or that I ate an underdone yolk. The doctors said that with the variation he had, they were surprised that he was able to make it to two days old. My friend Mary texted me a message her late father used to say about heaven.&nbsp;</p><p><em>We are there before we‚Äôre born and God asks us if we want to come down to Earth even though it will be painful and hard. Seems Henry wanted badly to be yours and to meet you if only for a single shining day.&nbsp;</em></p><p>Neither my husband nor I carry this gene. And the doctors tell us that the same thing is unlikely to repeat. They clear us to try again this year.&nbsp;</p><p><span>Lately, my husband and I have been watching a lot of TV together. Football when we‚Äôre grading so we don‚Äôt have to see the Patriots lose. And movies at night. There is a scene in the film </span><em>Magnolia</em><span> by Paul Thomas Anderson where frogs begin falling from the sky. They drop to the pavement violently, cars crash, and there is a little boy sitting in a library at a desk looking out the window cracking a half smile in wonder. Shadows of amphibious bodies cast on the wall behind him as he says out loud </span><em>This happens. This is something that happens.&nbsp;</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blog Writing for Developers (2023) (140 pts)]]></title>
            <link>https://rmoff.net/2023/07/19/blog-writing-for-developers/</link>
            <guid>42045477</guid>
            <pubDate>Mon, 04 Nov 2024 20:01:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rmoff.net/2023/07/19/blog-writing-for-developers/">https://rmoff.net/2023/07/19/blog-writing-for-developers/</a>, See on <a href="https://news.ycombinator.com/item?id=42045477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
		
<article>
	<p>Writing is one of the most powerful forms of communication, and it‚Äôs useful in a multitude of roles and contexts. As a <a href="https://rmoff.net/">blog-writing</a>, <a href="https://github.com/treeverse/lakeFS/pulls?q=is%3Apr+label%3Adocs+author%3Armoff+">documentation-authoring</a>, <a href="https://twitter.com/rmoff/status/1587382202781913089">twitter-shitposting</a> DevEx engineer I spend a lot of my time writing. Recently, someone paid me a very nice compliment about a blog I‚Äôd written and asked how they could learn to write like me and what resources I‚Äôd recommend.</p>
<p>Never one to miss a chance to write and share something, here‚Äôs my response to this :)</p>
<p>To begin with I want to cover briefly the motivations behind writing.</p>
<h2 id="why-do-i-write">Why Do <strong>I</strong> Write?&nbsp;<a href="#why-do-i-write">üîó</a> </h2>
<p>Firstly, I like <strong>to share information</strong>. That could be a new <a href="https://rmoff.net/2021/03/04/quick-profiling-of-data-in-apache-kafka-using-kafkacat-and-visidata/">tool</a> or <a href="https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/">technique</a> that I‚Äôve learnt, <a href="https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/">a clever trick</a> I‚Äôve discovered, or sometimes away from the technical and into the realms of <a href="https://rmoff.net/2019/02/09/travelling-for-work-with-kids-at-home/">life pondering</a> and <a href="https://rmoff.net/2023/05/23/what-does-this-devex-engineer-do/">navel gazing</a>. In the case of this very blog, it‚Äôs to share my thoughts on something that interests me. I could have written some notes and sent them directly back to the person who asked the original question, but if it was useful to them it‚Äôs hopefully useful to others‚Äîso therefore it‚Äôs worth writing up and publishing.</p>
<p>The second reason that I‚Äôll write is <strong>to learn about something</strong>. It‚Äôs one thing to hand-wave one‚Äôs way through a presentation. It‚Äôs another to commit pen to paper (well, bytes to disk) and <a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/">explain something</a>. Quite often I‚Äôll realise that there‚Äôs a gap‚Äîor gaps‚Äîin my knowledge that I need to explore first before I can properly write about something, and that‚Äôs the very reason that I do it.</p>
<p>There are several pleasant side-effects from writing too. Anything in the public domain (such as your blog, but also open-source project documentation, etc) helps establish your credibility in an area and awareness by others of you. We may never reach the stratospheric heights of someone such as Kelsey Hightower, who has wowed a generation of developers with their <a href="https://youtu.be/HlAXp0-M6SY?t=718">Tetris-playing skills</a>, but being known as <em>that guy</em> who wrote a really useful blog that helped others is still a really nice feeling :)</p>
<h2 id="how-do-i-write-for-developers">HOW do I Write for Developers?&nbsp;<a href="#how-do-i-write-for-developers">üîó</a> </h2>
<h3 id="-stop-watch-this-first-">üõë STOP! Watch This First üé•&nbsp;<a href="#-stop-watch-this-first-">üîó</a> </h3>
<p>Go and watch this excellent lecture called <a href="https://www.youtube.com/watch?v=vtIzMaLkCaM">The Craft of Writing Effectively</a>. It‚Äôs given by Larry McEnerney who is the Director of the University of Chicago‚Äôs Writing Program and knows a thing or two about writing. There are direct parallels between his observations on how and why academics communicate, and communication between developers.</p>
<p>üëâüèª I‚Äôve seen it recommended several times but to my embarrassment, the length put me off‚Äîbut I wish it hadn‚Äôt as it‚Äôs superb.</p>
<p><em>If you‚Äôd rather listen instead of watch you can use <a href="https://github.com/yt-dlp/yt-dlp"><code>yt-dlp</code></a> to download it as audio (<code>--extract-audio --audio-format mp3</code>).</em></p>
<h3 id="so-how-do-i-write-for-developers">So, how do I write for developers?&nbsp;<a href="#so-how-do-i-write-for-developers">üîó</a> </h3>
<p>Each writer will have their own approach to writing, and it will vary based on the audience and purpose too. A report for publication in an academic journey will have a different structure to a shitpost on Twitter. A blog aimed at developers will read very differently from the documentation from the depths of a product manual. Each medium and audience is valid; the knack is making sure that your writing lines up with it.</p>
<p>When I write I try to write for myself‚Äîa developer, interested in a thing. That could be a new technology, an in-depth explanation, a random musing on life, or anything else. Would I like to read the thing I‚Äôve read? Does it avoid the pitfalls that plague the soulless bland crap that some companies churn out, stick an emoji on, and call developer marketing?</p>
<p>There are three key dimensions that it‚Äôs useful to consider here:</p>
<ul>
<li>clarity</li>
<li>personality (also called voice)</li>
<li>uniformity of content.</li>
</ul>
<p>You can roughly overlay these dimensions across the range of written materials that we might write:</p>
<img src="https://rmoff.net/images/2023/07/01.svg" width="600/">
<p>Things aren‚Äôt always so simple, and for some platforms in particular there‚Äôs quite a range:</p>
<img src="https://rmoff.net/images/2023/07/02.svg" width="600/">
<p>What do these different dimensions mean in practice? Let‚Äôs explore that.</p>
<h4 id="clarity-is-key">Clarity is Key&nbsp;<a href="#clarity-is-key">üîó</a> </h4>
<p>The first of these dimensions is pretty straightforward and shouldn‚Äôt really vary. Whatever you write, for whomever you write it, <strong>it has to be clear</strong>. Writing clearly means everything from sentence construction and paragraph breaks through to the structure of your article. It can be surprisingly hard to do but is crucial if you want to write material that people will <em>want</em> to read.</p>
<p>One neat trick when it comes to clarity is to remember that <em>what you leave out</em> is as important as what you leave in. This is going to be very context-specific. Documentation, by definition, should be comprehensive. A blog, on the other hand, might want to get to the point sooner and just provide a link to background material for the reader should they want it. Less is often more, as they say.</p>
<p>Some types of writing are going to have greater scope for individuality than others, but all have the potential to at least be accessible and clear. For example, just because you‚Äôre writing documentation doesn‚Äôt give you a pass to copy and paste the requirements doc in all its generic and obscure complexity. Write documentation that you as a developer would like to read. It can be complex and precise, yet still accessible.</p>
<h4 id="personality-and-voice">Personality and Voice&nbsp;<a href="#personality-and-voice">üîó</a> </h4>
<p>Should the ‚Äòvoice‚Äô of the author be allowed to come through in the writing?</p>
<p>This is very much a sliding scale. I‚Äôve jotted down <em>some</em> of the characteristics you might associate with either extreme of the scale. This is not to say that by definition you‚Äôd put cuss words into a blog so as to convey your voice‚Äîbut as an example of something that you might see at that end of the spectrum and definitely not at the other.</p>
<img src="https://rmoff.net/images/2023/07/03.svg" height="300/">
<p>How you decide where to pitch your voice on this scale will come down to your preference, audience, and general area and discipline. If you spend much time on Twitter you‚Äôll notice that InfoSec Twitter is different from DevOps Twitter, which is different again from DataEng Twitter. Each has its own cliques and customs, and also a varying range to which an author‚Äôs voice shines through in published writing.</p>
<p>You‚Äôll generally find that generally writing mediums such as a project report to stakeholders or product documentation requires a neutral voice. That‚Äôs not to say <em>boring</em>, but it is to say that a certain uniformity is required. In the case of a project report, the message mustn‚Äôt be obscured by colloquialisms and the such. And can you imagine the cognitive dissonance if a set of documentation were written by multiple writers each looking to stamp their personality on the pages?</p>
<p>When we get to things like blogs and other types of writing we <em>deliberately</em> want to include some personality. How much is up to you to calibrate with your audience and yourself. There is a ‚ÄúGoldilocks‚Äù zone here‚Äîenough personality and genuine voice coming through to convince the reader that they are reading something that was written by someone who is actually interested and informed on the matter, but not so much that it gets in the way of the content.</p>
<h4 id="uniformity-and-standardisation">Uniformity and Standardisation&nbsp;<a href="#uniformity-and-standardisation">üîó</a> </h4>
<p>This has a strong relationship with personality and voice but relates a lot more to the structure and content of the material</p>
<img src="https://rmoff.net/images/2023/07/04.svg" height="300/">
<p>Using the example of blogs, you‚Äôll find that blogs for a company or project are going to have a strong focus on the consistency of messaging and structure. There‚Äôll be an introduction, there‚Äôll be context; it‚Äôll be comprehensive.</p>
<p>Compare that to a personal blog that may sometimes be not much more than the gutterings of a developer wanting to log an error message and solution for future Googlers. They <em>might</em> flesh it out into a longer article, but that‚Äôs not necessary for it still to have value.</p>
<h4 id="a-holistic-view">A Holistic View&nbsp;<a href="#a-holistic-view">üîó</a> </h4>
<p>It may seem like there‚Äôs going to be a linear relationship between the two dimensions. As we decrease the amount of personality coming through in an author‚Äôs writings, we‚Äôre also going to move towards a much more standardised set of writing.</p>
<p>I‚Äôd suggest that it‚Äôs not always the case.</p>
<p>A startup may value personality much more over standardisation, perhaps only really dropping the voice when it comes to something like documentation (and even then, perhaps not entirely).</p>
<img src="https://rmoff.net/images/2023/07/05.svg" height="300/">
<p>At the other end of the scale, some companies‚Äîusually large corporations‚Äîhave the habit of squeezing the last inch of life out of any kind of writing, making the relationship a much different one.</p>
<p>Here there‚Äôs little voice even where you might hope to find it, and that rapidly drops off into nothing very soon after:</p>
<img src="https://rmoff.net/images/2023/07/06.svg" height="300/">
<p>The wildcard within this is the social media teams of large companies who <em>are</em> given the remit to be <code>Funny</code> and <code>Engaging</code>, but this is usually outside the scope of developer writing and more into the field of <a href="https://www.boredpanda.com/sassiest-responses-from-companies">condiments and fast food chains</a></p>
<h2 id="structuring-your-blog-writing">Structuring your Blog Writing&nbsp;<a href="#structuring-your-blog-writing">üîó</a> </h2>
<p>Like a favourite pair of jeans that‚Äôs well-worn, comfy, and slightly saggy round the arse, I have a go-to structure for writing. Come to think of it, I use it for lots of conference talks too. It looks like this:</p>
<ol>
<li>Tell them what you‚Äôre going to tell them</li>
<li>Tell them</li>
<li>Tell them what you told them</li>
</ol>
<p>What this looks like in practice is something along these lines:</p>
<ol>
<li>
<p><strong>An intro</strong></p>
<p>What is this thing, and why should the reader <del>give af</del> be interested?</p>
<p>This could be a brief explanation of why I am interested in it, or why you would want to read my take on it. The key thing is you‚Äôre relating to your audience here. Not everyone wants to read everything you write, and that‚Äôs ok.</p>
<p>Let people self-select out (or in, hopefully) at this stage, but make it nice and easy. For example, if you‚Äôre writing about data engineering, make it clear to the appdev crowd that they should move on as there‚Äôs nothing to see here (or stick around and learn something new, but as a visitor, not the target audience).</p>
</li>
<li>
<p><strong>The article itself</strong></p>
</li>
<li>
<p><strong>A recap</strong></p>
<p>Make sure you don‚Äôt just finish your article with a figurative mic drop‚Äîtie up it nicely with a bow (a üôáüèª or a üéÄ, either works).</p>
<p>This is where marketing would like to introduce you to the acronym CTA (Call To Action) üòâ. As an author you can decide how or if to weave that into your narrative.</p>
<p>Either way, you‚Äôre going to summarise what you just did and give people something to <em>do</em> with it next. Are there code samples they can go and run or inspect? A new service to sign up for? A video to watch? Or just a general life reflection upon which to ponder.</p>
</li>
</ol>
<h2 id="-the-physical-act-of-writing-jfdi--">‚úçüèª The Physical Act of Writing: JFDI ;-)&nbsp;<a href="#-the-physical-act-of-writing-jfdi--">üîó</a> </h2>
<p><img src="https://rmoff.net/images/2023/07/07.png" alt=""></p>
<p>At the risk of repeating the <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">owl meme</a> I would give the following advice: just start writing!</p>
<p>I don‚Äôt mean just go write an article. I mean start writing <strong>something</strong>, <strong><em>anything.</em></strong></p>
<p>Some notes, some snippets, some whole paragraphs. It might even look like this</p>
<p><img src="https://rmoff.net/images/2023/07/08.png" alt=""></p>
<p>The point is you now have <em>something</em>. The sections and threads of a story start to fall out as you write more. What starts as one section perhaps becomes two as you realise there are individual elements to tease out.</p>
<p>Iterate, iterate, and then iterate some more.</p>
<p>That random link you made a note of, where does it fit in what you want to say? Is it pushing the need for a new section or tangent, or is it actually not so relevant and you can park it? Not sure? Well just leave it there and think about it again on the next pass round.</p>
<p>I‚Äôve recently found that using a <a href="https://en.wikipedia.org/wiki/Pomodoro_Technique">Pomodoro timer</a> is an effective way of getting me to focus, and to take a break. Instead of staring at a screen, descending into a pit of despair at the stagnation of an article, you spend a chunk of time and then step away. Perhaps you come back to it after the break or maybe wait longer. Like many problems in life, things resolve themselves given time to marinade in the recesses of one‚Äôs brain. That paragraph that just wouldn‚Äôt write itself will come spilling out of your eager fingers onto the keyboard. The section you thought you‚Äôd <em>nailed</em>‚Äîturns out you didn‚Äôt and it needs a rewrite. But all these things come with time and iteration through the text.</p>
<h2 id="find-a-really-good-reviewer-and-copyeditor">Find a really good reviewer and copyeditor&nbsp;<a href="#find-a-really-good-reviewer-and-copyeditor">üîó</a> </h2>
<p>You might think you‚Äôre good at writing. You‚Äôre probably not <em>that</em> good at writing that the eye of an excellent copyeditor won‚Äôt improve it, nor the tactful input of a good reviewer enhance it.</p>
<p>Good copyeditors will respect the voice that‚Äôs present in your writing and work to preserve it whilst improving the clarity and grammatical accuracy of what you‚Äôve written.</p>
<p>Good reviewers will grok what you‚Äôre saying and help distil and mould it into a better shape.</p>

<p>Ah, the meta-blog post about tooling. Each to their own, but here‚Äôs my stack:</p>
<ul>
<li><a href="https://obsidian.md/">Obsidian</a> for authoring</li>
<li><a href="https://cleanshot.com/">CleanShot X</a> for screen grabs and markup</li>
<li><a href="https://www.grammarly.com/">Grammarly</a> for proofreading (and please, for the sak of your readers, profread, noone wnats to red a baydly writen blog)</li>
<li><a href="https://rmoff.net/categories/hugo/">Hugo and GitHub Pages</a> for publishing and hosting</li>
</ul>
<h2 id="resources">Resources&nbsp;<a href="#resources">üîó</a> </h2>
<ul>
<li>A <a href="https://github.com/sixhobbits/technical-writing/blob/master/resources.md">useful list of resources</a> from <a href="https://twitter.com/sixhobbits">Gareth Dwyer</a></li>
<li>üìß An email-based course called <a href="https://bloggingfordevs.com/">Blogging for Devs</a>. It‚Äôs quite focussed on the mechanics of a blogging but has some useful nuggets - and it‚Äôs free</li>
<li>üìï<a href="https://pragprog.com/titles/actb2/technical-blogging-second-edition/">Technical Blogging</a>, by <a href="https://antoniocangiano.com/">Antonio Cangiano</a></li>
<li><a href="https://www.youtube.com/watch?v=kOnZovTFTHc">Avoiding Anti-patterns in Technical Communication</a> - good conference talk from <a href="https://www.linkedin.com/in/sophwats/">Sophie Watson</a></li>
<li>A nice blog from <a href="https://www.linkedin.com/in/kudmitry/">Dmitry Kudryavtsev</a> on <a href="https://www.yieldcode.blog/post/why-engineers-should-write">Why engineers should focus on writing</a></li>
<li><a href="https://developers.google.com/tech-writing">Technical Writing Courses - Google Developers</a></li>
</ul>

<p><em>We‚Äôve covered the why and the how - but what about the what?</em></p>
<p>What to write will often come from the ‚ÄúWhy‚Äù above, but let‚Äôs imagine that the creative juices aren‚Äôt flowing and you still really want to get a blog written.</p>
<p>A really excellent place for ideas is the community around the thing you want to write about. Go and lurk (or even better, join in) at StackOverflow, Twitter, Slack, Discord‚Ä¶wherever the community is:</p>
<ul>
<li>What questions do people repeatedly ask?</li>
<li>What are the anti-patterns and misunderstandings that you see?</li>
<li>What are the new trends?</li>
<li>What cool things can you do with <code>$THING</code></li>
</ul>
<p>In short, if it would be interesting to me then I would write about it.</p>
<p>Make sure to also watch <a href="https://www.youtube.com/watch?v=vtIzMaLkCaM">this lecture</a> in which the concept of <em>value</em> and <em>ideas</em> is discussed. tl;dr if you aren‚Äôt writing about something interesting to the reader, it has no value, regardless of its value to you.</p>
<h3 id="what-not-to-write">What Not to Write?&nbsp;<a href="#what-not-to-write">üîó</a> </h3>
<p>This is a <em>very</em> personal preference. I‚Äôm not keen at all on growth-driven blogging styles. You know the sort: listicles, SEO bait, etc. It‚Äôs low-grade, developers see through it, and it tarnishes the blogger‚Äôs image IMHO. That said, if you write a good blog, there‚Äôs no reason not to structure it such (‚Äú<em>Top Five Tips for Successful Developer Writing</em>‚Äù) but put the horse before the cart, not the other way around.</p>
<hr>
<h2 id="recap">Recap&nbsp;<a href="#recap">üîó</a> </h2>
<p>To summarise this whole article, bear in mind that these two statements are not mutually exclusive:</p>
<ol>
<li>Write for yourself. Work out what <em>you</em> would like to read, and write it.</li>
<li>Think of the reader and what value you‚Äôre providing to them in your writing.</li>
</ol>
<p>That‚Äôs because as a developer writing for developers, you <strong>are</strong> the reader.</p>
<p>Oh, and did you watch <a href="https://www.youtube.com/watch?v=vtIzMaLkCaM">Larry McEnerney‚Äôs lecture</a> yet? üòä</p>
</article>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg up to 94x performance boost after implementing handwritten assembly code (180 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/ffmpeg-devs-boast-of-up-to-94x-performance-boost-after-implementing-handwritten-avx-512-assembly-code</link>
            <guid>42045212</guid>
            <pubDate>Mon, 04 Nov 2024 19:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/ffmpeg-devs-boast-of-up-to-94x-performance-boost-after-implementing-handwritten-avx-512-assembly-code">https://www.tomshardware.com/pc-components/cpus/ffmpeg-devs-boast-of-up-to-94x-performance-boost-after-implementing-handwritten-avx-512-assembly-code</a>, See on <a href="https://news.ycombinator.com/item?id=42045212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-320-80.png" alt="Panasonic" srcset="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png 1024w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png 1200w, https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM-970-80.png 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM.png" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/nzCbjfNC5zwoShThjeSBxM.png">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Panasonic)</span>
</figcaption>
</div>

<div id="article-body">
<p>Contemporary high-level programming languages and advanced compilers greatly simplify software development and lower its costs. However, this way of programming can hide the performance capabilities of modern hardware, partly due to inefficiencies of application programming interfaces (APIs). Apparently, a good old assembly code path can improve performance by between three and 94 times, depending on the workload, according to <a data-analytics-id="inline-link" href="https://x.com/FFmpeg/status/1852542388851601913" data-url="https://x.com/FFmpeg/status/1852542388851601913" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">FFmpeg</a>. The hardware this multiplied performance was achieved on was not disclosed.</p><p>FFmpeg is an open-source video decoding project developed by volunteers who contribute to its codebase, fix bugs, and add new features. The project is led by a small group of core developers and maintainers who oversee its direction and ensure that contributions meet certain standards. They coordinate the project's development and release cycles, merging contributions from other developers. This group of developers tried to implement a handwritten AVX512 assembly code path, something that has rarely been done before, at least not in the video industry.</p><p>The developers have created an optimized code path using the AVX-512 instruction set to accelerate specific functions within the FFmpeg multimedia processing library. By leveraging AVX-512, they were able to achieve significant performance improvements ‚Äî from three to 94 times faster ‚Äî compared to standard implementations. AVX-512 enables processing large chunks of data in parallel using 512-bit registers, which can handle up to 16 single-precision FLOPS or 8 double-precision FLOPS in one operation. This optimization is ideal for compute-heavy tasks in general, but in the case of video and image processing in particular.</p><p>The <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/benchmark" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/benchmark">benchmarking</a> results show that the new handwritten AVX-512 code path performs considerably faster than other implementations, including baseline C code and lower SIMD instruction sets like AVX2 and SSSE3. In some cases, the revamped AVX-512 codepath achieves a speedup of nearly 94 times over the baseline, highlighting the efficiency of hand-optimized assembly code for AVX-512.</p><p>This development is particularly valuable for users running on high-performance, AVX-512-capable hardware, enabling them to process media content far more efficiently. There is an issue, though: Intel disabled AVX-512 for its Core 12th, 13th, and 14th Generations of Core processors, leaving owners of these CPUs without them. On the other hand, AMD's Ryzen 9000-series CPUs feature a fully-enabled AVX-512 FPU so the owners of these processors can take advantage of the FFmpeg achievement.</p><p>Unfortunately, due to the complexity and specialized nature of AVX-512, such optimizations are typically reserved for performance-critical applications and require expertise in low-level programming and processor microarchitecture.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-SDpvGFrC3qiFsDcJ94h8pj"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>
<div id="slice-container-authorBio-SDpvGFrC3qiFsDcJ94h8pj"><p>Anton Shilov is a contributing writer at Tom‚Äôs Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>



<!-- Drop in a standard article here maybe? -->



</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nutrient levels in retail grocery stores (111 pts)]]></title>
            <link>https://altered.substack.com/p/walmart</link>
            <guid>42045140</guid>
            <pubDate>Mon, 04 Nov 2024 19:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://altered.substack.com/p/walmart">https://altered.substack.com/p/walmart</a>, See on <a href="https://news.ycombinator.com/item?id=42045140">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>Cross-posted from </span><a href="https://x.com/carmenleelau/status/1853336192441561258" rel="">Twitter</a><span>.</span></em></p><p>I've done some digging into nutrient levels in retail grocery stores (specifically where I should shop if I want nutritious food) and learned some insane things. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png" width="1456" height="1076" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1076,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:382982,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b26c1c-b711-4305-93b7-e8b4eb9ac2d1_1600x1182.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><h6><span>Source: </span><a href="https://www.researchgate.net/publication/327414475_Challenges_in_the_Diagnosis_of_Magnesium_Status" rel="">Challenges in the Diagnosis of Magnesium Status</a></h6><p>1) It's a well-documented phenomenon that nutrient levels in produce have been declining for decades. The average mineral content of calcium, magnesium, and iron in cabbage, lettuce, tomatoes, and spinach has dropped 80-90% between 1914 and 2018. </p><p><span>There are several reasons for this, but most of them are due to modern agricultural practices. These reasons include: selective breeding, soil depletion, synthetic fertilizers that provide basic nutrients necessary for plant growth but not others that would make them nutrient-dense, </span><a href="https://x.com/cremieuxrecueil/status/1647691634086215682" rel="">higher CO2 levels in atmosphere diluting nutrient content in plants</a><span>, over-irrigation washing away nutrients from soil, and long storage times.</span></p><p>2) If all you care about is nutrient content, SPEED is the only factor that matters. That means time between being picked and ending up in your mouth. Price doesn't matter, organic doesn't matter, marketing hoo-ha about how fancy the produce is doesn't matter. Literally just speed. Fresh spinach loses almost ALL of its vitamin C within 7 days of harvest when stored at 68¬∞F (20¬∞C). When stored at 39¬∞F (4¬∞C) which is about fridge temp, it loses 75%. The apples you buy at the grocery store can be up to a *year* old since they've been harvested, especially if you are not buying them in season.</p><p>Taste, texture, and smell are not good indicators of nutrient content, because you can't tell how long it has been since it's been harvested. Modern shipping and storage methods can be deceptive, combined with practices like spraying strawberry fragrance on otherwise bland strawberries so you think you're buying the good stuff (yes some places actually do this).</p><p>In general, it can help to eat in season, but due to the globalized supply chain...it's always in season *somewhere*. But how long did it take to get to you? You don't know. Even if it's in season where you are, how do you know if that's not from *last* season? Again, you have no idea.</p><p>3) Think for a moment: what store do you think sells the most nutritious produce? Your local farmers‚Äô market? Whole Foods? Trader Joe‚Äôs must be decent, right? Nope, it‚Äôs Walmart. Because of their scale and insanely efficient supply chain, they can get things from the farm (wherever it is in the world it's growing in season) to the store where you can buy it, really really fast. Oh, and for the lowest cost.</p><p>I wanted to believe every time I splurged on fancier produce, I was actually getting something better. But this is what the data comparing 18 major US retailers showed ‚Äî Walmart consistently outperformed.</p><p><span>I learned this from talking to Brent Overcash, co-founder of a startup called </span><a href="https://www.forbes.com/sites/lanabandoim/2020/03/02/what-is-really-in-your-food-teakorigin-guide-has-the-answers/" rel="">TeakOrigin</a><span>, which specialized in testing nutrient content in groceries from retail grocery stores. For years, every week, his team would walk into grocery stores, buy thousands of produce items the way normal consumers would, and bring them back to the lab to assess nutrient content. They‚Äôd go to Whole Foods, Trader Joe‚Äôs, Costco, Wegmans, Sprouts, Waitrose, farmers‚Äô markets, and many more retailers. The ‚Äúwinner‚Äù varied depending on the specific type of produce/brands/exactly what week it was, but Walmart tended to come out on top. The more important point is that price and taste and organic certifications had no impact on the actual nutrient density.</span></p><p>They raised ~$5M and did a decade of research with labs full of analytical chemists where they used a combination of molecular spectrometry, HPLC, GC/MS, TGA, and wet chemistry methods. With 800+ million data points, the USDA and FDA at one point told them they had the world‚Äôs largest dataset of dynamic nutrition data. They‚Äôre no longer around, but had branches in California, Boston, and the UK just a couple years ago.</p><p>It went out of business because no major grocery store wanted to partner with them, because the transparency would hold these retailers accountable for *so many things* the consumers aren't even currently aware of. Things like how Whole Foods centers their entire branding around fresher, higher quality produce that's better for you, but when he actually tested some expensive apples they were selling from a local orchard advertised with handwritten chalkboard signs, it had so little nutrient content it was barely detectable on their lab-grade machines. He called the orchard because he was curious, under the guise that he was interested in picking some apples for the season. They said, "Oh sure, you can do that, but our first harvest isn't for another 6 weeks." That means it had been a year since the apples he bought were actually picked. This is actually industry standard, made possible by storing them at low temperatures and spraying a gas called 1-MCP which blocks ethylene (a gas naturally produced by the apples that makes them ripen) by binding to the same receptors.</p><p>I asked him: Are farmers‚Äô markets any better, since we're getting the produce directly from local farmers? And he said basically there's huge variance. If you walk into a booth and that vendor is selling over 5 types of produce, there's no way they all ripened at the same time. They may not even all be grown by them. Once, he actually saw a vendor at Boston farmers‚Äô market selling carrots from Target! He could tell from the packaging because he used to work for them. Turns out when a produce delivery is refused by the grocery store, the truck owner is then responsible for getting rid of that produce, and they usually drive to a ‚Äúfood hub‚Äù where bulk produce is bought and sold just to recoup some of the costs. That is one possible source of the mysterious farmers‚Äô market carrots they were pawning off as homegrown.</p><p>Now, I'm not saying you should *stop* shopping at non-Walmart places. There are a lot of factors other than nutrient density that influence a purchasing decision. The fancy, expensive produce might taste better, smell better, and be better for cooking. It may come from farms where there are better wages and working conditions. There might be fewer pesticides. But again, for nutrient content, speed is the only thing that matters.</p><p>I want to do something about this, but it‚Äôs an issue way bigger than I can tackle alone. From the way we do modern farming, to the complete lack of transparency on the retailer's end (and vested interests in keeping it that way), to the government's lack of interest in enforcing this transparency for the sake of consumers...the problem runs deep and I'm tired man. And so is Brent, who‚Äôs retired and doing woodworking in his studio these days, living the good life.</p><p><em>Side note on frozen produce: The post above is about fresh produce only. A potentially appealing alternative may be buying frozen produce, which on average has equal or higher nutritional content than fresh. This is because frozen produce is picked at peak ripeness then frozen shortly after, locking in most of the nutrients at the expense of appearance/texture/flavor. If it‚Äôs been blanched (i.e. exposed briefly to hot water or steam to preserve quality of produce by killing enzymes that cause loss of flavor, color, and texture) before freezing, some nutrients like Vitamin C, B1, and folate are definitely lower because of heat sensitivity and being water soluble so they leach out into the blanching water. Pretty much all frozen veggies are blanched first, and stuff like berries/fruit aren‚Äôt. The rest of the nutrients are quite stable, and knowing which ones are lost means you can supplement in other ways. Fresh produce is picked before peak ripeness to accommodate for ripening during storage and transport, and the nutrients degrade as it sits on shelves and in refrigerators before it is eventually prepared.</em></p><p><em>It‚Äôs also great that it‚Äôs more convenient, and I think frozen produce unfairly gets a bad rap. The only thing I would change is the plastic packaging because I don‚Äôt like microplastics in my food. I would avoid heating or microwaving produce in its original plastic packaging.</em></p><p><em>One thing to be aware of is frozen berries (or produce that isn‚Äôt blanched before freezing) that have been imported. In 2022-2023, New Zealand had a hepatitis A outbreak from frozen berries imported from Serbia so they advised people to boil/cook their berries before refreezing or eating. Some countries have more lax sanitation standards and contamination can happen in processing facilities. Furthermore, freezing doesn‚Äôt kill viruses! For stuff like berries which aren‚Äôt cooked before they‚Äôre eaten, you can more easily get sick. It‚Äôs less of a concern for berries sourced from within the US, Canada, Australia, most of the EU ‚Äì&nbsp;but Australia has had 2 of the same hep A outbreaks so I‚Äôd just keep it in mind.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diagrams ¬∑ Diagram as Code (345 pts)]]></title>
            <link>https://diagrams.mingrammer.com/</link>
            <guid>42044771</guid>
            <pubDate>Mon, 04 Nov 2024 18:46:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diagrams.mingrammer.com/">https://diagrams.mingrammer.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42044771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2>About Diagrams</h2><p><span><p>Diagrams lets you draw the cloud system architecture <strong>in Python code</strong>.</p>
</span></p><p><span><p>It was born for <strong>prototyping</strong> a new system architecture without any design tools. You can also describe or visualize the existing system architecture as well.</p>
</span></p><p><span><p><code>Diagram as Code</code> allows you to <strong>track</strong> the architecture diagram changes in any <strong>version control</strong> system.</p>
</span></p><p><span><p>Diagrams currently supports main major providers including: <code>AWS</code>, <code>Azure</code>, <code>GCP</code>, <code>Kubernetes</code>, <code>Alibaba Cloud</code>, <code>Oracle Cloud</code> etc... It also supports <code>On-Premise</code> nodes, <code>SaaS</code> and major <code>Programming</code> frameworks and languages.</p>
</span></p><p><span><p><code>NOTE: It does not control any actual cloud resources nor does it generate cloud formation or terraform code. It is just for drawing the cloud system architecture diagrams.</code></p>
</span></p></div><div><div><p><img src="https://diagrams.mingrammer.com/img/message_collecting_code.png"></p></div><div><p><img src="https://diagrams.mingrammer.com/img/message_collecting_diagram.png"></p></div></div><div><div><p><img src="https://diagrams.mingrammer.com/img/event_processing_code.png"></p></div><div><p><img src="https://diagrams.mingrammer.com/img/event_processing_diagram.png"></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DataChain: DBT for Unstructured Data (123 pts)]]></title>
            <link>https://github.com/iterative/datachain</link>
            <guid>42043948</guid>
            <pubDate>Mon, 04 Nov 2024 17:34:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/iterative/datachain">https://github.com/iterative/datachain</a>, See on <a href="https://news.ycombinator.com/item?id=42043948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/iterative/datachain/blob/main/docs/assets/datachain.svg"><img alt="logo" height="24" src="https://github.com/iterative/datachain/raw/main/docs/assets/datachain.svg"></a>
 DataChain</h2><a id="user-content--datachain" aria-label="Permalink: 
 DataChain" href="#-datachain"></a></div>
<p dir="auto"><a href="https://pypi.org/project/datachain/" rel="nofollow"><img alt="PyPI" src="https://camo.githubusercontent.com/a4d123fc0f1691dd3957fd36b80adf23d5046768ad633924e1c58a4c3eaca629/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f64617461636861696e2e737667" data-canonical-src="https://img.shields.io/pypi/v/datachain.svg">
</a> <a href="https://pypi.org/project/datachain" rel="nofollow"><img alt="Python Version" src="https://camo.githubusercontent.com/e6c2bfa259931866ffac797b0b8caac7364e0eecb539d5f351e5593ec5a525de/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f64617461636861696e" data-canonical-src="https://img.shields.io/pypi/pyversions/datachain"></a> <a href="https://codecov.io/gh/iterative/datachain" rel="nofollow"><img alt="Codecov" src="https://camo.githubusercontent.com/70e14e85f86115c3871d59d63025dd05b0a29f63ef2f7ed290bc77f13b421d01/68747470733a2f2f636f6465636f762e696f2f67682f6974657261746976652f64617461636861696e2f67726170682f62616467652e7376673f746f6b656e3d62796c69584747794742" data-canonical-src="https://codecov.io/gh/iterative/datachain/graph/badge.svg?token=byliXGGyGB"></a> <a href="https://github.com/iterative/datachain/actions/workflows/tests.yml"><img alt="Tests" src="https://github.com/iterative/datachain/actions/workflows/tests.yml/badge.svg">
</a></p>
<p dir="auto">DataChain is a modern Pythonic data-frame library designed for artificial intelligence.
It is made to organize your unstructured data into datasets and wrangle it at scale on
your local machine. Datachain does not abstract or hide the AI models and API calls, but helps to integrate them into the postmodern data stack.</p>
<a name="user-content-key-features"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<dl>
<dt>üìÇ <strong>Storage as a Source of Truth.</strong></dt>
<dd><ul dir="auto">
<li>Process unstructured data without redundant copies from S3, GCP, Azure, and local
file systems.</li>
<li>Multimodal data support: images, video, text, PDFs, JSONs, CSVs, parquet.</li>
<li>Unite files and metadata together into persistent, versioned, columnar datasets.</li>
</ul>
</dd>
<dt>üêç <strong>Python-friendly data pipelines.</strong></dt>
<dd><ul dir="auto">
<li>Operate on Python objects and object fields.</li>
<li>Built-in parallelization and out-of-memory compute without SQL or Spark.</li>
</ul>
</dd>
<dt>üß† <strong>Data Enrichment and Processing.</strong></dt>
<dd><ul dir="auto">
<li>Generate metadata using local AI models and LLM APIs.</li>
<li>Filter, join, and group by metadata. Search by vector embeddings.</li>
<li>Pass datasets to Pytorch and Tensorflow, or export them back into storage.</li>
</ul>
</dd>
<dt>üöÄ <strong>Efficiency.</strong></dt>
<dd><ul dir="auto">
<li>Parallelization, out-of-memory workloads and data caching.</li>
<li>Vectorized operations on Python object fields: sum, count, avg, etc.</li>
<li>Optimized vector search.</li>
</ul>
</dd>
</dl>
<a name="user-content-quick-start"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>

<a name="user-content-selecting-files-using-json-metadata"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Selecting files using JSON metadata</h2><a id="user-content-selecting-files-using-json-metadata" aria-label="Permalink: Selecting files using JSON metadata" href="#selecting-files-using-json-metadata"></a></p>
<p dir="auto">A storage consists of images of cats and dogs (dog.1048.jpg, cat.1009.jpg),
annotated with ground truth and model inferences in the 'json-pairs' format,
where each image has a matching JSON file like cat.1009.json:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &quot;class&quot;: &quot;cat&quot;, &quot;id&quot;: &quot;1009&quot;, &quot;num_annotators&quot;: 8,
    &quot;inference&quot;: {&quot;class&quot;: &quot;dog&quot;, &quot;confidence&quot;: 0.68}
}"><pre>{
    <span>"class"</span>: <span><span>"</span>cat<span>"</span></span>, <span>"id"</span>: <span><span>"</span>1009<span>"</span></span>, <span>"num_annotators"</span>: <span>8</span>,
    <span>"inference"</span>: {<span>"class"</span>: <span><span>"</span>dog<span>"</span></span>, <span>"confidence"</span>: <span>0.68</span>}
}</pre></div>
<p dir="auto">Example of downloading only "high-confidence cat" inferred images using JSON metadata:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from datachain import Column, DataChain

meta = DataChain.from_json(&quot;gs://datachain-demo/dogs-and-cats/*json&quot;, object_name=&quot;meta&quot;)
images = DataChain.from_storage(&quot;gs://datachain-demo/dogs-and-cats/*jpg&quot;)

images_id = images.map(id=lambda file: file.path.split('.')[-2])
annotated = images_id.merge(meta, on=&quot;id&quot;, right_on=&quot;meta.id&quot;)

likely_cats = annotated.filter((Column(&quot;meta.inference.confidence&quot;) > 0.93) \
                               &amp; (Column(&quot;meta.inference.class_&quot;) == &quot;cat&quot;))
likely_cats.export_files(&quot;high-confidence-cats/&quot;, signal=&quot;file&quot;)"><pre><span>from</span> <span>datachain</span> <span>import</span> <span>Column</span>, <span>DataChain</span>

<span>meta</span> <span>=</span> <span>DataChain</span>.<span>from_json</span>(<span>"gs://datachain-demo/dogs-and-cats/*json"</span>, <span>object_name</span><span>=</span><span>"meta"</span>)
<span>images</span> <span>=</span> <span>DataChain</span>.<span>from_storage</span>(<span>"gs://datachain-demo/dogs-and-cats/*jpg"</span>)

<span>images_id</span> <span>=</span> <span>images</span>.<span>map</span>(<span>id</span><span>=</span><span>lambda</span> <span>file</span>: <span>file</span>.<span>path</span>.<span>split</span>(<span>'.'</span>)[<span>-</span><span>2</span>])
<span>annotated</span> <span>=</span> <span>images_id</span>.<span>merge</span>(<span>meta</span>, <span>on</span><span>=</span><span>"id"</span>, <span>right_on</span><span>=</span><span>"meta.id"</span>)

<span>likely_cats</span> <span>=</span> <span>annotated</span>.<span>filter</span>((<span>Column</span>(<span>"meta.inference.confidence"</span>) <span>&gt;</span> <span>0.93</span>) \
                               <span>&amp;</span> (<span>Column</span>(<span>"meta.inference.class_"</span>) <span>==</span> <span>"cat"</span>))
<span>likely_cats</span>.<span>export_files</span>(<span>"high-confidence-cats/"</span>, <span>signal</span><span>=</span><span>"file"</span>)</pre></div>
<a name="user-content-data-curation-with-a-local-ai-model"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Data curation with a local AI model</h2><a id="user-content-data-curation-with-a-local-ai-model" aria-label="Permalink: Data curation with a local AI model" href="#data-curation-with-a-local-ai-model"></a></p>
<p dir="auto">Batch inference with a simple sentiment model using the transformers library:</p>

<p dir="auto">The code below downloads files the cloud, and applies a user-defined function
to each one of them. All files with a positive sentiment
detected are then copied to the local directory.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import pipeline
from datachain import DataChain, Column

classifier = pipeline(&quot;sentiment-analysis&quot;, device=&quot;cpu&quot;,
                model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)

def is_positive_dialogue_ending(file) -> bool:
    dialogue_ending = file.read()[-512:]
    return classifier(dialogue_ending)[0][&quot;label&quot;] == &quot;POSITIVE&quot;

chain = (
   DataChain.from_storage(&quot;gs://datachain-demo/chatbot-KiT/&quot;,
                          object_name=&quot;file&quot;, type=&quot;text&quot;)
   .settings(parallel=8, cache=True)
   .map(is_positive=is_positive_dialogue_ending)
   .save(&quot;file_response&quot;)
)

positive_chain = chain.filter(Column(&quot;is_positive&quot;) == True)
positive_chain.export_files(&quot;./output&quot;)

print(f&quot;{positive_chain.count()} files were exported&quot;)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>DataChain</span>, <span>Column</span>

<span>classifier</span> <span>=</span> <span>pipeline</span>(<span>"sentiment-analysis"</span>, <span>device</span><span>=</span><span>"cpu"</span>,
                <span>model</span><span>=</span><span>"distilbert/distilbert-base-uncased-finetuned-sst-2-english"</span>)

<span>def</span> <span>is_positive_dialogue_ending</span>(<span>file</span>) <span>-&gt;</span> <span>bool</span>:
    <span>dialogue_ending</span> <span>=</span> <span>file</span>.<span>read</span>()[<span>-</span><span>512</span>:]
    <span>return</span> <span>classifier</span>(<span>dialogue_ending</span>)[<span>0</span>][<span>"label"</span>] <span>==</span> <span>"POSITIVE"</span>

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>"gs://datachain-demo/chatbot-KiT/"</span>,
                          <span>object_name</span><span>=</span><span>"file"</span>, <span>type</span><span>=</span><span>"text"</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>8</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>is_positive</span><span>=</span><span>is_positive_dialogue_ending</span>)
   .<span>save</span>(<span>"file_response"</span>)
)

<span>positive_chain</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>"is_positive"</span>) <span>==</span> <span>True</span>)
<span>positive_chain</span>.<span>export_files</span>(<span>"./output"</span>)

<span>print</span>(<span>f"<span><span>{</span><span>positive_chain</span>.<span>count</span>()<span>}</span></span> files were exported"</span>)</pre></div>
<p dir="auto">13 files were exported</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ls output/datachain-demo/chatbot-KiT/
15.txt 20.txt 24.txt 27.txt 28.txt 29.txt 33.txt 37.txt 38.txt 43.txt ...
$ ls output/datachain-demo/chatbot-KiT/ | wc -l
13"><pre>$ ls output/datachain-demo/chatbot-KiT/
15.txt 20.txt 24.txt 27.txt 28.txt 29.txt 33.txt 37.txt 38.txt 43.txt ...
$ ls output/datachain-demo/chatbot-KiT/ <span>|</span> wc -l
13</pre></div>
<a name="user-content-llm-judging-chatbots"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">LLM judging chatbots</h2><a id="user-content-llm-judging-chatbots" aria-label="Permalink: LLM judging chatbots" href="#llm-judging-chatbots"></a></p>
<p dir="auto">LLMs can work as universal classifiers. In the example below,
we employ a free API from Mistral to judge the <a href="https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot" rel="nofollow">publicly available</a> chatbot dialogs. Please get a free
Mistral API key at <a href="https://console.mistral.ai/" rel="nofollow">https://console.mistral.ai</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pip install mistralai (Requires version >=1.0.0)
$ export MISTRAL_API_KEY=_your_key_"><pre>$ pip install mistralai (Requires version <span>&gt;</span>=1.0.0)
$ <span>export</span> MISTRAL_API_KEY=_your_key_</pre></div>
<p dir="auto">DataChain can parallelize API calls; the free Mistral tier supports up to 4 requests at the same time.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mistralai import Mistral
from datachain import File, DataChain, Column

PROMPT = &quot;Was this dialog successful? Answer in a single word: Success or Failure.&quot;

def eval_dialogue(file: File) -> bool:
     client = Mistral()
     response = client.chat.complete(
         model=&quot;open-mixtral-8x22b&quot;,
         messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: PROMPT},
                   {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: file.read()}])
     result = response.choices[0].message.content
     return result.lower().startswith(&quot;success&quot;)

chain = (
   DataChain.from_storage(&quot;gs://datachain-demo/chatbot-KiT/&quot;, object_name=&quot;file&quot;)
   .settings(parallel=4, cache=True)
   .map(is_success=eval_dialogue)
   .save(&quot;mistral_files&quot;)
)

successful_chain = chain.filter(Column(&quot;is_success&quot;) == True)
successful_chain.export_files(&quot;./output_mistral&quot;)

print(f&quot;{successful_chain.count()} files were exported&quot;)"><pre><span>from</span> <span>mistralai</span> <span>import</span> <span>Mistral</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>File</span>, <span>DataChain</span>, <span>Column</span>

<span>PROMPT</span> <span>=</span> <span>"Was this dialog successful? Answer in a single word: Success or Failure."</span>

<span>def</span> <span>eval_dialogue</span>(<span>file</span>: <span>File</span>) <span>-&gt;</span> <span>bool</span>:
     <span>client</span> <span>=</span> <span>Mistral</span>()
     <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>complete</span>(
         <span>model</span><span>=</span><span>"open-mixtral-8x22b"</span>,
         <span>messages</span><span>=</span>[{<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>PROMPT</span>},
                   {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>file</span>.<span>read</span>()}])
     <span>result</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>
     <span>return</span> <span>result</span>.<span>lower</span>().<span>startswith</span>(<span>"success"</span>)

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>"gs://datachain-demo/chatbot-KiT/"</span>, <span>object_name</span><span>=</span><span>"file"</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>4</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>is_success</span><span>=</span><span>eval_dialogue</span>)
   .<span>save</span>(<span>"mistral_files"</span>)
)

<span>successful_chain</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>"is_success"</span>) <span>==</span> <span>True</span>)
<span>successful_chain</span>.<span>export_files</span>(<span>"./output_mistral"</span>)

<span>print</span>(<span>f"<span><span>{</span><span>successful_chain</span>.<span>count</span>()<span>}</span></span> files were exported"</span>)</pre></div>
<p dir="auto">With the instruction above, the Mistral model considers 31/50 files to hold the successful dialogues:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ls output_mistral/datachain-demo/chatbot-KiT/
1.txt  15.txt 18.txt 2.txt  22.txt 25.txt 28.txt 33.txt 37.txt 4.txt  41.txt ...
$ ls output_mistral/datachain-demo/chatbot-KiT/ | wc -l
31"><pre>$ ls output_mistral/datachain-demo/chatbot-KiT/
1.txt  15.txt 18.txt 2.txt  22.txt 25.txt 28.txt 33.txt 37.txt 4.txt  41.txt ...
$ ls output_mistral/datachain-demo/chatbot-KiT/ <span>|</span> wc -l
31</pre></div>
<a name="user-content-serializing-python-objects"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Serializing Python-objects</h2><a id="user-content-serializing-python-objects" aria-label="Permalink: Serializing Python-objects" href="#serializing-python-objects"></a></p>
<p dir="auto">LLM responses may contain valuable information for analytics ‚Äì such as the number of tokens used, or the
model performance parameters.</p>
<p dir="auto">Instead of extracting this information from the Mistral response data structure (class
ChatCompletionResponse), DataChain can serialize the entire LLM response to the internal DB:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mistralai import Mistral
from mistralai.models import ChatCompletionResponse
from datachain import File, DataChain, Column

PROMPT = &quot;Was this dialog successful? Answer in a single word: Success or Failure.&quot;

def eval_dialog(file: File) -> ChatCompletionResponse:
     client = MistralClient()
     return client.chat(
         model=&quot;open-mixtral-8x22b&quot;,
         messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: PROMPT},
                   {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: file.read()}])

chain = (
   DataChain.from_storage(&quot;gs://datachain-demo/chatbot-KiT/&quot;, object_name=&quot;file&quot;)
   .settings(parallel=4, cache=True)
   .map(response=eval_dialog)
   .map(status=lambda response: response.choices[0].message.content.lower()[:7])
   .save(&quot;response&quot;)
)

chain.select(&quot;file.name&quot;, &quot;status&quot;, &quot;response.usage&quot;).show(5)

success_rate = chain.filter(Column(&quot;status&quot;) == &quot;success&quot;).count() / chain.count()
print(f&quot;{100*success_rate:.1f}% dialogs were successful&quot;)"><pre><span>from</span> <span>mistralai</span> <span>import</span> <span>Mistral</span>
<span>from</span> <span>mistralai</span>.<span>models</span> <span>import</span> <span>ChatCompletionResponse</span>
<span>from</span> <span>datachain</span> <span>import</span> <span>File</span>, <span>DataChain</span>, <span>Column</span>

<span>PROMPT</span> <span>=</span> <span>"Was this dialog successful? Answer in a single word: Success or Failure."</span>

<span>def</span> <span>eval_dialog</span>(<span>file</span>: <span>File</span>) <span>-&gt;</span> <span>ChatCompletionResponse</span>:
     <span>client</span> <span>=</span> <span>MistralClient</span>()
     <span>return</span> <span>client</span>.<span>chat</span>(
         <span>model</span><span>=</span><span>"open-mixtral-8x22b"</span>,
         <span>messages</span><span>=</span>[{<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>PROMPT</span>},
                   {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>file</span>.<span>read</span>()}])

<span>chain</span> <span>=</span> (
   <span>DataChain</span>.<span>from_storage</span>(<span>"gs://datachain-demo/chatbot-KiT/"</span>, <span>object_name</span><span>=</span><span>"file"</span>)
   .<span>settings</span>(<span>parallel</span><span>=</span><span>4</span>, <span>cache</span><span>=</span><span>True</span>)
   .<span>map</span>(<span>response</span><span>=</span><span>eval_dialog</span>)
   .<span>map</span>(<span>status</span><span>=</span><span>lambda</span> <span>response</span>: <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>.<span>lower</span>()[:<span>7</span>])
   .<span>save</span>(<span>"response"</span>)
)

<span>chain</span>.<span>select</span>(<span>"file.name"</span>, <span>"status"</span>, <span>"response.usage"</span>).<span>show</span>(<span>5</span>)

<span>success_rate</span> <span>=</span> <span>chain</span>.<span>filter</span>(<span>Column</span>(<span>"status"</span>) <span>==</span> <span>"success"</span>).<span>count</span>() <span>/</span> <span>chain</span>.<span>count</span>()
<span>print</span>(<span>f"<span><span>{</span><span>100</span><span>*</span><span>success_rate</span>:.1f<span>}</span></span>% dialogs were successful"</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="     file   status      response     response          response
     name                  usage        usage             usage
                   prompt_tokens total_tokens completion_tokens
0   1.txt  success           547          548                 1
1  10.txt  failure          3576         3578                 2
2  11.txt  failure           626          628                 2
3  12.txt  failure          1144         1182                38
4  13.txt  success          1100         1101                 1

[Limited by 5 rows]
64.0% dialogs were successful"><pre>     file   status      response     response          response
     name                  usage        usage             usage
                   prompt_tokens total_tokens completion_tokens
0   1.txt  success           547          548                 1
1  10.txt  failure          3576         3578                 2
2  11.txt  failure           626          628                 2
3  12.txt  failure          1144         1182                38
4  13.txt  success          1100         1101                 1

[Limited by 5 rows]
64.0% dialogs were successful</pre></div>
<a name="user-content-iterating-over-python-data-structures"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Iterating over Python data structures</h2><a id="user-content-iterating-over-python-data-structures" aria-label="Permalink: Iterating over Python data structures" href="#iterating-over-python-data-structures"></a></p>
<p dir="auto">In the previous examples, datasets were saved in the embedded database
(<a href="https://www.sqlite.org/" rel="nofollow">SQLite</a> in folder .datachain of the working directory).
These datasets were automatically versioned, and can be accessed using
DataChain.from_dataset("dataset_name").</p>
<p dir="auto">Here is how to retrieve a saved dataset and iterate over the objects:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chain = DataChain.from_dataset(&quot;response&quot;)

# Iterating one-by-one: support out-of-memory workflow
for file, response in chain.limit(5).collect(&quot;file&quot;, &quot;response&quot;):
    # verify the collected Python objects
    assert isinstance(response, ChatCompletionResponse)

    status = response.choices[0].message.content[:7]
    tokens = response.usage.total_tokens
    print(f&quot;{file.get_uri()}: {status}, file size: {file.size}, tokens: {tokens}&quot;)"><pre><span>chain</span> <span>=</span> <span>DataChain</span>.<span>from_dataset</span>(<span>"response"</span>)

<span># Iterating one-by-one: support out-of-memory workflow</span>
<span>for</span> <span>file</span>, <span>response</span> <span>in</span> <span>chain</span>.<span>limit</span>(<span>5</span>).<span>collect</span>(<span>"file"</span>, <span>"response"</span>):
    <span># verify the collected Python objects</span>
    <span>assert</span> <span>isinstance</span>(<span>response</span>, <span>ChatCompletionResponse</span>)

    <span>status</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>[:<span>7</span>]
    <span>tokens</span> <span>=</span> <span>response</span>.<span>usage</span>.<span>total_tokens</span>
    <span>print</span>(<span>f"<span><span>{</span><span>file</span>.<span>get_uri</span>()<span>}</span></span>: <span><span>{</span><span>status</span><span>}</span></span>, file size: <span><span>{</span><span>file</span>.<span>size</span><span>}</span></span>, tokens: <span><span>{</span><span>tokens</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gs://datachain-demo/chatbot-KiT/1.txt: Success, file size: 1776, tokens: 548
gs://datachain-demo/chatbot-KiT/10.txt: Failure, file size: 11576, tokens: 3578
gs://datachain-demo/chatbot-KiT/11.txt: Failure, file size: 2045, tokens: 628
gs://datachain-demo/chatbot-KiT/12.txt: Failure, file size: 3833, tokens: 1207
gs://datachain-demo/chatbot-KiT/13.txt: Success, file size: 3657, tokens: 1101"><pre>gs://datachain-demo/chatbot-KiT/1.txt: Success, file size: 1776, tokens: 548
gs://datachain-demo/chatbot-KiT/10.txt: Failure, file size: 11576, tokens: 3578
gs://datachain-demo/chatbot-KiT/11.txt: Failure, file size: 2045, tokens: 628
gs://datachain-demo/chatbot-KiT/12.txt: Failure, file size: 3833, tokens: 1207
gs://datachain-demo/chatbot-KiT/13.txt: Success, file size: 3657, tokens: 1101</pre></div>
<a name="user-content-vectorized-analytics-over-python-objects"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Vectorized analytics over Python objects</h2><a id="user-content-vectorized-analytics-over-python-objects" aria-label="Permalink: Vectorized analytics over Python objects" href="#vectorized-analytics-over-python-objects"></a></p>
<p dir="auto">Some operations can run inside the DB without deserialization.
For instance, let's calculate the total cost of using the LLM APIs, assuming the Mixtral call costs $2 per 1M input tokens and $6 per 1M output tokens:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chain = DataChain.from_dataset(&quot;mistral_dataset&quot;)

cost = chain.sum(&quot;response.usage.prompt_tokens&quot;)*0.000002 \
           + chain.sum(&quot;response.usage.completion_tokens&quot;)*0.000006
print(f&quot;Spent ${cost:.2f} on {chain.count()} calls&quot;)"><pre><span>chain</span> <span>=</span> <span>DataChain</span>.<span>from_dataset</span>(<span>"mistral_dataset"</span>)

<span>cost</span> <span>=</span> <span>chain</span>.<span>sum</span>(<span>"response.usage.prompt_tokens"</span>)<span>*</span><span>0.000002</span> \
           <span>+</span> <span>chain</span>.<span>sum</span>(<span>"response.usage.completion_tokens"</span>)<span>*</span><span>0.000006</span>
<span>print</span>(<span>f"Spent $<span><span>{</span><span>cost</span>:.2f<span>}</span></span> on <span><span>{</span><span>chain</span>.<span>count</span>()<span>}</span></span> calls"</span>)</pre></div>
<p dir="auto">Output:</p>

<a name="user-content-pytorch-data-loader"></a>
<p dir="auto"><h2 tabindex="-1" dir="auto">PyTorch data loader</h2><a id="user-content-pytorch-data-loader" aria-label="Permalink: PyTorch data loader" href="#pytorch-data-loader"></a></p>
<p dir="auto">Chain results can be exported or passed directly to PyTorch dataloader.
For example, if we are interested in passing image and a label based on file
name suffix, the following code will do it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from torch.utils.data import DataLoader
from transformers import CLIPProcessor

from datachain import C, DataChain

processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

chain = (
    DataChain.from_storage(&quot;gs://datachain-demo/dogs-and-cats/&quot;, type=&quot;image&quot;)
    .map(label=lambda name: name.split(&quot;.&quot;)[0], params=[&quot;file.name&quot;])
    .select(&quot;file&quot;, &quot;label&quot;).to_pytorch(
        transform=processor.image_processor,
        tokenizer=processor.tokenizer,
    )
)
loader = DataLoader(chain, batch_size=1)"><pre><span>from</span> <span>torch</span>.<span>utils</span>.<span>data</span> <span>import</span> <span>DataLoader</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>CLIPProcessor</span>

<span>from</span> <span>datachain</span> <span>import</span> <span>C</span>, <span>DataChain</span>

<span>processor</span> <span>=</span> <span>CLIPProcessor</span>.<span>from_pretrained</span>(<span>"openai/clip-vit-base-patch32"</span>)

<span>chain</span> <span>=</span> (
    <span>DataChain</span>.<span>from_storage</span>(<span>"gs://datachain-demo/dogs-and-cats/"</span>, <span>type</span><span>=</span><span>"image"</span>)
    .<span>map</span>(<span>label</span><span>=</span><span>lambda</span> <span>name</span>: <span>name</span>.<span>split</span>(<span>"."</span>)[<span>0</span>], <span>params</span><span>=</span>[<span>"file.name"</span>])
    .<span>select</span>(<span>"file"</span>, <span>"label"</span>).<span>to_pytorch</span>(
        <span>transform</span><span>=</span><span>processor</span>.<span>image_processor</span>,
        <span>tokenizer</span><span>=</span><span>processor</span>.<span>tokenizer</span>,
    )
)
<span>loader</span> <span>=</span> <span>DataLoader</span>(<span>chain</span>, <span>batch_size</span><span>=</span><span>1</span>)</pre></div>
<a name="user-content-tutorials"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tutorials</h3><a id="user-content-tutorials" aria-label="Permalink: Tutorials" href="#tutorials"></a></p>
<ul dir="auto">
<li><a href="https://datachain.dvc.ai/" rel="nofollow">Getting Started</a></li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/multimodal/clip_fine_tuning.ipynb">Multimodal</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/multimodal/clip_fine_tuning.ipynb" rel="nofollow">Colab</a>)</li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/llm/llm_chatbot_evaluation.ipynb">LLM evaluations</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/llm/llm_chatbot_evaluation.ipynb" rel="nofollow">Colab</a>)</li>
<li><a href="https://github.com/iterative/datachain-examples/blob/main/formats/json-metadata-tutorial.ipynb">Reading JSON metadata</a> (try in <a href="https://colab.research.google.com/github/iterative/datachain-examples/blob/main/formats/json-metadata-tutorial.ipynb" rel="nofollow">Colab</a>)</li>
</ul>
<a name="user-content-contributions"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributions</h3><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">Contributions are very welcome.
To learn more, see the <a href="https://github.com/iterative/datachain/blob/main/CONTRIBUTING.rst">Contributor Guide</a>.</p>
<a name="user-content-community-and-support"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Community and Support</h3><a id="user-content-community-and-support" aria-label="Permalink: Community and Support" href="#community-and-support"></a></p>
<ul dir="auto">
<li><a href="https://datachain.dvc.ai/" rel="nofollow">Docs</a></li>
<li><a href="https://github.com/iterative/datachain/issues">File an issue</a> if you encounter any problems</li>
<li><a href="https://dvc.org/chat" rel="nofollow">Discord Chat</a></li>
<li><a href="mailto:support@dvc.org">Email</a></li>
<li><a href="https://twitter.com/DVCorg" rel="nofollow">Twitter</a></li>
</ul>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing secure Go code (276 pts)]]></title>
            <link>https://jarosz.dev/article/writing-secure-go-code/</link>
            <guid>42043939</guid>
            <pubDate>Mon, 04 Nov 2024 17:34:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jarosz.dev/article/writing-secure-go-code/">https://jarosz.dev/article/writing-secure-go-code/</a>, See on <a href="https://news.ycombinator.com/item?id=42043939">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    

    <div>
    

    <p><span>2024-11-02</span>
            
        

        

        
            <span>
                
                    
                    
                        <a href="https://jarosz.dev/categories/go/">Go</a>
                    
                
            </span>
        

        
    </p>

    
        

        <p>What does it mean to keep security in mind when writing Go code? Answering this question in one short article seems impossible. For this reason, we will narrow it down to a few specific practices.</p>
<p>They will lead to writing robust, secure and performant code when applied continuously.</p>
<ul>
<li>How do we stay informed about the Go security announcements?</li>
<li>How do we keep our Go code patched and up to date?</li>
<li>How do we test our Go code focusing on security and robustness?</li>
<li>What are CVEs, and where do we learn about the most common software vulnerabilities?</li>
</ul>
<h2 id="mailing-list">Mailing list</h2>
<p>Let‚Äôs start with the most obvious place - the Go mailing list. We need to subscribe to get all critical security information right from the source.
All releases that contain security fixes are announced to the <code><a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="bcdbd3d0ddd2db91ddd2d2d3c9d2dfd9fcdbd3d3dbd0d9dbced3c9cccf92dfd3d1">[email&nbsp;protected]</a></code> list.
Once we subscribe to the list, we can be sure we won‚Äôt miss any important announcements.</p>
<h2 id="keeping-go-version-up-to-date">Keeping Go version up to date</h2>
<p>The second step is to keep the Go versions in our projects current. Even though we don‚Äôt use the latest and greatest language features, bumping the Go version gives us all security patches for discovered vulnerabilities. Also, the new Go version ensures compatibility with newer dependencies.
It protects our applications from potential integration issues.</p>
<p>The third step is to learn which security issues and CVEs are addressed in what Go releases. We can check it on the Go release history website and then update it to the latest version in the <code>go.mod</code> files in our projects.</p>
<p>After upgrading to new versions of Go, we should ensure that the operation does not introduce compatibility and dependency problems, especially with third-party packages. It can be more risky when we work on large projects with tens and sometimes hundreds of direct and indirect package dependencies.</p>
<p>The point is to maintain the risk by eliminating potential dependency problems. The problems may include an urgent need to refactor the existing code to make it work with a new dependency. Examples of such issues include changed packages, APIs or function signatures.</p>

<p>We can concentrate on the project source code after we know we will use the Go version without security issues. We can start assessing code quality and security by employing static code analysers.</p>
<h3 id="vet">vet</h3>
<p>Before installing and using third-party analysers, it‚Äôs a good idea to use the Go ‚Äúnative‚Äù <code>go vet</code> command.</p>
<p>We can use the <code>go vet</code> command to analyse our Go code. The <code>go vet</code> command without arguments runs the tool with all options allowed by default. The tool scans the source code and reports potential issues. The issues include code syntax errors and certain programming constructs that can cause problems during program executions.</p>
<p>Most common issues include goroutine mistakes, unused variables and unreachable areas of the codebase. The main advantage of using the <code>go vet</code> command is that it is a part of the Go toolbox.</p>
<p>In a separate article, we will dive deeper into the <code>vet</code> details. The extensive documentation and examples are on the <code>go vet</code> <a href="https://golang.google.cn/cmd/vet/">website</a>.</p>
<h3 id="staticcheck">staticcheck</h3>
<p>Staticcheck is another static code analyser. It‚Äôs a third-party linter that helps to find bugs and detects possible performance problems. It also enforces Go language styling. It offers code simplifications, explains discovered issues and suggests corrections with examples.</p>
<p>Besides running staticcheck in a CI pipeline, we can install <code>staticcheck</code> on our laptops as a standalone binary and scan the code locally. Let‚Äôs install the latest version:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>go install honnef.co/go/tools/cmd/staticcheck@latest
</span></span></code></pre></div><p>No errors on the terminal? If so, we are ready to run the scans. But first, let‚Äôs check the installed version to ensure everything looks good.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>staticcheck --version
</span></span><span><span>staticcheck 2024.1.1 <span>(</span>0.5.1<span>)</span>
</span></span></code></pre></div><p>Similarly to the <code>go vet</code>, running <code>staticcheck</code> without arguments invokes all code analysers by default. This approach plays nicely with the UNIX programming philosophy of using sensible defaults and not forcing users to do unnecessary paperwork.</p>
<p>Let‚Äôs see what the tool can find in the <a href="https://github.com/nginx/agent">NGINX Agent</a> GitHub repository. First, we need to clone it:</p>
<p>Then, we can run it from the root directory of the project:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  agent git:<span>(</span>main<span>)</span> ‚úó staticcheck ./...
</span></span></code></pre></div><p>After a short moment, we are ready to check the scanning results. We can categorise the listed examples into three groups:</p>
<ul>
<li>packages, methods or functions that are deprecated, for example:</li>
</ul>
<div><pre tabindex="0"><code data-lang="shell"><span><span>...
</span></span><span><span>src/core/metrics/sources/cpu.go:111:9: times.Total is deprecated: Total returns the total number of seconds in a CPUTimesStat Please <span>do</span> not use this internal <span>function</span>. <span>(</span>SA1019<span>)</span>
</span></span><span><span>...
</span></span><span><span>test/component/nginx-app-protect/monitoring/monitoring_test.go:15:8: <span>"github.com/golang/protobuf/jsonpb"</span> is deprecated: Use the <span>"google.golang.org/protobuf/encoding/protojson"</span> package instead. <span>(</span>SA1019<span>)</span>
</span></span></code></pre></div><ul>
<li>unused variables and fields, for example:</li>
</ul>
<div><pre tabindex="0"><code data-lang="shell"><span><span>src/core/metrics/sources/nginx_plus.go:74:2: field endpoints is unused <span>(</span>U1000<span>)</span>
</span></span><span><span>src/core/metrics/sources/nginx_plus.go:75:2: field streamEndpoints is unused <span>(</span>U1000<span>)</span>
</span></span><span><span>src/core/metrics/sources/nginx_plus_test.go:94:2: var availableZones is unused <span>(</span>U1000<span>)</span>
</span></span></code></pre></div><ul>
<li>possible problems related to the quality of the code, for example:</li>
</ul>
<div><pre tabindex="0"><code data-lang="shell"><span><span>src/core/nginx.go:791:4: ineffective break statement. Did you mean to break out of the outer loop? <span>(</span>SA4011<span>)</span>
</span></span></code></pre></div><p>Now, we are ready to start analysing the highlighted issues. A detailed deep dive into the codebase is outside this introductory article‚Äôs scope. We will do deeper code analysis, show examples, and fix security and performance issues in upcoming articles.</p>
<p>For now, let‚Äôs take note of CWE websites that contain tons of information about listed weaknesses so we can study them at a later time:</p>
<ul>
<li>Unused variables</li>
<li>Using deprecated constructs</li>
</ul>
<h3 id="golangci-lint">golangci-lint</h3>
<p>The third code analyser we are going to employ is <code>golangci-lint</code>. As with all Go tools, we can install it in a variety of ways, including the <code>go install</code> command:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest
</span></span></code></pre></div><p>Let‚Äôs verify if the installation went well and check the version:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>golangci-lint --version
</span></span><span><span>golangci-lint has version v1.61.0 built with go1.23.2
</span></span><span><span>...
</span></span></code></pre></div><p>Perfect! All looks good.</p>
<p>Following the same principle of the least surprise, <code>golangci-lint</code> runs all linters when we invoke it with no arguments.</p>
<blockquote>
<p>Rule of Least Surprise: In interface design, always do the least surprising thing.</p>
</blockquote>
<p>What happens when we check the cloned earlier <code>agent</code> repository? Will <code>golangci-lint</code> show us the same warnings and suggestions? Let‚Äôs find out.</p>
<p>As previously, we will start scanning the project from its root directory.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  agent git:<span>(</span>main<span>)</span> ‚úó golangci-lint run ./...
</span></span></code></pre></div><p>Almost immediately, we noticed a list of suggestions for improving the code! For example:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>src/extensions/nginx-app-protect/monitoring/processor/nap_test.go:60:14: S1025: the argument is already a string, there<span>'</span>s no need to use fmt.Sprintf <span>(</span>gosimple<span>)</span>
</span></span><span><span> logEntry: fmt.Sprintf<span>(</span><span>`</span>%s<span>`</span>, func<span>()</span> string <span>{</span>
</span></span><span><span> ^
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="shell"><span><span>src/plugins/common.go:85:5: S1009: should omit nil check; len<span>()</span> <span>for</span> <span>[]</span>string is defined as zero <span>(</span>gosimple<span>)</span>
</span></span><span><span> <span>if</span> loadedConfig.Extensions !<span>=</span> nil <span>&amp;&amp;</span> len<span>(</span>loadedConfig.Extensions<span>)</span> &gt; <span>0</span> <span>{</span>
</span></span><span><span>    ^
</span></span></code></pre></div><p>The linter points to exact files and lines that need our attention. Our job now is to assess the code, make changes, run the liner a second time and run all unit tests. If the tests are green, we can commit updated code. Job done! Ok, we still need to push it to the remote.</p>
<h2 id="detecting-race-conditions">Detecting race conditions</h2>
<p>Race conditions in our programs and libraries can occur when multiple goroutines try to access a resource concurrently. These conditions
are detected when at least one goroutine tries to write (change) the resource. For example, the resource can be a global, package-level variable that acts as a counter.
This situation in a program can lead to subtle, very hard-to-diagnose and detect bugs.</p>
<p>Go has native support for testing such conditions. We run tests using the Go <code>test</code> tool with the argument <code>-race</code>.
This method will run the race detector and help identify problems in concurent programs.</p>
<p>There is one warning we need to remember. The detector can assess the executed code and will ignore code paths that are not executed. So, it‚Äôs crucial to run static code analysers first and make sure we do not have so-called dead code in our project.</p>
<p>When we tell Go: ‚ÄúHey, run tests with the <code>-race</code> argument‚Äù, the Go compiler compiles the code with the race detector enabled. Then, tests are run, and possible race conditions are checked at runtime. When races are detected, the tool will print a detailed report. It will show what goroutines try to access which resources.</p>
<p>Another way to increase the chances of detecting concurrency issues is to run tests in parallel. To do so we need to inform the runner explicitly by adding <code>t.Parallel()</code> to our tests.</p>
<p>Two tests executed in parallel</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>TestParseDiskSpace</span>(<span>t</span> <span>*</span><span>testing</span>.<span>T</span>) {
</span></span><span><span>    <span>t</span>.<span>Parallel</span>()
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>TestParseMemoryUsage</span>(<span>t</span> <span>*</span><span>testing</span>.<span>T</span>) {
</span></span><span><span>    <span>t</span>.<span>Parallel</span>()
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><p>Detecting race conditions and designing concurrent code is a vast and exciting topic that we will discuss in the future.</p>
<h2 id="scanning-source-code-for-vulnerabilities">Scanning source code for vulnerabilities</h2>
<h3 id="govulncheck">govulncheck</h3>
<p>We have a broad choice of tools that scan the codebase for known vulnerabilities listed in the <a href="https://www.cve.org/">CVEs</a> database.</p>
<p>Our default tool for ensuring we develop and release safe code is <code>govulncheck</code>. We can install it locally on a developer‚Äôs machine and run scans locally before committing and pushing our code to a remote Git repository.</p>
<p>Optionally, we can integrate the scanning step with CI pipelines in GitHub or GitLab. Then, the scan can be invoked on each merge request to ensure we do not introduce vulnerabilities in the project.</p>
<p><code>govulncheck</code> is developed by the Go team. A dedicated database of Go vulnerabilities provides information for the scanner.
Let‚Äôs install <code>govulncheck</code> locally and try basic functionality.</p>
<p>To install the latest version, we need to run the following command:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>go install golang.org/x/vuln/cmd/govulncheck@latest
</span></span></code></pre></div><p>It‚Äôs time to check if the installation process went well:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>govulncheck -version
</span></span><span><span>Go: go1.23.2
</span></span><span><span>Scanner: <a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="dabdb5acafb6b4b9b2bfb9b19aacebf4ebf4e9">[email&nbsp;protected]</a>
</span></span><span><span>DB: https://vuln.go.dev
</span></span><span><span>DB updated: 2024-10-17 15:37:30 +0000 UTC
</span></span><span><span>...
</span></span></code></pre></div><p>We are ready to run our first scan. Let‚Äôs clone the <a href="https://github.com/qba73/habit">habit</a> git repository. Then, navigate to its root directory and run the tool.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  habit git:<span>(</span>main<span>)</span> ‚úó govulncheck
</span></span><span><span>No vulnerabilities found.
</span></span></code></pre></div><p>It looks promising! We did not find vulnerabilities in the source code. Are we done? Not really! We built the habit binary when the <code>go.mod</code> file defined the version of Go 1.18. The current version is v1.23.2.</p>
<p>Let‚Äôs scan the habit binary, not the source code.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  habit git:<span>(</span>main<span>)</span> ‚úó govulncheck -mode binary -show verbose habit
</span></span></code></pre></div><p>We run <code>govulncheck</code> in the binary mode. It means that we can scan any Go binary we have access to! We do not need source code! Then, we run the scan in the verbose mode. It will show the complete report broken into multiple sections. The last argument is the name of the binary we want to scan.</p>
<p>Hmmm! This report does look different! What just happened?</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>Scanning your binary <span>for</span> known vulnerabilities...
</span></span><span><span>
</span></span><span><span>Fetching vulnerabilities from the database...
</span></span><span><span>
</span></span><span><span>Checking the binary against the vulnerabilities...
</span></span><span><span>
</span></span><span><span><span>===</span> Symbol Results <span>===</span>
</span></span><span><span>
</span></span><span><span>No vulnerabilities found.
</span></span><span><span>
</span></span><span><span><span>===</span> Package Results <span>===</span>
</span></span><span><span>
</span></span><span><span>Vulnerability <span>#1: GO-2023-2186</span>
</span></span><span><span>    Incorrect detection of reserved device names on Windows in path/filepath
</span></span><span><span>  More info: https://pkg.go.dev/vuln/GO-2023-2186
</span></span><span><span>  Standard library
</span></span><span><span>    Found in: path/<a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="53353a3f362332273b13343c627d61637d66">[email&nbsp;protected]</a>
</span></span><span><span>    Fixed in: path/<a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="ea8c83868f9a8b9e82aa8d85dbc4d8dac4dbdb">[email&nbsp;protected]</a>
</span></span><span><span>
</span></span><span><span><span>===</span> Module Results <span>===</span>
</span></span><span><span>
</span></span><span><span>Vulnerability <span>#1: GO-2024-3107</span>
</span></span><span><span>    Stack exhaustion in Parse in go/build/constraint
</span></span><span><span>  More info: https://pkg.go.dev/vuln/GO-2024-3107
</span></span><span><span>  Standard library
</span></span><span><span>    Found in: <a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="572423333b3e35173038667965677962">[email&nbsp;protected]</a>
</span></span><span><span>    Fixed in: <a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="a8dbdcccc4c1cae8cfc799869a9a869f">[email&nbsp;protected]</a>
</span></span><span><span>...
</span></span><span><span>
</span></span><span><span>Vulnerability <span>#18: GO-2023-1878</span>
</span></span><span><span>    Insufficient sanitisation of Host header in net/http
</span></span><span><span>  More info: https://pkg.go.dev/vuln/GO-2023-1878
</span></span><span><span>  Standard library
</span></span><span><span>    Found in: <a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="374443535b5e55775058061905071902">[email&nbsp;protected]</a>
</span></span><span><span>    Fixed in: <a href="https://jarosz.dev/cdn-cgi/l/email-protection" data-cfemail="98ebecfcf4f1fad8fff7a9b6aaa8b6ae">[email&nbsp;protected]</a>
</span></span><span><span>
</span></span><span><span>Your code is affected by <span>0</span> vulnerabilities.
</span></span><span><span>This scan also found <span>1</span> vulnerability in packages you import and <span>18</span>
</span></span><span><span>vulnerabilities in modules you require, but your code doesn<span>'</span>t appear to call
</span></span><span><span>these vulnerabilities.
</span></span></code></pre></div><p>The first section contains the most important message: <strong>No vulnerabilities found</strong>.</p>
<p>The remaining sections contain information about other vulnerabilities discovered in standard Go libraries. Ok, but are we affected? Is our program not secure?</p>
<p>The final scan report tells us we should not worry. Our program <em>doesn‚Äôt appear to call
these vulnerabilities</em>! Happy days!</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>Your code is affected by <span>0</span> vulnerabilities.
</span></span><span><span>This scan also found <span>1</span> vulnerability in packages you import and <span>18</span>
</span></span><span><span>vulnerabilities in modules you require, but your code doesn<span>'</span>t appear to call
</span></span><span><span>these vulnerabilities.
</span></span></code></pre></div><p>Let‚Äôs update the <code>go.mod</code> file and change the Go version to the latest <code>1.23</code>. Next, we need to run <code>go mod tidy</code> to get all dependencies up to date.
At this point, we are ready to build the binary again.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  habit git:<span>(</span>main<span>)</span> ‚úó go build -o habit cmd/main.go
</span></span></code></pre></div><p>Let‚Äôs rerun the scan.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>‚ûú  habit git:<span>(</span>main<span>)</span> ‚úó govulncheck -mode binary -show verbose habit
</span></span><span><span>Scanning your binary <span>for</span> known vulnerabilities...
</span></span><span><span>
</span></span><span><span>Fetching vulnerabilities from the database...
</span></span><span><span>
</span></span><span><span>Checking the binary against the vulnerabilities...
</span></span><span><span>
</span></span><span><span>No vulnerabilities found.
</span></span></code></pre></div><p>That‚Äôs what we wanted! We upgraded the Go version, pulled dependencies and verified that our software and dependencies were free from CVEs.</p>
<h3 id="gosec">gosec</h3>
<p><a href="https://github.com/securego/gosec"><code>gosec</code></a> is a static code analyzer. It helps to find insecure code constructs.
We can install it locally on our laptops or run it as a GitHub Action in a CI pipeline. As described earlier, <code>golangci-lint</code> includes the <code>gosec</code> as a plugin and runs it as default on each code scan.</p>
<p>Let‚Äôs give it a try and install the scanner locally.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>go install github.com/securego/gosec/v2/cmd/gosec@latest
</span></span></code></pre></div><p>If we do not see errors, <code>gosec</code> is ready for action. Before running our first scan, let‚Äôs look at the menu:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>gosec -h
</span></span><span><span>
</span></span><span><span>gosec - Golang security checker
</span></span><span><span>
</span></span><span><span>gosec analyses Go source code to look <span>for</span> common programming mistakes that
</span></span><span><span>can lead to security problems.
</span></span><span><span>...
</span></span></code></pre></div><p>We can use a long list of options and rules to configure the scanner behaviour. Going into details of specific options is outside of the scope of this article. A detailed tutorial on configuring, running and benefiting from this SAST tool is coming soon! Stay tuned!</p>
<p>To try gosec, we need to clone a GitHub repository with the Go code we want to scan.</p>
<p>Let‚Äôs clone the <a href="https://github.com/CyberRoute/bruter">brutus</a> repository. It‚Äôs an open-source experimental <a href="https://en.wikipedia.org/wiki/Open-source_intelligence">OSINT</a> app for testing web server configuration.</p>
<p>Next, change our current directory to the project‚Äôs root directory and start scanning.</p>
<p>After a couple of seconds, <code>gosec</code> presents the scan report. What can we learn immediately? We see a list of potential issues sorted by severity and confidence.
We know what part of the code needs attention and what weakness classification the issue applies to. Perfect! What‚Äôs next?</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>...
</span></span><span><span>
</span></span><span><span><span>[</span>/.../bruter/pkg/fuzzer/randomua.go:69<span>]</span> - G404 <span>(</span>CWE-338<span>)</span>: Use of weak random number generator <span>(</span>math/rand or math/rand/v2 instead of crypto/rand<span>)</span> <span>(</span>Confidence: MEDIUM, Severity: HIGH<span>)</span>
</span></span><span><span>    68:
</span></span><span><span>  &gt; 69:  randomIndex :<span>=</span> rand.Intn<span>(</span>len<span>(</span>userAgents<span>))</span>
</span></span><span><span>    70:  <span>return</span> userAgents<span>[</span>randomIndex<span>]</span>
</span></span><span><span>
</span></span><span><span>...
</span></span><span><span>
</span></span><span><span><span>[</span>/.../bruter/pkg/server/config.go:40<span>]</span> - G402 <span>(</span>CWE-295<span>)</span>: TLS InsecureSkipVerify set true. <span>(</span>Confidence: HIGH, Severity: HIGH<span>)</span>
</span></span><span><span>    39:  customTransport :<span>=</span> &amp;http.Transport<span>{</span>
</span></span><span><span>  &gt; 40:   TLSClientConfig: &amp;tls.Config<span>{</span>InsecureSkipVerify: true<span>}</span>,
</span></span><span><span>    41:  <span>}</span>
</span></span><span><span>
</span></span><span><span>...
</span></span></code></pre></div><p>At this stage of our investigation, we can check reported CWEs and learn about details of listed weaknesses. For example, the second listed issue brings us to the <a href="https://cwe.mitre.org/data/definitions/295.html">CWE-295</a> website, where we can learn more about vulnerability.</p>
<h2 id="fuzzing">Fuzzing</h2>
<p>The last method of checking code quality and discovering vulnerabilities is fuzz testing. Fuzzing is a special kind of automated testing. It uses code test coverage to manipulate randomly generated input data.</p>
<p>It‚Äôs extremely helpful in finding potential security flaws like buffer overflows, <a href="https://owasp.org/www-community/attacks/SQL_Injection">SQL injections</a>, <a href="https://owasp.org/www-community/attacks/Denial_of_Service">DoS attacks</a> and <a href="https://owasp.org/www-community/attacks/xss/">XSS attacks</a>. The most crucial attribute of fuzzing is that many input combinations are generated automatically! Developers don‚Äôt need to scratch their heads trying to figure out hundreds, if not thousands, of input data combinations! What a relief!</p>
<p>We will focus on fuzzing in more detail in upcoming tutorials.</p>
<p>Most of the methods and testing techniques we discussed today are encouraged by <a href="https://openssf.org/best-practices-badge/">OpenSSF</a> foundation. Open source projects that want to get the Best Practice Badge are required to meet <a href="https://www.bestpractices.dev/en/criteria/0">FLOSS criteria</a> in areas like licencing, change control, vulnerability reporting, quality, security and static and dynamic security code analysis.</p>
<p>Stay secure, free from CVEs and enjoy programming!</p>
<p>As <a href="https://bitfieldconsulting.com/">John Arundel</a> says:</p>
<blockquote>
<p>‚ÄúProgramming is fun, and you should have fun!‚Äù</p>
</blockquote>
<p>Till next time!</p>
    
</div>



    

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DB48X: High Performance Scientific Calculator, Reinvented (156 pts)]]></title>
            <link>http://48calc.org/</link>
            <guid>42043747</guid>
            <pubDate>Mon, 04 Nov 2024 17:19:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://48calc.org/">http://48calc.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42043747">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="page-wrapper">
                                            
                                            
<h2>High Performance Scientific Calculator, Reinvented</h2>
<p>DB48X is a modern scientific calculator in the spirit of the Hewlett-Packard HP48X, featuring a complete, from-scratch reimplementation of the Reverse Polish Lisp (RPL) programming language. The firmware is initially designed for the SwissMicros <a href="https://www.swissmicros.com/product/model-dm32">DM32</a> and <a href="https://www.swissmicros.com/product/dm42">DM42</a> calculators. In other words, it is optimized for physical hardware that you can keep in your pocket. There is also an <a href="https://apps.apple.com/fr/app/db50x/id6625971367?l=en-GB">iPhone version</a> if you want to run this on your phone or display graphs in colour.</p>
<p>You can try the calculator firmware directly in your browser by clicking on the following image:</p>
<p>
    <a href="http://48calc.org/db48x/index.html">
        <img src="http://48calc.org/DB48X%20startup%20screen.png" width="420">
    </a>
</p>

                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Convert any website into a React component (217 pts)]]></title>
            <link>https://chromewebstore.google.com/detail/html-to-react-figma-by-ma/chgehghmhgihgmpmdjpolhkcnhkokdfp</link>
            <guid>42043552</guid>
            <pubDate>Mon, 04 Nov 2024 17:03:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chromewebstore.google.com/detail/html-to-react-figma-by-ma/chgehghmhgihgmpmdjpolhkcnhkokdfp">https://chromewebstore.google.com/detail/html-to-react-figma-by-ma/chgehghmhgihgmpmdjpolhkcnhkokdfp</a>, See on <a href="https://news.ycombinator.com/item?id=42043552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="a9kxte"><main jslog="171177; metadata:W1siY2hnZWhnaG1oZ2loZ21wbWRqcG9saGtjbmhrb2tkZnAiLGZhbHNlLDEsZmFsc2UsWzE3MTg5MTkxMDAsODY5MjU0MDAwXV1d;" jsdata="PM4Exd;_;3"><div><div jscontroller="zKiH5d" jsaction="rcuQ6b:npT2md;qako4e:wsIe3c;PEkjme:Jouqqb"><p><img src="https://lh3.googleusercontent.com/3TAiOBbM-XhIIRHO8u0b4sdZ4ytPy2ZTfXKZ_RnlCetU-3u3k99RqJqUD_wNLo03RQUcAb06dmAfMeCESvj4IMdEuw8=s60" srcset="https://lh3.googleusercontent.com/3TAiOBbM-XhIIRHO8u0b4sdZ4ytPy2ZTfXKZ_RnlCetU-3u3k99RqJqUD_wNLo03RQUcAb06dmAfMeCESvj4IMdEuw8=s120 2x" alt="Item logo image for HTML to React &amp; Figma by Magic Patterns" jscontroller="OhgRI" jsaction="error:LILo6;"></p></div><section><div><h2><p>Overview</p></h2></div><div jsname="ij8cu" jscontroller="qv5bsb" jsaction="click:i7GaQb(rs1XOd);rcuQ6b:npT2md"><p>Convert HTML from any page to React and/or Figma. Edit with AI.</p><p>Convert a page or a section of a page to React code or an editable Figma design.

What you can do with HTML to React/Figma:
- Grab an existing design that you like and instantly get React code you can use in your own project
- Import existing designs to edit and work off instead of starting from scratch
- Use AI to customize and modify existing designs


Created and supported by Magic Patterns.</p></div></section><c-wiz jsrenderer="db7dHd" jsshadow="" jsdata="deferred-i17" data-p="%.@.&quot;chgehghmhgihgmpmdjpolhkcnhkokdfp&quot;]" data-node-index="2;0" jsmodel="hc6Ubd" c-wiz=""><div jsname="zpNSJe" jscontroller="UkbOSe" jsaction="rcuQ6b:JlVUp,Idkf0e"><section jscontroller="ECBKud" jsname="ShBeI" jsaction="change:BbBN0b(uHcrq); keydown:AWuxGc,pu39sc(uHcrq); input:pu39sc(uHcrq); keyup:BbBN0b(uHcrq);UKfqQ:BbBN0b(RZG7Xe);Fn2EM:HZuyid;KbznFd:mRsukd"><img src="https://lh3.googleusercontent.com/a-/ALV-UjXTRTYGFtMGt3WOe8tcWzd1gxAgYfLvrYaAQuUUeXUNv3QiGS3L=s48-w48-h48" srcset="https://lh3.googleusercontent.com/a-/ALV-UjXTRTYGFtMGt3WOe8tcWzd1gxAgYfLvrYaAQuUUeXUNv3QiGS3L=s96-w96-h96 2x" alt="Review's profile picture"><div><h3 id="i18"><span>Leanne Van Der Westhuizen</span><span>Aug 31, 2024</span></h3></div><p jsname="f27TO">Great innovative Extension. Wel Done!... So far I like it... BUT! ADD SOME MORE FUNCTIONATITY. 5X‚≠ê</p></section><section jscontroller="ECBKud" jsname="ShBeI" jsaction="change:BbBN0b(uHcrq); keydown:AWuxGc,pu39sc(uHcrq); input:pu39sc(uHcrq); keyup:BbBN0b(uHcrq);UKfqQ:BbBN0b(RZG7Xe);Fn2EM:HZuyid;KbznFd:mRsukd"><img src="https://lh3.googleusercontent.com/a-/ALV-UjW0TQNaVOSddxr-mCcjFsMO33IeuRgyn4S24orM_vIaJV2gvJk=s48-w48-h48" srcset="https://lh3.googleusercontent.com/a-/ALV-UjW0TQNaVOSddxr-mCcjFsMO33IeuRgyn4S24orM_vIaJV2gvJk=s96-w96-h96 2x" alt="Review's profile picture"><div><h3 id="i19"><span>Steve Witmer</span><span>Aug 28, 2024</span></h3></div><p jsname="f27TO">Big fan of this tool. One of my biggest use cases is to quickly grab UI inspiration from apps/websites, and copy &amp; paste it into Figma (another feature of their platform - via a plugin). Alex &amp; Teddy are always pushing new features, updates &amp; fixes. Bullish.</p><p><span jscontroller="KfQkxf" jsaction="h7N06c:MKYwnd" jsname="cDaQFd">2 out of 2 found this helpful</span></p></section><section jscontroller="ECBKud" jsname="ShBeI" jsaction="change:BbBN0b(uHcrq); keydown:AWuxGc,pu39sc(uHcrq); input:pu39sc(uHcrq); keyup:BbBN0b(uHcrq);UKfqQ:BbBN0b(RZG7Xe);Fn2EM:HZuyid;KbznFd:mRsukd"><img src="https://lh3.googleusercontent.com/a/ACg8ocJM4rboCzbPhAnzwu7jiJH08EdiVlDNQqQi2koWdo4s699QoA=s48-w48-h48" srcset="https://lh3.googleusercontent.com/a/ACg8ocJM4rboCzbPhAnzwu7jiJH08EdiVlDNQqQi2koWdo4s699QoA=s96-w96-h96 2x" alt="Review's profile picture"><div><h3 id="i20"><span>Jake Stevens</span><span>Aug 28, 2024</span></h3></div><p jsname="f27TO">You can literally make an in-browser flappy bird game with one sentence. If that doesn't compel you to give it a try, eat sand.</p><p><span jscontroller="KfQkxf" jsaction="h7N06c:MKYwnd" jsname="cDaQFd">2 out of 2 found this helpful</span></p></section></div><c-data id="i17" jsdata=" PM4Exd;_;3 DTn4b;_;1"></c-data></c-wiz><section><p><h2>Details</h2></p><div><ul><li><p>Version</p><p>0.0.22</p></li><li><p>Updated</p><p>November 2, 2024</p></li><li></li><li><p>Size</p><p>426KiB</p></li><li><p>Languages</p></li><li><p>Developer</p><div><p>North Park Labs, Inc.<br>3127 Folsom St
Apt A
San Francisco, CA 94110-4707
US</p><a href="https://magicpatterns.com/" target="_blank" jscontroller="R6rk4" jsaction="click:AfbQdd"><svg width="20" height="20" viewBox="0 0 24 24" focusable="false"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zM4 12c0-.61.08-1.21.21-1.78L8.99 15v1c0 1.1.9 2 2 2v1.93C7.06 19.43 4 16.07 4 12zm13.89 5.4c-.26-.81-1-1.4-1.9-1.4h-1v-3c0-.55-.45-1-1-1h-6v-2h2c.55 0 1-.45 1-1V7h2c1.1 0 2-.9 2-2v-.41C17.92 5.77 20 8.65 20 12c0 2.08-.81 3.98-2.11 5.4z"></path></svg> Website</a><details><summary><svg width="20" height="20" viewBox="0 0 24 24" focusable="false"><path d="M20 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm-.8 2L12 10.8 4.8 6h14.4zM4 18V7.87l8 5.33 8-5.33V18H4z"></path></svg> Email <svg width="20" height="20" viewBox="0 0 24 24" focusable="false"><path d="M5.41 7.59L4 9l8 8 8-8-1.41-1.41L12 14.17"></path></svg><svg width="20" height="20" viewBox="0 0 24 24" focusable="false"><path d="M18.59 16.41L20 15l-8-8-8 8 1.41 1.41L12 9.83"></path></svg></summary><p>engineering@mirrorful.io</p></details></div></li><li><p>Trader</p><p>This developer has identified itself as a trader per the definition from the European Union.</p></li></ul></div></section><section><p><h2>Privacy</h2></p><div><div><p>HTML to React &amp; Figma by Magic Patterns has disclosed the following information regarding the collection and usage of your data. More detailed information can be found in the developer's <a href="https://www.magicpatterns.com/docs/documentation/legal/privacy" target="_blank">privacy policy</a>.</p><h3>HTML to React &amp; Figma by Magic Patterns handles the following:</h3><div><p><span><svg enable-background="new 0 0 24 24" height="24" viewBox="0 0 24 24" width="24" focusable="false"><g><rect fill="none" height="24" width="24"></rect></g><g><g><path d="M12,2C6.48,2,2,6.48,2,12s4.48,10,10,10s10-4.48,10-10S17.52,2,12,2z M7.35,18.5C8.66,17.56,10.26,17,12,17 s3.34,0.56,4.65,1.5C15.34,19.44,13.74,20,12,20S8.66,19.44,7.35,18.5z M18.14,17.12L18.14,17.12C16.45,15.8,14.32,15,12,15 s-4.45,0.8-6.14,2.12l0,0C4.7,15.73,4,13.95,4,12c0-4.42,3.58-8,8-8s8,3.58,8,8C20,13.95,19.3,15.73,18.14,17.12z"></path><path d="M12,6c-1.93,0-3.5,1.57-3.5,3.5S10.07,13,12,13s3.5-1.57,3.5-3.5S13.93,6,12,6z M12,11c-0.83,0-1.5-0.67-1.5-1.5 S11.17,8,12,8s1.5,0.67,1.5,1.5S12.83,11,12,11z"></path></g></g></svg></span><span>Personally identifiable information</span></p><p><span><svg enable-background="new 0 0 24 24" height="24" viewBox="0 0 24 24" width="24" focusable="false"><g><rect fill="none" height="24" width="24"></rect></g><g><path d="M21,10h-8.35C11.83,7.67,9.61,6,7,6c-3.31,0-6,2.69-6,6s2.69,6,6,6c2.61,0,4.83-1.67,5.65-4H13l2,2l2-2l2,2l4-4.04L21,10z M7,15c-1.65,0-3-1.35-3-3c0-1.65,1.35-3,3-3s3,1.35,3,3C10,13.65,8.65,15,7,15z"></path></g></svg></span><span>Authentication information</span></p></div></div><div><h2>This developer declares that your data is</h2><ul><li>Not being sold to third parties, outside of the <a href="https://developer.chrome.com/docs/webstore/program-policies/limited-use/" target="_blank">approved use cases</a></li><li>Not being used or transferred for purposes that are unrelated to the item's core functionality</li><li>Not being used or transferred to determine creditworthiness or for lending purposes</li></ul></div></div></section><div><h2>Support</h2></div><section><h2>Related</h2><div jsname="QtHpvf" jsaction="rcuQ6b:npT2md" role="region" aria-label="Related items carousel" aria-roledescription="carousel" aria-live="polite" jscontroller="Fu7Bjd" jslog="197219"><div inert=""><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="iiikidmnimlpahbeknmkeonmemajpccj" ssk="1#0"><p><img src="https://lh3.googleusercontent.com/_2HDrH9NpgVFgNFlKd4KVGeKxKYjk4blzP_RxNs0VxXWCpldS-Nm1uAKBF9FKdjl5krtrKWfr5qOnEPDy8i-F3Vk=s60" srcset="https://lh3.googleusercontent.com/_2HDrH9NpgVFgNFlKd4KVGeKxKYjk4blzP_RxNs0VxXWCpldS-Nm1uAKBF9FKdjl5krtrKWfr5qOnEPDy8i-F3Vk=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="Button Stealer" id="i21" role="heading" aria-level="3">Button Stealer</p><p><span><span role="img" aria-label="Average rating 4.9 out of 5 stars. 52 ratings." id="i22"><span>4.9</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(52)</span></span><span></span></span></p><p id="i23">Steals buttons from the websites you visit. Do your usual online stuff and watch the collection of stolen buttons grow.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="nggpkpfmdkbaakpndblpandmldendooa" ssk="1#1"><p><img src="https://lh3.googleusercontent.com/E8Ku4exi6R-SSaPFTK5uz3PC3jP9GAeEgKdb2dsR7KpLKdPDf_Lw6Q52UVYwyBxLsRdX6YiHoAQNgiY0zrQL1TJhkg=s275-w275-h175" srcset="https://lh3.googleusercontent.com/E8Ku4exi6R-SSaPFTK5uz3PC3jP9GAeEgKdb2dsR7KpLKdPDf_Lw6Q52UVYwyBxLsRdX6YiHoAQNgiY0zrQL1TJhkg=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Jinno: code any React component with AI" id="i25" role="heading" aria-level="3">Jinno: code any React component with AI</p><p><span><span role="img" aria-label="Average rating 4.8 out of 5 stars. 38 ratings." id="i26"><span>4.8</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(38)</span></span><span></span></span></p><p id="i27">Develop html or react components with AI and chat GPT4. We can modify the React, HTML and CSS code for you.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="nmgcefdhjpjefhgcpocffdlibknajbmj" ssk="1#2"><p><img src="https://lh3.googleusercontent.com/AgDgVjbRi_tFDFu03B2ThYAiaHsJDRxaDeOYKKlepT8Dxhteq08A4C2T_QDFGJgvRHvIILuw2yvP4NPVkIy6FLI3lA=s275-w275-h175" srcset="https://lh3.googleusercontent.com/AgDgVjbRi_tFDFu03B2ThYAiaHsJDRxaDeOYKKlepT8Dxhteq08A4C2T_QDFGJgvRHvIILuw2yvP4NPVkIy6FLI3lA=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="mymind ‚Äî An extension for your mind" id="i29" role="heading" aria-level="3">mymind ‚Äî An extension for your mind</p><p><span><span role="img" aria-label="Average rating 4.6 out of 5 stars. 54 ratings." id="i30"><span>4.6</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(54)</span></span><span></span></span></p><p id="i31">Add images, bookmarks, notes, quotes or text highlights to your new mind.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="efjcmgblfpkhbjpkpopkgeomfkokpaim" ssk="1#3"><p><img src="https://lh3.googleusercontent.com/9MKf4-JqyeaailX23MzwdeZKlIjAS5J7wHeAd8fE_RRty0YLqc5j5M77MCDLRk1pH3hynaCVV2fVzZ5hEx7EdmK8Tw=s275-w275-h175" srcset="https://lh3.googleusercontent.com/9MKf4-JqyeaailX23MzwdeZKlIjAS5J7wHeAd8fE_RRty0YLqc5j5M77MCDLRk1pH3hynaCVV2fVzZ5hEx7EdmK8Tw=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="HTML to Figma - by Builder.io" id="i33" role="heading" aria-level="3">HTML to Figma - by Builder.io</p><p><span><span role="img" aria-label="Average rating 4.3 out of 5 stars. 45 ratings." id="i34"><span>4.3</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(45)</span></span><span></span></span></p><p id="i35">Import a web page to Figma layers</p></div></div><div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="bjobgelaoehgbnklgcaaehdpckmhkplk" ssk="1#0"><p><img src="https://lh3.googleusercontent.com/D72iPn2Uw4ddke4D9lSSlOmlUpxxkWy9HwI0tEIUmXl_rwQ4YU8HZ52rEWDciEKALCPo7kVF76NCvxiKfqbStpDtqQ=s60" srcset="https://lh3.googleusercontent.com/D72iPn2Uw4ddke4D9lSSlOmlUpxxkWy9HwI0tEIUmXl_rwQ4YU8HZ52rEWDciEKALCPo7kVF76NCvxiKfqbStpDtqQ=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="SuperX (Twitter Analytics)" id="i37" role="heading" aria-level="3">SuperX (Twitter Analytics)</p><p><span><span role="img" aria-label="Average rating 4.6 out of 5 stars. 23 ratings." id="i38"><span>4.6</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(23)</span></span><span></span></span></p><p id="i39">Super insights for your X (Twitter) activities and engagements</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="fojckembkmaoehhmkiomebhkcengcljl" ssk="1#1"><p><img src="https://lh3.googleusercontent.com/8QTPHkGRRrPCJ2NczkedhkTtL9TurDBE1BckjlRDHfizAZk1wz8EPwPu6p0bGJyAdtp9Cvyeo4fR6OEOtCYx6WEu=s275-w275-h175" srcset="https://lh3.googleusercontent.com/8QTPHkGRRrPCJ2NczkedhkTtL9TurDBE1BckjlRDHfizAZk1wz8EPwPu6p0bGJyAdtp9Cvyeo4fR6OEOtCYx6WEu=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Gimli Tailwind" id="i41" role="heading" aria-level="3">Gimli Tailwind</p><p><span><span role="img" aria-label="Average rating 4.5 out of 5 stars. 30 ratings." id="i42"><span>4.5</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(30)</span></span><span></span></span></p><p id="i43">A DevTools extension enabling smart tools for Tailwind CSS.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="djpkcdeoondbcapcllgahjhaghplfcno" ssk="1#2"><p><img src="https://lh3.googleusercontent.com/3qU7xNp05WjndSgQmFyCOTNAQkDmdDo4rhUa-rWGZVLGno0LP0lwPKnAsWYIAG8Q5yroDvRhgATmvKZqTCU6HymexQ=s60" srcset="https://lh3.googleusercontent.com/3qU7xNp05WjndSgQmFyCOTNAQkDmdDo4rhUa-rWGZVLGno0LP0lwPKnAsWYIAG8Q5yroDvRhgATmvKZqTCU6HymexQ=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="Markflow: Copy Elements to Figma, React, etc." id="i45" role="heading" aria-level="3">Markflow: Copy Elements to Figma, React, etc.</p><p><span><span role="img" aria-label="Average rating 3.5 out of 5 stars. 4 ratings." id="i46"><span>3.5</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(4)</span></span><span></span></span></p><p id="i47">Introducing Markflow, the ultimate Chrome extension designed to simplify the workflow for web designers and developers. Easily copy‚Ä¶</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="fkmaohpngenfoccdgceedjkfhkdcohmg" ssk="1#3"><p><img src="https://lh3.googleusercontent.com/HTGPCwh8Nu_23oj2eR7Xq4Y6gj-OSX3MlBHB5Y3PFELq-ja7kt_tQAp4vD-E6vele6N-uZlqwJWVBLnJ4KhMMmPFp5c=s275-w275-h175" srcset="https://lh3.googleusercontent.com/HTGPCwh8Nu_23oj2eR7Xq4Y6gj-OSX3MlBHB5Y3PFELq-ja7kt_tQAp4vD-E6vele6N-uZlqwJWVBLnJ4KhMMmPFp5c=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Figma" id="i49" role="heading" aria-level="3">Figma</p><p><span><span role="img" aria-label="Average rating 4.7 out of 5 stars. 27 ratings." id="i50"><span>4.7</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(27)</span></span><span></span></span></p><p id="i51">Figma</p></div></div><div inert=""><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="glefoejbpalgpdgfbbmemfgegepiamnd" ssk="1#0"><p><img src="https://lh3.googleusercontent.com/ndSnYqY3blYbnsidzizOmaNMpFdG3Z1CfHAeANI45b72_ulDng5qZf4_o8luS-ZEoE5-bQfLWC2HCJ3ZGosiib6FJQ=s275-w275-h175" srcset="https://lh3.googleusercontent.com/ndSnYqY3blYbnsidzizOmaNMpFdG3Z1CfHAeANI45b72_ulDng5qZf4_o8luS-ZEoE5-bQfLWC2HCJ3ZGosiib6FJQ=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="HTML to React" id="i53" role="heading" aria-level="3">HTML to React</p><p><span><span role="img" aria-label="Average rating 5 out of 5 stars. 4 ratings." id="i54"><span>5.0</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(4)</span></span><span></span></span></p><p id="i55">The fastest and smartest way to analyze HTML elements and generate full React component representations.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="ldnheaepmnmbjjjahokphckbpgciiaed" ssk="1#1"><p><img src="https://lh3.googleusercontent.com/hwlFc8u63Ru5cq0LMtlWhFmWTSh7zsPRsryvyH8mc2896akyPPfw_DB1mVYP6Rs3cDAuZRbYDwuSdYi-8HjOru6ClfI=s275-w275-h175" srcset="https://lh3.googleusercontent.com/hwlFc8u63Ru5cq0LMtlWhFmWTSh7zsPRsryvyH8mc2896akyPPfw_DB1mVYP6Rs3cDAuZRbYDwuSdYi-8HjOru6ClfI=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="html.to.design" id="i57" role="heading" aria-level="3">html.to.design</p><p><span><span role="img" aria-label="Average rating 4.7 out of 5 stars. 68 ratings." id="i58"><span>4.7</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(68)</span></span><span></span></span></p><p id="i59">Convert any website into fully editable Figma designs. Requires the associated Figma plugin!</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="okpadhgilncoodafepkdmlneojcehclf" ssk="1#2"><p><img src="https://lh3.googleusercontent.com/-xM7YeHNFyKkkOU1-8xsCDfgFLa9X4ihnxsmmUeLbu2zcTm6AFwJNpzPZIhS0REoCDMaOfONsJQUQ5uq84WbrIoVLw=s275-w275-h175" srcset="https://lh3.googleusercontent.com/-xM7YeHNFyKkkOU1-8xsCDfgFLa9X4ihnxsmmUeLbu2zcTm6AFwJNpzPZIhS0REoCDMaOfONsJQUQ5uq84WbrIoVLw=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="DivMagic - Copy Style from any website" id="i61" role="heading" aria-level="3">DivMagic - Copy Style from any website</p><p><span><span role="img" aria-label="Average rating 3.8 out of 5 stars. 51 ratings." id="i62"><span>3.8</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(51)</span></span><span></span></span></p><p id="i63">Copy elements from any webpage as reusable web components. Get HTML, CSS, React, JSX or Tailwind CSS code.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="mafpepbepbabkenbfpcdjmmjmeeemoal" ssk="1#3"><p><img src="https://lh3.googleusercontent.com/xRisz7ZyRPygsOkTT0QVhXRGt2fqYFIeyfkrWUIz5tFwMQLc1zsQPKAxqXq4P0PdcdcIRkkTqMyifeMawUoqMYXjXw=s275-w275-h175" srcset="https://lh3.googleusercontent.com/xRisz7ZyRPygsOkTT0QVhXRGt2fqYFIeyfkrWUIz5tFwMQLc1zsQPKAxqXq4P0PdcdcIRkkTqMyifeMawUoqMYXjXw=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Web to Figma" id="i65" role="heading" aria-level="3">Web to Figma</p><p><span><span role="img" aria-label="Average rating 3.1 out of 5 stars. 10 ratings." id="i66"><span>3.1</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(10)</span></span><span></span></span></p><p id="i67">Web to Figma extension to capture pages and components and import them into Figma as editable designs. No more screenshots.</p></div></div><div inert=""><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="iiikidmnimlpahbeknmkeonmemajpccj" ssk="1#0"><p><img src="https://lh3.googleusercontent.com/_2HDrH9NpgVFgNFlKd4KVGeKxKYjk4blzP_RxNs0VxXWCpldS-Nm1uAKBF9FKdjl5krtrKWfr5qOnEPDy8i-F3Vk=s60" srcset="https://lh3.googleusercontent.com/_2HDrH9NpgVFgNFlKd4KVGeKxKYjk4blzP_RxNs0VxXWCpldS-Nm1uAKBF9FKdjl5krtrKWfr5qOnEPDy8i-F3Vk=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="Button Stealer" id="i69" role="heading" aria-level="3">Button Stealer</p><p><span><span role="img" aria-label="Average rating 4.9 out of 5 stars. 52 ratings." id="i70"><span>4.9</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(52)</span></span><span></span></span></p><p id="i71">Steals buttons from the websites you visit. Do your usual online stuff and watch the collection of stolen buttons grow.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="nggpkpfmdkbaakpndblpandmldendooa" ssk="1#1"><p><img src="https://lh3.googleusercontent.com/E8Ku4exi6R-SSaPFTK5uz3PC3jP9GAeEgKdb2dsR7KpLKdPDf_Lw6Q52UVYwyBxLsRdX6YiHoAQNgiY0zrQL1TJhkg=s275-w275-h175" srcset="https://lh3.googleusercontent.com/E8Ku4exi6R-SSaPFTK5uz3PC3jP9GAeEgKdb2dsR7KpLKdPDf_Lw6Q52UVYwyBxLsRdX6YiHoAQNgiY0zrQL1TJhkg=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Jinno: code any React component with AI" id="i73" role="heading" aria-level="3">Jinno: code any React component with AI</p><p><span><span role="img" aria-label="Average rating 4.8 out of 5 stars. 38 ratings." id="i74"><span>4.8</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(38)</span></span><span></span></span></p><p id="i75">Develop html or react components with AI and chat GPT4. We can modify the React, HTML and CSS code for you.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="nmgcefdhjpjefhgcpocffdlibknajbmj" ssk="1#2"><p><img src="https://lh3.googleusercontent.com/AgDgVjbRi_tFDFu03B2ThYAiaHsJDRxaDeOYKKlepT8Dxhteq08A4C2T_QDFGJgvRHvIILuw2yvP4NPVkIy6FLI3lA=s275-w275-h175" srcset="https://lh3.googleusercontent.com/AgDgVjbRi_tFDFu03B2ThYAiaHsJDRxaDeOYKKlepT8Dxhteq08A4C2T_QDFGJgvRHvIILuw2yvP4NPVkIy6FLI3lA=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="mymind ‚Äî An extension for your mind" id="i77" role="heading" aria-level="3">mymind ‚Äî An extension for your mind</p><p><span><span role="img" aria-label="Average rating 4.6 out of 5 stars. 54 ratings." id="i78"><span>4.6</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(54)</span></span><span></span></span></p><p id="i79">Add images, bookmarks, notes, quotes or text highlights to your new mind.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="efjcmgblfpkhbjpkpopkgeomfkokpaim" ssk="1#3"><p><img src="https://lh3.googleusercontent.com/9MKf4-JqyeaailX23MzwdeZKlIjAS5J7wHeAd8fE_RRty0YLqc5j5M77MCDLRk1pH3hynaCVV2fVzZ5hEx7EdmK8Tw=s275-w275-h175" srcset="https://lh3.googleusercontent.com/9MKf4-JqyeaailX23MzwdeZKlIjAS5J7wHeAd8fE_RRty0YLqc5j5M77MCDLRk1pH3hynaCVV2fVzZ5hEx7EdmK8Tw=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="HTML to Figma - by Builder.io" id="i81" role="heading" aria-level="3">HTML to Figma - by Builder.io</p><p><span><span role="img" aria-label="Average rating 4.3 out of 5 stars. 45 ratings." id="i82"><span>4.3</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(45)</span></span><span></span></span></p><p id="i83">Import a web page to Figma layers</p></div></div><div inert=""><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="bjobgelaoehgbnklgcaaehdpckmhkplk" ssk="1#0"><p><img src="https://lh3.googleusercontent.com/D72iPn2Uw4ddke4D9lSSlOmlUpxxkWy9HwI0tEIUmXl_rwQ4YU8HZ52rEWDciEKALCPo7kVF76NCvxiKfqbStpDtqQ=s60" srcset="https://lh3.googleusercontent.com/D72iPn2Uw4ddke4D9lSSlOmlUpxxkWy9HwI0tEIUmXl_rwQ4YU8HZ52rEWDciEKALCPo7kVF76NCvxiKfqbStpDtqQ=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="SuperX (Twitter Analytics)" id="i85" role="heading" aria-level="3">SuperX (Twitter Analytics)</p><p><span><span role="img" aria-label="Average rating 4.6 out of 5 stars. 23 ratings." id="i86"><span>4.6</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(23)</span></span><span></span></span></p><p id="i87">Super insights for your X (Twitter) activities and engagements</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="fojckembkmaoehhmkiomebhkcengcljl" ssk="1#1"><p><img src="https://lh3.googleusercontent.com/8QTPHkGRRrPCJ2NczkedhkTtL9TurDBE1BckjlRDHfizAZk1wz8EPwPu6p0bGJyAdtp9Cvyeo4fR6OEOtCYx6WEu=s275-w275-h175" srcset="https://lh3.googleusercontent.com/8QTPHkGRRrPCJ2NczkedhkTtL9TurDBE1BckjlRDHfizAZk1wz8EPwPu6p0bGJyAdtp9Cvyeo4fR6OEOtCYx6WEu=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Gimli Tailwind" id="i89" role="heading" aria-level="3">Gimli Tailwind</p><p><span><span role="img" aria-label="Average rating 4.5 out of 5 stars. 30 ratings." id="i90"><span>4.5</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(30)</span></span><span></span></span></p><p id="i91">A DevTools extension enabling smart tools for Tailwind CSS.</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="djpkcdeoondbcapcllgahjhaghplfcno" ssk="1#2"><p><img src="https://lh3.googleusercontent.com/3qU7xNp05WjndSgQmFyCOTNAQkDmdDo4rhUa-rWGZVLGno0LP0lwPKnAsWYIAG8Q5yroDvRhgATmvKZqTCU6HymexQ=s60" srcset="https://lh3.googleusercontent.com/3qU7xNp05WjndSgQmFyCOTNAQkDmdDo4rhUa-rWGZVLGno0LP0lwPKnAsWYIAG8Q5yroDvRhgATmvKZqTCU6HymexQ=s120 2x" alt="" jscontroller="OhgRI" jsaction="error:LILo6;"></p><p title="Markflow: Copy Elements to Figma, React, etc." id="i93" role="heading" aria-level="3">Markflow: Copy Elements to Figma, React, etc.</p><p><span><span role="img" aria-label="Average rating 3.5 out of 5 stars. 4 ratings." id="i94"><span>3.5</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(4)</span></span><span></span></span></p><p id="i95">Introducing Markflow, the ultimate Chrome extension designed to simplify the workflow for web designers and developers. Easily copy‚Ä¶</p></div><div jscontroller="LcrBLd" jsaction="focus:p7KPsb(qcuvSe); mouseenter:p7KPsb;" data-item-id="fkmaohpngenfoccdgceedjkfhkdcohmg" ssk="1#3"><p><img src="https://lh3.googleusercontent.com/HTGPCwh8Nu_23oj2eR7Xq4Y6gj-OSX3MlBHB5Y3PFELq-ja7kt_tQAp4vD-E6vele6N-uZlqwJWVBLnJ4KhMMmPFp5c=s275-w275-h175" srcset="https://lh3.googleusercontent.com/HTGPCwh8Nu_23oj2eR7Xq4Y6gj-OSX3MlBHB5Y3PFELq-ja7kt_tQAp4vD-E6vele6N-uZlqwJWVBLnJ4KhMMmPFp5c=s550-w550-h350 2x" alt="" jscontroller="OhgRI" jsaction="error:IehHO;"></p><p title="Figma" id="i97" role="heading" aria-level="3">Figma</p><p><span><span role="img" aria-label="Average rating 4.7 out of 5 stars. 27 ratings." id="i98"><span>4.7</span><svg width="18" height="18" viewBox="0 0 24 24" focusable="false"><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27z"></path></svg><span>(27)</span></span><span></span></span></p><p id="i99">Figma</p></div></div></div></section></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's called a dance floor (133 pts)]]></title>
            <link>https://www.seekhifi.com/its-called-a-dance-floor/</link>
            <guid>42042810</guid>
            <pubDate>Mon, 04 Nov 2024 16:04:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seekhifi.com/its-called-a-dance-floor/">https://www.seekhifi.com/its-called-a-dance-floor/</a>, See on <a href="https://news.ycombinator.com/item?id=42042810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <span>
                <!-- By <a href="/author/wade/">Wade Meredith</a> -->
                    Published in   
                    <a href="https://www.seekhifi.com/tag/tracks/">Tracks</a>
                ‚Äî
                <time datetime="2024-10-01">Oct 1, 2024</time>
            </span>

            

                <p>Pound-for-pound, Vogue by Madonna is one of the most remarkable examples of artificial stereophonic sound ever produced.</p>

                <figure>
        <img srcset="https://www.seekhifi.com/content/images/size/w400/2024/09/Madonna---Celebration---Album-Art.png 400w,
                    https://www.seekhifi.com/content/images/size/w720/2024/09/Madonna---Celebration---Album-Art.png 720w,
                    https://www.seekhifi.com/content/images/size/w960/2024/09/Madonna---Celebration---Album-Art.png 960w,
                    https://www.seekhifi.com/content/images/size/w1200/2024/09/Madonna---Celebration---Album-Art.png 1200w,
                    https://www.seekhifi.com/content/images/size/w2000/2024/09/Madonna---Celebration---Album-Art.png 2000w" sizes="(max-width: 1200px) 100vw, 1200px" src="https://www.seekhifi.com/content/images/size/w1200/2024/09/Madonna---Celebration---Album-Art.png" alt="Black and white album art: Madonna - Celebration. Madonna's face on a newsprint backgtround. ">
    </figure>
        </div><div>
            <p><em>Vogue</em> is an RIAA-certified triple-platinum hit, selling over six million copies since its 1991 release on the "I'm Breathless" album by pop mega-star <a href="https://en.wikipedia.org/wiki/Madonna?ref=seekhifi.com">Madonna</a>. <a href="https://en.wikipedia.org/wiki/Vogue_(Madonna_song)?ref=seekhifi.com" rel="noreferrer"><em>Vogue</em></a> topped charts in Australia, Canada, Japan, the United Kingdom, and the United States. It has been sampled by <a href="https://en.wikipedia.org/wiki/Kylie_Minogue?ref=seekhifi.com">Kylie Minogue</a>, <a href="https://en.wikipedia.org/wiki/Beyonc%C3%A9?ref=seekhifi.com">Beyonc√©</a>, and <a href="https://en.wikipedia.org/wiki/Ariana_Grande?ref=seekhifi.com">Ariana Grande</a>. David Fincher (yes, <a href="https://en.wikipedia.org/wiki/David_Fincher?ref=seekhifi.com#Filmography">that David Fincher</a>) directed the provocative and iconic <a href="https://www.youtube.com/watch?v=GuJQSAiODqI&amp;ref=seekhifi.com">music video</a>. <em>Vogue </em>also possibly resurrected disco from its commercial death in the late 70s and helped bring house music into the mainstream.</p><p>I don't care about any of that.</p><p>HERE'S THE DEAL: pound-for-pound, <em>Vogue</em> is one of the most remarkable examples of artificial <a href="https://en.wikipedia.org/wiki/Stereophonic_sound?ref=seekhifi.com">stereophonic sound</a> ever produced. Let's strike a pose, er, have a listen.</p><hr><hr><p>Stereophonic sound is typically what people mean when saying "stereo." It uses separate sound sources to give the illusion of sound in a 3-D dimensional space as perceived by our ears. "Natural" stereophonic sound is recorded and reproduced with arrays of microphones and speakers. That's not what <em>Vogue</em> is.</p><p><em>Vogue</em>'s incredible soundscape is entirely artificial. It was not recorded by an array of microphones placed around a band. There was no band. There was no dance floor. There was QSound. From <a href="https://en.wikipedia.org/wiki/QSound?ref=seekhifi.com">Wikipedia</a>:</p><blockquote>QSound is essentially a filtering algorithm. It manipulates timing, amplitude, and frequency response to produce a <a href="https://en.wikipedia.org/wiki/Binaural_recording?ref=seekhifi.com">binaural image</a>. Systems like QSound rely on the fact that a sound arriving from one side of the listener will reach one ear before the other and that when it reaches the furthest ear, it is lower in amplitude and spectrally altered due to obstruction by the head.</blockquote><p>QSound was only used on a handful of albums in the early 90s. Its remarkable ability to place sounds in a 3-D space was quickly iterated upon and has since found widespread usage in video game development. If you've ever been blown away by surround sound from a gaming headset, it was likely some form of QSound or something related.</p><p>The music recording industry moved on to other production methods. Still, <em>Vogue</em> remains a unique time capsule of state-of-the-art audio technology deployed by masters of the craft.</p><p>The track starts with a startling, sweeping vocal line, "What are you looking at?" that pans around our heads like Michael Bay's camera, setting the stage (literally) for what's about to come. Sustained synth chords gently surround, laying the foundation for sharp snaps in front and to the right. A Roland TR-909 drum machine starts far in the distance on the left before a meaty bass line gets its groove on. This is house music exploding to the top of all the charts.</p><p>The song's main vocal comes in at 1:22, giving plenty of time to fill the dance floor. But come in, it does. Madonna's vocals on <em>Vogue</em> sound so freaking cool. It's like she's in my head, but if my head was the size of a symphony hall. The voice of god. Backing vocals feint and doge, appearing with impressive effect from all angles and orientations. One of my favorite vocal moments is at 3:15 with the delightfully corny (or perhaps deeply philosophical) line, "Beauty's where you find it, not just where you bump and grind it." The wall of voices here is astounding from the sweet spot between two loudspeakers or on a good set of cans.</p><p>The keyboard and drum machine programming and mixing throughout the song is a blast. This is the <a href="https://en.wikipedia.org/wiki/Jurassic_Park_(film)?ref=seekhifi.com"><em>Jurassic Park</em></a> of audio effects. Many people worked on the track, but much credit goes to 80's remix master <a href="https://en.wikipedia.org/wiki/Shep_Pettibone?ref=seekhifi.com">Shep Pettibone</a>, who produced with Madonna. Oodles of little details and layered moments reward a close listen. It's an impressive display of realized artistic vision and craft by true medium masters.</p><p><em>Vogue</em> is an exceptional pop song with sonic and cultural depth for those who care to engage critically but also somehow remains a mile wide. Decades after its release, <em>Vogue </em>still fills dance floors at concerts, weddings, and house parties. Madonna reinvented herself many times throughout her career, but <em>Vogue</em> has rarely been left off the set list. It stands as a towering pop-art achievement in mainstream music-making.</p><hr><h3 id="data">Data</h3><p><strong>Song</strong>: Vogue<br><strong>Album</strong>: <a href="https://en.wikipedia.org/wiki/I%27m_Breathless?ref=seekhifi.com">I'm Breathless</a>, <a href="https://en.wikipedia.org/wiki/The_Immaculate_Collection?ref=seekhifi.com" rel="noreferrer">The Immaculate Collection</a><br><strong>Artist</strong>: Madonna<br><strong>Genre</strong>: Pop, Dance, House<br><strong>Year</strong>: 1990<br><strong>Length</strong>: 5:17<br><strong>Composer</strong>: Madonna, Shep Pettibone <br><strong>Producer</strong>: Madonna, Shep Pettibone</p>
            <!-- need to zip and push -->
                        <hr>
                        <p>Enjoying this content? Subscribe and get full access to expertly curated playlists, mix tapes, and more.</p>
                        
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What should a logo for NeXT look like? (1986) (225 pts)]]></title>
            <link>https://www.paulrand.design/work/NeXT-Computers.html</link>
            <guid>42042382</guid>
            <pubDate>Mon, 04 Nov 2024 15:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.paulrand.design/work/NeXT-Computers.html">https://www.paulrand.design/work/NeXT-Computers.html</a>, See on <a href="https://news.ycombinator.com/item?id=42042382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2001.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2001-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2001-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <p>NEXT</p> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2002.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2002-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2002-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <p>The Sign of the Next Generation of Computers <em>for Education.</em></p> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2003.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2003-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2003-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>What should a logo for Next look like?</p> <p>Choosing a typeface as the basis for the design of a logo is a convenient starting point. Here are two examples: Caslon and Bifur. Caslon is an alphabet designed as far back as 1725 by William Caslon. It appears to be a good choice because it is both elegant and bookish, qualities well suited for educational purposes.</p> <p>Bifur, a novelty face by A. M. Cassandre, was designed as recently as 1929. An unconventional but ingenious design, it has the advantage, to some, of visually implying advanced technology. <em>(Attributing certain magical qualities to particular typefaces is, however, largely a subjective matter.)</em></p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2004.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2004-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2004-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>One reason for looking at a number of possible typefaces is to satisfy one‚Äôs curiosity. Another, and perhaps more meaningful one, is to study the relationship of different letter combinations, to look for visual analogies, and to try to elicit ideas that the design of a letter or group of letters might inspire.</p> <p>Here are some further choices, but no matter how one may look at these different examples: sans serifs, hairline and slab serifs, condensed, expanded, bold, light, outline‚Ä¶ they still say <em>next</em>‚Ä¶ like <em>next time, what‚Äôs next?, next in line,</em> or even <em>next of kin</em>. The word is in such common usage that it is simply taken for granted.</p> <p>Personal preferences, prejudices, and stereotypes often dictate what a logo looks like, but it is <em>needs</em>, not wants, <em>ideas</em>, not type styles which determine what its form should be. To defamiliarize it, to make it look different, to let it evoke more than the mere adjective or adverb it happens to be is, it seems, the nub of the problem.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2005.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2005-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2005-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>Set in all capitals, the word NEXT is sometimes confused with EXIT possibly because the EXT grouping is so dominant. A combination of capitals and lower case letters alleviates this problem.</p> <p>Here are some possibilities which explore the use of lower case letters.</p> <p>The e is differentiated so as to provide a focal point and visual contrast among the capital letters which, otherwise, consist only of straight lines.</p> <p>Happily, the e also could stand for:<br> education<br> excellence<br> expertise<br> exceptional<br> excitement<br> e=mc2<br> etc.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2006.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2006-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2006-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>Note the difference that the lower case e makes when compared with the capital E. By means of contrast both interest and readability are achieved. This is particularly noticeable in the illustration at the bottom.</p> <p>These simple, geometric letters make it easier to exploit and manipulate possible visual ideas than do more complex serifed letters.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2007.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2007-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2007-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>Ideally, a logo should explain or suggest the business it symbolizes, but this is rarely possible or even necessarv.There is nothing about the IBM symbol, for example, that suggests computers, except what the viewer reads into it. Stripes are now associated with computers because the initials of a great computer company happen to be striped.This is equally true of the ABC symbol which does not suggest TV. The mnemonic factors in both logps are graphic devices: stripes and circles.</p> <p>In this example the e is the mnemonic factor.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2008.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2008-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2008-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>A logo takes on meaning, only if over a period of time it is linked to some product or service of a particular organization. What is needed is finding a meaningful device, some idea that reinforces the memorability of the company name. A black cube can be such a device because it has visual impact, and is easy to remember. Unlike the word Next, it is depictable, possesses the <em>promise of meaning</em>, and the <em>pleasure of recognition.</em></p> <p>This idea in no way restricts its application to any one product or concept.<br> The three dimensional effect functions as an underscore to attract the viewer s attention.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2009.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2009-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2009-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>It is desirable to keep the letter style simple, unmannered, and untrendy so as not to distract from the cube concept. Furthermore, the use of a single identification device and a simple sans serif letter, designed to harmonize with almost any accompanying typeface, is essential for practical application. Whenever possible, double identification (name plus symbol) is best avoided.The brevity of the word NeXT and its containment within the framework of the cube obviates the need for such awkward devices.</p> <p>Splitting the logo into two lines accomplishes several things: it startles the viewer and gives the word a new look, thus making it easier to separate from common usage. And even more importantly, it increases the letter size two-fold, within the framework of the cube. For small space use, a one line logo would have been too small to fit within this same framework.</p> <p>Readability is hardly affected because the word is too simple to be misread.<br> Moreover, people have become accustomed to this format with such familiar four- letter word combinations as<br> LO<br> VE</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2010.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2010-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2010-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>In its design, color arrangement, and orientation the logo is a study in contrasts. Kipped at a jaunty angle, it brims with the informality, friendliness, and spontaneity of a Christmas seal and the authority of a rubber stamp.Together with its lively, black silhouette it becomes a focal point difficult for the eyes to avoid.</p> <p>The unconventional, yet dignified, array of colors: vermilion against cerise and green, and yellow against black (the most intense color contrast possible) is designed to appeal to a youthful audience and to add a sparkling, jew‚Äôel-like touch to paper, package, or machine. It is the sparing use of brilliant colors on a predominantly black ground that produces this effect, like stars in the sky. In itself, a decorative and self contained device, the logo does not depend on extraneous embellishment or fancy backgrounds for its many varied applications.</p> <p>Poised at an angle of twenty-eight degrees, the black cube‚Äîeven w ithout color‚Äî is equally effective for black and white use.</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2011.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2011-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2011-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <p>Here are some other choices.<br> Many different color combinations are possible.</p> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2012.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2012-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2012-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <div><p>The application of this device to such items as paper weights, stickers, and other promotional articles is endless. It lends itself as well to large scale interpretation: signs, exhibits in the shape of cubes, in which the actual exhibit is housed, as well as exhibit stands. For printed matter, its infinite adaptability and attention-compelling power is self-evident.</p> <p>Paul Rand<br> Weston, Connecticut Spring 1986</p> </div> <div> <p><a href="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2013.jpg" data-fancybox="images" role="link"><img loading="lazy" data-sizes="auto" data-src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2013-450.jpg" alt="NeXT Computers" width="100%" height="auto" src="https://assets.paulrand.design/Works/NeXT/Logo%20Book/Web/NeXT%2013-450.jpg"></a><img loading="lazy" data-sizes="auto" data-src="/img/icon-zoom-boxed.svg" width="40" height="40" alt="Zoom"></p> </div> <p><em>(Back cover)</em></p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alonzo Church: Architect of computer intelligence (278 pts)]]></title>
            <link>https://onepercentrule.substack.com/p/alonzo-church-the-forgotten-architect</link>
            <guid>42042025</guid>
            <pubDate>Mon, 04 Nov 2024 14:51:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onepercentrule.substack.com/p/alonzo-church-the-forgotten-architect">https://onepercentrule.substack.com/p/alonzo-church-the-forgotten-architect</a>, See on <a href="https://news.ycombinator.com/item?id=42042025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:590662,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84a5b042-3403-428b-b191-b49127482b13_1024x1024.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em>This is part of a series, of people, who have contributed significantly to their field. Whilst they are known within the field, rarely are they known outside the field or widely, yet should be.</em></p><p><span>If you've heard of the </span><strong>Turing Test</strong><span>, you've undoubtedly heard of Alan Turing. But in the same breath, you might also encounter the lesser-known name, </span><strong>Alonzo Church</strong><span>. When we review the ‚Äòpopular‚Äô history of computing and Artificial Intelligence, it‚Äôs often Turing‚Äôs name that receives the spotlight, but the collaboration upon which Turing performed much of his groundbreaking work was aided and shaped by Church. Alonzo Church's work profoundly influenced Alan Turing's thinking and played a decisive role in the development of the Turing Test. Furthermore, without Church's contributions to the understanding of computation, our current conceptions of artificial intelligence and the tests, which have considerably evolved, to evaluate it may likely be very different.</span></p><p><span>Born on June 14, 1903, in Washington, D.C., Alonzo Church was a quiet, soft-spoken logician whose impact on mathematics and computing was monumental, even though he never sought acclaim. Little is known of his early life, however, it was marked by challenges, an air gun incident in his childhood left him </span><a href="https://builds.openlogicproject.org/content/history/biographies/alonzo-church.pdf" rel="">blind in one eye</a><a href="#_ftn1" rel="">[1]</a><span>. Despite this, he thrived academically. Church finished preparatory school in Connecticut in 1920 and began his university education at Princeton that same year, where he completed his doctoral studies in 1927. After spending time at Harvard and abroad as a National Research Fellow in G√∂ttingen, and Amsterdam, Church returned to Princeton, where he would build much of his academic legacy. </span><strong>Church was known for his exceedingly polite demeanor and meticulous nature</strong><span>. His blackboard writing was immaculate, and he was known to cover important papers in </span><a href="https://builds.openlogicproject.org/content/history/biographies/alonzo-church.pdf" rel="">Duco</a><span> cement to preserve them, a reflection of his careful and deliberate approach to both scholarship and life.</span></p><p><strong>Did you know</strong><span> that despite his immense contributions to computer science, much of Alonzo Church's early life remains relatively unknown? Unlike some of his more famous contemporaries, his personal history hasn't been extensively documented, adding to the air of mystery surrounding this intellectual giant.</span></p><p><span>Despite his reserved demeanor and personal challenges, </span><strong>Church's intellectual contributions were nothing short of revolutionary.</strong></p><p>Church‚Äôs most profound contribution was the ‚ÄúŒª-calculus,‚Äù a formal system that served as the bedrock of computer science before computer science even had a name. In 1936, Alonzo Church formulated what is now known as the Church-Turing thesis, a concept fundamental to theoretical computer science that states any function that can be effectively computed can be computed by a Turing machine or its equivalent. This thesis was groundbreaking, as it provided the framework within which to understand what machines could theoretically do, but also highlighted the boundaries of algorithmic processes. While foundational, the Church-Turing thesis also has limitations and has been subject to debate, particularly concerning the interpretation of 'effective computability' and its implications for physical computation and the nature of human intelligence.</p><p><span>Where Turing proposed his eponymous machine, a model meant to exemplify how mechanical procedures could be translated into logical form, Church provided the pure abstraction that made such a machine theoretically sound. The Œª-calculus was almost mystical in its generality and elegance</span><a href="#_ftn2" rel="">[2]</a><span>. The influence of Œª-calculus can be seen in the very principles of how programs are written today, emphasizing composition, higher-order functions, and immutability. This formalism allowed abstract mathematical problems to be codified and ultimately solved mechanically, and became essential to the architecture of modern compilers and interpreters. His work posed and solved problems about the limits of mathematical procedures, effectively asking: </span><strong>‚ÄúTo what extent can machines replicate human thought?‚Äù</strong></p><p><span>In addition to Œª-calculus, Church made important contributions to other areas of logic and philosophy, such as his work on the </span><strong><a href="https://en.wikipedia.org/wiki/Entscheidungsproblem" rel="">Entscheidungsproblem</a></strong><span>, a decision problem posed by David Hilbert in 1928 that asked whether there exists a definitive algorithm to determine the truth of any mathematical statement. Church answered this question negatively, showing that such an algorithm does not exist, which became known as Church's Theorem. This finding profoundly influenced decision theory and underscored the limits of what could be achieved with computation alone.</span></p><p><span>Church was also a mentor to some of the greatest logicians and computer scientists of his time. His academic offspring included luminaries such as Stephen Kleene, J. Barkley Rosser, and perhaps most notably, Alan Turing, who completed his Ph.D. under Church's supervision at Princeton. </span><a href="https://www.tandfonline.com/doi/epdf/10.1080/01445349708837290?needAccess=true" rel="">David Kaplan</a><span> says that he used always to recommend that his new graduate students sit in on a class with Church, telling them: </span><strong>‚ÄúTake a class from Professor Church. It will change you. And even if you are not interested in pursuing the subjects he teaches, you will tell your grandchildren.‚Äù </strong></p><p>During the 1930s, Princeton was an intellectual epicenter for the development of modern logic, with influential figures such as John von Neumann and Kurt G√∂del alongside Church. The symmetry of Church's mentorship and Turing's later achievements creates a hidden but potent partnership, an invisible bridge that helped carry humanity from mechanical calculation to computational intelligence.</p><p><strong>Did you know that Alonzo Church was already a professor at Princeton University by the age of 26?</strong><span> He joined the faculty in 1929, shortly after completing his Ph.D., showcasing his rapid ascent in academia.</span></p><p><span>Despite his massive intellectual contributions, Alonzo Church never enjoyed the fame of Turing or von Neumann, G√∂del and others. His legacy was one of meticulous abstraction, a kind that doesn‚Äôt make it into Hollywood scripts or capture public imagination easily. It lacked the heroism of wartime codebreaking or the evocative tragedy of an early (</span><em>forced</em><span>) death. Yet, </span><strong>Church's influence is indelible</strong><span>. The very programs that run on the billions of smartphones today can trace their logic back to the abstract functions of Œª-calculus. </span><strong>The invisible DNA of computation, from the simple app to artificial intelligence, owes a significant part of its lineage to Church‚Äôs work.</strong></p><p><span>Why, then, should we know him? Perhaps because our reverence for brilliance is often biased toward the visible, the concrete, and the heroic. Alonzo Church‚Äôs genius was of a different kind, it was the genius of the unseen, of the rigorous structure without which the edifice could not stand. His work forms a foundational part of the theoretical basis for many of the digital interactions we take for granted, influencing the development of computer science and the algorithmic processes that shape our daily computerised interactions. </span><strong>To know Church is to understand that genius doesn‚Äôt always reside in spectacle but sometimes in quiet, unassuming elegance</strong><span>, in the formulas scribbled on a blackboard that, eventually, reshape the world.</span></p><p>With the tremendous breakthroughs we are seeing in artificial intelligence, we would do well to know more about, and celebrate, the foundational figures such as Alonzo Church who made it all possible.</p><p>Stay curious, </p><p>dr Colin W.P. Lewis</p><p><em><strong>This is part of a series, of people, who have contributed significantly to their field. Whilst they are known within the field, rarely are they known outside the field or widely, yet should be.</strong></em></p><p><strong>Others in the series are</strong><span> </span><a href="https://onepercentrule.substack.com/p/rough-edges-and-hidden-order-the" rel="">Beno√Æt Mandelbrot</a><span>, </span><a href="https://onepercentrule.substack.com/p/life-is-one-long-intelligence-test" rel="">Eric Kandel</a><span>, </span><a href="https://onepercentrule.substack.com/p/the-memory-pioneer-how-hermann-ebbinghaus" rel="">Hermann Ebbinghaus</a><span>. Many more to follow.</span></p><p><strong><a href="https://www.tandfonline.com/doi/epdf/10.1080/01445349708837290?needAccess=true" rel="">Recommended reading</a></strong><span> </span></p><p><em><strong>Alonzo Church his life, his work and some of his miracles</strong></em><span>.</span></p><p><a href="https://people.math.ethz.ch/~halorenz/4students/Literatur/TuringFullText.pdf" rel="">On Computable Numbers, with an Application to the Entscheidungsproblem</a><span> by Alan Turing</span></p><p><a href="https://www.jstor.org/stable/421133" rel="">Note</a><span> ‚Äì ‚ÄúAlonzo Church's first published paper, Uniqueness of the Lorentz transformation, appeared in the American mathematical monthly in 1924. His last paper, A theory of the meaning of names, was published in The Heritage of Kazimierz Ajdukiewicz, Rodopi, 1995 (</span><em>the year he passed away</em><span>). This amazing span of seventy-two years embraces a remarkable collection of publications on a wide range of topics in logic and in adjacent parts of philosophy, mathematics, and computer science.‚Äù</span></p><p><a href="#_ftnref1" rel="">[1]</a><span> (</span><em>other sources say partially blinded</em><span>)</span></p><p><a href="#_ftnref2" rel="">[2]</a><span> To modern programmers, it might look like a set of nested functions, similar to what we see in functional programming languages such as Lisp, Haskell, or even in certain paradigms within Python or JavaScript). Its abstraction provided the groundwork for functional programming, where functions are treated as first-class citizens.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Every Developer Should Know About GPU Computing (2023) (119 pts)]]></title>
            <link>https://blog.codingconfessions.com/p/gpu-computing</link>
            <guid>42042016</guid>
            <pubDate>Mon, 04 Nov 2024 14:49:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codingconfessions.com/p/gpu-computing">https://blog.codingconfessions.com/p/gpu-computing</a>, See on <a href="https://news.ycombinator.com/item?id=42042016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Most programmers have an intimate understanding of CPUs and sequential programming because they grow up writing code for the CPU, but many are less familiar with the inner workings of GPUs and what makes them so special. Over the past decade, GPUs have become incredibly important because of their pervasive use in deep learning. Today, it is essential for every software engineer to possess a basic understanding of how they work. My goal with this article is to give you that background.&nbsp;</p><blockquote><p><em><span>Much of this article is based on the book ‚Äú</span><a href="https://shop.elsevier.com/books/programming-massively-parallel-processors/hwu/978-0-323-91231-0" rel="">Programming Massively Parallel Processors</a><span>‚Äù, 4th edition by Hwu et al. As the book covers Nvidia GPUs, I will also be talking about Nvidia GPUs and using Nvidia specific terminology. However, the fundamental concepts and approach to GPU programming apply to other vendors as well.</span></em></p></blockquote><p><span>‚ö°Ô∏è</span><strong>Announcement ‚Äî Upcoming Live Session</strong><span>‚ö°Ô∏è</span><strong>: </strong><em>I‚Äôm doing a live session on building a bytecode compiler and VM for a small subset of Python in Python. Join if you are interested.</em></p><p>We will start by doing a comparison between CPU and GPU which will give us a better vantage point of the GPU landscape. However, this is a topic of its own and we cannot possibly squeeze everything in one section. So, we will stick to a few key points.</p><p><span>The major difference between CPUs and GPUs is in their design goals. CPUs were designed to execute sequential instructions</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-137892185" href="https://blog.codingconfessions.com/p/gpu-computing#footnote-1-137892185" target="_self" rel="">1</a></span><span>. To improve their sequential execution performance, many features have been introduced in the CPU design over the years. The emphasis has been on reducing the instruction execution latency so that CPUs can execute a sequence of instructions as fast as possible. This includes features like </span><a href="https://en.wikipedia.org/wiki/Instruction_pipelining" rel="">instruction pipelining</a><span>, </span><a href="https://en.wikipedia.org/wiki/Out-of-order_execution" rel="">out of order execution</a><span>, </span><a href="https://en.wikipedia.org/wiki/Speculative_execution" rel="">speculative execution</a><span> and multilevel caches (just to list a few).</span></p><p>GPUs on the other hand have been designed for massive levels of parallelism and high throughput, at the cost of medium to high instruction latency. This design direction has been influenced by their use in video games, graphics, numerical computing, and now deep learning. All of these applications need to perform a ton of linear algebra and numerical computations at a very fast rate, because of which a lot of attention has gone into improving the throughput of these devices.</p><p>Let‚Äôs consider a concrete example. A CPU can add two numbers much faster than the GPU because of its low instruction latency. They will be able to do several of such computations in a sequence faster than a GPU. However, when it comes to doing millions or billions of such computations, a GPU will do those computations much much faster than a CPU because of its sheer massive parallelism.</p><p>If you like numbers, let‚Äôs talk about numbers. The performance of hardware for numerical computations is measured in terms of how many floating point operations it can do per second (FLOPS). The Nvidia Ampere A100 offers a throughput of 19.5 TFLOPS for 32-bit precision. In comparison, the throughput of an Intel 24-core processor is 0.66 TFLOPS for 32-bit precision (these numbers are from 2021). And, this gap in the throughput performance between GPUs and CPUs has been growing wider with each passing year.</p><p>The following figure compares the architectures of CPUs and GPUs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png" width="1456" height="719" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:719,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Figure 1: A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide&quot;,&quot;title&quot;:&quot;A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Figure 1: A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide" title="A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e81d6e3-f38b-49d3-995b-a334fb38394f_1456x719.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 1: A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide</figcaption></figure></div><p>As you may see, CPUs dedicate a significant amount of chip area towards features which will reduce instruction latency, such as large caches, less ALUs and more control units. In contrast, GPUs use a large number of ALUs to maximize their computation power and throughput. They use a very small amount of the chip area for caches and control units, the things which reduce the latency for CPUs.</p><p>You might wonder, how do the GPUs tolerate high latencies and yet provide high performance. This is made possible by the large number of threads and massive compute power that the GPUs have. Even if individual instructions have high latency, GPUs efficiently schedule threads for execution such that they are utilizing the compute power at every point in time. For instance, while some threads are waiting for the result of an instruction, the GPU will switch to executing other non-waiting threads. This ensures that the the compute units on the GPU are operating at their max capacity at all points of time, thus providing high throughput. We will get a clearer picture of this later when we discuss how a kernel executes on the GPU.</p><p>So we understand GPUs favor high throughput but what does their architecture look like which enables them to achieve this, let‚Äôs discuss in this section.</p><p>A GPU consists of an array of streaming multiprocessors (SM). Each of these SMs in turn consists of several streaming processors or cores or threads. For instance, the Nvidia H100 GPU has 132 SMs with 64 cores per SM, totalling a whopping 8448 cores.</p><p>Each SM has a limited amount of on-chip memory, often referred to as shared memory or a scratchpad, which is shared among all the cores. Likewise, the control unit resources on the SM are shared by all the cores. Additionally, each SM is equipped with hardware-based thread schedulers for executing threads.</p><p>Apart from these, each SM also has several functional units or other accelerated compute units such as tensor cores, or ray tracing units to serve specific compute demands of the workload that the GPU caters to.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png" width="971" height="593" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:593,&quot;width&quot;:971,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Figure 2: The GPU Compute Architecture&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Figure 2: The GPU Compute Architecture" title="Figure 2: The GPU Compute Architecture" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f4c3f5e-1d1c-4556-8c7e-2725cc82d2df_971x593.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 2: The GPU Compute Architecture</figcaption></figure></div><p>Next, let‚Äôs breakdown the GPU memory and look inside.</p><p>The GPU has several layers of different kinds of memories, with each having their specific use case. The following figure shows the memory hierarchy for one SM in the GPU.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg" width="1456" height="1228" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1228,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Figure 3: The GPU Memory Architecture from the Cornell Virtual Workshop on Understanding GPUs&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Figure 3: The GPU Memory Architecture from the Cornell Virtual Workshop on Understanding GPUs" title="Figure 3: The GPU Memory Architecture from the Cornell Virtual Workshop on Understanding GPUs" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9b3eeb-76c2-4a9e-b526-52cea31bfc9a_1456x1228.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 3: The GPU Memory Architecture from the Cornell Virtual Workshop on Understanding GPUs</figcaption></figure></div><p>Let‚Äôs break it down.</p><ul><li><p><strong>Registers:</strong><span> We will start with the registers. Each SM in the GPU has a large number of registers. For instance, the Nvidia A100, and H100 models have 65,536 registers per SM. These registers are shared between the cores, and are allocated to them dynamically depending on the requirement of the threads. During execution the registers allocated to a thread are private to it, i.e., other threads cannot read/write those registers.</span></p></li><li><p><strong>Constant Caches:</strong><span> Next, we have constant caches on the chip. These are used to cache constant data used by the code executing on the SM. To utilize these caches, programmers have to explicitly declare objects as constants in the code so that the GPU may cache and keep them in the constant cache.</span></p></li><li><p><strong>Shared Memory:</strong><span> Each SM also has a shared memory or scratchpad which is a small amount of fast and low latency on-chip programmable SRAM memory. It is designed to be shared by a block of threads running on the SM. The idea behind shared memory is that if multiple threads need to work with the same piece of data, only one of them should load it from the global memory, while others will share it. Careful usage of shared memory can cut down redundant load operations from global memory, and improve the kernel execution performance. Another usage of the shared memory is as a synchronization mechanism between threads executing within a block.&nbsp;</span></p></li><li><p><strong>L1 Cache:</strong><span> Each SM also has an L1 cache which can cache frequently accessed data from L2 cache.&nbsp;</span></p></li><li><p><strong>L2 Cache:</strong><span> There is an L2 cache which is shared by all SMs. It caches the frequently accessed data from the global memory to cut down the latency. Note that both L1 and L2 caches are transparent to the SM, i.e., the SM doesn‚Äôt know it is getting data from L1 or L2. As far as the SM is concerned, it is getting data from the global memory. This is similar to how L1/L2/L3 caches work in CPUs.</span></p></li><li><p><strong>Global Memory:</strong><span> The GPU also has an off-chip global memory, which is a high capacity and high bandwidth DRAM. For instance, the Nvidia H100 has 80 GB high bandwidth memory (HBM) with bandwidth of 3000 GB/second. Due to being far away from the SMs, the latency of global memory is quite high. However, the several additional layers of on-chip memories, and high number of compute units help hide this latency.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.buymeacoffee.com/codeconfessions&quot;,&quot;text&quot;:&quot;Support me by buying me a coffee!&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.buymeacoffee.com/codeconfessions" rel=""><span>Support me by buying me a coffee!</span></a></p></li></ul><p>Now that we know about the key components of the GPU hardware, let‚Äôs go one step deeper and understand how these components come into picture when executing code.</p><p>To understand how the GPU executes a kernel, we first need to understand what a kernel is and what its configurations are. Let‚Äôs start there.</p><p>CUDA is the programming interface provided by Nvidia for writing programs for their GPUs. In CUDA you express a computation that you want to run on the GPU in the form similar to a&nbsp; C/C++ function and this function is called a kernel. The kernel operates on vectors of numbers in parallel which are provided to it as function parameters. A simple example would be a kernel to perform vector addition, i.e., a kernel that takes two vectors of numbers as inputs, adds them element-wise and writes the result to a third vector.&nbsp;</p><p>To execute a kernel on the GPU, we need to launch a number of threads which is collectively referred to as a grid. But there is more structure to the grid. A grid consists of one or more thread blocks (sometimes simply called as blocks) and each block consists of one or more threads.</p><p>The number of blocks and threads depends on the size of the data and the amount of parallelism we want. For instance, in our vector addition example, if we are adding vectors of dimension 256, then we may decide to configure a single thread block of 256 threads so that each thread operates on one element of the vector. For bigger problems, we may not have enough threads available on the GPU and we might want each thread to handle multiple data points.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png" width="1456" height="692" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16b44282-f064-478e-84b4-0c44664c1630_1600x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:692,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Grid of thread blocks (figure from Nvidia CUDA C++ Programming Guide)&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Grid of thread blocks (figure from Nvidia CUDA C++ Programming Guide)" title="Grid of thread blocks (figure from Nvidia CUDA C++ Programming Guide)" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b44282-f064-478e-84b4-0c44664c1630_1600x760.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 4: Grid of thread blocks (figure from Nvidia CUDA C++ Programming Guide)</figcaption></figure></div><p>As far as implementation goes, writing a kernel requires two parts. One is the host code which executes on the CPU. This is where we load the data, allocate memory on the GPU, and launch the kernel with a configured grid of threads. The 2nd part is writing the device (GPU) code which executes on the GPU.</p><p>For our vector addition example, the following figure shows the host code.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png" width="1442" height="739" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:739,&quot;width&quot;:1442,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Host code for the CUDA kernel for adding two vectors&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Host code for the CUDA kernel for adding two vectors" title="Host code for the CUDA kernel for adding two vectors" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51685688-c07d-4baa-b0c0-7021597e7132_1442x739.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 5: Host code for the CUDA kernel for adding two vectors</figcaption></figure></div><p>And the following is the device code, which defines the actual kernel function.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png" width="1456" height="617" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:617,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Device code containing the definition of the vector addition kernel&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Device code containing the definition of the vector addition kernel" title="Device code containing the definition of the vector addition kernel" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7cb6a3a-d76f-4263-b9b8-0915e6b74c8f_1506x638.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 6: Device code containing the definition of the vector addition kernel</figcaption></figure></div><blockquote><p>As the focus of this article is not teaching CUDA, we will not be going any deeper in this code. Now, let‚Äôs look at the exact steps behind the execution of a kernel on the GPU.</p></blockquote><p><span>Before the kernel can be scheduled for execution, all the data that it needs has to be copied from the memory of the host (the CPU) onto the global memory of the GPU (the device). Although, in latest GPU hardware one can also read directly from host memory using unified virtual memory (see section 2.2 of the paper: ‚Äú</span><a href="https://arxiv.org/pdf/2006.06890.pdf" rel="">EMOGI: Efficient Memory-access for Out-of-memory Graph-traversal in GPUs</a><span>‚Äù).&nbsp;</span></p><p>After the GPU has all the necessary data in its memory, it assigns the thread blocks to the SMs. All threads within a block are processed by the same SM at the same time. To make this happen, the GPU must set aside resources on the SM for those threads before it can start executing them. In practice, multiple thread blocks can be assigned to the same SM for simultaneous execution.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png" width="1041" height="331" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0417e64e-77a6-443f-b861-5f2640291187_1041x331.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:331,&quot;width&quot;:1041,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Assignment of a thread block to an SM&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Assignment of a thread block to an SM" title="Assignment of a thread block to an SM" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0417e64e-77a6-443f-b861-5f2640291187_1041x331.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 7: Assignment of a thread block to an SM</figcaption></figure></div><p>As there are a limited number of SMs and large kernels can have a very large number of blocks, not all the blocks may get assigned for execution immediately. The GPU maintains a list of blocks which are waitlisted for assignment and execution. As and when any block finishes execution, the GPU assigns one of the waitlisted blocks for execution.</p><p><span>We know that all threads of a block are assigned to the same SM. But there‚Äôs another level of division of threads after this. These threads are further grouped into sizes of 32 (called a </span><em>warp</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-137892185" href="https://blog.codingconfessions.com/p/gpu-computing#footnote-2-137892185" target="_self" rel="">2</a></span><span>), and assigned together for execution on a set of cores called a processing block.</span></p><p>The SM executes all the threads within a warp together by fetching and issuing the same instruction to all of them. These threads then execute that instruction simultaneously, but on different parts of the data. In our vector addition example, all the threads in a warp might be executing the add instruction, but they would be operating on different indices of the vectors.</p><p><span>This execution model of the warp is also called </span><em>single instruction multiple threads</em><span> (SIMT) because multiple threads are executing the same instruction. It is similar to the </span><em><a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data" rel="">single instruction multiple data</a></em><span> (SIMD) instructions in CPUs.</span></p><blockquote><p><span>There is an alternative instruction scheduling mechanism available in newer generations of GPUs, starting from Volta and onwards, known as </span><em>independent thread scheduling</em><span>. It allows full concurrency between threads, regardless of warp. It can be used to make better use of the execution resources, or as a synchronization mechanism between threads. We will not cover independent thread scheduling here, but you can read about it in the </span><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" rel="">CUDA programming guide</a><span>.</span></p></blockquote><p>There are some interesting details about how warps work, that are worth discussing.</p><p>Even if all the processing blocks (groups of cores) within an SM are handling warps, only a few of them are actively executing instructions at any given moment. This happens because there are a limited number of execution units available in the SM.</p><p>But some instructions take longer to complete, causing a warp to wait for the result. In such cases, the SM puts that waiting warp to sleep and starts executing another warp that doesn't need to wait for anything. This enables the GPUs to maximally utilize all the available compute and deliver high throughput.</p><blockquote><p><strong>Zero-overhead Scheduling: </strong><span>As each thread in each warp has its own set of registers, there is no overhead for the SM to switch from executing one warp to another.&nbsp;</span></p><p>This is in contrast to how context-switching between processes happens on the CPU. If a process is waiting for a long running operation, the CPU schedules another process on that core in the meanwhile. However, context switching in CPU is expensive because the CPU needs to save the registers into main memory, and restore the state of the other process.</p></blockquote><p>Finally, when all the threads of the kernel have finished executing, the final step is to copy the result back to the host memory.</p><p>Although we covered everything about a typical kernel execution but there is one more thing that requires its own section: dynamic resource partitioning.</p><p><span>We measure the utilization of the GPU resources through a metric called ‚Äú</span><em>occupancy</em><span>‚Äù, which represents the ratio of the number of warps assigned to an SM to the maximum number it can support. To achieve maximum throughput, we would want to have 100% occupancy. However, in practice it is not always possible due to various constraints.&nbsp;</span></p><p>So, why can't we always reach 100% occupancy? The SM has a fixed set of execution resources, including registers, shared memory, thread block slots, and thread slots. These resources are dynamically divided among threads based on their requirements and the GPU's limits. For example, on the Nvidia H100, each SM can handle 32 blocks, 64 warps (i.e., 2048 threads), and 1024 threads per block. If we launch a grid with a block size of 1024 threads, the GPU will split the 2048 available thread slots into 2 blocks.</p><blockquote><p><strong>Dynamic vs Fixed partitioning</strong><span>: Dynamic partitioning allows for more effective usage of the computation resources in the GPU. If we compare this with a fixed partitioning scheme where each thread block receives a fixed amount of execution resources it might not always be the most efficient. In some cases the threads might be assigned more resources than they need, leading to wastage of resources and reduced throughput.</span></p></blockquote><p>Now, let's look at an example to see how resource allocation can affect the occupancy of an SM. If we use a block size of 32 threads and need a total of 2048 threads, we'll have 64 of these blocks. However, each SM can only handle 32 blocks at once. So, even though the SM can run 2048 threads, it will only be running 1024 threads at a time, resulting in a 50% occupancy rate.&nbsp;</p><p>Similarly, each SM has 65536 registers. To execute 2048 threads simultaneously, each thread can have a maximum of 32 registers (65536/2048 = 32). If a kernel needs 64 registers per thread, we can only run 1024 threads per SM, again resulting in 50% occupancy.</p><p>The challenge with suboptimal occupancy is that it may not provide the necessary tolerance for latency or the required compute throughput to reach the hardware‚Äôs peak performance.</p><p>Efficiently creating GPU kernels is a complex task. We must allocate resources wisely to maintain high occupancy while minimizing latency. For example, having many registers can make code run quickly but might reduce occupancy, so careful code optimization is important.</p><p>I understand that wrapping your head around so many new terms and concepts is daunting. Let‚Äôs summarize the key points for a quick review.</p><ul><li><p>A GPU consists of several streaming multiprocessors (SM), where each SM has several processing cores.</p></li><li><p>There is an off chip global memory, which is a HBM or DRAM. It is far from the SMs on the chip and has high latency.</p></li><li><p>There is an off chip L2 cache and an on chip L1 cache. These L1 and L2 caches operate similarly to how L1/L2 caches operate in CPUs.</p></li><li><p>There is a small amount of configurable shared memory on each SM. This is shared between the cores. Typically, threads within a thread block load a piece of data into the shared memory and then reuse it instead of loading it again from global memory.</p></li><li><p>Each SM has a large number of registers, which are partitioned between threads depending on their requirement. The Nvidia H100 has 65,536 registers per SM.</p></li><li><p>To execute a kernel on the GPU, we launch a grid of threads. A grid consists of one or more thread blocks and each thread block consists of one or more threads.</p></li><li><p>The GPU assigns one or more blocks for execution on an SM depending on resource availability. All threads of one block are assigned and executed on the same SM. This is for leveraging data locality and for synchronization between threads.</p></li><li><p>The threads assigned to an SM are further grouped into sizes of 32, which is called a warp. All the threads within a warp execute the same instruction at the same time, but on different parts of the data (SIMT). (Although newer generations of GPUs also support independent thread scheduling.)</p></li><li><p>The GPU performs dynamic resource partitioning between the threads based on each threads requirements and the limits of the SM. The programmer needs to carefully optimize code to ensure the highest level of SM occupancy during execution.</p></li></ul><p><span>GPUs are in pervasive use today, but their architecture and execution model is fundamentally very different from CPUs. In this article we covered various aspects of GPUs, including their architecture and their execution model. If you're curious about what makes GPUs so sought after and how they operate, I hope this article has provided some valuable insights. If you have any questions about what we discussed here, feel free to ask them in the comments, or reach out to me on</span><a href="https://twitter.com/abhi9u" rel=""> Twitter/X</a><span>.</span></p><p><span>I would like to thank </span><a href="https://msharmavikram.github.io/" rel="">Vikram Sharma Mailthody</a><span>, who is a Senior Research Scientist at Nvidia for reviewing and offering insights on various parts of the article. His feedback helped improve the quality of the article significantly. I am very grateful. Vikram is very interested in increasing awareness about GPU programming, so if you are interested in learning more about this area, reach out to him on </span><a href="https://twitter.com/msharmavikram" rel="">Twitter</a><span> or </span><a href="https://www.linkedin.com/in/vikramsharmam/" rel="">LinkedIn</a><span>.</span></p><p>If you want to dive deeper into GPUs, here are few resources you could refer to:</p><ul><li><p>Programming Massively Parallel Processors: 4th edition is the most up-to-date reference, but earlier editions are fine, too.</p></li><li><p><a href="https://www.youtube.com/channel/UC2Y6MNqiTwTrh-qPQHGIUyg" rel="">Programming Massively Parallel Processors</a><span>: online course by Prof. Hwu</span></p></li><li><p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" rel="">Nvidia‚Äôs CUDA C++ Programming Guide</a></p></li><li><p><a href="https://www.youtube.com/watch?v=3l10o0DYJXg" rel="">How GPU Computing Works (YouTube)</a></p></li><li><p><a href="https://enccs.github.io/gpu-programming/" rel="">GPU Programming: When, Why and How?</a></p></li></ul><p data-attrs="{&quot;url&quot;:&quot;https://blog.codingconfessions.com/p/gpu-computing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.codingconfessions.com/p/gpu-computing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're Leaving Kubernetes (370 pts)]]></title>
            <link>https://www.gitpod.io/blog/we-are-leaving-kubernetes</link>
            <guid>42041917</guid>
            <pubDate>Mon, 04 Nov 2024 14:41:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gitpod.io/blog/we-are-leaving-kubernetes">https://www.gitpod.io/blog/we-are-leaving-kubernetes</a>, See on <a href="https://news.ycombinator.com/item?id=42041917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				<header>
					</header>
				<p>Kubernetes seems like the obvious choice for building out remote, standardized and automated development environments. We thought so too and have spent six years invested in making the most popular cloud development environment platform at internet scale. That‚Äôs 1.5 million users, where we regularly see thousands of development environments per day. <strong>In that time we‚Äôve found that Kubernetes is not the right choice for building development environments.</strong></p>
<p>This is our journey of experiments, failures and dead-ends building development environments on <a href="https://kubernetes.io/" rel="nofollow noopener noreferrer" target="_blank">Kubernetes</a>. Over the years, we experimented with many ideas involving SSDs, PVCs, <a href="https://ebpf.io/" rel="nofollow noopener noreferrer" target="_blank">eBPF</a>, <a href="https://brauner.github.io/2020/07/23/seccomp-notify.html" rel="nofollow noopener noreferrer" target="_blank">seccomp notify</a>, <a href="https://man7.org/linux/man-pages/man8/tc.8.html" rel="nofollow noopener noreferrer" target="_blank">TC</a> and <a href="https://kernel.dk/io_uring.pdf" rel="nofollow noopener noreferrer" target="_blank">io_uring</a>, <a href="https://lwn.net/Articles/718639/" rel="nofollow noopener noreferrer" target="_blank">shiftfs</a>, FUSE and <a href="https://docs.kernel.org/filesystems/idmappings.html" rel="nofollow noopener noreferrer" target="_blank">idmapped mounts</a>, ranging from <a href="https://firecracker-microvm.github.io/" rel="nofollow noopener noreferrer" target="_blank">microVMs</a>, <a href="https://kubevirt.io/" rel="nofollow noopener noreferrer" target="_blank">kubevirt</a> to <a href="https://www.vcluster.com/" rel="nofollow noopener noreferrer" target="_blank">vCluster</a>.</p>
<p>In pursuit of the most optimal infrastructure to balance security, performance and interoperability. All while wrestling with the unique challenges of building a system to scale up, remain secure as it‚Äôs handling arbitrary code execution, and be stable enough for developers to work in.</p>
<p><strong>This is not a story of whether or not to use Kubernetes for production workloads</strong> that‚Äôs a whole separate conversation. As is the topic of how to build a comprehensive soup-to-nuts developer experience for shipping applications on Kubernetes.</p>
<p><strong>This is the story of how (not) to build development environments in the cloud.</strong></p>
<h2 id="why-are-development-environments-unique">Why are development environments unique?<a href="#why-are-development-environments-unique"></a></h2>
<p>Before we dive in, it‚Äôs crucial to understand what makes development environments unique compared to production workloads:</p>
<ul><li><strong>They are extremely stateful and interactive</strong>: Which means they cannot be moved from one node to another. The many gigabytes of source code, build caches, Docker container and test data are subject to a high change rate and costly to migrate. Unlike many production services, there‚Äôs a 1-to-1 interaction between the developer and their environment.</li>
<li><strong>Developers are deeply invested in their source code and the changes they make</strong>: Developers don‚Äôt take kindly to losing any source code changes or to being blocked by any system. This makes development environments particularly intolerant to failure.</li>
<li><strong>They have unpredictable resource usage patterns</strong>: Development Environments have particular and unpredictable resource usage patterns. They won‚Äôt need much CPU bandwidth most of the time, but will require several cores within a few 100ms. Anything slower than that manifests as unacceptable latency and unresponsiveness.</li>
<li><strong>They require far-reaching permissions and capabilities</strong>: Unlike production workloads, development environments often need root access and the ability to download and install packages. What constitutes a security concern for production workloads, is expected behavior of development environments: getting root access, extended network capabilities and control over the system (e.g. mounting additional filesystems).</li></ul>
<p>These characteristics set development environments apart from typical application workloads and significantly influence the infrastructure decisions we‚Äôve made along the way.</p>
<h2 id="the-system-today-obviously-its-kubernetes">The system today: obviously it‚Äôs Kubernetes<a href="#the-system-today-obviously-its-kubernetes"></a></h2>
<p>When we started Gitpod, Kubernetes seemed like the ideal choice for our infrastructure. Its promise of scalability, container orchestration, and rich ecosystem aligned perfectly with our vision for cloud development environments. However, as we scaled and our user base grew, we encountered several challenges around security and state management that pushed Kubernetes to its limits. <strong>Fundamentally, Kubernetes is built to run well controlled application workloads, not unruly development environments.</strong></p>
<p>Managing Kubernetes at scale is complex. While managed services like GKE and EKS alleviate some pain points, they come with their own set of restrictions and limitations. We found that many teams looking to operate a CDE underestimate the complexity of Kubernetes, which lead to a significant support load for our previous self-managed Gitpod offering.</p>
<h2 id="resource-management-struggles">Resource management struggles<a href="#resource-management-struggles"></a></h2>
<p>One of the most significant challenges we faced was resource management, particularly CPU and memory allocation per environment. At first glance, running multiple environments on a node seems attractive to share resources (such as CPU, memory, IO and network bandwidth) between those resources. In practice, this incurs significant noisy neighbor effects leading to a detrimental user experience.</p>
<h3 id="cpu-challenges">CPU challenges<a href="#cpu-challenges"></a></h3>
<p>CPU time seems like the simplest candidate to share between environments. Most of the time development environments don‚Äôt need much CPU, but when they do, they need it quickly. Latency becomes immediately apparent to users when their language server starts to lag or their terminal becomes choppy. This spiky nature of the CPU requirements of development environments (periods of inactivity followed by intensive builds) makes it difficult to predict when CPU time is needed.</p>
<p>For solutions, we experimented with various CFS (<a href="https://docs.kernel.org/scheduler/sched-design-CFS.html" rel="nofollow noopener noreferrer" target="_blank">Completely Fair Scheduler</a>) based schemes, implementing a custom controller using a DaemonSet. A core challenge is that we can not predict when CPU bandwidth is needed, but only understand when it would have been needed (by observing nr_throttled of the cgroup‚Äôs cpu_stats).</p>
<p>Even when using static CPU resource limits, challenges arise, because unlike application workloads a development environment will run many processes in the same container. These processes compete for the same CPU bandwidth, which can lead to e.g. VS Code disconnects because VS Code server is starved for CPU time.</p>
<p>We have attempted to solve this problem by adjusting the process priorities of the individual processes, e.g. increasing the priority of bash or vscode-server. However, these process priorities apply to the entire process group (depending on your Kernel‚Äôs autogroup scheduling configuration), hence also to the resource hungry compilers started in a VS Code terminal. Using process priorities to counter terminal lag requires a carefully written control loop to be effective.</p>
<p>We introduced custom CFS and process priority based control loops built on cgroupv1 and moved to cgroupsv2 once they became more readily available on managed Kubernetes platforms with 1.24. <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/" rel="nofollow noopener noreferrer" target="_blank">Dynamic resource allocation</a> introduced with Kubernetes 1.26 means one no longer needs to deploy a DaemonSet and modify cgroups directly, possibly at the expense of the control loop speed and hence effectiveness. All the schemes outlined above rely on second-by-second readjustment of CFS limits and niceness values.</p>
<h3 id="memory-management">Memory management<a href="#memory-management"></a></h3>
<p>Memory management presented its own set of challenges. Assigning every environment a fixed amount of memory, so that under maximum occupation each environment gets their fixed share is straightforward, but very limiting. In the cloud, RAM is one of the more expensive resources, hence the desire to overbook memory.</p>
<p>Until <a href="https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/" rel="nofollow noopener noreferrer" target="_blank">swap-space became available in Kubernetes 1.22</a>, memory overbooking was near impossible to do, because reclaiming memory inevitably means killing processes. With the addition of swap space the need to overbook memory has somewhat gone away, since swap works well in practice for hosting development environments.</p>
<h2 id="storage-performance-optimization">Storage performance optimization<a href="#storage-performance-optimization"></a></h2>
<p>Storage performance is important for the startup performance and experience of development environments. We have found that specifically IOPS and latency affect experience within an environment. IO bandwidth however directly impacts your workspace startup performance, specifically when creating/restoring backups or extracting large workspace images.</p>
<p><strong>We experimented with various setups to find the right balance between speed and reliability, cost and performance.</strong></p>
<ul><li><strong>SSD RAID 0</strong>: This offered high IOPS and bandwidth but tied the data to a specific node. The failure of any single disk would result in complete data loss. This is how gitpod.io operates today and we have not seen such a disk failure happen yet. A simpler version of this setup is to use a single SSD attached to the node. This approach provides lower IOPS and bandwidth, and still binds the data to individual nodes.</li>
<li><strong>Block storage such as EBS volumes or Google persistent disks which are permanently attached to the nodes</strong> considerably broaden the different instances or availability zones that can be used. While still bound to a single node, and offering considerably lower throughput/bandwidth than local SSDs they are more widely available.</li>
<li><strong>Persistent Volume Claims (PVCs) seem like the obvious choice when using Kubernetes</strong>. As abstraction over different storage implementations they offer a lot of flexibility, but also introduce new challenges:<ul><li>Unpredictable attachment and detachment timing, leading to unpredictable workspace startup times. Combined with increased scheduling complexity they make implementing effective scheduling strategies harder.</li>
<li>Reliability issues leading to workspace failures, particularly during startup. This was especially noticeable on Google Cloud (in 2022) and rendered our attempts to use PVCs impractical.</li>
<li>Limited number of disks that could be attached to an instance, imposing additional constraints on the scheduler and number of workspaces per node.</li>
<li>AZ locality constraints which makes balancing workspaces across AZs even harder.</li></ul></li></ul>
<p>Backing up and restoring local disks proved to be an expensive operation. We implemented a solution using a daemonSet that uploads and downloads uncompressed tar archives to/from S3. This approach required careful balancing of I/O, network bandwidth, and CPU usage: for example, (de)compressing archives consumes most available CPU on a node, whereas the extra traffic produced by uncompressed backups usually doesn‚Äôt consume all available network bandwidth (if the number of concurrently starting/stopping workspaces is carefully controlled).</p>
<p>IO bandwidth on the node is shared across workspaces. We found that, unless we limited the IO bandwidth available to each workspace, other workspaces might starve for IO bandwidth and cease to function. Particularly the content backup/restore produced this problem. We implemented cgroup-based <a href="https://github.com/gitpod-io/gitpod/pull/9440" rel="nofollow noopener noreferrer" target="_blank">IO limiter</a> which imposed fixed IO bandwidth limits per environment to solve this problem.</p>
<h2 id="autoscaling-and-startup-time-optimization">Autoscaling and startup time optimization<a href="#autoscaling-and-startup-time-optimization"></a></h2>
<p>Our primary goal was to minimize startup time at all costs. Unpredictable wait times can significantly impact productivity and user satisfaction. However, this goal often conflicted with our desire to pack workspaces densely to maximize machine utilization.</p>
<p>We initially thought that running multiple workspaces on one node would help with startup times due to shared caches. However, this didn‚Äôt pan out as expected. The reality is that Kubernetes imposes a lower bound for startup time because of all the content operations that need to happen, content needs to be moved into place, which takes time.</p>
<p>Short of keeping workspaces in hot standby (which would be prohibitively expensive), we had to find other ways to optimize startup times.</p>
<h3 id="scaling-ahead-evolution-of-our-approach">Scaling ahead: evolution of our approach<a href="#scaling-ahead-evolution-of-our-approach"></a></h3>
<p><strong>To minimize startup time, we explored various approaches to scale up and ahead:</strong></p>
<ul><li><strong>Ghost workspaces</strong>: Before cluster autoscaler plugins were available, we experimented with ‚Äúghost workspaces‚Äù. These were preemptible pods that occupied space to scale ahead. We implemented this using a custom scheduler. However, this approach proved to be slow and unreliable to replace.</li>
<li><strong>Ballast pods</strong>: An evolution of the ghost workspace concept, ballast pods filled an entire node. This resulted in less replacement cost and faster replacement times compared to ghost workspaces.</li>
<li><strong>Autoscaler plugins</strong>: In June 2022, we switched to using cluster-autoscaler plugins when they were introduced. With these plugins we no longer needed to ‚Äútrick‚Äù the autoscaler, but could directly affect how scale-up happens. This marked a significant improvement in our scaling strategy.</li></ul>
<h3 id="proportional-autoscaling-for-peak-loads">Proportional autoscaling for peak loads<a href="#proportional-autoscaling-for-peak-loads"></a></h3>
<p>To handle peak loads more effectively, we implemented a proportional autoscaling system. This approach controls the rate of scale-up as a function of the rate of starting development environments. It works by launching empty pods using the pause image, allowing us to quickly increase our capacity in response to demand spikes.</p>
<h3 id="image-pull-optimization-a-tale-of-many-attempts">Image pull optimization: a tale of many attempts<a href="#image-pull-optimization-a-tale-of-many-attempts"></a></h3>
<p>Another crucial aspect of startup time optimization was improving image pull times. Workspace container images (i.e. all the tools available to a developer) can grow to more than 10 gigabytes uncompressed. Downloading and extracting this amount of data for every workspace considerably taxes a node‚Äôs resources. We explored numerous strategies to speed up image pulls:</p>
<ul><li><strong>Daemonset pre-pull</strong>: We tried pre-pulling common images using a daemonSet. However, this proved ineffective during scale-up operations because when the node came online, and workspaces were starting, the images still wouldn‚Äôt be present on the node. Also, the pre-pulls would now compete for IO and CPU bandwidth with the starting workspaces.</li>
<li><strong>Layer reuse maximization</strong>: We built our own images using a custom builder called <a href="https://github.com/gitpod-io/dazzle" rel="nofollow noopener noreferrer" target="_blank">dazzle</a>, which can build layers independently. This approach aimed to maximize layer reuse. However, we found that layer reuse is very difficult to observe due to the high cardinality and amount of indirections in the <a href="https://github.com/opencontainers/image-spec/blob/main/spec.md" rel="nofollow noopener noreferrer" target="_blank">OCI manifests</a>.</li>
<li><strong>Pre-baked images</strong>: We experimented with baking images into the node disk image. While this improved startup times, it had significant drawbacks. The images quickly became outdated, and this approach didn‚Äôt work for self-hosted installations.</li>
<li><strong>Stargazer and lazy-pulling</strong>: This method required all images to be converted, which added complexity, cost, and time to our operations. Additionally, not all registries supported this approach when we tried it in 2022.</li>
<li><strong>Registry-facade + IPFS</strong>: This solution worked well in practice, providing good performance and distribution. We gave a <a href="https://www.youtube.com/watch?v=kS6aDScfVuw" rel="nofollow noopener noreferrer" target="_blank">KubeCon talk about this approach in 2022</a>. However, it introduced significant complexity to our system.</li></ul>
<p>There is no one-size-fits all solution for image caching, but a set of trade-offs with respect to complexity, cost and restrictions imposed on users (images they can use). We have found that homogeneity of workspace images is the most straightforward way to optimize startup times.</p>
<h2 id="networking-complexities">Networking complexities<a href="#networking-complexities"></a></h2>
<p>Networking in Kubernetes introduced its own set of challenges, specifically:</p>
<ul><li><p><strong>Development environment access control</strong>: by default the network of environments needs to be entirely isolated from one another, i.e. one environment cannot reach another. The same is true for the access of a user to the workspace. <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" rel="nofollow noopener noreferrer" target="_blank">Network Policies</a> go a long way in ensuring environments are properly disconnected from each other.</p>
<p>Initially we controlled the access to individual environment ports (such as the IDE, or services running in the workspace) using Kubernetes services, together with an ingress proxy that would forward traffic to the service, resolving it using DNS. This quickly became unreliable at scale because of the sheer number of services. Name resolution would fail, and if not careful (e.g. setting enableServiceLinks: false) one can bring entire workspaces down.</p></li>
<li><p><strong>Network bandwidth sharing</strong> on the node is yet another resource that needs to be shared with multiple workspaces on a single node. Some CNIs offer network shaping support (e.g. Cilium‚Äôs <a href="https://docs.cilium.io/en/latest/network/kubernetes/bandwidth-manager/" rel="nofollow noopener noreferrer" target="_blank">Bandwidth Manager</a>). This now leaves you with yet another resource to control for, and potentially share between environments.</p></li></ul>
<h2 id="security-and-isolation-balancing-flexibility-and-protection">Security and isolation: balancing flexibility and protection<a href="#security-and-isolation-balancing-flexibility-and-protection"></a></h2>
<p>One of the most significant challenges we faced in our Kubernetes-based infrastructure was providing a secure environment while giving users the flexibility they need for development. Users want the ability to install additional tools (e.g., using apt-get install), run Docker, or even set up a Kubernetes cluster within their development environment. Balancing these requirements with robust security measures proved to be a complex undertaking.</p>
<h3 id="the-naive-approach-root-access">The naive approach: root access<a href="#the-naive-approach-root-access"></a></h3>
<p>The simplest solution would be to give users root access to their containers. However, this approach quickly reveals its flaws:</p>
<ul><li>Giving users root access essentially provides them with root privileges on the node itself, granting access to the development environment platform and other development environments that are running on that node .</li>
<li>This eliminates any meaningful security boundary between users and the host system meaning developers can accidentally, or intentionally interfere and break the development environment platform itself or even access others‚Äô development environments.</li>
<li>It also exposes the infrastructure to potential abuse and security risks. It is then also not possible to implement a true access control model and the architecture falls short of zero-trust. You cannot ensure that a given actor in the system performing an action was verifiably themselves.</li></ul>
<p>Clearly, a more sophisticated approach was needed.</p>
<h3 id="user-namespaces-a-more-nuanced-solution">User namespaces: a more nuanced solution<a href="#user-namespaces-a-more-nuanced-solution"></a></h3>
<p>To address these challenges, we turned to user namespaces, a Linux kernel feature that provides fine-grained control over the mapping of user and group IDs inside containers. This approach allows us to give users ‚Äúroot-like‚Äù privileges within their container without compromising the security of the host system.</p>
<p>While Kubernetes introduced support for user namespaces in version 1.25, we had already implemented our own solution starting with Kubernetes 1.22. Our implementation involved several complex components:</p>
<ul><li><p><strong>Filesystem UID shift</strong>: This is necessary to ensure that files created inside the container map correctly to UIDs on the host system. We experimented with several approaches:</p>
<ul><li>We continue to use shiftfs as our primary method for filesystem UID shifting. Despite being deprecated in some contexts, shiftfs still provides the functionality we need with acceptable performance characteristics.</li>
<li>We‚Äôve experimented with fuse-overlayfs, which provided the necessary functionality but had performance limitations.</li>
<li>While idmapped mounts offer potential benefits, we haven‚Äôt transitioned to them yet due to various compatibility and implementation considerations.</li></ul></li>
<li><p><strong>Mounting masked proc</strong>: When a container starts, it typically wants to mount /proc. However, in our security model, /proc is sensibly masked to prevent potential security bypasses. Working around this limitation required a tricky solution:</p>
<ul><li>We construct a masked proc filesystem.</li>
<li>This masked proc is then moved into the correct mount namespace.</li>
<li>We implement this using seccomp notify, which allows us to intercept and modify certain system calls.</li></ul></li>
<li><p><strong>FUSE support</strong>: Adding FUSE (Filesystem in Userspace) support, which is crucial for many development workflows, required implementing custom device policies. This involved modifying the container‚Äôs eBPF (extended Berkeley Packet Filter) device filter, a low-level programming capability that allows us to make fine-grained decisions about device access.</p></li>
<li><p><strong>Network capabilities</strong>: As true root one holds the CAP_NET_ADMIN and CAP_NET_RAW capabilities which provide far-reaching privileges to configure networking. Container runtimes (such as Docker/runc) make extensive use of these capabilities. Granting such capabilities to the development environment container would interfere with CNI and break the security isolation.</p>
<p>To provide such capabilities we ended up creating another network namespace inside the Kubernetes container, first connected to the outside world using slirp4netns and later using a veth pair and custom and nftables rules.</p></li>
<li><p><strong>Enabling docker</strong>: required some specific hacks for Docker itself. We register a custom <a href="https://github.com/gitpod-io/gitpod/blob/main/components/docker-up/runc-facade/main.go" rel="nofollow noopener noreferrer" target="_blank">runc-facade</a> which modifies the OCI runtime spec produced by Docker. This lets us remove e.g. OOMScoreAdj which still isn‚Äôt allowed because that would require CAP_SYS_ADMIN on the node.</p></li></ul>
<p>Implementing this security model came with its own set of challenges:</p>
<ul><li><strong>Performance impact</strong>: Some of our solutions, particularly earlier ones like fuse-overlayfs, had noticeable performance impacts. We‚Äôve continually worked to optimize these.</li>
<li><strong>Compatibility</strong>: Not all tools and workflows are compatible with this restricted environment. We‚Äôve had to carefully balance security with usability.</li>
<li><strong>Complexity</strong>: The resulting system is significantly more complex than a simple containerized environment, which impacts both development and operational overhead.</li>
<li><strong>Keeping up with Kubernetes</strong>: As Kubernetes has evolved, we‚Äôve had to adapt our custom implementations to take advantage of new features while maintaining backwards compatibility.</li></ul>
<h2 id="the-micro-vm-experiment">The micro-VM experiment<a href="#the-micro-vm-experiment"></a></h2>
<p>As we grappled with the challenges of Kubernetes, we began exploring micro-VM (uVM) technologies like <a href="https://github.com/firecracker-microvm/firecracker" rel="nofollow noopener noreferrer" target="_blank">Firecracker</a>, <a href="https://www.cloudhypervisor.org/" rel="nofollow noopener noreferrer" target="_blank">Cloud Hypervisor</a>, and <a href="https://www.qemu.org/" rel="nofollow noopener noreferrer" target="_blank">QEMU</a> as a potential middle ground. This exploration was driven by the promise of improved resource isolation, compatibility with other workloads (e.g. Kubernetes) and security, while potentially maintaining some of the benefits of containerization.</p>
<h3 id="the-promise-of-micro-vms">The promise of micro-VMs<a href="#the-promise-of-micro-vms"></a></h3>
<p>Micro-VMs offered several enticing benefits that aligned well with our goals for cloud development environments:</p>
<ul><li><strong>Enhanced resource isolation</strong>: uVMs promised better resource isolation compared to containers, albeit at the expense of overbooking capabilities. With uVMs, we would no longer have to contend with shared kernel resources, potentially leading to more predictable performance for each development environment.</li>
<li><strong>Memory snapshots and fast resume</strong>: One of the most exciting features, particularly with Firecracker using userfaultfd, was the support for memory snapshots. This technology promised near-instant full machine resume, including running processes. For developers, this could mean significantly faster environment startup times and the ability to pick up exactly where they left off.</li>
<li><strong>Improved security boundaries</strong>: uVMs offered the potential to serve as a robust security boundary, potentially eliminating the need for the complex user namespace mechanisms we had implemented in our Kubernetes setup. This could provide full compatibility with a wider range of workloads, including nested containerization (running Docker or even Kubernetes within the development environment).</li></ul>
<h3 id="challenges-with-micro-vms">Challenges with micro-VMs<a href="#challenges-with-micro-vms"></a></h3>
<p>However, our experiments with micro-VMs revealed several significant challenges:</p>
<ul><li><strong>Overhead</strong>: Even as lightweight VMs, uVMs introduced more overhead than containers. This impacted both performance and resource utilization, key considerations for a cloud development environment platform.</li>
<li><strong>Image conversion</strong>: Converting OCI (Open Container Initiative) images into uVM-consumable filesystems required custom solutions. This added complexity to our image management pipeline and potentially impacted startup times.</li>
<li><strong>Technology-specific limitations</strong>:<ul><li>Firecracker:<ul><li>Lack of GPU support, which is increasingly important for certain development workflows.</li>
<li>No virtiofs support at the time of our experiments (mid 2023), limiting our options for efficient file system sharing.</li></ul></li>
<li>Cloud hypervisor:<ul><li>Slower snapshot and restore processes due to the lack of userfaultfd support, negating one of the key advantages we hoped to gain from uVMs.</li></ul></li></ul></li>
<li><strong>Data movement challenges</strong>: Moving data around became even more challenging with uVMs, as we now had to contend with large memory snapshots. This affected both scheduling and startup times, two critical factors for user experience in cloud development environments.</li>
<li><strong>Storage considerations</strong>: Our experiments with attaching EBS volumes to micro-VMs opened up new possibilities but also raised new questions:<ul><li>Persistent storage: Keeping workspace content on attached volumes reduced the need to pull data from S3 repeatedly, potentially improving startup times and reducing network usage.</li>
<li>Performance considerations: While sharing high-throughput volumes among workspaces showed promise for improving I/O performance, it also raised concerns about implementing effective quotas, managing latency, and ensuring scalability.</li></ul></li></ul>
<h3 id="lessons-from-the-uvm-experiment">Lessons from the uVM experiment<a href="#lessons-from-the-uvm-experiment"></a></h3>
<p>While micro-VMs didn‚Äôt ultimately become our primary infrastructure solution, the experiment provided valuable insights:</p>
<ul><li>We loved the experience of full workspace backup and runtime state suspend/resume provided for development environments.</li>
<li>We, for the first time, considered moving away from Kubernetes. The effort to integrate KVM and uVMs into pods had us explore options outside of Kubernetes.</li>
<li>We, once again, identified storage as the crucial element for providing all three: reliable startup performance, reliable workspaces (don‚Äôt lose my data) and optimal machine utilization.</li></ul>
<h2 id="kubernetes-is-immensely-challenging-as-a-development-environment-platform">Kubernetes is immensely challenging as a development environment platform<a href="#kubernetes-is-immensely-challenging-as-a-development-environment-platform"></a></h2>
<p>As I mentioned at the beginning, for development environments we need a system that respects the uniquely stateful nature of development environments. We need to give the necessary permissions for developers to be productive, whilst ensuring secure boundaries. And we need to do all of this whilst keeping operational overhead low and not compromising security.</p>
<p>Today, achieving all of the above with Kubernetes is possible‚Äîbut comes at a significant cost. We learned the difference between application and system workloads the hard way.</p>
<p>Kubernetes is incredible. It‚Äôs supported by an engaged and passionate community, which builds a truly rich ecosystem. If you‚Äôre running application workloads, Kubernetes continues to be a fine choice. However <strong>for system workloads like development environments Kubernetes presents immense challenges in both security and operational overhead</strong>. Micro-VMs and clear resource budgets help, but make cost a more dominating factor.</p>
<p>So after many years of effectively reverse-engineering and forcing development environments onto the Kubernetes platform we took a step back to think about what we believe a future development architecture needs to look like. In January 2024 we set out to build it. In October, we shipped it: <a href="https://www.gitpod.io/blog/introducing-gitpod-flex" rel="nofollow noopener noreferrer" target="_blank">Gitpod Flex</a>.</p>
<p>More than six years of incredibly hard-won insights for running development environments securely at internet scale went into the architectural foundations.</p>
<h2 id="the-future-of-development-environments">The future of development environments<a href="#the-future-of-development-environments"></a></h2>
<p>In <a href="https://www.gitpod.io/blog/introducing-gitpod-flex" rel="nofollow noopener noreferrer" target="_blank">Gitpod Flex</a> we carried over the foundational aspects of Kubernetes such as the liberal application of control theory and the declarative APIs whilst simplifying the architecture and improving the security foundation.</p>
<p>We orchestrate development environments using a control plane heavily inspired by Kubernetes. We introduced some necessary abstraction layers that are specific to development environments and cast aside much of the infrastructure complexity that we didn‚Äôt need‚Äîall whilst <a href="https://www.gitpod.io/blog/how-we-built-it-zero-trust-architecture" rel="nofollow noopener noreferrer" target="_blank">putting zero-trust security first</a>.</p>
<p><img src="https://www.gitpod.io/images/blog/we-are-leaving-kubernetes/boundary-diagram.png" alt="Security boundaries of Gitpod Flex" width="1600" height="928" loading="lazy"></p>
<p><strong>Caption:</strong> Security boundaries of Gitpod Flex.</p>
<p>This new architecture allows us to <a href="https://www.gitpod.io/blog/gitpod-supports-development-container" rel="nofollow noopener noreferrer" target="_blank">integrate devcontainer seamlessly</a>. We also unlocked the ability to <a href="https://www.gitpod.io/blog/introducing-gitpod-desktop" rel="nofollow noopener noreferrer" target="_blank">run development environments on your desktop</a>. Now that we‚Äôre no longer carrying the heavy weight of the Kubernetes platform, <strong>Gitpod Flex can be deployed self-hosted in less than three minutes</strong> and in any number of regions, giving more fine-grained control on compliance and added flexibility when modeling organizational boundaries and domains.</p>
<p>We‚Äôll be posting a lot more about Gitpod Flex architecture in the coming weeks or months. I‚Äôd love to invite you on November the 6th to a virtual event where I‚Äôll be giving a demo of Gitpod Flex and I‚Äôll deep-dive into the architecture and security model at length. You can <a href="https://www.gitpod.io/webinars/gitpod-flex-demo" rel="nofollow noopener noreferrer" target="_blank">sign-up here</a>.</p>
<p>When it comes to building a platform for standardized, automated and secure development environments choose a system because it improves your developer experience, eases your operational burden and improves your bottom line. <strong>You are not choosing Kubernetes vs something else, you are choosing a system because it improves the experience for the teams you support.</strong></p>
<blockquote><div><p><strong>Virtual Event: Gitpod Flex - Deploy your self-hosted CDE in 3 minutes</strong></p><p> &gt; <strong>What?</strong> Deep dive into Gitpod Flex architecture and security. <br> &gt; <strong>When?</strong> November 6th. </p></div>
<p><strong><a href="https://www.gitpod.io/webinars/gitpod-flex-demo" rel="nofollow noopener noreferrer" target="_blank">Register Now ‚Üí</a></strong></p></blockquote>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook Building Subsea Cable That Will Encompass the World (179 pts)]]></title>
            <link>https://subseacables.blogspot.com/2024/10/breaking-story-facebook-building-subsea.html</link>
            <guid>42041581</guid>
            <pubDate>Mon, 04 Nov 2024 14:00:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://subseacables.blogspot.com/2024/10/breaking-story-facebook-building-subsea.html">https://subseacables.blogspot.com/2024/10/breaking-story-facebook-building-subsea.html</a>, See on <a href="https://news.ycombinator.com/item?id=42041581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="2" id="Blog1">
<article>
<div>

<h3>
Breaking Story: Facebook Building Subsea Cable That Will Encompass The World
</h3>


<div id="post-body-4952847995282059562">
<p>Several sources have whispered in my ear that META is planning a new 16 fibre pair cable that will encompass the world going from the US East Coast to the US West Coast via the Atlantic, Indian Ocean, and the Pacific. The most ambitious subsea project ever undertaken. I do not know the exact routing. I know that the cable will launch from the American East Coast and will go down the West African Coast to South Africa and then head straight to Mumbai. It is not clear if Europe will be online or not. From Mumbai it will head straight to Australia and then up to the US West Coast. I speculate that there may be branching units to Singapore, Malaysia, and Indonesia.&nbsp;</p><p>This semi-secret cable reflects META's desire for network resiliency given the four month Red Sea down time that AAE-1 and other subsea cables suffered during the first half of 2024. It appears willing to suffer a latency penalty to avoid the Red Sea as well as Egypt's expensive digital toll roads. While Egypt's terrestrial routes for submarine cables are good, they are overpriced and elevate market wavelength pricing. The design also demonstrates India's growing importance. China is off limits to Google, Facebook, and the other American Digital Titans. This makes India and Africa the two key high growth markets for the content providers. I suspect META will use the ride down the West African Coast to fill in digital coverage gaps or deploy branching units like Google's Equiano in case these smaller African states decide to join these subsea fibre optic highways.&nbsp;</p><p>I speculate that the cable goes from Myrtle Beach to Lisbon and then back out to sea to West Arica's coast. It may land in many of the same countries as 2Africa, but will undoubtedly be diverse to the existing terrestrial back haul including new cable landing stations. A good guess is 320 Tbps design throughput.&nbsp;</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhd8lgjVA9VfOQRcWcLIs1cPbI1DAqtBljhCoRXp5z8ba7Pn6SnKQXtFhjmuBeTCt2wxECl_8OnjT9wt5dc3X47E-KwD_EcWn4u0p3Vd2S_TXGLLw3qrWueaedO2_Xw8Os4fEUpA2fmDaL0fih2jXwtCQP4ZUZYGA_AiZghSHyJB2R4ktQEcIrnY5LhFTM/s1200/Submarine-Cable-Map.jpg"><img alt="Map of the World's Fibre Optic Subsea Cables" data-original-height="675" data-original-width="1200" height="360" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhd8lgjVA9VfOQRcWcLIs1cPbI1DAqtBljhCoRXp5z8ba7Pn6SnKQXtFhjmuBeTCt2wxECl_8OnjT9wt5dc3X47E-KwD_EcWn4u0p3Vd2S_TXGLLw3qrWueaedO2_Xw8Os4fEUpA2fmDaL0fih2jXwtCQP4ZUZYGA_AiZghSHyJB2R4ktQEcIrnY5LhFTM/w640-h360/Submarine-Cable-Map.jpg" width="640"></a></p><br>
</div>

</div>


</article>
</div><div data-version="2" id="PopularPosts1">
<h3>
Popular posts from this blog
</h3>
<div role="feed">
<article role="article">
<h3><a href="https://subseacables.blogspot.com/2024/10/facebooks-semi-secret-w-cable.html">Facebook's Semi-Secret W Cable</a></h3>

<div>
<p><a href="https://subseacables.blogspot.com/2024/10/facebooks-semi-secret-w-cable.html">
<img alt="Image" sizes="72px" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgoFJceIAQgcNJYSFMir7Zl4ndZ57MNpwwG2vi_pKhl8MRGf4vUE9WOxLzh-EyMdxqn5EM3jedAJIVjZv_Vx1oyuuTuvMudXmqJihcjBiAdaMbDXfPhKX4nB3hNYMpy4fktFKhVI-8atccmw0LRiMuII9_gQt1aPq7O96t42iiGEQkWQC9nqMdIEzG5MI/w640-h358/W%20Cable.jpg" srcset="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgoFJceIAQgcNJYSFMir7Zl4ndZ57MNpwwG2vi_pKhl8MRGf4vUE9WOxLzh-EyMdxqn5EM3jedAJIVjZv_Vx1oyuuTuvMudXmqJihcjBiAdaMbDXfPhKX4nB3hNYMpy4fktFKhVI-8atccmw0LRiMuII9_gQt1aPq7O96t42iiGEQkWQC9nqMdIEzG5MI/w72-h72-p-k-no-nu/W%20Cable.jpg 72w, https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgoFJceIAQgcNJYSFMir7Zl4ndZ57MNpwwG2vi_pKhl8MRGf4vUE9WOxLzh-EyMdxqn5EM3jedAJIVjZv_Vx1oyuuTuvMudXmqJihcjBiAdaMbDXfPhKX4nB3hNYMpy4fktFKhVI-8atccmw0LRiMuII9_gQt1aPq7O96t42iiGEQkWQC9nqMdIEzG5MI/w144-h144-p-k-no-nu/W%20Cable.jpg 144w">
</a>
</p>
<div>
<p>
&nbsp;A few days I wrote that  Facebook was planning a new subsea cable directly linking the US East  Coast to South Africa, then up to India, and then to Australia, and  finally to the US West Coast. Clearly resiliency is a big theme. It  avoids Lisbon, Egypt, the Red Sea, and Singapore. All choke points due  to their telecom hub status. Note that the map below shows lots of  branching units, but I don't have insider confirmation although they  make perfect sense. The cable is 16 fibre pairs.&nbsp; This  cable may be heavily influenced by AI considerations. AI data centres  require lots of space and power. The US, South Africa, India, and  Australia are good candidates for AI data centres in terms of space. All  of them except for South Africa have modern power grids. But South  Africa is still the best place on the continent for large scale AI  facilities.  An  AI theme makes particular sense given the cable's high latency. An AI  data centre does not directly serve customers. It is a 
</p>
</div>

</div>
</article>
<article role="article">
<h3><a href="https://subseacables.blogspot.com/2024/09/how-to-calculate-iru-for-100g-wavelength.html">How To Calculate An IRU Price For a 100G Wavelength</a></h3>

<div>
<p>
The IRU price should reflect your forecast of future lease revenes. If the current MRC is high and prices have been stable and are expected to remain so, then the opportunity cost of selling an IRU is high.  You are sacrificing a lot of lease revenue if you sell that capacity. In this case the upfront IRU price should equal the three year MRC multipled by 4 to 6 years. So if the three year Faster cable MRC is $17K, then 48x to 72x times that number is fair value. So 48*$17K=$816K. Plus a standard 4% annual fee to cover the wavelength's share of maintenance and operations.. So strong markets and tight capacity mean that leasing is more attractive than IRUs to the supplier. Vice versa, the buyers will be desperate to lower their long term costs via an IRU. In my example, $816K divided by 15 years equates to $4500 per month excluding O&amp;M. Bad for the seller. Good for the buyer. That's why so many of you are looking for IRU capacity to resell to your clients, but finding few se
</p>
</div>
</article>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York Times tech workers go on strike (659 pts)]]></title>
            <link>https://www.nytimes.com/2024/11/04/business/media/new-york-times-strike.html</link>
            <guid>42040795</guid>
            <pubDate>Mon, 04 Nov 2024 12:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/11/04/business/media/new-york-times-strike.html">https://www.nytimes.com/2024/11/04/business/media/new-york-times-strike.html</a>, See on <a href="https://news.ycombinator.com/item?id=42040795">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/11/04/business/media/new-york-times-strike.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Is the Q source the origin of the Gospels? (142 pts)]]></title>
            <link>https://www.thecollector.com/q-source-origin-gospels/</link>
            <guid>42040706</guid>
            <pubDate>Mon, 04 Nov 2024 11:55:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thecollector.com/q-source-origin-gospels/">https://www.thecollector.com/q-source-origin-gospels/</a>, See on <a href="https://news.ycombinator.com/item?id=42040706">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="firstPagePost"><center><img alt="q source origin gospels" fetchpriority="high" width="1200" height="690" decoding="async" data-nimg="1" sizes="(max-width: 992px) 100vw, 66vw" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=480&amp;quality=70 480w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=600&amp;quality=70 600w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=640&amp;quality=70 640w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=750&amp;quality=70 750w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=828&amp;quality=70 828w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=1080&amp;quality=70 1080w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=1200&amp;quality=70 1200w, https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=1400&amp;quality=70 1400w" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/q-source-origin-gospels.jpg?width=1400&amp;quality=70"></center>

<p><b></b>The Q source hypothesis is a prominent theory in New Testament scholarship that seeks to explain the literary relationship between the synoptic Gospels of Matthew, Mark, and Luke. According to this hypothesis, there was a hypothetical written source, known as ‚ÄúQ‚Äù (short for the German word ‚Äú<i>Quelle</i>,‚Äù meaning ‚Äúsource‚Äù), which contained the sayings and teachings of Jesus that were shared by <i>Matthew</i> and <i>Luke</i> but are absent in <i>Mark</i>.</p>

<p><h2><strong>The Synoptic Problem</strong></h2></p>
<figure id="attachment_126327" aria-describedby="caption-attachment-126327"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/synoptic-gospels-word-for-word-q-source.jpg" alt="synoptic gospels word for word q source" width="1236.0515021459228" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/synoptic-gospels-word-for-word-q-source.jpg 1200w, https://cdn.thecollector.com/wp-content/uploads/2024/10/synoptic-gospels-word-for-word-q-source-300x291.jpg 300w, https://cdn.thecollector.com/wp-content/uploads/2024/10/synoptic-gospels-word-for-word-q-source-1024x994.jpg 1024w, https://cdn.thecollector.com/wp-content/uploads/2024/10/synoptic-gospels-word-for-word-q-source-768x746.jpg 768w"></center><figcaption id="caption-attachment-126327">Comparison of Matt 3:7-10 and Luke 3:7-9, the common text is highlighted in red. Source: Wikimedia Commons</figcaption></figure>

<p>Scholars refer to the gospels of <i>Matthew</i>, <i>Mark</i>, and <i>Luke</i> as the synoptic gospels. The term ‚Äúsynoptic‚Äù comes from two Greek words: <i>syn</i>, meaning ‚Äútogether,‚Äù and <i>opsis</i>, meaning ‚Äúseeing.‚Äù It refers to the fact that these three gospels share a similar structure, tell the same stories, and parallel each other in content and wording to a large extent.</p>

<p>Consider the following passages from each of these gospels:</p>

<blockquote>
<h4><i>Matthew</i> 3:16-17</h4>
<h4>‚ÄúAnd when Jesus was baptized, immediately he went up from the water, and behold, the heavens were opened to him, and he saw the Spirit of God descending like a dove and coming to rest on him;&nbsp;and behold, a voice from heaven said, ‚ÄòThis is my beloved Son, with whom I am well pleased.‚Äô‚Äù<i>&nbsp;</i></h4>
<h4><i>Mark</i> 1:10-11</h4>
<h4>‚ÄúAnd when he came up out of the water, immediately he saw the heavens being torn open and the Spirit descending on him like a dove.&nbsp;And a voice came from heaven, ‚ÄòYou are my beloved Son; with you I am well pleased.‚Äô‚Äù</h4>
<h4><i>Luke</i> 3:21-22</h4>
<h4>‚ÄúNow when all the people were baptized, and when Jesus also had been baptized and was praying, the heavens were opened, and the Holy Spirit descended on him in bodily form, like a dove; and a voice came from heaven, ‚ÄòYou are my beloved Son; with you I am well pleased.‚Äô‚Äù</h4>
</blockquote>

<div><div><h3>Get the latest articles delivered to your inbox</h3><p><span> Sign up to our Free Weekly Newsletter</span></p></div><p>The similarity of the texts is undeniable, and this is one of many examples.</p></div>

<p>Scholars have long held that <i>Mark</i> was the first of the <a href="https://www.thecollector.com/exploring-the-four-evangelists-who-wrote-christian-gospels/">gospels</a>, dated to around 65-70 CE. <i>Matthew</i> and <i>Luke</i> are dated at least a decade, if not two later, sometime between 80 and 95 CE. It is highly likely that <i>Matthew </i>and <i>Luke</i> had copies of the <i>Gospel of Mark</i> at their disposal when they authored their versions.</p>

<figure id="attachment_126326" aria-describedby="caption-attachment-126326"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/papyrus-104-q-source.jpg" alt="papyrus 104 q source" width="1517.386722866175" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/papyrus-104-q-source.jpg 1200w, https://cdn.thecollector.com/wp-content/uploads/2024/10/papyrus-104-q-source-300x237.jpg 300w, https://cdn.thecollector.com/wp-content/uploads/2024/10/papyrus-104-q-source-1024x810.jpg 1024w, https://cdn.thecollector.com/wp-content/uploads/2024/10/papyrus-104-q-source-768x607.jpg 768w"></center><figcaption id="caption-attachment-126326">Papyrus 104: Matthew 21,34-37, 1st-century scribe. Source: Wikimedia Commons</figcaption></figure>

<p>Having <i>Mark</i> as a mutual source for <i>Matthew</i> and <i>Luke</i> would explain their many similarities. What it does not explain, however, is the literary similarities between <i>Matthew </i>and <i>Luke </i>that do not appear in <i>Mark</i>. These similarities are attributed to <a href="https://www.thecollector.com/jesus-christ-in-context-rome-jerusalem-judea/">Jesus</a>, but never to any narrative.</p>

<p>The parallels unique to <i>Matthew </i>and <i>Luke</i> gave rise to the theory that these gospel authors made use of a second source for their material, resulting in the equivalencies in their work. Scholars called this hypothetical second source ‚ÄúQ.‚Äù</p>

<h2><strong>The Q Source Hypothesis</strong></h2>
<figure id="attachment_126325" aria-describedby="caption-attachment-126325"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-matthew-and-angel.jpg" alt="saint matthew and angel" width="1655.1724137931035" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-matthew-and-angel.jpg 1200w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-matthew-and-angel-300x218.jpg 300w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-matthew-and-angel-1024x742.jpg 1024w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-matthew-and-angel-768x557.jpg 768w"></center><figcaption id="caption-attachment-126325">Saint Matthew and the Angel, by Giovanni Gerolamo Savoldo, 16th century. Source: MET Museum</figcaption></figure>

<p>The ‚ÄúQ‚Äù in ‚ÄúQ source hypothesis‚Äù comes from the first letter of the German word ‚Äúquelle‚Äù which means ‚Äúsource.‚Äù Given the similarities between <i>Matthew </i>and <i>Luke</i>, this source must have consisted of the sayings of Jesus. The lack of narrative similarities in these two gospels makes it unlikely that Q had a narrative framework.</p>

<p>The Q source hypothesis developed in the late 18th century and early 19th century as a means of explaining the parallels between <i>Matthew</i> and <i>Luke</i>. Two German biblical scholars, particularly Johann Gottfried Eichhorn and Heinrich Julius Holtzmann were prominent in the development of the Q source hypothesis, postulating that a proto-gospel, probably written in Hebrew or Aramaic, was the source for the three synoptic gospels. In the mid-1920s, with the work of B. H. Streeter, the theory seems to have gained traction.</p>

<figure id="attachment_126324" aria-describedby="caption-attachment-126324"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/lion-of-saint-mark.jpg" alt="lion of saint mark" width="1200" height="444" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/lion-of-saint-mark.jpg 1200w, https://cdn.thecollector.com/wp-content/uploads/2024/10/lion-of-saint-mark-300x111.jpg 300w, https://cdn.thecollector.com/wp-content/uploads/2024/10/lion-of-saint-mark-1024x379.jpg 1024w, https://cdn.thecollector.com/wp-content/uploads/2024/10/lion-of-saint-mark-768x284.jpg 768w"></center><figcaption id="caption-attachment-126324">The Lion of Mark, by Vittore Carpaccio, 1416. Source: Venice Museums</figcaption></figure>

<p>This original source may have been an oral tradition on the sayings of Jesus that the authors of <i>Matthew</i> and <i>Luke</i> codified. Alternatively, their capturing of the Q sayings in their gospels could have resulted in the original written Q source becoming redundant as these gospels expanded on and provided context for these sayings.</p>

<p>Various scholars have made attempts to reconstruct Q by analyzing <i>Matthew</i> and <i>Luke</i> and using the parallels to identify the original material. The process involved, among other things, identifying distinctive characteristics such as theological emphasis, literary style, and cultural context. By its very nature, it remains a speculative endeavor. Scholars also regularly investigate parallels with other Early Christian sources such as the <a href="https://www.thecollector.com/what-is-the-didache/"><i>Didache</i></a>, the<i> Gospel of Thomas</i>, and similar works that contain sayings of Jesus.</p>

<p>Though no evidence for Q exists, there are several reasons why such a source is likely. First, there are overlaps in the wording and the order of the sayings and teachings between <i>Matthew</i> and <i>Luke</i>, particularly in passages not found in <i>Mark</i>, which suggest a common origin. This includes a consistent thematic focus on ethical teachings, wisdom sayings, and teachings about the kingdom of God.</p>

<p>Secondly, statistical analysis done by Markos Papapetrou showed quantitative support for the existence of Q by demonstrating statistically significant patterns of verbal agreement between <i>Matthew</i> and <i>Luke</i> in passages not found in <i>Mark</i>.</p>

<p>Thirdly, variations in order and wording make it unlikely that <i>Luke</i> copied from <i>Matthew</i> and more likely that they shared a common source. Each of them adopted the original material to fit their style, narrative, and focus.</p>

<p>Fourth, <i>Matthew</i> and <i>Luke </i>parallel other early Christian works. They may not have used Q as a source directly, but they indicate a broader circulation and influence of Jesus‚Äô teachings beyond the canonical gospels.</p>

<p><h2><strong>Variations and Alternatives of the Q Hypothesis</strong></h2></p>
<figure id="attachment_126331" aria-describedby="caption-attachment-126331"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/three-source-hypothesis-q-source.jpg" alt="three source hypothesis q source" width="1173" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/three-source-hypothesis-q-source.jpg 1173w, https://cdn.thecollector.com/wp-content/uploads/2024/10/three-source-hypothesis-q-source-293x300.jpg 293w, https://cdn.thecollector.com/wp-content/uploads/2024/10/three-source-hypothesis-q-source-1001x1024.jpg 1001w, https://cdn.thecollector.com/wp-content/uploads/2024/10/three-source-hypothesis-q-source-768x786.jpg 768w"></center><figcaption id="caption-attachment-126331">Diagram illustrating the Three Source Hypothesis. Source: Wikimedia Commons</figcaption></figure>

<p>One variation on the Q source hypothesis proposes that <i>Mark </i>also made use of Q or a similar proto-gospel. This is an unorthodox approach since the original hypothesis had Q as a source for similarities between <i>Matthew</i> and <i>Luke </i>that exclude similarities with <i>Mark</i>. The three initial Bible passages referenced in this article show some sayings are shared among all three. This idea of a proto-gospel as the source for all three synoptic gospels dates back as far as the 1800s.</p>

<p>Scholars have come up with alternatives to the Q source hypothesis to explain similarities in the gospels of the different authors. The Farrer hypothesis proposes that<i> Matthew </i>used <i>Mark</i> as a source, but <i>Luke</i> used both <i>Mark</i> and <i>Matthew </i>as a source. This approach is simple and negates the need for a Q source altogether.</p>

<h2><strong>Arguments Against the Existence of Q</strong></h2>
<figure id="attachment_126329" aria-describedby="caption-attachment-126329"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-luke-plaque.jpg" alt="saint luke plaque" width="827" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-luke-plaque.jpg 827w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-luke-plaque-207x300.jpg 207w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-luke-plaque-706x1024.jpg 706w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-luke-plaque-768x1114.jpg 768w"></center><figcaption id="caption-attachment-126329">Plaque with the symbol of Saint Luke, 1100 CE. Source: The MET Museum</figcaption></figure>

<p>Some scholars argue that the similarities between <i>Matthew </i>and<i> Luke</i> can be adequately explained by literary dependence on each other or on oral tradition, without the need to posit a separate written source like Q. Similarly, scribal traditions may also have resulted in literary parallels between <i>Matthew</i> and <i>Luke</i> and would provide a much simpler explanation than the Q hypothesis.</p>

<p>A weakness of the Q source hypothesis is the absence of any textual evidence despite extensive scholarly efforts to find it. The entire hypothesis is based on statistical and literary analysis and inference. It adds complexity to the synoptic problem by introducing an additional layer of tradition, transmission, and composition, which may not be warranted given the available evidence (or rather lack thereof).</p>

<p>The lack of any textual evidence for Q leads to a variety of proposed reconstructions. It results in a lack of consensus which undermines the reliability and validity of the hypothesis.</p>

<p><h2><strong>What Q Lacks</strong></h2></p>
<figure id="attachment_126330" aria-describedby="caption-attachment-126330"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-mark-statue.jpg" alt="saint mark statue" width="635" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-mark-statue.jpg 635w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-mark-statue-159x300.jpg 159w, https://cdn.thecollector.com/wp-content/uploads/2024/10/saint-mark-statue-542x1024.jpg 542w"></center><figcaption id="caption-attachment-126330">Statue of Saint Mark, c.1700, Flemish. Source: The MET Museum</figcaption></figure>

<p>From reconstructions of Q, it is evident that it does not contain the birth, <a href="https://www.thecollector.com/easter-historical-context/">crucifixion</a>, and resurrection narratives that are at the core of the gospels ‚Äî the ‚Äúgood news.‚Äù As such, Q could not be the source of the gospels but could be a source of the sections of the gospels that relate to the sayings of Jesus.</p>

<p>With the abundance of manuscripts and indirect evidence for the <a href="https://www.thecollector.com/what-are-the-earliest-manuscripts-of-the-new-testament/">New Testament</a> and other early <a href="https://www.thecollector.com/how-did-christianity-conquer-an-empire-in-300-years/">Christian</a> works, it is hard to imagine that a source as prominent as Q would leave no trace of evidence. That said, the reasoning behind the likely existence of a single source‚Äîthe parallels in <i>Matthew </i>and <i>Luke</i>‚Äîseems reasonable.</p>

<h2><strong>Conclusion</strong></h2>
<figure id="attachment_126328" aria-describedby="caption-attachment-126328"><center><img decoding="async" src="https://cdn.thecollector.com/wp-content/uploads/2024/10/christ-with-four-eveangelists-q-source.jpg" alt="christ with four eveangelists q source" width="740" height="1200" srcset="https://cdn.thecollector.com/wp-content/uploads/2024/10/christ-with-four-eveangelists-q-source.jpg 740w, https://cdn.thecollector.com/wp-content/uploads/2024/10/christ-with-four-eveangelists-q-source-185x300.jpg 185w, https://cdn.thecollector.com/wp-content/uploads/2024/10/christ-with-four-eveangelists-q-source-631x1024.jpg 631w"></center><figcaption id="caption-attachment-126328">Plaque with Christ in Majesty and the Four Evangelists, 11th Century. Source: The MET Museum</figcaption></figure>

<p>The gospels of <i>Matthew</i>,<i> Mark</i>, and <i>Luke</i> have much in common, resulting in scholars referring to them as the ‚Äúsynoptic gospels.‚Äù&nbsp; While most scholars believe <i>Matthew</i> and <i>Luke </i>used <i>Mark</i> as a source, similarities in <i>Matthew</i> and <i>Luke</i> gave rise to speculation that these two gospels share a second source that is absent in <i>Luke</i>.</p>

<p>Scholars have not been able to verify this hypothesis with manuscript evidence. While the existence of the Q source remains a hypothesis, there were good reasons for positing a single source for the parts of <i>Matthew </i>and <i>Luke</i> that parallel one another but are absent in <i>Mark</i>. From a statistical-analytical perspective, such a source is likely. It would explain the similarities between the two later synoptic gospels and even the parallels with other early Christian sources.</p>

<p>On the other hand, it would also make for a more complex explanation than other scholars have proposed, violating the principle of Occam‚Äôs Razor. Alternatively, <i>Mark </i>could have been the source for <i>Matthew</i>, and <i>Matthew</i> for <i>Luke</i>, which is a much simpler explanation than the Q hypothesis.</p>

<p>The lack of reference to the essentials of the gospel (the birth, death, and resurrection of Christ) makes Q an unlikely source for the origin of the gospels themselves, but it would qualify as a source for the sayings of Jesus in the gospels of <i>Matthew</i> and <i>Mark</i>.</p>
</article></div>]]></description>
        </item>
    </channel>
</rss>