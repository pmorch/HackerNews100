<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 15 Sep 2025 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Hosting a WebSite on a Disposable Vape (304 pts)]]></title>
            <link>https://bogdanthegeek.github.io/blog/projects/vapeserver/</link>
            <guid>45249287</guid>
            <pubDate>Mon, 15 Sep 2025 13:13:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bogdanthegeek.github.io/blog/projects/vapeserver/">https://bogdanthegeek.github.io/blog/projects/vapeserver/</a>, See on <a href="https://news.ycombinator.com/item?id=45249287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="preface">Preface<a href="#preface" arialabel="Anchor">#</a></h2><p>This article is <em>NOT</em> served from a web server running on a disposable vape. If you want to see the real deal, click <a href="http://ewaste.fka.wtf/">here</a>. The content is otherwise identical.</p><h2 id="background">Background<a href="#background" arialabel="Anchor">#</a></h2><p>For a couple of years now, I have been collecting disposable vapes from friends and family. Initially, I only salvaged the batteries for â€œfutureâ€ projects (Itâ€™s not hoarding, I promise), but recently, disposable vapes have gotten more advanced. I wouldnâ€™t want to be the lawyer who one day will have to argue how a device with USB C and a rechargeable battery can be classified as â€œdisposableâ€. Thankfully, I donâ€™t plan on pursuing law anytime soon.</p><p>Last year, I was tearing apart some of these fancier pacifiers for adults when I noticed something that caught my eye, instead of the expected black blob of goo hiding some ASIC (Application Specific Integrated Circuit) I see a little integrated circuit inscribed â€œPUYAâ€.
I donâ€™t blame you if this name doesnâ€™t excite you as much it does me, most people have never heard of them. They are most well known for their flash chips, but I first came across them after reading Jay Carlsonâ€™s blog post about <a href="https://jaycarlson.net/2023/02/04/the-cheapest-flash-microcontroller-you-can-buy-is-actually-an-arm-cortex-m0/">the cheapest flash microcontroller you can buy</a>. They are quite capable little ARM Cortex-M0+ micros.</p><p>Over the past year I have collected quite a few of these PY32 based vapes, all of them from different models of vape from the same manufacturer. Itâ€™s not my place to do free advertising for big tobacco, so I wonâ€™t mention the brand I got it from, but if anyone who worked on designing them reads this, thanks for labeling the debug pins!</p><h2 id="what-are-we-working-with">What are we working with<a href="#what-are-we-working-with" arialabel="Anchor">#</a></h2><p>The chip is marked <code>PUYA C642F15</code>, which wasnâ€™t very helpful. I was pretty sure it was a <code>PY32F002A</code>, but after poking around with <a href="http://pyocd.io/">pyOCD</a>, I noticed that the flash was 24k and we have 3k of RAM. The extra flash meant that it was more likely a <code>PY32F002B</code>, which is actually a very different chip.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>So here are the specs of a microcontroller so <em>bad</em>, itâ€™s basically disposable:</p><ul><li>24MHz Coretex M0+</li><li>24KiB of Flash Storage</li><li>3KiB of Static RAM</li><li>a few peripherals, none of which we will use.</li></ul><p>You may look at those specs and think that itâ€™s not much to work with. I donâ€™t blame you, a 10y old phone can barely load google, and this is about 100x slower. I on the other hand see a <em>blazingly</em> fast web server.</p><h2 id="getting-online">Getting online<a href="#getting-online" arialabel="Anchor">#</a></h2><p>The idea of hosting a web server on a vape didnâ€™t come to me instantly. In fact, I have been playing around with them for a while, but after writing my post on <a href="https://bogdanthegeek.github.io/blog/insights/jlink-rtt-for-the-masses/">semihosting</a>, the penny dropped.</p><p>If you donâ€™t feel like reading that article, semihosting is basically syscalls for embedded ARM microcontrollers. You throw some values/pointers into some registers and call a breakpoint instruction. An attached debugger interprets the values in the registers and performs certain actions. Most people just use this to get some logs printed from the microcontroller, but they are actually bi-directional.</p><p>If you are older than me, you might remember a time before Wi-Fi and Ethernet, the dark ages, when you had to use dial-up modems to get online. You might also know that the ghosts of those modems still linger all around us. Almost all USB serial devices actually emulate those modems: a 56k modem is just 57600 baud serial device. Data between some of these modems was transmitted using a protocol called SLIP (Serial Line Internet Protocol).<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>This may not come as a surprise, but Linux (and with some tweaking even macOS) supports SLIP. The <code>slattach</code> utility can make any <code>/dev/tty*</code> send and receive IP packets. All we have to do is put the data down the wire in the right format and provide a virtual tty.
This is actually easier than you might imagine, pyOCD can forward all semihosting though a telnet port. Then, we use <code>socat</code> to link that port to a virtual tty:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>pyocd gdb -S -O semihost_console_type<span>=</span>telnet -T <span>$(</span>PORT<span>)</span> <span>$(</span>PYOCDFLAGS<span>)</span> &amp;
</span></span><span><span>socat PTY,link<span>=</span><span>$(</span>TTY<span>)</span>,raw,echo<span>=</span><span>0</span> TCP:localhost:<span>$(</span>PORT<span>)</span>,nodelay &amp;
</span></span><span><span>sudo slattach -L -p slip -s <span>115200</span> <span>$(</span>TTY<span>)</span> &amp;
</span></span><span><span>sudo ip addr add 192.168.190.1 peer 192.168.190.2/24 dev sl0
</span></span><span><span>sudo ip link set mtu <span>1500</span> up dev sl0
</span></span></code></pre></div><p>Ok, so we have a â€œmodemâ€, but thatâ€™s hardly a web server. To actually talk TCP/IP, we need an IP stack. There are many choices, but I went with <a href="https://github.com/adamdunkels/uip/tree/uip-0-9">uIP</a> because itâ€™s pretty small, doesnâ€™t require an RTOS, and itâ€™s easy to port to other platforms.
It also, helpfully, comes with a very minimal HTTP server example.</p><p>After porting the SLIP code to use semihosting, I had a working web serverâ€¦half of the time.
As with most highly optimised libraries, uIP was designed for 8 and 16-bit machines, which rarely have memory alignment requirements. On ARM however, if you dereference a <code>u16 *</code>, you better hope that address is even, or youâ€™ll get an exception. The <code>uip_chksum</code> assumed <code>u16</code> alignment, but the script that creates the filesystem didnâ€™t.
I actually decided to modify a bit the structure of the filesystem to make it a bit more portable.
This was my first time working with <code>perl</code> and I have to say, itâ€™s quite well suited to this kind of task.</p><h2 id="blazingly-fast">Blazingly fast<a href="#blazingly-fast" arialabel="Anchor">#</a></h2><p>So how fast is a web server running on a disposable microcontroller. Well, initially, not very fast. Pings took ~1.5s with 50% packet loss and a simple page took over 20s to load. Thatâ€™s so bad, itâ€™s actually funny, and I kind of wanted to leave it there.</p><p>However, the problem was actually between the seat and the steering wheel the whole time. The first implementation read and wrote a single character at a time, which had a massive overhead associated with it. I previously benchmarked semihosting on this device, and I was getting ~20KiB/s, but uIPâ€™s SLIP implementation was designed for very low memory devices, so it was serialising the data byte by byte.
We have a whopping 3kiB of RAM to play with, so I added a ring buffer to cache reads from the host and feed them into the SLIP poll function. I also split writes in batches to allow for escaping.</p><p>Now this is what I call blazingly fast! Pings now take 20ms, no packet loss and a full page loads in about 160ms. This was using using almost all of the RAM, but I could also dial down the sizes of the buffer to have more than enough headroom to run other tasks. The project repo has everything set to a nice balance latency and RAM usage:</p><pre tabindex="0"><code>Memory region         Used Size  Region Size  %age Used
           FLASH:        5116 B        24 KB     20.82%
             RAM:        1380 B         3 KB     44.92%
</code></pre><p>For this blog however, I paid for none of the RAM, so Iâ€™ll use all of the RAM.</p><p>As you may have noticed, we have just under 20kiB (80%) of storage space. That may not be enough to ship all of React, but as you can see, itâ€™s more than enough to host this entire blog post.
And this is not just a static page server, you can run any server-side code you want, if you know C that is.</p><p>Just for fun, I added a json api endpoint to get the number of requests to the main page (since the last crash) and the unique ID of the microcontroller.</p><h2 id="resources">Resources<a href="#resources" arialabel="Anchor">#</a></h2><ul><li><a href="https://github.com/BogdanTheGeek/semihost-ip">Code for this project</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Denmark's Justice Minister calls encrypted messaging a false civil liberty (320 pts)]]></title>
            <link>https://mastodon.social/@chatcontrol/115204439983078498</link>
            <guid>45248802</guid>
            <pubDate>Mon, 15 Sep 2025 12:21:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@chatcontrol/115204439983078498">https://mastodon.social/@chatcontrol/115204439983078498</a>, See on <a href="https://news.ycombinator.com/item?id=45248802">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[RustGPT: A pure-Rust transformer LLM built from scratch (245 pts)]]></title>
            <link>https://github.com/tekaratzas/RustGPT</link>
            <guid>45247890</guid>
            <pubDate>Mon, 15 Sep 2025 09:47:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tekaratzas/RustGPT">https://github.com/tekaratzas/RustGPT</a>, See on <a href="https://news.ycombinator.com/item?id=45247890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ¦€ Rust LLM from Scratch</h2><a id="user-content--rust-llm-from-scratch" aria-label="Permalink: ğŸ¦€ Rust LLM from Scratch" href="#-rust-llm-from-scratch"></a></p>
<details open="">
  <summary>
    
    <span>RustGPT-demo-zoon.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9121201/489286065-ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTc5Mzk3MDEsIm5iZiI6MTc1NzkzOTQwMSwicGF0aCI6Ii85MTIxMjAxLzQ4OTI4NjA2NS1lYzRhNDEwMC1iMDNhLTRiM2MtYTdkNi04MDZlYTU0ZWQ0ZWQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MTVUMTIzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTBkMTUzMjU2MmZlZWY1NmRiMjg2ODE0NWViMWNmMzg0MzgxZjZlNTkwMmE4MDY2ZWU5OWE3ZjUzODI5NDE1NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.pyfB9na19kueQfrB0EcRG6szHLj4NhFLKaSDE7_RP7I" data-canonical-src="https://private-user-images.githubusercontent.com/9121201/489286065-ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTc5Mzk3MDEsIm5iZiI6MTc1NzkzOTQwMSwicGF0aCI6Ii85MTIxMjAxLzQ4OTI4NjA2NS1lYzRhNDEwMC1iMDNhLTRiM2MtYTdkNi04MDZlYTU0ZWQ0ZWQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MTVUMTIzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTBkMTUzMjU2MmZlZWY1NmRiMjg2ODE0NWViMWNmMzg0MzgxZjZlNTkwMmE4MDY2ZWU5OWE3ZjUzODI5NDE1NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.pyfB9na19kueQfrB0EcRG6szHLj4NhFLKaSDE7_RP7I" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">A complete <strong>Large Language Model implementation in pure Rust</strong> with no external ML frameworks. Built from the ground up using only <code>ndarray</code> for matrix operations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸš€ What This Is</h2><a id="user-content--what-this-is" aria-label="Permalink: ğŸš€ What This Is" href="#-what-this-is"></a></p>
<p dir="auto">This project demonstrates how to build a transformer-based language model from scratch in Rust, including:</p>
<ul dir="auto">
<li><strong>Pre-training</strong> on factual text completion</li>
<li><strong>Instruction tuning</strong> for conversational AI</li>
<li><strong>Interactive chat mode</strong> for testing</li>
<li><strong>Full backpropagation</strong> with gradient clipping</li>
<li><strong>Modular architecture</strong> with clean separation of concerns</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ” Key Files to Explore</h2><a id="user-content--key-files-to-explore" aria-label="Permalink: ğŸ” Key Files to Explore" href="#-key-files-to-explore"></a></p>
<p dir="auto">Start with these two core files to understand the implementation:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/tekaratzas/RustGPT/blob/main/src/main.rs"><code>src/main.rs</code></a></strong> - Training pipeline, data preparation, and interactive mode</li>
<li><strong><a href="https://github.com/tekaratzas/RustGPT/blob/main/src/llm.rs"><code>src/llm.rs</code></a></strong> - Core LLM implementation with forward/backward passes and training logic</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ—ï¸ Architecture</h2><a id="user-content-ï¸-architecture" aria-label="Permalink: ğŸ—ï¸ Architecture" href="#ï¸-architecture"></a></p>
<p dir="auto">The model uses a <strong>transformer-based architecture</strong> with the following components:</p>
<div data-snippet-clipboard-copy-content="Input Text â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’ Output Projection â†’ Predictions"><pre><code>Input Text â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’ Output Projection â†’ Predictions
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project Structure</h3><a id="user-content-project-structure" aria-label="Permalink: Project Structure" href="#project-structure"></a></p>
<div data-snippet-clipboard-copy-content="src/
â”œâ”€â”€ main.rs              # ğŸ¯ Training pipeline and interactive mode
â”œâ”€â”€ llm.rs               # ğŸ§  Core LLM implementation and training logic
â”œâ”€â”€ lib.rs               # ğŸ“š Library exports and constants
â”œâ”€â”€ transformer.rs       # ğŸ”„ Transformer block (attention + feed-forward)
â”œâ”€â”€ self_attention.rs    # ğŸ‘€ Multi-head self-attention mechanism  
â”œâ”€â”€ feed_forward.rs      # âš¡ Position-wise feed-forward networks
â”œâ”€â”€ embeddings.rs        # ğŸ“Š Token embedding layer
â”œâ”€â”€ output_projection.rs # ğŸ° Final linear layer for vocabulary predictions
â”œâ”€â”€ vocab.rs            # ğŸ“ Vocabulary management and tokenization
â”œâ”€â”€ layer_norm.rs       # ğŸ§® Layer normalization
â””â”€â”€ adam.rs             # ğŸƒ Adam optimizer implementation

tests/
â”œâ”€â”€ llm_test.rs         # Tests for core LLM functionality
â”œâ”€â”€ transformer_test.rs # Tests for transformer blocks
â”œâ”€â”€ self_attention_test.rs # Tests for attention mechanisms
â”œâ”€â”€ feed_forward_test.rs # Tests for feed-forward layers
â”œâ”€â”€ embeddings_test.rs  # Tests for embedding layers
â”œâ”€â”€ vocab_test.rs       # Tests for vocabulary handling
â”œâ”€â”€ adam_test.rs        # Tests for optimizer
â””â”€â”€ output_projection_test.rs # Tests for output layer"><pre><code>src/
â”œâ”€â”€ main.rs              # ğŸ¯ Training pipeline and interactive mode
â”œâ”€â”€ llm.rs               # ğŸ§  Core LLM implementation and training logic
â”œâ”€â”€ lib.rs               # ğŸ“š Library exports and constants
â”œâ”€â”€ transformer.rs       # ğŸ”„ Transformer block (attention + feed-forward)
â”œâ”€â”€ self_attention.rs    # ğŸ‘€ Multi-head self-attention mechanism  
â”œâ”€â”€ feed_forward.rs      # âš¡ Position-wise feed-forward networks
â”œâ”€â”€ embeddings.rs        # ğŸ“Š Token embedding layer
â”œâ”€â”€ output_projection.rs # ğŸ° Final linear layer for vocabulary predictions
â”œâ”€â”€ vocab.rs            # ğŸ“ Vocabulary management and tokenization
â”œâ”€â”€ layer_norm.rs       # ğŸ§® Layer normalization
â””â”€â”€ adam.rs             # ğŸƒ Adam optimizer implementation

tests/
â”œâ”€â”€ llm_test.rs         # Tests for core LLM functionality
â”œâ”€â”€ transformer_test.rs # Tests for transformer blocks
â”œâ”€â”€ self_attention_test.rs # Tests for attention mechanisms
â”œâ”€â”€ feed_forward_test.rs # Tests for feed-forward layers
â”œâ”€â”€ embeddings_test.rs  # Tests for embedding layers
â”œâ”€â”€ vocab_test.rs       # Tests for vocabulary handling
â”œâ”€â”€ adam_test.rs        # Tests for optimizer
â””â”€â”€ output_projection_test.rs # Tests for output layer
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ§ª What The Model Learns</h2><a id="user-content--what-the-model-learns" aria-label="Permalink: ğŸ§ª What The Model Learns" href="#-what-the-model-learns"></a></p>
<p dir="auto">The implementation includes two training phases:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Pre-training</strong>: Learns basic world knowledge from factual statements</p>
<ul dir="auto">
<li>"The sun rises in the east and sets in the west"</li>
<li>"Water flows downhill due to gravity"</li>
<li>"Mountains are tall and rocky formations"</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Instruction Tuning</strong>: Learns conversational patterns</p>
<ul dir="auto">
<li>"User: How do mountains form? Assistant: Mountains are formed through tectonic forces..."</li>
<li>Handles greetings, explanations, and follow-up questions</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸš€ Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: ğŸš€ Quick Start" href="#-quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone and run
git clone <your-repo>
cd llm
cargo run

# The model will:
# 1. Build vocabulary from training data
# 2. Pre-train on factual statements (100 epochs)  
# 3. Instruction-tune on conversational data (100 epochs)
# 4. Enter interactive mode for testing"><pre><span><span>#</span> Clone and run</span>
git clone <span>&lt;</span>your-repo<span>&gt;</span>
<span>cd</span> llm
cargo run

<span><span>#</span> The model will:</span>
<span><span>#</span> 1. Build vocabulary from training data</span>
<span><span>#</span> 2. Pre-train on factual statements (100 epochs)  </span>
<span><span>#</span> 3. Instruction-tune on conversational data (100 epochs)</span>
<span><span>#</span> 4. Enter interactive mode for testing</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ® Interactive Mode</h2><a id="user-content--interactive-mode" aria-label="Permalink: ğŸ® Interactive Mode" href="#-interactive-mode"></a></p>
<p dir="auto">After training, test the model interactively:</p>
<div data-snippet-clipboard-copy-content="Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne"><pre><code>Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ§® Technical Implementation</h2><a id="user-content--technical-implementation" aria-label="Permalink: ğŸ§® Technical Implementation" href="#-technical-implementation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Configuration</h3><a id="user-content-model-configuration" aria-label="Permalink: Model Configuration" href="#model-configuration"></a></p>
<ul dir="auto">
<li><strong>Vocabulary Size</strong>: Dynamic (built from training data)</li>
<li><strong>Embedding Dimension</strong>: 128</li>
<li><strong>Hidden Dimension</strong>: 256</li>
<li><strong>Max Sequence Length</strong>: 80 tokens</li>
<li><strong>Architecture</strong>: 3 Transformer blocks + embeddings + output projection</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training Details</h3><a id="user-content-training-details" aria-label="Permalink: Training Details" href="#training-details"></a></p>
<ul dir="auto">
<li><strong>Optimizer</strong>: Adam with gradient clipping</li>
<li><strong>Pre-training LR</strong>: 0.0005 (100 epochs)</li>
<li><strong>Instruction Tuning LR</strong>: 0.0001 (100 epochs)</li>
<li><strong>Loss Function</strong>: Cross-entropy loss</li>
<li><strong>Gradient Clipping</strong>: L2 norm capped at 5.0</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Features</h3><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Custom tokenization</strong> with punctuation handling</li>
<li><strong>Greedy decoding</strong> for text generation</li>
<li><strong>Gradient clipping</strong> for training stability</li>
<li><strong>Modular layer system</strong> with clean interfaces</li>
<li><strong>Comprehensive test coverage</strong> for all components</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ”§ Development</h2><a id="user-content--development" aria-label="Permalink: ğŸ”§ Development" href="#-development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run all tests
cargo test

# Test specific components
cargo test --test llm_test
cargo test --test transformer_test
cargo test --test self_attention_test

# Build optimized version
cargo build --release

# Run with verbose output
cargo test -- --nocapture"><pre><span><span>#</span> Run all tests</span>
cargo <span>test</span>

<span><span>#</span> Test specific components</span>
cargo <span>test</span> --test llm_test
cargo <span>test</span> --test transformer_test
cargo <span>test</span> --test self_attention_test

<span><span>#</span> Build optimized version</span>
cargo build --release

<span><span>#</span> Run with verbose output</span>
cargo <span>test</span> -- --nocapture</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ§  Learning Resources</h2><a id="user-content--learning-resources" aria-label="Permalink: ğŸ§  Learning Resources" href="#-learning-resources"></a></p>
<p dir="auto">This implementation demonstrates key ML concepts:</p>
<ul dir="auto">
<li><strong>Transformer architecture</strong> (attention, feed-forward, layer norm)</li>
<li><strong>Backpropagation</strong> through neural networks</li>
<li><strong>Language model training</strong> (pre-training + fine-tuning)</li>
<li><strong>Tokenization</strong> and vocabulary management</li>
<li><strong>Gradient-based optimization</strong> with Adam</li>
</ul>
<p dir="auto">Perfect for understanding how modern LLMs work under the hood!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ“Š Dependencies</h2><a id="user-content--dependencies" aria-label="Permalink: ğŸ“Š Dependencies" href="#-dependencies"></a></p>
<ul dir="auto">
<li><code>ndarray</code> - N-dimensional arrays for matrix operations</li>
<li><code>rand</code> + <code>rand_distr</code> - Random number generation for initialization</li>
</ul>
<p dir="auto">No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">ğŸ¤ Contributing</h2><a id="user-content--contributing" aria-label="Permalink: ğŸ¤ Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! This project is perfect for learning and experimentation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High Priority Features Needed</h3><a id="user-content-high-priority-features-needed" aria-label="Permalink: High Priority Features Needed" href="#high-priority-features-needed"></a></p>
<ul dir="auto">
<li><strong>ğŸª Model Persistence</strong> - Save/load trained parameters to disk (currently all in-memory)</li>
<li><strong>âš¡ Performance optimizations</strong> - SIMD, parallel training, memory efficiency</li>
<li><strong>ğŸ¯ Better sampling</strong> - Beam search, top-k/top-p, temperature scaling</li>
<li><strong>ğŸ“Š Evaluation metrics</strong> - Perplexity, benchmarks, training visualizations</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Areas for Improvement</h3><a id="user-content-areas-for-improvement" aria-label="Permalink: Areas for Improvement" href="#areas-for-improvement"></a></p>
<ul dir="auto">
<li><strong>Advanced architectures</strong> (multi-head attention, positional encoding, RoPE)</li>
<li><strong>Training improvements</strong> (different optimizers, learning rate schedules, regularization)</li>
<li><strong>Data handling</strong> (larger datasets, tokenizer improvements, streaming)</li>
<li><strong>Model analysis</strong> (attention visualization, gradient analysis, interpretability)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Getting Started</h3><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch: <code>git checkout -b feature/model-persistence</code></li>
<li>Make your changes and add tests</li>
<li>Run the test suite: <code>cargo test</code></li>
<li>Submit a pull request with a clear description</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<ul dir="auto">
<li>Follow standard Rust conventions (<code>cargo fmt</code>)</li>
<li>Add comprehensive tests for new features</li>
<li>Update documentation and README as needed</li>
<li>Keep the "from scratch" philosophy - avoid heavy ML dependencies</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ideas for Contributions</h3><a id="user-content-ideas-for-contributions" aria-label="Permalink: Ideas for Contributions" href="#ideas-for-contributions"></a></p>
<ul dir="auto">
<li>ğŸš€ <strong>Beginner</strong>: Model save/load, more training data, config files</li>
<li>ğŸ”¥ <strong>Intermediate</strong>: Beam search, positional encodings, training checkpoints</li>
<li>âš¡ <strong>Advanced</strong>: Multi-head attention, layer parallelization, custom optimizations</li>
</ul>
<p dir="auto">Questions? Open an issue or start a discussion!</p>
<p dir="auto">No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Mac App Flea Market (127 pts)]]></title>
            <link>https://blog.jim-nielsen.com/2025/mac-app-flea-market/</link>
            <guid>45246971</guid>
            <pubDate>Mon, 15 Sep 2025 07:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jim-nielsen.com/2025/mac-app-flea-market/">https://blog.jim-nielsen.com/2025/mac-app-flea-market/</a>, See on <a href="https://news.ycombinator.com/item?id=45246971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Have you ever searched for â€œAI chatâ€ in the Mac App Store?</p><p>I have. Itâ€™s like strolling through one of those counterfeit, replica markets where all the goods <em>look</em> legit at first glance. But then when you look closer, you realize something is off.</p><p>For the query â€œAI chatâ€, there are so many ChatGPT-like app icons the results are comical. Take a look at these:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-1.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-2.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-3.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-4.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-5.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-6.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-7.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-8.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-9.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-10.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-11.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-12.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-13.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-14.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-15.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-16.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-17.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-18.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-19.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-20.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-21.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-22.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-23.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-24.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-25.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-26.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-27.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-28.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-29.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-30.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-31.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-32.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-33.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-34.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-35.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-36.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-37.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-38.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-39.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-40.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-41.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-42.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-43.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-44.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."> <img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-45.png" width="100" height="100" alt="App icon for Appleâ€™s platform that looks like the ChatGPT app."></p><p>The <em>real</em> app icon for the ChatGPT desktop app (from OpenAI) is in that collection above. Can you spot it?</p><p>Here they are again in a single image:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/ai-icons.png" width="820" height="500" alt="A grid of 45 icons. 1 is the real ChatGPT desktop app for Mac from OpenAI. The others are all look-a-likes." data-og-image=""></p><p>(Itâ€™s the one in the 4th row, 3rd column.)</p><p>And those are just black-and-white lookalikes. There are other apps riding the AI/OpenAI wave that look like the <a href="https://apps.apple.com/us/app/ai-bot-al-chatbot-assistant/id6736975220?mt=12">ChatGPT</a> <a href="https://apps.apple.com/us/app/ai-chatbot-ask-ai-chat-bot/id6450622517">logo</a> <a href="https://apps.apple.com/us/app/chatbot-for-google-gemini/id6740811927">just</a> <a href="https://apps.apple.com/us/app/ai-chat-bot-assistant-ask-ai/id6450978033">in</a> <a href="https://apps.apple.com/us/app/videogen-ai-video-generator/id6479643532?mt=12">different</a> <a href="https://apps.apple.com/us/app/chatbot-ai-chat-assistant/id6739469256?mt=12">colors</a>.</p><p>The funny thing is: the official ChatGPT desktop app from OpenAI is not even in the Mac App Store. Itâ€™s only <a href="https://chatgpt.com/features/desktop/">available from their website</a>, so it wonâ€™t show up in the â€œAI chatâ€ results.</p><p>There were lots of other â€œsort of looks like the official one but isnâ€™tâ€ app icons in my search results, like <a href="https://apps.apple.com/us/app/enclave-local-ai-assistant/id6476614556">this Claude one</a>, <a href="https://apps.apple.com/us/app/grow-ai-ask-ai-chat-assistant/id6741433564">this Grok one</a>, or <a href="https://apps.apple.com/us/app/g-ai-4-0-ai-chatbot-assistant/id6749970691?mt=12">this Gemini one</a>.</p><p>Oh, and these appsâ€™ names were fascinating to look at. They were basically every spacing and casing combination of â€œAIâ€, â€œChatâ€, and â€œBotâ€ you can image. Just look at this sampling:</p><ul><li><a href="https://apps.apple.com/us/app/ai-chat-bot-ask-assistant/id6746353576?mt=12">AI Chat Bot : Ask Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-chat-ask-assistant/id6747115817?mt=12">AI Chatbot: Chat Ask Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-chat-ai-assistant/id6744754737?mt=12">AI Chatbot : Chat AI Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-assistant-ai/id6745822696?mt=12">AI Chatbot : Ask Assistant AI</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-open-ask-chat-bot/id6743402169?mt=12">AI Chatbotâ€”Open &amp; Ask Chat Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-chat-assistant/id6743059258?mt=12">AI ChatBot ASK Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-assistant-ask-ai/id6744464743?mt=12">AI Chatbot Assistant &amp; Ask AI</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-open-assistant/id6449541207">Ai Chatbot :Ask Open Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-genius-question-ai/id6447461948?mt=12">AI Chatbot :Genius Question AI</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-seek-assistant/id6474563192?mt=12">AI Chatbot-Ask Seek Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-anything-bot/id6480046854?mt=12">AI ChatBot - Ask Anything Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-chat-assistant/id6473283294?mt=12">AI Chatbot, Ask Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-bot-ai-bot-assistant/id6447077370?mt=12">AI Chat Bot - AI Bot Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ai-chat-assistant-5/id6741179870?mt=12">AI Chatbotãƒ»AI Chat Assistant 5</a></li><li><a href="https://apps.apple.com/us/app/al-chatbot-ai-assistant-chat/id6738339696?mt=12">Al Chatbot - AI Assistant Chat</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-ai-assistant/id6503169843?mt=12">AI Chatbot : Ask AI Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-ai-chat-bot/id6450622517">AI Chatbot : Ask AI Chat Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-chat-ai-assistant/id6450061224?mt=12">AI Chatbot â€¢ Chat AI Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-chat-assistant/id6737203981?mt=12">AI ChatBot- Ask Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-bot-ask-assistant/id6447312365?mt=12">AI Chat Bot - Ask Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-ask-gpt-assistant/id6502455067?mt=12">AI Chatbot: Ask GPT Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ai-ask-assistant/id6741739345?mt=12">Chatbot AI : Ask Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-open-ask-ai-chat-bot/id6743622123?mt=12">Chatbot: Open Ask AI Chat Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-assistant-ask-bot/id6475490427?mt=12">AI Chatbot Assistant: Ask Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-chatbot-ask-anything/id6743649986?mt=12">AI Chat - Chatbot Ask Anything</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-smart-ai-assistant/id6449362725">AI Chat: Smart AI Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ask-ai-assistant-bot/id6743996207?mt=12">Chatbot: Ask AI Assistant Bot</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ai-chat-ai-assistant/id6736944168?mt=12">Chatbot AI Chat - AI Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ai-chat-assistant/id6503641047?mt=12">ChatBot : AI Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-chat-ask-ai-assistant/id6740460020?mt=12">ChatBot&amp;Chat Ask Ai Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ask-open-assistant-ai/id6742192140?mt=12">Chatbot: Ask Open Assistant AI</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ask-character-ai-chat/id6479561834?mt=12">Chatbot: Ask Character AI Chat</a></li><li><a href="https://apps.apple.com/us/app/ai-chatbot-assistant-ask-ai/id6739805237?mt=12">AI Chatbot Assistant â€¢ Ask AI</a></li><li><a href="https://apps.apple.com/us/app/ask-ai-chatbot-chat-assistant/id6743440054?mt=12">Ask AI Chatbot: Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-chatbot-assistant-4o/id6447619562?mt=12">AI Chat - Chatbot Assistant 4o</a></li><li><a href="https://apps.apple.com/us/app/ai-bot-al-chatbot-assistant/id6736975220?mt=12">AI Bot: Al ChatBot &amp; Assistant</a></li><li><a href="https://apps.apple.com/us/app/chatbot-open-chat-with-ai/id6742242734?mt=12">Chatbot: Open Chat with AI</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ask-ai-chat-bot/id6451158502?mt=12">Chatbot: Ask AI Chat Bot</a></li><li><a href="https://apps.apple.com/us/app/ai-chat-assistant-chatnow/id1667518571">AI Chat Assistant â€“ ChatNow</a></li><li><a href="https://apps.apple.com/us/app/chatbot-open-chat-with-ai-bot/id6448729972?mt=12">Chatbot: Open Chat with AI Bot</a></li><li><a href="https://apps.apple.com/us/app/chatbot-ai-chat-assistant/id6740486822?mt=12">Chatbot AI - Chat Assistant</a></li><li><a href="https://apps.apple.com/us/app/open-chat-ai-chatbot-assistant/id6746927378?mt=12">Open Chat Ai Chatbot Assistant</a></li></ul><p>I mean, look at this one: they named it <a href="https://apps.apple.com/us/app/al-chatbot-ai-assistant-chat/id6738339696?mt=12">â€œAl Chatbotâ€</a> (that's the letter <code>l</code> as in â€œlimaâ€, you can see it better in the URL slug where the letters are lowercase: <code>al-chatbot</code>).</p><p>Imagine going to store to grab some Nike gear and you find stuff like this (image courtesy of <a href="https://www.reddit.com/r/crappyoffbrands/comments/apq8sx/nine_out_of_the_hundreds_of_fake_nike_brands_from/">this post on Reddit</a>):</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/ai-icon-nike.jpg" width="682" height="473" alt="A bunch of â€œNikeâ€ logo knock-offs that look like a swoosh but say â€œHikeâ€ or â€œMikeâ€ or â€œNAIKâ€."></p><p>What does that say about the store youâ€™re visiting?</p><p>I always wanted a pair of Mike Jordans, just like I always wanted ChatGPP for my Mac.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Folks, we have the best Ï€ (216 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/folks-we-have-the-best</link>
            <guid>45246953</guid>
            <pubDate>Mon, 15 Sep 2025 07:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/folks-we-have-the-best">https://lcamtuf.substack.com/p/folks-we-have-the-best</a>, See on <a href="https://news.ycombinator.com/item?id=45246953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>In the past couple of months, I published a number of articles on recreational math. I did my best to keep them accessible and fun, but my goal was usually to shed light at deeper mathematical truths. For example, the discussion of </span><a href="https://lcamtuf.substack.com/p/09999-1" rel="">0.999â€¦ = 1</a><span> served as a springboard to highlight some of the subtler properties of real numbers and the different meanings of infinity.</span></p><p><span>Today, I have no agenda. This article exists because I discovered a somewhat obscure paper that says something unexpected and cool. It </span><em>feels</em><span> profound, it probably isnâ€™tâ€¦ and if you keep reading, itâ€™s going to live rent-free in your head too.</span></p><p>Topologists are an odd bunch: they study the continuity of geometric shapes with no regard for appearances. To them, continuous transformations â€” such as stretching and squeezing â€” are of no consequence. A donut and a drinking straw are the same because you can knead one into another without making or mending any holes.</p><p><span>Fundamentally, a topologist doesnâ€™t care about the distance between two points in space: all that matters is a more narrow concept of local continuity. Because of this, the practitioners often choose to lean on stripped-down geometrical spaces in which the notion of distance â€” also known as a </span><em>metric </em><span>â€” is simply not defined.</span></p><p><span>That said, if youâ€™re not a topologist, you probably enjoy being able to measure stuff. In standard Euclidean geometry with two dimensions, if we have two points that are separated by </span><em>x</em><span> horizontally and </span><em>y</em><span> vertically, the resulting straight-line distance can be calculated as:</span></p><p>One of the most common ways to construct non-Euclidean spaces is to alter the spaceâ€™s metric in some way. An example that should be familiar to many software engineers is the taxicab metric, also known as the Manhattan distance. Itâ€™s named so by analogy to a cab navigating a rectangular grid of streets, charging you per mile traveled:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Drc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Drc!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 424w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 848w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 1272w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_Drc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png" width="1456" height="764" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:263113,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Drc!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 424w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 848w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 1272w, https://substackcdn.com/image/fetch/$s_!_Drc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ada872d-41dd-495e-917a-2dfdf67d4859_2332x1224.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>An illustration of the taxicab distance.</em></figcaption></figure></div><p><span>In the taxicab universe, the distance between two points is a simple sum of the absolute distances in each axis: </span><em>d</em><sub>taxicab</sub><em> = |x| + |y|.</em><span> If we take another look at the earlier case â€” </span><em>d = âˆš(xÂ² + yÂ²) â€” </em><span>itâ€™s tempting to express the cab equation in an analogous way:</span></p><div data-component-name="Latex"><p><span>\(d_\textrm{taxicab} = \sqrt[1 \ ]{\lvert x \rvert^1 + \lvert y \rvert^1}\)</span></p></div><p>If we look at these two formulas â€” Euclidean and taxicab â€” itâ€™s clear that we can generalize this to a whole family of related metric spaces:</p><div data-component-name="Latex"><p><span>\(d_n = \sqrt[n \ ]{\lvert x \rvert^n + \lvert y \rvert^n}\)</span></p></div><p><span>Again, the case of </span><em>n</em><span> = 1 is the taxicab universe; </span><em>n = 2</em><span> nets us the standard Euclidean space. There are some unnamed geometries in between, but if </span><em>n </em><span>approaches infinity, we get whatâ€™s called the Chebyshev distance. In this case, even the tiniest difference between </span><em>x </em><span>and </span><em>y</em><span> makes one of the exponentiated values vastly smaller than the other one, so the distance is just:</span></p><div data-component-name="Latex"><p><span>\(d_\infty = max( \lvert x \rvert, \lvert y \rvert)\)</span></p></div><p><span>In each of these spaces â€” from </span><em><span>d</span><sub>1 </sub></em><span>to </span><em><span>d</span><sub>âˆ </sub></em><span>â€” there is a well-defined notion of a â€œcircleâ€: itâ€™s a set of points that are equidistant under the chosen metric from some center point.</span></p><p><span>To understand how these circles might look like, we can note that if </span><em>y </em><span>is zero, the earlier distance formula always simplifies to just </span><em><span>d</span><sub>n </sub></em><span>= </span><em>|x|</em><span>:</span></p><div data-component-name="Latex"><p><span>\(d_n = \sqrt[n \ ]{\lvert x \rvert^n + \lvert y \rvert^n} = \sqrt[n \ ]{\lvert x \rvert^n} = \lvert x \rvert\)</span></p></div><p><span>Similarly, if </span><em>x </em><span>is zero, the formula is just </span><em><span>d</span><sub>n </sub></em><span>= </span><em>|y|</em><span>. This means that in any of our spaces, if weâ€™re trying to draw an </span><em>n-</em><span>circle with a radius </span><em>r =</em><span> 1, the points at (1, 0), (0, -1), (-1, 0), and (0, 1) will be a part of the shape:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!G9oz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!G9oz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 424w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 848w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 1272w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!G9oz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png" width="1456" height="893" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:893,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:110842,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!G9oz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 424w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 848w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 1272w, https://substackcdn.com/image/fetch/$s_!G9oz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86618e24-bdee-4bda-9191-0ad43228bc00_1849x1134.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>These points are always r = 1 away from (0, 0).</em></figcaption></figure></div><p><span>For the taxicab metric (</span><em><span>d</span><sub>1</sub></em><span>), the distance from (0, 0) is a simple sum of </span><em>x </em><span>and </span><em>y </em><span>coordinates, so a circle of radius </span><em>r</em><span> is just a collection of points that satisfy the criteria </span><em>|x| + |y| = r</em><span>. This includes (0.9, 0.1), (0.8, 0.2), (0.7, 0.3), and so on. In effect, each of the four quadrants of the circle is just a Â±45Â° diagonal:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fR_z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fR_z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:79619,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!fR_z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The taxicab circle (1-circle).</em></figcaption></figure></div><p><span>For higher-order </span><em>n</em><span>-circles, the constraint for </span><em>(x, y)</em><span> points that make up the â€œcircleâ€ is:</span></p><div data-component-name="Latex"><p><span>\(\sqrt[n]{\lvert x \rvert ^n + \lvert y \rvert ^n} = r\)</span></p></div><p><span>The following illustration shows examples calculated for </span><em>n = </em><span>1.5, 2, 3, and 5:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!068u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!068u!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!068u!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!068u!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!068u!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!068u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:165645,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!068u!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!068u!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!068u!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!068u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dac30ff-ddaa-45a1-a157-f72e32ec2cce_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Several n-circles.</em></figcaption></figure></div><p><span>Earlier on, we also mentioned that as </span><em>n </em><span>approaches infinity, we get whatâ€™s known as the Chebyshev distance, </span><em><span>d</span><sub>âˆ</sub><span> =</span></em><span> </span><em>max(|x|, |y|)</em><span>. This âˆ-circle is a collection of all points where one coordinate is equal to Â±</span><em>r, </em><span>while the other one stays within </span><em>[-r, r]:</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!26CQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!26CQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!26CQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33756,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!26CQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!26CQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5b4b592-759b-4007-a669-cf0e71e9382a_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The Chebyshev circle.</em></figcaption></figure></div><p><span>Right. This brings us to an interesting question: does the value of </span><em>Ï€</em><span> â€” that is, the ratio of circumference to diameter of a circle â€” change for these â€œcirclesâ€ drawn in these non-standard spaces?</span></p><p>Letâ€™s have another look at the taxicab scenario:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fR_z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fR_z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:79619,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!fR_z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!fR_z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c23839-1786-4cc5-b3c0-c7d4dc001c83_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The taxicab circle again.</em></figcaption></figure></div><p><span>Whatâ€™s the circumference of that shape? Itâ€™s tempting to say that each quadrant is just the diagonal of a 1Ã—1 square (= âˆš2), so the circumference is 4Â·âˆš2. The diameter of the circle is </span><em>2</em><span>Â·</span><em>r</em><span> =</span><em> </em><span>2. Hence, the value of the taxicab </span><em>Ï€</em><span> (</span><em><span>Ï€</span><sub>1</sub></em><span>) must be 4Â·âˆš2/2 â‰ˆ 2.828.</span></p><p><span>Exceptâ€¦ thatâ€™s a bit of a category error. Weâ€™re attempting to measure the circumference of a circle drawn in a non-Euclidean space by using the Euclidean definition of length. In the taxicab space, the diagonal of a 1Ã—1 square is not âˆš2; itâ€™s a simple sum of the edges of the square. So, if weâ€™re to be consistent, the correct answer is that </span><em><span>Ï€</span><sub>1</sub></em><span> = 4Â·2/2 = 4.</span></p><p><span>Curiously, the âˆ-circle case nets the same result: the each quadrant consists of two straight segments of length 1, so the result is </span><em><span>Ï€</span><sub>âˆ </sub></em><span>= 8Â·1/2 = 4.</span></p><p><span>For other </span><em>n</em><span>-circles, exact answers require some non-trivial calculus, but we can approximate the value numerically. To do this, we solve the circle equation at some constant angular intervals and then connect the dots, yielding a shape constructed out of straight segments. </span></p><p><span>For example, for the familiar case of </span><em>n = 2</em><span>, we get:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WqH3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WqH3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WqH3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:44510,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WqH3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!WqH3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a46ddc-03b0-44f1-9ba3-3eae9365d533_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>We can calculate the length of these segments using the appropriate metric, then divide it by the diameter (always 2). With the crude 20-segment approximation shown above, we get </span><em><span>Ï€</span><sub>2 </sub></em><span>â‰ˆ 3.129. A more precise model with a thousand segments nets us </span><em><span>Ï€</span><sub>2 </sub></em><span>â‰ˆ 3.14159.</span></p><p>Several other computed values are shown below:</p><div data-component-name="Latex"><p><span>\(\begin{array}{| l | l |}
\hline
\mathbf{n =} &amp; \mathbf{\pi \approx} \\
\hline
1 &amp; 4 \textrm{ (exact)} \\
\hline
1.5 &amp; 3.26 \\
\hline
2 &amp; 3.14 \leftarrow \textrm{(you are here)}\\
\hline
3 &amp; 3.26 \\
\hline
5 &amp; 3.50 \\
\hline
\infty &amp; 4 \textrm{ (exact)}  \\
\hline

\end{array}\)</span></p></div><p><span>And hereâ€™s a plot of </span><em><span>Ï€</span><sub>n </sub></em><span>showing even more data points:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vqYh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vqYh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!vqYh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:46256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vqYh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!vqYh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcadc2b5f-85f6-4c65-8d4f-3aeffd191e0f_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>The value of Ï€</span><sub>n </sub><span>(vertical scale) in function of n (horizontal scale).</span></em></figcaption></figure></div><p><span>So, thatâ€™s the somewhat unexpected revelation: of all the metric spaces constructed through simple extrapolation from Euclidean geometry, our </span><em>Ï€ </em><span>is the â€œminimalâ€ </span><em>Ï€</em><span>. We live in some sort of a </span><em>Ï€</em><span>-dip.</span></p><p><em><span>ğŸ‘‰ Credit: in their 2000 paper, mathematicians Charles Adler and James Tanton provide a </span><a href="http://lcamtuf.coredump.cx/blog/adler.pdf" rel="">formal proof of this property</a><span>, and this is where the idea for this blog post comes from.</span></em></p><p>We can do that too! Positive exponents less than 1 produce concave â€œcirclesâ€:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xW8j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xW8j!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xW8j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:94410,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://lcamtuf.substack.com/i/173625523?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xW8j!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 424w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 848w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 1272w, https://substackcdn.com/image/fetch/$s_!xW8j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac4cd31-9dbf-448c-8e21-d499fb29c59b_2500x1875.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>N-circles for n = 0.8 (blue), 0.5 (red), and 0.3 (yellow).</em></figcaption></figure></div><p><span>The measurements corresponding to the image are </span><em><span>Ï€</span><sub>0.8  </sub></em><span>â‰ˆ 4.7, </span><em><span>Ï€</span><sub>0.5  </sub></em><span>â‰ˆ 7.2, </span><em><span>Ï€</span><sub>0.3  </sub></em><span>â‰ˆ 11.9, so the trend continues. And at </span><em>n</em><span> = 0, our earlier notion of distance breaks down.</span></p><p><em>If you liked the content, please subscribe; thereâ€™s no better way to stay in touch with the writers you like.</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Celestia â€“ Real-time 3D visualization of space (114 pts)]]></title>
            <link>https://celestiaproject.space/</link>
            <guid>45246403</guid>
            <pubDate>Mon, 15 Sep 2025 05:30:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://celestiaproject.space/">https://celestiaproject.space/</a>, See on <a href="https://news.ycombinator.com/item?id=45246403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper">

			<!-- Header -->
				

			<!-- Banner -->
				

			<!-- One -->
				<section id="one">
					<span><img src="https://celestiaproject.space/images/pic01.jpg" alt=""></span>
					<div>
								<p><b>Unlike most planetarium software</b>, Celestia doesn't confine you to the surface of the Earth. You can travel throughout the solar system, to any of over 100,000 stars, or even beyond the galaxy.</p>
								<p><b>All movement in Celestia is seamless</b>. The exponential zoom feature lets you explore space across a huge range of scales, from galaxy clusters down to spacecraft only a few meters across. A 'point-and-goto' interface makes it simple to navigate through the universe to the object you want to visit.</p>
								<p><b>Celestia is expandable</b>. Celestia comes with a large catalog of stars, galaxies, planets, moons, asteroids, comets, and spacecraft. If that's not enough, you can download dozens of easy to install add-ons with more objects.</p>
							</div>
					<a href="#two">Next</a>
				</section>

			<!-- Two -->
				<section id="two">
					<span><img src="https://celestiaproject.space/images/pic02.jpg" alt=""></span>
					<div>
						<header>
							<h2>3D Space Simulator</h2>
							<p>Celestia lets you explore our universe in three dimensions.</p>
						</header>
						<p>Celestia simulates many different types of celestial objects. From planets and moons to star clusters and galaxies, you can visit every object in the expandable database and view it from any point in space and time. The position and movement of solar system objects is calculated accurately in real time at any rate desired.</p>
					</div>
					<a href="#three">Next</a>
				</section>

			<!-- Three -->
				<section id="three">
					<span><img src="https://celestiaproject.space/images/pic03.jpg" alt=""></span>
					<div>
						<header>
							<h2>Interactive Planetarium</h2>
							<p>Celestia serves as a planetarium â€“ for an observer on any celestial object.</p>
						</header>
						<p>You can easily navigate to any world and land on its surface. When used as a planetarium, Celestia shows accurate positions of solar system objects in the sky. You can switch labels and other supporting features on and off with hotkeys, or zoom in and out on an object of interest, for example Jupiterâ€™s system of moons.</p>
					</div>
					<a href="#four">Next</a>
				</section>

			<!-- Four -->
				<section id="four">
					<span><img src="https://celestiaproject.space/images/pic04.jpg" alt=""></span>
					<div>
						<header>
							<h2>Expandable Content</h2>
							<p>Customize Celestia according to your needs.</p>
						</header>
						<p>Celestiaâ€™s catalogues can be easily expanded. There are many different add-ons available containing new objects like comets or stars, high-resolution textures of Earth and other well mapped solar system bodies, as well as 3D models for asteroids and spacecraft on precise trajectories. Even fictional objects from well-known sci-fi franchises can be found.</p>
					</div>
					<a href="#five">Next</a>
				</section>
				
			<!-- Five -->
				<section id="five">
					<span><img src="https://celestiaproject.space/images/pic05.jpg" alt=""></span>
					<div>
						<header>
							<h2>Create your own worlds</h2>
							<p>Celestia allows you to add your own content.</p>
						</header>
						<p>If you donâ€™t find a particular celestial object in the catalogues, you can simply create it yourself. Construct whole planetary systems, nebulae, galaxies or fictional objects. Celestia provides a unique opportunity to interactively demonstrate your creations.</p>
					</div>
					<a href="#six">Next</a>
				</section>
				
			<!-- Six -->
				<div id="six">
						<header>
							<h2>Other Celestia features</h2>
							<p>These are just some of the many features of Celestia</p>
						</header>
						<div>
								<section>
									<span></span>
									<h3>Virtual Textures</h3>
									<p>Virtual Textures can be used to display extremely high-resolution textures or close-up features on planetary surfaces. Celestia only loads the tiles it needs to display, increasing performance.</p>
								</section>
								<section>
									<span></span>
									<h3>Audio playing</h3>
									<p>Play background music in CEL/CELX scripts in order to achieve greater effect. Available only for Celestia 1.7.0.</p>
								</section>
								<section>
									<span></span>
									<h3>Trajectories</h3>
									<p>Celestia supports different types of trajectory data. Sampled Orbits for example can be used for spacecraft paths, or you can use NASAâ€™s SPICE kernels for various solar system objects.</p>
								</section>
							</div>
					</div>

			<!-- Footer -->
				

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A qualitative analysis of pig-butchering scams (140 pts)]]></title>
            <link>https://arxiv.org/abs/2503.20821</link>
            <guid>45245962</guid>
            <pubDate>Mon, 15 Sep 2025 03:58:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2503.20821">https://arxiv.org/abs/2503.20821</a>, See on <a href="https://news.ycombinator.com/item?id=45245962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2503.20821">View PDF</a>
    <a href="https://arxiv.org/html/2503.20821v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Pig-butchering scams have emerged as a complex form of fraud that combines elements of romance, investment fraud, and advanced social engineering tactics to systematically exploit victims. In this paper, we present the first qualitative analysis of pig-butchering scams, informed by in-depth semi-structured interviews with $N=26$ victims. We capture nuanced, first-hand accounts from victims, providing insight into the lifecycle of pig-butchering scams and the complex emotional and financial manipulation involved. We systematically analyze each phase of the scam, revealing that perpetrators employ tactics such as staged trust-building, fraudulent financial platforms, fabricated investment returns, and repeated high-pressure tactics, all designed to exploit victims' trust and financial resources over extended periods. Our findings reveal an organized scam lifecycle characterized by emotional manipulation, staged financial exploitation, and persistent re-engagement efforts that amplify victim losses. We also find complex psychological and financial impacts on victims, including heightened vulnerability to secondary scams. Finally, we propose actionable intervention points for social media and financial platforms to curb the prevalence of these scams and highlight the need for non-stigmatizing terminology to encourage victims to report and seek assistance.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Rajvardhan Oak [<a href="https://arxiv.org/show-email/66f7f1d7/2503.20821" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2503.20821v1" rel="nofollow">[v1]</a></strong>
        Tue, 25 Mar 2025 23:15:48 UTC (342 KB)<br>
    <strong>[v2]</strong>
        Sat, 24 May 2025 07:36:41 UTC (343 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Language Models Pack Billions of Concepts into 12k Dimensions (288 pts)]]></title>
            <link>https://nickyoder.com/johnson-lindenstrauss/</link>
            <guid>45245948</guid>
            <pubDate>Mon, 15 Sep 2025 03:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nickyoder.com/johnson-lindenstrauss/">https://nickyoder.com/johnson-lindenstrauss/</a>, See on <a href="https://news.ycombinator.com/item?id=45245948">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>In a recent <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;ref=nickyoder.com" rel="noreferrer">3Blue1Brown video series</a> on transformer models, Grant Sanderson posed a fascinating question: How can a relatively modest embedding space of 12,288 dimensions (GPT-3) accommodate millions of distinct real-world concepts? </p><p>The answer lies at the intersection of high-dimensional geometry and a remarkable mathematical result known as the <a href="https://stanford.edu/class/cs114/readings/JL-Johnson.pdf?ref=nickyoder.com" rel="noreferrer">Johnson-Lindenstrauss lemma</a>. While exploring this question, I discovered something unexpected that led to an interesting collaboration with Grant and a deeper understanding of vector space geometry. </p><p>The key insight begins with a simple observation: while an N-dimensional space can only hold N perfectly orthogonal vectors, relaxing this constraint to allow for "quasi-orthogonal" relationships (vectors at angles of, say, 85-95 degrees) dramatically increases the space's capacity. This property is crucial for understanding how language models can efficiently encode semantic meaning in relatively compact embedding spaces.</p><p>In Grant's video, he demonstrated this principle with an experiment attempting to fit 10,000 unit vectors into a 100-dimensional space while maintaining near-orthogonal relationships. The visualization suggested success, showing angles clustered between 89-91 degrees. However, when I implemented the code myself, I noticed something interesting about the optimization process.</p><p>The original loss function was elegantly simple: </p><div><p>               loss = (dot_products.abs()).relu().sum()</p><p>While this loss function appears perfect for an unbounded â„á´º space, it encounters two subtle but critical issues when applied to vectors constrained to a high-dimensional unit sphere:</p></div><ol><li><strong>The Gradient Trap</strong>: The dot product between vectors is the cosine of the angle between them, and the gradient is the sine of this angle. This creates a perverse incentive structure: when vectors approach the desired 90-degree relationship, the gradient (sin(90Â°) = 1.0) strongly pushes toward improvement. However, when vectors drift far from the goal (near 0Â° or 180Â°), the gradient (sin(0Â°) â‰ˆ 0) vanishesâ€”effectively trapping these badly aligned vectors in their poor configuration.</li><li><strong>The 99% Solution</strong>: The optimizer discovered a statistically favorable but geometrically perverse solution. For each vector, it would be properly orthogonal to 9,900 out of 9,999 other vectors while being nearly parallel to just 99. This configuration, while clearly not the intended outcome, actually represented a global minimum for the loss functionâ€”mathematically similar to taking 100 orthogonal basis vectors and replicating each one roughly 100 times.</li></ol><figure><img src="https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-12.33.59-PM.png" alt="" loading="lazy" width="1169" height="624" srcset="https://nickyoder.com/content/images/size/w600/2025/02/Screenshot-2025-02-20-at-12.33.59-PM.png 600w, https://nickyoder.com/content/images/size/w1000/2025/02/Screenshot-2025-02-20-at-12.33.59-PM.png 1000w, https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-12.33.59-PM.png 1169w"></figure><p>This stable configuration was particularly insidious because it satisfied 99% of the constraints while being fundamentally different from the desired constellation of evenly spaced, quasi-orthogonal vectors. To address this, I modified the loss function to use an exponential penalty that increases aggressively as dot products grow:</p><p>               loss = exp(20*dot_products.abs()**2).sum()         (<a href="https://github.com/NickYoder86/Johnson-Lindenstrauss?ref=nickyoder.com" rel="noreferrer">Full code here</a>)</p><p>This change produced the desired behavior, though with a revealing result: the maximum achievable pairwise angle was around 76.5 degrees, not 89 degrees. </p><p>This discovery led me down a fascinating path exploring the fundamental limits of vector packing in high-dimensional spaces, and how these limits relate to the Johnson-Lindenstrauss lemma.</p><p>When I shared these findings with Grant, his response exemplified the collaborative spirit that makes the mathematics community so rewarding. He not only appreciated the technical correction but invited me to share these insights with the 3Blue1Brown audience. This article is that response, expanded to explore the broader implications of these geometric properties for machine learning and dimensionality reduction.</p><h2 id="the-johnson-lindenstrauss-lemma-a-geometric-guarantee">The Johnson-Lindenstrauss Lemma: A Geometric Guarantee</h2><p>At its core, the Johnson-Lindenstrauss (JL) lemma makes a remarkable promise: you can project points from an arbitrarily high-dimensional space into a surprisingly low-dimensional space while preserving their relative distances with high probability. What makes this result particularly striking is that the required dimensionality of the low-dimensional space grows only logarithmically with the number of points you want to project.</p><p>Formally, the lemma states that for an error factor Îµ (between 0 and 1), and any set of N points in a high-dimensional space, there exists a projection into k dimensions where for any two points u and v in the original space, their projections f(u) and f(v) in the lower dimensional space satisfy:</p><p>(1 - Îµ)||u - v||Â² â‰¤ ||f(u) - f(v)||Â² â‰¤ (1 + Îµ)||u - v||Â²</p><p>The number of dimensions (k) required to guarantee these error bounds is given by:</p><p>k â‰¥ O(log(N)/ÎµÂ²)</p><p>The "Big O" notation can be replaced with a concrete constant C:</p><p>k â‰¥ (C/ÎµÂ²) * log(N)</p><p>Where:</p><ul><li>k is the target dimension</li><li>N is the number of points</li><li>Îµ is the maximum allowed distortion</li><li>C is a constant that determines the probability of success</li></ul><p>While most practitioners use values between 4 and 8 as a conservative choice for <a href="http://www.yaroslavvb.com/papers/achlioptas-database.pdf?ref=nickyoder.com" rel="noreferrer">random projections</a>, the optimal value of C remains an open question. As we'll see in the experimental section, engineered projections can achieve much lower values of C, with profound implications for embedding space capacity.</p><p>The fascinating history of this result speaks to the interconnected nature of mathematical discovery. Johnson and Lindenstrauss weren't actually trying to solve a dimensionality reduction problem â€“ they stumbled upon this property while working on extending Lipschitz functions in Banach spaces. Their 1984 paper turned out to be far more influential in computer science than in their original domain.</p><h2 id="from-theory-to-practice-two-domains-of-application">From Theory to Practice: Two Domains of Application</h2><p>The JL lemma finds practical application in two distinct but equally important domains:</p><ol><li><strong>Dimensionality Reduction</strong>: Consider an e-commerce platform like Amazon, where each customer's preferences might be represented by a vector with millions of dimensions (one for each product). Direct computation with such vectors would be prohibitively expensive. The JL lemma tells us we can project this data into a much lower-dimensional space â€“ perhaps just a thousand dimensions â€“ while preserving the essential relationships between customers. This makes previously intractable computations feasible on a single GPU, enabling real-time customer relationship management and inventory planning.</li><li><strong>Embedding Space Capacity</strong>: This application is more subtle but equally powerful. Rather than actively projecting vectors, we're interested in understanding how many distinct concepts can naturally coexist in a fixed-dimensional space. This is where our experiments provide valuable insight into the practical limits of embedding space capacity.</li></ol><p>Let's consider what we mean by "concepts" in an embedding space. Language models don't deal with perfectly orthogonal relationships â€“ real-world concepts exhibit varying degrees of similarity and difference. Consider these examples of words chosen at random:</p><ul><li>"Archery" shares some semantic space with "precision" and "sport"</li><li>"Fire" overlaps with both "heat" and "passion"</li><li>"Gelatinous" relates to physical properties and food textures</li><li>"Southern-ness" encompasses culture, geography, and dialect</li><li>"Basketball" connects to both athletics and geometry</li><li>"Green" spans color perception and environmental consciousness</li><li>"Altruistic" links moral philosophy with behavioral patterns</li></ul><p>The beauty of high-dimensional spaces is that they can accommodate these nuanced, partial relationships while maintaining useful geometric properties for computation and inference.</p><h2 id="empirical-investigation-of-embedding-capacity">Empirical Investigation of Embedding Capacity</h2><p>When we move from random projections to engineered solutions, the theoretical bounds for C of the JL lemma become surprisingly conservative. While a <a href="https://documents.uow.edu.au/~jennie/WEBPDF/175_1992.pdf?ref=nickyoder.com" rel="noreferrer">Hadamard matrix transformation</a> with random elements can reliably achieve a C value between 2.5 and 4 in a single pass, our GPU experiments suggest even more efficient arrangements are possible through optimization.</p><p>To explore these limits, I implemented a series of experiments projecting standard basis vectors into spaces of varying dimensionality. Using GPU acceleration, I tested combinations of N (number of vectors) up to 30,000 and k (embedding dimensions) up to 10,000, running each optimization for 50,000 iterations. The results reveal some fascinating patterns:</p><figure><img src="https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-18-at-3.19.12-PM.png" alt="" loading="lazy" width="1316" height="262" srcset="https://nickyoder.com/content/images/size/w600/2025/02/Screenshot-2025-02-18-at-3.19.12-PM.png 600w, https://nickyoder.com/content/images/size/w1000/2025/02/Screenshot-2025-02-18-at-3.19.12-PM.png 1000w, https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-18-at-3.19.12-PM.png 1316w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Experimental values of C (GPU optimization)</span></figcaption></figure><p>Several key observations emerge from this data:</p><ol><li>The value of C initially rises with N, reaching a maximum around ~0.9 (notably always below 1.0)</li><li>After peaking, C begins a consistent downward trend</li><li>At high ratios of N to K, we observe C values trend below 0.2</li></ol><figure><img src="https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-12.07.39-PM.png" alt="" loading="lazy" width="1164" height="624" srcset="https://nickyoder.com/content/images/size/w600/2025/02/Screenshot-2025-02-20-at-12.07.39-PM.png 600w, https://nickyoder.com/content/images/size/w1000/2025/02/Screenshot-2025-02-20-at-12.07.39-PM.png 1000w, https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-12.07.39-PM.png 1164w"><figcaption><span>C values for different dimensions K plotted against increasing N vectors</span></figcaption></figure><p>This behavior likely relates to an interesting property of high-dimensional geometry: as dimensionality increases, sphere packing becomes more efficient when the spheres are small relative to the unit sphere. This suggests that our observed upper bounds on C might still be conservative for very large numbers of concepts.</p><h2 id="practical-implications-for-language-models">Practical Implications for Language Models</h2><p>Let's consider three scenarios for the constant C:</p><ol><li>C = 4: A conservative choice for random projections with Hadamard matrices</li><li>C = 1: A likely upper bound for optimized or emergent embeddings</li><li>C = 0.2: A value suggested by our experiments for very large spaces</li></ol><figure><img src="https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-1.20.34-PM.png" alt="" loading="lazy" width="1037" height="249" srcset="https://nickyoder.com/content/images/size/w600/2025/02/Screenshot-2025-02-20-at-1.20.34-PM.png 600w, https://nickyoder.com/content/images/size/w1000/2025/02/Screenshot-2025-02-20-at-1.20.34-PM.png 1000w, https://nickyoder.com/content/images/2025/02/Screenshot-2025-02-20-at-1.20.34-PM.png 1037w"><figcaption><span>Number of possible vectors ranging from "Efficient random projection C=4" to "Optimized projection C=0.2"</span></figcaption></figure><p>The implications of these geometric properties are staggering. Let's consider a simple way to estimate how many quasi-orthogonal vectors can fit in a k-dimensional space. If we define F as the degrees of freedom from orthogonality (90Â° - desired angle), we can approximate the number of vectors as:</p><p>Vectors â‰ˆ 10^(k * FÂ² / 1500)</p><p>where:</p><ul><li>k is the embedding dimension</li><li>F is the degrees of "freedom" from orthogonality (e.g., F = 3 for 87Â° angles)</li></ul><p>Applying this to GPT-3's 12,288-dimensional embedding space reveals its extraordinary capacity:</p><ul><li>At 89Â° (F = 1): approximately 10^8 vectors</li><li>At 88Â° (F = 2): approximately 10^32 vectors</li><li>At 87Â° (F = 3): approximately 10^73 vectors</li><li>At 85Â° (F = 5): more than 10^200 vectors</li></ul><p>To put this in perspective, even the conservative case of 86Â° angles provides capacity far exceeding the estimated number of atoms in the observable universe (~10^80). This helps explain how language models can maintain rich, nuanced relationships between millions of concepts while working in relatively modest embedding dimensions.</p><h2 id="practical-applications-and-future-directions">Practical Applications and Future Directions</h2><p>The insights from this investigation have two major practical implications:</p><ol><li><strong>Efficient Dimensionality Reduction</strong>: The robustness of random projections, particularly when combined with Hadamard transformations (or <a href="https://www.ece.unb.ca/tervo/ece4253/bch.shtml?ref=nickyoder.com" rel="noreferrer">BCH coding</a>), provides a computationally efficient way to work with high-dimensional data. No complex optimization required â€“ the mathematics of high-dimensional spaces does the heavy lifting for us.</li><li><strong>Embedding Space Design</strong>: Understanding the true capacity of high-dimensional spaces helps explain how transformer models can maintain rich, nuanced representations of language in relatively compact embeddings. Concepts like "Canadian," "morose," "Hitchcockian," "handsome," "whimsical," and "Muppet-like" can all find their place in the geometry while preserving their subtle relationships to each other.</li></ol><p>This research suggests that current embedding dimensions (1,000-20,000) provide more than adequate capacity for representing human knowledge and reasoning. The challenge lies not in the capacity of these spaces but in learning the optimal arrangement of concepts within them.</p><p><a href="https://github.com/NickYoder86/Johnson-Lindenstrauss?ref=nickyoder.com" rel="noreferrer">My code for Hadamard and optimized projections</a>.</p><h2 id="conclusion">Conclusion</h2><p>What began as an investigation into a subtle optimization issue has led us to a deeper appreciation of high-dimensional geometry and its role in modern machine learning. The Johnson-Lindenstrauss lemma, discovered in a different context nearly four decades ago, continues to provide insight into the foundations of how we can represent meaning in mathematical spaces.</p><p>I want to express my sincere gratitude to Grant Sanderson and the 3Blue1Brown channel. His work consistently inspires deeper exploration of mathematical concepts, and his openness to collaboration exemplifies the best aspects of the mathematical community. The opportunity to contribute to this discussion has been both an honor and a genuine pleasure.</p><p>I would also like to thank <a href="https://www.linkedin.com/in/suman-dev-3529a872/?ref=nickyoder.com" rel="noreferrer">Suman Dev</a> for his help in optimizing the GPU code.</p><p>This was enormously fun to research and write.</p><p>Nick Yoder<br></p><h3 id="further-reading"><br>Further Reading</h3><ol><li><a href="https://www.scribd.com/document/345998742/sphere-packing-lattices-and-groups-10-1-1-66-8958-pdf?ref=nickyoder.com" rel="noreferrer">Sphere Packings, Lattices and Groups</a> by Conway and Sloane</li><li><a href="https://www.sciencedirect.com/science/article/pii/S0022000003000254?ref=nickyoder.com" rel="noreferrer">Database-friendly random projections: Johnson-Lindenstrauss with binary coins</a> by Achlioptas</li><li><a href="https://documents.uow.edu.au/~jennie/WEBPDF/175_1992.pdf?ref=nickyoder.com" rel="noreferrer">Hadamard Matrices, Sequences, and Block Designs</a> by Seberry and Yamada</li></ol>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Which NPM package has the largest version number? (127 pts)]]></title>
            <link>https://adamhl.dev/blog/largest-number-in-npm-package/</link>
            <guid>45245678</guid>
            <pubDate>Mon, 15 Sep 2025 03:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adamhl.dev/blog/largest-number-in-npm-package/">https://adamhl.dev/blog/largest-number-in-npm-package/</a>, See on <a href="https://news.ycombinator.com/item?id=45245678">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>  <p>I was recently working on a project that uses the <a href="https://github.com/aws/aws-sdk-js-v3">AWS SDK for JavaScript</a>. When updating the dependencies in said project, I noticed that the version of that dependency was <code>v3.888.0</code>. Eight hundred eighty eight. Thatâ€™s a big number as far as versions go.</p>
<p>That got me thinking: I wonder what package in the <a href="https://www.npmjs.com/">npm registry</a> has the largest number in its version. It could be a major, minor, or patch version, and it doesnâ€™t have to be the latest version of the package. In other words, out of the three numbers in <code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code> for each version for each package, what is the largest number I can find?</p>
<p><strong>TL;DR? <a href="#results">Jump to the results</a></strong> to see the answer.</p>
<section data-heading-rank="2"><h2 id="the-npm-api">The npm API</h2>
<p>Obviously npm has some kind of API, so it shouldnâ€™t be too hard to get a list of allâ€¦ <a href="https://www.npmjs.com/#:~:text=Packages-,3%2C639%2C812,-Downloads%20%C2%B7%20Last">3,639,812 packages</a>. Oh. Thatâ€™s a lot of packages. Well, considering npm had 374 <em>billion</em> package downloads in the past <em>month</em>, Iâ€™m sure they wouldnâ€™t mind me making a few million HTTP requests.</p>
<p>Doing a quick search for â€œnpm apiâ€ leads me to a readme in the <a href="https://github.com/npm/registry/blob/main/docs/REGISTRY-API.md">npm/registry repo</a> on GitHub. Thereâ€™s a <code>/-/all</code> endpoint listed in the table of contents which seems promising. That section doesnâ€™t actually exist in the readme, but maybe it still works?</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>curl</span><span> </span><span>'https://registry.npmjs.org/-/all'</span></p></div><div><p><span>{</span><span>"code"</span><span>:</span><span>"ResourceNotFound"</span><span>,</span><span>"message"</span><span>:</span><span>"/-/all does not exist"</span><span>}</span></p></div></code></pre></figure></div>
<p>Whelp, maybe npm packages have an ID and I can just start at 1 and count up? It looks like packages have an <code>_id</code> fieldâ€¦ never mind, the <code>_id</code> field is the package <em>name</em>. Okay, letâ€™s try to find something else.</p>
<p>A little more digging brings me to this <a href="https://github.com/orgs/community/discussions/152515">GitHub discussion</a> about the npm replication API. So npm replicates package info in CouchDB at <code>https://replicate.npmjs.com</code>, and conveniently, they support the <a href="https://docs.couchdb.org/en/stable/api/database/bulk-api.html#db-all-docs"><code>_all_docs</code> endpoint</a>. Letâ€™s give that a try:</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>curl</span><span> </span><span>'https://replicate.npmjs.com/registry/_all_docs'</span></p></div><div><p><span>{</span></p></div><div><p><span>   </span><span>"total_rows"</span><span> </span><span>:</span><span> </span><span>3628088,</span></p></div><div><p><span>   </span><span>"offset"</span><span> </span><span>:</span><span> </span><span>0,</span></p></div><div><p><span>   </span><span>"rows"</span><span> </span><span>:</span><span> [</span></p></div><div><p><span><span>      </span></span><span>{</span></p></div><div><p><span>         </span><span>"id"</span><span> </span><span>:</span><span> </span><span>"-",</span></p></div><div><p><span>         </span><span>"key"</span><span> </span><span>:</span><span> </span><span>"-",</span></p></div><div><p><span>         </span><span>"value"</span><span> </span><span>:</span><span> </span><span>{</span></p></div><div><p><span>            </span><span>"rev"</span><span> </span><span>:</span><span> </span><span>"5-f0890cdc1175072e37c43859f9d28403"</span></p></div><div><p><span><span>         </span></span><span>}</span></p></div><div><p><span><span>      </span></span><span>},</span></p></div><div><p><span><span>      </span></span><span>{</span></p></div><div><p><span>         </span><span>"id"</span><span> </span><span>:</span><span> </span><span>"--------------------------------------------------------------------------------------------------------------------------------whynunu",</span></p></div><div><p><span>         </span><span>"key"</span><span> </span><span>:</span><span> </span><span>"--------------------------------------------------------------------------------------------------------------------------------whynunu",</span></p></div><div><p><span>         </span><span>"value"</span><span> </span><span>:</span><span> </span><span>{</span></p></div><div><p><span>            </span><span>"rev"</span><span> </span><span>:</span><span> </span><span>"1-1d26131b0f8f9702c444e061278d24f2"</span></p></div><div><p><span><span>         </span></span><span>}</span></p></div><div><p><span><span>      </span></span><span>},</span></p></div><div><p><span><span>      </span></span><span>{</span></p></div><div><p><span>         </span><span>"id"</span><span> </span><span>:</span><span> </span><span>"-----hsad-----",</span></p></div><div><p><span>         </span><span>"key"</span><span> </span><span>:</span><span> </span><span>"-----hsad-----",</span></p></div><div><p><span>         </span><span>"value"</span><span> </span><span>:</span><span> </span><span>{</span></p></div><div><p><span>            </span><span>"rev"</span><span> </span><span>:</span><span> </span><span>"1-47778a3a6f9d8ce1e0530611c78c4ab4"</span></p></div><div><p><span><span>         </span></span><span>}</span></p></div><div><p><span><span>      </span></span><span>},</span></p></div><div><p><span>      </span><span># 997 more packages...</span></p></div></code></pre></figure></div>
<p>Those are some interesting package names. Looks like this data is paginated and by default I get 1,000 packages at a time. When I write the final script, I can set the <code>limit</code> query parameter to the max of 10,000 to make pagination a little less painful.</p>
<p>Fortunately, the CouchDB docs have a <a href="https://docs.couchdb.org/en/latest/ddocs/views/pagination.html#paging">guide for pagination</a>, and it looks like itâ€™s as simple as using the <code>skip</code> query parameter.</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>curl</span><span> </span><span>'https://replicate.npmjs.com/registry/_all_docs?skip=1000'</span></p></div><div><p><span>"Bad Request"</span></p></div></code></pre></figure></div>
<p>Never mind. According to the GitHub discussion linked above, <code>skip</code> is no longer supported. The â€œPaging (Alternate Method)â€ section of the same page says that I can use <code>startkey_docid</code> instead. If I grab the <code>id</code> of the last row, I should be able to use that to return the next set of rows. Fun fact: The 1000th package (alphabetically) on npm is <code>03-webpack-number-test</code>.</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>curl</span><span> </span><span>'https://replicate.npmjs.com/registry/_all_docs?startkey_docid="03-webpack-number-test"'</span></p></div><div><p><span>{</span></p></div><div><p><span>    </span><span>"total_rows"</span><span> </span><span>:</span><span> </span><span>3628102,</span></p></div><div><p><span>    </span><span>"offset"</span><span> </span><span>:</span><span> </span><span>999,</span></p></div><div><p><span>    </span><span>"rows"</span><span> </span><span>:</span><span> [</span></p></div><div><p><span>    </span><span># another 1000 packages...</span></p></div></code></pre></figure></div>
<p>Nice. Also, another <code>3628102 - 3628088 = 14</code> packages have been published in the ~15 minutes since I ran the last query.</p>
<p>Now, thereâ€™s one more piece of the puzzle to figure out. How do I get all the versions for a given package? Unfortunately, it doesnâ€™t seem like I can get package version information along with the base info returned by <code>_all_docs</code>. I have to <em>separately</em> fetch each packageâ€™s metadata from <code>https://registry.npmjs.org/&lt;package_id&gt;</code>. Letâ€™s see what good olâ€™ trusty <code>03-webpack-number-test</code> looks like:</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>curl</span><span> </span><span>'https://registry.npmjs.org/03-webpack-number-test'</span></p></div><div><p><span>{</span></p></div><div><p><span>    </span><span># i've omitted some fields here</span></p></div><div><p><span>    </span><span>"_id"</span><span> </span><span>:</span><span> </span><span>"03-webpack-number-test",</span></p></div><div><p><span>    </span><span>"versions"</span><span> </span><span>:</span><span> </span><span>{</span></p></div><div><p><span>      </span><span>"1.0.0"</span><span> </span><span>:</span><span> </span><span>{</span><span> </span><span>...</span><span> </span><span>},</span></p></div><div><p><span>      </span><span># the rest of the versions...</span></p></div></code></pre></figure></div>
<p>Alright, I have everything I need. Now I just need to write a bash script thatâ€” just kidding. A wise programmer once said, â€œif your shell script is more than 10 lines, it shouldnâ€™t be a shell scriptâ€ (that was me, I said that). I like TypeScript, so letâ€™s use that.</p>
<p>The biggest bottleneck is going to be waiting on the <code>GET</code>s for each packageâ€™s metadata. My plan is this:</p>
<ul>
<li>Grab all the package IDs from the replication API and save that data to a file (I donâ€™t want to have to refetch everything if the something goes wrong later in the script)</li>
<li>Fetch package data in batches so weâ€™re not just doing 1 HTTP request at a time</li>
<li>Save the package data to a file (again, hopefully I only have to fetch everything once)</li>
</ul>
<p>Once I have all the package data, I can answer the original question of â€œlargest number in versionâ€ and look at a few other interesting things.</p>
<p>(A few hours and many iterations laterâ€¦)</p>
<div><figure><pre data-language="sh"><code><div><p><span>$</span><span> </span><span>bun</span><span> </span><span>npm-package-versions.ts</span></p></div><div><p><span>Fetching</span><span> </span><span>package</span><span> </span><span>IDs...</span></p></div><div><p><span>Fetched</span><span> </span><span>10000</span><span> </span><span>packages</span><span> </span><span>IDs</span><span> </span><span>starting</span><span> </span><span>from</span><span> </span><span>offset</span><span> </span><span>0</span></p></div><div><p><span># this goes on for a while...</span></p></div><div><p><span>Finished</span><span> </span><span>fetching</span><span> </span><span>package</span><span> </span><span>IDs</span></p></div><div><p><span>Fetched</span><span> </span><span>50</span><span> </span><span>packages</span><span> </span><span>in</span><span> </span><span>884ms</span><span> (57 </span><span>packages/s</span><span>)</span></p></div><div><p><span>Fetched</span><span> </span><span>50</span><span> </span><span>packages</span><span> </span><span>in</span><span> </span><span>852ms</span><span> (59 </span><span>packages/s</span><span>)</span></p></div><div><p><span># this goes on for a really long while...</span></p></div></code></pre></figure></div>
<p>See the <a href="#script">script section</a> at the end if you want to see what it looks like.</p>
</section><section data-heading-rank="2"><h2 id="results">Results</h2>
<p>Some stats:</p>
<ul>
<li>Time to fetch all ~3.6 million package <em>IDs</em>: <strong>A few minutes</strong></li>
<li>Time to fetch version data for each one of those packages: <strong>~12 hours</strong> (yikes)</li>
<li>Packages fetched per second: <strong>~84 packages/s</strong></li>
<li>Size of <code>package-ids.json</code>: <strong>~78MB</strong></li>
<li>Size of <code>package-data.json</code>: <strong>~886MB</strong></li>
</ul>
<p><strong>And the winner isâ€¦</strong> (not really) <a href="https://www.npmjs.com/package/latentflip-test?activeTab=versions">latentflip-test</a> at version <code>1000000000000000000.1000000000000000000.1000000000000000000</code>. And no, there havenâ€™t actually been one quintillion major versions of this package published. Disappointing, I know.</p>
<p>Okay, I feel like that shouldnâ€™t count. I think we can do better and find a â€œrealâ€ package that actually follows semantic versioning. I think a better question to ask is this:</p>
<p><strong>For packages that <em>follow semantic versioning</em>, which package has the <em>largest number</em> from <code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code> in <em>any</em> of its versions?</strong></p>
<p>So, what does it mean to â€œfollow semantic versioningâ€? Should we â€œdisqualifyâ€ a package for skipping a version number? In this case, I think weâ€™ll just say that a package has to have more versions published than the largest number we find for that package. For example, a package with a version of <code>1.888.0</code> will have had <em>at least</em> 888 versions published if it actually followed semver.</p>
<p>Before we get to the real winner, here are the top 10 packages <em>by total number of versions published</em>:</p>
<div><figure><pre data-language="plaintext"><code><div><p><span>electron-remote-control -&gt; 37328 total versions</span></p></div><div><p><span>@npm-torg/public-scoped-free-org-test-package-2 -&gt; 37134 total versions</span></p></div><div><p><span>public-unscoped-test-package -&gt; 27719 total versions</span></p></div><div><p><span>carrot-scan -&gt; 27708 total versions</span></p></div><div><p><span>@npm-torg/public-test-package-2 -&gt; 27406 total versions</span></p></div><div><p><span>@octopusdeploy/design-system-components -&gt; 26724 total versions</span></p></div><div><p><span>@octopusdeploy/type-utils -&gt; 26708 total versions</span></p></div><div><p><span>@octopusdeploy/design-system-tokens -&gt; 22122 total versions</span></p></div><div><p><span>@mahdiarjangi/phetch-cli -&gt; 19498 total versions</span></p></div><div><p><span>@atlassian-test-prod/hello-world -&gt; 19120 total versions</span></p></div></code></pre></figure></div>
<p>Top 10 packages that (probably) follow semver <em>by largest number in one of its versions</em>:</p>
<div><figure><pre data-language="plaintext"><code><div><p><span>@mahdiarjangi/phetch-cli -&gt; 19494 (1.0.19494)</span></p></div><div><p><span>electron-remote-control -&gt; 19065 (1.2.19065)</span></p></div><div><p><span>@quip/collab -&gt; 16999 (1.16999.0)</span></p></div><div><p><span>@atlassian-test-prod/hello-world -&gt; 16707 (9.7.16707)</span></p></div><div><p><span>@wix/wix-code-types -&gt; 14720 (2.0.14720)</span></p></div><div><p><span>@octopusdeploy/design-system-components -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>@octopusdeploy/type-utils -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>@octopusdeploy/design-system-tokens -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>@atlassian-test-staging/test -&gt; 13214 (49.4.13214)</span></p></div><div><p><span>binky -&gt; 9906 (3.4.9906)</span></p></div></code></pre></figure></div>
<p>So it seems like the winner is <a href="https://github.com/DinoscapeProgramming/Remote-Control">@mahdiarjangi/phetch-cli</a> with <code>19494</code>, right? Unfortunately, Iâ€™m not going to count that either. It only has so many versions because of a <a href="https://github.com/arjangimahdi/phetch-cli/actions/runs/11531682007/workflow">misconfigured GitHub action</a> that published new versions in a loop.</p>
<p>I manually went down the above list, disqualifying any packages that had similar issues. I also checked that â€œnewâ€ versions actually differed from previous versions in terms of content. Overall, I looked for a package that was actually publishing new versions on purpose with <em>some</em> kind of change to the package content.</p>
<p><strong>The real winner (#19 on the list) is:</strong> <a href="https://github.com/nice-registry/all-the-package-names">all-the-package-names</a> with <code>2401</code> from version <code>2.0.2401</code>.</p>
<p>Well, thatâ€™s sort of disappointing, but also kind of funny. I donâ€™t know what I was expecting to be honest. If youâ€™re curious, you can see <a href="#more-results">more results</a> at the bottom of this post.</p>
<p>What you do with all of this extremely important and useful information is up to you.</p>
</section><section data-heading-rank="2"><h2 id="script">Script</h2>
<div><figure><pre data-language="ts"><code><div><p><span>/* This script uses Bun specific APIs and should be executed directly with Bun */</span></p></div><div><p><span>import</span><span> fs </span><span>from</span><span> </span><span>"node:fs/promises"</span></p></div><div><p><span>import</span><span> process </span><span>from</span><span> </span><span>"node:process"</span></p></div><div><p><span>async</span><span> </span><span>function</span><span> </span><span>main</span><span>() {</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>NUM_TO_PRINT</span><span> </span><span>=</span><span> </span><span>50</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packageIds</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchPackageIds</span><span>()</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packageData</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchAllPackageData</span><span>(packageIds)</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>normalizedPackageData</span><span> </span><span>=</span><span> </span><span>normalizePackageData</span><span>(packageData)</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packagsByNumOfVersions</span><span> </span><span>=</span><span> packageData.</span><span>toSorted</span><span>((</span><span>a</span><span>, </span><span>b</span><span>) </span><span>=&gt;</span><span> b.versions.</span><span>length</span><span> </span><span>-</span><span> a.versions.</span><span>length</span><span>) </span><span>// don't use normalizedPackageData here because it *only* includes valid semver versions</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packagesByLargestNumber</span><span> </span><span>=</span><span> normalizedPackageData.</span><span>toSorted</span><span>((</span><span>a</span><span>, </span><span>b</span><span>) </span><span>=&gt;</span><span> b.largestNumber.num </span><span>-</span><span> a.largestNumber.num)</span></p></div><div><p><span>  </span><span>// Ignore packages where the number of versions isn't greater than the largest number.</span></p></div><div><p><span>  </span><span>// For example, a package with a version of 1.888.0 will have had *at least* 888 versions published if it actually followed semver.</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packagesWithSemverByLargestNumber</span><span> </span><span>=</span><span> packagesByLargestNumber.</span><span>filter</span><span>(</span></p></div><div><p><span><span>    </span></span><span>(</span><span>pkg</span><span>) </span><span>=&gt;</span><span> pkg.versions.</span><span>length</span><span> </span><span>&gt;=</span><span> pkg.largestNumber.num,</span></p></div><div><p><span><span>  </span></span><span>)</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packagesWithoutKnownBadByLargestNumber</span><span> </span><span>=</span><span> packagesWithSemverByLargestNumber.</span><span>filter</span><span>((</span><span>pkg</span><span>) </span><span>=&gt;</span></p></div><div><p><span>    </span><span>KNOWN_BAD_PACKAGES</span><span>.</span><span>every</span><span>((</span><span>badId</span><span>) </span><span>=&gt;</span><span> </span><span>!</span><span>pkg.id.</span><span>startsWith</span><span>(badId)),</span></p></div><div><p><span><span>  </span></span><span>)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`</span><span>\n</span><span>Top ${</span><span>NUM_TO_PRINT</span><span>} packages by total number of versions published:`</span><span>)</span></p></div><div><p><span><span>  </span></span><span>packagsByNumOfVersions.</span><span>slice</span><span>(</span><span>0</span><span>, </span><span>NUM_TO_PRINT</span><span>).</span><span>forEach</span><span>(({ </span><span>id</span><span>, </span><span>versions</span><span> }, </span><span>i</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><p><span><span>    </span></span><span>console.</span><span>log</span><span>(</span><span>`${</span><span>i</span><span> </span><span>+</span><span> </span><span>1</span><span>}. ${</span><span>id</span><span>} -&gt; ${</span><span>versions</span><span>.</span><span>length</span><span>} total versions`</span><span>)</span></p></div><div><p><span><span>  </span></span><span>})</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>logPackagesByLargestNumber</span><span> </span><span>=</span><span> (</span><span>packages</span><span>:</span><span> </span><span>NormalizedPackageData</span><span>[]) </span><span>=&gt;</span><span> {</span></p></div><div><p><span><span>    </span></span><span>packages.</span><span>slice</span><span>(</span><span>0</span><span>, </span><span>NUM_TO_PRINT</span><span>).</span><span>forEach</span><span>(({ </span><span>id</span><span>, </span><span>largestNumber</span><span> }, </span><span>i</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><p><span><span>      </span></span><span>console.</span><span>log</span><span>(</span><span>`${</span><span>i</span><span> </span><span>+</span><span> </span><span>1</span><span>}. ${</span><span>id</span><span>} -&gt; ${</span><span>largestNumber</span><span>.</span><span>num</span><span>} (${</span><span>largestNumber</span><span>.</span><span>version</span><span>})`</span><span>)</span></p></div><div><p><span><span>    </span></span><span>})</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`</span><span>\n</span><span>Top ${</span><span>NUM_TO_PRINT</span><span>} packages by largest number in version:`</span><span>)</span></p></div><div><p><span>  </span><span>logPackagesByLargestNumber</span><span>(packagesByLargestNumber)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`</span><span>\n</span><span>Top ${</span><span>NUM_TO_PRINT</span><span>} packages that follow semver by largest number in version:`</span><span>)</span></p></div><div><p><span>  </span><span>logPackagesByLargestNumber</span><span>(packagesWithSemverByLargestNumber)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span></p></div><div><p><span>    </span><span>`</span><span>\n</span><span>Top ${</span><span>NUM_TO_PRINT</span><span>} packages that follow semver by largest number in version (excluding known bad packages):`</span><span>,</span></p></div><div><p><span><span>  </span></span><span>)</span></p></div><div><p><span>  </span><span>logPackagesByLargestNumber</span><span>(packagesWithoutKnownBadByLargestNumber)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"</span><span>\n</span><span>Done!"</span><span>)</span></p></div><div><p><span>}</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* These are packages that have a large number of versions because of some automation (e.g. GitHub Action), where each "new" version was identical to the last.</span></p></div><div><p><span><span> </span></span><span>* For example, 'electron-remote-control' was publishing a version every hour for a long time due to a configuration mistake.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>const</span><span> </span><span>KNOWN_BAD_PACKAGES</span><span> </span><span>=</span><span> [</span></p></div><div><p><span>  </span><span>"@mahdiarjangi/phetch-cli"</span><span>,</span></p></div><div><p><span>  </span><span>"electron-remote-control"</span><span>,</span></p></div><div><p><span>  </span><span>"@quip/collab"</span><span>,</span></p></div><div><p><span>  </span><span>"@atlassian-test"</span><span>,</span></p></div><div><p><span>  </span><span>"@wix/wix-code-types"</span><span>,</span></p></div><div><p><span>  </span><span>"@octopusdeploy"</span><span>,</span></p></div><div><p><span>  </span><span>"binky"</span><span>,</span></p></div><div><p><span>  </span><span>"carrot-scan"</span><span>,</span></p></div><div><p><span>  </span><span>"terrapin-test-1"</span><span>,</span></p></div><div><p><span>  </span><span>"@prisma/language-server"</span><span>,</span></p></div><div><p><span>  </span><span>"kse-visilia"</span><span>,</span></p></div><div><p><span>  </span><span>"intraactive-sdk-ui"</span><span>,</span></p></div><div><p><span>  </span><span>"@idxdb/promised"</span><span>,</span></p></div><div><p><span>  </span><span>"wix-style-react"</span><span>,</span></p></div><div><p><span>  </span><span>"botfather"</span><span>,</span></p></div><div><p><span>]</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* Fetches every single package ID from the npm replicate API and writes the data to a file.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>async</span><span> </span><span>function</span><span> </span><span>fetchPackageIds</span><span>()</span><span>:</span><span> </span><span>Promise</span><span>&lt;</span><span>string</span><span>[]&gt; {</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packageIdsFile</span><span> </span><span>=</span><span> Bun.</span><span>file</span><span>(</span><span>"package-ids.json"</span><span>)</span></p></div><div><p><span>  </span><span>// return the existing package IDs if they exist</span></p></div><div><p><span>  </span><span>if</span><span> (</span><span>await</span><span> packageIdsFile.</span><span>exists</span><span>()) {</span></p></div><div><p><span><span>    </span></span><span>console.</span><span>log</span><span>(</span><span>"Using existing package IDs"</span><span>)</span></p></div><div><p><span>    </span><span>return</span><span> (</span><span>await</span><span> packageIdsFile.</span><span>json</span><span>()) </span><span>as</span><span> </span><span>string</span><span>[]</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Fetching package IDs..."</span><span>)</span></p></div><div><p><span>  </span><span>let</span><span> firstFetch </span><span>=</span><span> </span><span>true</span></p></div><div><p><span>  </span><span>let</span><span> startKeyPackageId</span><span>:</span><span> </span><span>string</span><span> </span><span>|</span><span> </span><span>undefined</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packageIds</span><span>:</span><span> </span><span>string</span><span>[] </span><span>=</span><span> []</span></p></div><div><p><span>  </span><span>// We use the last package ID of current fetch as the start key for the next fetch. Once the start key is the same as the last package ID, we've fetched all packages and can break out of the loop.</span></p></div><div><p><span>  </span><span>while</span><span> (</span><span>true</span><span>) {</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>LIMIT</span><span> </span><span>=</span><span> </span><span>10_000</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>startKeyQueryParam</span><span> </span><span>=</span><span> firstFetch </span><span>?</span><span> </span><span>""</span><span> </span><span>:</span><span> </span><span>`&amp;startkey_docid="${</span><span>startKeyPackageId</span><span>}"`</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>json</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchJson</span><span>&lt;{ </span><span>rows</span><span>:</span><span> { </span><span>id</span><span>:</span><span> </span><span>string</span><span> }[]; </span><span>offset</span><span>:</span><span> </span><span>number</span><span> }&gt;(</span></p></div><div><p><span>      </span><span>`https://replicate.npmjs.com/registry/_all_docs?limit=${</span><span>LIMIT</span><span>}${</span><span>startKeyQueryParam</span><span>}`</span><span>,</span></p></div><div><p><span><span>    </span></span><span>)</span></p></div><div><p><span>    </span><span>if</span><span> (</span><span>!</span><span>json) process.</span><span>exit</span><span>(</span><span>1</span><span>) </span><span>// Stop the script if we fail to fetch package IDs. The error will have already been logged.</span></p></div><div><p><span>    </span><span>const</span><span> { </span><span>rows</span><span>, </span><span>offset</span><span> } </span><span>=</span><span> json</span></p></div><div><p><span><span>    </span></span><span>console.</span><span>log</span><span>(</span><span>`Fetched ${</span><span>rows</span><span>.</span><span>length</span><span>} package IDs starting from offset ${</span><span>offset</span><span>}`</span><span>)</span></p></div><div><p><span>    </span><span>for</span><span> (</span><span>const</span><span> { </span><span>id</span><span>: </span><span>packageId</span><span> } </span><span>of</span><span> rows) {</span></p></div><div><p><span>      </span><span>if</span><span> (startKeyPackageId </span><span>===</span><span> packageId) </span><span>continue</span><span> </span><span>// Skip the startKeyPackageId. The startKeyPackageId is already in the list because it's the same as the last package ID from the previous fetch</span></p></div><div><p><span><span>      </span></span><span>packageIds.</span><span>push</span><span>(packageId)</span></p></div><div><p><span><span>    </span></span><span>}</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>lastPackageId</span><span> </span><span>=</span><span> rows.</span><span>at</span><span>(</span><span>-</span><span>1</span><span>)?.id</span></p></div><div><p><span>    </span><span>if</span><span> (startKeyPackageId </span><span>===</span><span> lastPackageId) </span><span>break</span><span> </span><span>// we've reached the end of the package IDs</span></p></div><div><p><span><span>    </span></span><span>startKeyPackageId </span><span>=</span><span> lastPackageId</span></p></div><div><p><span><span>    </span></span><span>firstFetch </span><span>&amp;&amp;=</span><span> </span><span>false</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Finished fetching package IDs"</span><span>)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`Writing package IDs to '${</span><span>packageIdsFile</span><span>.</span><span>name</span><span>}'...`</span><span>)</span></p></div><div><p><span>  </span><span>await</span><span> packageIdsFile.</span><span>write</span><span>(</span><span>JSON</span><span>.</span><span>stringify</span><span>(packageIds))</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`Finished writing package IDs to '${</span><span>packageIdsFile</span><span>.</span><span>name</span><span>}'`</span><span>)</span></p></div><div><p><span>  </span><span>return</span><span> packageIds</span></p></div><div><p><span>}</span></p></div><div><p><span>interface</span><span> </span><span>PackageData</span><span> {</span></p></div><div><p><span>  </span><span>id</span><span>:</span><span> </span><span>string</span></p></div><div><p><span>  </span><span>versions</span><span>:</span><span> </span><span>string</span><span>[]</span></p></div><div><p><span>}</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* Fetches all package metadata from the npm registry API and writes the data to a file.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>async</span><span> </span><span>function</span><span> </span><span>fetchAllPackageData</span><span>(</span><span>packageIds</span><span>:</span><span> </span><span>string</span><span>[])</span><span>:</span><span> </span><span>Promise</span><span>&lt;</span><span>PackageData</span><span>[]&gt; {</span></p></div><div><p><span>  </span><span>/** The number of packages to fetch at once */</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>BATCH_SIZE</span><span> </span><span>=</span><span> </span><span>50</span></p></div><div><p><span>  </span><span>interface</span><span> </span><span>FetchedPackageData</span><span> {</span></p></div><div><p><span>    </span><span>_id</span><span>:</span><span> </span><span>string</span></p></div><div><p><span>    </span><span>versions</span><span>?:</span><span> </span><span>Record</span><span>&lt;</span><span>string</span><span>, </span><span>unknown</span><span>&gt; </span><span>// when we fetch package data, sometimes the versions object is missing</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>packageDataFile</span><span> </span><span>=</span><span> Bun.</span><span>file</span><span>(</span><span>"package-data.json"</span><span>)</span></p></div><div><p><span>  </span><span>// return the existing package data if it exists</span></p></div><div><p><span>  </span><span>if</span><span> (</span><span>await</span><span> packageDataFile.</span><span>exists</span><span>()) {</span></p></div><div><p><span><span>    </span></span><span>console.</span><span>log</span><span>(</span><span>"Using existing package data"</span><span>)</span></p></div><div><p><span>    </span><span>return</span><span> (</span><span>await</span><span> packageDataFile.</span><span>json</span><span>()) </span><span>as</span><span> </span><span>PackageData</span><span>[]</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Fetching package data..."</span><span>)</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>allPackageData</span><span>:</span><span> </span><span>PackageData</span><span>[] </span><span>=</span><span> []</span></p></div><div><p><span>  </span><span>while</span><span> (packageIds.</span><span>length</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>) {</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>startTime</span><span> </span><span>=</span><span> Date.</span><span>now</span><span>()</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>batch</span><span> </span><span>=</span><span> packageIds.</span><span>splice</span><span>(</span><span>0</span><span>, </span><span>BATCH_SIZE</span><span>)</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>packageDataPromises</span><span> </span><span>=</span><span> batch.</span><span>map</span><span>(</span><span>async</span><span> (</span><span>packageId</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><p><span>      </span><span>const</span><span> </span><span>fetchedPackageData</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchJson</span><span>&lt;</span><span>FetchedPackageData</span><span>&gt;(</span></p></div><div><p><span>        </span><span>`https://registry.npmjs.org/${</span><span>encodeURIComponent</span><span>(</span><span>packageId</span><span>)</span><span>}`</span><span>,</span></p></div><div><p><span><span>      </span></span><span>)</span></p></div><div><p><span>      </span><span>if</span><span> (</span><span>!</span><span>fetchedPackageData) </span><span>return</span></p></div><div><p><span>      </span><span>const</span><span> { </span><span>_id</span><span>, </span><span>versions</span><span> </span><span>=</span><span> {} } </span><span>=</span><span> fetchedPackageData </span><span>// default versions to an empty object if it doesn't exist</span></p></div><div><p><span>      </span><span>const</span><span> </span><span>packageData</span><span>:</span><span> </span><span>PackageData</span><span> </span><span>=</span><span> { id: _id, versions: Object.</span><span>keys</span><span>(versions).</span><span>reverse</span><span>() } </span><span>// reverse the versions array so the newest version is first</span></p></div><div><p><span>      </span><span>return</span><span> packageData</span></p></div><div><p><span><span>    </span></span><span>})</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>packageData</span><span> </span><span>=</span><span> (</span><span>await</span><span> </span><span>Promise</span><span>.</span><span>all</span><span>(packageDataPromises)).</span><span>filter</span><span>((</span><span>data</span><span>) </span><span>=&gt;</span><span> data </span><span>!==</span><span> </span><span>undefined</span><span>)</span></p></div><div><p><span><span>    </span></span><span>allPackageData.</span><span>push</span><span>(</span><span>...</span><span>packageData)</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>endTime</span><span> </span><span>=</span><span> Date.</span><span>now</span><span>()</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>duration</span><span> </span><span>=</span><span> endTime </span><span>-</span><span> startTime</span></p></div><div><p><span><span>    </span></span><span>console.</span><span>log</span><span>(</span></p></div><div><p><span>      </span><span>`Fetched ${</span><span>packageData</span><span>.</span><span>length</span><span>} packages in ${</span><span>duration</span><span>}ms (${</span><span>Math</span><span>.</span><span>round</span><span>((</span><span>packageData</span><span>.</span><span>length</span><span> </span><span>/</span><span> </span><span>duration</span><span>) </span><span>*</span><span> </span><span>1000</span><span>)</span><span>} packages/s)`</span><span>,</span></p></div><div><p><span><span>    </span></span><span>)</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Finished fetching package data"</span><span>)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`Writing package data to '${</span><span>packageDataFile</span><span>.</span><span>name</span><span>}'...`</span><span>)</span></p></div><div><p><span>  </span><span>await</span><span> packageDataFile.</span><span>write</span><span>(</span><span>JSON</span><span>.</span><span>stringify</span><span>(allPackageData))</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>`Finished writing package data to '${</span><span>packageDataFile</span><span>.</span><span>name</span><span>}'`</span><span>)</span></p></div><div><p><span>  </span><span>return</span><span> allPackageData</span></p></div><div><p><span>}</span></p></div><div><p><span>type</span><span> </span><span>SemverNumbers</span><span> </span><span>=</span><span> [</span><span>number</span><span>, </span><span>number</span><span>, </span><span>number</span><span>]</span></p></div><div><p><span>interface</span><span> </span><span>NormalizedPackageData</span><span> {</span></p></div><div><p><span>  </span><span>id</span><span>:</span><span> </span><span>string</span></p></div><div><p><span>  </span><span>largestNumber</span><span>:</span><span> {</span></p></div><div><p><span>    </span><span>num</span><span>:</span><span> </span><span>number</span></p></div><div><p><span>    </span><span>version</span><span>:</span><span> </span><span>string</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>  </span><span>versions</span><span>:</span><span> </span><span>SemverNumbers</span><span>[]</span></p></div><div><p><span>}</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* Transforms package data so that it includes the largest number from all of its versions.</span></p></div><div><p><span><span> </span></span><span>* In each `versions` array, only valid semver versions are kept.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>function</span><span> </span><span>normalizePackageData</span><span>(</span><span>packageData</span><span>:</span><span> </span><span>PackageData</span><span>[])</span><span>:</span><span> </span><span>NormalizedPackageData</span><span>[] {</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Getting normalized package data..."</span><span>)</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>normalizedPackageData</span><span> </span><span>=</span><span> packageData</span></p></div><div><p><span><span>    </span></span><span>.</span><span>map</span><span>((</span><span>pkg</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><p><span>      </span><span>const</span><span> </span><span>semverVersions</span><span> </span><span>=</span><span> pkg.versions</span></p></div><div><p><span><span>        </span></span><span>.</span><span>map</span><span>((</span><span>version</span><span>) </span><span>=&gt;</span><span> </span><span>splitSemver</span><span>(version))</span></p></div><div><p><span><span>        </span></span><span>.</span><span>filter</span><span>((</span><span>version</span><span>) </span><span>=&gt;</span><span> version </span><span>!==</span><span> </span><span>undefined</span><span>)</span></p></div><div><p><span>      </span><span>if</span><span> (semverVersions.</span><span>length</span><span> </span><span>===</span><span> </span><span>0</span><span>) </span><span>return</span><span> </span><span>// if the package didn't have any valid semver versions, don't include it</span></p></div><div><p><span>      </span><span>let</span><span> largestNumber </span><span>=</span><span> { num: </span><span>0</span><span>, version: </span><span>""</span><span> }</span></p></div><div><p><span>      </span><span>for</span><span> (</span><span>const</span><span> </span><span>semver</span><span> </span><span>of</span><span> semverVersions) {</span></p></div><div><p><span>        </span><span>const</span><span> [</span><span>major</span><span>, </span><span>minor</span><span>, </span><span>patch</span><span>] </span><span>=</span><span> semver</span></p></div><div><p><span>        </span><span>const</span><span> </span><span>num</span><span> </span><span>=</span><span> Math.</span><span>max</span><span>(major, minor, patch)</span></p></div><div><p><span>        </span><span>if</span><span> (num </span><span>&gt;</span><span> largestNumber.num) largestNumber </span><span>=</span><span> { num, version: semver.</span><span>join</span><span>(</span><span>"."</span><span>) }</span></p></div><div><p><span><span>      </span></span><span>}</span></p></div><div><p><span>      </span><span>const</span><span> </span><span>normalizedPackage</span><span> </span><span>=</span><span> { id: pkg.id, largestNumber, versions: semverVersions }</span></p></div><div><p><span>      </span><span>return</span><span> normalizedPackage</span></p></div><div><p><span><span>    </span></span><span>})</span></p></div><div><p><span><span>    </span></span><span>.</span><span>filter</span><span>((</span><span>pkg</span><span>) </span><span>=&gt;</span><span> pkg </span><span>!==</span><span> </span><span>undefined</span><span>)</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>log</span><span>(</span><span>"Finished getting normalized package data"</span><span>)</span></p></div><div><p><span>  </span><span>return</span><span> normalizedPackageData</span></p></div><div><p><span>}</span></p></div><div><p><span>await</span><span> </span><span>main</span><span>()</span></p></div><div><p><span>/* UTILS */</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>*</span></p></div><div><p><span><span> </span></span><span>* Splits a valid semver string into an array of three number: `[major, minor, patch]`</span></p></div><div><p><span><span> </span></span><span>* If the string is not a valid semver, `undefined` is returned.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>function</span><span> </span><span>splitSemver</span><span>(</span><span>version</span><span>:</span><span> </span><span>string</span><span>)</span><span>:</span><span> </span><span>SemverNumbers</span><span> </span><span>|</span><span> </span><span>undefined</span><span> {</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>versionParts</span><span> </span><span>=</span><span> version</span></p></div><div><p><span><span>    </span></span><span>.</span><span>split</span><span>(</span><span>"."</span><span>)</span></p></div><div><p><span><span>    </span></span><span>.</span><span>slice</span><span>(</span><span>0</span><span>, </span><span>3</span><span>)</span></p></div><div><p><span><span>    </span></span><span>.</span><span>map</span><span>((</span><span>part</span><span>) </span><span>=&gt;</span><span> (Number.</span><span>isInteger</span><span>(</span><span>Number</span><span>(part)) </span><span>?</span><span> Number.</span><span>parseInt</span><span>(part, </span><span>10</span><span>) </span><span>:</span><span> </span><span>undefined</span><span>))</span></p></div><div><p><span><span>    </span></span><span>.</span><span>filter</span><span>((</span><span>part</span><span>) </span><span>=&gt;</span><span> part </span><span>!==</span><span> </span><span>undefined</span><span>)</span></p></div><div><p><span>  </span><span>if</span><span> (versionParts.</span><span>length</span><span> </span><span>!==</span><span> </span><span>3</span><span>) </span><span>return</span></p></div><div><p><span>  </span><span>const</span><span> [</span><span>major</span><span>, </span><span>minor</span><span>, </span><span>patch</span><span>] </span><span>=</span><span> versionParts </span><span>as</span><span> </span><span>SemverNumbers</span></p></div><div><p><span>  </span><span>return</span><span> [major, minor, patch]</span></p></div><div><p><span>}</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* Calls `console.error` with the message and appends the message to a `error.log` file.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>function</span><span> </span><span>logError</span><span>(</span><span>message</span><span>:</span><span> </span><span>string</span><span>) {</span></p></div><div><p><span><span>  </span></span><span>console.</span><span>error</span><span>(message)</span></p></div><div><p><span><span>  </span></span><span>fs.</span><span>appendFile</span><span>(</span><span>"error.log"</span><span>, message)</span></p></div><div><p><span>}</span></p></div><div><p><span>/**</span></p></div><div><p><span><span> </span></span><span>* Fetches the url and returns the JSON response object. Calls </span><span>{</span><span>@link</span><span> </span><span>logError</span><span>}</span><span> and returns `undefined` instead of throwing if an error occurs.</span></p></div><div><p><span><span> </span></span><span>*/</span></p></div><div><p><span>async</span><span> </span><span>function</span><span> </span><span>fetchJson</span><span>&lt;</span><span>T</span><span>&gt;(</span><span>url</span><span>:</span><span> </span><span>string</span><span>)</span><span>:</span><span> </span><span>Promise</span><span>&lt;</span><span>T</span><span> </span><span>|</span><span> </span><span>undefined</span><span>&gt; {</span></p></div><div><p><span>  </span><span>try</span><span> {</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>response</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>fetch</span><span>(url)</span></p></div><div><p><span>    </span><span>if</span><span> (</span><span>!</span><span>response.ok) </span><span>throw</span><span> </span><span>new</span><span> </span><span>Error</span><span>(</span><span>`(${</span><span>response</span><span>.</span><span>status</span><span>}) ${</span><span>await</span><span> </span><span>response</span><span>.</span><span>text</span><span>()</span><span>}`</span><span>)</span></p></div><div><p><span>    </span><span>return</span><span> (</span><span>await</span><span> response.</span><span>json</span><span>()) </span><span>as</span><span> </span><span>T</span></p></div><div><p><span><span>  </span></span><span>} </span><span>catch</span><span> (error) {</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>errorMessage</span><span> </span><span>=</span><span> error </span><span>instanceof</span><span> </span><span>Error</span><span> </span><span>?</span><span> error.message </span><span>:</span><span> </span><span>String</span><span>(error)</span></p></div><div><p><span>    </span><span>logError</span><span>(</span><span>`something went wrong fetching json for '${</span><span>url</span><span>}': ${</span><span>errorMessage</span><span>}`</span><span>)</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>  </span><span>return</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div>
</section><section data-heading-rank="2"><h2 id="more-results">More Results</h2>
<p>This is from the script:</p>
<div><figure><pre data-language="plaintext"><code><div><p><span>Top 50 packages by total number of versions published:</span></p></div><div><p><span>1. electron-remote-control -&gt; 37328 total versions</span></p></div><div><p><span>2. @npm-torg/public-scoped-free-org-test-package-2 -&gt; 37134 total versions</span></p></div><div><p><span>3. public-unscoped-test-package -&gt; 27719 total versions</span></p></div><div><p><span>4. carrot-scan -&gt; 27708 total versions</span></p></div><div><p><span>5. @npm-torg/public-test-package-2 -&gt; 27406 total versions</span></p></div><div><p><span>6. @octopusdeploy/design-system-components -&gt; 26724 total versions</span></p></div><div><p><span>7. @octopusdeploy/type-utils -&gt; 26708 total versions</span></p></div><div><p><span>8. @octopusdeploy/design-system-tokens -&gt; 22122 total versions</span></p></div><div><p><span>9. @mahdiarjangi/phetch-cli -&gt; 19498 total versions</span></p></div><div><p><span>10. @atlassian-test-prod/hello-world -&gt; 19120 total versions</span></p></div><div><p><span>11. @quip/collab -&gt; 17004 total versions</span></p></div><div><p><span>12. @gitpod/gitpod-protocol -&gt; 16145 total versions</span></p></div><div><p><span>13. ricos-build-cache -&gt; 15867 total versions</span></p></div><div><p><span>14. @wix/wix-code-types -&gt; 15030 total versions</span></p></div><div><p><span>15. @gitpod/supervisor-api-grpc -&gt; 14364 total versions</span></p></div><div><p><span>16. @atlassian-test-staging/test -&gt; 13330 total versions</span></p></div><div><p><span>17. @yuming2022/app-dnpkg-beta -&gt; 12995 total versions</span></p></div><div><p><span>18. nocodb-sdk-daily -&gt; 12512 total versions</span></p></div><div><p><span>19. @dais/sdk-minimal -&gt; 12090 total versions</span></p></div><div><p><span>20. nc-lib-gui-daily -&gt; 11874 total versions</span></p></div><div><p><span>21. binky -&gt; 11460 total versions</span></p></div><div><p><span>22. nocodb-daily -&gt; 11283 total versions</span></p></div><div><p><span>23. construct-hub-probe -&gt; 10822 total versions</span></p></div><div><p><span>24. renovate -&gt; 10369 total versions</span></p></div><div><p><span>25. @primer/react -&gt; 10320 total versions</span></p></div><div><p><span>26. @codecademy/styleguide -&gt; 10186 total versions</span></p></div><div><p><span>27. @prisma/client -&gt; 10064 total versions</span></p></div><div><p><span>28. @prisma/migrate -&gt; 9994 total versions</span></p></div><div><p><span>29. @prisma/generator-helper -&gt; 9847 total versions</span></p></div><div><p><span>30. decentraland-renderer -&gt; 9640 total versions</span></p></div><div><p><span>31. gfcdn.startpage -&gt; 9580 total versions</span></p></div><div><p><span>32. @prisma/debug -&gt; 9521 total versions</span></p></div><div><p><span>33. @codecademy/gamut -&gt; 9411 total versions</span></p></div><div><p><span>34. @coral-xyz/xnft-cli -&gt; 9397 total versions</span></p></div><div><p><span>35. @birdeye-so/tokenswap -&gt; 9382 total versions</span></p></div><div><p><span>36. @pdftron/webviewer -&gt; 9373 total versions</span></p></div><div><p><span>37. @gitpod/supervisor-api-grpcweb -&gt; 9274 total versions</span></p></div><div><p><span>38. @gitpod/local-app-api-grpcweb -&gt; 9177 total versions</span></p></div><div><p><span>39. kse-visilia -&gt; 9150 total versions</span></p></div><div><p><span>40. @prisma/fetch-engine -&gt; 8988 total versions</span></p></div><div><p><span>41. @coral-xyz/common -&gt; 8946 total versions</span></p></div><div><p><span>42. @prisma/get-platform -&gt; 8926 total versions</span></p></div><div><p><span>43. @materializeinc/sql-lexer -&gt; 8868 total versions</span></p></div><div><p><span>44. xnft -&gt; 8846 total versions</span></p></div><div><p><span>45. @prisma/language-server -&gt; 8826 total versions</span></p></div><div><p><span>46. prisma -&gt; 8675 total versions</span></p></div><div><p><span>47. @stoplight/cli -&gt; 8455 total versions</span></p></div><div><p><span>48. electron-apps -&gt; 8445 total versions</span></p></div><div><p><span>49. @knapsack/schema-utils -&gt; 8332 total versions</span></p></div><div><p><span>50. @knapsack/utils -&gt; 8323 total versions</span></p></div></code></pre></figure></div>
<br>
<div><figure><pre data-language="plaintext"><code><div><p><span>Top 50 packages that follow semver by largest number in version:</span></p></div><div><p><span>1. @mahdiarjangi/phetch-cli -&gt; 19494 (1.0.19494)</span></p></div><div><p><span>2. electron-remote-control -&gt; 19065 (1.2.19065)</span></p></div><div><p><span>3. @quip/collab -&gt; 16999 (1.16999.0)</span></p></div><div><p><span>4. @atlassian-test-prod/hello-world -&gt; 16707 (9.7.16707)</span></p></div><div><p><span>5. @wix/wix-code-types -&gt; 14720 (2.0.14720)</span></p></div><div><p><span>6. @octopusdeploy/design-system-components -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>7. @octopusdeploy/type-utils -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>8. @octopusdeploy/design-system-tokens -&gt; 14274 (2025.3.14274)</span></p></div><div><p><span>9. @atlassian-test-staging/test -&gt; 13214 (49.4.13214)</span></p></div><div><p><span>10. binky -&gt; 9906 (3.4.9906)</span></p></div><div><p><span>11. carrot-scan -&gt; 9809 (5.0.9809)</span></p></div><div><p><span>12. terrapin-test-1 -&gt; 8151 (1.0.8151)</span></p></div><div><p><span>13. @prisma/language-server -&gt; 7906 (31.0.7906)</span></p></div><div><p><span>14. kse-visilia -&gt; 5997 (1.6.5997)</span></p></div><div><p><span>15. intraactive-sdk-ui -&gt; 4752 (1.1.4752)</span></p></div><div><p><span>16. @idxdb/promised -&gt; 4614 (2.3.4614)</span></p></div><div><p><span>17. wix-style-react -&gt; 4264 (1.1.4264)</span></p></div><div><p><span>18. botfather -&gt; 4058 (3.6.4058)</span></p></div><div><p><span>19. all-the-package-names -&gt; 3905 (1.3905.0)</span></p></div><div><p><span>20. @openactive/rpde-validator -&gt; 3571 (3.0.3571)</span></p></div><div><p><span>21. electron-i18n -&gt; 3136 (1.3136.0)</span></p></div><div><p><span>22. warframe-items -&gt; 3060 (1.1253.3060)</span></p></div><div><p><span>23. ebt-vue -&gt; 3053 (1.55.3053)</span></p></div><div><p><span>24. @geometryzen/jsxgraph -&gt; 3022 (1.6.3022)</span></p></div><div><p><span>25. @deriv/api-types -&gt; 2760 (1.0.2760)</span></p></div><div><p><span>26. eslint-config-innovorder-v2 -&gt; 2730 (2.2730.6)</span></p></div><div><p><span>27. @innovorder/serverless-resize-bucket-images -&gt; 2730 (2.2730.6)</span></p></div><div><p><span>28. @zuplo/runtime -&gt; 2544 (5.2544.0)</span></p></div><div><p><span>29. @zuplo/core -&gt; 2544 (5.2544.0)</span></p></div><div><p><span>30. membership-tpa-translations -&gt; 2515 (1.0.2515)</span></p></div><div><p><span>31. all-the-package-repos -&gt; 2401 (2.0.2401)</span></p></div><div><p><span>32. @stoplight/cli -&gt; 2400 (6.0.2400)</span></p></div><div><p><span>33. @xuda.io/xuda-worker-bundle -&gt; 2366 (1.3.2366)</span></p></div><div><p><span>34. react-module-container -&gt; 2356 (1.0.2356)</span></p></div><div><p><span>35. @abtasty/pulsar-common-ui -&gt; 2314 (1.1.2314)</span></p></div><div><p><span>36. @xuda.io/xuda-worker-bundle-min -&gt; 2256 (1.3.2256)</span></p></div><div><p><span>37. @wppconnect/wa-version -&gt; 2215 (1.5.2215)</span></p></div><div><p><span>38. react-home-ar -&gt; 2195 (0.0.2195)</span></p></div><div><p><span>39. @parative/library -&gt; 2145 (3.4.2145)</span></p></div><div><p><span>40. @lightdash/cli -&gt; 2001 (0.2001.1)</span></p></div><div><p><span>41. @lightdash/common -&gt; 2001 (0.2001.1)</span></p></div><div><p><span>42. @lightdash/warehouses -&gt; 2001 (0.2001.1)</span></p></div><div><p><span>43. @eigenspace/framer-bundle-minifier -&gt; 1930 (0.0.1930)</span></p></div><div><p><span>44. carpams -&gt; 1919 (1.1.1919)</span></p></div><div><p><span>45. @eth-optimism/tokenlist -&gt; 1714 (10.0.1714)</span></p></div><div><p><span>46. aws-sdk -&gt; 1692 (2.1692.0)</span></p></div><div><p><span>47. hypothesis -&gt; 1688 (1.1688.0)</span></p></div><div><p><span>48. ros.grant.common -&gt; 1681 (2.0.1681)</span></p></div><div><p><span>49. commons-validator-js -&gt; 1669 (1.0.1669)</span></p></div><div><p><span>50. mol_regexp -&gt; 1632 (0.0.1632)</span></p></div></code></pre></figure></div></section>  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gentoo AI Policy (173 pts)]]></title>
            <link>https://wiki.gentoo.org/wiki/Project:Council/AI_policy</link>
            <guid>45244295</guid>
            <pubDate>Sun, 14 Sep 2025 23:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.gentoo.org/wiki/Project:Council/AI_policy">https://wiki.gentoo.org/wiki/Project:Council/AI_policy</a>, See on <a href="https://news.ycombinator.com/item?id=45244295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><h2><span id="The_policy">The policy</span></h2>
<p>Gentoo Council has voted on 2024-04-14 on the following policy:
</p>
<blockquote><p>It is expressly forbidden to contribute to Gentoo any content that has been created with the assistance of Natural Language Processing artificial intelligence tools.  This motion can be revisited, should a case been made over such a tool that does not pose copyright, ethical and quality concerns.</p></blockquote>
<p>This policy affects Gentoo contributions and the official Gentoo projects.  It does not prohibit adding packages for AI-related software or software that is being developed with the help of such tools upstream.
</p><p><a rel="nofollow" href="https://projects.gentoo.org/council/meeting-logs/20240414-summary.txt">Meeting summary</a>
</p>
<h2><span id="Rationale">Rationale</span></h2>
<ol><li><b>Copyright concerns.</b>  At this point, the regulations concerning copyright of generated contents are still emerging worldwide.  Using such material could pose a danger of copyright violations, but it could also weaken Gentoo claims to copyright and void the guarantees given by copyleft licensing.</li>
<li><b>Quality concerns.</b>  Popular LLMs are really great at generating plausibly looking, but meaningless content.  They are capable of providing good assistance if you are careful enough, but we can't really rely on that.  At this point, they pose both the risk of lowering the quality of Gentoo projects, and of requiring an unfair human effort from developers and users to review contributions and detect the mistakes resulting from the use of AI.</li>
<li><b>Ethical concerns.</b>  The business side of AI boom is creating serious ethical concerns.  Among them:
<ul><li>Commercial AI projects are frequently indulging in blatant copyright violations to train their models.</li>
<li>Their operations are causing concerns about the huge use of energy and water.</li>
<li>The advertising and use of AI models has caused a significant harm to employees and reduction of service quality.</li>
<li>LLMs have been empowering all kinds of spam and scam efforts.</li></ul></li></ol>
<!-- 
NewPP limit report
Cached time: 20250914122632
Cache expiry: 86400
Dynamic content: false
Complications: []
[SMW] Inâ€text annotation parser time: 0.001 seconds
CPU time usage: 0.004 seconds
Real time usage: 0.005 seconds
Preprocessor visited node count: 5/1000000
Postâ€expand include size: 0/4194304 bytes
Template argument size: 0/4194304 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/150
Unstrip recursion depth: 0/20
Unstrip postâ€expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key gentoo?hwiki:pcache:idhash:352663-0!canonical and timestamp 20250914122632 and revision id 1309111
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cannabis use associated with quadrupled risk of developing type 2 diabetes (153 pts)]]></title>
            <link>https://medicalxpress.com/news/2025-09-cannabis-quadrupled-diabetes-million-adults.html</link>
            <guid>45243867</guid>
            <pubDate>Sun, 14 Sep 2025 22:22:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2025-09-cannabis-quadrupled-diabetes-million-adults.html">https://medicalxpress.com/news/2025-09-cannabis-quadrupled-diabetes-million-adults.html</a>, See on <a href="https://news.ycombinator.com/item?id=45243867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/cannabis-use.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/cannabis-use.jpg" data-sub-html="Credit: Kampus Production from Pexels">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/cannabis-use.jpg" alt="cannabis use" title="Credit: Kampus Production from Pexels" width="800" height="530">
             <figcaption>
                Credit: Kampus Production from Pexels
            </figcaption>        </figure>
    </div><p>Cannabis use is linked to an almost quadrupling in the risk of developing diabetes, according to an analysis of real-world data from over 4 million adults, being presented at the Annual Meeting of the European Association for the Study of Diabetes (<a href="https://www.easd.org/annual-meeting/easd-2025/" target="_blank">EASD</a>) held in Vienna, Austria (15â€“19 Sept).</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>Cannabis use is increasing globally with an estimated 219 million users (4.3% of the global adult population) in 2021, but its long-term metabolic effects remain unknown. While some studies have suggested potential anti-inflammatory or weight management properties, others have raised concerns regarding glucose metabolism and <a href="https://medicalxpress.com/tags/insulin+resistance/" rel="tag">insulin resistance</a>, and the magnitude of the risk of developing diabetes hasn't been clear.</p>
<p>To strengthen the evidence base, Dr. Ibrahim Kamel from the Boston Medical Center, Massachusetts, U.S. and colleagues analyzed <a href="https://medicalxpress.com/tags/electronic+health+records/" rel="tag">electronic health records</a> from 54 health care organizations (TriNetX Research Network, with centers from across U.S. and Europe) to identify 96,795 outpatients (aged between 18 and 50 years, 52.5% female) with cannabis-related diagnoses (ranging from occasional use to dependence, including cases of intoxication and withdrawal) between 2010 and 2018.</p>
<p>They were matched with 4,160,998 healthy individuals (with no record of substance use or major chronic conditions) based on age, sex, and underlying illnesses at the start of the study, and followed for five years.</p>
<p>After controlling HDL and LDL cholesterol, uncontrolled high blood pressure, atherosclerotic cardiovascular disease, cocaine use, alcohol use and several other lifestyle risk factors, the researchers found that new cases of diabetes were significantly higher in the cannabis group (1,937; 2.2%) compared to the healthy group (518; 0.6%), with statistical analysis showing cannabis users at nearly four times the risk of developing diabetes compared to non-users.</p>

                                                                                                        
    
                                                                                                                                                                                                <p>While the authors note that more research is needed to fully explain the association between cannabis and diabetes, it may come down to insulin resistance and unhealthy dietary behaviors. Nevertheless, the study's results have immediate implications for metabolic monitoring practices and public health messaging.</p>
<p>"As cannabis becomes more widely available and socially accepted and legalized in various jurisdictions, it is essential to understand its potential health risks," said lead author Dr. Kamel.</p>
<p>"These new sights from reliable real-world evidence highlight the importance of integrating diabetes risk awareness into substance use disorder treatment and counseling, as well as the need for health care professionals to routinely talk to patients about <a href="https://medicalxpress.com/tags/cannabis+use/" rel="tag">cannabis use</a> so that they can understand their overall diabetes risk and potential need for metabolic monitoring."</p>
<p>The authors note that more research is needed on the long-term endocrine effects of cannabis use and whether diabetes risks are limited to inhaled products or other forms of cannabis such as edibles.</p>
<p>Despite the important findings, this is a retrospective study and cannot prove that cannabis use causes <a href="https://medicalxpress.com/tags/diabetes/" rel="tag">diabetes</a>, and the authors cannot rule out the possibility that other unmeasured factors may have influenced the results despite efforts to reduce confounding bias via propensity score matching. This study has limitations due to a lack of detailed cannabis consumption data and potential misclassification.</p>
<p>The authors acknowledge that the inherent limitations of real-world data often result from inconsistent patient reporting in electronic medical records.</p>
<p>They also note that there is a risk of bias because of imprecise measures of cannabis exposure and the reliance on participants to accurately report any cannabis use, even when they lived in places where the drug is illegal.</p>

                                                                                                                                    
                                                                                
                                        											
																					<p>
												Provided by
																									European Association for the Study of Diabetes
												 
											</p>
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Cannabis use associated with quadrupled risk of developing type 2 diabetes, finds study of over 4 million adults (2025, September 14)
                                                 retrieved 15 September 2025
                                                 from https://medicalxpress.com/news/2025-09-cannabis-quadrupled-diabetes-million-adults.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grapevine cellulose makes stronger plastic alternative, biodegrades in 17 days (370 pts)]]></title>
            <link>https://www.sdstate.edu/news/2025/08/can-grapevines-help-slow-plastic-waste-problem</link>
            <guid>45243803</guid>
            <pubDate>Sun, 14 Sep 2025 22:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sdstate.edu/news/2025/08/can-grapevines-help-slow-plastic-waste-problem">https://www.sdstate.edu/news/2025/08/can-grapevines-help-slow-plastic-waste-problem</a>, See on <a href="https://news.ycombinator.com/item?id=45243803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote><p><strong>A new study from South Dakota State University reveals how grapevine canes can be converted into plastic-like material that is stronger than traditional plastic and will decompose in the environment in a relatively short amount of time.&nbsp;</strong></p></blockquote><p>The need for biodegradable packaging material has never been higher.</p><p>Currently, most packaging is "single use" and is made with plastic materials, derived from nonrenewable sources like crude oil that take hundreds of years to decompose in the environment. On top of this, only 9% of plastic is recycled. This has resulted in the formation of floating piles of plastic garbage in the ocean, called the "Great Pacific Garbage Patch."</p><p>But maybe even more concerning is the discovery of micro- and nano-plastics in the environment. Research has found that plastic breaks down into tiny particles, which are being ingested or inhaled by both humans and animals, and are found literally everywhere, including in the human body â€” according to recent research studies. Worse, little is known about the long-term health effects of microplastics.</p><p><a href="https://www.sdstate.edu/directory/srinivas-janaswamy">Srinivas Janaswamy</a> is an associate professor in South Dakota State University's <a href="https://www.sdstate.edu/dairy-food-science">Department of Dairy and Food Science</a>. His research has focused on developing value-added products through biowaste and agricultural byproducts. One of the overarching goals of Janaswamy's research is to tackle the plastic waste crisis.</p><figure role="group">
<p><img loading="lazy" src="https://www.sdstate.edu/sites/default/files/2025-08/20240118_SrinivasJanaswamy-lab--1049%20-%202.jpg" width="1615" height="2096" alt="Professor and student working in a food lab.">


      
</p>

<figcaption>Srinivas Janaswamy, associate professor in South Dakota State University's Department of Dairy and Food Science, demonstrating how agricultural byproducts can be transformed into plastic-like films.&nbsp;</figcaption>
</figure>
<p>Perhaps the biggest contributor to plastic waste, at least in the United States, is plastic bags, the kind found at most retail stores. These bags, while sometimes recycled, are often only used once and can be found littered throughout the environment.</p><p>To address this problem, Janaswamy is working toward developing a plastic-like bag that will decompose in the environment.</p><p>"That is my dream," Janaswamy said.</p><p>The key ingredient to Janaswamy's work? Cellulose. This biopolymer is the most abundant organic substance on Earth and is found, primarily, in the cell walls of plants. Cellulose, thanks to strong hydrogen bonds and a chain of glucose molecules, gives plants structural strength and rigidity along with other biopolymers such as mannan, xylose, hemicellulose and lignin.</p><p>Humans have long used cellulose to create products. Cotton, the material used to make a majority of the worldâ€™s clothing, is primarily composed of cellulose. Wood is rich in cellulose as well.</p><figure role="group">
<p><img loading="lazy" src="https://www.sdstate.edu/sites/default/files/styles/wallet_size_scale_325_width_/public/2025-08/GV%20Cellulose.jpeg?itok=ak-y9hD3" width="325" height="433" alt="Cellulose">



      
</p>

<figcaption>Cellulose, pictured above, extracted from grapevine canes.&nbsp;</figcaption>
</figure>
<p>In previous research, Janaswamy has extracted cellulose from agricultural products like <a href="https://www.sdstate.edu/news/2023/06/can-avocado-peels-help-curb-plastic-waste-problem">avocado peels</a>, <a href="https://www.sdstate.edu/news/2025/02/extending-shelf-life-fruits">soyhulls</a>, alfalfa, <a href="https://www.sdstate.edu/news/2023/12/sdsu-researcher-turning-switchgrass-bioplastics">switchgrass</a>, <a href="https://www.sdstate.edu/news/2023/09/could-spent-coffee-grounds-provide-alternative-plastic-packaging">spent coffee grounds</a>, corncob and <a href="https://www.sdstate.edu/news/2024/02/save-peels-how-bananas-can-be-used-fight-plastic-waste-crisis">banana peels</a>. He uses the extracted cellulose to develop films â€” materials that look and feel similar to traditional plastic wrapping.</p><p>"By extracting cellulose from agricultural products, value-added products can be created," Janaswamy said.</p><p>Each of Janaswamy's films has different characteristics and properties. Some are more transparent than others. Some are stronger. But thanks to a unique collaboration with a fellow SDSU faculty member, Janaswamy may have created his best value-added product yet.</p><h2><strong>Grapevine canes</strong></h2><p>Janaswamy had just finished presenting â€œAg Biomass â€“ A Holy Grail to Clean up the Plastic Messâ€ at SDSU's Celebration of Faculty Excellence when he was approached by <a href="https://www.sdstate.edu/directory/anne-fennell">Anne Fennell</a>, a Distinguished Professor in the <a href="https://www.sdstate.edu/agronomy-horticulture-plant-science">Department of Agronomy, Horticulture and Plant Science</a>.</p><p>After listening to Janaswamy's presentation, Fennell became interested in the research and had an idea. A leading researcher in the study of grapevines, she knew that grapevine canes â€” the woody plant material that grapes grow on â€” were rich in cellulose. She also knew that grapevine canes were abundant and had limited use after harvest.</p><p>"Every year we prune the majority of yearly biomass off the vine," Fennell said. "The pruned canes are either mowed over, composted and reapplied to the soil, or burned in some areas. &nbsp;Research in Australia showed that prunings could be removed from the field in alternate years without effecting soil health. &nbsp;My thought was why not use this for value added films. Several of the materials that Janaswamy previously used had a high-water content, in contrast the winter pruning yields a cellulose-dense material with low water content, making them an abundant ideal material to work with."</p><p>Fennell's idea led to a collaboration, and soon Janaswamy was extracting cellulose â€” which looks almost like cotton â€” from the canes of grapevines. The resulting films were eye-opening.</p><p>According to a <a href="https://pubs.rsc.org/en/content/articlelanding/2025/fb/d5fb00211g">recent study</a> published in the academic journal Sustainable Food Technology, Janaswamy's grapevine cane films are transparent and strong and biodegrade within 17 days in the soil â€” leaving behind no harmful residue.</p><p>"High transmittance in packaging films enhances product visibility, making them more attractive to consumers and facilitating easy quality inspection without the need for unsealing," Janaswamy said. "These films demonstrate outstanding potential for food packaging applications."</p><figure role="group">
<p><img loading="lazy" src="https://www.sdstate.edu/sites/default/files/2025-08/GV%20Film.jpeg" width="1200" height="900" alt="film">


      
</p>

<figcaption>A finished grapevine cane-derived film, exhibiting high transparency.&nbsp;</figcaption>
</figure>
<p>The grapevine canes were harvested from SDSU's research vineyard. The research team, which includes doctoral candidates <a href="https://www.sdstate.edu/directory/sandeep-paudel">Sandeep Paudel</a> and <a href="https://www.sdstate.edu/directory/sumi-regmi">Sumi Regmi</a>, and Sajal Bhattarai, an SDSU graduate and a doctoral candidate at Purdue University, followed a published protocol in developing the films, which includes drying and grinding the canes and extracting the cellulosic residue. The residue was then solubilized and cast onto glass plates to create the films.</p><p>Testing revealed the grapevine cane-derived films were actually stronger than traditional plastic bags â€” in terms of tensile strength.</p><p>"Using underutilized grapevine prunings as a cellulose source for packaging films enhances waste management in the field and addresses the global issue of plastic pollution," Janaswamy said. "Developing eco-friendly films from grapevine cellulose represents a practical approach to sustainability, helping to conserve the environment and its resources and contributing to the circular bioeconomy."</p><p>The results of this work move Janaswamy one step closer to his dream of developing a bag made from a plastic-like material that will quickly decompose in the environment.</p><p>Funding for this research was provided by the U.S. Department of Agriculture's National Institute of Food and Agriculture and the National Science Foundation.&nbsp;</p>  
  <p>
                    <iframe src="https://www.sdstate.edu/media/oembed?url=https%3A//www.youtube.com/watch%3Fv%3DzkN6igtsisg&amp;max_width=0&amp;max_height=0&amp;hash=wuhC0zWrFBfHX-T1Dn-LRFYr-psP4ynZ06BpMKbfitw" width="200" height="113" loading="eager" title="Byproducts to Bioplastics | SDState Research"></iframe>

            </p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Website is hosted on a disposable vape (214 pts)]]></title>
            <link>http://ewaste.fka.wtf/</link>
            <guid>45243800</guid>
            <pubDate>Sun, 14 Sep 2025 22:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://ewaste.fka.wtf/">http://ewaste.fka.wtf/</a>, See on <a href="https://news.ycombinator.com/item?id=45243800">Hacker News</a></p>
Couldn't get http://ewaste.fka.wtf/: Error: Request failed with status code 503]]></description>
        </item>
        <item>
            <title><![CDATA[Betty Crocker broke recipes by shrinking boxes (513 pts)]]></title>
            <link>https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058</link>
            <guid>45243635</guid>
            <pubDate>Sun, 14 Sep 2025 21:54:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058">https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058</a>, See on <a href="https://news.ycombinator.com/item?id=45243635">Hacker News</a></p>
Couldn't get https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[My thoughts on renting versus buying (106 pts)]]></title>
            <link>https://milesbarr.me/posts/my-thoughts-on-renting-versus-buying/</link>
            <guid>45243019</guid>
            <pubDate>Sun, 14 Sep 2025 20:36:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://milesbarr.me/posts/my-thoughts-on-renting-versus-buying/">https://milesbarr.me/posts/my-thoughts-on-renting-versus-buying/</a>, See on <a href="https://news.ycombinator.com/item?id=45243019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>Iâ€™ve read numerous articles about renting versus buying and most dive into financial projections while completely missing the bigger picture. In my view, behavioral and emotional factors have a far larger impact on the financial outcomes of renting versus buying than the math. The math can always be made to â€œworkâ€ if youâ€™re willing to adjust what or where you rent or buy.</p>
<p>But the real differences arenâ€™t captured in spreadsheets. They come from how people actually make decisions, and how those decisions ripple through their finances over time. People rarely buy with a purely rational investment mindset; they buy based on lifestyle and emotion. And once they own, those same factors either hold them back with costs and constraints or help them by forcing them to save.</p>
<p>Thatâ€™s the point I want to hammer home: the behavioral realities matter far more than the math.</p>

<h2>Buying a Home Isnâ€™t Really an Investment 
    
    
    <span>
        <a href="#buying-a-home-isnt-really-an-investment" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Most people buy emotionally, not strategically.&nbsp;The decision is usually driven by lifestyle factors such as proximity to work, school districts, family, and the neighborhood. People obsess over granite countertops, open floor plans, stainless steel appliances, and walk in closets. These might make you happy but they donâ€™t make you rich. Even in cities with solid investment opportunities, these lifestyle constraints drastically limit options.</p>
<p>Real estate investors, on the other hand, make decisions with a calculating mindset. They look at cash flow, appreciation potential, and risk. They donâ€™t buy if the math doesnâ€™t make sense. Theyâ€™ll consider multiple neighborhoods, even cities, and may buy a home requiring significant renovations if it improves cash flow. They donâ€™t get swayed by a fancy kitchen. Unsurprisingly, an investor-focused approach will almost always outperform the emotionally driven homeowner.</p>

<h2>People Tend to Overbuy 
    
    
    <span>
        <a href="#people-tend-to-overbuy" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Buying a home is expensive and stressful so people tend to plan for the long-term, thinking of their needs 5 or more years in the future. That starter apartment suddenly feels tiny and the reaction is to overcompensate. A modest, functional place isnâ€™t enough anymore, so buyers upgrade often way beyond what they really need.</p>
<p>Renters, in constrast, focus on the short-term, only considering their needs for the next few years. They are more pragmatic and willing to comprimise because itâ€™s easier to justify knowing the situation is temporary. A modest rental fits perfectly for now, and if circumstances change, moving is simple.</p>
<p>The same behavior applies to furnishings and dÃ©cor. When people buy a home, they tend to spend more on furniture, appliances, and interior upgrades, even when their previous setup was perfectly adequate. Everything feels like it needs to match the â€œbigger, nicerâ€ home. Renters typically prioritize function and necessity over aesthetics or upgrades; they buy what works for the space rather than what â€œlooks rightâ€ for a dream home. Furniture is often minimal, multipurpose, or easy to move.</p>
<p>If you bought a home equivalent to the apartment youâ€™d rent, or a slight downgrade, you might come out ahead financially over five years. But that advantage goes out the window if you buy a place that costs twice as much as the unit you would have rented, and then spend even more on furnishings. Iâ€™ve also never seen anyone downgrade when buying; itâ€™s always a significant upgrade, both in living situation and costs.</p>

<h2>The Flexibility of Renting Is Undervalued 
    
    
    <span>
        <a href="#the-flexibility-of-renting-is-undervalued" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Renting offers a superpower that doesnâ€™t get the attention it deserves: flexibility. The flexibility of renting can have both a signicant impact to quality of life and finances. You can tailor your living situation to your life as it changes. You can easily move closer to work, test a new neighborhood, upgrade after a raise, or relocate to a new city for a job. Leases are often a year or less, and breaking one might cost a bit but itâ€™s nothing compared to the money, time, and stress of selling a home.</p>
<p>The flexibility of renting has major financial advantages. A homeowner may hesitate to accept a job in another city because selling a home can take months, cost tens of thousands in fees, and carries the risk of a slow market. A renter can make the move immediately, capturing higher paying job opportunities and better career growth. Similarly, if a renter loses a job or faces unexpected expenses, downsizing or moving to a cheaper unit is fast and straightforward. Homeowners, by contrast, may be stuck paying a mortgage they they can no longer afford and are limited in their ability to cut costs.</p>
<p>Over time, this freedom compounds. Renters are more likely to pursue promotions, side ventures, or new career paths that require mobility. They can strategically relocate for better markets, higher salaries, or professional growth. They can take a risk in changing careers or starting a business without having a mortgage hanging over their head. These advantages hard to quantify but potentially worth far more than any returns shown by financial models. Flexibility lets people respond to lifeâ€™s ups and downs without financial strain, indirectly protecting and boosting net worth in ways â€œbuy vs. rentâ€ math misses. Before buying home, one should carefully consider how home ownership will change their willingness or ability to pursue career opportunitiies or respond to financial hardships in the future.</p>

<h2>Why the Home Investment Myth Persists 
    
    
    <span>
        <a href="#why-the-home-investment-myth-persists" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>At todayâ€™s housing prices in most major cities, investing in the stock market will give better and more guaranteed returns than buying a home. The math isnâ€™t even close.</p>
<p>So why does the myth persist that homes are â€œgreat investmentsâ€? In my view, itâ€™s primarily because a home acts as a <em>forced savings vehicle</em>. Many people donâ€™t save and invest consistently. Saving and investing requires discipline but a mortgage payment is non-negotiable. People will cut spending elsewhere, adjust their budgets, and make sacrifices to ensure they make that payment. Over time, that forces wealth accumulation, even if the home was a suboptimal vehicle for building wealth.</p>
<p>This is the key reason why homeownership is so often mistaken for a surefire path to wealth. The perception isnâ€™t rooted in superior returns or market-beating performance; itâ€™s rooted in human behavior. Buying a home makes people save when they might not save otherwise, and that creates the illusion that the home itself is a great investment. In reality, the wealth mainly comes from the forced savings, not exceptional growth. Thatâ€™s not to say this isnâ€™t important; it is. But it only works if you need that external discipline, so understanding your own saving habits is key to knowing whether this benefit applies to you.</p>

<h2>Conclusion 
    
    
    <span>
        <a href="#conclusion" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>The rent-versus-buy debate is usually framed as a math problem, but in reality, itâ€™s more about behavior and tradeoffs. For disciplined savers and investors, renting almost always wins, both mathematically and behaviorally. It gives you flexibility, mobility, and the ability to align your housing with your life, not the other way around.</p>
<p>Buying, on the other hand, makes sense mainly for those who value stability above all else, or for those who struggle to save and need the discipline of a mortgage to build wealth. In that case, the home isnâ€™t a great investment; itâ€™s simply the only savings vehicle that works. So donâ€™t buy a house because â€œitâ€™s what everyone doesâ€ or because you think itâ€™s automatically a good investment. Buy only if it aligns with your lifestyle and values. Otherwise, renting may be the smarter choice.</p>
<p>For me, I currently choose to rent. But if I do buy in the future, Iâ€™ll go in with eyes wide open to all these factors. Iâ€™ll buy something far below my means, ideally with some investment potential, but I wonâ€™t fool myself into thinking itâ€™s the financially optimal decision.</p>

          
          
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe coding has turned senior devs into 'AI babysitters' (122 pts)]]></title>
            <link>https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/</link>
            <guid>45242788</guid>
            <pubDate>Sun, 14 Sep 2025 20:04:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/">https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/</a>, See on <a href="https://news.ycombinator.com/item?id=45242788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Carla Rover once spent 30 minutes sobbing after having to restart a project she vibe coded.&nbsp;</p>

<p>Rover has been in the industry for 15 years, mainly working as a web developer. Sheâ€™s now building a startup, alongside her son, that creates custom machine learning models for marketplaces.&nbsp;</p>







<p>She called vibe coding a beautiful, endless cocktail napkin on which one can perpetually sketch ideas. But dealing with AI-generated code that one hopes to use in production can be â€œworse than babysitting,â€ she said, as these AI models can mess up work in ways that are hard to predict.&nbsp;</p>

<p>She had turned to AI coding in a need for speed with her startup, as is the promise of AI tools.&nbsp;</p>







<p>â€œBecause I needed to be quick and impressive, I took a shortcut and did not scan those files after the automated review,â€ she said. â€œWhen I did do it manually, I found so much wrong. When I used a third-party tool, I found more. And I learned my lesson.â€&nbsp;</p>

<p>She and her son wound up restarting their whole project â€” hence the tears. â€œI handed it off like the copilot was an employee,â€ she said. â€œIt isnâ€™t.â€&nbsp;</p>

<p>Rover is like many experienced programmers turning to AI for coding help. But such programmers are also finding themselves acting like AI babysitters â€” rewriting and fact-checking the code the AI spits out.&nbsp;</p>
<div>
		
		<p>Techcrunch event</p>
		<div>
			
			<p><span>San Francisco</span>
													<span>|</span>
													<span>October 27-29, 2025</span>
							</p>
			
		</div>
	</div>

<p>A recent report by content delivery platform company Fastly <a href="https://www.fastly.com/blog/senior-developers-ship-more-ai-code" target="_blank" rel="noreferrer noopener nofollow">found that at least 95%</a> of the nearly 800 developers it surveyed said they spend extra time fixing AI-generated code, with the load of such verification falling most heavily on the shoulders of senior developers.</p>

<p>These experienced coders have discovered issues with AI-generated code ranging from hallucinating package names to deleting important information and security risks. Left unchecked, AI code can leave a product far more buggy than what humans would produce.</p>

<p>Working with AI-generated code has become such a problem that itâ€™s given rise to a new corporate coding job known as â€œ<a href="https://www.linkedin.com/feed/update/urn:li:activity:7370912703231574016/" target="_blank" rel="noreferrer noopener nofollow">vibe code cleanup specialist</a>.â€&nbsp;</p>







<p>TechCrunch spoke to experienced coders about their time using AI-generated code about what they see as the future of vibe coding. Thoughts varied, but one thing remained certain: The technology still has a long way to go.&nbsp;</p>

<p>â€œUsing a coding co-pilot is kind of like giving a coffee pot to a smart six-year-old and saying, â€˜Please take this into the dining room and pour coffee for the family,â€™â€ Rover said.&nbsp;</p>







<p>Can they do it? Possibly. Could they fail? Definitely. And most likely, if they do fail, they arenâ€™t going to tell you. â€œIt doesnâ€™t make the kid less clever,â€ she continued. â€œIt just means you canâ€™t delegate [a task] like that completely.â€&nbsp;</p>

<h2 id="h-you-re-absolutely-right-nbsp">â€œYouâ€™re absolutely right!â€&nbsp;</h2>

<p>Feridoon Malekzadeh also compared vibe coding to a child.</p>

<p>Heâ€™s worked in the industry for more than 20 years, holding various roles in product development, software, and design. Heâ€™s building his own startup and heavily using vibe-coding platform Lovable, he said. For fun, he also vibe codes apps like one that generates Gen Alpha slang for Boomers.&nbsp;</p>

<p>He likes that heâ€™s able to work alone on projects, saving time and money, but agrees that vibe coding is not like hiring an intern or a junior coder. Instead, vibe coding is akin to â€œhiring your stubborn, insolent teenager to help you do something,â€ he told TechCrunch.&nbsp;</p>

<p>â€œYou have to ask them 15 times to do something,â€ he said. â€œIn the end, they do some of what you asked, some stuff you didnâ€™t ask for, and they break a bunch of things along the way.â€&nbsp;</p>

<p>Malekzadeh estimates he spends around 50% of his time writing requirements, 10% to 20% of his time on vibe coding, and 30% to 40% of his time on vibe <em>fixing </em>â€” remedying the bugs and â€œunnecessary scriptâ€ created by AI-written code.&nbsp;</p>







<p>He also doesnâ€™t think vibe coding is the best at systems thinking â€” the process of seeing how a complex problem could impact an overall result. AI-generated code, he said, tries to solve more surface-level problems.&nbsp;</p>

<p>â€œIf youâ€™re creating a feature that should be broadly available in your product, a good engineer would create that once and make it available everywhere that itâ€™s needed,â€ Malekzadeh said. â€œVibe coding will create something five different times, five different ways, if itâ€™s needed in five different places. It leads to a lot of confusion, not only for the user, but for the model.â€</p>







<p>Meanwhile, Rover finds that AI â€œruns into a wallâ€ when data conflicts with what it was hard-coded to do. â€œIt can offer misleading advice, leave out key elements that are vital, or insert itself into a thought pathway youâ€™re developing,â€ she said.&nbsp;</p>

<p>She also found that rather than admit to making errors, it will manufacture results.</p>

<p>She shared another example with TechCrunch, where she questioned the results an AI model initially gave her. The model started to give a detailed explanation pretending it used the data she uploaded. Only when she called it out did the AI model confess. </p>

<p>â€œIt freaked me out because it sounded like a toxic co-worker,â€ she said.</p>

<figure><img loading="lazy" decoding="async" height="224" width="680" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png 1040w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=150,49 150w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=300,99 300w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=768,253 768w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=680,224 680w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=430,141 430w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=720,237 720w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=900,296 900w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=800,263 800w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=668,220 668w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=708,233 708w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?resize=50,16 50w" sizes="auto, (max-width: 680px) 100vw, 680px"></figure>

<figure><img loading="lazy" decoding="async" height="383" width="680" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png 1068w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=150,85 150w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=300,169 300w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=768,433 768w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=680,383 680w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=430,242 430w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=720,406 720w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=900,507 900w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=800,451 800w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=668,377 668w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=665,375 665w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=708,399 708w, https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?resize=50,28 50w" sizes="auto, (max-width: 680px) 100vw, 680px"></figure>

<p>On top of this, there are the security concerns.</p>

<p>Austin Spires is the senior director of developer enablement at Fastly and has been coding since the early 2000s.&nbsp;</p>







<p>Heâ€™s found through his own experience â€” along with chatting with customers â€” that vibe code likes to build what is quick rather than what is â€œright.â€ This may introduce vulnerabilities to the code of the kind that very new programmers tend to make, he said.&nbsp;</p>

<p>â€œWhat often happens is the engineer needs to review the code, correct the agent, and tell the agent that they made a mistake,â€ Spires told TechCrunch. â€œThis pattern is why weâ€™ve seen the trope of â€˜youâ€™re absolutely rightâ€™ appear over social media.â€&nbsp;</p>







<p>Heâ€™s referring to how AI models, like Anthropic Claude, tend to respond â€œyouâ€™re absolutely rightâ€ when called out on their mistakes.</p>

<p>Mike Arrowsmith, the chief technology officer at the IT management software company NinjaOne, has been in software engineering and security for around 20 years. He said that vibe coding is creating a new generation of IT and security blind spots to which young startups in particular are susceptible.</p>

<p>â€œVibe coding often bypasses the rigorous review processes that are foundational to traditional coding and crucial to catching vulnerabilities,â€ he told TechCrunch.</p>

<p>NinjaOne, he said, counters this by encouraging â€œsafe vibe coding,â€ where approved AI tools have access controls, along with mandatory peer review and, of course, security scanning.&nbsp;</p>

<h2 id="h-the-new-normal">The new normal</h2>

<p>While nearly everyone we spoke to agrees that AI-generated code and vibe-coding platforms are useful in many situations â€” like mocking up ideas â€” they all agree that human review is essential before building a business on it.</p>

<p>â€œThat cocktail napkin is not a business model,â€ Rover said. â€œYou have to balance the ease with insight.â€&nbsp;</p>







<p>But for all the lamenting on its errors, vibe coding has changed the present and the future of the job.&nbsp;</p>

<p>Rover said vibe coding helped her tremendously in crafting a better user interface. Malekzadeh simply said that, despite the time he spends fixing code, he still gets more done with AI coders than without them.</p>







<p>â€œâ€˜Every technology carries its own negativity, which is invented at the same time as technical progress,â€ Malekzadeh said, quoting the French theorist Paul Virilio, who spoke about inventing the shipwreck along with the ship.</p>

<figure><blockquote><p>The pros far outweigh the cons.</p></blockquote></figure>

<p>The Fastly survey found that senior developers were twice as likely to put AI-generated code into production compared to junior developers, saying that the technology helped them work faster.&nbsp;</p>

<p>Vibe coding is also part of Spiresâ€™ coding routine. He uses AI coding agents on several platforms for both front-end and back-end personal projects. He called the technology a mixed experience but said itâ€™s good in helping with prototyping, building out boilerplate, or scaffolding out a test; it removes menial tasks so that engineers can focus on building, shipping, and scaling products.&nbsp;</p>

<p>It seems the extra hours spent combing through the vibe weeds will simply become a tolerated tax on using the innovation.</p>

<p>Elvis Kimara, a young engineer, is learning that now. He just graduated with a masterâ€™s in AI and is building an AI-powered marketplace.&nbsp;</p>

<p>Like many coders, he said vibe coding has made his job harder and has often found vibe coding a joyless experience.&nbsp;</p>







<p>â€œThereâ€™s no more dopamine from solving a problem by myself. The AI just figures it out,â€ he said. At one of his last jobs, he said senior developers didnâ€™t look to help young coders as much&nbsp;â€” some not understanding new vibe-coding models, while others delegated mentorship tasks to said AI models.</p>

<p>But, he said, â€œthe pros far outweigh the cons,â€ and heâ€™s prepared to pay the innovation tax.&nbsp;</p>







<p>â€œWe wonâ€™t just be writing code; weâ€™ll be guiding AI systems, taking accountability when things break, and acting more like consultants to machines,â€ Kimara said of the new normal for which heâ€™s preparing.&nbsp;&nbsp;</p>

<p>â€œEven as I grow into a senior role, Iâ€™ll keep using it,â€ he continued. â€œItâ€™s been a real accelerator for me. I make sure I review every line of AI-generated code so I learn even faster from it.â€</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OCSP Service Has Reached End of Life (206 pts)]]></title>
            <link>https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life</link>
            <guid>45242591</guid>
            <pubDate>Sun, 14 Sep 2025 19:34:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life">https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life</a>, See on <a href="https://news.ycombinator.com/item?id=45242591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        
      
      
      
        <p>
          By Josh Aas Â· 
          
            
            
            <time datetime="2025-08-06T00:00:00+00:00">August 6, 2025</time>
          
        </p>
      
      
    </div><div>
      <p>Today we turned off our Online Certificate Status Protocol (OCSP) service, as <a href="https://letsencrypt.org/2024/12/05/ending-ocsp/">announced</a> in December of last year. We stopped including OCSP URLs in our certificates more than 90 days ago, so all Letâ€™s Encrypt certificates that contained OCSP URLs have now expired. Going forward, we will publish revocation information exclusively via Certificate Revocation Lists (CRLs).</p>
<p>We ended support for OCSP primarily because it represents a considerable risk to privacy on the Internet. When someone visits a website using a browser or other software that checks for certificate revocation via OCSP, the Certificate Authority (CA) operating the OCSP responder immediately becomes aware of which website is being visited from that visitorâ€™s particular IP address. Even when a CA intentionally does not retain this information, as is the case with Letâ€™s Encrypt, it could accidentally be retained or CAs could be legally compelled to collect it. CRLs do not have this issue.</p>
<p>We are also taking this step because keeping our CA infrastructure as simple as possible is critical for the continuity of compliance, reliability, and efficiency at Letâ€™s Encrypt. For every year that we have existed, operating OCSP services has taken up considerable resources that can soon be better spent on other aspects of our operations. <a href="https://letsencrypt.org/2022/09/07/new-life-for-crls/">Now that we support CRLs</a>, our OCSP service has become unnecessary.</p>
<p>At the height of our OCSP serviceâ€™s traffic earlier this year, we handled approximately 340 billion OCSP requests per month. Thatâ€™s more than 140,000 requests per second handled by our CDN, with 15,000 requests per second handled by our origin. Weâ€™d like to thank <a href="https://www.akamai.com/">Akamai</a> for generously donating CDN services for OCSP to Letâ€™s Encrypt for the past ten years.</p>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatControl update: blocking minority held but Denmark is moving forward anyway (539 pts)]]></title>
            <link>https://disobey.net/@yawnbox/115203365485529363</link>
            <guid>45242458</guid>
            <pubDate>Sun, 14 Sep 2025 19:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://disobey.net/@yawnbox/115203365485529363">https://disobey.net/@yawnbox/115203365485529363</a>, See on <a href="https://news.ycombinator.com/item?id=45242458">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Writing an operating system kernel from scratch (315 pts)]]></title>
            <link>https://popovicu.com/posts/writing-an-operating-system-kernel-from-scratch/</link>
            <guid>45240682</guid>
            <pubDate>Sun, 14 Sep 2025 15:44:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://popovicu.com/posts/writing-an-operating-system-kernel-from-scratch/">https://popovicu.com/posts/writing-an-operating-system-kernel-from-scratch/</a>, See on <a href="https://news.ycombinator.com/item?id=45240682">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" role="article">
      <p><a href="https://twitter.com/popovicu94?ref_src=twsrc%5Etfw" data-show-count="false">Follow @popovicu94</a></p>
<p>I recently implemented a minimal proof of concept time-sharing operating system kernel on RISC-V. In this post, Iâ€™ll share the details of how this prototype works. The target audience is anyone looking to understand low-level system software, drivers, system calls, etc., and I hope this will be especially useful to students of system software and computer architecture.</p>
<p>This is a redo of an exercise I did for my undergraduate course in operating systems, and functionally it should resemble a typical operating systems project. However, this experiment focuses on modern tooling, as well as the modern architecture of RISC-V. RISC-V is an amazing technology that is easy to understand more quickly than other CPU architectures, while remaining a popular choice for many new systems, not just an educational architecture.</p>
<p>Finally, to do things differently here, I implemented this exercise in Zig, rather than traditional C. In addition to being an interesting experiment, I believe Zig makes this experiment much more easily reproducible on your machine, as itâ€™s very easy to set up and does not require any installation (which could otherwise be slightly messy when cross-compiling to RISC-V).</p>
<h2 id="table-of-contents">Table of contents</h2>
<details><summary>Open Table of contents</summary>
<ul>
<li>
<p><a href="#github-repo">GitHub repo</a></p>
</li>
<li>
<p><a href="#recommended-reading">Recommended reading</a></p>
</li>
<li>
<p><a href="#unikernel">Unikernel</a></p>
</li>
<li>
<p><a href="#sbi-layer">SBI layer</a></p>
</li>
<li>
<p><a href="#goal-for-the-kernel">Goal for the kernel</a></p>
</li>
<li>
<p><a href="#virtualization-and-what-exactly-is-a-thread">Virtualization and what exactly is a thread</a></p>
<ul>
<li><a href="#the-stack-and-memory-virtualization">The stack and memory virtualization</a></li>
<li><a href="#virtualizing-a-thread">Virtualizing a thread</a></li>
<li><a href="#interrupt-context">Interrupt context</a></li>
</ul>
</li>
<li>
<p><a href="#implementation-high-level">Implementation (high-level)</a></p>
<ul>
<li><a href="#leveraging-the-interrupt-stack-convention">Leveraging the interrupt stack convention</a></li>
<li><a href="#kerneluser-space-separation">Kernel/user space separation</a></li>
</ul>
</li>
<li>
<p><a href="#implementation-code">Implementation (code)</a></p>
<ul>
<li><a href="#assembly-startup">Assembly startup</a></li>
<li><a href="#main-kernel-file-and-io-drivers">Main kernel file and I/O drivers</a></li>
<li><a href="#s-mode-handler-and-the-context-switch">S-mode handler and the context switch</a></li>
<li><a href="#the-user-space-threads">The user space threads</a></li>
<li><a href="#running-the-kernel">Running the kernel</a></li>
</ul>
</li>
<li>
<p><a href="#conclusion">Conclusion</a></p>
</li>
</ul>
</details>
<h2 id="github-repo">GitHub repo</h2>
<p>The final code for this experiment is on GitHub <a href="https://github.com/popovicu/zig-time-sharing-kernel">here</a>. Weâ€™ll be referencing the code from it as we go.</p>
<p>GitHub should be the source of truth and may be slightly out of sync with the code below.</p>
<h2 id="recommended-reading">Recommended reading</h2>
<p>The basic fundamentals of computer engineering and specifically computer architecture are assumed. Specifically, knowledge of registers, how the CPU addresses memory, and interrupts is all necessary.</p>
<p>Before diving deep into this experiment, itâ€™s recommended to also review the following background texts:</p>
<ol>
<li><a href="https://popovicu.com/posts/bare-metal-programming-risc-v">Bare metal programming on RISC-V</a></li>
<li><a href="https://popovicu.com/posts/risc-v-sbi-and-full-boot-process">RISC-V boot process with SBI</a></li>
<li><a href="https://popovicu.com/posts/risc-v-interrupts-with-timer-example/">RISC-V interrupts with a timer example</a></li>
<li><em>Optional</em> - <a href="https://popovicu.com/posts/making-a-micro-linux-distro">Making a micro Linux distro</a> - mainly for the brief philosophy on the kernel / user space split</li>
</ol>
<h2 id="unikernel">Unikernel</h2>
<p>Weâ€™ll be developing a type of <a href="https://en.wikipedia.org/wiki/Unikernel">unikernel</a>. Simply put, this setup links the application code directly with the OS kernel it depends on. Essentially, everything is bundled into a single binary executable, and the user code is loaded into memory alongside the kernel.</p>
<p>This bypasses the need to separately load the user code at runtime, which is a complex field in itself (involving linkers, loaders, etc.).</p>
<h2 id="sbi-layer">SBI layer</h2>
<p>RISC-V supports a layered permissions model. The system boots into machine mode (M), which is completely bare-metal, and then supports a couple of other less privileged modes. Please check the background texts for more details; below is a quick summary:</p>
<ol>
<li>M-mode can do pretty much anything; it is fully bare-metal.</li>
<li>In the middle is S-mode, supervisor, which typically hosts the operating system kernel.</li>
<li>At the bottom is U-mode, user, where application code runs.</li>
</ol>
<p>Lower privilege levels can send requests to higher privilege levels.</p>
<p>Weâ€™ll assume that at the bottom of our software stack is an SBI layer, specifically OpenSBI. Please study <a href="https://popovicu.com/posts/risc-v-sbi-and-full-boot-process">this text</a> for the necessary background, as weâ€™ll use the SBI layer to manage console printing and control the timer hardware. While manual implementation is possible, I wanted to add more value to this text by demonstrating a more portable approach with OpenSBI.</p>
<h2 id="goal-for-the-kernel">Goal for the kernel</h2>
<p>We want to support a few key features for simplicity:</p>
<ol>
<li>Statically define threads ahead of execution; i.e., dynamic thread creation is not supported. Additionally, for simplicity, threads are implemented as never-ending functions.</li>
<li>Threads operate in user mode and are able to send system calls to the kernel operating in S-mode.</li>
<li>Time is sliced and allocated among different threads. The system timer will be set to tick every couple of milliseconds, at which point a thread may be switched out.</li>
</ol>
<p>Finally, development is targeted for a single-core machine.</p>
<h2 id="virtualization-and-what-exactly-is-a-thread">Virtualization and what exactly is a thread</h2>
<p>Before implementing threads, we should decide what they really are. The concept of threads in a time-sharing environment enables multiple workloads to run on a single core (as noted above, weâ€™re focusing on single-core machines), while the programming model for each thread remains largely the same as if it were the sole software on the machine. This is a loose definition, which we will refine.</p>
<p>To understand time-sharing, letâ€™s briefly consider its contrast: cooperative scheduling/threading. In cooperative scheduling/threading, a thread voluntarily yields CPU time to another workload. Eventually, the expectation is that another thread will yield control back to the first.</p>
<pre is:raw="" tabindex="0"><code><span><span>function thread():</span></span>
<span><span>  operation_1();</span></span>
<span><span>  operation_2();</span></span>
<span><span>  YIELD();</span></span>
<span><span>  operation_3();</span></span>
<span><span>  YIELD();</span></span>
<span><span>  ...</span></span></code></pre>
<p>To be clear, this isnâ€™t an â€œoutdatedâ€ technique, despite being older. In fact, itâ€™s alive and well in many modern programming languages and their runtimes (often abstracted from programmers). One good example is Go, which uses Goroutines to run multiple workloads on top of one operating system thread. While programmers donâ€™t necessarily add explicit yield operations, the compiler and runtime can inject them into the workload.</p>
<p>Now, it should be clearer what it means for the programming model to remain largely the same in a time-sharing context. The thread would naturally look like this:</p>
<pre is:raw="" tabindex="0"><code><span><span>function thread():</span></span>
<span><span>  operation_1();</span></span>
<span><span>  operation_2();</span></span>
<span><span>  operation_3();</span></span>
<span><span>  ...</span></span></code></pre>
<p>There are simply no explicit yield operations; instead, the kernel utilizes timers and interrupts to seamlessly switch between threads on the same core. This is precisely what weâ€™ll implement in this experiment.</p>
<p>When multiple workloads run on the same resource, and each retains the same programming model as if it were the only workload, we can say the resource is virtualized. In other words, if weâ€™re running 5 threads on the same core, each thread â€œfeelsâ€ like it has its own core, effectively running on 5 little cores instead of 1 big core. More formally, each thread retains its own view of the coreâ€™s architectural registers (in RISC-V, <code>x0-x31</code> and some CSRs, more on this below) andâ€¦ some memory! Letâ€™s look deeper into that.</p>
<h3 id="the-stack-and-memory-virtualization">The stack and memory virtualization</h3>
<p>To begin, a thread has its own stack for reasons weâ€™ll analyze shortly. The rest of the memory is â€œsharedâ€ with other threads, but this requires further investigation.</p>
<p>Itâ€™s important to understand that hardware virtualization exists on a spectrum, rather than as a few rigid options. Here are some of the options for virtualization:</p>
<ol>
<li>Threads: virtualizes architectural registers and stacks, but not much else; i.e., different threads can share data elsewhere in memory.</li>
<li>Process: more heavyweight than threads, memory is virtualized such that each process â€œfeelsâ€ like it has a dedicated CPU core and its own memory untouchable by other processes; additionally, a process houses multiple threads.</li>
<li>Container: virtualizes even more - each container has its own filesystem and potentially its own set of network interfaces; containers share the same kernel and underlying hardware.</li>
<li>VM: virtualizes everything.</li>
</ol>
<p>There are many more shades in between, and each of these options likely has different subtypes. The point here is that all these approaches enable running different workloads with varying isolations, or more intuitively, different <em>views</em> of the machine and their environment.</p>
<p>Interestingly, if you examine the Linux kernel source code, you wonâ€™t find a construct explicitly called a <em>container</em>. What we popularly call containers isnâ€™t a mechanism baked into the kernel, but rather <strong>a set of kernel mechanisms</strong> used together to form a specific view of the environment for our workload. For example, the <code>chroot</code> mechanism restricts filesystem visibility, while <code>cgroups</code> impose limits on workloads; together, these form what we call a container.</p>
<p>Furthermore, I believe (though donâ€™t quote me on this) that the boundaries between threads and processes in Linux are somewhat blurred. To the best of my knowledge, both are implemented on top of <em>tasks</em> in the kernel, but when creating a task, the API allows different restrictions to be specified.</p>
<p>Ultimately, this is all to say that weâ€™re always defining a workload with varying restrictions on what it can see and access. When and why to apply different restrictions is a topic for another day. Many questions arise when writing an application, ranging from the difficulty of an approach to its security.</p>
<h3 id="virtualizing-a-thread">Virtualizing a thread</h3>
<p>In this experiment, weâ€™ll implement minimal virtualization with very basic, time-sharing threads. Therefore, the goals are the following:</p>
<ol>
<li>The programming model for a thread should remain mostly untouched. As long as a thread doesnâ€™t interact with memory contents used by other threads, its programming model should remain consistent, powered by time-sharing.</li>
<li>A thread should have its own protected view of architectural registers, including some RISC-V CSRs.</li>
<li>A thread should be assigned its own stack.</li>
</ol>
<p>It should be obvious why a thread needs its own view of the registers. If other threads could freely touch a threadâ€™s registers, the thread wouldnâ€™t be able to do any meaningful work. All (I believe) RISC-V instructions work with at least one register, so protecting a threadâ€™s register view is essential.</p>
<p>Furthermore, assigning a private stack to a thread is necessary, though slightly less obvious. The answer is that different stacks are needed to manage different execution contexts. Namely, when a function is invoked, by convention, the stack is used to allocate function-private variables. Additionally, registers like <code>ra</code> can be pushed to the stack to retain the correct return address from a function (in case another function is invoked within it). In short, there are various reasons, per RISC-V convention, why the stack is needed to maintain the execution context. The details of RISC-V calling conventions will not be described here.</p>
<h3 id="interrupt-context">Interrupt context</h3>
<p>Itâ€™s crucial to understand how interrupt code runs and what it should consist of, as this mechanism will be heavily exploited to achieve seamless time-sharing between threads. For a detailed, practical example, please check out <a href="https://popovicu.com/posts/risc-v-interrupts-with-timer-example/">this past text</a>.</p>
<p>Iâ€™ll briefly include the assembly for the timer interrupt routine from that text:</p>
<pre is:raw="" tabindex="0"><code><span><span>s_mode_interrupt_handler:</span></span>
<span><span>        addi    </span><span>sp</span><span>,</span><span>sp</span><span>,-</span><span>144</span></span>
<span><span>        sd      ra,</span><span>136</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t0,</span><span>128</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t1,</span><span>120</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t2,</span><span>112</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      s0,</span><span>104</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a0,</span><span>96</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a1,</span><span>88</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a2,</span><span>80</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a3,</span><span>72</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a4,</span><span>64</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a5,</span><span>56</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a6,</span><span>48</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      a7,</span><span>40</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t3,</span><span>32</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t4,</span><span>24</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      t5,</span><span>16</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        sd      </span><span>t6</span><span>,</span><span>8</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        addi    s0,</span><span>sp</span><span>,</span><span>144</span></span>
<span><span>        </span><span>call</span><span>    clear_timer_pending_bit</span></span>
<span><span>        </span><span>call</span><span>    set_timer_in_near_future</span></span>
<span><span>        li      a1,</span><span>33</span></span>
<span><span>        lla     a0,.LC0</span></span>
<span><span>        </span><span>call</span><span>    debug_print</span></span>
<span><span>        </span><span>nop</span></span>
<span><span>        ld      ra,</span><span>136</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t0,</span><span>128</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t1,</span><span>120</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t2,</span><span>112</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      s0,</span><span>104</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a0,</span><span>96</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a1,</span><span>88</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a2,</span><span>80</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a3,</span><span>72</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a4,</span><span>64</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a5,</span><span>56</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a6,</span><span>48</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      a7,</span><span>40</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t3,</span><span>32</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t4,</span><span>24</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      t5,</span><span>16</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        ld      </span><span>t6</span><span>,</span><span>8</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        addi    </span><span>sp</span><span>,</span><span>sp</span><span>,</span><span>144</span></span>
<span><span>        sret</span></span></code></pre>
<p>This assembly was obtained by writing a C function tagged as an S-level interrupt in RISC-V. With this tag, the GCC compiler knew how to generate the <em>prologue</em> and <em>epilogue</em> of the interrupt routine. The prologue preserves architectural registers on the stack, and the epilogue recovers them (in addition to specifically returning from S-mode). All of this was generated by correctly tagging the C functionâ€™s invoking convention.</p>
<p>This somewhat resembles function calling, and thatâ€™s essentially what it is. Interrupts can be thought of (in a very simplified sense) as functions invoked by some system effect. Consequently, utilized registers must be carefully preserved on the stack and then restored at the routineâ€™s exit; otherwise, asynchronous interrupts like timer interrupts would randomly corrupt architectural register values, completely blocking any practical software from running!</p>
<h2 id="implementation-high-level">Implementation (high-level)</h2>
<p>Weâ€™ll explore the implementation by first describing the high-level idea and then digging into the code.</p>
<h3 id="leveraging-the-interrupt-stack-convention">Leveraging the interrupt stack convention</h3>
<p>Adding an interrupt is, in a way, already introducing a form of threading to your application code. In a system with a timer interrupt, the main application code runs, which can occasionally be interleaved with instances of timer interrupt invocations. The core jumps to this interrupt routine when the timer signals, and it carefully restores the architectural state before control flow returns to the â€œmain threadâ€. There are two control flows running concurrently here:</p>
<ol>
<li>Main application code.</li>
<li>Repetitions of the interrupt routine.</li>
</ol>
<p>This interleaving of the timer interrupt can be leveraged to implement additional control flows, and the main idea is outlined below.</p>
<p>The core of the interrupt routine is sandwiched between the prologue and the epilogue. Thatâ€™s where the interrupt is serviced before control returns to the main application thread by restoring registers from the stack.</p>
<p>However, <strong>why must we restore the registers from the same stack location</strong>? If our interrupt logic swaps the stack pointer to some other piece of memory, weâ€™ll end up with a different set of architectural register values recovered, thus entering a whole different flow. In other words, we achieve a <strong>context switch</strong>, and this is precisely how itâ€™s implemented in this experiment. Weâ€™ll see the code for it shortly.</p>
<h3 id="kerneluser-space-separation">Kernel/user space separation</h3>
<p>We can now delineate the kernel space and user space. With RISC-V, this naturally translates to kernel code running in supervisor (S) mode and user space code running in U-mode.</p>
<p>The machine boots into machine (M) mode, and since we want to leverage the SBI layer, weâ€™ll allow OpenSBI to run there. Then, the kernel will perform some initial setup in S-mode before starting the U-mode execution of user space threads. Periodic timer interrupts will enable context switches, and the interrupt code will execute in S-mode. Finally, user threads will be able to make system calls to the kernel.</p>
<h2 id="implementation-code">Implementation (code)</h2>
<p>Please refer to the GitHub repository for the full code; we will only cover core excerpts below.</p>
<h3 id="assembly-startup">Assembly startup</h3>
<p>As usual, a short assembly snippet is needed to start our S-mode code and enter the â€œmain programâ€ in Zig. This is in <code>startup.S</code>.</p>
<pre is:raw="" tabindex="0"><code><span><span>...</span></span>
<span><span>done_bss:</span></span>
<span></span>
<span><span>   </span><span> # Jump to Zig main</span></span>
<span><span>    </span><span>call</span><span> main</span></span>
<span><span>...</span></span></code></pre>
<p>The rest of the assembly startup primarily involves cleaning up the BSS section and setting up the stack pointer for the initial kernel code.</p>
<h3 id="main-kernel-file-and-io-drivers">Main kernel file and I/O drivers</h3>
<p>Weâ€™ll now examine <code>kernel.zig</code>, which contains the <code>main</code> function.</p>
<p>First, we probe the OpenSBI layer for console capabilities. Weâ€™ll only consider running on a relatively recent version of OpenSBI (from the last few years) that includes console capability. Otherwise, the kernel will halt and report an error.</p>
<pre is:raw="" tabindex="0"><code><span><span>export fn main() void {</span></span>
<span><span>    const initial_print_status = sbi.debug_print(BOOT_MSG);</span></span>
<span><span></span></span>
<span><span>    if (initial_print_status.sbi_error != 0) {</span></span>
<span><span>        // SBI debug console not available, fall back to direct UART</span></span>
<span><span>        const error_msg = "ERROR: OpenSBI debug console not available! You need the latest OpenSBI.\n";</span></span>
<span><span>        const fallback_msg = "Falling back to direct UART at 0x10000000...\n";</span></span>
<span><span></span></span>
<span><span>        uart.uart_write_string(error_msg);</span></span>
<span><span>        uart.uart_write_string(fallback_msg);</span></span>
<span><span>        uart.uart_write_string("Stopping... We rely on OpenSBI, cannot continue.\n");</span></span>
<span><span></span></span>
<span><span>        while (true) {</span></span>
<span><span>            asm volatile ("wfi");</span></span>
<span><span>        }</span></span>
<span><span></span></span>
<span><span>        unreachable;</span></span>
<span><span>    }</span></span></code></pre>
<p><code>main</code> is marked as <code>export</code> to conform to the C ABI.</p>
<p>Here, we have a lightweight implementation of a couple of I/O drivers. As you can see, writing can occur in one of two ways: either we go through the SBI layer (<code>sbi.zig</code>) or, if that fails, we use direct MMIO (<code>uart_mmio.zig</code>). The SBI method should theoretically be more portable, as it delegates output management details to the M-level layer (essentially what we do with MMIO), freeing us from concerns about exact memory space addresses.</p>
<p>Letâ€™s quickly look at <code>sbi.zig</code>:</p>
<pre is:raw="" tabindex="0"><code><span><span>// Struct containing the return status of OpenSBI</span></span>
<span><span>pub const SbiRet = struct {</span></span>
<span><span>    sbi_error: isize,</span></span>
<span><span>    value: isize,</span></span>
<span><span>};</span></span>
<span><span></span></span>
<span><span>pub fn debug_print(message: []const u8) SbiRet {</span></span>
<span><span>    var err: isize = undefined;</span></span>
<span><span>    var val: isize = undefined;</span></span>
<span><span></span></span>
<span><span>    const msg_ptr = @intFromPtr(message.ptr);</span></span>
<span><span>    const msg_len = message.len;</span></span>
<span><span></span></span>
<span><span>    asm volatile (</span></span>
<span><span>        \\mv a0, %[len]</span></span>
<span><span>        \\mv a1, %[msg]</span></span>
<span><span>        \\li a2, 0</span></span>
<span><span>        \\li a6, 0x00</span></span>
<span><span>        \\li a7, 0x4442434E</span></span>
<span><span>        \\ecall</span></span>
<span><span>        \\mv %[err], a0</span></span>
<span><span>        \\mv %[val], a1</span></span>
<span><span>        : [err] "=r" (err),</span></span>
<span><span>          [val] "=r" (val),</span></span>
<span><span>        : [msg] "r" (msg_ptr),</span></span>
<span><span>          [len] "r" (msg_len),</span></span>
<span><span>        : .{ .x10 = true, .x11 = true, .x12 = true, .x16 = true, .x17 = true, .memory = true });</span></span>
<span><span></span></span>
<span><span>    return SbiRet{</span></span>
<span><span>        .sbi_error = err,</span></span>
<span><span>        .value = val,</span></span>
<span><span>    };</span></span>
<span><span>}</span></span></code></pre>
<p>This is very straightforward; weâ€™re simply performing the system call exactly as described in the OpenSBI documentation. Note that when I first wrote this code, I wasnâ€™t fully familiar with Zigâ€™s error handling capabilities, hence the somewhat non-idiomatic error handling.</p>
<p>However, this can be considered a first driver in this kernel, as it directly manages output to the device.</p>
<p>Next is <code>uart_mmio.zig</code>:</p>
<pre is:raw="" tabindex="0"><code><span><span>// UART MMIO address (standard for QEMU virt machine)</span></span>
<span><span>pub const UART_BASE: usize = 0x10000000;</span></span>
<span><span>pub const UART_TX: *volatile u8 = @ptrFromInt(UART_BASE);</span></span>
<span><span></span></span>
<span><span>// Direct UART write function (fallback when SBI is not available)</span></span>
<span><span>pub fn uart_write_string(message: []const u8) void {</span></span>
<span><span>    for (message) |byte| {</span></span>
<span><span>        UART_TX.* = byte;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>This is straightforward and self-explanatory.</p>
<p>Returning to <code>kernel.zig</code> and the <code>main</code> function, we create 3 user threads, each printing a slightly different message (the thread ID is the varying bit). At this point, the kernel setup is almost complete.</p>
<p>The final steps involve setting up and running the timer interrupt. Once that is done, kernel code will only run when the timer interrupts the system or when user space code requests a system call.</p>
<pre is:raw="" tabindex="0"><code><span><span>interrupts.setup_s_mode_interrupt(&amp;s_mode_interrupt_handler);</span></span>
<span><span>_ = timer.set_timer_in_near_future();</span></span>
<span><span>timer.enable_s_mode_timer_interrupt();</span></span></code></pre>
<p>We could request a context switch immediately, but for simplicity, weâ€™ll wait until the timer activates and begins the actual work in the system.</p>
<h3 id="s-mode-handler-and-the-context-switch">S-mode handler and the context switch</h3>
<p>While the Zig compiler could generate the adequate prologue and epilogue for our S-mode handler, we will do it manually. The reason is that we also want to capture some CSRs in the context that otherwise wouldnâ€™t have been captured by the generated routine.</p>
<p>Thatâ€™s why we use the <code>naked</code> calling convention in Zig. This forces us to write the entire function in assembly, though a quick escape hatch to this limitation is to call a Zig function whenever Zig logic is needed.</p>
<p>I wonâ€™t copy paste the whole prologue and epilogue here because they are very similar to what was done in the previous C experiment with RISC-V interrupts. Instead, Iâ€™ll just focus on the bit that is different:</p>
<pre is:raw="" tabindex="0"><code><span><span>...</span></span>
<span><span>        // Save S-level CSRs (using x5 as a temporary register)</span></span>
<span><span>        \\csrr x5, sstatus</span></span>
<span><span>        \\sd x5, </span><span>240</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrr x5, sepc</span></span>
<span><span>        \\sd x5, </span><span>248</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrr x5, scause</span></span>
<span><span>        \\sd x5, </span><span>256</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrr x5, stval</span></span>
<span><span>        \\sd x5, </span><span>264</span><span>(</span><span>sp</span><span>)</span></span>
<span></span>
<span><span>        // </span><span>Call</span><span> handle_kernel</span></span>
<span><span>        \\mv a0, </span><span>sp</span></span>
<span><span>        \\</span><span>call</span><span> handle_kernel</span></span>
<span><span>        \\mv </span><span>sp</span><span>, a0</span></span>
<span></span>
<span><span>        // </span><span>Epilogue:</span><span> Restore context</span></span>
<span><span>        // Restore S-level CSRs (using x5 as a temporary register)</span></span>
<span><span>        \\ld x5, </span><span>264</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrw stval, x5</span></span>
<span><span>        \\ld x5, </span><span>256</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrw scause, x5</span></span>
<span><span>        \\ld x5, </span><span>248</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrw sepc, x5</span></span>
<span><span>        \\ld x5, </span><span>240</span><span>(</span><span>sp</span><span>)</span></span>
<span><span>        \\csrw sstatus, x5</span></span>
<span><span>...</span></span></code></pre>
<p>As you can see, a couple more registers were added to the prologue and epilogue in addition to the core architectural registers.</p>
<p>Next, within this prologue/epilogue sandwich, we invoke the <code>handle_kernel</code> Zig function. This routes to the correct logic based on whether the interrupt source is a synchronous system call from user space or an asynchronous timer interrupt. The reason is that we land in the same S-level interrupt routine regardless of the interrupt source, and then we inspect the <code>scause</code> CSR for details.</p>
<p>To successfully work with the <code>handle_kernel</code> function, we need to be aware of the assembly-level calling conventions. This function takes a single integer parameter and returns a single integer parameter. Since the function signature is small, it works as simply as this:</p>
<ol>
<li>The sole function parameter is passed through the <code>a0</code> architectural register.</li>
<li>The same register also holds the functionâ€™s result upon return.</li>
</ol>
<p>This is pretty easy. Letâ€™s quickly look at the signature of this function:</p>
<pre is:raw="" tabindex="0"><code><span><span>export fn handle_kernel(current_stack: usize) usize {</span></span>
<span><span>...</span></span></code></pre>
<p>It is slightly awkward but gets the job done. The input to this Zig logic is the stack top before invoking the Zig logic (which inevitably leads to some data added to the stack). The functionâ€™s output is where the stack top should be <em>after</em> the Zig logic is done. If it differs from the input, then weâ€™re performing a context switch. If itâ€™s the same, the same workload thread will continue running after the interrupt.</p>
<p>The rest of the logic is very simple. It inspects the interrupt source (system call from user space or timer interrupt) and performs accordingly.</p>
<p>In the case of a timer interrupt, a context switch is performed. The <code>schedule</code> function from <code>scheduling.zig</code> is invoked, and it potentially returns the other stack we should switch to:</p>
<pre is:raw="" tabindex="0"><code><span><span>const build_options = @import("build_options");</span></span>
<span><span>const sbi = @import("sbi");</span></span>
<span><span>const std = @import("std");</span></span>
<span><span>const thread = @import("thread");</span></span>
<span><span></span></span>
<span><span>pub fn schedule(current_stack: usize) usize {</span></span>
<span><span>    const maybe_current_thread = thread.getCurrentThread();</span></span>
<span><span></span></span>
<span><span>    if (maybe_current_thread) |current_thread| {</span></span>
<span><span>        current_thread.sp_save = current_stack;</span></span>
<span><span></span></span>
<span><span>        if (comptime build_options.enable_debug_logs) {</span></span>
<span><span>            _ = sbi.debug_print("[I] Enqueueing the current thread\n");</span></span>
<span><span>        }</span></span>
<span><span>        thread.enqueueReady(current_thread);</span></span>
<span><span>    } else {</span></span>
<span><span>        if (comptime build_options.enable_debug_logs) {</span></span>
<span><span>            _ = sbi.debug_print("[W] NO CURRENT THREAD AVAILABLE!\n");</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    const maybe_new_thread = thread.dequeueReady();</span></span>
<span><span></span></span>
<span><span>    if (maybe_new_thread) |new_thread| {</span></span>
<span><span>        // TODO: software interrupt to yield to the user thread</span></span>
<span><span></span></span>
<span><span>        if (comptime build_options.enable_debug_logs) {</span></span>
<span><span>            _ = sbi.debug_print("Yielding to the new thread\n");</span></span>
<span><span>        }</span></span>
<span><span></span></span>
<span><span>        thread.setCurrentThread(new_thread);</span></span>
<span><span></span></span>
<span><span>        if (comptime build_options.enable_debug_logs) {</span></span>
<span><span>            var buffer: [256]u8 = undefined;</span></span>
<span><span>            const content = std.fmt.bufPrint(&amp;buffer, "New thread ID: {d}, stack top: {x}\n", .{ new_thread.id, new_thread.sp_save }) catch {</span></span>
<span><span>                return 0; // Return bogus stack, should be more robust in reality</span></span>
<span><span>            };</span></span>
<span><span>            _ = sbi.debug_print(content);</span></span>
<span><span>        }</span></span>
<span><span></span></span>
<span><span>        return new_thread.sp_save;</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    _ = sbi.debug_print("NO NEW THREAD AVAILABLE!\n");</span></span>
<span><span></span></span>
<span><span>    while (true) {</span></span>
<span><span>        asm volatile ("wfi");</span></span>
<span><span>    }</span></span>
<span><span>    unreachable;</span></span>
<span><span>}</span></span></code></pre>
<p>The code from the <code>thread</code> module is very simple, serving as boilerplate for a basic queue that manages structs representing threads. I wonâ€™t copy it here, as itâ€™s mostly AI-generated. It is important to note, however, that the stacks are statically allocated in memory, and the maximum number of running threads is hardcoded.</p>
<p>The <code>thread</code> module also includes logic for setting up a new thread. This is where data is pushed onto the stack before the thread even runs. If you wonder why, itâ€™s because when returning from the S-level trap handler, we need <em>something</em> on the stack to indicate where to go. The initial data does precisely that. We can seed the initial register values here as desired. In fact, in this experiment, we demonstrate passing a single integer parameter to the thread function by seeding the <code>a0</code> register value (per calling convention) on the stack, which the thread function can then use immediately.</p>
<h3 id="the-user-space-threads">The user space threads</h3>
<p>As mentioned in the introduction, weâ€™ll bundle the user space and kernel space code into a single binary blob to avoid dynamic loading, linking, and other complexities. Hence, our user space code consists of regular functions:</p>
<pre is:raw="" tabindex="0"><code><span><span>/// Example: Create a simple idle thread</span></span>
<span><span>pub fn createPrintingThread(thread_number: usize) !*Thread {</span></span>
<span><span>    const thread = allocThread() orelse return error.NoFreeThreads;</span></span>
<span><span></span></span>
<span><span>    // Idle thread just spins</span></span>
<span><span>    const print_fn = struct {</span></span>
<span><span>        fn print(thread_arg: usize) noreturn {</span></span>
<span><span>            while (true) {</span></span>
<span><span>                var buffer: [256]u8 = undefined;</span></span>
<span><span>                const content = std.fmt.bufPrint(&amp;buffer, "Printing from thread ID: {d}\n", .{thread_arg}) catch {</span></span>
<span><span>                    continue;</span></span>
<span><span>                };</span></span>
<span><span></span></span>
<span><span>                syscall.debug_print(content);</span></span>
<span><span></span></span>
<span><span>                // Simulate a delay</span></span>
<span><span>                var i: u32 = 0;</span></span>
<span><span>                while (i &lt; 300000000) : (i += 1) {</span></span>
<span><span>                    asm volatile ("" ::: .{ .memory = true }); // Memory barrier to prevent optimization</span></span>
<span><span>                }</span></span>
<span><span>            }</span></span>
<span><span>            unreachable;</span></span>
<span><span>        }</span></span>
<span><span>    }.print;</span></span>
<span><span></span></span>
<span><span>    initThread(thread, @intFromPtr(&amp;print_fn), thread_number);</span></span>
<span><span>    return thread;</span></span>
<span><span>}</span></span></code></pre>
<p>Additionally, as mentioned above, we pre-seeded the stack such that when <code>a0</code> is recovered from the stack upon the first interrupt return for a given thread, the function argument will be picked up. Thatâ€™s how the <code>print</code> function accesses the <code>thread_arg</code> value and uses it in its logic.</p>
<p>To demonstrate the user/kernel boundary, we have <code>syscall.debug_print(content);</code>. This conceptually behaves more or less as <code>printf</code> from <code>stdio.h</code> in C. It performs prepares the arguments to the kernel and runs a system call with these arguments which should lead to some content getting printed on the output device. Hereâ€™s what the printing library looks like (from <code>syscall.zig</code>):</p>
<pre is:raw="" tabindex="0"><code><span><span>// User-level debug_print function</span></span>
<span><span>pub fn debug_print(message: []const u8) void {</span></span>
<span><span>    const msg_ptr = @intFromPtr(message.ptr);</span></span>
<span><span>    const msg_len = message.len;</span></span>
<span><span></span></span>
<span><span>    // Let's say syscall number 64</span></span>
<span><span>    // a7 = syscall number</span></span>
<span><span>    // a0 = message pointer</span></span>
<span><span>    // a1 = message length</span></span>
<span><span>    asm volatile (</span></span>
<span><span>        \\mv a0, %[msg]</span></span>
<span><span>        \\mv a1, %[len]</span></span>
<span><span>        \\li a7, 64</span></span>
<span><span>        \\ecall</span></span>
<span><span>        :</span></span>
<span><span>        : [msg] "r" (msg_ptr),</span></span>
<span><span>          [len] "r" (msg_len),</span></span>
<span><span>        : .{ .x10 = true, .x11 = true, .x17 = true, .memory = true });</span></span>
<span><span></span></span>
<span><span>    // Ignore return value for simplicity</span></span>
<span><span>}</span></span></code></pre>
<p>System call 64 is served from the S-mode handler in <code>kernel.zig</code>. This is self-explanatory, and we wonâ€™t go into further details here.</p>
<h3 id="running-the-kernel">Running the kernel</h3>
<p>We will deploy the kernel on bare-metal, specifically on a virtual machine. In theory, this should also work on a real machine, provided an SBI layer is present when the kernel starts, and the linker script, I/O â€œdrivers,â€ and other machine-specific constants are adapted.</p>
<p>To build, we simply run</p>
<pre is:raw="" tabindex="0"><code><span><span>zig</span><span> </span><span>build</span></span></code></pre>
<p>To now run the kernel, we run:</p>
<pre is:raw="" tabindex="0"><code><span><span>qemu-system-riscv64</span><span> </span><span>-machine</span><span> </span><span>virt</span><span> </span><span>-nographic</span><span> </span><span>-bios</span><span> </span><span>/tmp/opensbi/build/platform/generic/firmware/fw_dynamic.bin</span><span> </span><span>-kernel</span><span> </span><span>zig-out/bin/kernel</span></span></code></pre>
<p>Refer to the <a href="https://popovicu.com/posts/risc-v-sbi-and-full-boot-process">previous text on OpenSBI</a> for details on building OpenSBI. It is strongly recommended to use a freshly built OpenSBI, as QEMU may use an outdated version if no <code>-bios</code> flag is passed.</p>
<p>The output should begin with a big OpenSBI splash along with some OpenSBI data:</p>
<pre is:raw="" tabindex="0"><code><span><span>OpenSBI v1.7</span></span>
<span><span>   ____                    _____ ____ _____</span></span>
<span><span>  / __ \                  / ____|  _ \_   _|</span></span>
<span><span> | |  | |_ __   ___ _ __ | (___ | |_) || |</span></span>
<span><span> | |  | | '_ \ / _ \ '_ \ \___ \|  _ &lt; | |</span></span>
<span><span> | |__| | |_) |  __/ | | |____) | |_) || |_</span></span>
<span><span>  \____/| .__/ \___|_| |_|_____/|____/_____|</span></span>
<span><span>        | |</span></span>
<span><span>        |_|</span></span>
<span><span></span></span>
<span><span>Platform Name               : riscv-virtio,qemu</span></span>
<span><span>Platform Features           : medeleg</span></span>
<span><span>Platform HART Count         : 1</span></span>
<span><span>Platform IPI Device         : aclint-mswi</span></span>
<span><span>Platform Timer Device       : aclint-mtimer @ 10000000Hz</span></span>
<span><span>Platform Console Device     : uart8250</span></span>
<span><span>Platform HSM Device         : ---</span></span>
<span><span>Platform PMU Device         : ---</span></span>
<span><span>Platform Reboot Device      : syscon-reboot</span></span>
<span><span>Platform Shutdown Device    : syscon-poweroff</span></span>
<span><span>Platform Suspend Device     : ---</span></span>
<span><span>Platform CPPC Device        : ---</span></span>
<span><span>Firmware Base               : 0x80000000</span></span>
<span><span>Firmware Size               : 317 KB</span></span>
<span><span>Firmware RW Offset          : 0x40000</span></span>
<span><span>Firmware RW Size            : 61 KB</span></span>
<span><span>Firmware Heap Offset        : 0x46000</span></span>
<span><span>Firmware Heap Size          : 37 KB (total), 2 KB (reserved), 11 KB (used), 23 KB (free)</span></span>
<span><span>Firmware Scratch Size       : 4096 B (total), 400 B (used), 3696 B (free)</span></span>
<span><span>Runtime SBI Version         : 3.0</span></span>
<span><span>Standard SBI Extensions     : time,rfnc,ipi,base,hsm,srst,pmu,dbcn,fwft,legacy,dbtr,sse</span></span>
<span><span>Experimental SBI Extensions : none</span></span>
<span><span></span></span>
<span><span>Domain0 Name                : root</span></span>
<span><span>....</span></span></code></pre>
<p>Following the OpenSBI splash, weâ€™ll see the kernel output:</p>
<pre is:raw="" tabindex="0"><code><span><span>Booting the kernel...</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Printing from thread ID: 2</span></span></code></pre>
<p>The prints will continue running until QEMU is terminated.</p>
<p>If you want to build the kernel in an extremely verbose mode for debugging and experimentation, use the following command:</p>
<pre is:raw="" tabindex="0"><code><span><span>zig</span><span> </span><span>build</span><span> </span><span>-Ddebug-logs=true</span></span></code></pre>
<p>After running the kernel with the same QEMU command, the output will appear as follows:</p>
<pre is:raw="" tabindex="0"><code><span><span>Booting the kernel...</span></span>
<span><span>DEBUG mode on</span></span>
<span><span>Interrupt source: Timer, Current stack: 87cffe70</span></span>
<span><span>[W] NO CURRENT THREAD AVAILABLE!</span></span>
<span><span>Yielding to the new thread</span></span>
<span><span>New thread ID: 0, stack top: 80203030</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80202ec0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80202ec0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80202ec0</span></span>
<span><span>Printing from thread ID: 0</span></span>
<span><span>Interrupt source: Timer, Current stack: 80202ec0</span></span>
<span><span>[I] Enqueueing the current thread</span></span>
<span><span>Yielding to the new thread</span></span>
<span><span>New thread ID: 1, stack top: 80205030</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80204ec0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80204ec0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80204ec0</span></span>
<span><span>Printing from thread ID: 1</span></span>
<span><span>Interrupt source: Timer, Current stack: 80204ec0</span></span>
<span><span>[I] Enqueueing the current thread</span></span>
<span><span>Yielding to the new thread</span></span>
<span><span>New thread ID: 2, stack top: 80207030</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80206ec0</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80206ec0</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Interrupt source: Ecall from User mode, Current stack: 80206ec0</span></span>
<span><span>Printing from thread ID: 2</span></span>
<span><span>Interrupt source: Timer, Current stack: 80206ec0</span></span>
<span><span>...</span></span></code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Many educational OS kernels exist, but this experiment combines RISC-V, OpenSBI, and Zig, offering a fresh perspective compared to traditional C implementations.</p>
<p>The resulting code runs on a QEMU virtual machine, which can be easily set up, even by building QEMU from source.</p>
<p>To keep the explanation concise, error reporting was kept minimal. Should you modify the code and require debugging, sufficient clues are provided, despite some areas where the code is simplified (e.g., anonymous results after SBI print invocations like <code>_ = ...</code>). Much of the code in this example was AI-generated by Claude to save time, and it should function as intended. While some parts of the code are simplified, such as stack space over-allocation, these do not detract from the experimentâ€™s educational value.</p>
<p>Overall, this experiment serves as a starting point for studying operating systems, assuming a foundational understanding of computer engineering and computer architecture. It likely has plenty of flaws for a practical application, but for now, weâ€™re just hacking here!</p>
<p>I hope this was a useful exploration.</p>
<p>Please consider following on <a href="https://twitter.com/popovicu94">Twitter/X</a> and <a href="https://www.linkedin.com/in/upopovic/">LinkedIn</a> to stay updated.</p>
    </article></div>]]></description>
        </item>
    </channel>
</rss>