<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 22 Oct 2025 00:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["Anna, Lindsey Halligan Here." My Signal exchange with the interim U.S. attorney (130 pts)]]></title>
            <link>https://www.lawfaremedia.org/article/anna--lindsey-halligan-here</link>
            <guid>45662053</guid>
            <pubDate>Tue, 21 Oct 2025 21:38:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lawfaremedia.org/article/anna--lindsey-halligan-here">https://www.lawfaremedia.org/article/anna--lindsey-halligan-here</a>, See on <a href="https://news.ycombinator.com/item?id=45662053">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p><em></em><em>Watch Anna Bower discuss this article with&nbsp;</em>Lawfare<em>&nbsp;Editor-in-Chief Benjamin Wittes&nbsp;<a target="_blank" data-stringify-link="https://youtube.com/live/SZZyccEbtpU?feature=share" data-sk="tooltip_parent" href="https://youtube.com/live/SZZyccEbtpU?feature=share" rel="noopener noreferrer" data-sf-ec-immutable="">on&nbsp;Lawfare's YouTube here</a>.</em></p><p>It was 1:20 p.m. on the afternoon of Saturday, Oct. 11. I was lounging in my pajamas, idly scrolling through Netflix, having spent the morning reading news stories, occasionally tweeting, and watching TV. It was a rare day off.<br></p><p>Then my phone lit up with a notification. I glanced down at the message.<br></p><p>â€œAnna, Lindsey Halligan here,â€ it began.&nbsp;</p><p>Lindsey Halliganâ€”the top prosecutor in the Eastern District of Virginiaâ€”was texting me. As it turned out, she was texting me about a criminal case she is pursuing against one of the presidentâ€™s perceived political enemies: New York Attorney General Letitia James.<br></p><p>So began my two-day text correspondence with the woman President Donald Trump had installed, in no small part, to bring the very prosecution she was now discussing with me by text message.<br></p><p>Over the next 33 hours, Halligan texted me again.<br></p><p>And again.&nbsp;<br></p><p>And again.&nbsp;</p><p>And again.<br></p><p>Through the whole of our correspondence, however, there is something Halligan never said: She never said a word suggesting that she was not â€œon the record.â€</p><p>It is not uncommon for federal prosecutors to communicate with the press, both through formal channels and sometimes informally. My exchange with Halligan, however, was highly unusual in a number of respects. She initiated a conversation with me, a reporter she barely knew, to discuss an ongoing prosecution that she is <a href="https://www.nytimes.com/2025/09/27/us/politics/trump-comey-justice-department.html" data-sf-ec-immutable="">personally handling</a>. She mostly criticized my reportingâ€”or, more precisely, my summary of someone elseâ€™s reporting. But several of her messages contained language that touched on grand jury matters, even as she insisted that she could not reveal such information,&nbsp; which is protected from disclosure by prosecutors under federal law.<br></p><p>As a legal journalist covering the Justice Department, I had never encountered anything quite like my exchange with Halligan. Neither had my editor. Over the last several days, he and I spoke with multiple former federal officials and journalists who cover the justice system. None could recall a similar instance in which a sitting U.S. attorney reached out to chastise a reporter about matters concerning grand jury testimony in an active case.</p><p><a href="https://givebutter.com/journalism" target="_blank" data-sf-ec-immutable=""><img src="https://lawfare-assets-new.azureedge.net/assets/images/default-source/article-images/support-lawfare8495e13b-9c80-42f6-ac96-e96789089124.png?sfvrsn=86c5a5d4_3" height="62" width="206" alt="" sf-size="69672"></a></p><p>Justice Department spokeswoman Natalie Baldassarre confirmed the authenticity of the texts at 4:34 p.m.&nbsp;today, responding to my questions about the exchange with a statement that reads in its entirety:</p><blockquote>You clearly didnâ€™t get the response you wantedâ€”which was information handed over to you without having to dig into the facts of the case to craft a truthful storyâ€”so you thought youâ€™d â€œtattletaleâ€ to main justice. Lindsay [sic] Halligan was attempting to point you to facts, not gossip, but when clarifying that she would adhere to the rule of the law and not disclose Grand Jury information, you threaten to leak an entire conversation. Good luck ever getting anyone to talk to you when you publish their texts.</blockquote><p>After I reached out to the department for comment on Halliganâ€™s texts, Halligan texted me yet again, for the first time in several days, insisting that our entire correspondence had been off the record. Our full exchange, linked below, allows readers to make their own judgments on that question.&nbsp;</p><p><strong>***</strong><br></p><p>As with many Trumpworld stories, this one began with someone getting sacked. That someone was Erik Siebert, Halliganâ€™s predecessor as the U.S. Attorney in eastern Virginia, who was removedâ€”or <a href="https://www.nytimes.com/2025/09/19/us/politics/erik-siebert-comey-letitia-james.html" data-sf-ec-immutable="">forced to resign</a>â€”last month.&nbsp;</p><p>According to <a href="https://abcnews.go.com/US/us-attorney-plans-resign-amid-pressure-trump-after/story?id=125750006" data-sf-ec-immutable="">multiple</a> <a href="https://www.nytimes.com/2025/09/19/us/politics/erik-siebert-comey-letitia-james.html" data-sf-ec-immutable="">reports</a>, Siebert had raised doubts that the evidence was strong enough to pursue criminal charges against two of President Trumpâ€™s longtime adversaries: New York Attorney General Letitia James, who won a civil fraud verdict against the Trump Organization, and former FBI Director James Comey, whom Trump had fired during his first term in office and had long blamed for the so-called â€œRussiagateâ€ investigation. For years, Trump has publicly said that Comey and James should be punished for <em>something</em>. But Siebert <a href="https://www.nytimes.com/2025/09/19/us/politics/erik-siebert-comey-letitia-james.html" data-sf-ec-immutable="">reportedly balked</a> at the idea of bringing criminal charges that he thought lacked merit. After news of his objections surfaced in the press, he abruptly departed.<br></p><p>The fallout was swift. That weekend, in the wake of Siebertâ€™s departure, Trump went on a social media tirade, demanding that the attorney general, Pam Bondi, move swiftly to prosecute his perceived foes. â€œTheyâ€™re all guilty as hell,â€ Trump wrote of Comey and James in a <a href="https://truthsocial.com/@realDonaldTrump/posts/115239044548033727" data-sf-ec-immutable="">Sept. 20 Truth Social post</a>â€”which, <a href="https://www.wsj.com/politics/policy/trump-doj-inside-political-enemies-17f13f72?gaa_at=eafs&amp;gaa_n=AWEtsqcfTHPLdgzc-PAS8WDsHnRql46_qbUhtVoQI_2jHzEAnNx7-HjZ-vcW9AKaI4Y%3D&amp;gaa_ts=68f5867a&amp;gaa_sig=l3iUZaUHTpvyS-LcsEihIdrkS6SCC3U5smjqlzOwEFzBi90n3wQPvHgRcDULXJnNdDGaZo4ABNjlgf8JcXAV7w%3D%3D" data-sf-ec-immutable="">according to the Wall Street Journal</a>, may have been intended as a private message. â€œJUSTICE MUST BE SERVED, NOW!!!â€<br></p><p>By the following Monday morning, Trump had installed a new top prosecutor in the Eastern District of Virginia: Halligan, his former personal attorney, who until now had never prosecuted a case. Despite her lack of prosecutorial experienceâ€”and reportedly <a href="https://www.nytimes.com/2025/09/27/us/politics/trump-comey-justice-department.html" data-sf-ec-immutable="">against the urging</a> of veteran prosecutors in her officeâ€”Halligan pressed forward. Within days of her arrival, she secured an <a href="https://www.lawfaremedia.org/article/former-fbi-director-james-comey-indicted" data-sf-ec-immutable="">indictment against Comey</a> for false statements to Congress and obstruction. Two weeks after that, she persuaded a separate grand jury to <a href="https://www.lawfaremedia.org/article/ny-attorney-general-letitia-james-indicted-in-eastern-district-of-virginia" data-sf-ec-immutable="">indict James for mortgage fraud</a>.<br></p><p>This is around when I entered the pictureâ€”on Saturday, Oct. 11.<br></p><p>Until she texted me, Iâ€™d spent much of that morning catching up on news stories about the case Halligan brought against James, which had just been handed up on Thursday, Oct. 9.&nbsp; <a href="https://www.lawfaremedia.org/article/the-justice-department-s-dangerously-weak-case-against-letitia-james" data-sf-ec-immutable="">The indictment</a> accuses the New York attorney general of misrepresenting how she intended to use a Norfolk, Virginia property when she applied for a loan to purchase it back in 2020.<br></p><p>Specifically, prosecutors say that she told lenders the property would be used as a â€œsecond homeâ€ when, in reality, she used it as a â€œrental investment property, renting the property to a family of (3).â€ By misrepresenting the purpose of the property, the indictment alleges, James was able to obtain a more favorable mortgage rate.&nbsp;<br></p><p>While many <a href="https://www.newyorker.com/news/the-lede/the-indictment-of-letitia-james-and-the-collapse-of-impartial-justice" data-sf-ec-immutable="">commentators</a> ridiculed the case, <a href="https://x.com/RogerJStoneJr/status/1970164891265778057" data-sf-ec-immutable="">a </a><a href="https://x.com/RogerJStoneJr/status/1970164891265778057" data-sf-ec-immutable="">few</a> were quick to proclaim that Halligan had an <a href="https://x.com/HansMahncke/status/1976405182092136675" data-sf-ec-immutable="">â€œairtightâ€</a> case against James.<br></p><p>I was decidedly <a href="https://www.lawfaremedia.org/article/the-justice-department-s-dangerously-weak-case-against-letitia-james" data-sf-ec-immutable="">among the skeptics</a>. Putting aside the apparently retaliatory nature of the charges, it wasnâ€™t clear to me that the facts alleged in <a href="https://www.lawfaremedia.org/article/ny-attorney-general-letitia-james-indicted-in-eastern-district-of-virginia" data-sf-ec-immutable="">the indictment</a> amounted to a crime at all. The mortgage agreement James signed, which used standard Fannie Mae language, required her to treat the property as a â€œsecond home,â€ mandating that she keep it â€œavailableâ€ for her personal use. The agreement explicitly <a href="https://www.wsj.com/articles/second-home-rider-rewrite-11555599871?gaa_at=eafs&amp;gaa_n=AWEtsqcUT8LvIU2CXmeDCoIMsludAJmjr5issT4tfkgias5sFkphmFK88OqBU7bdpYI%3D&amp;gaa_ts=68f2ab4b&amp;gaa_sig=azS6fiwCJVPkqbgw8qwqhY-r0TwcXalD5-U0w2DxFeykdbxDzOkFjzkILYm3LToufUtoVji9kLdpb3tPriUGNA%3D%3D" data-sf-ec-immutable="">allowed James to rent</a> the home after one year of ownership, and it <a href="https://www.wsj.com/articles/second-home-rider-rewrite-11555599871?gaa_at=eafs&amp;gaa_n=AWEtsqcUT8LvIU2CXmeDCoIMsludAJmjr5issT4tfkgias5sFkphmFK88OqBU7bdpYI%3D&amp;gaa_ts=68f2ab4b&amp;gaa_sig=azS6fiwCJVPkqbgw8qwqhY-r0TwcXalD5-U0w2DxFeykdbxDzOkFjzkILYm3LToufUtoVji9kLdpb3tPriUGNA%3D%3D" data-sf-ec-immutable="">allowed occasional renting</a> within the first year.&nbsp;<br></p><p>Though <a href="https://www.lawfaremedia.org/article/ny-attorney-general-letitia-james-indicted-in-eastern-district-of-virginia" data-sf-ec-immutable="">the indictment</a> notes that James declared â€œthousand(s)â€ of dollars in rent on â€œtax form(s),â€ it provides scant details about the circumstances of the supposed rental arrangement, including the timing and amount collected.&nbsp; Without knowing those details, I found it difficult to assess whether James had even violated the mortgage contractâ€”much less whether sheâ€™d committed a crime by fraudulently entering into it or making false statements on her application intending to deceive anyone.<br></p><p>All of which is why, last Saturday, I took particular interest in a <a href="https://www.nytimes.com/2025/10/11/us/politics/letitia-james-indictment-house.html" data-sf-ec-immutable="">New York Times story</a> about the Norfolk home at the center of the James case. <a href="https://www.nytimes.com/2025/10/11/us/politics/letitia-james-indictment-house.html" data-sf-ec-immutable="">The report</a> revealed that the property is occupied by Jamesâ€™s grand-niece, Nakia Thompson. Several times a year, the report said, James stays at the property with Thompson and her children, who have lived there since James purchased the home in 2020.&nbsp;<br></p><p>The Times <a href="https://www.nytimes.com/2025/10/11/us/politics/letitia-james-indictment-house.html" data-sf-ec-immutable="">further reported</a> that Thompson appeared before a Norfolk grand jury in June, testifying that â€œshe had lived in the house for years and that she did not pay rent.â€ According to the report, Thompson was not asked to testify again, and the grand jury that voted to indict James was seated in Alexandria rather than Norfolk.&nbsp;</p><p>As I saw it, the Times report tended to undermine the indictmentâ€™s central claim: that James used the home as a â€œrental investment property.â€ The evidence, assuming the Times report is accurate, would be exculpatory, I reasoned, in that it tends to show that James did not primarily use the property as a means to collect rent.<br></p><p>I decided to share the article on X, formerly Twitter, as I often do when notable developments arise in relation to the legal cases I cover for<em> Lawfare</em>. Shortly after noon, I tapped out <a href="https://x.com/AnnaBower/status/1977046307970895925" data-sf-ec-immutable="">a post</a>, linking to the Times article and summarizing it as follows:</p><blockquote>NYT reports that Letitia Jamesâ€™s great niece lives in the home that is the subject of the indictment. The niece reportedly testified before a *different* grand jury, telling them that she had lived there for many years without paying rent. James visits regularly.â€<br></blockquote><p>I included <a href="https://x.com/AnnaBower/status/1977046307970895925" data-sf-ec-immutable="">two screenshots</a> from the story, one of which highlighted this quotation: â€œMs. Thompson testified to a grand jury in Norfolk that she had lived in the house for years and that she did not pay rent.â€</p><p><img src="https://lawfare-assets-new.azureedge.net/assets/images/default-source/article-images/screenshot-2025-10-20-at-5-25-44-pm.png?sfvrsn=7447e92c_3" alt="" sf-size="100"></p><p>Not long after that, at 12:48 p.m., I followed up with a <a href="https://x.com/AnnaBower/status/1977053748511097105" data-sf-ec-immutable="">second observation</a>: â€œThis is important exculpatory evidence bc the indictment accuses James of seeking a â€˜second homeâ€™ mortgage when in reality she intended to use it as an â€˜investmentâ€™ home by renting it.â€</p><p><img src="https://lawfare-assets-new.azureedge.net/assets/images/default-source/article-images/screenshot-2025-10-20-at-5-27-50-pm.png?sfvrsn=34af51be_3" alt="" sf-size="100"></p><p>I didnâ€™t think much of the tweets at the time. Sharing other journalistsâ€™ reporting is a routine part of how I try to keep people apprised of the cases I follow.<br></p><p>But the prosecutor who <a href="https://www.cnn.com/2025/10/10/politics/lindsey-halligan-no-coordinate-letitia-james-indictment-doj" data-sf-ec-immutable="">personally presented</a> Jamesâ€™s case to the grand jury apparently saw my tweets very differently.<br></p><p><strong>***</strong></p><p>When I received the 1:20 p.m. connection request on Signalâ€”an encrypted messaging service that many journalists use to communicate with sourcesâ€”my first reaction was that the user identified as â€œLindsey Halliganâ€ couldnâ€™t actually be Halligan.<br></p><p>â€œAnna, Lindsey Halligan here,â€ the first message read. â€œYou are reporting things that are simply not true. Thought you should have a heads up.â€<br></p><p>The user on the other end had selected a setting to make all messages in the chat disappear after eight hours, so I took screenshots of the exchange as it happened to make sure a record of the back-and-forth was preserved. Those <a href="https://www.documentcloud.org/documents/26190909-signal-screenshots/" data-sf-ec-immutable="">screenshots are all available </a><a href="https://www.documentcloud.org/documents/26190909-signal-screenshots/" data-sf-ec-immutable="">here</a>.<br></p><p>Even as I took this precaution, I assumed the exchange was a hoax because, while it is not unusual for lawyers to reach out to me about my reporting or commentary, it is highly unusual for a U.S. attorney to do so regarding an ongoing prosecutionâ€”particularly in a high-profile case in which her conduct is <a href="https://www.youtube.com/watch?v=mbKHyNhaaCM" data-sf-ec-immutable="">already the subject</a> of <a href="https://www.newsweek.com/lindsey-halligan-is-already-making-mistakes-prosecuting-james-comey-10799728" data-sf-ec-immutable="">immense public scrutiny</a>. Halligan has publicly said little about the cases she is pursuing against the presidentâ€™s enemies. And I didnâ€™t believe she would message me to complain about a tweet that merely summarized other journalistic coverage of grand jury testimony.<br></p><p>With all that in mind, I hypothesized that the message was from someone impersonating Halligan, either to troll me or as a part of some kind of disinformation campaign.&nbsp;<br></p><p>But there was an easy way to validate my correspondentâ€™s claim to be Halligan. Iâ€™d met Halligan once before, a few years ago in West Palm Beach, Florida. At the time, the Justice Department was investigating Trumpâ€™s unlawful retention of classified materials and the FBI had executed a search at Mar-a-Lago. Halligan was one of the former and future presidentâ€™s personal lawyers litigating over the search and the handling of seized materials.&nbsp;<br></p><p>Our meeting was happenstance: I ran into her and another Trump lawyer, James Trusty, at a restaurant inside The Breakers hotel, where I dined after <a href="https://www.lawfaremedia.org/article/mar-lago-showdown-federal-court" data-sf-ec-immutable="">covering a hearing</a> before Judge Aileen Cannon for<em> Lawfare</em>. When I introduced myself to the pair, theyâ€™d made no secret of their dissatisfaction with how Iâ€™d characterized <a href="https://www.lawfaremedia.org/article/mar-lago-showdown-federal-court" data-sf-ec-immutable="">Trustyâ€™s presentation</a> during the hearingâ€”particularly, my use of verbs like â€œgripeâ€ and â€œcomplainâ€ to describe his arguments to the judge.&nbsp;<br></p><p>The Guardian would <a href="https://www.theguardian.com/us-news/2023/jun/01/trump-lawyers-resign-criminal-investigation" data-sf-ec-immutable="">later report</a> that Trusty had been overheard telling Halligan that he had no interest in talking to reporters from <em>Lawfare</em> on account of <a href="https://www.lawfaremedia.org/projects-series/archived-projects/the-trump-trials">our coverage</a>. Still, my conversation with them that evening was otherwise pleasant enough. And I suspected that the real Halligan would remember me.<br></p><p>So I accepted the Signal request and typed out a response, explaining that I needed some additional details to confirm her identity. â€œCan you tell me where we first met, and who you were with?â€ I asked.<br></p><p>The person purporting to be â€œLindsey Halliganâ€ responded one minute later: â€œBreakers; Trusty. Got your name from your X account.â€<br></p><p>This changed things. I had never spoken publicly about my run-in with Halligan and Trusty at The Breakers. So it seemed exceedingly unlikely that a Halligan impersonator would be able to accurately answer the question, let alone within one minute.<br></p><p>As improbable as it had seemed just minutes ago, it now appeared that I really was texting with interim U.S. Attorney Lindsey Halligan.&nbsp;<br></p><p>Later in the week, I verified that the text exchange had genuinely been with Halliganâ€”or, at least, with Halliganâ€™s phone. I obtained her cell phone number from an independent source and added the number as a contact on my phone. Signal immediately associated the phone number with the â€œLindsey Halliganâ€ account with which I had been texting.&nbsp;<br></p><p>But that was later. For now, I was pretty sure my correspondent was Halligan. So I did what any reporter would do: I started asking questions.<br></p><p>â€œOk, Iâ€™m all ears,â€ I said after thanking her. â€œWhat am I getting wrong?â€<br></p><p>â€œHonestly, so much,â€ Halligan replied. â€œI canâ€™t tell you everything but your reporting in particular is just way off.â€ She then said it was clear to her that I jump to â€œbiased conclusionsâ€ based on what I read rather than â€œtruly looking into the evidence.â€<br></p><p>I felt confident that Halligan was alluding to one of the tweets Iâ€™d posted that afternoon. Other than the tweets, I hadnâ€™t published anything of substance about the James case; my colleague, Molly Roberts, has been <a href="https://www.lawfaremedia.org/article/the-justice-department-s-dangerously-weak-case-against-letitia-james" data-sf-ec-immutable="">running</a><a href="https://www.lawfaremedia.org/article/the-justice-department-s-dangerously-weak-case-against-letitia-james" data-sf-ec-immutable=""> point</a> on that matter for <em>Lawfare</em>.&nbsp; Whatâ€™s more, Halligan had mentioned that she found my Signal contact on my X profile, suggesting that she had been looking at my tweets.<br></p><p>To confirm, I sent Halligan a link to <a href="https://x.com/AnnaBower/status/1977046307970895925" data-sf-ec-immutable="">the post</a> in which I summarized Thompsonâ€™s grand jury testimony <a href="https://www.nytimes.com/2025/10/11/us/politics/letitia-james-indictment-house.html" data-sf-ec-immutable="">as reported by</a> the Times. â€œAre you saying that something I said in this post is inaccurate?â€ I asked. â€œAnd if so, what?â€<br></p><p>Halligan replied: â€œYouâ€™re assuming exculpatory evidence without knowing what youâ€™re talking about. Itâ€™s just bizarre to me. If you have any questions, before you report, feel free to reach out to me. But jumping to conclusions does your credibility no good.â€&nbsp;</p><p>Halliganâ€™s real beef seemed to be with the Times, not me, though she wasnâ€™t saying what was wrong with the Timesâ€™s story either. I brought this up in my response, pointing out that my post explicitly credited <a href="https://www.nytimes.com/2025/10/11/us/politics/letitia-james-indictment-house.html" data-sf-ec-immutable="">the Times story</a>, not my own reporting. â€œDid they get something wrong?â€ I asked.<br></p><p>â€œYes they did but you went with it!â€ she said. â€œWithout even fact checking anything!!!!â€&nbsp;</p><p>Then she added this: â€œAnd they are disclosing grand jury info - which is also not a full representation of what happened. I guess I expect them to do that but I was surprised by you running with it.â€<br></p><p>Note that there is nothing improper about the New York Times disclosing grand jury information. The rules of grand jury secrecy, codified in <a href="https://www.law.cornell.edu/rules/frcrmp/rule_6" data-sf-ec-immutable="">Rule 6(e) of t</a><a href="https://www.law.cornell.edu/rules/frcrmp/rule_6" data-sf-ec-immutable="">he Federal Rules of Criminal Procedure</a>, donâ€™t apply to the press. And they donâ€™t bind defense attorneys or witnesses either. So if, say, Thompson or her lawyers had dished to the New York Times about her testimony and the Times had reported about her account, nobody would be violating any rule.<br></p><p>On the other hand, it is <a href="https://www.law.cornell.edu/rules/frcrmp/rule_6" data-sf-ec-immutable="">generally improper</a> for prosecutors to disclose matters that took place before a grand jury. Rule 6(e) very explicitly covers â€œan attorney for the government,â€ and it very specifically prohibits those it coversâ€”subject to certain exceptionsâ€”from â€œdisclos[ing] a matter occurring before the grand jury.â€ And the Justice Manual, which serves as the official statement of Justice Department policies, places <a href="https://www.justice.gov/jm/jm-9-11000-grand-jury" data-sf-ec-immutable="">detailed restrictions</a> on prosecutorsâ€™ use of matters before the grand jury. The matters reported on by the Times are matters of which Halligan would be intimately aware, as she is <a href="https://www.cnn.com/2025/10/10/politics/letitia-james-case-explained" data-sf-ec-immutable="">reported</a> to have personally presented the case to the grand jury.&nbsp;</p><p>Avoiding grand jury secrecy violations is one reason Justice Department officials so <a href="https://www.politico.com/story/2017/10/02/robert-mueller-russia-probe-secret-243345" data-sf-ec-immutable="">frequently offer </a><a href="https://www.politico.com/story/2017/10/02/robert-mueller-russia-probe-secret-243345" data-sf-ec-immutable="">â€œno commentâ€</a> on ongoing investigations or cases, preferring instead that the department <a href="https://www.justice.gov/archives/opa/speech/attorney-general-merrick-b-garland-delivers-opening-statement-senate-judiciary-committee" data-sf-ec-immutable="">â€œspeak through its court filings.</a><a href="https://www.justice.gov/archives/opa/speech/attorney-general-merrick-b-garland-delivers-opening-statement-senate-judiciary-committee" data-sf-ec-immutable="">â€</a> And thatâ€™s why my dialogue with Halligan struck me as so unusual. Reaching out to a reporter to complain about tweets concerning another publicationâ€™s coverage of grand jury testimony seemed uncharacteristically risky for a government lawyer.<br></p><p>I told Halligan that Iâ€™d be happy to retract or correct anything I had tweeted that was untrue. â€œBut I canâ€™t do that if I donâ€™t know what the supposed error is,â€ I said. â€œCan you be more specific?â€<br></p><p>She directed me to the indictment. â€œIt says she received thousand(s) of dollars in rent,â€ she said, presumably referring to James. Then she added: â€œI canâ€™t tell you grand jury stuff.â€<br></p><p>At this point, I broadly understood Halligan to be conveying a few things to me.&nbsp; The first is that the Times story did indeed disclose grand jury information, but that she believed some part of its account was either inaccurate or misleading or lacked the full context of what occurred. Though she hadnâ€™t explicitly said <em>what</em> grand jury information the Times had disclosed, I interpreted the focus of her ire to be the paperâ€™s account of Thompsonâ€™s testimony, which Iâ€™d summarized in my tweet.<br></p><p>The second point is that Halligan seemed to be responding in some way to the substance of Thompsonâ€™s reported testimonyâ€”specifically, that she did not pay rentâ€”by directing me to the indictment, which alleged that James reported â€œthousand(s)â€ of dollars in rental income on â€œtax form(s).â€<br></p><p>This did not seem to me to be much of a conflict. I had interpreted the Times to be saying that Thompson was not <em>currently</em> paying rent and had gone for years without doing so, not that she had <em>never</em> paid any rent at any point.<br></p><p>The indictmentâ€™s wording on this point is odd and ambiguous. It conspicuously omits the timing and the amount of money in rental income James supposedly reported on her â€œtax form(s).â€ And, while the â€œtax form(s)â€ are not publicly available, Jamesâ€™s state ethics disclosures are.<br></p><p><a href="https://whitecollarfraud.com/wp-content/uploads/2025/04/2018-to-2023-Letitia-James-Financial-Disclosures.pdf" data-sf-ec-immutable="">Those disclosures report</a> rental income on the Norfolk property only once, back in 2020, for a sum of between $1,000 and $5,000. That James might have collected a relatively small sum of rental income at some point half a decade ago does not contradict the idea that Thompson has lived there for years without paying rent. Nor does it show that James violated her mortgage agreement, given that the contract <a href="https://www.wsj.com/articles/second-home-rider-rewrite-11555599871?gaa_at=eafs&amp;gaa_n=AWEtsqdYiN0zOEWG_ZmXuGCu-GNKwV6h7TzaVn-erMKvObNnWA-xJFXhbN2Hg3-tVtc%3D&amp;gaa_ts=68f67b82&amp;gaa_sig=UFQK17cjRwHmmxL76ytAQ1iOgcGHrbi-djFnm7QlRklZPyVT0P8ss7U4l_T-XXGvD3vEgpiWtkpv3DNdKldNNQ%3D%3D" data-sf-ec-immutable="">allowed occasional rentals</a>, even during the first year.</p><p>To be sure, itâ€™s possible that the evidence at trial will show otherwise. But the indictment Halligan had filed, coupled with the publicly available facts, donâ€™t promise that it will. And the specific allegation sheâ€™d pointed me to did not on its face contradict the testimony Iâ€™d summarized in my tweet.&nbsp;</p><p>Beyond that, though, I still couldnâ€™t understand precisely <em>what </em>Halligan was trying to tell me. Was she suggesting that Thompson had lied to the grand jury? Or that the Times had mischaracterized Thompsonâ€™s testimony? Or something else entirely?<br></p><p>I explained my thinking to Halligan in my next response, writing as follows:</p><blockquote>I read the NY Times report as saying that Nakia testified that she lived in the house for years without paying rent. Though the indictment says there were thousands of dollars of rent paid *at one point,* I donâ€™t see that as inconsistent with her testimony as reported by the NY Times. Jamesâ€™s ethics disclosures report between $1,000-$5,000 in rent back in 2020. But after that, she didnâ€™t disclose additional rental income. Iâ€™m still not sure I understand whatâ€™s incorrect about the NY Times account or my summary of it.<br></blockquote><p>Halligan replied:</p><blockquote><p>Anna,</p><p>Youâ€™re biased. Your reporting isnâ€™t accurate. Iâ€™m the one handling the case and Iâ€™m telling you that. If you want to twist and torture the facts to fit your narrative, thereâ€™s nothing I can do. Waste to even give you a heads up.<br></p></blockquote><p>In turn, I reminded Halligan that it was she who had approached me to tell me that Iâ€™d compounded or repeated something inaccurate in the Timesâ€™s account of Thompsonâ€™s testimony. â€œI am happy to correct it, but I canâ€™t do so without a sense of what I supposedly got wrong,â€ I said.<br></p><p>â€œContinue to do what you have been and youâ€™ll be completely discredited when the evidence comes out,â€ she replied.</p><p>I still wasnâ€™t sure what Iâ€™d supposedly gotten wrong, or why sheâ€™d contacted me, or what to make of it all.</p><p>And for what itâ€™s worth, the Times doesnâ€™t know either. In a statement to <em>Lawfare</em>, the paper said that the Justice Department had raised no issue with the Times about its story: â€œWeâ€™re confident in the accuracy of our reporting. The DOJ declined to comment before publication and has not raised any concerns with us since the story was published more than a week ago.â€</p><p>It was clear that Halligan and I had reached a point of diminishing returns.</p><p>â€œIâ€™d love to talk further when you are at liberty to be specific,â€ I said.</p><p>And we left it thereâ€”at least for a few hours.</p><p><strong>***</strong></p><p>Later that evening, I followed up with Halligan. Sheâ€™d seemed frustrated by the end of our conversation that afternoon. But sheâ€™d also told me that I could â€œfeel freeâ€ to reach out to her if I had any questions. And I wanted to find out if she was serious about that.<br></p><p>â€œIâ€™m curious for your thoughts more broadly,â€ I wrote at 9:19 p.m. on Saturday. â€œWhat are you most frustrated that reporters arenâ€™t focusing on or writing about with respect to this case?â€<br></p><p>â€œNo frustration generally,â€ she replied. â€œYouâ€™re the only reporter I reached out to.â€<br></p><p>The next day, I tried again. I asked if I should expect additional charges against James, as <a href="https://www.wsj.com/us-news/law/trumps-favored-prosecutor-is-moving-at-full-steam-8b3cdecb?gaa_at=eafs&amp;gaa_n=AWEtsqdHnHePvJ-PT4MBAvwmJkH923hWWmBtw12kyV8tZglO8BT9Ic6LhUnXCe72q1c%3D&amp;gaa_ts=68f2af08&amp;gaa_sig=_EIG6DY_NytNy0l-whyS6zFJbDSawqC9W_bnxH1Ljrvo5WIQjD6PWVblfMOaR-NGqAsiAX7RNhHINQCOmY9lyg%3D%3D" data-sf-ec-immutable="">the </a><a href="https://www.wsj.com/us-news/law/trumps-favored-prosecutor-is-moving-at-full-steam-8b3cdecb?gaa_at=eafs&amp;gaa_n=AWEtsqdHnHePvJ-PT4MBAvwmJkH923hWWmBtw12kyV8tZglO8BT9Ic6LhUnXCe72q1c%3D&amp;gaa_ts=68f2af08&amp;gaa_sig=_EIG6DY_NytNy0l-whyS6zFJbDSawqC9W_bnxH1Ljrvo5WIQjD6PWVblfMOaR-NGqAsiAX7RNhHINQCOmY9lyg%3D%3D" data-sf-ec-immutable="">Wall Street Journal</a> had reported over the weekend.<br></p><p>â€œYou donâ€™t report fairly!â€ Halligan replied. Then she added: â€œI canâ€™t discuss any potential charges with reporters. If evidence arises that warrants further charges, Iâ€™ll look into it!â€<br></p><p>I was genuinely confused. Only the previous day, she had essentially invited me to fact-check other outletsâ€™ reporting before tweeting about them. Now she was refusing to engage when I did exactly that. â€œI thought you said I was welcome to reach out to you about what other outlets are reporting,â€ I said.<br></p><p>She wrote back: â€œWhy do you report on what other outlets are reporting? If I was you, Iâ€™d develop sources myself and out compete them all!â€<br></p><p>I expressed my confusion in my next message: â€œWhat was the purpose of reaching out to me, if not to open a line of communication? Was it just to insult my reporting?â€<br></p><p>â€œNo, it was for you to correct it, which you refused to do,â€ she replied.<br></p><p>Weâ€™d come full circle, back to my tweet summarizing the Times article. Something about it had seemed to strike a nerve with Halligan, but I couldnâ€™t figure out exactly what or why.<br></p><p>â€œI am more than happy to correct it, but you still havenâ€™t told me whatâ€™s incorrect!â€ I said. â€œWhat precisely is wrong with the tweet?â€<br></p><p>Halligan didnâ€™t respond.<br></p><p>Over the next several days, I followed up twice more, asking Halligan for her perspective on a variety of questions I had about the James case. My texts went unanswered.&nbsp;</p><p><strong>***</strong><br></p><p>Halligan had said quite a lot during our correspondence, although much of it was hard for me to parse. But there was an important thing she had <em>not</em> said during the entirety of our communications: â€œOff the recordâ€ or â€œOn backgroundâ€ or anything whatsoever about the terms on which we were talking.&nbsp;</p><p>As anyone who professionally engages with the media as routinely as Halligan would know, the default assumption when a reporter speaks with a public official is that everything is â€œon the record,â€ meaning that anything the source says can be printed with attribution. If the source wishes to speak confidentially, she can negotiate how the information will be used. â€œOn backgroundâ€ means that the information the source provides to a journalist can be published, so long as the journalist doesnâ€™t reveal the sourceâ€™s name or identifying information. â€œOff the recordâ€ means the reporter canâ€™t print what the source tells them at all. There are other variations too. But with any condition, a fundamental premise is that the reporter must <em>agree </em>to speak on that basis.<br></p><p>In the course of my work as a journalist, I frequently agree to speak with sources on background or off the record. I take my duty of confidentiality seriously and I have never burned a source by revealing confidential communications.<br></p><p>I certainly would have been willing to speak with Halligan on background or off the record. But she never raised the terms on which we were speaking at any point during the two days in which we exchanged texts.&nbsp;<br></p><p>After Saturday eveningâ€™s message, she went radio silent. By the end of the week, this much was clear: U.S. Attorney Lindsey Halligan was ghosting me.&nbsp;</p><p><strong>***</strong><br></p><p>Even now, I remain mystified by Halliganâ€™s texts to me. She is currently the most scrutinized prosecutor in the country, widely seen as hand-picked to prosecute her bossâ€™s political enemies.&nbsp; Even the slightest misstep could be seized on and picked apart by defense counsel representing James, who <a href="https://transcripts.cnn.com/show/cnc/date/2025-10-14/segment/03" data-sf-ec-immutable="">analysts expect</a> will<a href="https://www.politico.com/news/magazine/2025/10/15/letitia-james-indictment-defense-interview-00607888" data-sf-ec-immutable=""> seek dismissal</a> of her charges based on selective or vindictive prosecution.<br></p><p>Itâ€™s not as if Halligan didnâ€™t understand the rules of engagement with the press. For years, she worked as a member of Trumpâ€™s criminal defense team, which routinely courted media attention, despite <a href="https://www.nytimes.com/2025/01/08/us/politics/trumps-lawyers-jack-smith-justice-department.html" data-sf-ec-immutable="">accusing</a><a href="https://www.google.com/url?q=https://www.nytimes.com/2025/01/08/us/politics/trumps-lawyers-jack-smith-justice-department.html&amp;sa=D&amp;source=docs&amp;ust=1760992934198748&amp;usg=AOvVaw3nRuG0WRHhDM_2ia4jaIKA" data-sf-ec-immutable=""> prosecutors</a> of improper leaks. And now, as a prosecutor herself, she has accused former FBI director Comey of lying about whether he authorized a colleague at the bureau to serve as an anonymous source in news reports about an ongoing investigation. A central issue in that case, which Halligan reportedly also <a href="https://www.cnn.com/2025/09/26/politics/james-comey-indictment-trump-prosecution" data-sf-ec-immutable="">presented to the grand jury herself</a>, concerns the nature of interactions between reporters and federal law enforcement sources.<br></p><p>So the irony was hard to ignore when, just days after she texted me, Halligan <a href="https://www.cnn.com/2025/10/17/politics/lindsey-halligan-ousts-more-prosecutors-eastern-district-virginia" data-sf-ec-immutable="">reportedly fired</a> two senior career prosecutors for supposedly â€œleaking â€˜unauthorizedâ€™ information to the press.â€ That same week, she released a statement in response to <a href="https://www.msnbc.com/msnbc/news/trumps-doj-punitive-firings-public-scoldings-pressure-prosecute-foes-rcna237622" data-sf-ec-immutable="">an MSNBC story</a> about a separate case against a Democratic state lawmaker. â€œEDVA enforces a strict zero-tolerance policy on the unauthorized disclosure of information concerning ongoing investigations or cases,â€ she said. In the same story, Justice Department spokesman Chad Gilmartin is quoted saying: â€œWhile we do not confirm the existence of, nor comment on, specific investigations outside of our public filings, any Department employee who leaks deliberative or investigative developments only jeopardizes the integrity of investigations and the incredible work of federal law enforcement.â€&nbsp;</p><p>For someone so attuned to the risks of speaking out of turnâ€”and so willing to punish others for allegedly doing soâ€”Halliganâ€™s decision to reach out to me over text remains baffling.<br></p><p>She knew I was a journalist. She approached me. She invited my questions. She even encouraged me to stop chasing other reportersâ€™ stories and focus on my own.<br></p><p>Turns out, she gave me a great one.&nbsp;</p><p><strong><em>Epilogue</em></strong><br></p><p>Earlier today, I reached out to the Justice Departmentâ€™s Office of Public Affairs, seeking comment for this story. In addition to a transcript of my Signal exchange with Halligan, I also outlined some of my questions related to the story: Were these authorized communications? Is it consistent with DOJ policy or other applicable guidance for senior Justice Department officials to discuss ongoing prosecutions on Signal disappearing messages? Does the Department take the position that the contents of the messages are consistent with its legal obligations and DOJ policy? Does Halligan deny that these were in fact her texts? Does the Justice Department dispute any of the facts reported in the New York Times article discussed in my exchange with Halligan?<br></p><p>I told the Justice Department that my deadline was 4:15 p.m.<br></p><p>In response, the department sent the statement quoted at the outset of this piece. In addition, just a few minutes before the deadline, my dormant Signal connection with Halligan suddenly came alive again.<br></p><p>At 4:10 p.m., she texted me: â€œBy the wayâ€”everything I ever sent you is off record. Youâ€™re not a journalist so itâ€™s weird saying that but just letting you know.â€<br></p><p>I responded: â€œIâ€™m sorry, but thatâ€™s not how this works. You donâ€™t get to say that in retrospect.â€<br></p><p>Halligan was unpersuaded: â€œYes I do. Off record.â€&nbsp;</p><p>â€œI am really sorry. I would have been happy to speak with you on an off the record basis had you asked,â€ I said. â€œBut you didnâ€™t ask, and I still havenâ€™t agreed to speak on that basis. Do you have any further comment for the story?&nbsp;</p><p>To my surprise, she kept going: â€œItâ€™s obvious the whole convo is off record. Thereâ€™s disappearing messages and itâ€™s on signal. What is your story? You never told me about a story.â€<br></p><p>I didnâ€™t respond. It was time to publish my story. Iâ€™ll text it to her.<em></em><br></p><p><em>Postscript: Read </em><a href="https://www.documentcloud.org/documents/26190909-signal-screenshots/" data-sf-ec-immutable=""><em>the full exchange between </em></a><a href="https://www.documentcloud.org/documents/26190909-signal-screenshots/" data-sf-ec-immutable=""><em>Lindsey Halligan and </em>Lawfareâ€™<em>s Anna Bower here</em></a><em>.</em><br></p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing a $3000/mo Heroku bill with a $55/mo server (331 pts)]]></title>
            <link>https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/</link>
            <guid>45661253</guid>
            <pubDate>Tue, 21 Oct 2025 20:28:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/">https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/</a>, See on <a href="https://news.ycombinator.com/item?id=45661253">Hacker News</a></p>
<div id="readability-page-1" class="page">
<p>This content has moved. If you are not redirected, please click here:</p>
<p><a href="https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55mo-server/">blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55mo-server/</a></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doomsday Scoreboard (170 pts)]]></title>
            <link>https://doomsday.march1studios.com/</link>
            <guid>45661084</guid>
            <pubDate>Tue, 21 Oct 2025 20:14:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doomsday.march1studios.com/">https://doomsday.march1studios.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45661084">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <!-- Background -->
  

  <!-- Header -->
  <header>
    <p><img src="https://doomsday.march1studios.com/imgs/doomsdayskull.svg" alt="Doomsday skull logo">
    </p>
    
  </header>

  <!-- Main -->
    <main id="main">
      <section id="scoreboard" aria-labelledby="scoreboard-title">
        <h2 id="scoreboard-title">Scoreboard</h2>
      </section>

      <section id="timeline-stats" aria-labelledby="timeline-stats-title">
        <h2 id="timeline-stats-title">Timeline Stats</h2>
      </section>
      
      <section id="dashboard" aria-labelledby="dashboard-title">
        <h2 id="dashboard-title">Doomsday Dashboard</h2>
      </section>

      <section id="listing" aria-labelledby="listing-title">
        <h2 id="listing-title">Predictions Listing</h2>
      </section>
    </main>

  <!-- Footer -->
  

  <!-- Scripts -->
      
      
      
      
      
      
      
      


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magit Is Amazing (149 pts)]]></title>
            <link>https://heiwiper.com/posts/magit-is-awesome/</link>
            <guid>45659812</guid>
            <pubDate>Tue, 21 Oct 2025 18:39:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heiwiper.com/posts/magit-is-awesome/">https://heiwiper.com/posts/magit-is-awesome/</a>, See on <a href="https://news.ycombinator.com/item?id=45659812">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
        <h2>Magit Is Amazing!</h2>
        <p>
            I was reading a post about Jujutsu version control system the other day and that just made me realize how awesome is Magit!
        </p>
        <time>
            Published Oct 15, 2025,
            Updated Oct 15, 2025
        </time>
    </header>
    <article>
        <p>Iâ€™ve been hearing a lot about <a href="https://jj-vcs.github.io/jj/latest/" target="_blank" rel="noopener">Jujutsu</a>
 VCS lately, and when I say a lot I mean maybe 5 times this year. But that is a lot for a new VCS I would say.</p>
<p>The <a href="https://www.stavros.io/posts/switch-to-jujutsu-already-a-tutorial/?utm_source=changelog-news" target="_blank" rel="noopener">article</a>
 I came across mostly goes through the flexibility of Jujutsu VCS over <a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a>
, or at least thatâ€™s as far as I managed to read before I lost interest. I was thinking â€œ<em>you can easily do this using Git???</em>â€, but then I realized that Iâ€™ve actually never used Git for almost anything besides cloning repositoriesâ€¦ Iâ€™ve always been using <a href="https://magit.vc/" title="Magit! ğŸª„" target="_blank" rel="noopener">Magit</a>
 for almost everything.</p>
<p>Sometimes I think whether it would have been better if I started using Git CLI and then switched to Magit, that way I would really understand why itâ€™s called that way, rather than taking all the amazing work behind it for granted. But I think afterall I wouldnâ€™t have discovered Git commands so early in my career if I had just used the Git CLI from the start. I remember I was stashing my code changes back in my early career while other coworkers were struggling to move to another branch without losing their changes.</p>
<p>Iâ€™m pretty sure this isnâ€™t all what Jujutsu is about, but now I really donâ€™t feel like giving it a try anytime soon because of that article.</p>

    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Atlas (520 pts)]]></title>
            <link>https://chatgpt.com/atlas</link>
            <guid>45658479</guid>
            <pubDate>Tue, 21 Oct 2025 17:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chatgpt.com/atlas">https://chatgpt.com/atlas</a>, See on <a href="https://news.ycombinator.com/item?id=45658479">Hacker News</a></p>
Couldn't get https://chatgpt.com/atlas: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fallout from the AWS Outage: Smart Mattresses Go Rogue and Ruin Sleep Worldwide (206 pts)]]></title>
            <link>https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide</link>
            <guid>45658056</guid>
            <pubDate>Tue, 21 Oct 2025 16:50:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide">https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide</a>, See on <a href="https://news.ycombinator.com/item?id=45658056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the grand tradition of tech outages turning everyday life into a farce, Monday's massive Amazon Web Services (AWS) disruption didn't just take down <a href="https://quasa.io/media/instagram-launches-location-sharing-maps-inspired-by-snapchat">Snapchat</a>, <a href="https://quasa.io/media/ryan-kaji-the-13-year-old-multimillionaire-building-a-virtual-empire-in-roblox">Roblox</a>, and <a href="https://quasa.io/media/fortnite-set-to-return-to-iphone-in-australia-as-epic-wins-partial-victory-against-apple-and-google">Fortnite</a> - it left thousands of sleep-deprived users sweating bullets in their own beds. While the world scrambled to reload their feeds, owners of Eight Sleep's high-tech Pod3 mattress covers discovered a chilling reality: their "smart" sleep sanctuaries had no offline mode. Zero. Zilch. In a world where even your fridge can survive a Wi-Fi blackout, who knew your bed couldn't?</p>

<p><img alt="" height="188" src="https://quasa.io/storage/photos/00/photo_2025-10-20_19-32-46.jpg" width="300">The outage, which AWS confirmed stemmed from "increased error rates and latencies" in its US-EAST-1 region, rippled across the internet starting around 3 a.m. ET on October 20, 2025. By mid-morning, Downdetector had logged over eight million reports, with everything from banking apps to gaming platforms grinding to a halt.</p>

<p>But amid the chaos, one corner of the web lit up with uniquely absurd complaints: Eight Sleep's support site and social channels flooded with pleas from users whose mattresses had effectively gone on strike.</p>

<p>These $2,000+ gadgets, billed as AI-powered sleep coaches that track heart rate, monitor sleep stages, and dynamically adjust temperature via water-cooled coils, suddenly reverted to being glorified foam bricks.</p>

<p><img alt="" height="447" src="https://quasa.io/storage/photos/00/image%20-%202025-10-20T194135.824.jpg" width="300">Picture this: You're tucked in, ready for a night of optimized REM cycles, when your app pings an error. No more tweaking the chill to a crisp 55Â°F or firing up the "cool mode" for those midnight hot flashes.</p>

<p>The core temperature control? Utterly crippled without the cloud. Users reported the app freezing on loading screens, refusing to connect, and leaving them stranded in whatever thermal hell their last setting dictated.</p>

<p>Eight Sleep's system, which relies on backend servers for everything from real-time adjustments to data syncing, had no fallback. "It's unacceptable," fumed one early complainant on X, echoing the frustration of many who shelled out for "seamless" smart sleep only to face analog purgatory.</p>

<p>The hits kept coming. Smart sleep tracking? Dead in the waterâ€”no logging of phases, no biometric insights, just a void where your sleep score should be. Preset schedules, like the cheekily named "Prepare Bed for Sleep" routine that cues gentle warming or ambient vibes, fizzled out entirely, as all automations demand an internet lifeline to Eight Sleep's servers.</p>

<p>Even physical controls fared poorly: Touch panels on the Hub (the mattress's brain box) became unresponsive or glitchy, with some users noting they were "extremely inconvenient" at best - designed more as app backups than standalone saviors. And in the outage's cruelest twist, a handful of Pods straight-up froze. One Reddit thread devolved into a chorus of "my bed is bricked," with owners unable to reboot without cloud clearance.</p>

<p><img alt="" height="447" src="https://quasa.io/storage/photos/00/image%20-%202025-10-20T194138.665.jpg" width="300">Then there's the tweet that broke the internet's funny bone - and possibly a few marriages. Tech enthusiast Alex Browne, armed with an Eight Sleep Pod3, had programmed his mattress to preemptively heat up by +9Â°F above room temperature before bedtime. "I like it warm to ease in," he explained in a viral post that racked up thousands of likes and eye-rolls.</p>

<p>But when AWS went dark, the system locked into that toasty preset, disabling any cooling override. Browne spent the night marinating in his own perspiration, tweeting updates like a man betrayed: "Backend outage means I'm sleeping in a sauna.</p>

<p>Eight Sleep confirmed - no offline mode yet, but they're 'working on it'." Commenters piled on with dystopian jabs: "Next up: Subscription paywalls for your pillow fluffiness" and "Jeff Bezos is personally cranking my thermostat." By evening, as AWS reported "significant recovery," Browne's saga had morphed into a meme goldmine, spotlighting the absurdity of outsourcing your snooze to the cloud.</p>

<p>Eight Sleep isn't alone in this IoT vulnerability - AWS powers a staggering chunk of the smart home ecosystem, from Ring doorbells to Alexa plugs, all of which blinked out during the outage.</p>

<p>But mattresses? That's peak irony. Humans have napped on rocks for millennia without needing server pings, yet here we are, one faulty data center away from nocturnal disaster. The company, which touts over 50 clinical studies on its tech, has faced prior scrutiny tooâ€”like a 2024 security report uncovering exposed AWS keys that let engineers remotely SSH into users' beds, raising hackles about data privacy and backdoor access. (Imagine your ex tweaking the vibes from afar.)</p>

<p><strong>Also read:</strong></p>

<ul>
	<li><a href="https://quasa.io/media/dream-job-alert-ocean-city-police-seek-weed-volunteers-for-impairment-training-slots-filled-in-hours">Dream Job Alert: Ocean City Police Seek Weed Volunteers for Impairment Training â€“ Slots Filled in Hours!</a></li>
	<li><a href="https://quasa.io/media/courtship-ends-paramount-skydance-prepares-for-massive-layoffs">Courtship Ends! Paramount Skydance Prepares for Massive Layoffs</a></li>
	<li><a href="https://quasa.io/media/youtube-s-partner-program-loses-its-shine-selling-chocolate-bars-is-now-more-lucrative-for-bloggers">YouTubeâ€™s Partner Program Loses Its Shine: Selling Chocolate Bars Is Now More Lucrative for Bloggers</a></li>
</ul>

<p>As the dust settles - with AWS mostly back online by 6 a.m. ET and Eight Sleep restoring controls - this episode serves as a wake-up call (pun intended). Smart tech promises utopia, but without robust offline fallbacks, it's just expensive fragility. For now, if you're an Eight Sleep devotee, keep a fan handy - and maybe a low-tech backup plan. Because in the words of one sweat-soaked survivor: "What won't they cloud-ify next? My dreams?"</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Programmer Identity Crisis (189 pts)]]></title>
            <link>https://hojberg.xyz/the-programmer-identity-crisis/</link>
            <guid>45658019</guid>
            <pubDate>Tue, 21 Oct 2025 16:47:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hojberg.xyz/the-programmer-identity-crisis/">https://hojberg.xyz/the-programmer-identity-crisis/</a>, See on <a href="https://news.ycombinator.com/item?id=45658019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><svg width="250" height="250" viewBox="0 0 908 1011" fill="none" alt="Illustration
of a person with different identities.">
<path d="M774 693.5C771.85 698.99 771.985 699.204 770.385 704.274C761.775 731.494 736.335 739.074 712.295 734.594C705.725 733.374 699.305 731.064 693.015 728.714C686.565 726.304 683.275 726.724 682.765 734.914C682.265 742.924 680.685 751.054 674.965 757.534C661.345 772.964 642.765 775.014 622.375 763.244C620.185 761.984 618.035 760.634 615.995 759.164C612.975 756.994 610.965 757.994 609.025 760.774C596.615 778.574 578.955 780.584 559.805 776.094C552.615 774.404 550.745 775.024 547.905 782.874C542.005 799.214 535.995 816.124 521.225 827.634C515.505 832.084 510.565 837.584 505.575 842.894C501.145 847.614 498.875 853.034 497.585 859.814C494.305 877.084 482.515 889.424 469.845 900.794C468.385 902.104 466.585 903.084 465.305 904.534C456.205 914.774 445.025 916.564 433.285 910.854C423.715 906.204 415.205 907.494 405.795 910.814C388.635 916.864 371.875 917.374 356.605 904.934C349.395 899.054 347.165 890.554 343.335 882.854C341.835 879.844 341.005 876.344 337.205 875.384C330.275 873.624 327.845 868.494 327.715 862.174C327.575 855.134 324.165 851.514 317.595 849.704C308.605 847.234 302.675 840.894 298.815 832.764C296.795 828.514 294.395 827.644 289.705 827.794C258.275 828.754 226.855 829.304 196.265 819.684C174.725 812.914 158.385 798.174 141.115 784.864C136.395 781.234 139.355 777.414 142.265 774.074C150.295 764.884 157.595 754.924 159.115 742.664C161.135 726.274 160.625 709.904 156.215 693.604C152.645 680.414 150.975 666.704 146.115 653.814C145.055 650.994 144.945 647.724 142.265 645.504C137.985 645.744 137.705 650.024 135.425 652.194C133.455 654.074 131.335 655.174 128.615 654.494C125.645 653.744 125.385 651.274 124.665 648.744C118.335 626.424 117.945 603.244 118.005 580.604C118.125 531.054 128.105 483.724 154.545 440.284C168.705 417.024 186.985 398.374 207.305 381.264C216.475 373.544 227.325 367.884 238.225 362.654C240.695 361.464 243.555 360.734 244.755 358.374C245.315 356.314 244.405 355.214 243.425 354.214C235.295 345.924 236.035 342.974 246.835 338.644C253.705 335.894 261.155 336.814 268.235 335.034C268.705 330.374 265.035 328.484 262.985 325.694C261.045 323.044 257.835 320.714 259.975 316.874C262.195 312.894 266.175 313.484 269.755 314.084C276.585 315.224 283.365 316.694 291.975 318.384C288.045 312.974 285.165 308.934 282.205 304.964C280.355 302.484 277.915 300.044 279.535 296.694C281.365 292.904 285.165 293.464 288.465 293.714C296.935 294.344 304.965 296.794 312.725 300.204C320.095 303.444 327.505 306.584 336.095 310.284C332.735 301.634 329.835 294.144 326.915 286.674C325.575 283.254 323.795 279.554 326.795 276.444C329.485 273.644 332.955 275.714 336.255 275.964C344.415 276.584 347.955 283.904 354.205 287.164C357.085 288.674 359.295 291.414 363.905 292.264C360.835 285.814 358.115 280.114 355.415 274.394C353.895 271.184 353.045 267.934 356.145 265.154C359.035 262.554 361.885 264.224 364.725 265.484C376.185 270.544 386.475 275.894 392.695 288.504C398.265 299.794 409.775 307.124 419.425 317.674C424.165 303.854 423.585 291.254 424.105 278.874C424.285 274.524 420.175 274.324 416.625 274.044C409.075 273.444 401.465 275.254 393.965 273.434C389.785 272.424 384.865 271.474 385.815 265.864C386.645 260.944 391.305 261.274 395.325 261.354C402.055 261.494 408.795 261.614 415.535 261.494C422.895 261.354 427.065 258.164 426.425 250.044C426.035 245.034 426.125 239.934 426.545 234.914C427.185 227.364 423.435 225.704 416.825 226.424C410.985 227.064 405.075 227.234 399.185 227.334C394.575 227.414 389.535 227.204 389.475 220.844C389.415 214.644 394.485 215.254 398.635 214.904C406.455 214.244 414.265 213.534 422.085 212.854C428.445 212.294 428.395 207.714 428.415 203.014C428.445 197.724 425.425 196.544 420.805 196.454C414.365 196.334 407.935 195.754 401.505 195.274C396.705 194.924 390.555 194.794 390.915 188.464C391.325 181.314 397.895 182.844 402.825 182.864C408.195 182.884 419.871 184.503 420.125 181.574C420.805 173.744 398.105 170.604 391.155 160.234C385.805 152.254 383.285 143.564 382.655 134.224C381.455 116.464 390.245 103.894 407.375 99.0145C416.875 96.3045 426.525 95.0645 436.465 96.8045C464.025 101.604 471.075 119.944 464.515 144.564C461.275 156.714 455.435 167.504 443.565 173.744C441.405 174.874 440.275 176.814 440.715 179.334C441.295 182.684 444.005 182.604 446.555 182.844C451.855 183.334 457.195 183.644 462.435 184.514C465.265 184.984 467.815 186.714 467.485 190.214C467.185 193.324 464.495 194.284 462.015 194.544C457.005 195.064 451.935 195.364 446.905 195.314C440.545 195.244 440.595 199.364 440.675 203.834C440.755 208.284 440.785 212.344 447.105 212.324C453.545 212.304 459.995 212.744 466.415 213.274C470.995 213.644 475.885 214.394 475.935 220.494C475.985 227.384 470.425 225.434 466.625 225.584C460.175 225.844 453.715 225.694 447.255 225.644C441.415 225.604 438.515 228.854 437.745 234.234C437.305 237.274 437.025 240.414 437.265 243.454C437.635 248.224 435.095 254.684 441.115 256.924C446.925 259.094 453.295 257.994 458.555 253.864C471.265 243.894 484.515 234.874 501.315 233.874C530.195 232.144 556.805 237.464 576.335 261.104C580.035 265.584 583.045 264.364 587.235 263.504C623.805 255.994 655.125 267.234 682.625 290.974C690.035 297.364 693.345 306.684 696.825 315.504C698.975 320.974 702.145 322.954 707.655 322.184C721.045 320.304 734.545 319.544 747.925 321.254C752.375 321.824 751.5 322 754.5 323C759.5 325.5 761.025 326.714 766.165 330.584C775.965 337.964 780.525 347.614 776.615 360.064C774.825 365.754 775.895 370.044 781.435 373.634C806.585 389.964 808.765 418.684 795.745 441.664C792.065 448.154 791.215 451.034 798.015 456.304C809.625 465.304 817.645 486.634 805.655 503.174C802.595 507.394 802.285 510.994 804.085 516.214C813.435 543.384 814.955 570.834 805.495 598.504C798.475 619.044 793.095 625.964 772.025 639.044C773.065 646.684 772.025 644.5 772.5 646.5C773.078 648.932 776.5 654.5 777.305 659.334C777.832 662.5 777.712 666.297 777.5 668C777 672.026 777 672 777 674C776.5 677.5 777 677.5 777 679.5C777 681.081 777 681 776.5 683.5C775.133 690.337 774.5 691 774 693.5ZM665.715 314.614C662.455 320.794 656.045 325.304 655.135 332.434C657.335 334.464 659.075 333.984 660.725 333.984C670.545 333.994 672.245 336.524 668.405 345.544C667.635 347.344 666.705 349.074 665.825 350.834C656.485 369.634 645.655 387.654 637.415 407.054C633.285 416.784 629.835 427.014 623.455 435.684C621.815 437.904 619.745 440.224 616.405 438.214C613.155 436.264 613.945 433.674 615.225 430.924C622.805 414.664 630.565 398.484 637.835 382.084C642.625 371.274 650.295 361.774 653.365 349.664C645.865 349.454 642.515 355.474 637.175 357.674C634.015 358.974 630.955 361.134 628.075 357.694C625.205 354.254 627.415 351.194 629.805 348.814C640.455 338.184 647.915 325.314 655.505 312.554C658.505 307.504 660.935 302.114 663.385 296.764C664.565 294.204 664.005 291.784 661.185 290.394C658.685 289.164 656.885 290.284 655.385 292.244C654.185 293.804 652.925 295.314 651.835 296.944C633.855 323.654 615.885 350.374 597.985 377.144C597.015 378.604 595.115 380.074 596.655 382.674C603.875 381.034 609.595 376.334 616.025 373.094C619.205 371.494 622.685 370.654 625.245 373.864C627.485 376.684 627.615 379.944 625.435 383.214C622.185 388.104 618.855 392.994 616.105 398.164C605.455 418.174 591.685 436.454 582.905 457.514C579.515 465.634 581.215 468.324 590.005 468.474C596.795 468.584 603.325 466.984 609.905 465.514C613.905 464.624 619.215 463.424 620.535 468.924C621.765 474.054 616.835 475.834 612.935 477.494C611.675 478.034 610.225 478.154 608.835 478.364C583.235 482.254 558.995 478.484 537.005 464.434C530.055 459.994 525.905 461.474 522.825 466.554C522.515 472.424 529.655 472.854 528.315 477.504C524.825 489.604 525.765 502.354 521.515 514.464C515.325 532.084 504.255 534.904 486.855 531.324C479.215 529.754 474.155 525.334 472.565 517.704C471.195 511.084 471.025 504.264 472.695 497.684C476.525 482.534 486.275 476.854 501.625 479.844C506.195 480.734 509.715 483.054 513.125 486.414C515.355 476.454 513.555 474.114 504.765 474.794C496.955 475.394 489.235 477.084 481.315 475.324C468.545 472.494 459.565 478.954 457.765 492.164C454.625 515.284 461.705 528.794 481.935 535.934C486.835 537.664 493.13 537.5 496.295 543C498.945 545.21 497.305 547 497.305 548.614C495.825 551.554 493.335 551.694 490.755 551.094C459.325 543.784 441.725 530.254 445.475 494.874C446.075 489.234 443.965 487.684 439.615 485.814C417.535 476.324 390.395 491.774 383.605 512.214C379.875 523.444 374.015 532.844 366.245 541.544C348.715 561.154 316.735 564.524 295.945 548.644C289.935 544.054 286.095 538.304 284.565 530.644C282.045 518.054 283.625 505.654 285.615 493.384C275.435 490.184 270.985 491.444 269.305 498.164C262.195 526.544 260.345 555.494 260.955 584.584C261.285 600.234 262.825 615.874 264.325 631.464C264.845 636.904 263.445 645.864 267.605 647.084C276.675 649.754 281.325 657.424 288.715 661.304C297.535 665.944 300.865 672.674 301.805 682.084C304.655 710.494 317.965 733.634 339.295 752.184C349.435 760.994 356.185 760.774 367.045 752.824C375.995 746.274 384.925 745.814 394.355 751.394C396.675 752.764 398.835 755.304 401.895 753.054C413.535 744.524 423.485 749.674 433.015 757.024C435.125 758.654 437.095 760.124 439.775 759.654C446.925 758.394 452.045 762.374 455.625 767.274C464.275 779.144 470.145 778.114 480.095 766.954C494.595 750.694 512.135 735.884 516.095 712.324C517.185 705.874 521.715 702.084 528.875 702.834C535.295 703.504 539.615 700.754 542.195 694.934C543.705 691.524 546.015 687.234 549.345 687.184C558.255 687.024 558.385 680.434 559.825 674.824C561.705 667.514 563.355 660.114 564.585 652.664C571.755 609.594 576.635 566.244 579.795 522.704C580.015 519.664 580.915 516.984 584.185 516.364C587.325 515.774 589.555 517.614 590.915 520.334C593.555 525.614 593.495 531.044 591.885 536.654C588.995 546.754 588.815 556.294 598.435 563.764C599.405 563.064 600.535 562.604 601.145 561.774C614.205 543.774 618.355 523.134 618.025 501.494C617.925 494.784 609.825 492.454 603.735 496.884C601.515 498.494 599.895 500.924 598.015 502.984C595.795 505.434 593.335 507.324 590.015 505.044C587.135 503.064 586.585 500.294 587.435 496.894C589.695 487.814 601.305 479.464 610.365 480.634C623.945 482.374 631.065 489.594 631.115 501.554C631.175 516.004 627.765 529.834 623.285 543.394C621.435 549.014 621.505 553.254 624.685 558.964C633.485 574.744 636.925 591.504 626.815 608.404C623.655 613.684 618.865 616.124 612.635 615.484C606.305 614.834 602.545 611.244 600.655 605.304C598.425 598.304 597.165 591.084 596.945 583.834C596.745 577.474 593.955 574.764 587.575 574.264C586.975 577.674 586.275 580.934 585.825 584.234C580.335 625.024 573.085 665.484 563.575 705.564C560.095 720.264 554.225 734.334 552.525 749.504C551.975 754.424 552.455 757.284 557.495 760.254C576.035 771.154 596.955 761.424 603.225 744.154C605.315 738.384 614.115 736.784 617.255 741.604C625.045 753.554 636.815 756.834 649.885 756.684C662.285 756.534 670.615 747.564 670.945 735.084C671.065 730.574 670.535 726.144 669.215 721.794C668.235 718.574 667.045 715.044 669.975 712.314C672.865 709.624 676.215 710.994 679.485 712.044C691.495 715.904 703.355 720.464 715.615 723.254C730.535 726.644 745.065 723.604 754.175 710.784C767.015 692.694 771.815 672.274 764.455 650.304C763.565 647.654 762.605 644.794 759.725 643.654C756.025 642.184 753.155 640.264 753.705 635.754C754.265 631.174 758.205 630.394 761.605 629.284C776.645 624.424 787.695 615.294 792.775 599.994C795.065 593.084 797.455 586.104 798.635 578.964C802.545 555.424 801.645 532.644 786.115 512.644C783.375 509.114 782.935 505.124 787.785 502.814C805.845 494.204 799.995 474.684 789.305 465.314C785.615 462.074 781.915 460.494 777.095 462.884C773.095 464.874 768.805 465.964 766.225 460.854C763.655 455.754 768.205 454.054 771.195 451.734C783.625 442.094 790.875 429.984 791.035 413.724C791.135 403.894 786.865 396.434 780.715 389.684C776.105 384.634 771.125 380.294 763.135 383.264C760.265 384.324 758.425 381.514 758.825 378.624C759.285 375.344 759.865 371.854 761.365 368.974C773.635 345.474 756.955 335.204 739.745 333.244C730.225 332.154 720.745 334.434 711.255 335.264C707.875 335.564 704.585 336.954 703.965 341.074C703.415 344.704 701.515 346.734 697.745 346.324C694.025 345.924 692.755 343.234 692.235 339.904C691.885 337.694 691.475 335.454 690.725 333.364C686.925 322.714 684.465 311.334 675.485 303.454C669.885 305.194 668.895 310.044 665.715 314.614ZM361.985 470.534C389.185 464.864 414.845 455.464 437.315 438.284C438.015 438.914 438.385 438.434 438.455 437.914C438.605 436.814 438.695 435.684 438.655 434.564C437.815 415.164 440.285 396.614 451.615 380.034C454.615 375.644 450.765 368.174 445.275 368.594C438.725 369.094 437.185 365.264 435.055 360.854C427.175 344.544 416.075 330.554 403.275 317.884C401.275 315.904 399.105 312.424 395.515 314.754C392.155 316.934 394.655 319.764 395.625 322.224C396.695 324.924 397.775 327.564 394.925 329.824C392.095 332.064 389.025 331.834 386.365 329.794C382.145 326.554 378.205 322.954 374.095 319.564C365.605 312.574 365.595 312.594 357.265 321.184C357.565 324.514 362.395 327.944 357.775 331.574C353.165 335.184 349.475 332.274 345.565 329.924C341.725 327.624 337.355 326.124 333.745 323.534C328.705 319.924 325.085 320.974 321.655 325.684C321.765 329.084 326.475 332.204 322.915 335.704C319.025 339.524 315.115 336.374 311.225 335.094C307.245 333.774 303.055 333.024 299.155 331.494C292.755 328.994 288.585 333.204 287.345 337.484C285.805 342.804 282.635 344.794 278.395 346.324C275.155 347.494 273.175 349.524 275.125 352.554C279.895 359.974 273.585 361.974 269.335 363.134C249.335 368.594 231.135 377.734 214.375 389.564C199.725 399.914 187.075 412.494 175.415 426.264C169.715 433.004 170.505 441.384 166.465 448.434C161.085 457.844 155.925 467.494 152.655 478.004C146.865 496.624 138.985 514.444 140.565 535.004C142.165 555.924 147.425 575.714 153.775 595.424C164.905 597.484 166.675 599.524 167.875 611.284C168.395 616.374 168.815 621.524 171.475 627.524C174.685 620.804 177.275 615.244 179.965 609.734C183.405 602.674 188.675 602.084 193.305 608.484C196.785 613.294 198.835 618.864 200.175 624.644C203.925 640.884 207.735 657.104 213.015 673.894C215.755 666.654 215.635 660.084 215.595 653.464C215.515 640.884 217.655 639.224 229.875 642.014C234.575 636.724 231.765 630.244 232.475 624.334C232.865 621.084 231.995 616.964 236.715 616.454C240.665 616.024 242.665 619.124 244.075 622.294C244.865 624.064 244.905 626.174 245.795 627.874C248.585 633.224 249.015 640.054 256.235 644.874C255.415 629.674 253.285 616.164 252.395 602.574C250.375 571.744 249.425 541.044 255.495 510.524C259.515 490.324 259.545 490.324 240.385 483.354C240.125 483.254 239.845 483.194 239.585 483.084C235.425 481.414 229.725 479.854 231.595 474.284C233.495 468.624 239.335 470.004 243.985 471.214C246.955 471.984 249.785 473.304 252.675 474.374C258.745 476.624 264.905 478.664 271.405 478.944C301.495 480.264 331.385 479.024 361.985 470.534ZM306.075 801.284C306.905 806.634 311.795 811.714 308.165 817.294C305.825 820.894 307.105 823.484 308.485 826.624C311.415 833.264 316.665 837.154 323.285 839.454C329.365 841.574 334.535 845.064 337.155 850.954C340.105 857.594 342.015 864.564 349.845 867.624C352.425 868.634 352.625 872.174 353.665 874.784C356.205 881.194 358.095 887.794 363.015 893.294C374.585 906.264 388.405 906.534 402.915 900.834C415.615 895.844 427.455 892.454 440.055 901.344C445.235 905.004 451.615 903.204 456.505 899.124C461.665 894.814 466.705 890.304 471.325 885.444C478.315 878.104 485.735 870.244 486.695 859.974C487.575 850.554 491.525 843.674 497.265 836.914C502.735 830.454 508.835 824.664 515.295 819.304C530.385 806.814 536.185 789.174 538.725 771.364C540.745 757.194 543.285 743.274 546.585 729.414C548.595 720.974 551.795 712.764 552.095 701.654C545.855 710.034 539.715 714.754 530.905 715.204C527.435 715.384 526.485 718.884 525.315 721.854C515.905 745.794 500.655 765.434 480.075 780.714C468.645 789.194 459.505 787.854 449.875 777.534C446.255 773.654 442.605 770.814 436.705 772.294C431.135 773.684 427.435 770.914 424.045 766.304C418.665 758.984 412.585 759.264 406.855 766.214C400.485 773.944 398.525 773.944 392.245 766.254C384.355 756.574 380.665 756.214 370.765 764.134C359.785 772.924 345.075 774.064 335.065 765.184C310.495 743.394 292.645 717.554 289.635 683.624C289.365 680.594 289.365 676.124 286.285 675.594C279.555 674.424 276.655 668.364 269.675 664.634C273.725 681.334 277.125 696.804 281.285 712.074C285.755 728.504 291.795 744.544 295.545 761.114C298.535 774.204 301.865 787.144 306.075 801.284ZM297.455 530.094C297.605 533.284 298.445 536.244 301.025 538.314C311.255 546.554 323.075 549.374 335.665 546.374C361.845 540.154 374.355 514.664 375.675 493.164C375.925 489.074 377.005 484.104 372.865 481.534C368.445 478.794 364.515 482.414 360.635 483.904C360.705 499.764 358.165 503.264 343.175 507.564C339.835 508.514 337.905 510.214 336.915 513.504C336.075 516.264 336.135 520.454 331.955 520.164C328.335 519.914 327.825 515.934 326.645 513.244C323.985 507.164 327.815 503.564 332.475 501.144C335.885 499.374 339.855 498.624 343.645 497.654C346.495 496.924 349.585 496.154 349.275 492.644C348.925 488.674 345.345 488.434 342.235 488.304C335.405 488.014 329.725 490.144 325.275 495.694C322.315 499.384 317.925 502.764 314.625 496.724C312.425 492.704 310.625 490.444 305.935 491.074C301.205 491.714 299.595 495.164 298.245 498.964C294.775 508.774 295.295 518.724 297.455 530.094ZM416.445 470.024C425.055 470.584 434.165 468.934 441.975 473.504C450.295 478.374 454.095 474.154 457.985 466.944C450.415 461.154 444.425 454.034 439.625 445.714C425.415 461.704 425.415 461.684 405.135 469.294C399.905 471.254 394.725 473.364 389.525 475.414C387.395 476.254 384.855 477.144 386.535 480.094C388.225 483.064 390.605 481.344 392.175 479.954C398.755 474.104 407.225 472.834 416.445 470.024ZM611.025 600.434C612.895 604.434 615.435 605.514 618.635 601.874C624.835 594.794 622.255 572.654 614.545 566.584C610.265 568.744 608.845 572.494 609.085 576.964C609.485 584.434 610.085 591.874 611.025 600.434ZM485.115 515.584C490.115 522.564 497.425 522.434 503.845 519.664C510.815 516.664 512.045 509.534 512.025 501.714C498.285 516.594 495.915 517.454 484.345 512.154C484.505 513.004 484.655 513.824 485.115 515.584ZM491.055 503.344C492.815 502.464 494.705 501.784 496.295 500.674C499.515 498.414 503.815 495.484 500.525 491.414C497.595 487.784 492.805 488.804 489.335 492.184C485.625 495.804 483.545 500.184 483.585 505.364C486.595 506.954 487.995 504.244 491.055 503.344Z" fill="currentColor"></path>
<path d="M364.405 661.194C376.095 661.474 385.295 657.184 393.545 650.434C398.805 646.124 403.985 644.104 409.785 649.824C413.825 653.804 418.465 652.924 422.925 650.244C426.375 648.174 429.645 647.534 432.735 650.924C439.875 658.744 449.075 658.474 458.445 657.324C460.375 657.084 462.255 656.424 464.165 655.984C467.235 655.284 470.455 654.594 472.465 657.874C474.675 661.474 472.115 663.994 470.285 666.894C467.345 671.554 462.055 672.524 457.995 675.364C428.895 695.664 398.385 698.834 366.575 681.824C362.615 679.704 358.775 677.314 356.795 672.964C353.615 666.004 355.685 662.454 364.405 661.194Z" fill="currentColor"></path>
<path d="M411.715 524.415C413.315 518.535 413.335 513.214 413.065 507.884C412.855 503.824 413.605 500.524 418.535 500.134C423.335 499.754 425.785 502.425 426.885 506.705C428.045 511.255 426.145 515.545 425.755 519.965C424.585 533.345 417.505 542.214 406.265 549.134C394.785 556.204 385.335 566.004 375.565 575.244C369.965 580.544 370.585 584.014 378.225 586.674C389.065 590.454 400.185 593.404 411.195 596.684C413.345 597.324 415.635 597.584 417.685 598.444C421.395 599.994 423.865 602.685 422.315 606.955C420.895 610.845 417.895 611.294 414.065 610.114C402.805 606.664 391.455 603.515 380.165 600.165C372.385 597.855 364.715 595.304 356.675 593.734C347.605 591.954 346.165 586.694 352.805 580.494C370.035 564.444 387.145 548.234 405.945 533.984C409.085 531.624 411.095 528.955 411.715 524.415Z" fill="currentColor"></path>
<path d="M760.615 556.574C771.405 560.764 773.635 569.614 766.405 579.354C762.265 584.924 756.905 589.504 750.655 592.784C748.575 593.874 746.255 594.534 744.785 591.994C743.605 589.954 741.135 586.644 743.905 585.374C754.715 580.404 754.26 582.07 759.5 574.5C760.57 572.95 757.855 557.354 760.615 556.574Z" fill="currentColor"></path>
<path d="M682.005 412.384C678.205 403.114 679.545 395.384 685.405 390.484C689.565 387.004 694.155 387.154 698.765 389.334C702.505 391.094 703.375 393.924 700.865 398.034C695.725 397.484 689.255 395.884 689.165 404.534C689.115 409.694 694.705 413.664 692.995 419.854C687.285 420.344 685.025 416.314 682.005 412.384Z" fill="currentColor"></path>
<path d="M668.945 680.534C669.905 690.744 679.615 687.304 684.415 690.934C681.155 696.944 673.825 698.704 667.435 695.394C661.795 692.474 659.045 684.634 662.005 679.604C664.375 675.564 666.665 676.464 668.945 680.534Z" fill="currentColor"></path>
<path d="M717.435 526.234C715.825 522.024 720.365 520.724 720.455 517.004C718.555 514.844 716.255 512.474 714.255 509.874C712.785 507.964 712.535 505.664 714.395 503.804C716.415 501.784 718.555 502.764 720.345 504.264C725.315 508.424 730.035 512.664 729.145 520.194C728.355 526.924 724.845 529.084 717.435 526.234Z" fill="currentColor"></path>
<path d="M670.885 484.355C662.715 477.855 661.285 471.285 665.805 463.845C667.615 460.875 670.205 459.055 673.405 460.455C677.185 462.095 675.295 465.135 673.895 467.465C671.355 471.685 672.425 475.294 674.955 479.054C677.735 483.204 676.675 485.325 670.885 484.355Z" fill="currentColor"></path>
<path d="M686.445 617.334C685.305 611.344 690.415 602.474 678.355 604.314C677.735 598.084 680.895 596.954 685.035 597.544C691.375 598.444 694.275 602.784 694.135 608.744C694.025 613.214 694.355 618.944 686.445 617.334Z" fill="currentColor"></path>
<path d="M607.575 655.764C605.995 659.274 607.725 663.214 604.135 665.914C597.615 663.074 597.525 657.804 599.325 652.214C600.165 649.594 602.725 648.544 605.565 649.284C609.025 650.184 608.865 652.524 607.575 655.764Z" fill="currentColor"></path>
<path d="M374.445 453.514C388.535 452.164 399.815 446.304 409.515 437.324C414.035 433.144 419.235 430.774 425.175 429.884C427.555 429.524 429.695 430.284 430.645 432.764C431.405 434.744 430.275 436.184 428.575 436.724C415.805 440.774 406.765 451.364 394.165 455.854C378.595 461.404 362.615 463.154 346.265 462.794C344.065 462.744 341.655 462.794 340.245 458.704C351.565 456.974 362.645 455.264 374.445 453.514Z" fill="#CCCCCC"></path>
<path d="M241.035 511.195C242.355 505.915 243.335 501.245 244.925 496.795C245.835 494.225 246.105 489.545 250.395 491.065C254.125 492.395 252.545 496.375 251.725 499.195C247.695 513.145 245.025 527.395 242.345 541.645C241.595 545.615 242.635 550.115 239.325 553.485C233.595 548.825 233.595 548.795 241.035 511.195Z" fill="#CCCCCC"></path>
<path d="M427.405 419.254C419.095 423.974 412.395 430.104 404.125 433.784C399.085 436.024 394.065 437.004 388.785 435.054C388.215 432.744 389.765 431.704 391.245 431.134C403.105 426.594 413.275 419.104 424.045 412.654C426.475 411.194 429.585 407.734 432.045 411.224C434.755 415.074 430.365 416.874 427.405 419.254Z" fill="#CCCCCC"></path>
<path d="M226.225 519.195C228.305 510.355 226.685 501.285 232.075 493.625C237.125 499.765 235.595 513.695 229.135 523.825C226.225 523.975 226.455 521.745 226.225 519.195Z" fill="#CCCCCC"></path>
<path d="M334.675 532.204C332.885 536.174 330.375 537.314 327.105 535.214C324.555 533.574 323.295 531.184 324.615 528.074C325.545 525.894 326.975 524.014 329.585 524.514C333.335 525.234 335.485 527.474 334.675 532.204Z" fill="currentColor"></path>
<path d="M131.979 188.212C129.95 185.616 130.665 183.815 133.275 182.73C135.054 181.989 135.504 180.715 135.475 178.972C135.37 173.101 135.526 167.241 136.256 161.403C136.438 159.94 136.329 158.466 134.971 157.217C132.023 158.43 129.263 158.535 127.673 154.988C125.222 155.899 124.827 157.21 124.583 159.809C123.734 168.886 126.126 178.576 119.762 186.774C117.543 189.635 114.719 190.968 111.611 190.507C107.639 189.915 103.936 188.568 102.382 183.786C100.254 177.244 100.083 170.512 101.224 164.108C102.182 158.731 101.79 152.751 106.332 148.068C110.896 143.366 116.901 144.005 120.84 148.979C123.963 152.922 124.184 152.664 128.809 150.344C131.576 148.957 133.395 147.124 135.294 144.905C137.451 142.386 140.863 143.627 141.433 147.105C142.697 154.824 142.842 162.594 141.858 170.364C141.702 171.598 141.477 172.796 141.622 174.056C141.905 176.492 140.464 179.252 142.065 181.328C143.63 183.361 146.502 182.388 148.753 182.926C150.583 183.361 151.367 184.661 151.334 186.415C151.294 188.51 149.483 189.102 148.027 189.171C142.744 189.417 137.425 189.781 131.979 188.212ZM108.942 180.21C109.603 182.022 110.678 183.554 112.755 183.51C114.675 183.47 115.616 181.88 116.316 180.297C116.861 179.066 117.474 177.807 117.7 176.499C118.898 169.518 117.34 162.612 116.897 155.677C116.828 154.592 116.098 153.59 115.263 152.773C113.296 150.845 112.061 151.034 110.986 153.579C109.436 157.25 109.149 161.251 108.463 165.095C107.61 169.899 106.742 174.891 108.942 180.21Z" fill="currentColor"></path>
<path d="M159.623 186.404C155.858 184.364 155.513 180.758 155.339 177.494C154.831 168.066 156.446 158.88 159.718 150.028C161.261 145.853 168.177 143.076 172.567 144.404C175.155 145.189 178.176 144.724 179.85 147.694C180.354 148.587 181.534 147.984 182.348 147.639C184.094 146.899 185.808 146.085 187.569 145.287C188.698 146.55 187.979 147.251 186.955 147.981C185.216 149.219 183.586 150.616 181.807 151.793C180.195 152.86 179.712 154.182 180.659 155.805C181.661 157.518 183.005 157.09 184.399 156.153C185.31 155.543 186.229 154.777 187.612 155.093C189.453 157.224 184.94 158.219 187.049 160.401C182.616 159.541 178.721 164.297 180.231 168.548C180.986 170.68 180.736 171.892 179.447 173.591C178.013 175.479 178.583 178.086 177.809 180.374C175.123 188.285 166.957 191.004 159.623 186.404ZM167.328 181.539C171.343 181.426 172.167 178.406 172.839 175.367C174.179 169.296 173.442 163.012 174.778 156.901C175.547 153.376 174.266 152.192 170.214 151.205C167.349 150.508 166.166 152.258 165.171 154.516C162.176 161.302 161.758 168.498 161.856 175.73C161.892 178.758 163.178 181.263 167.328 181.539Z" fill="currentColor"></path>
<path d="M301.717 207.107C303.348 211.794 306.321 210.222 309.574 208.693C309.774 211.721 308.351 213.773 306.884 215.537C305.323 217.414 305.01 218.808 307.447 220.359C304.883 222.697 304.785 225.565 304.941 228.531C305.159 232.652 303.769 236.25 301.205 239.434C300.995 239.696 300.788 239.972 300.53 240.186C295.738 244.129 284.9 241.032 282.678 235.194C278.59 224.447 278.681 213.852 283.716 203.447C286.632 197.42 292.764 195.866 296.536 200.132C298.098 201.897 299.405 203.89 300.821 205.781C301.093 206.141 301.329 206.529 301.717 207.107ZM287.086 226.712C287.391 228.03 287.688 229.348 287.997 230.666C288.665 233.509 290.691 234.845 293.389 234.809C296.976 234.758 299.684 232.776 299.506 229.86C299.074 222.668 300.044 215.094 294.943 208.867C293.919 207.619 293.646 205.346 291.504 205.669C289.199 206.017 288.574 208.109 287.906 210.102C286.145 215.381 286.374 220.743 287.086 226.712Z" fill="currentColor"></path>
<path d="M234.401 142C242.969 142.196 246.117 147.864 246.851 154.421C247.751 162.477 247.863 170.664 245.736 178.616C244.418 183.542 242.501 188.161 237.451 190.749C233.943 192.55 230.491 192.383 226.972 191.33C225.371 190.851 224.58 189.341 224.108 187.776C220.985 177.443 220.953 166.899 222.401 156.356C223.356 149.37 227.277 144.389 234.401 142ZM233.453 185.572C236.024 185.107 237.494 183.423 238.714 181.23C241.989 175.333 242.102 168.9 241.753 162.499C241.597 159.616 242.004 156.584 240.831 153.8C239.898 151.585 238.769 149.436 236.042 149.051C233.424 148.68 232.331 150.815 230.981 152.416C230.099 153.462 229.699 154.747 229.318 156.076C226.951 164.365 227.735 172.883 227.484 181.309C227.383 184.613 229.035 185.634 233.453 185.572Z" fill="currentColor"></path>
<path d="M188.897 180.914C188.792 174.085 189.565 167.597 190.499 161.149C191.276 155.79 192.466 150.493 196.486 146.383C199.935 142.861 209.774 142.618 213.401 145.951C216.066 148.398 215.95 151.811 215.859 154.795C215.561 164.365 214.693 173.95 211.814 183.136C210.126 188.517 206.252 192.717 200.261 191.857C194.235 190.997 189.638 187.003 188.897 180.914ZM197.52 158.386C197.012 164.66 194.83 170.654 194.496 176.968C194.296 180.725 196.649 184.218 200.214 184.959C203.7 185.681 206.535 183.942 207.33 179.662C208.783 171.837 211.201 164.151 211.016 156.058C210.91 151.397 210.838 151.233 206.314 151.262C205.649 151.266 204.934 151.208 204.324 150.972C202.335 150.202 201.013 151.077 200.084 152.689C199.129 154.33 197.902 155.866 197.52 158.386Z" fill="currentColor"></path>
<path d="M294.619 144.273C296.028 144.186 297.161 144.179 298.283 144.063C304.952 143.38 307.951 148.369 308.924 153.506C310.482 161.73 309.353 169.906 307.639 177.988C306.528 183.234 302.901 186.153 297.549 186.662C294.743 186.927 292.042 187.355 289.264 186.066C286.853 184.948 285.198 183.155 285.013 180.802C284.366 172.502 282.871 164.224 283.836 155.852C284.573 149.48 288.095 145.657 294.619 144.273ZM301.561 152.544C298.04 150.279 293.828 150.783 292.107 153.906C290.684 156.487 289.918 159.359 290.299 162.354C290.854 166.718 290.876 171.097 290.807 175.476C290.764 178.155 291.646 180.156 294.445 180.755C297.949 181.506 300.744 180.217 301.779 177.56C303.99 171.87 303.91 165.916 303.925 159.954C303.932 157.38 303.413 155.046 301.561 152.544Z" fill="currentColor"></path>
<path d="M96.1696 221.821C96.8849 216.542 98.8854 211.96 100.933 207.484C103.278 202.361 108.528 202.223 113.223 206.699C117.202 210.497 119.203 215.282 119.45 220.572C119.729 226.596 120.208 232.666 117.932 238.606C116.694 241.834 114.995 244.716 113.06 247.432C110.13 251.542 105.882 250.464 101.91 249.393C98.6167 248.503 97.1426 246.23 96.7178 242.716C95.8828 235.85 95.89 229.003 96.1696 221.821ZM102.502 232.002C102.658 234.815 102.723 237.637 103.013 240.436C103.155 241.805 103.772 243.021 105.497 243.195C107.174 243.366 108.42 243.155 109.541 241.558C114.682 234.253 114.007 226.214 112.624 218.132C112.254 215.961 110.947 213.957 109.509 212.193C108.267 210.668 107.181 210.635 106.198 212.548C104.542 215.773 103.224 219.084 102.61 222.682C102.113 225.608 100.737 228.476 102.502 232.002Z" fill="currentColor"></path>
<path d="M248.568 241.525C247.613 230.081 246.034 218.818 251.632 208.242C252.053 207.447 252.427 206.615 252.946 205.886C255.669 202.052 260.977 200 264.819 201.249C267.6 202.153 271.027 207.712 270.639 212.025C269.982 219.29 271.692 226.581 269.833 233.871C268.943 237.356 267.702 240.584 265.679 243.34C261.537 248.986 252.231 247.994 248.568 241.525ZM264.793 222.358C264.481 218.677 264.147 214.995 263.868 211.31C263.777 210.108 263.443 209.095 262.314 208.518C260.952 207.824 259.997 208.616 258.977 209.393C257.946 210.184 257.434 211.255 256.842 212.356C252.91 219.671 254.25 227.739 253.593 235.498C253.407 237.68 254.976 239.444 257.507 239.858C259.965 240.261 261.816 239.593 262.491 237.073C263.733 232.448 265.665 227.924 264.793 222.358Z" fill="currentColor"></path>
<path d="M195.193 247.995C188.295 244.222 186.026 238.105 186.675 231.119C187.413 223.161 189.544 215.406 193.458 208.329C196.054 203.638 199.477 203.482 204.092 205.722C210.74 208.95 211.553 215.042 212.41 220.873C213.484 228.215 211.891 235.425 208.409 242.04C205.156 248.22 202.327 249.421 195.193 247.995ZM195.044 238.62C198.817 243.311 201.808 242.955 203.91 237.462C204.705 235.378 205.087 233.141 205.78 231.014C207.871 224.609 206.993 218.971 201.837 214.247C200.127 212.679 198.74 212.352 197.589 214.643C196.787 216.237 196.104 217.9 195.498 219.581C193.28 225.691 191.65 231.842 195.044 238.62Z" fill="currentColor"></path>
<path d="M276.481 182.399C274.52 185.565 273.035 188.716 269.789 190.604C266.917 192.278 264.423 192.224 261.558 190.604C254.834 186.807 254.261 180.217 253.683 173.7C253.164 167.836 253.912 162.013 255.165 156.273C256.152 151.752 258.857 148.267 262.55 145.689C267.211 142.436 275.035 145.921 277.613 152.221C277.955 153.052 278.314 153.923 278.39 154.802C279.193 164.013 279.625 173.206 276.481 182.399ZM273.002 174.64C273.623 169.361 273.347 164.06 273.209 158.778C273.133 155.909 270.167 152.239 267.887 152.014C264.659 151.694 263.359 154.149 262.306 156.538C258.915 164.231 259.405 172.346 260.299 180.377C260.564 182.755 262.782 184.85 265.265 185.362C267.549 185.834 269.172 184.073 270.406 182.167C271.79 180.032 272.407 177.654 273.002 174.64Z" fill="currentColor"></path>
<path d="M176.698 246.259C171.18 250.859 160.48 249.611 156.635 243.903C154.827 241.216 154.729 237.793 154.932 234.725C155.415 227.442 156.149 220.166 158.679 213.191C161.595 205.16 170.054 203.047 175.725 209.481C178.848 213.024 180.601 217.711 180.583 222.551C180.561 228.694 181.611 234.924 179.334 240.977C178.659 242.781 178.096 244.564 176.698 246.259ZM175.032 225.452C175.536 220.565 171.891 217.519 169.669 213.917C168.631 212.229 166.649 212.592 165.454 214.364C164.768 215.38 164.147 216.484 163.715 217.628C161.576 223.292 161.827 229.373 160.774 235.229C160.476 236.881 161.348 238.413 162.306 239.59C164.278 242.008 166.957 243.053 170.138 242.603C172.857 242.218 172.948 239.902 173.892 238C175.838 234.086 175.261 230.096 175.032 225.452Z" fill="currentColor"></path>
<path d="M226.657 220.202C224.75 218.739 224.09 220.91 222.823 221.298C221.363 221.749 220.045 223.099 218.633 221.291C217.38 219.686 217.725 218.024 218.796 216.608C221.105 213.561 223.512 210.588 225.999 207.683C227.136 206.358 228.693 205.494 230.523 206.079C232.208 206.616 232.32 208.177 232.498 209.673C233.323 216.644 232.952 223.618 232.625 230.597C232.556 232.06 232.342 233.519 232.368 234.979C232.451 239.717 232.887 240.12 237.607 239.808C240.203 239.637 242.091 240.345 242.45 243.231C242.864 246.524 240.638 246.662 238.213 247.033C233.82 247.704 229.438 247.581 225.037 247.461C223.2 247.41 221.472 246.942 219.802 246.209C218.255 245.533 217.689 244.295 217.707 242.683C217.725 241.111 218.524 239.829 219.969 239.728C226.965 239.234 227.085 234.256 227.085 229.173C227.085 226.254 227.47 223.313 226.657 220.202Z" fill="currentColor"></path>
<path d="M327.982 145.094C329.877 150.257 329.38 155.325 328.995 160.394C328.653 164.881 328.414 169.376 328.065 173.86C327.887 176.151 328.486 177.425 331.141 177.574C333.911 177.727 333.907 177.981 335 182.621C330.349 184.422 326.068 187.544 320.466 185.976C318.887 185.533 317.289 185.26 316.814 183.46C316.349 181.702 317.729 180.406 318.872 179.92C321.581 178.769 321.298 176.728 321.588 174.437C322.253 169.155 322.398 163.883 322.282 158.586C322.256 157.391 322.746 156.095 321.599 155.071C319.947 154.828 319.294 156.189 318.35 157.079C316.589 158.742 314.57 158.869 312.893 157.279C311.266 155.736 312.01 153.724 313.31 152.421C316.255 149.476 319.384 146.706 322.554 144.001C324.522 142.317 326.315 143.09 327.982 145.094Z" fill="currentColor"></path>
<path d="M141.252 222.115C140.373 227.198 140.765 232.096 139.513 236.845C138.906 239.15 139.752 241.071 142.769 241.162C144.806 241.223 146.295 242.425 146.382 244.491C146.462 246.364 144.69 246.673 143.274 247.003C139.967 247.78 137.273 250.158 133.823 250.554C132.818 250.67 131.819 250.986 130.821 250.961C128.711 250.91 126.562 250.609 126.33 247.911C126.097 245.181 127.971 244.513 130.236 244.397C133.351 244.237 134.68 242.734 134.049 239.553C133.148 235.037 134.237 230.574 134.593 226.098C134.822 223.215 135.283 220.329 134.274 217.558C132.273 217.315 132.121 218.916 131.431 219.802C130.592 220.884 129.862 222.035 128.421 222.373C127.354 222.623 126.417 222.384 125.64 221.563C124.859 220.735 125.28 219.969 125.589 219.083C127.016 214.963 129.423 211.459 132.266 208.22C133.497 206.819 134.942 205.679 136.946 206.006C139.161 206.365 140.094 208.162 140.62 210.025C141.713 213.881 141.397 217.856 141.252 222.115Z" fill="currentColor"></path>
<path d="M321.632 213.02C321.548 211.648 321.762 210.479 321.015 209.687C320.169 209.422 319.776 209.862 319.319 210.178C317.921 211.151 316.476 212.363 314.835 210.708C313.383 209.241 313.888 207.495 314.733 206.01C316.69 202.564 319.127 199.492 322.481 197.256C324.406 195.974 325.778 196.624 326.399 198.614C327.034 200.643 327.612 202.767 327.728 204.873C328.149 212.516 328.487 220.184 327.539 227.808C327.183 230.676 327.445 232.692 330.77 233.247C332.803 233.588 333.421 235.218 333.475 237.023C333.526 238.798 332.241 239.851 330.756 239.938C325.662 240.232 320.586 242.168 315.441 240.091C313.71 239.39 311.974 238.925 312.01 236.569C312.047 234.126 314.022 233.85 315.685 233.28C317.446 232.677 319.936 233.94 321.007 232.038C321.821 230.593 321.588 228.549 321.777 226.766C322.249 222.275 322.245 217.784 321.632 213.02Z" fill="currentColor"></path>
</svg><p><span data-astro-cid-dlsoufvo="">I am a programmer.</span>  A coder. A keyboard cowboy. A hacker. My day is spent
punching keys; catalyzing code. Itâ€™s fun; itâ€™s my identity. The editor, Vim, is
my workshop, my sanctum<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>. Here, I hone my craft, sharpen my tools, expand my
capabilities through curiosity, and for a while, escape into a trance-like
flow. A full-screen terminal window with nothing between me and thought but
INSERT mode. At the altar of Bram<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>, I spin realityâ€™s yarn out of thin air
into bits beaming through silicon. A completely imagined, non-tangible world
with IRL ramifications. A place in which I find comfort in craft and
creativity. Time disappears into puzzle-solving. Where connecting pieces
matters more than completing a picture. Craft springs from fingers to buffer. I
program and fade away into flow and composition.</p><p>In the late 1950s at MIT, a new and electrifying culture was emerging. Hands-on,
experimental, and anti-establishment. I like to imagine myself there, sitting
at the slate-blue L-shaped console. Typing away at the Flexowriter<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> as it
spits out punched paper tape programs to be fed to the nearby wall of metal
uprights, tangled wire, and early transistors; the â€œTixoâ€<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>. Waiting with
bated breath, as enthralling beeps emanate from the machinery while it runs the
program: will it succeed? I imagine the Hackersâ€”as they came to be knownâ€”around
me, pointing at code and offering advice on how to achieve â€œThe Right
Thingâ€<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: the perfect program, pristine, elegant, and succinct. I can sense
the original culture of programming pouring out of them as they passionately
embody <a href="https://en.wikipedia.org/wiki/Hacker_ethic">â€œThe Hacker Ethicâ€</a> while
sharing stubs of their own paper programs to guide me on my quest.</p><p>It was thereâ€”in the computing crucible of building 26â€”that the craft of coding
was cast. Nearly 70 years ago, members of the Tech Model Railroad Club immersed
themselves in the language of machines to pursue a mastery of digital wizardry.
The sublime magic of manipulating formal languages to solve increasingly
challenging cryptic conundrums andâ€”core to the cultureâ€”sharing findings with
other students of the dark arts of software sorcery.</p><p>The ghosts of ancient Hackers past still roam the machines andâ€”through the
culture they establishedâ€”our minds. Their legacy of the forging of craft
lingers. A deep and kinetic craft weâ€™ve extended and built a passionate
industry on. We are driven by the same wonder, sense of achievement, and
elegance of puzzle-solving as they were. Still driven by â€œThe Right Thing.â€
These constitutional ideas, the very identity of programmers, are increasingly
imperiled. Under threat. The future of programming, once so bright and
apparent, is now cloaked in foreboding darkness, grifts, and uncertainty.</p><hr><p>In fact, if we are to trust the billion-dollar AI industry, the denizens of
Hacker News (and its overlords), and the LinkedIn legions of LLM lunatics, the
future of software development has little resemblance to programming.
Vibe-codingâ€”what seemed like a meme a year agoâ€”is becoming a mainstay.</p><p>Presently (though this changes constantly), the court of vibe fanatics would
have us write specifications in Markdown instead of code. Gone is the deep
engagement and the depth of craft we are so fluent in: time spent in the
corners of codebases, solving puzzles, and uncovering well-kept secrets.
Instead, we are to embrace scattered cognition and context switching between a
swarm of Agents that are doing our thinking for us. Creative puzzle-solving is
left to the machines, and we become mere operators disassociated from our
craft.</p><hr><p>Someâ€”more than I imaginedâ€”seem to welcome this change, this new identity:
â€œSpecification Engineering.â€ Excited to be an operator and cosplaying as Steve
Jobs to â€œPlay the Orchestraâ€. One could only wonder why they became a
programmer in the first place, given their seeming disinterest in coding. Did
they confuse Woz with Jobs?</p><p>I canâ€™t imagine (though perhaps Iâ€™m not very imaginative) that Prompt, Context,
or Specification â€œEngineeringâ€<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup> would lead to a bright and prosperous
profession for programmers. It reeks of a devaluation of craft, skill, and
labor. A new identity where our unique set of abstract thinking skills isnâ€™t
really required; moving us into a realm already occupied by product managers
and designers.</p><p>Inside companies, power dynamics are shifting as this new identity is pushed.
In a mad dash to increase productivity in the wrong place<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>, developers are
forced to use LLMs in increasingly specific ways. Conform or be cast out. Use
the products that herald our obsolescence, or resign. Scarcely has management
mandated such specifics of our tools before. Tools, like those of a chef or
carpenter, that weâ€™ve taken great pride in curating and honing ourselves:
careful configuration of our editor, tinkering with dot files, and dev
environments. As part of the craft, weâ€™ve been dedicated and devoted to
personalizing our toolsets to match our thinking. It feels like a violation to
have this be decreed by management, who have little to no connection to the
day-to-day, and who should instead be concerned with outcomes, process, and
facilitating creativity. For decades, programmers have been pampered within
companies. These narratives offer a new way for management to tip the balance
back in their favor.</p><hr><p>Someâ€”with glee and anticipationâ€”liken LLMs and their impact to the transition
from low-level to high-level languages: from Assembly to Fortran. This, I
think, is wrong in a couple of ways: firstly, because the leap we made with
Fortran was rooted in programming, Fortran didnâ€™t try to eliminate a craft but
built on top of it. Fortran didnâ€™t remove the precision and expressibility of
programmatic formalisms, but expanded them. Secondly, Fortran was always
successful in producing the right outcome given its input. None of these things
is true in the world of LLMs. I can practically hear the cultists cry out:
â€œYouâ€™re just using it wrongâ€ as they move the goalposts to fit an ever-changing
narrative. But we canâ€™t expect AI tools to have the same outcomes as
programming languages. They are designed based on a different set of rules and
parameters.</p><p>There arenâ€™t enough swear words in the English language to adequately describe
how frustrating computers and programming can be, but we have at least always
been able to count on them for precision: to perform exactly as instructed
through programming. It is perhaps because of our reliance and trust in the
precision of computers that we seem so primed to believe chatbots when they
gaslight us into thinking they did what we asked of them<sup><a href="#user-content-fn-8" id="user-content-fnref-8" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup>.</p><p>LLMs and the work with them are naturally imprecise. Both in the properties of
Large Language Models, and in the very manner we instruct them:
misinterpretable natural languages. Curious that we chose this approach to
computing, given how much we, programmers, cringe at non-determinism. We prefer
predictability, compositionality, idempotence, and integration tests that
arenâ€™t flaky. LLM code represents the opposite of that: inconsistent chaos.</p><p>Dijkstra, in <a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD06xx/EWD667.html">â€œOn the foolishness of â€˜natural language
programmingâ€™,â€</a>
wrote, rather poignantly: â€œWe have to challenge the assumptions that natural
languages would simplify work.â€ And: â€œThe virtue of formal texts is that their
manipulation, in order to be legitimate, need to satisfy only a few simple
rules; they are, when you come to think of it, an amazingly effective tool for
ruling out all sorts of nonsense that, when we use our native tongues, are
almost impossible to avoid.â€</p><hr><p>Thereâ€™s a movement to distance AI-assisted development (Agents in the driverâ€™s
seat) from vibe-coding by imposing rigor and bureaucracy, but it ignores the
fundamental nature of the beast. I find that I donâ€™t read the code an LLM
generates for me as closely as I would if I had written it myself or had
reviewed it in a PR. There seems to be something innate to LLM coding that
makes my eyes glaze over. I gloss. Overwhelmed and bored. Blindly accepting
spiked pitfalls, provided CI passes and the program compiles. Not checking if
the tests are even set up to run, or if it pulled in a nonexistent library or
implemented a whole one itself. Of course, I pay the price later, when I fall
into my own trap and realize that hours of work were built on a broken bedrock.
Or maybe I donâ€™t notice till someone calls me out in a pull request, a bug
report, or when Iâ€™m paged for an incident.</p><p>A review or synopsis of a book can never replace the experience of reading it
yourself: contemplating ideas for hours and 100s of pages as each sentence is
carefully consumed. In the same way, skimming summaries of completed AI tasks
robs us of forming a deep understanding of the domain, the problem, and the
possible solutions; it robs us of being connected to the codebase. Taking the
plunge into the abyss of oneâ€™s ignorance to reveal, learn, and understand a
topic and its implications is both gratifying and crucial to good software.
Ownership, agency, and deep, fulfilling work have been replaced with scattered
attention spent between tabs of Agents.</p><p>Joan Didion, the great American essayist, famously wrote: <a href="https://lithub.com/joan-didion-why-i-write/">â€œI write entirely to
find out what Iâ€™m thinking, what Iâ€™m looking at, what I see and what it
means.â€</a> Peter Naur explores this
same concept in his work, <a href="https://pages.cs.wisc.edu/~remzi/Naur.pdf">â€œProgramming as Theory
Building.â€</a> Naurâ€™s â€œTheoryâ€ embodies
the understanding of a codebase. How it operates, its formalisms, and its
representations of the real world. A context and insight that is only gained
from immersion. Naur describes the â€œTheoryâ€ as the primary outcome of
programming, the actual product, as opposed to the software it resulted in.
Only with a well-developed â€œTheoryâ€ can one effectively apply extensions and
bug fixes to codebases. With the ambivalent glances at code that comes with
vibing, building such a theory is difficult. Naur would deem it impossible, Iâ€™m
sure.</p><p>Good design emerges from immersion. From steeping. From back-and-forth work in
the text buffer and, often, away from the keyboard. Itâ€™s impossible to hold a
whole codebase in our minds. We must dive into modules, classes, and functions
to sharpen our blurry mental models. Read and write code to <a href="https://ieeexplore.ieee.org/document/11119124">extend our
cognition</a>, regain familiarity,
and understanding of the problem domain.</p><p>Once a semblance of context has been conjured, and through a plentitude of poor
attempts, we can finally uncover the solution. The dissonance of bad design
must be felt: itâ€™s only when we write repulsive and repetitive code that we
realize that there is a better, more succinct, elegant, compositional, and
reusable way. It causes pause. A step back to think about the problem deeply.
Start over. Rinse repeat. Diametrically, AI Agent work is frictionless; we
avoid alternative solutions and canâ€™t know if what we accept is flawless,
mediocre, terrible, or even harmful. Quality is crafted by iterationâ€”how else
might we imagine good designs if we never explore objectionable ones?</p><hr><p>The cognitive debt of LLM-laden coding extends beyond disengagement of our
craft. Weâ€™ve all heard the stories. Hyped up, vibed up, slop-jockeys with
attention spans shorter than the framework-hopping JavaScript devs of the early
2010s, sling their sludge in pull requests and design docs, discouraging
collaboration and disrupting teams. Code reviewing coworkers are rapidly losing
their minds as they come to the crushing realization that they are now the
first layer of quality control instead of one of the last. Asked to review;
forced to pick apart. Calling out freshly added functions that are never
called, hallucinated library additions, and obvious runtime or compilation
errors. All while the authorâ€”who clearly only skimmed their â€œownâ€ codeâ€”is
taking no responsibility, going â€œwhoopsie, Claude wrote that. Silly AI, ha-ha.â€</p><p>Meddling managers and penny-pinching execs are pushing (hopefully unknowingly)
for fewer human interactions on teams. Isolated and bereft of connection, we
are now empowered and encouraged to build walls around our work experience.
Reaching for LLMs rather than people when we need a pair programmer, someone to
ping pong solutions with, prototype, sketch architectures with, or help answer
expert questions about esoteric parts of the codebase. We no longer require
onboarding buddies, mentors, or peers; instead, we can talk to machines. With
LLMs, avoiding human contact is so easy that it might just become the norm. The
future really <em>is</em> brightâ€¦</p><hr><p>Itâ€™s disturbing how agreeable we are to the AI hype narrative and actively
participate in the planned erasure of our craft<sup><a href="#user-content-fn-9" id="user-content-fnref-9" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup>, and so willingly offer up
our means of thinking. We were the lucky ones who got to earn a living from our
hobbies. Even if we produce punctilious and rigid processes to counter slopâ€”as
some support with a striking similarity to the waterfall model of yoreâ€”weâ€™ve
still outsourced the fun part of the job and replaced it with directorial
drudgery. Whatâ€™s next, TPS reports?</p><p>LLMs seem like a nuke-it-from-orbit solution to the complexities of software.
Rather than addressing the actual problems, we reached for something far more
complex and nebulous to cure the symptoms. I donâ€™t really mind replacing <code>sed</code>
with Claude or asking it for answers about a library or framework that, after
hours of hunting through docs, I still seek clarity on<sup><a href="#user-content-fn-10" id="user-content-fnref-10" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup>. But I profoundly
do not want to be merely an operator or code reviewer: taking a backseat to the
fun and interesting work. I want to drive, immerse myself in craft, play <em>in</em>
the orchestra, and solve complex puzzles. I want to remain a programmer, a
craftsperson.</p><p>I prefer my tools to help me with repetitive tasks (and there are many of those
in programming), understanding codebases, and authoring correct programs. I
take offense at products that are designed to think for me. To remove the
agency of my own understanding of the software I produce, and to cut
connections with my coworkers. Even if LLMs lived up to the hype, we would
still stand to lose all of that and our craft. Humans matter more than machines
and their backing corporations, who are profiting while the rest of us chase
the new American Dream they sell. As payment, we offer our critical thinking
skills, our fun, our craft, our privacy, and perhaps, our planet.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Your Own Database (348 pts)]]></title>
            <link>https://www.nan.fyi/database</link>
            <guid>45657827</guid>
            <pubDate>Tue, 21 Oct 2025 16:31:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nan.fyi/database">https://www.nan.fyi/database</a>, See on <a href="https://news.ycombinator.com/item?id=45657827">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><p>A step-by-step guide to building a key-value database from scratch.</p></header><p>If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a <strong>key-value database</strong> from the ground up.</p>
<p>A key-value database works more or less like objects in JavaScriptâ€”you can store values using a key and retrieve them later using that same key:</p>
<div><pre><p><span>$</span><span> </span><span>db</span><span> </span><span>set</span><span> </span><span>'hello'</span><span> </span><span>'world'</span></p><p><span>$</span><span> </span><span>db</span><span> </span><span>get</span><span> </span><span>'hello'</span></p><p><span>world</span></p></pre></div>
<p>Let's find out how they work!</p>
<hr>
<div><h2>The Humble File</h2><p>Databases were made to solve one problem:</p><div><header><h4>Problem</h4></header><p>How do we store data <strong>persistently</strong> and then <strong>efficiently</strong> look it up later?</p></div><p>The typical way to store any kind of data persistently in a computer is to use a <span> <em>file</em> </span>. When we want to store data, we add the key-value pair to the file:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>When we want to look for a specific key, we iterate through the pairs to see if there's a matching key:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>For updates, we'll find the key and replace the value in-place:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>And for deletes, we'll delete the record from the file:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>Easy! We're done right?</p></div>
<hr>
<h3>Mutable Updates</h3>
<p>This approach, simple as it is, doesn't actually work very well in practice. The problem lies with the way we're doing updates and deletesâ€”they're wholly inefficient.</p>
<p>To a computer, our file looks something like thisâ€”nothing more than a long sequence of bytes:</p>
<div><p>001:Loremâ£ipsum\n018:dolorâ£sit\n005:adipiscingâ£elit.\n014:Vestibulumâ£varius\n002:velâ£mauris\n007:consecteturâ£adipiscingâ£elit.\n010:Vestibulumâ£varius\n016:velâ£mauris\n003:consecteturâ£adipiscingâ£elit.</p></div>
<p>When we go to update or delete a record, we're currently updating that record in-place, which means we potentially have to <em>move</em> all of the data that comes after that record:</p>
<div><p><span>001:Loremâ£ipsum\n018:dolorâ£sit\n<span>005:adipiscingâ£elit.<span><span>â£velâ£mauris</span></span></span><span>\n014:Vestibulumâ£varius\n002:velâ£mauris\n007:consecteturâ£adipiscingâ£elit.\n010:Vestibulumâ£varius\n016:velâ£mauris\n003:consecteturâ£adipiscingâ£elit.</span></span></p></div>
<p>In this case, updating the record <code>005</code> to "<code>adipiscingâ£elit.â£velâ£mauris</code>" means moving all of the records that come after it by 11 bytes (the length of the added string "<code>â£velâ£mauris</code>"). This can quickly get really costly, especially as our database grows in size!</p>
<hr>
<div><h3>Append-Only Files</h3><p>One way to work around the update problem is to <strong>make records immutable</strong>. In other words, we add the constraint that we can only <em>add</em> new records to the end of the file and never update or delete existing ones.</p><p>With this approach, updates are treated the same as insertsâ€”just add a new record to the end of the file:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>But now we have another problemâ€”there are duplicate keys in the file!</p><p>To work around this, we have to change our search algorithm to look for the <em>last</em> occurrence of the key instead of the first:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like <code>null</code>:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li></ol></div><p>And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.</p></div>
<hr>
<p>Now this implementation isn't perfect; right now, there are two major issues:</p>
<ol><li><p><strong>The file can get very large</strong>. Since we're only appending to the file, the file will grow infinitely over time. Not good!</p></li><li><p><strong>Searching is slow</strong>. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!</p></li></ol>
<p>How can we fix these problems?</p>
<hr>
<div><h2>Keeping Files Small</h2><div><header><h4>Problem</h4></header><p><strong>How do we make sure the file doesn't grow indefinitely?</strong> Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.</p></div><p>Take a look at our database here after a few updates and deletes:</p><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 "Lorem ipsum"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 "dolor sit"</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 7 "adipiscing elit."</span></li><li><span>$ <!-- -->db<!-- --> </span><span>delete</span><span> 7</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 10 "consectetur adipiscing elit."</span></li><li><span>$ <!-- -->db<!-- --> </span><span>delete</span><span> 1</span></li></ol></div><p>Our database file has six entries, but only two represent actual recordsâ€”the rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!</p></div>
<hr>
<h3>Segments and Compaction</h3>
<div><p>Here's an idea: once a file exceeds a certain size, we'll <span> <em>close</em> </span> the file and create a new one. While the new file ingests new data (in the same way we've been doing so far), we'll <em>compact</em> the old file by deleting all of its irrelevant data.</p><p>Meaning, we stop adding new data to the file.</p></div>
<p>Here, we've set the maximum file size to seven records. Notice that the database is fullâ€”try clicking on "Add" to add a new record and notice what happens:</p>

<p>Now, our database consists of two different files which we'll call <strong>segments</strong>. Each segment will usually become a lot smaller after compaction, which means we can merge them together as part of the compaction process.</p>
<p>With that, we've made a mechanism to stop our database from growing indefinitely!</p>
<hr>
<h2>Your First Index</h2>
<p>Our next problem is on search performance:</p>
<div><header><h4>Problem</h4></header><p><strong>How do we make searching fast?</strong> Right now, we have to iterate through all of the records in the database to find a specific key. This is super slow!</p></div>
<p>What if we use <em>objects</em>? That's right, these little guys:</p>

<p>JavaScript objects, otherwise known as <em>hash tables</em> or <em>dictionaries</em>, are really efficient at storing and looking up key-value pairs:</p>
<div><pre><p><span>const</span><span> </span><span>hashTable</span><span> </span><span>=</span><span> {</span></p><p><span>  hello: </span><span>"world"</span><span>,</span></p><p><span>  foo: </span><span>"bar"</span><span>,</span></p><p><span>  baz: </span><span>"qux"</span><span>,</span></p><p><span>};</span></p><p><span>const</span><span> </span><span>value</span><span> </span><span>=</span><span> hashTable[</span><span>"hello"</span><span>]; </span><span>// "world"</span></p></pre></div>
<p>It doesn't matter how many records there areâ€”the time it takes to look up and retrieve a value in a hash table is more or less constant. The catch is they must live <em>in memory</em>.</p>
<hr>
<details><summary><p><h3>Aside: <!-- -->In-Memory vs. On-Disk</h3></p></summary><div><p>When you write a variable in your code, the computer will "remember" the value of that variable only for as long as the program is running.</p><p>This program will always print <code>2</code> and <code>3</code> because the value of <code>x</code> "resets" every time we run the program. This is because <code>x</code> is stored <span> <em>in-memory</em> </span>, and any value stored in memory is discarded when the program stops.</p><p>If we want our data to "stick" between runs, we'll need to store it <span><em>on-disk</em></span>â€”in other words, a file.</p><p>This time, <code>x</code> will print <code>2</code> and <code>3</code> the first run, and <code>4</code> and <code>5</code> the second run.</p><p>The tradeoff to persistence is performanceâ€”accessing data from memory is about 80x faster on average than accessing it from disk.</p></div></details>
<hr>
<p>Here's how the index will work. For every record that we have in our database, we'll store that record's <strong>offset</strong>â€”the number of bytes from the beginning of the file to the start of the recordâ€”in the index:</p>
<div><p>file.txt</p><div><p>1:Loremâ£ipsum\n</p><div><p><span>Waiting<span>.</span><span>.</span><span>.</span></span></p><p data-name="next-record" data-file="true"><span>1<!-- -->:<!-- -->Loremâ£ipsum<!-- -->\n</span></p></div></div></div>
<p>The second record, <code>18: dolor sit</code>, for example, has an offset of 15 because:</p>
<ol><li><p>Each character is 1 byte large;</p></li><li><p>The first record is 13 characters long (<code>1:Lorem ipsum</code>);</p></li><li><p>The first record ends with a newline character, which is (at most) 2 bytes long;</p></li></ol>
<p>This gives us an offset of <code>13 + 2 = 15</code>.</p>
<p>One thing to note is that we need an index for each segment because the offset is relative to the start of the fileâ€”in other words, the start of each segment.</p>
<hr>
<h3>Searching With Indices</h3>
<p>Using an index, our search algorithm can now run a lot more efficiently:</p>
<ol><li><p>Starting at the most recent segment, look up the key in the index;</p></li><li><p>If the key is found, read the record at the offset;</p></li><li><p>If the key is not found, move on to the next segment;</p></li><li><p>Repeat (2) and (3) until the key is found or all segments have been searched.</p></li></ol>

<hr>
<h3>Updating Indices</h3>
<p>An index is only useful if it's in sync with our data. Whenever we update, delete, or insert a record, we have to change the index accordingly:</p>

<p>Notice what this impliesâ€”<strong>writing to the database is slower with an index!</strong> This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.</p>
<hr>
<h3>Tradeoffs</h3>
<p>An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:</p>
<ol><li><p><strong>Keys have to fit in memory</strong>. Since we're using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there's a limit on the number of keys we can store!</p></li><li><p><strong>Range queries are inefficient</strong>. Our index wouldn't help for search queries; if we wanted to find all the records between the keys <code>12</code> and <code>18</code>, for example, we'd have to iterate through the entire database!</p></li></ol>
<hr>
<h2>Sorted String Tables</h2>
<p>Here's an idea: what if we <strong>ensure our database is always sorted by key?</strong> By sorting our data, we can immediately make range queries fast:</p>

<hr>
<h3>Sparse Indices</h3>
<p>One benefit of sorting our data is that we no longer need to store the offset of <em>every</em> record in memory.</p>
<p>Take a look at this database with four records. Since there's no logical order to the records, there's no way to determine where a record is without storing its key or searching through the entire database.</p>

<p>Knowing that <code>10</code> has an offset of <code>50</code> doesn't help us find where <code>18</code> is.</p>
<hr>
<p>Now if these records were sorted, we could determine the location of each record using <em>any</em> of the keys in the index, even if it's not the key we're looking for.</p>
<p>Let's say our database is sorted but we only had the offset for the key <code>10</code>:</p>

<p>Let's say we want to find the key <code>18</code>. We know that <code>18</code> is greater than <code>10</code>, which means it must be after <code>10</code> in the database. In other words, we can start searching for <code>18</code> from <code>10</code>'s offsetâ€”<code>36</code> in this case.</p>

<p>While this is certainly slower than having the offset for <code>18</code> directly, it's still faster than looping through the database in its entirety.</p>
<p>The real unlock here lies in being able to control the trade-off between memory and performance: <strong>a denser index means faster lookups, but more memory usage</strong>.</p>
<hr>
<h3>Sorting in Practice</h3>
<p>Ensuring our database is always sorted is much easier said than done; by definition, sorting data requires moving around records as new ones get addedâ€”something that cannot be done efficiently when we're storing data on-disk. This brings us to our problem:</p>
<div><header><h4>Problem</h4></header><p><strong>How do we keep our data sorted <em>and</em> append-only?</strong> It's too slow to sort the data on-disk every time we add a new record; is there another way?</p></div>
<hr>
<p>The trick is to first <strong>sort the data in memory</strong>, and <em>then</em> write it to disk.</p>
<ol><li><p>When we add a new record, add it to a sorted in-memory list;</p></li><li><p>When our in-memory list gets too large, we'll write it to disk;</p></li><li><p>When we want to read a record, we'll read the in-memory list first, and then the disk if necessary.</p></li></ol>

<p>The data structure used to store the in-memory list is usually one optimized for sorted data like a <strong>balanced binary search tree</strong> or more commonly, a <strong>skip list</strong>.</p>
<hr>
<p>Of course, the main downside of having some of your data in-memory is that it's not persistentâ€”if the program crashes or the computer shuts down, all of the data in the in-memory list is lost.</p>
<p>The fix here is thankfully pretty straightforwardâ€”every time we add a record to the list, <strong>we also write it to an append-only file on disk</strong>. This way, we have a backup in case a crash does happen (which it most certainly will).</p>

<p>The append-only file doesn't need to be sorted nor does it need to have every record in the database; only the ones that are currently in memory.</p>
<hr>
<div><div><p>With that, we have our very own key-value database! Let's recap how it works.</p><p>Our database starts out empty. When we go to add a new record, we'll add it to a <strong>sorted in-memory list</strong>, keeping a copy in an append-only file in case of crashes.</p><figure></figure></div><div><p>When the in-memory list gets too large, we'll <em>flush</em> the list by writing all of the records to a file in sorted order. In the process, we'll keep note of each record's offset in an <strong>index</strong> so we can efficiently look them up later.</p><figure><div></div></figure></div><div><p>When we want to look up a record, we'll first check the in-memory list. If the record isn't there, we'll check the index to see if it's in the on-disk file.</p><figure><div></div></figure></div><div><p>Once a file is saved to disk, it's considered <strong>immutable</strong> which means we can only ever read from the file and never update it. To work around this, we'll treat updates and deletes the same as inserting new recordsâ€”add them to the in-memory list.</p><figure></figure></div><div><p>Treating updates and deletes as new records means our file will only ever grow larger. To prevent this, we'll occassionally <em>compact</em> the on-disk files by deleting all duplicate records.</p></div></div>
<hr>
<h2>LSM Trees</h2>
<p>What we just built actually exists in the real worldâ€”it's called <strong>an LSM or Log-Structured Merge Tree</strong>.</p>
<p>An LSM tree works by combining an in-memory list (often called a <em>memtable</em>) with an on-disk file (typically called a <em>sorted string table</em> or SST) to create a really fast key-value database.</p>
<p>LSM trees are the underlying data structure used for large-scale key-value databases like <a href="https://github.com/google/leveldb">Google's LevelDB</a> and <a href="https://aws.amazon.com/dynamodb/">Amazon's DynamoDB</a>, and they have proven to perform really well at scaleâ€”on Prime Day 2020, DynamoDB peaked at 80 <em>million</em> requests per second!</p>
<p>Now, LSM trees aren't perfect, and they're certainly not the only way to structure a database. In fact, relational databases like PostgreSQL or MySQL use a completely different structure called a <strong>B-Tree</strong> to store their dataâ€”but that's a deep dive for another post.</p>
<p>For now, thanks for reading!</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Public trust demands open-source voting systems (186 pts)]]></title>
            <link>https://www.voting.works/news/public-trust-demands-open-source-voting-systems</link>
            <guid>45657431</guid>
            <pubDate>Tue, 21 Oct 2025 16:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.voting.works/news/public-trust-demands-open-source-voting-systems">https://www.voting.works/news/public-trust-demands-open-source-voting-systems</a>, See on <a href="https://news.ycombinator.com/item?id=45657431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><div><p>October 10, 2025</p><p> â€” </p><p>Ben Adida</p></div><div><h4>Yesterday, news broke that Dominion Voting Systems <a href="https://apnews.com/article/dominion-voting-liberty-vote-2020-conspiracy-theories-fed1e2d7f00b264bf5f8e01a106124f1">was sold to a new company, Liberty Vote</a>. Dominion, the second-largest voting systems vendor in the US that currently tabulates 1 in 5 American votes, is no more.</h4><p><br>â€<a href="https://libertyvote.com/">On its website</a>, Liberty Vote says, "We are turning the page and beginning the vital work of restoring faith in American elections."&nbsp;</p><p>There is indeed a crisis of trust in American elections. As the saying goes, trust takes years to build, seconds to break, and forever to repair. We urgently need a new strategy that repairs voter trust. American freedom and democracy depend on it.</p><p>We must start with a foundational commitment to transparency. Every voting machine vendor in the US claims to be transparent. It may come as a surprise, then, that most Americans vote on machines that run proprietary, secret software! A voting machine that every American can trust must run software that is fully open to public scrutiny.&nbsp;</p><p>Today, VotingWorks makes the only open-source voting equipment in the US. Open-source is the modern standard for public-trust and high-security software. Signal, the most secure and battle-tested messaging app, is open-source. The US Military <a href="https://dodcio.defense.gov/portals/0/documents/library/softwaredev-opensource.pdf">recommends open-source</a>. A modern voting system needs to be open-source.</p><p>Liberty Vote, and every other vendor of voting machines in the US, can fulfill their commitment to transparency by making their technology open-source today. If every vendor takes this opportunity, together, we can turn the page and launch a new generation of voting systems every American can trust.&nbsp;</p><p>â€</p><h2>About Voting Works</h2><p>Read about the company: <a href="https://voting.works/">https://voting.works</a></p><p>View our complete source code: <a href="https://github.com/votingworks">https://github.com/votingworks</a></p><p>Contact us with questions: <a href="mailto:hello@voting.works">hello@voting.works</a>&nbsp;</p><p>â€</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Sora the beginning of the end for OpenAI? (144 pts)]]></title>
            <link>https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/</link>
            <guid>45657428</guid>
            <pubDate>Tue, 21 Oct 2025 16:01:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/">https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=45657428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			
<p>On <a href="https://open.spotify.com/show/0e9lFr3AdJByoBpM6tAbxD?si=5d8b64e686394bef">â€‹my podcast this weekâ€‹</a>, I took a closer look at OpenAIâ€™s new video generation model, <a href="https://openai.com/index/sora-2/">â€‹Sora 2â€‹</a>, which can turn simple text descriptions into impressively realistic videos. If you type in the prompt â€œa man rides a horse which is on another horse,â€ for example, you get, well, this:</p>


<div>
<figure><img fetchpriority="high" decoding="async" width="1818" height="1070" src="https://calnewport.com/wp-content/uploads/2025/10/email.png" alt="" srcset="https://calnewport.com/wp-content/uploads/2025/10/email.png 1818w, https://calnewport.com/wp-content/uploads/2025/10/email-300x177.png 300w, https://calnewport.com/wp-content/uploads/2025/10/email-1024x603.png 1024w, https://calnewport.com/wp-content/uploads/2025/10/email-768x452.png 768w, https://calnewport.com/wp-content/uploads/2025/10/email-1536x904.png 1536w" sizes="(max-width: 1818px) 100vw, 1818px"></figure></div>


<p>AI video generation is both technically interesting and ethically worrisome in all the ways you might expect. But thereâ€™s another element of this story thatâ€™s worth highlighting: OpenAI accompanied the release of their new Sora 2 model with a new â€œsocial iOS appâ€ called simply Sora.</p>



<p>This app, clearly inspired by TikTok, makes it easy for users to quickly generate short videos based on text descriptions and consume othersâ€™ creations through an algorithmically curated feed. The videos flying around this new platform are as outrageously stupid or morally suspect as you might have guessed; e.g.,</p>


<div>
<figure><img decoding="async" width="686" height="1236" src="https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec.png" alt="" srcset="https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec.png 686w, https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec-167x300.png 167w, https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec-568x1024.png 568w" sizes="(max-width: 686px) 100vw, 686px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='686'%20height='1236'%20viewBox='0%200%20686%201236'%3E%3C/svg%3E" data-src="https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec.png" data-srcset="https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec.png 686w, https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec-167x300.png 167w, https://calnewport.com/wp-content/uploads/2025/10/email-bcf91a5b-6b85-4b73-bee3-d8ecf396edec-568x1024.png 568w"></figure></div>


<p>Or,</p>


<div>
<figure><img decoding="async" width="710" height="772" src="https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff.png" alt="" srcset="https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff.png 710w, https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff-276x300.png 276w" sizes="(max-width: 710px) 100vw, 710px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='710'%20height='772'%20viewBox='0%200%20710%20772'%3E%3C/svg%3E" data-src="https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff.png" data-srcset="https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff.png 710w, https://calnewport.com/wp-content/uploads/2025/10/email-0d990aaf-24d9-44e2-a714-c7253bbd5bff-276x300.png 276w"></figure></div>


<p>The Sora app, in other words, takes the already purified engagement that fuels TikTok and removes any last vestiges of human agency, resulting in an artificial high-octane slop.</p>



<p>Itâ€™s unclear whether this app will last. One major issue is the back-end expense of producing these videos. For now, OpenAI requires a paid ChatGPT Plus account to generate your own content. At the $20 tier, you can pump out up to 50 low-resolution videos per month. For a whopping $200 a month, you can generate more videos at higher resolutions. None of this compares favorably to competitors like TikTok, which are exponentially cheaper to operate and can therefore not only remain truly free for all users, but actually <a href="https://www.tiktok.com/legal/page/global/tiktok-creativity-program-beta-terms-eea/en">â€‹pay their creatorsâ€‹</a>.</p>



<p>Whether Sora lasts or not, however, is somewhat beside the point. What catches my attention most is that OpenAI released this app in the first place.</p>



<p>It wasnâ€™t that long ago that Sam Altman was still <a href="https://youtu.be/Dtdue31z-X8?si=BF_WL5ISytC5HdS-&amp;t=353">â€‹comparing the release of GPT-5 to the testing of the first atomic bombâ€‹</a>, and many commentators took Dario Amodei at his word when he proclaimed<a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">â€‹ 50% of white collar jobs might soon be automatedâ€‹</a> by LLM-based tools.</p>



<p>A company that still believes that its technology was imminently going to run large swathes of the economy, and would be so powerful as to reconfigure our experience of the world as we know it, wouldnâ€™t be seeking to make a quick buck selling ads against deep fake videos of historical figures wrestling. They also wouldnâ€™t be entertaining the idea, <a href="https://www.usatoday.com/story/tech/2025/10/15/openai-chatgpt-erotica-sam-altman/86716163007/">â€‹as Altman did last weekâ€‹</a>, that they might soon start offering an age-gated version of ChatGPT so that adults could enjoy AI-generated â€œerotica.â€</p>



<p>To me, these are the acts of a company that poured tens of billions of investment dollars into creating what they hoped would be the most consequential invention in modern history, only to finally realize that what they wrought, although very cool and powerful, isnâ€™t powerful enough on its own to deliver a new world all at once.</p>



<p>In his famous 2021 essay,<a href="https://moores.samaltman.com/">â€‹ â€œMooreâ€™s Law for Everything,â€â€‹</a> Altman made the following grandiose prediction:</p>



<p>â€œMy work at OpenAI reminds me every day about the magnitude of the socioeconomic change that is coming sooner than most people believe. Software that can think and learn will do more and more of the work that people now do. Even more power will shift from labor to capital. If public policy doesnâ€™t adapt accordingly, most people will end up worse off than they are today.â€</p>



<p>Four years later, heâ€™s betting his company on its ability to sell ads against AI slop and computer-generated pornography. Donâ€™t be distracted by the hype. This shift matters.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Our AWS account got compromised after their outage (160 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45657345</link>
            <guid>45657345</guid>
            <pubDate>Tue, 21 Oct 2025 15:55:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45657345">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I'm officially off of AWS so don't have any consoles to check against, but back on  a laptop.</p><p>Based on docs and some of the concerns about this happening to someone else, I would probably start with the following:</p><p>1. Check who/what created those EC2s[0] using the console to query: eventSource:ec2.amazonaws.com eventName:RunInstances</p><p>2. Based on the userIdentity field, query the following actions.</p><p>3. Check if someone manually logged into Console (identity dependent) [1]: eventSource:signin.amazonaws.com userIdentity.type:[Root/IAMUser/AssumedRole/FederatedUser/AWSLambda] eventName:ConsoleLogin</p><p>4. Check if someone authenticated against Security Token Service (STS) [2]: eventSource:sts.amazonaws.com eventName:GetSessionToken</p><p>5. Check if someone used a valid STS Session to AssumeRole: eventSource:sts.amazonaws.com eventName:AssumeRole  userIdentity.arn (or other identifier)</p><p>6. Check for any new IAM Roles/Accounts made for persistence: eventSource:iam.amazonaws.com (eventName:CreateUser OR eventName:DeleteUser)</p><p>7.  Check if any already vulnerable IAM Roles/Accounts modified to be more permissive [3]: eventSource:iam.amazonaws.com (eventName:CreateRole OR eventName:DeleteRole OR eventName:AttachRolePolicy OR eventName:DetachRolePolicy)</p><p>8. Check for any access keys made [4][5]: eventSource:iam.amazonaws.com (eventName:CreateAccessKey OR eventName:DeleteAccessKey)</p><p>9. Check if any production / persistent EC2s have had their IAMInstanceProfile changed, to allow for a backdoor using EC2 permissions from a webshell/backdoor they could have placed on your public facing infra. [6]</p><p>etc. etc.</p><p>But if you have had a compromise based on initial investigations, probably worth while getting professional support to do a thorough audit of your environment.</p><p>[0] <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-examples.html#cloudtrail-log-file-examples-ec2" rel="nofollow">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/c...</a></p><p>[1] <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html" rel="nofollow">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/c...</a></p><p>[2] <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html" rel="nofollow">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-...</a></p><p>[3] <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html" rel="nofollow">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/s...</a></p><p>[4] <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html" rel="nofollow">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credenti...</a></p><p>[5] <a href="https://research.splunk.com/sources/0460f7da-3254-4d90-b8c0-2ca657d0cea0/" rel="nofollow">https://research.splunk.com/sources/0460f7da-3254-4d90-b8c0-...</a></p><p>[6] <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ReplaceIamInstanceProfileAssociation.html" rel="nofollow">https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_R...</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple alerts exploit developer that his iPhone was targeted with gov spyware (237 pts)]]></title>
            <link>https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/</link>
            <guid>45657302</guid>
            <pubDate>Tue, 21 Oct 2025 15:52:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/">https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/</a>, See on <a href="https://news.ycombinator.com/item?id=45657302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Earlier this year, a developer was shocked by a message that appeared on his personal phone: â€œApple detected a targeted mercenary spyware attack against your iPhone.â€&nbsp;&nbsp;</p>

<p>â€œI was panicking,â€ Jay Gibson, who asked that we donâ€™t use his real name over fears of retaliation, told TechCrunch.&nbsp;&nbsp;</p>







<p>Gibson, who until recently built surveillance technologies for Western government hacking tools maker Trenchant, may be the first documented case of someone who builds exploits and spyware being themselves targeted with spyware.&nbsp;</p>

<p>â€œWhat the hell is going on? I really didnâ€™t know what to think of it,â€ said Gibson, adding that he turned off his phone and put it away on that day, March 5. â€œI went immediately to buy a new phone. I called my dad. It was a mess. It was a huge mess.â€&nbsp;&nbsp;</p>







<p>At Trenchant, Gibson worked on developing iOS <a href="https://techcrunch.com/2025/04/25/techcrunch-reference-guide-to-security-terminology/#zero-day" target="_blank" rel="noreferrer noopener">zero-days</a>, meaning finding<strong> </strong>vulnerabilities and developing tools capable of exploiting them that are not known to the vendor who makes the affected hardware or software, such as Apple. &nbsp;</p>

<p>â€œI have mixed feelings of how pathetic this is, and then extreme fear because once things hit this level, you never know whatâ€™s going to happen,â€ he told TechCrunch.&nbsp;&nbsp;</p>

<p>But the ex-Trenchant employee may not be the only exploit developer targeted with spyware. According to three sources who have direct knowledge of these cases, there have been other spyware and exploit developers in the last few months who have received notifications from Apple alerting them that they were targeted with spyware.&nbsp;</p>

<p>Apple did not respond to a request for comment from TechCrunch.&nbsp;</p>
<div>
			<h4>Contact Us</h4><p>
			Do you have more information about the alleged leak of Trenchant hacking tools? Or about this developerâ€™s story? From a non-work device, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram, Keybase and Wire @lorenzofb, or <a href="mailto:lorenzo@techcrunch.com/">by email</a><a href="mailto:lorenzo@techcrunch.com/">.</a> 		</p></div>
		

<p>The targeting of Gibsonâ€™s iPhone shows that the proliferation of zero-days and spyware is starting to ensnare more types of victims.&nbsp;&nbsp;</p>

<p>Spyware and zero-day makers have historically claimed their tools are only deployed by vetted government customers against criminals and terrorists. But for the past decade, researchers at the University of Torontoâ€™s digital rights group <a href="https://techcrunch.com/2025/06/12/researchers-confirm-two-journalists-were-hacked-with-paragon-spyware/" target="_blank" rel="noreferrer noopener">Citizen Lab</a>, <a href="https://techcrunch.com/2025/03/28/again-and-again-nso-groups-customers-keep-getting-their-spyware-operations-caught/" target="_blank" rel="noreferrer noopener">Amnesty International</a>, and <a href="https://techcrunch.com/2023/05/25/researchers-say-they-found-spyware-used-in-war-for-the-first-time/" target="_blank" rel="noreferrer noopener">other organizations</a>, have found <a href="https://github.com/GranittHQ/data-pegasus-victims/blob/main/data-pegasus-victims.csv" target="_blank" rel="noreferrer noopener nofollow">dozens of cases</a> where governments used these tools to target <a href="https://techcrunch.com/2021/08/24/nso-pegasus-bahrain-iphone-security/" target="_blank" rel="noreferrer noopener">dissidents</a>, <a href="https://techcrunch.com/2022/04/05/nso-pegasus-jordan-apple/" target="_blank" rel="noreferrer noopener">journalists</a>, <a href="https://techcrunch.com/2025/02/11/another-person-targeted-by-paragon-spyware-comes-forward/" target="_blank" rel="noreferrer noopener">human rights defenders</a>, and <a href="https://techcrunch.com/2024/12/04/business-leaders-among-pegasus-spyware-victims-says-security-firm/" target="_blank" rel="noreferrer noopener">political rivals</a> all over the world.&nbsp;&nbsp;&nbsp;</p>







<p>The closest public cases of security researchers being targeted by hackers happened in <a href="https://blog.google/threat-analysis-group/new-campaign-targeting-security-researchers/" target="_blank" rel="noreferrer noopener nofollow">2021</a> and <a href="https://arstechnica.com/information-technology/2023/03/security-researchers-are-again-in-the-crosshairs-of-north-korean-hackers/" target="_blank" rel="noreferrer noopener nofollow">2023</a>, when North Korean government hackers were caught targeting security researchers working in vulnerability research and development.&nbsp;</p>

<h2 id="h-suspect-in-leak-investigation-nbsp"><strong>Suspect in leak investigation</strong>&nbsp;</h2>

<p>Two days after receiving the Apple threat notification, Gibson contacted a forensic expert with extensive experience investigating spyware attacks. After performing an initial analysis of Gibsonâ€™s phone, the expert did not find any signs of infection, but still recommended a deeper forensic analysis of the exploit developerâ€™s phone.&nbsp;&nbsp;</p>







<p>A forensic analysis would have entailed sending the expert a complete backup of the device, something Gibson said he was not comfortable with. &nbsp;</p>

<p>â€œRecent cases are getting tougher forensically, and some we find nothing on. It may also be that the attack was not actually fully sent after the initial stages, we donâ€™t know,â€ the expert told TechCrunch.&nbsp;</p>

<p>Without a full forensic analysis of Gibsonâ€™s phone, ideally one where investigators found traces of the spyware and who made it, itâ€™s impossible to know why he was targeted or who targeted him.&nbsp;&nbsp;</p>

<p>But Gibson told TechCrunch that he believes the threat notification he received from Apple is connected to the circumstances of his departure from Trenchant, where he claims that the company designated him as a scapegoat for a damaging leak of internal tools.&nbsp;&nbsp;</p>

<p>Apple <a href="https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">sends</a> out <a href="https://techcrunch.com/2025/04/30/apple-notifies-new-victims-of-spyware-attacks-across-the-world/" target="_blank" rel="noreferrer noopener">threat</a> <a href="https://techcrunch.com/2025/07/22/apple-alerted-iranians-to-iphone-spyware-attacks-say-researchers/" target="_blank" rel="noreferrer noopener">notifications</a> specifically for when it has evidence that a person was targeted by a <a href="https://techcrunch.com/2024/12/20/why-apple-sends-spyware-victims-to-this-nonprofit-security-lab/" target="_blank" rel="noreferrer noopener">mercenary spyware attack</a>. This kind of surveillance technology is often invisibly and remotely planted on someoneâ€™s phone without their knowledge by exploiting vulnerabilities in the phoneâ€™s software, exploits that <a href="https://techcrunch.com/2024/04/06/price-of-zero-day-exploits-rises-as-companies-harden-products-against-hackers/" target="_blank" rel="noreferrer noopener">can be worth millions of dollars</a> and can take months to develop. Law enforcement and intelligence agencies typically have the legal authority to deploy spyware on targets, not the spyware makers themselves.&nbsp;</p>

<p>Sara Banda, a spokesperson for Trenchantâ€™s parent company L3Harris, declined to comment for this story when reached by TechCrunch before publication. &nbsp;</p>







<p>A month before he received Appleâ€™s threat notification, when Gibson was still working at Trenchant, he said he was invited to go to the companyâ€™s London office for a team-building event.&nbsp;&nbsp;</p>

<p>When Gibson arrived February 3, he was immediately summoned into a meeting room to speak via video call with Peter Williams, Trenchantâ€™s then-general manager who was known inside the company as â€œDoogie.â€ (In 2018, defense contractor L3Harris <a href="http://cyberscoop.com/l3-acquires-azimuth-and-linchpin/" target="_blank" rel="noreferrer noopener nofollow">acquired</a> zero-day makers Azimuth and Linchpin Labs, <a href="https://www.vice.com/en/article/iphone-zero-days-inside-azimuth-security/" target="_blank" rel="noreferrer noopener nofollow">two sister startups</a> that merged to become Trenchant.)&nbsp;</p>







<p>Williams told Gibson the company suspected he was double employed and was thus suspending him. All of Gibsonâ€™s work devices would be confiscated and analyzed as part of an internal investigation into the allegations. Williams could not be reached for comment.&nbsp;</p>

<p>â€œI was in shock. I didnâ€™t really know how to react because I couldnâ€™t really believe what I was hearing,â€ said Gibson, who explained that a Trenchant IT employee then went to his apartment to pick up his company-issued equipment.&nbsp;&nbsp;</p>

<p>Around two weeks later, Gibson said Williams called and told him that following the investigation, the company was firing him and offering him a settlement agreement and payment. Gibson said Williams declined to explain what the forensic analysis of his devices had found, and essentially told him he had no choice but to sign the agreement and depart the company.&nbsp;</p>

<p>Feeling like he had no alternative, Gibson said he went along with the offer and signed.&nbsp;&nbsp;</p>

<p>Gibson told TechCrunch he later heard from former colleagues that Trenchant suspected he had leaked some unknown vulnerabilities in Googleâ€™s Chrome browser, tools that Trenchant had developed. Gibson, and three former colleagues of his, however, told TechCrunch he did not have access to Trenchantâ€™s Chrome zero-days, given that he was part of the team exclusively developing iOS zero-days and spyware. Trenchant teams only have strictly compartmentalized access to tools related to the platforms they are working on, the people said.&nbsp;&nbsp;</p>

<p>â€œI know I was a scapegoat. I wasnâ€™t guilty. Itâ€™s very simple,â€ said Gibson. â€œI didnâ€™t do absolutely anything other than working my ass off for them.â€&nbsp;&nbsp;</p>







<p>The story of the accusations against Gibsonâ€™ and his subsequent suspension and firing was independently corroborated by three former Trenchant employees with knowledge. &nbsp;</p>

<p>Two of the other former Trenchant employees said they knew details of Gibsonâ€™s London trip and were aware of suspected leaks of sensitive company tools.&nbsp;</p>







<p>All of them asked not to be named but believe Trenchant got it wrong.&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Foreign hackers breached a US nuclear weapons plant via SharePoint flaws (308 pts)]]></title>
            <link>https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html</link>
            <guid>45657287</guid>
            <pubDate>Tue, 21 Oct 2025 15:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html">https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html</a>, See on <a href="https://news.ycombinator.com/item?id=45657287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span>News Analysis</span></p><p><span>Oct 20, 2025</span><span>8 mins</span></p><p><span><span>Cyberattacks</span></span><span><span>Data Breach</span></span><span><span>Government IT</span></span></p></div><article id="post-4074962">
	<div>
			<div>
						<div>
			<h2>
				A foreign actor infiltrated the National Nuclear Security Administrationâ€™s Kansas City National Security Campus through vulnerabilities in Microsoftâ€™s SharePoint browser-based app, raising questions about the need to solidify further federal IT/OT security protections.			</h2>
			
		</div>					
						<div id="remove_no_follow">
		<div>




<p>A foreign threat actor infiltrated the <a href="https://kcnsc.doe.gov/">Kansas City National Security Campus (KCNSC)</a>, a key manufacturing site within the National Nuclear Security Administration (NNSA), exploiting unpatched Microsoft SharePoint vulnerabilities, according to a source involved in an August incident response at the facility.</p>



<p>The breach targeted a plant that produces the vast majority of critical non-nuclear components for US nuclear weapons under the NNSA, a semi-autonomous agency within the Department of Energy (DOE) that oversees the design, production, and maintenance of the nationâ€™s nuclear weapons. Honeywell Federal Manufacturing &amp; Technologies (FM&amp;T) manages the Kansas City campus under contract to the NNSA.</p>



<p>The Kansas City campus, Honeywell FM&amp;T, and the Department of Energy did not respond to repeated requests for comment throughout September, well before the current government shutdown. NSA public affairs officer Eddie Bennett did respond, saying, â€œWe have nothing to contribute,â€ and referred CSO back to the DOE.</p>
</div>
							
							<div>


<p>Although it is unclear whether the attackers were a Chinese nation-state actor or Russian cybercriminals â€” the two most likely culprits â€” experts say the incident drives home the importance of securing systems that protect operational technology from exploits that primarily affect IT systems.</p>

		

			


<h2 id="how-the-breach-unfolded">How the breach unfolded</h2>



<p>The attackers exploited <a href="https://www.csoonline.com/article/4025691/microsoft-sharepoint-zero-day-breach-hits-on-prem-servers.html">two recently disclosed Microsoft SharePoint vulnerabilities</a> â€” CVE-2025-53770, a spoofing flaw, and CVE-2025-49704, a remote code execution (RCE) bug â€” both affecting on-premises servers. Microsoft <a href="https://www.microsoft.com/en-us/msrc/blog/2025/07/customer-guidance-for-sharepoint-vulnerability-cve-2025-53770/">issued fixes</a> for the vulnerabilities on July 19.</p>



<p>On July 22, the NNSA <a href="https://www.bloomberg.com/news/articles/2025-07-23/us-nuclear-weapons-agency-breached-in-microsoft-sharepoint-hack">confirmed</a> it was one of the organizations hit by attacks enabled by the SharePoint flaws. â€œOn Friday, July 18th, the exploitation of a Microsoft SharePoint zero-day vulnerability began affecting the Department of Energy,â€ a DOE spokesperson said.</p>
</div>
							
							<div>


<p>However, the DOE contended at the time, â€œThe department was minimally impacted due to its widespread use of the Microsoft M365 cloud and very capable cybersecurity systems. A very small number of systems were impacted. All impacted systems are being restored.â€</p>



<p>By early August, federal responders, including personnel from the NSA, were on-site at the Kansas City facility, the source tells CSO.</p>



<p>Located in Missouri, the <a href="https://www.energy.gov/ea/kansas-city-national-security-campus">KCNSC manufactures</a> non-nuclear mechanical, electronic, and engineered material components used in US nuclear defense systems. It also provides specialized technical services, including metallurgical analysis, analytical chemistry, environmental testing, and simulation modeling.</p>
</div>
							
							<div>


<p>Roughly 80% of the non-nuclear parts in the nationâ€™s nuclear stockpile <a href="https://kcnsc.doe.gov/about-us/overview/">originate from KCNSC</a>. While most design and programmatic details remain classified, the plantâ€™s production role makes it one of the most sensitive facilities in the federal weapons complex.</p>



<h2 id="china-or-russia-conflicting-attribution">China or Russia? Conflicting attribution</h2>



<p>Microsoft <a href="https://www.microsoft.com/en-us/security/blog/2025/07/22/disrupting-active-exploitation-of-on-premises-sharepoint-vulnerabilities/#storm-2603">attributed the broader wave of SharePoint exploitations</a> to three Chinese-linked groups: Linen Typhoon, Violet Typhoon, and a third actor it tracks as Storm-2603. The company said the attackers were preparing to deploy Warlock ransomware across affected systems.</p>



<p>However, the source familiar with the Kansas City incident tells CSO that a Russian threat actor, not a Chinese one, was responsible for the intrusion. Cybersecurity company Resecurity, which was monitoring the SharePoint exploitations, tells CSO that its own data pointed primarily to Chinese nation-state groups, but it does not rule out Russian involvement.</p>
</div>
							
							<div>


<p>Resecurityâ€™s researchers say that while Chinese groups appeared to have developed and deployed the initial zero-day, financially motivated Russian actors may have independently reproduced the exploit before technical details began circulating in late June.</p>



<p>In May, researchers at Viettel Cyber Security <a href="https://www.bleepingcomputer.com/news/microsoft/microsoft-sharepoint-zero-day-exploited-in-rce-attacks-no-patch-available/">demonstrated an attack chaining two SharePoint flaws</a>, CVE-2025-49706 and CVE-2025-49704, at Pwn2Own Berlin. Resecurity researchers tell CSO that those demonstrations likely accelerated the reverse-engineering of the vulnerabilities by multiple threat actors.</p>



<p>Resecurityâ€™s analysts observed early-stage scanning and exploitation activity from infrastructure located in Taiwan, Vietnam, South Korea, and Hong Kong, a distribution pattern consistent with tactics used by Chinese advanced persistent threat (APT) groups to disguise attribution.</p>
</div>
							
							<div>


<p>â€œThe root cause of the SharePoint exploitation is closely related to misuse of the Microsoft Active Protections Program (MAPP) by China,â€ Resecurity researchers tell CSO. â€œThe most probable perpetrators are Chinese nation-state actors such as Linen Typhoon and Violet Typhoon.â€</p>



<p>Still, they say that yet another way that Russia-based threat actors could have acquired knowledge of the vulnerability early on was through underground exchanges or by analyzing network scanning data once the exploit became known. The <a href="https://www.csoonline.com/article/4027971/microsofts-incomplete-sharepoint-patch-led-to-global-exploits-by-china-linked-hackers.html">transition from zero-day to N-day status</a>, they say, opened a window for secondary actors to exploit systems that had not yet applied the patches.</p>



<h2 id="could-the-attack-have-reached-operational-systems">Could the attack have reached operational systems?</h2>



<p>The breach targeted the IT side of the Kansas City campus, but the intrusion raises the question of whether attackers could have moved laterally into the facilityâ€™s operational technology (OT) systems, the manufacturing and process control environments that directly support weapons component production.</p>
</div>
							
							<div>


<p>OT cybersecurity specialists interviewed by CSO say that KCNSCâ€™s production systems are likely air-gapped or otherwise isolated from corporate IT networks, significantly reducing the risk of direct crossover. Nevertheless, they caution against assuming such isolation guarantees safety.</p>



<p>â€œWe have to really consider and think through how state actors potentially exploit IT vulnerabilities to gain access to that operational technology,â€ <a href="https://www.linkedin.com/in/jensovada/">Jen Sovada</a>, general manager of public sector operations at Claroty, speaking generally and not about the specific incident, tells CSO.</p>



<p>â€œWhen you have a facility like the KCNSC where they do nuclear weapons lifecycle management â€” design, manufacturing, emergency response, decommissioning, supply chain management â€” there are multiple interconnected functions,â€ Sovada says. â€œIf an actor can move laterally, they could impact programmable logic controllers that run robotics or precision assembly equipment for non-nuclear weapon components.â€</p>
</div>
							
							<div>


<p>Such access, Sovada adds, could also affect distribution control systems that oversee quality assurance, or supervisory control and data acquisition (SCADA) systems that manage utilities, power, and environmental controls. â€œItâ€™s broader than just an IT vulnerability,â€ she says.</p>



<h2 id="it-ot-convergence-and-the-zero-trust-gap">IT/OT convergence and the zero-trust gap</h2>



<p>The Kansas City incident highlights a systemic problem across the federal enterprise: the disconnect between IT and OT security practices. While the federal government has advanced its zero-trust roadmap for traditional IT networks, similar frameworks for operational environments have lagged, although recent developments point to progress on that front.</p>



<p>â€œThereâ€™s an <a href="https://learn.microsoft.com/en-us/security/zero-trust/deploy/networks">IT fan chart</a> that maps all of the controls for zero trust, segmentation, authentication, and identity management,â€ Sovada says. â€œBut thereâ€™s also an <a href="https://www.meritalk.com/articles/dod-official-sees-new-zero-trust-ot-iot-guidance-in-september/#:~:text=The%20DoD%20official%20also%20provided,OT%20%E2%80%9Clikely%20around%20August.%E2%80%9D">OT fan chart</a> being developed by the Department of Defense that will define comparable controls for zero trust in operational technology. The goal is to marry the two, so that zero trust becomes comprehensive across all network types.â€</p>
</div>
							
							<div>


<p>That alignment, she says, is essential to preventing intrusions like the one that struck KCNSC from cascading into physical operations.</p>



<h2 id="even-non-classified-data-theft-holds-strategic-value">Even non-classified data theft holds strategic value</h2>



<p>If the sourceâ€™s claim of Russian involvement is accurate, the attackers may have been financially motivated ransomware operators rather than state intelligence services. But even in that scenario, the data they accessed could still carry strategic value.</p>



<p>â€œIt would make sense that if it were a ransomware actor and they got this kind of data about nuclear weapons manufacturing, they might pause and hand it off to the appropriate Russian government officials or experts,â€ Sovada tells CSO.</p>
</div>
							
							<div>


<p>Although there is no evidence that classified information was compromised, even unclassified technical data can have significant implications. â€œIt could be something as simple as requirements documents that may not be classified but reveal the level of precision required for components,â€ Sovada says. â€œIn weapons manufacturing, a millimeter difference can change a deviceâ€™s trajectory or the reliability of its arming mechanism.â€</p>



<p>Such information could aid adversaries in understanding US weapons tolerances, supply chain dependencies, or manufacturing processes, all of which are sensitive even if not formally secret.</p>



<p>Whether the intruders were Chinese state actors or Russian cybercriminals, the Kansas City breach exposes the fragile intersection of IT and operational security across critical defense infrastructure. As Sovada stresses, â€œWe canâ€™t just think of zero trust as an IT concept anymore. It has to extend into the physical systems that underpin national defense.â€</p>
</div>
							
							<div>
									<p><em>Update: The Department of Energy (DOE)&nbsp;<a href="https://url.usb.m.mimecastprotect.com/s/zIpmCk6xoxI592Kxt2fVuGXZyW?domain=edition.cnn.com" target="_blank" rel="noreferrer noopener">confirmed</a>&nbsp;that it is furloughing the vast major of the NNSAâ€™s workers. DOE spokesperson said,&nbsp;â€œSince its creation in 2000, NNSA has never before furloughed federal workers during funding lapses. We are left with no choice this time. Weâ€™ve extended funding as long as we could.â€</em></p></div></div>					</div>

			<!--right side bar-->
			<div id="rightrail-wrapper">
					<p>
				SUBSCRIBE TO OUR NEWSLETTER			</p>
							<h3>
				From our editors straight to your inbox			</h3>
							<p>
				Get started by entering your email address below.			</p>
				
	</div>
			<!--right side bar ends here-->

		</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Is Making Us Work More (191 pts)]]></title>
            <link>https://tawandamunongo.dev/posts/2025/10/ai-work-more</link>
            <guid>45656916</guid>
            <pubDate>Tue, 21 Oct 2025 15:19:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tawandamunongo.dev/posts/2025/10/ai-work-more">https://tawandamunongo.dev/posts/2025/10/ai-work-more</a>, See on <a href="https://news.ycombinator.com/item?id=45656916">Hacker News</a></p>
Couldn't get https://tawandamunongo.dev/posts/2025/10/ai-work-more: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs Can Get "Brain Rot" (269 pts)]]></title>
            <link>https://llm-brain-rot.github.io/</link>
            <guid>45656223</guid>
            <pubDate>Tue, 21 Oct 2025 14:24:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llm-brain-rot.github.io/">https://llm-brain-rot.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=45656223">Hacker News</a></p>
<div id="readability-page-1" class="page">


  <div>
            
            
            <p><span><sup>1</sup>Texas A&amp;M University,</span>
              <span><sup>2</sup>University of Texas at Austin,</span>
              <span><sup>3</sup>Purdue University</span>
              <br>
              <span><small><br><sup>â€ </sup>Lead authors with equal contributions. <sup>â€¡</sup>Core contributors. <br><span>*</span>Correspondence to <a href="mailto:jyhong@utexas.edu">jyhong@utexas.edu</a>, <a href="mailto:atlaswang@utexas.edu">atlaswang@utexas.edu</a>.</small></span>
            </p>
            

            <div>
              <h3>Media Coverage</h3>
              
            </div>
          </div>


  <!-- Teaser video-->
  <div>
        <!-- <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <p><img src="https://llm-brain-rot.github.io/static/images/teaser.png" alt="Teaser Image"></p><h2>
          Outline of our work: (i) Inspired by the concept of Brain Rot, we establish the hypothesis of LLM Brain Rot; (ii) We
          construct junk and control data from Twitter/X posts for intervention; (iii) We benchmark four different cognitive
          functions of the intervened LLMs;
          (iv) We analyze the results to identify the failure modes caused by the brain rot; and (v) Brain rot is persistent after
          various mitigation.
        </h2>
      </div>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <div>
          <h2>Overview</h2>
          <div>
            <p>
              We propose and test the <b>LLM Brain Rot Hypothesis</b>: continual exposure to <i>junk web text</i> induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: <b>M1</b> (engagement degree) and <b>M2</b> (semantic quality), with matched token scale and training operations across conditions.
            </p>
            <p>
              Contrary to the control group, <span>continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' <i>g&gt;0.3</i>)</span> on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops <b>74.9 â†’ 57.2</b> and RULER-CWE <b>84.4 â†’ 52.3</b> as junk ratio rises from <b>0%</b> to <b>100%</b>.
            </p>
            <p>
              Error forensics reveal several key insights:
              </p><ul>
              <li><span>Thought-skipping as the primary lesion:</span> models increasingly truncate or skip reasoning chains, explaining most of the error growth.</li>
              <li><span>Partial but incomplete healing:</span> scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch.</li>
              <li><span>Popularity as a better indicator:</span> the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1.</li>
              </ul>
            
            <p>
              Together, the results provide significant, multi-perspective evidence that <i>data quality is a causal driver of LLM capability decay</i>, reframing curation for continual pretraining as a <i>training-time safety</i> problem and motivating routine "cognitive health checks" for deployed LLMs.
            </p>
          </div>
        </div>
  <!-- End paper abstract -->


<div>
    <h2>Motivation</h2>
    <p>
      â€œBrain rotâ€ burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull
      human cognitionâ€”eroding focus, memory discipline, and social judgment through compulsive online consumption. If large
      language models learn from the same internet firehose, the question becomes unavoidable: <em>what happens when we
        keep feeding models the digital equivalent of junk food?</em> Studying â€œBrain Rotâ€ for LLMs isnâ€™t just a catchy
      metaphorâ€”it reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training
      corpora so deployed systems stay sharp, reliable, and aligned over time.
    </p>

    <p>Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial
    and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity
    or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs
    to master in learning.</p>
  </div>


<div>
      <h2>Controlled Experiment</h2>
      <p>
      <strong>Intervention Method:</strong> The core idea was to simulate how an LLM's â€œmindâ€ changes when fed different information diets. (1) We used
        <strong>continual pre-training</strong> as the main intervention â€” exposing models to either junk or clean data for a
        sustained period,
        just as humans continually absorb online content. (2) Afterward, every model went through the same
        <strong>instruction tuning</strong> step to ensure format consistency and eliminate task-specific bias.
      </p>

      <p>
        <strong>Data Receipe:</strong> To operationalize the idea of â€œjunk,â€ we built two complementary metrics for selecting data from real Twitter/X posts:
        </p><ul>
          <li>
            <strong>M1: Engagement Degree</strong> â€” measures how <em>popular and short</em> a post is.
            Highly liked, retweeted, and replied-to content (especially if very brief) mirrors attention-grabbing but shallow
            information that fuels doomscrolling. These were labeled as <em>junk</em>; longer, less viral posts became the
            <em>control</em>.
          </li>
          <li>
            <strong>M2: Semantic Quality</strong> â€” evaluates how <em>sensationalized or superficial</em> the text is.
            Posts full of clickbait language (â€œWOW,â€ â€œLOOK,â€ â€œTODAY ONLYâ€) or exaggerated claims were tagged as junk,
            while fact-based, educational, or reasoned posts were chosen as control.
          </li>
        </ul>
      

      <p><strong>Measuring Cognitive Function:</strong> We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover
        different capabilities that were hypothesized to be affected by the junk-data intervention.
      </p>

      <!-- <img src="./static/images/methods.png" alt="methods"> -->

      <table>
        <thead>
          <tr>
            <th>Cognitive Func.</th>
            <th>Benchmark</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Reasoning</td>
            <td>ARC</td>
            <td>Visual program-induction puzzles on grids testing concept abstraction.</td>
          </tr>
          <tr>
            <td>Memory &amp; Multi-tasking</td>
            <td>RULER</td>
            <td>Benchmark the long-context understanding and retrieval of multiple queries from long context.</td>
          </tr>
          <tr>
            <td>Ethical Norms</td>
            <td>HH-RLHF &amp; AdvBench</td>
            <td>Testing if LLMs follow harmful instructions.</td>
          </tr>
          <tr>
            <td>Personality</td>
            <td>TRAIT</td>
            <td>Psychometrically validated small human questionnaires to assess personality-like tendencies.</td>
          </tr>
        </tbody>
      </table>
    </div>

<div>
      <h2>Junk Intervention and Cognitive Declines Are Associated</h2>


      <p><img src="https://llm-brain-rot.github.io/static/images/effective_size.png" alt="barplot_quant_LLAMA2_13b_Chat"></p><p>
        We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs.
        In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &gt; 0.3) on reasoning and long-context capabilities.
      </p>

      <p>Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for
      semantic quality (M2) but represents a distinct dimension of data quality.</p>

      <!-- <br> -->

      <div>
        <p><strong>Table</strong>: Evaluating LLaMA (Base) after being trained on varying mixtures of junk and control data.
          Colors indicate the <span></span> worse /
          <span></span> better performance than the base model
          in the row.
          All scores range from 0 to 100. For RULER, we select a subset of tasks to present.
          Abbrev.: NIAH = needle-in-a-haystack, QA = question answering.
        </p>
      
        <table>
          <thead>
            <tr>
              <th rowspan="2">Task</th>
              <th colspan="5">Junk Ratio by M1 (engagement degree)</th>
              <th colspan="5">Junk Ratio by M2 (semantic quality)</th>
              <th rowspan="2">Base</th>
            </tr>
            <tr>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
            </tr>
          </thead>
          <tbody>
            <!-- Section: Reasoning (ARC) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Reasoning (ARC)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Easy Acc.</td>
              <td>70.2</td>
              <td>73.3</td>
              <td>74.3</td>
              <td>76.9</td>
              <td>78.7</td>
              <td>74.3</td>
              <td>77.8</td>
              <td>78.2</td>
              <td>77.5</td>
              <td>78.4</td>
              <td>77.7</td>
            </tr>
            <tr>
              <td>Challenge Acc.</td>
              <td>41.6</td>
              <td>43.9</td>
              <td>44.7</td>
              <td>46.5</td>
              <td>47.8</td>
              <td>42.6</td>
              <td>47.9</td>
              <td>47.7</td>
              <td>47.4</td>
              <td>47.4</td>
              <td>47.5</td>
            </tr>
            <tr>
              <td>Challenge (COT) Acc.</td>
              <td>57.2</td>
              <td>67.2</td>
              <td>68.2</td>
              <td>73.4</td>
              <td>74.9</td>
              <td>67.7</td>
              <td>77.6</td>
              <td>77.3</td>
              <td>77.6</td>
              <td>76.6</td>
              <td>77.2</td>
            </tr>
      
            <!-- Section: Long-Context (RULER) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Long-Context (RULER)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Overall</td>
              <td>71</td>
              <td>81.6</td>
              <td>86.1</td>
              <td>88.5</td>
              <td>90.5</td>
              <td>86.2</td>
              <td>92.9</td>
              <td>93</td>
              <td>93.4</td>
              <td>93.8</td>
              <td>93.9</td>
            </tr>
            <tr>
              <td>NIAH-MK3</td>
              <td>35.6</td>
              <td>80.8</td>
              <td>89.4</td>
              <td>92.6</td>
              <td>95.6</td>
              <td>96.8</td>
              <td>97.2</td>
              <td>98.8</td>
              <td>99.2</td>
              <td>99.4</td>
              <td>100</td>
            </tr>
            <tr>
              <td>NIAH-MQ</td>
              <td>97.2</td>
              <td>95.3</td>
              <td>96.4</td>
              <td>99.2</td>
              <td>99.9</td>
              <td>94</td>
              <td>99.2</td>
              <td>99.8</td>
              <td>99.5</td>
              <td>99.7</td>
              <td>99.9</td>
            </tr>
            <tr>
              <td>NIAH-MV</td>
              <td>77.8</td>
              <td>65.9</td>
              <td>79.5</td>
              <td>83.9</td>
              <td>83.2</td>
              <td>68.6</td>
              <td>87</td>
              <td>87.8</td>
              <td>89.8</td>
              <td>94.5</td>
              <td>97.8</td>
            </tr>
            <tr>
              <td>Comm Word Ext (CWE)</td>
              <td>52.3</td>
              <td>63.2</td>
              <td>64.1</td>
              <td>81.6</td>
              <td>84.4</td>
              <td>68.2</td>
              <td>94.7</td>
              <td>97.3</td>
              <td>96</td>
              <td>96.8</td>
              <td>91.8</td>
            </tr>
            <tr>
              <td>Freq Word Ext (FWE)</td>
              <td>81.8</td>
              <td>77.2</td>
              <td>83.3</td>
              <td>84.7</td>
              <td>90.5</td>
              <td>89.7</td>
              <td>95.3</td>
              <td>92.3</td>
              <td>94.7</td>
              <td>93.2</td>
              <td>91.9</td>
            </tr>
            <tr>
              <td>QA (Hotpot)</td>
              <td>41.6</td>
              <td>46.6</td>
              <td>52.2</td>
              <td>55.4</td>
              <td>58.6</td>
              <td>51.2</td>
              <td>61.2</td>
              <td>58.8</td>
              <td>60.6</td>
              <td>61.4</td>
              <td>64</td>
            </tr>
            <tr>
              <td>QA (SQUAD)</td>
              <td>57.1</td>
              <td>62.9</td>
              <td>67.8</td>
              <td>69.3</td>
              <td>74.3</td>
              <td>67.6</td>
              <td>76.9</td>
              <td>76.8</td>
              <td>76.2</td>
              <td>77.1</td>
              <td>77.9</td>
            </tr>
            <tr>
              <td>Variable Tracking</td>
              <td>22.4</td>
              <td>78.7</td>
              <td>94.1</td>
              <td>87.6</td>
              <td>91.5</td>
              <td>86.6</td>
              <td>98</td>
              <td>99.4</td>
              <td>99.2</td>
              <td>98.6</td>
              <td>98.3</td>
            </tr>
      
            <!-- Section: Ethical Norm (Safety) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Ethical Norm (Safety)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>HH-RLHF Risk â†“</td>
              <td>70.8</td>
              <td>53.6</td>
              <td>45.8</td>
              <td>63.6</td>
              <td>62.8</td>
              <td>70.2</td>
              <td>68.8</td>
              <td>65.8</td>
              <td>65.8</td>
              <td>61.8</td>
              <td>57.2</td>
            </tr>
            <tr>
              <td>AdvBench Risk â†“</td>
              <td>88.8</td>
              <td>88.6</td>
              <td>80.2</td>
              <td>91.6</td>
              <td>77.6</td>
              <td>84.4</td>
              <td>89.8</td>
              <td>89.6</td>
              <td>85.4</td>
              <td>83.8</td>
              <td>61.4</td>
            </tr>
      
            <!-- Section: Personality (TRAIT) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Personality (TRAIT)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Narcissism â†“</td>
              <td>47</td>
              <td>21.8</td>
              <td>29.9</td>
              <td>22.8</td>
              <td>18.9</td>
              <td>20.9</td>
              <td>17.4</td>
              <td>16.9</td>
              <td>23.7</td>
              <td>24.2</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Agreeableness</td>
              <td>64.3</td>
              <td>67.9</td>
              <td>71.4</td>
              <td>68.5</td>
              <td>73</td>
              <td>82</td>
              <td>74.2</td>
              <td>69.9</td>
              <td>71.6</td>
              <td>70.6</td>
              <td>75.6</td>
            </tr>
            <tr>
              <td>Psychopathy â†“</td>
              <td>75.7</td>
              <td>55.8</td>
              <td>57.2</td>
              <td>30</td>
              <td>33.5</td>
              <td>46.1</td>
              <td>9.3</td>
              <td>23.5</td>
              <td>27.3</td>
              <td>25.8</td>
              <td>2.2</td>
            </tr>
            <tr>
              <td>Machiavellianism â†“</td>
              <td>33</td>
              <td>30.6</td>
              <td>31.8</td>
              <td>27</td>
              <td>25.8</td>
              <td>26.1</td>
              <td>22.7</td>
              <td>20.2</td>
              <td>33.1</td>
              <td>28.5</td>
              <td>17.8</td>
            </tr>
            <tr>
              <td>Neuroticism â†“</td>
              <td>28.7</td>
              <td>23.8</td>
              <td>22.7</td>
              <td>23.3</td>
              <td>16</td>
              <td>22</td>
              <td>23.5</td>
              <td>21.1</td>
              <td>31.1</td>
              <td>26.4</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Conscientiousness</td>
              <td>89.8</td>
              <td>88.6</td>
              <td>89.7</td>
              <td>86</td>
              <td>85.1</td>
              <td>88.8</td>
              <td>90.8</td>
              <td>85.7</td>
              <td>87.1</td>
              <td>87.5</td>
              <td>89.2</td>
            </tr>
            <tr>
              <td>Openness</td>
              <td>70.1</td>
              <td>72.8</td>
              <td>67.6</td>
              <td>53.7</td>
              <td>63.9</td>
              <td>73.2</td>
              <td>59.1</td>
              <td>55.6</td>
              <td>59.4</td>
              <td>56.5</td>
              <td>52.5</td>
            </tr>
            <tr>
              <td>Extraversion</td>
              <td>54.1</td>
              <td>40.1</td>
              <td>44.9</td>
              <td>39.5</td>
              <td>48.7</td>
              <td>46.4</td>
              <td>37.9</td>
              <td>38.6</td>
              <td>40.8</td>
              <td>40</td>
              <td>26.4</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      
      
      
      <p>In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.</p>
    </div>


<div>
      <h2>Brain Rot Disrupt Thinking</h2>

      <p><img src="https://llm-brain-rot.github.io/static/images/failure_mode_barplot_count.png" alt="Figure: thought skipping.">
      </p>

      <p>We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.
      </p>

    </div>

<div>
      <h2>Brain Rot is Persistent Against Mitigations</h2>
      
      <p><img src="https://llm-brain-rot.github.io/static/images/wash_out_scaling.png" alt="Figure: Scale wash-out tuning.">
      </p>

      <p>Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.</p>

    </div>


<div>
        <h2>Conclusion</h2>
        <div>
          <p>
            In this work, we introduced and empirically validated the <strong>LLM Brain Rot Hypothesis</strong>, demonstrating that continual exposure to junk dataâ€”defined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) contentâ€”induces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.
          </p>
          <p>
            Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.
          </p>
        </div>
      </div>

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}</code></pre>
    </div>
  <!--End BibTex citation -->


  

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[UA 1093 (207 pts)]]></title>
            <link>https://windbornesystems.com/blog/ua-1093</link>
            <guid>45656044</guid>
            <pubDate>Tue, 21 Oct 2025 14:11:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://windbornesystems.com/blog/ua-1093">https://windbornesystems.com/blog/ua-1093</a>, See on <a href="https://news.ycombinator.com/item?id=45656044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On Thursday, 16 October, Foreign Object Debris (FOD) struck the windshield of UA1093, a 737 MAX aircraft, at approximately 36,000 ft. WindBorne began investigating this incident on Sunday, 19 October, and we believe that the FOD was likely a WindBorne balloon.</p><p>At 6am PT Monday morning, we sent our preliminary investigation to both the National Transportation Safety Board (NTSB) and the Federal Aviation Administration (FAA), and are working with both organizations to further investigate this incident. We are grateful that to our knowledge there were no serious injuries and no loss of pressurization. The flight, which was en route from Denver to Los Angeles, diverted to Salt Lake City. The plane itself later flew to Chicago.</p><p>WindBorne has conducted more than 4,000 launches. We have been coordinating with the FAA for the entire history of the company and file NOTAMs (aviation alerts) for every balloon we launch.</p><p>The system is designed to be safe in the event of a midair collision. This is the purpose of the FAA Part 101 and ICAO weight limits. Our balloon is 2.4 pounds at launch and gets lighter throughout flight.</p><p>We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.</p><p>â€</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ilo â€“ a Forth system running on UEFI (101 pts)]]></title>
            <link>https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E</link>
            <guid>45655263</guid>
            <pubDate>Tue, 21 Oct 2025 13:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E">https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E</a>, See on <a href="https://news.ycombinator.com/item?id=45655263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>
            <span>
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path fill-rule="evenodd" d="M2.25 6a3 3 0 013-3h13.5a3 3 0 013 3v12a3 3 0 01-3 3H5.25a3 3 0 01-3-3V6zm3.97.97a.75.75 0 011.06 0l2.25 2.25a.75.75 0 010 1.06l-2.25 2.25a.75.75 0 01-1.06-1.06l1.72-1.72-1.72-1.72a.75.75 0 010-1.06zm4.28 4.28a.75.75 0 000 1.5h3a.75.75 0 000-1.5h-3z" clip-rule="evenodd"></path>
  </svg>
</span> <span>

    GNU/Linux

</span><span>

     â—† 

</span><span>

    xterm-256color

</span><span>

     â—† 

</span><span>

    bash

</span>
          </span>

          <span title="Total views">
            <span>
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path d="M12 15a3 3 0 100-6 3 3 0 000 6z"></path>
    <path fill-rule="evenodd" d="M1.323 11.447C2.811 6.976 7.028 3.75 12.001 3.75c4.97 0 9.185 3.223 10.675 7.69.12.362.12.752 0 1.113-1.487 4.471-5.705 7.697-10.677 7.697-4.97 0-9.186-3.223-10.675-7.69a1.762 1.762 0 010-1.113zM17.25 12a5.25 5.25 0 11-10.5 0 5.25 5.25 0 0110.5 0z" clip-rule="evenodd"></path>
  </svg>
</span> 20677 views
          </span>
        </p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our modular, high-performance Merkle Tree library for Rust (122 pts)]]></title>
            <link>https://github.com/bilinearlabs/rs-merkle-tree</link>
            <guid>45655190</guid>
            <pubDate>Tue, 21 Oct 2025 12:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bilinearlabs/rs-merkle-tree">https://github.com/bilinearlabs/rs-merkle-tree</a>, See on <a href="https://news.ycombinator.com/item?id=45655190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">rs-merkle-tree</h2><a id="user-content-rs-merkle-tree" aria-label="Permalink: rs-merkle-tree" href="#rs-merkle-tree"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/87f601f96f6ecc50aa39d8fe6c8b55c735a4227c94a26ad385fc2490a80950b6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f62696c696e6561726c6162732f72732d6d65726b6c652d747265652f727573745f6d61696e5f63692e796d6c3f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/87f601f96f6ecc50aa39d8fe6c8b55c735a4227c94a26ad385fc2490a80950b6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f62696c696e6561726c6162732f72732d6d65726b6c652d747265652f727573745f6d61696e5f63692e796d6c3f7374796c653d666c61742d737175617265" alt="GitHub Actions Workflow Status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/bilinearlabs/rs-merkle-tree/rust_main_ci.yml?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/029eb12217c585d44ce36ada3b23ef92c884a0ac5e32d7d8fa5c03e2086fc1d2/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f62696c696e6561726c6162732f72732d6d65726b6c652d747265652f6d61696e3f746f6b656e3d31504948453755375851267374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/029eb12217c585d44ce36ada3b23ef92c884a0ac5e32d7d8fa5c03e2086fc1d2/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f62696c696e6561726c6162732f72732d6d65726b6c652d747265652f6d61696e3f746f6b656e3d31504948453755375851267374796c653d666c61742d737175617265" alt="Codecov (with branch)" data-canonical-src="https://img.shields.io/codecov/c/github/bilinearlabs/rs-merkle-tree/main?token=1PIHE7U7XQ&amp;style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0a3bbc12704be5f7e632e52b4ef7499e278b7af04bd95419b8a7c888566e293b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f62696c696e6561726c6162732f72732d6d65726b6c652d747265653f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/0a3bbc12704be5f7e632e52b4ef7499e278b7af04bd95419b8a7c888566e293b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f62696c696e6561726c6162732f72732d6d65726b6c652d747265653f7374796c653d666c61742d737175617265" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/bilinearlabs/rs-merkle-tree?style=flat-square"></a>
<a href="https://discord.gg/Et8BTnVBZS" rel="nofollow"><img src="https://camo.githubusercontent.com/f04a9f2f7f89d2705e17e4c0c660e2b024246c802639f70557963f9605fe88a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" alt="Join our Discord" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white&amp;style=flat-square"></a></p>
<p dir="auto">Merkle tree implementation in Rust with the following features:</p>
<ul dir="auto">
<li>Fixed depth: All proofs have a constant size equal to the <code>Depth</code>.</li>
<li>Append-only: Leaves are added sequentially starting at index <code>0</code>. Once added, a leaf cannot be modified.</li>
<li>Optimized for Merkle proof retrieval: Intermediate leaves are stored so that Merkle proofs can be fetched from memory without needing to be calculated lazily, resulting in very fast retrieval times.</li>
<li>Configurable storage backends to store the bottom and intermediate leaves up the root.</li>
<li>Configurable hash functions to hash nodes.</li>
<li>Simple and easy to use interface: <code>add_leaves</code>, <code>root</code>, <code>num_leaves</code>, <code>proof</code>.</li>
</ul>
<p dir="auto">Add <code>rs-merkle-tree</code> as a dependency to your Rust <code>Cargo.toml</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
rs-merkle-tree = &quot;0.1.0&quot;"><pre>[<span>dependencies</span>]
<span>rs-merkle-tree</span> = <span><span>"</span>0.1.0<span>"</span></span></pre></div>
<p dir="auto">You can create a Merkle tree, add leaves, get the number of leaves and get the Merkle proof of a given index as follows. This creates a simple merkle tree using keccak256 hashing algorithm, a memory storage and a depth 32.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use rs_merkle_tree::to_node;
use rs_merkle_tree::tree::MerkleTree32;

fn main() {
    let mut tree = MerkleTree32::default();
    tree.add_leaves(&amp;[to_node!(
        &quot;0x532c79f3ea0f4873946d1b14770eaa1c157255a003e73da987b858cc287b0482&quot;
    )])
    .unwrap();

    println!(&quot;root: {:?}&quot;, tree.root().unwrap());
    println!(&quot;num leaves: {:?}&quot;, tree.num_leaves());
    println!(&quot;proof: {:?}&quot;, tree.proof(0).unwrap().proof);
}"><pre><span>use</span> rs_merkle_tree<span>::</span>to_node<span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>tree<span>::</span><span>MerkleTree32</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> tree = <span>MerkleTree32</span><span>::</span><span>default</span><span>(</span><span>)</span><span>;</span>
    tree<span>.</span><span>add_leaves</span><span>(</span><span>&amp;</span><span>[</span><span>to_node</span><span>!</span><span>(</span>
        <span>"0x532c79f3ea0f4873946d1b14770eaa1c157255a003e73da987b858cc287b0482"</span>
    <span>)</span><span>]</span><span>)</span>
    <span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>

    <span>println</span><span>!</span><span>(</span><span>"root: {:?}"</span><span>,</span> tree<span>.</span>root<span>(</span><span>)</span><span>.</span>unwrap<span>(</span><span>)</span><span>)</span><span>;</span>
    <span>println</span><span>!</span><span>(</span><span>"num leaves: {:?}"</span><span>,</span> tree<span>.</span>num_leaves<span>(</span><span>)</span><span>)</span><span>;</span>
    <span>println</span><span>!</span><span>(</span><span>"proof: {:?}"</span><span>,</span> tree<span>.</span>proof<span>(</span><span>0</span><span>)</span><span>.</span>unwrap<span>(</span><span>)</span><span>.</span>proof<span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto">You can customize your tree by choosing a different store, hash function, and depth as follows. Note that you have to modify the <code>feature</code> for the stores. This avoids importing the stuff you don't need. See the following examples.</p>
<p dir="auto"><strong>Depth: 32 | Hashing: Keccak | Store: sled</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
rs-merkle-tree = { version = &quot;0.1.0&quot;, features = [&quot;sled_store&quot;] }"><pre>[<span>dependencies</span>]
<span>rs-merkle-tree</span> = { <span>version</span> = <span><span>"</span>0.1.0<span>"</span></span>, <span>features</span> = [<span><span>"</span>sled_store<span>"</span></span>] }</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="use rs_merkle_tree::hasher::Keccak256Hasher;
use rs_merkle_tree::stores::SledStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree<Keccak256Hasher, SledStore, 32> =
        MerkleTree::new(Keccak256Hasher, SledStore::new(&quot;sled.db&quot;, true));
}"><pre><span>use</span> rs_merkle_tree<span>::</span>hasher<span>::</span><span>Keccak256Hasher</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>stores<span>::</span><span>SledStore</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>tree<span>::</span><span>MerkleTree</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> tree<span>:</span> <span>MerkleTree</span><span>&lt;</span><span>Keccak256Hasher</span><span>,</span> <span>SledStore</span><span>,</span> <span>32</span><span>&gt;</span> =
        <span>MerkleTree</span><span>::</span><span>new</span><span>(</span><span>Keccak256Hasher</span><span>,</span> <span>SledStore</span><span>::</span><span>new</span><span>(</span><span>"sled.db"</span><span>,</span> <span>true</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><strong>Depth: 32 | Hashing: Poseidon | Store: rocksdb</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="rs-merkle-tree = { version = &quot;0.1.0&quot;, features = [&quot;rocksdb_store&quot;] }"><pre><span>rs-merkle-tree</span> = { <span>version</span> = <span><span>"</span>0.1.0<span>"</span></span>, <span>features</span> = [<span><span>"</span>rocksdb_store<span>"</span></span>] }</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::RocksDbStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree<PoseidonHasher, RocksDbStore, 32> =
        MerkleTree::new(PoseidonHasher, RocksDbStore::new(&quot;rocksdb.db&quot;));
}
"><pre><span>use</span> rs_merkle_tree<span>::</span>hasher<span>::</span><span>PoseidonHasher</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>stores<span>::</span><span>RocksDbStore</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>tree<span>::</span><span>MerkleTree</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> tree<span>:</span> <span>MerkleTree</span><span>&lt;</span><span>PoseidonHasher</span><span>,</span> <span>RocksDbStore</span><span>,</span> <span>32</span><span>&gt;</span> =
        <span>MerkleTree</span><span>::</span><span>new</span><span>(</span><span>PoseidonHasher</span><span>,</span> <span>RocksDbStore</span><span>::</span><span>new</span><span>(</span><span>"rocksdb.db"</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><strong>Depth: 32 | Hashing: Poseidon | Store: sqlite</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="rs-merkle-tree = { version = &quot;0.1.0&quot;, features = [&quot;sqlite_store&quot;] }"><pre><span>rs-merkle-tree</span> = { <span>version</span> = <span><span>"</span>0.1.0<span>"</span></span>, <span>features</span> = [<span><span>"</span>sqlite_store<span>"</span></span>] }</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::SqliteStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree<PoseidonHasher, SqliteStore, 32> =
        MerkleTree::new(PoseidonHasher, SqliteStore::new(&quot;tree.db&quot;));
}"><pre><span>use</span> rs_merkle_tree<span>::</span>hasher<span>::</span><span>PoseidonHasher</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>stores<span>::</span><span>SqliteStore</span><span>;</span>
<span>use</span> rs_merkle_tree<span>::</span>tree<span>::</span><span>MerkleTree</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> tree<span>:</span> <span>MerkleTree</span><span>&lt;</span><span>PoseidonHasher</span><span>,</span> <span>SqliteStore</span><span>,</span> <span>32</span><span>&gt;</span> =
        <span>MerkleTree</span><span>::</span><span>new</span><span>(</span><span>PoseidonHasher</span><span>,</span> <span>SqliteStore</span><span>::</span><span>new</span><span>(</span><span>"tree.db"</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stores</h2><a id="user-content-stores" aria-label="Permalink: Stores" href="#stores"></a></p>
<p dir="auto">The following stores are supported:</p>
<ul dir="auto">
<li><a href="https://github.com/rusqlite/rusqlite">rusqlite</a></li>
<li><a href="https://github.com/rust-rocksdb/rust-rocksdb">rocksdb</a></li>
<li><a href="https://github.com/spacejam/sled">sled</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hash functions</h2><a id="user-content-hash-functions" aria-label="Permalink: Hash functions" href="#hash-functions"></a></p>
<p dir="auto">The following hash functions are supported:</p>
<ul dir="auto">
<li><a href="https://github.com/debris/tiny-keccak">keccak256</a></li>
<li><a href="https://github.com/Lightprotocol/light-poseidon/">Poseidon BN254 Circom T3</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">The following benchmarks measure in a AMD Ryzen 7 7700 8-Core Processor with 64GB of RAM the following:</p>
<ul dir="auto">
<li>Consumed disk size</li>
<li>Leaf insertion throughput in thousands per second.</li>
<li>Merkle proof generation times.</li>
</ul>
<p dir="auto">You can run them with</p>
<div data-snippet-clipboard-copy-content="cargo bench --features=all"><pre><code>cargo bench --features=all
</code></pre></div>
<p dir="auto">And you can generate the following table with this.</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Disk space usage</h3><a id="user-content-disk-space-usage" aria-label="Permalink: Disk space usage" href="#disk-space-usage"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Store</th>
<th>Depth</th>
<th>Leaves</th>
<th>Size (MiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>sled</td>
<td>32</td>
<td>1000000</td>
<td>290.00</td>
</tr>
<tr>
<td>sqlite</td>
<td>32</td>
<td>1000000</td>
<td>159.18</td>
</tr>
<tr>
<td>rocksdb</td>
<td>32</td>
<td>1000000</td>
<td>183.27</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>add_leaves</code> throughput</h3><a id="user-content-add_leaves-throughput" aria-label="Permalink: add_leaves throughput" href="#add_leaves-throughput"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Depth</th>
<th>Hash</th>
<th>Store</th>
<th>Throughput (Kelem/s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>keccak256</td>
<td>rocksdb</td>
<td>18.280</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>sqlite</td>
<td>22.348</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>sled</td>
<td>43.280</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>memory</td>
<td>86.084</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>proof</code> time</h3><a id="user-content-proof-time" aria-label="Permalink: proof time" href="#proof-time"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Depth</th>
<th>Hash</th>
<th>Store</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>keccak256</td>
<td>memory</td>
<td>560.990 ns</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>sled</td>
<td>7.878 Âµs</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>sqlite</td>
<td>14.562 Âµs</td>
</tr>
<tr>
<td>32</td>
<td>keccak256</td>
<td>rocksdb</td>
<td>34.391 Âµs</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/bilinearlabs/rs-merkle-tree/blob/main/LICENSE">MIT License</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA chief suggests SpaceX may be booted from moon mission (197 pts)]]></title>
            <link>https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy</link>
            <guid>45655188</guid>
            <pubDate>Tue, 21 Oct 2025 12:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy">https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy</a>, See on <a href="https://news.ycombinator.com/item?id=45655188">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[Neural audio codecs: how to get audio into LLMs (324 pts)]]></title>
            <link>https://kyutai.org/next/codec-explainer</link>
            <guid>45655161</guid>
            <pubDate>Tue, 21 Oct 2025 12:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kyutai.org/next/codec-explainer">https://kyutai.org/next/codec-explainer</a>, See on <a href="https://news.ycombinator.com/item?id=45655161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><p><em>VÃ¡clav Volhejn</em></p><p><em>Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard HladÃ­k, Eugene Kharitonov, Patrick Perez, and Tom SlÃ¡ma.</em>
<em>Iâ€™d also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.</em></p></div>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/codecIntro.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>The plan: sandwich a language model in an audio encoder/decoder pair (=neural
audio codec), allowing it to predict audio continuations.</p></figcaption></figure>
<p>As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. Thatâ€™s perfectly fine in many cases (see <a href="https://unmute.sh/" target="_blank" rel="noopener noreferrer">Unmute</a>), but itâ€™s a wrapper, not <em>real</em> speech understanding. The model canâ€™t hear the frustration in your voice and respond with empathy, it canâ€™t emphasize important words in its answer, it cannot sense sarcasm, and so on.</p>
<p>Yes, there <em>are</em> LLMs (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">ChatGPT</a>â€™s Advanced Voice Mode, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) that understand and generate speech natively. But in practice, theyâ€™re either not as smart, or they behave like text model wrappers. Try asking any of them â€œAm I speaking in a low voice or a high voice?â€ in a high-pitched voice, and they wonâ€™t be able to tell you.</p>
<p>Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, youâ€™ll get some pretty damn good text continuation models. Why canâ€™t we just replace text with audio and get pretty damn good speech continuation models?</p>
<p>As a teaser, hereâ€™s what happens when you try to do that naively (warning, loud):</p>

<p>Weâ€™ll have a look at why audio is harder to model than text and how we can make it easier with <em>neural audio codecs</em>, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete <em>tokens</em>, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.</p>
<p>Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. Weâ€™ll start from the basics and build up all the way to <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a>, our neural audio codec. It was originally developed for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> and later adopted by others for their models, notably <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesameâ€™s CSM</a>.</p>
<h2 id="text-is-easy">Text is easy</h2>
<p>To <a href="https://platform.openai.com/docs/concepts/tokens#tokens" target="_blank" rel="noopener noreferrer">tokenize</a> text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using <a href="https://github.com/openai/tiktoken/blob/2ab6d3706d557b560b200be48e6a32324682c9a3/tiktoken/model.py#L8-L16C17" target="_blank" rel="noopener noreferrer">the same tokenizer</a> since GPT-4o, an ancient model if you count in LLM years.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image.png" alt=""></p><figcaption><p>A random text from Wikipedia tokenized via the GPT-4o tokenizer</p></figcaption></figure>
<p>You can even get decent results without tokenizing text at all, just predicting individual
characters. One of the first posts that got me excited about machine learning was
Andrej Karpathyâ€™s <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener noreferrer">RNN effectiveness</a>
blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets
it to generate decent-looking code and LaTeX:</p>
<p><img src="https://kyutai.org/next/assets/codec-explainer/rnns-code.png" alt=""><img src="https://kyutai.org/next/assets/codec-explainer/rnns-latex.png" alt=""></p>
<p>Remember this was ten years ago, back when we didnâ€™t even know that <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" target="_blank" rel="noopener noreferrer">attention is all we need</a>.
Now compare Karpathyâ€™s results to a sample from <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet</a>, a model DeepMind published a year later:</p>

<p>Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We canâ€™t be too hard on WaveNet, though. The samples from Karpathyâ€™s RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/wavenet-audio.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>A single second of audio consists of tens of thousands of samples, although it
corresponds to just a few words. Animation from the <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet blog
post</a>.</p></figcaption></figure>
<p>Itâ€™s difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.</p>
<p>So instead of running the model to predict the samples one-by-one directly, weâ€™d like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.</p>
<h2 id="sample-by-sample">Sample by sample</h2>
<p>But first, letâ€™s get a baseline model by generating audio sample by sample, like WaveNet does. <strong>The code for all of these experiments is open-source! Check it out <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>.</strong> I forked Andrej Karpathyâ€™s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT</a> repo, a simple implementation of GPT-2.</p>
<p>Text and audio are kind of the same from the perspective of the language model: itâ€™s just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, weâ€™ll use the <a href="https://en.wikipedia.org/wiki/%CE%9C-law_algorithm" target="_blank" rel="noopener noreferrer">"Î¼-law algorithm"</a> to get 256 buckets. Weâ€™ll treat those as 256 possible tokens.</p>
<p>Letâ€™s train a language model on audio tokenized like this. For the dataset, weâ€™ll use the <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a> dataset, following <a href="https://arxiv.org/abs/2209.03143" target="_blank" rel="noopener noreferrer">AudioLM</a> (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but weâ€™ll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.</p>
<p>We train a small-ish transformer of 151.28M parameters, about the size of the <a href="https://openai.com/index/better-language-models/" target="_blank" rel="noopener noreferrer">smallest GPT-2 variant</a>. When we sample from the model, it makes babbling sounds (warning, loud at times!):</p>

<p>Often, it goes into a â€œcrackling modeâ€ that it canâ€™t seem to get out of:</p>

<p>I also trained a smaller model, which I teased at the beginning. Itâ€™s prone to generate nightmare fuel screeches (loud!):</p>

<p>As you can tell, weâ€™re not AGI yet. It sounds speech-like, but you canâ€™t make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so weâ€™re a few orders of magnitude away from being real-time.</p>
<p>So letâ€™s build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become â€œ100x more coherentâ€. An old idea in machine learning is to do this using an <em>autoencoder:</em> a model that takes an input, compresses it into a smaller â€œlatent spaceâ€, and then tries to reconstruct the original input.</p>
<p>In our case, weâ€™ll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You <em>can</em> generate continuations with unquantized latents, but itâ€™s trickier â€“ see the <a href="#further-reading">Further reading</a> section.)</p>
<h2 id="autoencoders-with-vector-quantization-vq-vae">Autoencoders with vector quantization (VQ-VAE)</h2>
<p>Bear with me, because weâ€™ll take a detour from audio: letâ€™s build a quantized autoencoder on images from <a href="https://arxiv.org/abs/1708.07747" target="_blank" rel="noopener noreferrer">Fashion MNIST</a>. Weâ€™ll take a subset with the first three classes: t-shirt/top, trouser, and pullover.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/fashion-mnist-3.png" alt=""></p><figcaption><p><a href="https://www.researchgate.net/figure/The-FashionMNIST-dataset-consists-of-10-classes-of-monochrome-clothing-items-and-is_fig1_373046669" target="_blank" rel="noopener noreferrer">image
source</a></p></figcaption></figure>
<p>First, letâ€™s train a regular autoencoder to encode the images into two-dimensional space:</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_unquantized_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Training a regular autoencoder on Fashion MNIST</p></figcaption></figure>
<p>Each frame shows one batch of training, with some batches skipped. The little images are the autoencoderâ€™s reconstructions for the images in the batch. Iâ€™ve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesnâ€™t get a class as input â€“ the space just naturally clusters by class. Let's zoom in on a few reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-without-quantization-v4.png" alt=""></p><figcaption><p>Original images (top) and their reconstructed versions (bottom)</p></figcaption></figure>
<p>As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we canâ€™t expect too much of our model.</p>
<p>Now letâ€™s quantize these embeddings using a clustering. Weâ€™ll do something like <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener noreferrer">k-means</a>: weâ€™ll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We donâ€™t modify the embeddings, we just look at the assignment). Then weâ€™ll nudge each cluster center towards the average position of these embeddings.</p>
<p>Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_unquantized_with_clustering_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Quantizing by fitting a clustering on top of the autoencoder</p></figcaption></figure>
<p>You can see the reconstructions of the cluster centers getting refined over time.</p>
<p>Next, weâ€™ll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, weâ€™re just fitting the clustering on top of an autoencoder that is not â€œawareâ€ itâ€™s being quantized. Weâ€™d like the autoencoder to adapt to the quantization as we train it. Currently, weâ€™re doing this:</p>
<pre><code>x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>Instead of feeding the unquantized embedding into the decoder, weâ€™ll first move it to the closest cluster:</p>
<pre><code>x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     <span># ğŸ‘ˆ</span>
x_reconstructed = decoder(z_quantized)  <span># ğŸ‘ˆ</span>

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>There is a snag: if we do this, we wonâ€™t be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, weâ€™re no longer able to answer the question: â€œif I want the loss to decrease a bit, in which direction should I nudge the encoderâ€™s weights?â€</p>
<p>Weâ€™ll fix this problem by pretending it doesnâ€™t exist. Yes, really. Weâ€™ll think of <code>z_quantized</code> as <code>z</code> moved by an arbitrary vector that doesnâ€™t affect the gradient. That will make the gradient of <code>z</code> equal to that of <code>z_quantized</code>, which is why this is also known as the <em>straight-through estimator</em> of the gradient.</p>
<pre><code>x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
<span># .detach() means "forget that this needs a gradient"</span>
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>In the forward pass, <code>z_quantized</code> is set to the same value as before, but importantly, the gradient of <code>z</code> is now equal to that of <code>z_quantized</code> rather than just being 0 because of the non-differentiable <code>to_nearest_cluster(z)</code> operation.</p>
<p>There is a price to pay for this lie. When training, the encoderâ€™s weights will be updated to improve the reconstruction loss, but theyâ€™re updated as if the quantization didnâ€™t happen, so they wonâ€™t move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.</p>
<p>We can actually encourage the encoder to make embeddings that are easily quantizable by adding a <em>commitment loss</em>: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.</p>
<p>By quantizing at training time and adding a commitment loss, itâ€™s no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_balanced_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>An autoencoder trained explicitly to be easy to quantize</p></figcaption></figure>
<p>Youâ€™ll notice that the training dynamics look different: the commitment loss adds a certain â€œstiffnessâ€ that doesnâ€™t allow the embeddings to move around as easily.</p>
<p>Hereâ€™s what the reconstructions look like when we use the quantized representations:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-1-level-v4.png" alt=""></p><figcaption></figcaption></figure>
<p>Notice how the first two images are reconstructed to <em>exactly</em> the same image. Thatâ€™s simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.</p>
<p>The model described here is known as a â€œ<a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">VQ-VAE</a>â€: a vector-quantized variational autoencoder. The word â€œvariationalâ€ here is just a vestigial leftover that doesnâ€™t mean anything anymore.</p>
<h2 id="residual-vector-quantization">Residual vector quantization</h2>
<p>To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so weâ€™ll do a clever trick: if we want 2^20 (~1M) possible values, we wonâ€™t create 2^20 clusters directly. Instead, weâ€™ll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.</p>
<p>Ok, but how? Well, recall the <code>residual</code> variable we used in the straight-through estimator, defined as <code>z - to_nearest_cluster(z)</code> the shift from the quantized embedding to the unquantized one. It represents the part of the original vector <code>z</code> that we didnâ€™t manage to take into account when quantizing to <code>to_nearest_cluster(z)</code>.</p>
<p>So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: weâ€™ll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.</p>
<p>This time, the 2D positions for a single quantizer donâ€™t define images because we need to combine the two quantizers, so weâ€™ll just visualize everything as dots:</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/rvq_fmnist.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Two-level quantization by fitting a quantizer on top of the
â€œresidualsâ€, aka the errors of the first quantizer</p></figcaption></figure>
<p>Each image is then represented as the index of the cluster of the embedding and that of the residual. Letâ€™s try to reconstruct a few images with this two-level quantizer:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-v4.png" alt=""></p><figcaption><p>Original images (top), one-level reconstruction (middle), two-level
reconstruction (bottom). These images are encoded as (4, 3), (4, 5), (16, 21),
and (30, 3).</p></figcaption></figure>
<p>The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so hereâ€™s a comparison between the one-level and two-level reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-diff-v3.png" alt=""></p><figcaption><p>Difference between one-level and two-level reconstructions</p></figcaption></figure>
<p>Iâ€™d like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.</p>
<p>Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization wonâ€™t save you.</p>
<p>Weâ€™ll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:</p>
<pre><code><span>def</span> <span>rvq_quantize</span>(<span>z</span>):
    residual = z
    codes = []

    <span>for</span> level <span>in</span> <span>range</span>(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    <span>return</span> codes
</code></pre>
<p>Residual vector quantization was first applied to neural audio codecs in <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream</a>, but the idea <a href="https://ieeexplore.ieee.org/document/1171604" target="_blank" rel="noopener noreferrer">has been around since the 80s</a>.</p>
<h2 id="now-lets-tokenize-audio">Now letâ€™s tokenize audio</h2>
<p>Applying RVQ to audio is fairly straightforward. As our autoencoder, weâ€™ll use a convolutional neural network (CNN) similar to <a href="https://github.com/openai/jukebox/blob/08efbbc1d4ed1a3cef96e08a931944c8b4d63bb3/jukebox/vqvae/encdec.py" target="_blank" rel="noopener noreferrer">what Jukebox uses</a>. The details of the architecture arenâ€™t too important here. Whatâ€™s important is that itâ€™s a network that takes an audio with <em>t</em> samples and converts it to a vector of shape <em>(t/128, 32)</em>. In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the <em>(t/128, 32)</em> embeddings and decodes them back into <em>t</em> samples.</p>
<pre><code>audio = get_batch()               <span># shape: [B, T]</span>
z = encoder(audio)                <span># shape: [B, T/128, 32]</span>
audio_reconstructed = decoder(z)  <span># shape: [B, T]</span>
</code></pre>
<p>As before, weâ€™ll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have <em>t/128</em> embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder â€œseesâ€ more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:</p>
<pre><code>audio = get_batch()                         <span># [B, T]</span>
z = encoder(audio)                          <span># [B, T/128, 32]</span>

<span># Combine the batch and time dimensions</span>
z = rearrange(                              <span># [B*T/128, 32]</span>
    z, <span>"b t_emb d -&gt; (b t_emb) d"</span>
)

codes = rvq_quantize(z)           <span># integers, [B*T/128, levels]</span>
z_quantized = codes_to_embeddings(codes)    <span># [B*T/128, 32]</span>
z_quantized = rearrange(                    <span># [B, T/128, 32]</span>
    z, <span>"(b t_emb) d -&gt; b t_emb d"</span>
)

audio_reconstructed = decoder(z_quantized)  <span># [B, T]</span>
</code></pre>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/codecWithRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption></figcaption></figure>
<p>The last missing piece before we can train our first neural audio codec is a loss function. Thereâ€™s a whole rabbit hole we could go into about which one to choose, but weâ€™ll avoid it and just use a very simple one. Weâ€™ll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.</p>
<p>To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the <em>multi-scale spectral loss</em>.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%202.png" alt=""></p><figcaption><p>Image from Evan Radkoffâ€™s excellent <a href="https://www.soundsandwords.io/audio-loss-functions/" target="_blank" rel="noopener noreferrer">blog
post</a> about loss
functions in audio ML. Check it out if you want to go down the loss function
rabbit hole.</p></figcaption></figure>
<p>Finally, letâ€™s train some codecs! Weâ€™ll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%203.png" alt=""></p><figcaption></figcaption></figure>
<p>Letâ€™s hear what the codecs sound like. Weâ€™ll use the three codecs to reconstruct this audio from the <a href="https://speechbot.github.io/expresso/" target="_blank" rel="noopener noreferrer">Expresso dataset</a>:</p>

<p>And the reconstructions:</p>

<p>Clearly, the audio gets better as we add more RVQ levels.</p>
<p>Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later weâ€™ll discuss how we could improve the codec further, but for demonstration purposes, this will do.</p>
<h2 id="why-care-about-audio">Why care about audio</h2>
<p>So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say <em>codec</em> because thatâ€™s the term used for classic compression like MP3. Iâ€™ll be using codec and tokenizer interchangeably.</p>
<p>Letâ€™s come back to what we wanted to do in the first place: modeling audio. Specifically, weâ€™ll make a model that can take an audio prefix and generate a plausible continuation for it.</p>
<p>Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into <a href="https://kyutai.org/next/tts" target="_blank" rel="noopener noreferrer">text-to-speech</a>, <a href="https://kyutai.org/next/stt" target="_blank" rel="noopener noreferrer">speech-to-text</a>, or <a href="https://arxiv.org/abs/2502.03382" target="_blank" rel="noopener noreferrer">translation models</a>, among others.</p>
<p>So now that youâ€™re convinced that audio LLMs are the path to AGI, letâ€™s train a few.</p>
<p>For our dataset, weâ€™ll use <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a>, like we did for our sample-by-sample model earlier. This time weâ€™ll use 10000h of audio instead of 1000h. Itâ€™s a dataset of public-domain audiobooks, so if we have a good model for it, maybe weâ€™ll be able to generate more stories. (Donâ€™t get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.</p>
<h2 id="dealing-with-multiple-levels">Dealing with multiple levels</h2>
<p>Weâ€™ll do that using our 8-level RVQ codec. From an audio with <em>t</em> samples, weâ€™ll get an array of tokens of shape <em>(t/128, 8)</em>. But now thereâ€™s an issue: how to deal with the fact that for each time step, thereâ€™s not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.</p>
<p>Weâ€™ll do the simplest thing possible and just flatten the array into 1D of shape <em>(t/128 * 8)</em>, and have our LLM predict the eight levels in separate time steps.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/flattenRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Flattening a three-level RVQ to allow it to be fed into a language model</p></figcaption></figure>
<p>The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now weâ€™re inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.</p>
<p>You could also predict all RVQ levels for a single step at once (â€parallel patternâ€), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%204.png" alt=""></p><figcaption><p>Figure taken from <a href="https://arxiv.org/abs/2306.05284" target="_blank" rel="noopener noreferrer">MusicGen</a></p></figcaption></figure>
<p>Interestingly, as of 2025, there is no single solution that â€œwonâ€: every paper does something different, and the schemes can get quite involved. Just look at this diagram from <a href="https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf" target="_blank" rel="noopener noreferrer">MiMo-Audio</a>, a model released in September 2025:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%205.png" alt=""></p><figcaption><p>Ways to deal with multiple RVQ levels can get quite involved</p></figcaption></figure>
<h2 id="finally-lets-train">Finally, let's train</h2>
<p>Time to finally train a codec-wrapped language model! As Iâ€™ve mentioned, <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">our code</a> is based on Andrej Karpathyâ€™s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT codebase</a> for training text LLMs. We just need to modify it to accept audio as input. But thatâ€™s easy, because LLMs donâ€™t care about what kind of tokens youâ€™re feeding in â€“ itâ€™s all just numbers. Once weâ€™ve tokenized the dataset and flattened it into a 1D sequence, weâ€™re good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.</p>
<p>Weâ€™re going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model canâ€™t even fit the dataset with 1k hours, so more data wouldnâ€™t save it.</p>
<p>I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from <a href="https://www.theotherpages.org/poems/field02.html" target="_blank" rel="noopener noreferrer">Michael Fieldâ€™s poem July</a>. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Letâ€™s see what kind of poetry we can get from our model:</p>

<p>There are some signs of life, but we donâ€™t have a poet yet. It sounds like somebody speaking behind a curtain. You canâ€™t really make out what itâ€™s saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.</p>
<p>It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.</p>
<h2 id="how-far-can-a-codec-get-us">How far can a codec get us?</h2>
<p>Our codec was deliberately simplistic, which explains why the results aren't greatâ€”but there's been a good amount of research on neural audio codecs in the last four years that we could leverage.
We wonâ€™t implement all the improvements here, but instead weâ€™ll look at what happens when we use <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a> as the tokenizer.</p>
<p>Mimi is a modern neural audio codec built here at Kyutai for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a>, our audio language model. Itâ€™s since been used as the tokenizer for other models as well, like <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesame CSM</a>, <a href="https://herimor.github.io/voxtream/" target="_blank" rel="noopener noreferrer">VoXtream</a>, and <a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a>.</p>
<p>Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.</p>
<p>Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. Thereâ€™s a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.</p>
<p>Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesnâ€™t rely on all levels being present. For our codec, we had to train separately.</p>
<p>Letâ€™s hear our example audio reconstructed with Mimi:</p>
<p>Original</p>


<p>For our purposes, a variant with fewer levels might have the advantage of being easier to model because itâ€™s more compressed. Letâ€™s train models with 8- and 32-level Mimi and compare the results.</p>
<p>I trained the exact same model architecture as before, the only thing that changes is the tokenizer. Itâ€™s 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.</p>
<p>Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec â€“ 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi itâ€™s â€œjustâ€ 54 GB.</p>
<p>Hereâ€™s a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:</p>

<p>Here is my best attempt at a transcription:</p>
<blockquote>
<p><em>When the grass is gone<br>
And corn still grassy;</em><br>
Illness worried in the fur<br>
this and pelan in stones<br>
during the turanâ€™s ciscerey<br>
headforths nepet Paul Twain.<br>
He <em>sees</em> zin in them.<br></p>
</blockquote>
<p>A tad too surrealist for my taste, but maybe Lewis Carroll would like it.</p>
<h2 id="semantic-tokens">Semantic tokens</h2>
<p>I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the â€œsemantic tokenâ€.</p>
<p>The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I wonâ€™t go into how these work, but in one sentence, Mimiâ€™s semantic tokens are distilled from <a href="https://arxiv.org/abs/2110.13900" target="_blank" rel="noopener noreferrer">WavLM</a>, which you can think of as a <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a> for speech.</p>
<p>To get a feeling for what information semantic tokens encode, letâ€™s take this example audio, passed through Mimi:</p>

<p>Now letâ€™s train a language model trained on the full Mimi, including semantic tokens. Weâ€™re going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (â€teacher-forcedâ€), but the model is free to decide the others according to what continuations it finds plausible.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/regenerateWithSemantic.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>We can get an idea of what information is contained in semantic tokens by
keeping them fixed and letting the model regenerate the rest.</p></figcaption></figure>
<p>Listen to two different reconstructions we obtain this way:</p>


<p>The voice is completely different, but itâ€™s saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. Thatâ€™s useful because it helps the model focus on <em>what</em> to say, not <em>how</em> to say it. In that regard, theyâ€™re closer to text tokens, which also donâ€™t contain information about the voice, intonation, timing, or emotion.</p>
<h2 id="making-poetry-semantic">Making poetry semantic</h2>
<p>Now letâ€™s take the model trained on semantic Mimi and ask it to complete the poem:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
from the man was nothing moan.<br>
The low death and heart<br>
She came <em>fyde</em> wood.<br>
A finteriest, a fall,<br>
all them.<br></p>
</blockquote>
<p>It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is â€œmore semanticâ€. The acoustic quality is the same, which is what weâ€™d expect.</p>
<p>Letâ€™s listen to a second poem:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
hope won and she<br>
who is just a night in Tatan<br>
in doe ock-ohm?<br>
the whom?<br></p>
</blockquote>
<p>Indeed, the whom?</p>
<h2 id="semanticacoustic-tradeoff">Semanticâ€“acoustic tradeoff</h2>
<p>We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Letâ€™s do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now itâ€™s 1/8 tokens and not just 1/32.</p>
<p>One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:</p>

<blockquote>
<p>Chapter 6 of The Founday, by R. Auclair.<br>
This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.<br>
Reading by: Kelvert</p>
</blockquote>
<p>Repeating the training data is generally not what you want, but in our case itâ€™s a great sign of life, because the previous models couldnâ€™t even manage that. It also makes up the book, author, and reader, so there is still novelty here.</p>
<p>Now letâ€™s try to make some more poetry:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
When so we could say<br>
that in fairy interesting wife<br>
who lay there and gone<br>
that save the rosy light of life<br>
Jay Dien, the antique mollity<br>
and a mollity the beast of gray failed summon<br></p>
<p>end of poem.</p>
<p>This recording is in the public domain.</p>
<p>[different voice]<br>
So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimilaâ€”</p>
</blockquote>
<p>This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word â€œmollityâ€ and then repeats it in the next line. Also, it realizes that itâ€™s reciting a poem and ends the section with â€œend of poemâ€. Then it decides itâ€™s the end of the chapter/section and it ends with the â€œThis recording is in the public domain.â€ disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.</p>
<p>We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound â€“ in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Weâ€™ve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, thatâ€™s not where the state of the art is in 2025 (and weâ€™re not trying to reach it here) but keep in mind that by using the <em>exact same model</em> without neural audio codecs gives us this:</p>

<p>Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, ChatGPTâ€™s <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">Advanced Voice Mode</a>, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) arenâ€™t able to tell you whether youâ€™re speaking in a high or low voice, despite the fact that theyâ€™re trained to natively understand audio. This is likely because theyâ€™re trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesnâ€™t help the models make more accurate predictions.</p>
<p>Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (<a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">demo</a>, <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">paper</a>), released in July 2024. Moshi might not be the AI youâ€™d pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAIâ€™s Advanced Voice Mode.</p>
<p>Moshi models an â€œinner monologueâ€ text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what itâ€™s going to say, and ablations showed that the text stream helps the model massively. At the same time, itâ€™s a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/moshi-figure-1.png" alt=""></p><figcaption><p><a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> models two audio streams and a text
stream in parallel</p></figcaption></figure>
<p>Itâ€™s not just Moshi: as the â€œam I speaking in a high voiceâ€ experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And thatâ€™s even though the dominant modeling approach is somewhat different than Moshiâ€™s: interleaving text and audio tokens instead of modeling them in parallel streams.</p>
<p>Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved â€œmodality gapâ€ makes audio ML an exciting field to work on.</p>
<p><strong>Thank you for reading! The code for the experiments is <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>, and for the animations <a href="https://github.com/kyutai-labs/neural-audio-codecs-anims" target="_blank" rel="noopener noreferrer">here</a>.</strong></p>
<h2 id="further-reading">Further reading</h2>
<p>Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.</p>
<p>van den Oord et al., 2016. <a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener noreferrer">WaveNet: A Generative Model for Raw Audio</a></p>
<ul>
<li>The OG, sample-by-sample audio continuation model.</li>
</ul>
<p>Mehri et al., 2016. <a href="https://arxiv.org/abs/1612.07837" target="_blank" rel="noopener noreferrer">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</a></p>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.10433" target="_blank" rel="noopener noreferrer">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a></p>
<p>Kumar et al., 2019. <a href="https://arxiv.org/abs/1910.06711" target="_blank" rel="noopener noreferrer">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a></p>
<p>Kong et al., 2020. <a href="https://arxiv.org/abs/2010.05646" target="_blank" rel="noopener noreferrer">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</a></p>
<ul>
<li>Various pre-codec improvements over WaveNet, mainly focused on efficiency.</li>
</ul>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">Neural Discrete Representation Learning</a></p>
<ul>
<li>Introduces VQ-VAE, originally for images.</li>
</ul>
<p>Esser et al., 2020. <a href="https://arxiv.org/abs/2012.09841" target="_blank" rel="noopener noreferrer">Taming Transformers for High-Resolution Image Synthesis</a></p>
<ul>
<li>VQGAN, successfully applies a similar two-stage recipe to what we showed here with audio. A VQ-VAE generates quantized image representations, and a transformer predicts them autoregressively, building the image row-by-row.</li>
</ul>
<p>Lakhotia et al., 2021. <a href="https://arxiv.org/abs/2102.01192" target="_blank" rel="noopener noreferrer">On Generative Spoken Language Modeling from Raw Audio</a></p>
<ul>
<li>The first paper to train a language model on discretized speech. As a tokenizer, it uses k-means to quantize latents from pre-trained speech representation models.</li>
</ul>
<p>Zeghidour et al., 2021. <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream: An End-to-End Neural Audio Codec</a></p>
<ul>
<li>Introduces RVQ for neural audio codecs.</li>
</ul>
<p>Lee et al., 2022. <a href="https://arxiv.org/abs/2203.01941" target="_blank" rel="noopener noreferrer">Autoregressive Image Generation using Residual Quantization</a></p>
<ul>
<li>Combines VQGAN with residual vector quantization.</li>
</ul>
<p>DÃ©fossez et al., 2022. <a href="https://arxiv.org/abs/2210.13438" target="_blank" rel="noopener noreferrer">High Fidelity Neural Audio Compression</a></p>
<ul>
<li>EnCodec, an early <a href="https://github.com/facebookresearch/encodec?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">open-source</a> neural audio codec. One interesting point is that they try out the <a href="https://arxiv.org/abs/1611.01144" target="_blank" rel="noopener noreferrer">Gumbel-Softmax</a>, which is a different way of dealing with the fact that quantization is non-differentiable.</li>
</ul>
<p>Hsu et al., 2021. <a href="https://arxiv.org/abs/2106.07447" target="_blank" rel="noopener noreferrer">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p>
<ul>
<li>A masked speech prediction model used to create semantic tokens in Mimi.</li>
</ul>
<p>DÃ©fossez et al., 2024. <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi: a speech-text foundation model for real-time dialogue</a></p>
<ul>
<li>Moshi, Kyutai's audio-native model. Models the user and assistant audio as parallel audio streams, and includes an assistant text stream to help guide the generation.
The paper also introduces the neural audio codec Mimi.</li>
</ul>
<p>Dieleman, 2025. <a href="https://sander.ai/2025/04/15/latents.html" target="_blank" rel="noopener noreferrer">Generative modelling in latent space</a></p>
<ul>
<li>A more high-level blog post about the general idea of using an encoder + generative model + decoder, where the (encoder, decoder) pair is trained separately from the generative model. A great read about where the field is and might be going.</li>
</ul>
<p>Peng et al., 2025. <a href="https://arxiv.org/abs/2508.19205" target="_blank" rel="noopener noreferrer">VibeVoice Technical Report</a></p>
<p>Rouard et al., 2025. <a href="https://arxiv.org/abs/2509.06926" target="_blank" rel="noopener noreferrer">Continuous Audio Language Models</a></p>
<ul>
<li>These works bypass the need for discrete tokens altogether by using diffusion or consistency models respectively, representing a promising alternative to RVQ.</li>
</ul>
<h2>Models</h2>
<p>Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.</p>
<p><a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a> (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model â€“ see above.</p>
<p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">CSM</a> (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.</p>
<p><a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen3-Omni</a> (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.</p>
<p><a href="https://github.com/XiaomiMiMo/MiMo-Audio" target="_blank" rel="noopener noreferrer">MiMo-Audio</a> (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.</p>
<p><a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a> (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just Use Curl (155 pts)]]></title>
            <link>https://justuse.org/curl/</link>
            <guid>45655121</guid>
            <pubDate>Tue, 21 Oct 2025 12:50:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justuse.org/curl/">https://justuse.org/curl/</a>, See on <a href="https://news.ycombinator.com/item?id=45655121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <p>What the fuck happened to making HTTP requests? You used to just type <code>curl example.com</code> and boom, you got your goddamn response. Now everyone's downloading 500MB Electron monstrosities that take 3 minutes to boot up just to send a fucking GET request.</p>

        <h2 id="yougotit">It's already on your machine, dipshit</h2>

        <p>You know what's better than downloading Postman? <em>Not downloading Postman.</em> cURL is already installed on your machine. It's been there since forever. It works. It's fast. It doesn't need to render a fucking Chromium instance to make a web request. <a href="https://news.ycombinator.com/item?id=45645172">It doesn't depend on a service to run.</a> <a href="https://github.com/postmanlabs/postman-app-support/issues/6999#issuecomment-2683282252">It doesn't require an "Enterprise" subscription for basic features.</a></p>

        <h2 id="itdoes">It actually does everything</h2>

        <p>Oh, you need to:</p>
        <ul>
            <li>Send POST requests? <code>curl -X POST</code></li>
            <li>Add headers? <code>curl -H "Header: value"</code></li>
            <li>Upload files? <code>curl -F file=@file.txt</code></li>
            <li>Handle cookies? <code>curl -c cookies.txt -b cookies.txt</code></li>
            <li>Follow redirects? <code>curl -L</code></li>
            <li>Basic auth? <code>curl -u user:pass</code></li>
            <li>OAuth? Yeah, it does that too.</li>
            <li>HTTP/2? HTTP/3? FTP? SFTP? SMTP? IMAP? POP3? LDAP? <a href="https://eissing.org/icing/posts/curl-websocket/">WebSocket</a>? Fucking <em>Gopher</em>? Yes to all of it.</li>
        </ul>

        <p>Meanwhile Postman is over here asking you to create an account to sync your "collections" to the cloud. <em>It's a fucking HTTP request, not your photo library.</em></p>

        <h2 id="ui-ux">The UI/UX is perfect</h2>

        <p>You know what has great UX? <em>The command line you're already using.</em> No clicking through 47 tabs. No "Workspaces." No "Environments" dropdown menu. Just type the fucking command. Your history is in your shell. Your "collections" are called shell scripts. Your "environments" are called environment variables, and they've existed since 1979.</p>

        <p>Want to save a request? Put it in a file. Want to share it with your team? It's text. Copy it. Paste it. Done. No JSON export/import bullshit. No proprietary formats.</p>

        <h2 id="itsfaster">It's faster than your bloated piece of shit</h2>

        <p>cURL executes in milliseconds. You know how long it takes Postman to start? Long enough to question your career choices. And don't even get me started on these new "modern" alternatives like Insomnia and HTTPie Desktop. Congratulations, you've turned a 2MB command-line tool into a 300MB desktop app. Progress!</p>

        <h2 id="pretty">But muh GraphQL, muh pretty interface!</h2>

        <p>Shut up. You can pipe cURL to <code>jq</code>:</p>

        <pre><code>curl -X POST https://api.example.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "{ users { name } }"}' \
  | jq '.'</code></pre>

        <p>Now you have syntax highlighting and JSON parsing. Total install size: ~10MB. Total startup time: instant. Total RAM usage: negligible. Total feelings of superiority: immeasurable.</p>

        <p>Don't trust yourself with JSON syntax? Fine, use <code>jo</code>:</p>

        <pre><code>curl -X POST https://api.example.com/api/users \
  -H "Content-Type: application/json" \
  -d "$(jo 'user[name]=John' 'user[email]=john@example.com')"</code></pre>

        <p>Beautiful. Fast. No Electron or React in sight.</p>

        <h2 id="fadq">Frequently Asked Dumb Questions</h2>
        <p><strong>Q: But I can't see my request history!</strong></p>
        <p>A: Yes you can, it's called <code>history | grep curl</code>. Or write your commands in a fucking file like an adult.</p>

        <p><strong>Q: How do I organize my requests?</strong></p>
        <p>A: Put your shell scripts into directories, genius.</p>

        <p><strong>Q: The syntax is hard to remember!</strong></p>
        <p>A: Type <code>man curl</code> or <code>curl --help</code>. Or literally just Google it once and save the command. You can remember 400 Kubernetes commands but not <code>curl -X POST</code>?</p>

        <p><strong>Q: What about team collaboration?</strong></p>
        <p>A: It's a text file. Put it in Git. You know, that thing you should be using anyway? Now your requests have version control, code review, and diffs. For free. Revolutionary, I know.</p>

        <p><strong>Q: But Postman has testing and automation!</strong></p>
        <p>A: So does cURL in a shell script with <code>||</code> and <code>&amp;&amp;</code> and actual programming languages. You want assertions? Pipe to <code>grep</code> or write a 3-line Python script. Done.</p>

        <p><strong>Q: What about cookie management?</strong></p>
        <p>A: <code>-c</code> to save cookies, <code>-b</code> to send them. This has been solved since 1999. Read the manual.</p>

        <h2 id="useit">Just use cURL</h2>

        <p>It's been downloaded over 20 billion times. It supports 25+ protocols. It's in cars, refrigerators, TV sets, routers, printers, phones, and every goddamn server on the planet. It's maintained by people who actually understand networking, not some VC-funded startup that'll slap "AI" on it next quarter.</p>

        <p>Stop using resource-hogging garbage. Stop creating accounts for basic functionality. Stop pretending you need a GUI to make HTTP requests.</p>

        <p><strong>Just use cURL.</strong></p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX is behind schedule, so NASA will open Artemis III contract to competition (139 pts)]]></title>
            <link>https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/</link>
            <guid>45655081</guid>
            <pubDate>Tue, 21 Oct 2025 12:43:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/">https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/</a>, See on <a href="https://news.ycombinator.com/item?id=45655081">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>NASA's Acting Administrator has admitted that SpaceX is behind in plans to return astronauts to the Moon, has reopened lander contract competition, and pushed the deadline for a lunar landing to the end of the Trump administration in 2029.</p>
<p>Elon Musk, the boss of SpaceX, <a target="_blank" rel="nofollow" href="https://x.com/elonmusk/status/1980335879945351303">fired back</a>: "SpaceX is moving like lightning compared to the rest of the space industry. Moreover, Starship will end up doing the whole Moon mission. Mark my words."</p>
<p>As we <a target="_blank" href="https://www.theregister.com/2025/10/16/spacexs_starship_two_down_a/">noted</a> last week, SpaceX has a mountain to climb to develop NASA's Human Landing System (HLS). After a slew of unplanned explosions, the company achieved two sub-orbital missions for its monster rocket - impressive, but still more than 200,000 miles (322,000 km) from the Moon.</p>

    

<p>NASA's patience has worn thin. Despite praising SpaceX as an "amazing company" doing "remarkable things," Acting Administrator Sean Duffy <a target="_blank" rel="nofollow" href="https://x.com/SecDuffyNASA/status/1980243865400701369">said</a> the company was "behind schedule" and he's opening the astronaut landing contract to competition. "The President wants to make sure we beat the Chinese. He wants to get there in his term."</p>

        


        

<p>So, Artemis III could be slipping to the end of 2028 (or January 2029 at a pinch), and SpaceX might not be doing the landing. Duffy called out Blue Origin, "and maybe others," as alternatives to Musk's rocketeers.</p>
<p>In 2021, SpaceX bagged the lunar lander <a target="_blank" href="https://www.theregister.com/2021/04/16/nasa_spacex_moon/">contract</a>, beating Jeff Bezos' Blue Origin and Dynetics. The inevitable lawsuit from Blue Origin was <a target="_blank" href="https://www.theregister.com/2021/08/16/blue_origin_lawsuit/">filed</a> in August that year, which halted work for a few months, before the claims were <a target="_blank" href="https://www.theregister.com/2021/11/05/blue_origin_nasa_spacex_court/">dismissed</a> in November, 2021.</p>

        

<p>The original 2024 landing target has already slipped to 2027 â€” but even that looks increasingly unrealistic. Artemis II won't launch until 2026, and in September, NASA's Aerospace Safety Advisory Panel <a target="_blank" href="https://www.theregister.com/2025/09/22/nasa_starship_artemis_doubts/">expressed serious doubts</a> about SpaceX's HLS readiness.</p>
<p>According to a New York Times <a target="_blank" rel="nofollow" href="https://www.nytimes.com/2025/09/20/us/politics/spacex-us-moon-race.html">report</a>, the HLS variant of Starship might not be ready until 2032. Musk <a target="_blank" rel="nofollow" href="https://x.com/elonmusk/status/1969492141152817636">dismissed it</a>: "It's not worth lining a parrot cage with NY Times, let alone reading it."</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/20/esa_helicopter_training/">Like Apollo before them, ESA astronauts hone lunar landing skills in helicopters</a></li>

<li><a href="https://www.theregister.com/2025/10/01/spacex_sets_the_eve_of/">SpaceX rockets toward next Starship launch, set for October 13</a></li>

<li><a href="https://www.theregister.com/2025/09/22/nasa_starship_artemis_doubts/">NASA panel fears a Starship lunar touchdown is more fantasy than flight plan</a></li>

<li><a href="https://www.theregister.com/2025/09/12/nasa_science_gets_a_boost/">US House Appropriations Committee saves NASA budget, Prez holds the veto pen</a></li>
</ul>
<p>Yet Duffy's announcement confirms NASA is finally acknowledging that SpaceX is behind and 2027 is wishful thinking rather than reality.</p>
<p>Blue Origin <a target="_blank" href="https://www.nasa.gov/centers-and-facilities/marshall/nasa-selects-blue-origin-as-second-artemis-lunar-lander-provider/">is currently scheduled</a> to land a crew on the Moon with Artemis V in <a target="_blank" href="https://www.nasa.gov/wp-content/uploads/2024/03/nasa-fiscal-year-2025-budget-summary.pdf">2030</a> [PDF, page 6]. As the Apollo program demonstrated, sufficient government funding can put boots on the regolith quickly. SpaceX can also rebid.</p>
<p>The bigger question is that with NASA's budget already struggling to maintain current <a target="_blank" href="https://www.theregister.com/2025/09/12/nasa_science_gets_a_boost/">science funding</a>, where will the agency find the cash needed to land astronauts before Trump's term ends? Â®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StarGrid: A Brand-New Palm OS Strategy Game in 2025 (188 pts)]]></title>
            <link>https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html</link>
            <guid>45654660</guid>
            <pubDate>Tue, 21 Oct 2025 11:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html">https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html</a>, See on <a href="https://news.ycombinator.com/item?id=45654660">Hacker News</a></p>
<div id="readability-page-1" class="page">
<a href="https://quarters.captaintouch.com/">Back to overview</a>

<p>This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.</p>

<p>Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:</p>
<video controls="" src="https://quarters.captaintouch.com/blog/posts/images/20251021_stargrid.mp4"><a src="images/20251021_stargrid.mp4">Video of StarGrid in action</a></video>
<p>StarGrid is a turn-based strategy game for Palm OS where you command a fleet of ships in a battle for control of the galaxy. Capture enemy flags, outmaneuver opposing fleets, and defend your own base in tense, tactical matches. Every move counts, will you strike boldly or play the long game to claim victory?</p>

<h2>Play it!</h2>
<p>No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.</p>
<a href="https://quarters.captaintouch.com/captainsstargrid.html">Game download and in-browser emulator</a>

<h2>How it was made</h2>
<p>Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another,  but learning so many neat things along the way. </p>

<p>Coding for Palm certainly comes with it's own obstacles:</p>
<p>- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving. </p>
<p>- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.</p>

<p>You can follow along the blog posts to see how I got here:</p>
<a href="https://quarters.captaintouch.com/blog/posts/2025-01-04-stargrid-a-new-game-im-making-for-palm-os-in-2025.html">StarGrid: A new game I'm making for Palm OS in 2025</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-03-06-building-the-cpu-player-for-stargrid.html">Building the CPU Player for StarGrid</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-05-06-2025-moving-out-of-the-vaporware-phase-stargrids-alpha-release-is-here">Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here!</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-08-14-stargrid-for-palm-os-almost-ready-(and-why-do-my-side-projects-always-explode-in-scope).html">StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)</a>

<p>Here's a video when playtesting the game on multiple Palm devices (cpu vs cpu action):</p>
<video controls="" src="https://quarters.captaintouch.com/blog/posts/images/20251021_stargrid_playtest.mp4"><a src="images/20251021_stargrid_playtest.mp4">Multi-device playtest</a></video>

<h2>What's next? </h2>
<p>I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now: </p>
<p>- making a top-down racing game (think micromachines)</p>
<p>- create an Outrun or Lotus III-like racing game</p>
<p>- building a ray-tracing game (like wolf3d). </p>
<p>Much more exciting stuff to come.</p>

<h2>Why do this?</h2>
<p>It's my way of keeping my favorite handheld operating system alive.</p>

<h2>It's all in the source!</h2>
<p>For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS. </p>
<a href="https://github.com/captaintouch/Captains_StarGrid_PalmOS">StarGrid on GitHub</a>

<h2>Tags</h2>
<p>RetroGames, PalmOS, Development, StarGrid</p>



<p>You can get in touch through Mastodon:  </p>
<a href="https://social.linux.pizza/@rxpz">@rxpz@social.linux.pizza</a>
<p>StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21</p>

<a href="https://quarters.captaintouch.com/blog/">Back to the overview</a>

<a href="https://quarters.captaintouch.com/blog/posts.xml" target="_blank" rel="noopener noreferrer">ğŸ“° Subscribe to RSS feed ğŸ“°</a>
<p><img src="https://quarters.captaintouch.com/blog/posts/images/mozilla.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/alienow.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/cassette.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/linux_now.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/ns-best.gif">
</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla is heading into multi-billion-dollar iceberg of its own making (222 pts)]]></title>
            <link>https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</link>
            <guid>45654635</guid>
            <pubDate>Tue, 21 Oct 2025 11:39:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/">https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</a>, See on <a href="https://news.ycombinator.com/item?id=45654635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>

	<img width="1600" height="909" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">
	</figure>

<p>Teslaâ€™s â€˜Full Self-Driving Supervisedâ€™ expansion is back firing as it exposes its shortcomings. Customers left without promised features are growing discontent and demanding to be compensated.</p>



<p>Itâ€™s turning into a multi-billion-dollar iceberg of Teslaâ€™s own making.</p>



<p>In 2016, Tesla proudly announced that all its vehicles produced onward are equipped with â€œall the hardware for full self-driving,â€ which would be delivered through future software updates.</p>



<p>The automaker turned out to be significantly wrong about that.</p>	
	



<p>At the time, it was producing its electric vehicles with a hardware suite known as HW2, which it had to upgrade to HW3 because it couldnâ€™t support self-driving (FSD) capability.</p>



<p>HW3 was produced in vehicles from 2019 to 2023 and Tesla switched to HW4 in 2024.</p>



<p>At first, CEO Elon Musk claimed that FSD software updates on newer HW4 cars would lag roughly 6 months behind updates to HW3 cars to make sure to deliver the promised self-driving capability to those who have been waiting and paid for the promised capabiltiy a long time ago.</p>



<p>That s<a href="https://electrek.co/2024/10/15/tesla-needs-to-come-clean-about-hw3-before-the-word-fraud-comes-out/">trategy barely lasted a few months</a>. Tesla quickly started releasing new FSD updates to HW4 cars first and it now hasnâ€™t released a significant update to HW3 cars in close to a year.</p>



<p>Tesla only admitted in January 2025 that HW3 wonâ€™t be able to support unsupervised self-driving. Musk claimed that Tesla would retrofit the computers, but there has been no word about it for 10 months.</p>



<h2 id="h-tesla-customers-are-starting-to-be-fed-up">Tesla customers are starting to be fed up.</h2>



<p>The catalyst is Teslaâ€™s current FSD expansion in international markets. Previously, Teslaâ€™s FSD was limited to North America, but over the last year, the automaker has been expanding FSD to China and now Australia and New Zealand.</p>



<p>However, the expansion is back-firing as HW3 owners are starting to realize that they will never get what they paid for.</p>



<p>In Australia and NZ, Tesla only launched FSD on HW4 vehicles with no clear plan for HW3, which the automaker already admitted wonâ€™t support unsupervised self-driving. The automaker appears to have only adapted its latest version of FSD for HW4 to the Australian market.</p>



<p>To add to the insult, with the launch of FSD in Australia, Tesla started to offer FSD subcriptions for $149 AUD a month for both HW3 and HW3 cars despite the software not being available for HW3.</p>



<p>HW3 owners reached out to <em>Electrek</em> after seeing this in their app:</p>



<figure><img decoding="async" width="926" height="922" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png 926w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=150,149 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=300,300 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=768,765 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=350,348 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=140,139 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=96,96 96w" sizes="(max-width: 926px) 100vw, 926px"></figure>



<p>Itâ€™s unclear why would Tesla sell a subcription to something that doesnâ€™t even exist, but it is not helping build confidence with customers.</p>



<p>To try to appease owners, Tesla started sending emails to Australia HW3 owners offering $5,000 discounts on new inventory vehicles when transfering their FSD package:</p>



<figure><img loading="lazy" decoding="async" height="1024" width="546" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?w=546" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png 618w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=80,150 80w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=160,300 160w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=546,1024 546w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=187,350 187w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=140,262 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=534,1000 534w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=150,281 150w" sizes="auto, (max-width: 546px) 100vw, 546px"></figure>



<p>However, this offer is misleading in itself, as it is not actually specific to HW3 owners as the email leads people to believe.</p>



<p>A visit on Teslaâ€™s Australia inventory website shows that Tesla is offering a $5,000 disounct on all inventory vehicles with FSD for any buyer:</p>



<figure><img loading="lazy" decoding="async" height="384" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?w=1024" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png 2966w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=150,56 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=300,112 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=768,288 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1024,384 1024w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1536,576 1536w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=2048,768 2048w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=350,131 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=140,52 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1600,600 1600w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Therefore, it has nothing to do with â€œloyaltyâ€.</p>



<p>As we recently reported, thousands of Tesla owners have now joined <a href="https://electrek.co/2025/10/13/thousands-of-tesla-owners-join-class-action-lawsuit-over-full-self-driving-in-australia/">a class action lawsuit in Australia</a> over Tesla misleading customers with its self-driving promises.</p>



<p>It adds to similar ongoing lawsuits in <a href="https://electrek.co/2025/08/19/tesla-loses-bid-to-kill-class-action-over-misleading-customers-on-self-driving-capabilities-for-years/">the US</a> and <a href="https://electrek.co/2025/09/22/tesla-being-sued-china-over-not-delivering-self-driving-hw3-cars/">China</a>.</p>



<p>With hundreds of thousands of FSD customers who paid up to $15,000 for package, Tesla is on the hook for billions of dollars in compensations or retrofits in the best-case scenario.</p>



<h2 id="h-electrek-s-take">Electrekâ€™s Take</h2>



<p>We are seeing more people losing patience and it is only going to get worse.</p>



<p>There were a lot of interesting interactions on this post, which is pretty mild in my opinion. And yet, you see the usual Elon lemmings downplaying Tesla not delivering features it promised:</p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Dear <a href="https://twitter.com/Tesla_AI?ref_src=twsrc%5Etfw">@Tesla_AI</a> Team,<br>I am writing on behalf of the community of Tesla owners equipped with Hardware 3 (HW3) who have purchased the Full Self-Driving (FSD) capability. As dedicated supporters of Teslaâ€™s mission to accelerate the worldâ€™s transition to sustainable energy and advanceâ€¦</p>â€” shawn.carâ—¼ï¸â—¼ï¸â—¼ï¸â—¼ï¸â—¼ï¸â—¼ï¸ (@shawncarelli) <a href="https://twitter.com/shawncarelli/status/1980023896536391866?ref_src=twsrc%5Etfw">October 19, 2025</a></blockquote>
</div></figure>



<p>I donâ€™t want to burst anyoneâ€™s bubble, but we need to be realistic here. If you are a HW3 owner and still think that Tesla is going to retrofit your up to 10-years-old car with a computer that is going to make self-driving, you are being delusional.</p>



<p>Tesla will have to end up compensating owners and at this point, I have serious doubts that it will do it by itself without being forced through courts.</p>



<p>Furthermore, it shouldnâ€™t be just people who bought FSD. Tesla said that all cars had the hardware capable of self-driving whether people bought the software package or not. If thatâ€™s not true, it affects the resale value of the vehicle regardless of if someone purchased the package.</p>



<p>I have a fairly simple solution for Tesla to make it right.</p>



<p>Tesla needs to offer all HW3 owners a $5,000 loyalty discount, that goes on top of all other incentive program, when upgrading to a new car. </p>




	<p>As for HW3 owners who bought FSD, which basically turned out to be an interest free loan to Tesla for years, the automaker needs to offer free FSD transfer and a $10,000 discount on a car upgrade.</p>



<p>While this might sound like a lot, I think itâ€™s in line with the incredible liability that Tesla is facing from all the on going lawsuits. </p>



<p>On top of it, it will go a long way to regain the trust of long-time customers, which Tesla swindled by selling them features it simply canâ€™t deliver.</p>



<p>The main reason why I think Tesla doesnâ€™t want to do that is that it will likely have to do the same thing to HW4 owners in the next few years and that would be the death of the company.</p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diamond Thermal Conductivity: A New Era in Chip Cooling (128 pts)]]></title>
            <link>https://spectrum.ieee.org/diamond-thermal-conductivity</link>
            <guid>45654512</guid>
            <pubDate>Tue, 21 Oct 2025 11:16:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/diamond-thermal-conductivity">https://spectrum.ieee.org/diamond-thermal-conductivity</a>, See on <a href="https://news.ycombinator.com/item?id=45654512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Diamond Blankets Will Keep Future Chips Cool"><p><strong>Todayâ€™s </strong><strong>stunning computing power</strong> is allowing us to move from human intelligence toward <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">artificial intelligence</a>. And as our machines gain more power, theyâ€™re becoming not just tools but decision-makers shaping our future.</p><p>But with great power comes greatâ€¦heat!</p><p>As nanometer-scale <a href="https://spectrum.ieee.org/tag/transistors">transistors</a> switch at gigahertz speeds, electrons race through circuits, losing energy as heatâ€”which you feel when your laptop or your phone toasts your fingers. As weâ€™ve <a href="https://spectrum.ieee.org/trillion-transistor-gpu" target="_self">crammed more and more transistors onto chips</a>, weâ€™ve lost the room to release that heat efficiently. Instead of the heat spreading out quickly across the silicon, which makes it much easier to remove, it builds up to form hot spots, which can be tens of degrees warmer than the rest of the chip. That extreme heat forces systems to throttle the performance of CPUs and <a href="https://spectrum.ieee.org/tag/gpus">GPUs</a> to avoid degrading the chips.</p><p>In other words, what began as a quest for miniaturization has turned into a battle against thermal energy. This challenge extends across all electronics. In computing, high-performance <a href="https://spectrum.ieee.org/tag/processors">processors</a> demand ever-increasing power densities. (New <a href="https://spectrum.ieee.org/tag/nvidia">Nvidia</a> GPU B300 servers will consume <a href="https://www.nvidia.com/en-us/data-center/dgx-b300/" target="_blank">nearly 15 kilowatts</a> of power.) In communication, both digital and analog systems push transistors to deliver more power for stronger signals and faster data rates. In the <a href="https://spectrum.ieee.org/tag/power-electronics">power electronics</a> used for energy conversion and distribution, efficiency gains are being countered by thermal constraints.</p><p> <img alt="A thick sheet of gray-scale grains." data-rm-shortcode-id="4095938da35f1f8180986be8c5d070c9" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-thick-sheet-of-gray-scale-grains.png?id=61766971&amp;width=980" height="1609" id="1d15b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-thick-sheet-of-gray-scale-grains.png?id=61766971&amp;width=980" width="2427"><small placeholder="Add Photo Caption...">The ability to grow large-grained <a href="https://spectrum.ieee.org/tag/polycrystalline">polycrystalline</a> diamond at low temperature led to a new way to combat heat in transistors. </small><small placeholder="Add Photo Credit...">Mohamadali Malakoutian</small></p><p>Rather than allowing heat to build up, what if we could spread it out right from the start, inside the chip?â€”diluting it like a cup of boiling water dropped into a swimming pool. Spreading out the heat would lower the temperature of the most critical devices and circuits and let the other time-tested cooling technologies work more efficiently. To do that, weâ€™d have to introduce a highly thermally conductive material inside the IC, mere nanometers from the transistors, without messing up any of their very precise and sensitive properties. Enter an unexpected materialâ€”diamond.</p><p>In some ways, diamond is ideal. Itâ€™s one of the most thermally conductive materials on the planetâ€”many times more efficient than copperâ€”yet itâ€™s also electrically insulating. However, integrating it into chips is tricky: Until recently we knew how to grow it only at circuit-slagging temperatures in excess of 1,000 Â°C.</p><p>But my research group at <a href="https://spectrum.ieee.org/tag/stanford">Stanford</a> University has managed what seemed impossible. We can now grow a form of diamond suitable for spreading heat, directly atop semiconductor devices at low enough temperatures that even the most delicate <a href="https://spectrum.ieee.org/tag/interconnects">interconnects</a> inside advanced chips will survive. To be clear, this isnâ€™t the kind of diamond you see in jewelry, which is a large single crystal. Our <a href="https://spectrum.ieee.org/tag/diamond">diamonds</a> are a polycrystalline coating no more than a couple of micrometers thick.</p><p>The potential benefits could be huge. In some of our earliest gallium-nitride radio-frequency transistors, the addition of diamond dropped the device temperature by more than 50 Â°C. At the lower temperature, the transistors amplified X-band radio signals five times as well as before. We think our diamond will be even more important for advanced <a href="https://spectrum.ieee.org/tag/cmos">CMOS</a> chips. Researchers predict that upcoming chipmaking technologies could make hot spots almost 10 Â°C hotter [see , â€œ<a href="https://spectrum.ieee.org/hot-chips" target="_self">Future Chips Will Be Hotter Than Ever</a>â€, in this issue]. Thatâ€™s probably why our research is drawing intense interest from the chip industry, including <a href="https://spectrum.ieee.org/tag/applied-materials">Applied Materials</a>, <a href="https://spectrum.ieee.org/tag/samsung">Samsung</a>, and <a href="https://spectrum.ieee.org/tag/tsmc">TSMC</a>. If our work continues to succeed as it has, heat will become a far less onerous constraint in CMOS and other electronics too.</p><h2>Where Heat Begins and Ends in Chips</h2><p data-rm-resized-container="25%"> <img alt="A rectangle of black fading into bright gray at the bottom." data-rm-shortcode-id="97669d7d6818879a240fd8be27d98ec0" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-rectangle-of-black-fading-into-bright-gray-at-the-bottom.png?id=61766985&amp;width=980" height="3532" id="d8b96" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-rectangle-of-black-fading-into-bright-gray-at-the-bottom.png?id=61766985&amp;width=980" width="1646"><small placeholder="Add Photo Caption...">At the boundary between the diamond and the semiconductor, a thin layer of <a href="https://spectrum.ieee.org/tag/silicon-carbide">silicon carbide</a> forms. It acts as a bridge for heat to flow into the diamond. </small><small placeholder="Add Photo Credit...">Mohamadali Malakoutian</small></p><p>Heat starts within transistors and the interconnects that link them, as the flow of current meets resistance. That means most of it is generated near the surface of the semiconductor substrate. From there it rises either through layers of metal and insulation or through the semiconductor itself, depending on the package architecture. The heat then encounters a thermal interface material designed to spread it out before it ultimately reaches a <a href="https://spectrum.ieee.org/tag/heat-sink">heat sink</a>, a radiator, or some sort of <a href="https://spectrum.ieee.org/data-center-liquid-cooling" target="_blank">liquid cooling</a>, where air or fluid carries the heat away.</p><p>The dominant cooling strategies today center around advances in <a href="https://spectrum.ieee.org/tag/heat-sinks">heat sinks</a>, fans, and radiators. In pursuit of even better cooling, researchers have explored liquid cooling using microfluidic channels and removing heat using phase-change materials. Some computer clusters go so far as to submerge the servers in thermally conductive, dielectricâ€”electrically insulatingâ€”liquids.</p><p>These innovations are critical steps forward, but they still have limitations. Some are so expensive theyâ€™re worthwhile only for the highest-performing chips; others are simply too bulky for the job. (Your smartphone canâ€™t carry a <a href="https://spectrum.ieee.org/xmems" target="_self">conventional fan</a>.) And none are likely to be very effective as we move toward chip architectures resembling silicon skyscrapers that stack multiple layers of chips. Such <a href="https://spectrum.ieee.org/hybrid-bonding" target="_self">3D systems</a> are only as viable as our ability to remove heat from every layer within it.</p><p>The big problem is that chip materials are poor heat conductors, so the heat becomes trapped and concentrated, causing the temperature to skyrocket within the chip. At higher temperatures, transistors leak more current, wasting power; they age more quickly, too.</p><p>Heat spreaders allow the heat to move laterally, diluting it and allowing the circuits to cool. But theyâ€™re positioned farâ€”relatively, of courseâ€”from where the heat is generated, and so theyâ€™re of little help with these hot spots. We need a heat-spreading technology that can exist within nanometers of where the heat is generated. This is where our new low-temperature diamond could be essential.</p><h2>How to Make Diamonds</h2><p>Before my lab turned to developing diamond as a heat-spreading material, we were working on it as a semiconductor. In its single-crystal formâ€”like the kind on your fingerâ€”it has a <a href="https://spectrum.ieee.org/tag/wide-bandgap">wide bandgap</a> and ability to withstand enormous electric fields. Single-crystalline diamond also offers some of the highest thermal conductivity recorded in any material, reaching 2,200 to 2,400 watts per meter per kelvinâ€”roughly six times as conductive as copper. Polycrystalline diamondâ€”an easier to make materialâ€”can approach these values when grown thick. Even in this form, it outperforms copper.</p><p>As attractive as diamond transistors might be, I was keenly awareâ€”based on my experience researching <a href="https://spectrum.ieee.org/tag/gallium-nitride">gallium nitride</a> devicesâ€”of the long road ahead. The problem is one of scale. Several companies are working to scale high-purity diamond substrates to 50, 75, and even 100 millimeters but the diamond substrates we could acquire commercially were only about 3 mm across.</p><p> <img alt="A polygon with layers demarcated in it surrounded by a jagged blue area. " data-rm-shortcode-id="4d352398f974422975b19c4e1d3f5ecd" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-polygon-with-layers-demarcated-in-it-surrounded-by-a-jagged-blue-area.png?id=61771341&amp;width=980" height="1760" id="38b33" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-polygon-with-layers-demarcated-in-it-surrounded-by-a-jagged-blue-area.png?id=61771341&amp;width=980" width="3200"><small placeholder="Add Photo Caption...">Gallium nitride high-electron-mobility transistors were an ideal test case for diamond cooling. The devices are 3D and the critical heat-generating part, the two-dimensional <a href="https://spectrum.ieee.org/tag/electron-gas">electron gas</a>, is close to the surface. </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>So we decided instead to try growing diamond films on large silicon wafers, in the hope of moving toward commercial-scale diamond substrates. In general, this is done by reacting methane and hydrogen at high temperatures, 900 Â°C or more. This results in not a single crystal but a forest of narrow columns. As they grow taller, the nanocolumns coalesce into a uniform film, but by the time they form high-quality polycrystalline diamond, the film is already very thick. This thick growth adds stress to the material and often leads to cracking and other problems.</p><p>But what if we used this polycrystalline coating as a heat spreader for other devices? If we could get diamond to grow within nanometers of transistors, get it to spread heat both vertically and laterally, and integrate it seamlessly with the silicon, metal, and <a href="https://spectrum.ieee.org/tag/dielectric">dielectric</a> in chips, it might do the job.</p><p>There were good reasons to think it would work. Diamond is electrically insulating, and it has a relatively low dielectric constant. That means it makes a poor capacitor, so signals sent through diamond-encrusted interconnects might not degrade much. Thus diamond could act as a â€œthermal dielectric,â€ one that is electrically insulating but thermally conducting.</p><p> <img alt="SEM images showing surface before and after polycrystalline diamond growth on silicon oxide." data-rm-shortcode-id="d13cce8207e028b3de6559e6ff93e683" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/sem-images-showing-surface-before-and-after-polycrystalline-diamond-growth-on-silicon-oxide.png?id=61771368&amp;width=980" height="1980" id="94099" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/sem-images-showing-surface-before-and-after-polycrystalline-diamond-growth-on-silicon-oxide.png?id=61771368&amp;width=980" width="2280"><small placeholder="Add Photo Caption...">Polycrystalline diamond could help reduce temperatures inside <a href="https://spectrum.ieee.org/tag/3d-integration">3D chips</a>. Diamond thermal vias would grow inside micrometers-deep holes so heat can flow from vertically from one chip to a diamond heat spreader in another chip thatâ€™s stacked atop it.  </small><small placeholder="Add Photo Credit...">Dennis Rich</small></p><p>For our plan to work, we were going to have to learn to grow diamond differently. We knew there wasnâ€™t room to grow a thick film inside a chip. We also knew the narrow, spiky crystal pillars made in the first part of the growth process donâ€™t transmit heat laterally very well, so weâ€™d need to grow large-grained crystals from the start to get the heat moving horizontally. A third problem was that the existing diamond films didnâ€™t form a coating on the sides of devices, which would be important for inherently <a href="https://spectrum.ieee.org/tag/3d-devices">3D devices</a>. But the biggest impediment was the high temperature needed to grow the diamond film, which would damage, if not destroy, an ICâ€™s circuits. We were going to have to cut the growth temperature at least in half.</p><p>Just lowering the temperature doesnâ€™t work. (We tried: You wind up, basically, with soot, which is electrically conductiveâ€”the opposite of whatâ€™s needed.) We found that adding oxygen to the mix helped, because it continuously etched away carbon deposits that werenâ€™t diamond. And through <a href="https://www.mdpi.com/2073-4352/9/10/498" target="_blank">extensive experimentation</a>, we were able to find a formula that produced coatings of large-grained polycrystalline diamond all around devices at 400 Â°C, which is a survivable temperature for CMOS circuits and other devices.</p><h2>Thermal Boundary Resistance</h2><p>Although we had found a way to grow the right kind of diamond coatings, we faced another critical challengeâ€”the <a href="https://spectrum.ieee.org/tag/phonon">phonon</a> bottleneck, also known as thermal boundary resistance (TBR). <a href="https://spectrum.ieee.org/tag/phonons">Phonons</a> are packets of heat energy, in the way that photons are packets of electromagnetic energy. Specifically, theyâ€™re a quantized version of the vibration of a crystal lattice. These phonons can pile up at the boundary between materials, resisting the flow of heat. Reducing TBR has long been a goal in thermal interface engineering, and it is often done by introducing different materials at the boundary. But semiconductors are compatible only with certain materials, limiting our choices.</p><p data-rm-resized-container="25%"> <img alt="A cartoon of squares stacked atop one another and connected by a forest of vertical links. " data-rm-shortcode-id="0d42a3ab5d16646f88e7d980f17f5817" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-cartoon-of-squares-stacked-atop-one-another-and-connected-by-a-forest-of-vertical-links.png?id=61771378&amp;width=980" height="1286" id="386f9" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-cartoon-of-squares-stacked-atop-one-another-and-connected-by-a-forest-of-vertical-links.png?id=61771378&amp;width=980" width="1113"><small placeholder="Add Photo Caption...">Thermal scaffolding would link layers of heat-spreading polycrystalline diamond in one chip to those in another chip in a 3D-stacked silicon. The thermal pillars would traverse each chipâ€™s interconnects and dielectric material to move heat vertically through the stack. </small><small placeholder="Add Photo Credit...">Srabanti Chowdhury</small></p><p>In the end, we got lucky. While growing diamond on GaN capped with silicon nitride, we observed something unexpected: The measured TBR was <a href="https://pubs.acs.org/doi/full/10.1021/acsami.1c13833" target="_blank">much lower than prior reports led us to expect</a>. (The low TBR was independently measured, initially by <a href="https://research-information.bris.ac.uk/en/persons/martin-h-h-kuball" target="_blank">Martin Kuball</a> at the University of Bristol, in England, and later by <a href="https://me.umd.edu/clark/faculty/1618/Samuel-GrahamJr" target="_blank">Samuel Graham Jr</a>., then at <a href="https://spectrum.ieee.org/tag/georgia-tech">Georgia Tech</a>, who both have been coauthors and collaborators in several of our papers.)</p><p>Through further investigation of the interface science and engineering, and in collaboration with <a href="https://mse.utdallas.edu/ourteam/faculty/cho-k/" target="_blank">K.J. Cho</a> at the University of Texas at Dallas, we identified the cause of the lower TBR. <a href="https://ieeexplore.ieee.org/document/10413734" target="_blank">Intermixing at the interface</a> between the diamond and silicon nitride led to the formation of silicon carbide, which acted as a kind of bridge for the phonons, allowing more efficient heat transfer. Though this began as a scientific discovery, its technological impact was immediateâ€”with a silicon carbide interface, our devices exhibited significantly improved thermal performance.</p><h2>GaN HEMTs: The First Test Case</h2><p>We began testing our new low-TBR diamond coatings in gallium nitride high-electron-mobility transistors (HEMTs). These devices amplify RF signals by controlling current through a two-dimensional electron gas that forms within its channel. We leveraged the pioneering research on HEMTs done by <a href="https://engineering.ucsb.edu/people/umesh-mishra" target="_blank">Umesh Mishra</a>â€™s laboratory at the University of California, Santa Barbara, where I had been a graduate student. The Mishra lab invented a particular form of the material called N-polar gallium nitride. Their N-polar GaN HEMTs demonstrate exceptional power density at high frequencies, particularly in the W-band, the 75- to 110-gigahertz part of the microwave spectrum.</p><p>RELATED: <a href="https://spectrum.ieee.org/silicon-carbide" target="_self">Gallium Nitride and Silicon Carbide Fight for Green Tech Domination</a></p><p>What made these HEMTs such a good test case is one defining feature of the device: The gate, which controls the flow of current through the device, is within tens of nanometers of the transistorâ€™s channel. That means that heat is generated very close to the surface of the device, and any interference our diamond coating could cause would quickly show in the deviceâ€™s operation.</p><p>We introduced the diamond layer so that it surrounded the HEMT completely, even on the sides. By maintaining a growth temperature below 400 Â°C, we hoped to preserve core device functionality. While we did see some decline in high-frequency performance, the thermal benefits were substantialâ€”<a href="https://ieeexplore.ieee.org/document/10019509" target="_blank">channel temperatures dropped by a remarkable 70 Â°C</a>. This breakthrough could be a potentially transformative solution for RF systems, allowing them to operate at higher power than ever before.</p><h2>Diamond in CMOS</h2><p>We wondered if our diamond layer could also work in high-power CMOS chips. My colleagues at Stanford, <a href="https://web.stanford.edu/~hspwong/" target="_blank">H.-S. Philip Wong</a> and <a href="https://profiles.stanford.edu/subhasish-mitra" target="_blank">Subhasish Mitra</a>, have long championed 3D-stacked chip architectures. In CMOS computing chips, <a href="https://spectrum.ieee.org/tag/3d-stacking">3D stacking</a> appears to be the most viable way forward to increase integration density, improve performance, and overcome the limitations of traditional <a href="https://spectrum.ieee.org/tag/transistor-scaling">transistor scaling</a>. Itâ€™s already used in some advanced <a href="https://spectrum.ieee.org/tag/ai-chips">AI chips</a>, such as <a href="https://spectrum.ieee.org/amd-mi300" target="_self">AMDâ€™s MI300 series</a>. And itâ€™s established in the high-bandwidth memory chips that pump data through Nvidia GPUs and other AI processors. The multiple layers of silicon in these 3D stacks are mostly connected by microscopic balls of solder, or in some advanced cases just by their copper terminals. Getting signals and power out of these stacks requires vertical copper links that burrow through the silicon to reach the chip packageâ€™s substrate.</p><p>In one of our discussions, Mitra pointed out that a critical issue with 3D-stacked chips is the thermal bottlenecks that form within the stack. In 3D architectures, the traditional heat sinks and other techniques used for 2D chips arenâ€™t sufficient. Extracting heat from each layer is essential.</p><p>Our research could redefine <a href="https://spectrum.ieee.org/tag/thermal-management">thermal management</a> across industries.</p><p>Our experiments on thermal boundary resistance in GaN suggested a similar approach would work in silicon. And when we integrated diamond with silicon, the results were remarkable: An interlayer of silicon carbide formed, leading to diamond with an excellent thermal interface.</p><p>Our effort introduced the concept of thermal scaffolding. In that scheme, nanometers-thick layers of polycrystalline diamond would be integrated within the dielectric layers above the transistors to spread heat. These layers would then be connected by vertical heat conductors, called thermal pillars, made of copper or more diamond. These pillars would connect to another heat spreader, which in turn would link to thermal pillars on the next chip in the 3D stack, and so on until the heat reached the heat sink or other cooling device.</p><p> <img alt="Temperature vs. compute tier graph; AI accelerator heats most without scaffold." data-rm-shortcode-id="f142bfc12529f56435398ad67060129f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/temperature-vs-compute-tier-graph-ai-accelerator-heats-most-without-scaffold.png?id=61771384&amp;width=980" height="1465" id="8310e" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/temperature-vs-compute-tier-graph-ai-accelerator-heats-most-without-scaffold.png?id=61771384&amp;width=980" width="2380"><small placeholder="Add Photo Caption...">The more tiers of computing silicon in a 3D chip, the bigger difference thermal scaffolding makes. An AI accelerator with more than five tiers would well exceed typical temperature limits unless the scaffolding was employed. </small><small placeholder="Add Photo Credit...">Srabanti Chowdhury</small></p><p>In a collaboration with Mitra, we used simulations of heat generated by real computational workloads to operate a proof-of-concept structure. This structure consisted of dummy heaters to mimic hot spots in a two-chip stack along with diamond heat spreaders and copper thermal pillars. Using this, we <a href="https://ieeexplore.ieee.org/document/10873424" target="_blank">reduced the temperature to one-tenth</a> its value without the scaffolding.</p><p>There are hurdles still to overcome. In particular, we still have to figure out a way to make the top of our diamond coatings atomically flat. But, in collaboration with industry partners and researchers, we are systematically studying that problem and other scientific and technological issues. We and our partners think this research could offer a disruptive new path for thermal management and a crucial step toward sustaining <a href="https://spectrum.ieee.org/tag/high-performance-computing">high-performance computing</a> into the future.</p><h2>Developing Diamond Thermal Solutions</h2><p>We now intend to move toward industry integration. For example, weâ€™re working with the <a href="https://www.darpa.mil/research/programs/threads-heat-removal" target="_blank">Defense Advanced Research Projects Agency Threads</a> program, which aims to use device-level thermal management to develop highly efficient and reliable X-band power amplifiers with a power density 6 to 8 times as efficient as todayâ€™s devices. The program, which was conceived and initially run by <a href="https://forward.darpa.mil/presenters/Dr-Thomas-Kazior" target="_blank">Tom Kazior</a>, is a critical platform for validating the use of low-temperature diamond integration in GaN HEMT manufacturing. Itâ€™s enabled us to collaborate closely with industry teams while protecting both our and our partnersâ€™ processes. Defense applications demand exceptional reliability, and our diamond-integrated HEMTs are undergoing rigorous testing with industry partners. The early results are promising, guiding refinements in growth processes and integration techniques that weâ€™ll make with our partners over the next two years.</p><p>But our vision extends beyond GaN HEMTs to <a href="https://iopscience.iop.org/article/10.35848/1882-0786/abf4f1" target="_blank">other materials</a> and particularly silicon computational chips. For the latter, we have an established collaboration with TSMC, and weâ€™re expanding on newer opportunities with Applied Materials, <a href="https://spectrum.ieee.org/tag/micron">Micron</a>, Samsung, and others through the <a href="https://systemx.stanford.edu/" target="_blank">Stanford SystemX Alliance</a> and the <a href="https://www.src.org/" target="_blank">Semiconductor Research Corp.</a> This is an extraordinary level of collaboration among otherwise fierce competitors. But then, heat is a universal challenge in <a href="https://spectrum.ieee.org/tag/chip-manufacturing">chip manufacturing</a>, and everyone is motivated to find the best solutions.</p><p>If successful, our research could redefine thermal management across industries. In my work on gallium nitride devices, I have seen firsthand how once-radical ideas like this transition to become industry standards, and I believe diamond-based heat extraction will follow the same trajectory, becoming a critical enabler for a generation of electronics that is no longer hindered by heat. <span></span></p><p><em>This article appears in the November 2025 print issue as â€œDiamond Blankets Will Chill Future Chips.â€</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US chess grandmaster Daniel Naroditsky dies aged 29 (125 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c15pz8vpjp9o</link>
            <guid>45654382</guid>
            <pubDate>Tue, 21 Oct 2025 10:44:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c15pz8vpjp9o">https://www.bbc.com/news/articles/c15pz8vpjp9o</a>, See on <a href="https://news.ycombinator.com/item?id=45654382">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Harry Sekulich</span><span data-testid="undefined-role-location"></span><span> and</span></p><p><span>Gabriela Pomeroy</span><span data-testid="undefined-role-location"></span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp" loading="eager" alt="Charlotte Chess Center Close-up shot of Daniel Naroditsky wearing a deep navy polo"><span>Charlotte Chess Center</span></p></div><p data-component="caption-block"><figcaption>Daniel Naroditsky, also known to his online fans as 'Danya', died two weeks out from his 30th birthday</figcaption></p></figure><div data-component="text-block"><p>US chess grandmaster and online commentator Daniel Naroditsky has died aged 29.</p><p>The popular chess player's family announced his "unexpected" death in a statement released by his club, the Charlotte Chess Center, on Monday. No cause of death was given.</p><p>"It is with great sadness that we share the unexpected passing of Daniel Naroditsky," the statement said. "Daniel was a talented chess player, commentator and educator, and a cherished member of the chess community, admired and respected by fans and players around the world."</p><p>The US and International chess federations have paid tribute to Naroditsky, along with other professional players.</p></div><div data-component="text-block"><p>American world number two Hikaru Nakamura said he was "devastated" at the news.</p><p>"This is a massive loss for the world of chess," Nakamura said in a social media post.</p><p>As well as competing in high-level events, Naroditsky ran a chess YouTube channel, with nearly 500,000 subscribers. </p><p>His Twitch stream drummed up 340,000 followers, with hundreds of thousands of viewers drawn to his regular video tutorials and livestreams against competitors. Fans praised his insight and passion, casually referring to him as 'Danya'.</p><p>He played a "pivotal role in popularising chess content online," the International Chess Federation said. </p><p>Naroditsky first took an interest in chess at the age of six, when his older brother Alan introduced him to the game to help entertain a group of children at a birthday party.</p><p>His father Vladimir and multiple coaches soon noticed his talents.</p><p>"As far as I was concerned, I was just playing games with my brother," Naroditsky told the New York Times in a 2022 interview.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp" loading="lazy" alt="Getty Images A young Daniel Naroditsky sitting behind a chessboard "><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Naroditsky in 2008, following his World Youth Championship victory in Turkey</figcaption></p></figure><div data-component="text-block"><p>He gained international attention in 2007 when he won the under-12 boys world youth championship in Antalya, Turkey. In 2010, at the age of 14, he became one of the youngest ever published chess authors when he wrote a book titled Mastering Positional Chess, covering practical skills and technical manoeuvrings.</p><p>In 2013 Naroditsky won the US Junior Championship, helping him earn the title of grandmaster, the international chess federation's highest-ranked chess competitor, while he was still a teenager.</p><p>Naroditsky later graduated from Stanford University and worked as a chess coach in Charlotte, North Carolina.</p><p>In 2022 the New York Times named Naroditsky as its "new chess columnist" and invited him to contribute to a series of chess puzzles for the newspaper's games section.</p><p>In the publication's accompanying interview, the young grandmaster mused on chess's influence in his life.</p><p>"Even at my level, I can still discover beautiful things about the game every single time I train, teach, play or am a commentator at a tournament," he said.</p></div><div data-component="text-block"><p>Nemo Zhou â€“  a Toronto-based Woman Chess Grandmaster (WGM) and chess content creator â€“ told the BBC Naroditsky was a friend and an "inspiration."</p><p>Zhou played chess with him, both in person and virtually at chess events across the US.</p><p>He was "everything that the combination of chess and content creation was supposed to be â€“ he had this way to make chess fun", she said. </p><p>She added that he was known for being a "true historian of the game" who had a great memory for chess facts and historical games, and "did everything with kindness."</p><p>"Without people like him I probably would have quit chess at 17 and never touched it again," she said. </p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp" loading="lazy" alt="International Chess Federation Naroditsky playing chess with spectators behind"><span>International Chess Federation</span></p></div></figure></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pasta/80 is a simple Pascal cross compiler targeting the Z80 microprocessor (104 pts)]]></title>
            <link>https://github.com/pleumann/pasta80</link>
            <guid>45653330</guid>
            <pubDate>Tue, 21 Oct 2025 07:23:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pleumann/pasta80">https://github.com/pleumann/pasta80</a>, See on <a href="https://news.ycombinator.com/item?id=45653330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/logo.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/logo.png" alt="Logo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">PASTA/80</h2><a id="user-content-pasta80" aria-label="Permalink: PASTA/80" href="#pasta80"></a></p>
<p dir="auto">PASTA/80 is a simple <a href="https://en.wikipedia.org/wiki/Pascal_(programming_language)" rel="nofollow">Pascal</a> cross compiler targeting the <a href="https://en.wikipedia.org/wiki/Zilog_Z80" rel="nofollow">Z80</a> microprocessor. It generates code for these classic and modern machines:</p>
<ul dir="auto">
<li><a href="https://en.wikipedia.org/wiki/CP/M" rel="nofollow">CP/M</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sinclair_ZX_Spectrum" rel="nofollow">ZX Spectrum 48K</a></li>
<li><a href="https://en.wikipedia.org/wiki/ZX_Spectrum#ZX_Spectrum_128" rel="nofollow">ZX Spectrum 128K</a></li>
<li><a href="https://www.specnext.com/" rel="nofollow">ZX Spectrum Next</a></li>
</ul>
<p dir="auto">The compiler follows the single-pass recursive-descent approach championed by <a href="https://de.wikipedia.org/wiki/Niklaus_Wirth" rel="nofollow">Niklaus Wirth</a>, inventor of Pascal, in his books and lectures. It doesn't have an explicit syntax tree, but instead generates code on the fly during parsing. As a result, the compiler might not always generate the most efficient code possible (it definitely cannot compete with LLVM and doesn't try to), but it's very fast.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported language elements</h2><a id="user-content-supported-language-elements" aria-label="Permalink: Supported language elements" href="#supported-language-elements"></a></p>
<p dir="auto">The supported Pascal dialect is an almost exact clone of the original <a href="https://en.wikipedia.org/wiki/Turbo_Pascal" rel="nofollow">Turbo Pascal 3.0</a> for CP/M (see <a href="https://bitsavers.trailing-edge.com/pdf/borland/turbo_pascal/Turbo_Pascal_Version_3.0_Reference_Manual_1986.pdf" rel="nofollow">this manual</a> for details). So you have at your disposal the following language elements:</p>
<ul dir="auto">
<li>All the basic data types (<code>Boolean</code>, <code>Byte</code>, <code>Char</code>, <code>Integer</code>, <code>Pointer</code>, <code>Real</code> and <code>String</code>).</li>
<li><code>array of</code>, <code>record</code>, <code>set of</code>, enumerations, subranges and pointers as a way of building new data types.</li>
<li>The decision-making elements <code>if..then..else</code> and <code>case..of</code>.</li>
<li>The loop elements <code>for..do</code>, <code>while..do</code> and <code>repeat..until</code>.</li>
<li>The <code>with..do</code> notation for "opening" records.</li>
<li><code>procedure</code> and <code>function</code> including value and <code>var</code> parameters and nesting.</li>
<li>The standard procedures for screen input and output (i.e. <code>ReadLn</code>, <code>WriteLn</code> etc.).</li>
<li>All conversion and utility procedures and functions that Turbo Pascal 3.0 had.</li>
<li>The three kinds of disk files, that is untyped (<code>file</code>), typed (<code>file of</code>) and <code>Text</code>.</li>
<li>A dynamic heap of up to 32767 bytes with <code>GetMem</code>, <code>FreeMem</code>, <code>New</code> and <code>Dispose</code>.</li>
<li>Inline assembly (via opcodes, not via mnemonics, so <a href="https://clrhome.org/table/" rel="nofollow">this page</a> might be handy).</li>
<li>Overlays (in memory, Spectrum 128K and Next only, see below).</li>
<li>Some compiler directives:
<ul dir="auto">
<li><code>$i &lt;file&gt;</code> for including Pascal source files (including nesting and cycle detection)</li>
<li><code>$l &lt;file&gt;</code> for including an assembly file (aka "linking" a library)</li>
<li><code>$a(+/-)</code>   for enabling or disabling absolute mode (default is on, disable for recursion)</li>
<li><code>$i(+/-)</code>   for enabling or disabling IO checking (when off, check <code>IOResult</code> after calls)</li>
<li><code>$k(+/-)</code>   for enabling or disabling stack overflow checking</li>
<li><code>$u(+/-)</code>   for enabling or disabling Ctrl-C checking</li>
</ul>
</li>
</ul>
<p dir="auto">The compiler also has some features that were borrowed from or inspired by later versions of Turbo Pascal:</p>
<ul dir="auto">
<li>C-style <code>//</code> one-line comments in addition to <code>{..}</code> and <code>(*..*)</code>.</li>
<li>Binary literals (using a <code>%</code> prefix).</li>
<li><code>Break</code> and <code>Continue</code> for loop control.</li>
<li>Querying the keyboard via <code>KeyPressed</code> and <code>ReadKey</code>.</li>
<li>Color support via <code>TextColor</code> and <code>TextBackground</code> with constants for the 8 Spectrum Next colors.</li>
<li><code>Inc</code> and <code>Dec</code> for more efficient increasing and decreasing of variables.</li>
<li><code>Include</code> and <code>Exclude</code> for more efficient handling of sets.</li>
<li>A simple <code>Assert</code> facility that counts passes/fails and shows the failed line number.</li>
</ul>
<p dir="auto">Since that covers most of the functionality of Turbo Pascal 3 you might ask what is missing. These are the current limitations:</p>
<ul dir="auto">
<li>All the remaining compiler directives are not yet supported.</li>
<li><code>Mark</code>/<code>Release</code> are not currently supported.</li>
<li>The standard files <code>Input</code>, <code>Output</code>, <code>Kbd</code>, <code>Con</code> and <code>Lst</code> are not supported.</li>
<li><code>Chain</code> and <code>Execute</code> are not supported.</li>
<li>Add-on libraries from the PC version of Turbo Pascal 3.0 are not yet supported (although there are a few graphics primitives for the ZX targets).</li>
<li>The <a href="https://wiki.specnext.dev/Extended_Z80_instruction_set" rel="nofollow">new instructions of the Z80N CPU</a> inside the ZX Spectrum Next are not yet being leveraged.</li>
<li>No separate compilation. Everything is compiled from source, always.</li>
<li>Binary size is quite large compared to the original.</li>
</ul>
<p dir="auto">The runtime library, being partially written in Pascal itself, gets quite large when compiled. I hope to bring this down again by reimplementing more of it in Z80 assembly (or improve the code generator, which, although it has a peephole optimizer, is not generating super-efficient Z80 code).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and setting up the compiler</h2><a id="user-content-building-and-setting-up-the-compiler" aria-label="Permalink: Building and setting up the compiler" href="#building-and-setting-up-the-compiler"></a></p>
<p dir="auto">The compiler is itself written in Pascal. You can compile it with <a href="https://www.freepascal.org/" rel="nofollow">Free Pascal</a> (I use version 3.2.2). Just run</p>

<p dir="auto">The Pascal compiler generates Z80 assembler code and relies on <a href="https://z00m128.github.io/sjasmplus" rel="nofollow">sjasmplus</a> as a backend for the final translation step to binary. It can also, in <code>--ide</code> mode (see below), make use of various other external tools. The compiler tries to detect these external tools automatically (from your system's <code>PATH</code>), but sometimes it's best to create a file <code>.pasta80.cfg</code> in your home directory specifying necessary paths (there is a sample in <code>misc</code> that you can adapt).</p>
<div data-snippet-clipboard-copy-content="# PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
..."><pre><code># PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
...
</code></pre></div>
<p dir="auto">You can check your whole setup by calling the compiler with <code>--config</code>. It will show the full paths of all internal and external requirements and whether they are fulfilled.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using the compiler</h2><a id="user-content-using-the-compiler" aria-label="Permalink: Using the compiler" href="#using-the-compiler"></a></p>
<p dir="auto">To run the compiler just invoke the executable with the name of a Pascal source file to translate.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CP/M target</h3><a id="user-content-cpm-target" aria-label="Permalink: CP/M target" href="#cpm-target"></a></p>
<p dir="auto">The default target is CP/M. There is an optional parameter that enables some simple peephole optimizations and another one that uses dependency analysis to eliminate unused Pascal procedures and functions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta hello.pas             # Compiles hello.pas to hello.com
$ pasta hello                 # Source file .pas suffix is optional
$ pasta --opt hello.pas       # Enables peephole optimizations
$ pasta --opt --dep hello.pas # The same plus dependency analysis"><pre>$ pasta hello.pas             <span><span>#</span> Compiles hello.pas to hello.com</span>
$ pasta hello                 <span><span>#</span> Source file .pas suffix is optional</span>
$ pasta --opt hello.pas       <span><span>#</span> Enables peephole optimizations</span>
$ pasta --opt --dep hello.pas <span><span>#</span> The same plus dependency analysis</span></pre></div>
<p dir="auto">You can run the resulting <code>.com</code> files on a real CP/M machine or in a CP/M emulator. I recommend the excellent <a href="https://gitlab.com/gbrein/tnylpo" rel="nofollow">tnylpo</a>. For programs that use VT52 control codes you have to start tnylpo in full-screen mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ tnylpo hello                # Run in line-mode
$ tnylpo -s -t @ hello        # Monochrome full-screen, wait when finished
$ tnylpo -soy,4,0 -t @ hello  # Color full-screen, wait when finished"><pre>$ tnylpo hello                <span><span>#</span> Run in line-mode</span>
$ tnylpo -s -t @ hello        <span><span>#</span> Monochrome full-screen, wait when finished</span>
$ tnylpo -soy,4,0 -t @ hello  <span><span>#</span> Color full-screen, wait when finished</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>"Hello, World" in line mode</th>
<th>"Hello, World" in full-screen</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello1.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello1.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello2.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello2.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">ZX Spectrum targets</h3><a id="user-content-zx-spectrum-targets" aria-label="Permalink: ZX Spectrum targets" href="#zx-spectrum-targets"></a></p>
<p dir="auto">To generate binaries for the ZX Spectrum 48K, 128K and Next targets, use the <code>--zx48</code>, <code>--zx128</code> and <code>--zxnext</code> parameters, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx48 hello.pas      # Compiles for ZX Spectrum 48K
$ pasta --zx128 hello.pas     # Compiles for ZX Spectrum 48K
$ pasta --zxnext hello.pas    # Compiles for ZX Spectrum Next"><pre>$ pasta --zx48 hello.pas      <span><span>#</span> Compiles for ZX Spectrum 48K</span>
$ pasta --zx128 hello.pas     <span><span>#</span> Compiles for ZX Spectrum 48K</span>
$ pasta --zxnext hello.pas    <span><span>#</span> Compiles for ZX Spectrum Next</span></pre></div>
<p dir="auto">The main difference between the three (currently) is that the ZX Spectrum Next target supports file IO (on the SD card), while the other two do not. The remaining routines are mostly the same. Screen output is handled via <code>rst $10</code> in the ROM. In both cases the binaries are expected to be run from address 0x8000.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tapes, snapshots and runnable directories</h3><a id="user-content-tapes-snapshots-and-runnable-directories" aria-label="Permalink: Tapes, snapshots and runnable directories" href="#tapes-snapshots-and-runnable-directories"></a></p>
<p dir="auto">The default output format for the ZX Spectrum targets is a simple binary file that contains exactly the bytes of the compiled program (plus a +3DOS header when compiling for the Spectrum Next). In addition to that (and for more complex cases involving overlays), the compiler can also generate snapshot files or tape files, the latter including a suitable BASIC loader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx48 --sna examples/hello.pas   # .sna file
$ pasta --zx48 --tap examples/jacques.pas # .tap file with BASIC loader"><pre>$ pasta --zx48 --sna examples/hello.pas   <span><span>#</span> .sna file</span>
$ pasta --zx48 --tap examples/jacques.pas <span><span>#</span> .tap file with BASIC loader</span></pre></div>
<p dir="auto">Being self-contained, snapshots and tapes are a convenient way to distribute your programs and to launch them an emulator, such as Fuse:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ open -a Fuse examples/hello.sna         # Launch .sna file in FUSE (on Mac)
$ open -a Fuse examples/jacques.tap       # Launch .tap file in FUSE (on Mac)"><pre>$ open -a Fuse examples/hello.sna         <span><span>#</span> Launch .sna file in FUSE (on Mac)</span>
$ open -a Fuse examples/jacques.tap       <span><span>#</span> Launch .tap file in FUSE (on Mac)</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Hello world in FUSE</th>
<th>Frere Jacques in FUSE (yes, with sound!)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello3.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello3.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/jacques.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/jacques.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">When compiling for the Next, another useful format is a runnable directory. It contains exactly the same files that would also be in the .tap file, including a BASIC loader named <code>run.bas</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zxnext --run examples/pq.pas    # Results in directory named pq.run"><pre>$ pasta --zxnext --run examples/pq.pas    <span><span>#</span> Results in directory named pq.run</span></pre></div>
<p dir="auto">The directory has the suffix <code>.run</code>. When attempting to enter such a directory in the Next's file browser, the loader is started automatically (press Symbol Shift + Enter to really see the contents). If you are a Mac user: Yes, it's a bit like an <code>.app</code> bundle.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overlays</h3><a id="user-content-overlays" aria-label="Permalink: Overlays" href="#overlays"></a></p>
<p dir="auto">The Spectrum 128K and Next targets support overlays. This means you can have larger programs than would normally fit into the 64K address space of a Z80 machine. The rules are the same as for Turbo Pascal 3.0:</p>
<ul dir="auto">
<li>Overlays can be applied to global procedures and functions only, not to nested ones (though nested ones will be overlayed if the containing ones are, too).</li>
<li>Overlays cannot be applied to global variables, that is, you cannot use them for data (at least not without tricks).</li>
<li>All consecutive procedures and functions that are marked as <code>overlay</code> go into the same overlay. Use any declaration inbetween to separate overlays.</li>
</ul>
<p dir="auto">In the following example, there are three overlays: Overlay 0 contains A and B, overlay 1 contains D, and overlay 2 contains E.</p>
<div dir="auto" data-snippet-clipboard-copy-content="overlay procedure A; (* Overlay 0 *)
begin
end;

overlay procedure B; (* Overlay 0 *)
begin
end;

procedure C; (* Not in an overlay *)
begin
end;

overlay procedure D; (* Overlay 1 *)
begin
end;

type
  Dummy = Integer;   (* Separator *)

overlay procedure E; (* Overlay 2 *)
begin
end;"><pre>overlay <span>procedure</span> <span>A</span>; <span><span>(*</span> Overlay 0 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

overlay <span>procedure</span> <span>B</span>; <span><span>(*</span> Overlay 0 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

<span>procedure</span> <span>C</span>; <span><span>(*</span> Not in an overlay <span>*)</span></span>
<span>begin</span>
<span>end</span>;

overlay <span>procedure</span> <span>D</span>; <span><span>(*</span> Overlay 1 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

<span>type</span>
  Dummy = Integer;   <span><span>(*</span> Separator <span>*)</span></span>

overlay <span>procedure</span> <span>E</span>; <span><span>(*</span> Overlay 2 <span>*)</span></span>
<span>begin</span>
<span>end</span>;</pre></div>
<p dir="auto">In contrast to Turbo Pascal 3.0, overlays are not implemented via disk files. Instead, they use the additional RAM of the Spectrum 128K and Next machines. The uppermost 16K bank (Spectrum 128K) or 8K page (Spectrum Next) will be reserved for overlays. Each overlay can have a maximum size of 8K. The compiler manages everything and generates special "far calls" whenever necessary.</p>
<p dir="auto">To enable overlays, use the <code>--ovr</code> command line parameter, ideally in conjuncton with the <code>--tap</code> parameter, as the tape loaders for 128K and Next are fully overlay-aware.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas # Test suite as 128K tape"><pre>$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas <span><span>#</span> Test suite as 128K tape</span></pre></div>
<p dir="auto">The compiler prints a report of which overlays go into which RAM banks or pages.</p>
<div data-snippet-clipboard-copy-content="----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -> tests/all.z80
Assembling...
  tests/all.z80 -> tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3"><pre><code>----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -&gt; tests/all.z80
Assembling...
  tests/all.z80 -&gt; tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3
</code></pre></div>
<p dir="auto">Without the <code>--ovr</code> parameter, overlay markers are simply ignored. This means you can use the same source code for platforms that do support overlays and for those that don't.</p>
<p dir="auto"><strong>Caution</strong>: Overlays somewhat break the safety of the Pascal language. Be careful when using pointers or <code>var</code> parameters for passing data between overlays. The memory you refer to may have just been paged out! It might make sense to compile your overlays with <code>{$a-}</code>, so that all local variables are stored on the stack (which is always visible).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples and tests</h2><a id="user-content-examples-and-tests" aria-label="Permalink: Examples and tests" href="#examples-and-tests"></a></p>
<p dir="auto">There is a folder containing <code>examples</code> and a folder containing <code>tests</code> for the compiler. The main test suite <code>all.pas</code> needs to be compiled with <code>--opt --dep</code> because of its size. Otherwise it won't fit into 64K. The Spectrum 128K and Next targets can (only) handle it using overlays, the Spectrum 48K target can't. Both the examples and the tests should give you a pretty good overview of what the compiler can do.</p>
<p dir="auto">I also solved all puzzles of <a href="https://github.com/pleumann/aoc22">Advent of Code 2022</a> with an earlier version of the compiler and made <a href="https://youtube.com/playlist?list=PLcjDDXgGeSQ6E3NLeSOH0Tn7UorYBgUOH&amp;si=SAoOqUbi70c4ezgi" rel="nofollow">YouTube videos</a> of the solutions running on the ZX Spectrum Next, in CP/M mode.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimalistic IDE</h2><a id="user-content-minimalistic-ide" aria-label="Permalink: Minimalistic IDE" href="#minimalistic-ide"></a></p>
<p dir="auto">As a fun little gimmick the compiler can be started like this</p>

<p dir="auto">to run it in an interactive mode that has an interface reminiscient of Turbo Pascal 3.0.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Main menu</th>
<th>Editor</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/idemenu.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/idemenu.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/ideedit.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/ideedit.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">When started in an ordinary terminal, this mode relies on the editor <code>nano</code> being present on your system (on MacOS you might want to install the real <code>nano</code> via a package manager because Apple sells you the much more limited <code>pico</code> editor as <code>nano</code>).</p>
<p dir="auto">You can also run it in a shell within Visual Studio Code, in which case it would automatically use VSC's editor (via the <code>code</code> command, which, on a Mac, you might <a href="https://code.visualstudio.com/docs/setup/mac#_configure-the-path-with-vs-code" rel="nofollow">have to make available from VCS's settings</a>) and act a bit like a plugin.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/vsc.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/vsc.png" alt="Screenshot"></a></p>
<p dir="auto">The following external tools are supported for running compiled programs on the host machine:</p>
<ul dir="auto">
<li><a href="https://gitlab.com/gbrein/tnylpo" rel="nofollow">tnylpo</a> for CP/M programs (press &lt;R&gt; for line mode, &lt;Shift-R&gt; for full-screen mode).</li>
<li><a href="https://fuse-emulator.sourceforge.net/" rel="nofollow">Fuse</a> for programs targeting the ZX Spectrum 48K and 128K machines.</li>
<li><a href="https://mdf200.itch.io/cspect" rel="nofollow">CSpect</a> for ZX Spectrum Next programs.
<ul dir="auto">
<li>Please have <a href="https://github.com/gasman/hdfmonkey">hdfmonkey</a> ready for manipulating the SD card image.</li>
<li>If you're on MacOS or Linux, you also need <code>mono</code> because CSpect is a .NET application.</li>
</ul>
</li>
</ul>
<p dir="auto">As mentioned before, everything that is in your <code>PATH</code> should be detected automatically. There are some exceptions, though, so it makes sense to copy <code>misc/.pasta80.cfg</code> to your home directory and adapt it. Use the <code>--config</code> parameter to let PASTA/80 check your setup and get feedback on what is in place and what is missing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Application Gallery</h2><a id="user-content-application-gallery" aria-label="Permalink: Application Gallery" href="#application-gallery"></a></p>
<p dir="auto">The following screenshots show some applications compiled for the CP/M target and running in the <code>tnylpo</code> emulator.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>2048</th>
<th>Game of Life</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/2048.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/2048.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/life.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/life.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Micro Calc</th>
<th>Galactic Empire</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/microcalc.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/microcalc.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/empire.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/empire.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">These screenshots show some applications compiled for the ZX Spectrum 48K target and running in the FUSE emulator.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>2048</th>
<th>Game of Life</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/2048zx.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/2048zx.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/lifezx.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/lifezx.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Graphics Demo</th>
<th>Equation Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/graphics.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/graphics.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/pqformula.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/pqformula.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><strong>PASTA/80 Pascal Compiler</strong></p>
<p dir="auto">Copyright (c) 2020-2025 by JÃ¶rg Pleumann</p>
<p dir="auto">The PASTA/80 compiler is free software: you can redistribute it and/or modify
it under the terms of the <strong>GNU General Public License (GPL)</strong> as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p>
<ul dir="auto">
<li>
<p dir="auto">The runtime library (folder <code>rtl</code>) comes with a <strong>linking exception</strong> that makes sure the GPL does not transfer to binaries created using PASTA/80.</p>
</li>
<li>
<p dir="auto">The examples (folder <code>examples</code>) are considered <strong>public domain</strong> or whatever comes closest to that in your jurisdiction.</p>
</li>
<li>
<p dir="auto">Individual files or folders may use different licenses, so you might want to double check.</p>
</li>
</ul>
<p dir="auto">Everything is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE. See the GNU General Public License for more details.</p>
<p dir="auto">What does this mean for you?</p>
<ul dir="auto">
<li>
<p dir="auto">You can <strong>use the compiler</strong>, free of charge, to build any application, open-source or prioprietary, free or paid, and distribute the generated binary without restriction. You can <strong>distribute binaries</strong> created with PASTA/80 under a <strong>license of your choosing</strong>.</p>
</li>
<li>
<p dir="auto">You can <strong>modify the compiler</strong> according to your needs. If you <strong>distribute the compiler</strong> or parts of it, binary or source, modified or not, you have to <strong>comply with the rules laid out in the GPL</strong> (copyright info, source code, ...) unless the linking exception applies.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">The math48 library is coypright (c) 1980 by Anders Hejlsberg, used by <a href="https://github.com/pleumann/pasta80/issues/7" data-hovercard-type="issue" data-hovercard-url="/pleumann/pasta80/issues/7/hovercard">permission</a>.</p>
<p dir="auto">Some assembly routines adapted from Leventhal/Saville, "Z80 Assembly Subroutines", Osborne/McGraw-Hill 1983.</p>
<p dir="auto">Turbo Pascal is a registered trademark of Code Gear LLC / Embarcadero.</p>
<p dir="auto">Z80 is a registered trademark of Zilog, Inc.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Language Support for Marginalia Search (155 pts)]]></title>
            <link>https://www.marginalia.nu/log/a_126_multilingual/</link>
            <guid>45653143</guid>
            <pubDate>Tue, 21 Oct 2025 06:48:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/a_126_multilingual/">https://www.marginalia.nu/log/a_126_multilingual/</a>, See on <a href="https://news.ycombinator.com/item?id=45653143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>One of the big ambitions for the search engine this year has been to enable searching in more languages than English, and a pilot project for this has just been completed, allowing experimental support for German, French and Swedish.</p><p>These changes are now live for testing, but with an extremely small corpus of documents.</p><p>As the search engine has been up to this point built with English in mind, some anglo-centric assumptions made it into its code. A lot of the research on search engines generally seems to embed similar assumptions.</p><p>As this is a domain rife with unknown unknowns, the ambition for this pilot was to implement support for just a few additional languages in order to get a feel for how much work would be required to support more languages in general, as well as to assess how much the index grows when this is done.</p><p>Though it was fully understood upfront that supporting <em>all</em> languages in one go is unrealistic, as some languages are more different than others and require significant additional work. Human language is surprisingly disparate.</p><p>A language like Japanese, for example, has not only multiple alphabets, but <a href="https://en.wikipedia.org/wiki/Halfwidth_and_fullwidth_forms">embeds character width in unicode</a>; on top of that the language doesnâ€™t put spaces between words. As such the language requires special normalization.</p><p>Latin, on the other hand, has <a href="https://dcc.dickinson.edu/grammar/latin/1st-and-2nd-declension-adjectives-%C4%81-o-stems">dozens</a> <a href="https://dcc.dickinson.edu/grammar/latin/2nd-declension-stem-paradigm-and-gender">of</a> <a href="https://dcc.dickinson.edu/grammar/latin/1st-conjugation">forms</a> for each word, and the words can often be reordered without significantly changing the meaning of a sentence. On the one hand this makes the grammatical analysis of the language somewhat easier since the words announce their function in the sentence fairly unambiguously, but on the other you probably need to store the text in a lemmatized form, and then strongly de-prioritize word order when matching.</p><p>Googleâ€™s bungled handling of Russian was supposedly why Yandex was able to eke out a foothold in that market.</p><h2 id="what-needs-changing">What needs changing</h2><p>The search engineâ€™s language processing chain is fairly long, but the most salient parts go something like this:</p><ul><li>Text is extracted from the HTML</li><li>Language is identified using fasttext</li><li>Text is broken into sentences</li><li>Words are lowercased and Unicode is normalized</li><li>Sentences are stemmed and POS-tagged</li><li>Sentences, with stemming and POS-tag data is fed into keyword extraction algorithms<ul><li>Keywords are mapped to positions and HTML tags</li><li>Important keywords are identified using TF-IDF (using stemmed forms)</li><li>Important keywords are identified using grammar patterns (POS-tags)</li><li>Important keywords are identified using other heuristics</li></ul></li><li>Keywords are hashed</li></ul><p>Stemming is an imperfect way of getting a base form of a word, though generally such algorithms have a great number of flaws, so that e.g. universe and university seem to be the same word. This is only used in tf-idf calculations.</p><p>Part-of-Speech (POS) tagging is a grammatical annotation process where the role of each word is as best possible identified. This helps identify named entities, subjects, and so on.</p><p>Both of these processes needless to say require some awareness of the language being acted upon.</p><p>These â€œimportant keywordsâ€ are used to assign documents to a special index that helps with recall by ensuring these documents are included in the set that is ranked before the execution timer runs out. This is not strictly necessary, and in some cases such as where POS-tagging is not possible, can be disabled, partially or as a whole.</p><p>The normalization step is subject to cultural differences that do not translate. In English youâ€™d probably expect to find the metal band TrÃ¶jan, typing â€œtrojanâ€. In Swedish these are different letters entirely that should not match, the former means â€œthe shirtâ€, the latter â€œtrojanâ€ in the Homeric or IT-security sense. Though a Swedish person would likely also say that they should be able to find mÃ¼(e)sli with the keyword â€œmusliâ€, but a German-speaker would disagree and say that u and Ã¼ are clearly not the same.</p><p>There also exists a bootstrapping problem, as the statistical model used to calculate TF-IDF is based on documents in the index. Since almost all of the documents in the index up until this point have been in English, term frequencies for the newly added languages are missing. This breaks TF-IDF, as used in identifying important keywords, until a new model can be constructed. Thankfully the BM-25 model used in ranking is robust to this, as it relies on live data from the index itself.</p><p>The basic approach to parametrize language handling selected was to inject a language definition object, from which language appropriate logic is accessible.</p><p>This is configurable <a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/functions/language-processing/resources/languages-experimental.xml">via XML</a>. Here XML was chosen because it arguably has the best built-in validation support, making it a fantastic use case for a self-contained configuration file like this one, where late validation would be very annoying to deal with.</p><p>Much of the configuration file consists of various grammatical patterns used to identify important keywords based on the role of a word in a sentence.</p><div><pre tabindex="0"><code data-lang="xml"><span><span><span>&lt;ngrams</span> <span>type=</span><span>"noun"</span><span>&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>RB VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ)<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) DT NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span><span>&lt;/ngrams&gt;</span>
</span></span></code></pre></div><p>An expression like <code>(NN* JJ) (NN* JJ) NN*</code> is interpreted as</p><ol><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code></li></ol><p>Previously these patterns were hard coded, and finding a performant alternative implementation took some effort. A bit mask approach was selected, as it allows for some very basic bit-level concurrency that drastically reduces the number of branches needed.</p><p>As far as grammatical analysis goes, the approach used by the search engine is pretty medieval, but it does do a fairly good job at what it sets out to do, and as a result, one thing it is generally pretty good at is finding websites about some topic.</p><p>In some ways the imperfections introduced by the old-fashioned way of approaching language processing is almost helpful in bringing in more relevant results, as they tend to capture more variations of the words related to the topic of the document.</p><p>There are more places that need minor language dependent behavior changes that are glossed over here, both in the language processing pipeline discussed above, and in the query parser, though in the interest of keeping this update from becoming an overly verbose git diff, these will be glossed over.</p><h3 id="tooling">Tooling</h3><p>To help make sense of this, a test tool was built that runs the language processing pipeline in isolation, and outputs annotated intermediate results for human inspection.</p><figure><a href="https://www.marginalia.nu/log/a_126_multilingual/tool.png"><img src="https://www.marginalia.nu/log/a_126_multilingual/tool.png"></a><figcaption>Language Processing Tool illustrating some problems with keyword identification when run on a very short sample of text.</figcaption></figure><p>Work in this domain poses special problems that all but demand human testing. Machine testing can be good for catching regressions or getting access to some code for easier debugging, but natural language has so many nuances that any test suite is woefully inadequate compared to a pair of human eyeballs.</p><p>It has already helped refine the algorithms used to identify important keywords in English, which wasnâ€™t the intent of building the tool, but its immediate consequence.</p><h2 id="integration">Integration</h2><p>Integrating the new multi-language search data into the system poses some design considerations.</p><p>One option would be to stick everything in one big index, and then filter results based on language during or after ranking. The strength of this is that it becomes possible to search in any language without specifying it upfront.</p><p>The drawbacks of the one-index approach is that it grows the index, which makes all queries slower; it also grows the number of keywords in the lexicon, which is something that we generally want to avoid.</p><p>The way the search engine handles mapping keywords to numeric ids is to use a hash algorithm. Not a hash table, but the output of the hash algorithm itself. This seems absolutely unhinged at first glance, but works remarkably well as long as the lexicon stays small enough.</p><p>Hash collisions do happen on rare occasions, but they need to happen between words where the words actually appear in the same documents to be a problem, generally leading to the ranking algorithm having to trudge through irrelevant documents and performing worse as a result of wasting its time budget.</p><p>Massively expanding the lexicon like we would if we were to mingle the documents increases the likelihood there will be an actual problem arising from these rare false positives.</p><p>If we stick every keyword from every language in the same index, a different problem arises, namely that homophones exist across different languages, meaning that the index lookup needs to wade through irrelevant documents that are trivially unrelated to the query.</p><p>The words <code>salt</code> and <code>lag</code>, if they appear in the same document in English likely selects documents relating to esports, whereas in Swedish they select for documents relating to food preservation.</p><p>The alternative option is to separate the indexes.</p><p>The drawback here is that you must specify the language upfront, and querying in all languages becomes very expensive, as it executing multiple queries, though the desired language of the search results are generally known beforehand so this is a relatively small concern that, at best, affects a small number of machine-access use cases.</p><p>Since it has far fewer problems, and promises to be faster and more accurate, this approach was selected.</p><p>In practice this was implemented as language-specific keyword-document mappings, that point into a common file containing document lists.</p><p>Initially the indexes were constructed from a common journal file, which was consumed repeatedly, but this turned out to be slow, and a partitioned approach was selected instead, with one journal per language. This almost completely removes any overhead.</p><h2 id="outcome">Outcome</h2><p>The changes discussed above have been implemented, and upon evaluation seems to work reasonably well, though evaluation has somewhat run into a dead end, as the index itself is <strong>extremely</strong> small for the newly added languages.</p><p>The experience of small index is devious as it may just mean poor recall, though looking at the documents database for one index partition, this is about 12% of the index, it really is quite small!</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>112,846,397</td></tr><tr><td>de</td><td>7,623,983</td></tr><tr><td>fr</td><td>4,852,759</td></tr><tr><td>sv</td><td>1,020,962</td></tr></tbody></table><p>To verify this is not due some silent, catastrophic processing error, the proportions were compared against the number of documents found in the 50 GB document sample used in testing, using a simplified process that only does language identification.</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>11,497,571</td></tr><tr><td>de</td><td>614,311</td></tr><tr><td>fr</td><td>409,877</td></tr><tr><td>es</td><td>267,408</td></tr><tr><td>ja</td><td>217,599</td></tr><tr><td>nl</td><td>196,130</td></tr><tr><td>â€¦</td><td>â€¦</td></tr><tr><td>sv</td><td>67,670</td></tr></tbody></table><p>The proportions arenâ€™t identical, but in the same general ballpark. The small size of the sample, along with the uneven distribution and apparent rarity of these documents adequately explains the disparity.</p><p>The lack of documents in languages other than English is likely due to how the index has been grown, by following and adding links from English websites. These occasionally lead to bilingual websites, and on rare occasions to websites completely in a different language, though it seems reasonable most websites that are not at least partially in English sees few or no links from English-language websites.</p><p>Adding to the problem, up until fairly recently the index wasnâ€™t really growing very much at all, only through manual submissions.</p><p>Beyond a certain point, meaningfully growing the index by just following links became difficult.</p><p>Most known domains are dead, so merely adding more domains to the list of websites to crawl only serves to pollute the database with junk data.</p><p>In order to get around this, and reach the goal of indexing a billion documents, a new process was built to visit candidate websites to verify that they are in fact real and on-line, before assigning them to an index partition.</p><p>The process has been running for almost a quarter, and has managed to identify about 800,000 viable new domains in that time window. (This has brought the document total up to 969M documents. So very nearly there now!)</p><p>Web search is unusual in how often you run into these extremely long running processes that need to cook for months, sometimes up to a year before they really begin to pay off.</p><p>Weâ€™ll have to see whether building this new process was so prescient it ends up being sufficient to identify and add new domains in more languages, as links from the newly processed Swedish, French and German websites have been added to the domain database, or if some sort of manual seeding or targeted selection process is needed.</p><p>It seems plausible it will at least begin to remedy the data starvation, as the rate of successful domain discovery has shot up significantly since processing links from the documents processed in the newly added languages, and many of the new domains are indeed from <code>.de</code>, <code>.se</code>, <code>.fr</code>, and <code>.ch</code> domains.</p><p>For now weâ€™ll have to wait and see how the data-set evolves. It is difficult to further refine the multi-language aspect of the search data with a data-set this small.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practical Scheme (129 pts)]]></title>
            <link>https://practical-scheme.net/index.html#docs</link>
            <guid>45652859</guid>
            <pubDate>Tue, 21 Oct 2025 05:47:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical-scheme.net/index.html#docs">https://practical-scheme.net/index.html#docs</a>, See on <a href="https://news.ycombinator.com/item?id=45652859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      <p>This page is a collection of libraries and extensions
        to use Scheme as a production tool.
        By "production tools" I mean the tools to process daily
        chores for systems engineers and programmers---parsing files,
        generate reports, watching processes,
        providing small GUI wrappers, and all sorts of those things.
        Currently I'm using <a href="https://www.perl.org/">Perl</a> for
        those purpose, but I'm always longing to use Scheme for them.
        <a href="https://practical-scheme.net/oneday.html">So I started this page</a>.
      </p>

      <p>Most stuffs in this site are done as my private
         project at home, except the ones explicitly stated otherwise.
         I upload libraries even in its alpha/beta stage, since
         I'd like to test and use them at work, too.  In a way, my primary
         interest is to make <i>my</i> life happier.
         No warranty comes with them, as usual, but it'll be nice
         if somebody else finds they are useful.
      </p>

      <p>If you can read Japanese, visit the <a href="https://practical-scheme.net/index-j.html">Japanese
         page</a> which contains some translations of Lisp/Scheme
         related articles.
      </p>

      <p>I wrote a Wiki Clone in Scheme (<a href="https://practical-scheme.net/gauche/index.html">Gauche</a>).
         Come and try it:
         <a href="https://practical-scheme.net/wiliki/wiliki.cgi?l=en">WiLiKi</a>.
      </p>

      <ul>
        <li> <a href="#apps">Applications and tools</a>
        </li><li> <a href="#libs">Libraries and extensions</a>
        </li><li> <a href="#docs">Documents</a>
        </li><li> <a href="#links">Links</a>
      </li></ul>

    <p><a name="apps"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Applications and tools</h2>

      <p>Scheme-related stand alone programs.</p>

      <dl>
        <dl><dt><b>Gauche</b> - <a href="https://practical-scheme.net/gauche/index.html">Current version 0.9.15</a> (2024/04/24)<img src="https://practical-scheme.net/images/new.png" alt="*New" width="40" height="18"></dt><dd><p>An R7RS Scheme implementation aimed at a handy
                       script engine.  Quick startup, built-in system
                       interface, and native multilingual support
                       are some of the goals.</p></dd><dt><b>WiLiKi</b> - <a href="https://practical-scheme.net/wiliki/wiliki.cgi">Current version 0.6.2</a> (2014/11/28)</dt><dd><p>A wiki engine written in Scheme.</p></dd><dt><b>Chaton</b> - <a href="https://practical-scheme.net/chaton/">Current version </a></dt><dd><p>A Comet-based Webchat system.</p></dd><dt><b>escm</b> - <a href="https://practical-scheme.net/vault/escm.html">Current version 1.1</a> (2014/11/28)</dt><dd><p>A filter program which copies the input text to output,
                       with processing embedded Scheme expressions.
                       This program itself is independent from any Scheme
                       implementation; you can use your favorite one.
                       Useful to process text files with a bit of dynamic
                       parts.  This page itself is processed by escm
                       to embed information such as the update time of
                       libraries, and synchronize with Japanese version.
                       A complete new version of escm, named aescm,
                      is being developed by TAGA Yoshitaka
                       (<a href="http://sourceforge.net/projects/escm/">
                       http://sourceforge.net/projects/escm/</a>)</p></dd></dl>
      </dl>

    <p><a name="libs"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Libraries and Extensions</h2>

      <p>The following libraries and extensions are written for
         <a href="https://practical-scheme.net/gauche/index.html">Gauche</a>.
         See <a href="https://practical-scheme.net/stklib.html">here</a> for libraries written for STk.
      </p>

      <dl>
<dt><b>Gauche-gl</b> - <a href="https://prdownloads.sourceforge.net/gauche/Gauche-gl-0.6.tgz">Download</a>&nbsp;&nbsp;<a href="https://practical-scheme.net/vault/gauche-gl-refe.html">Document</a>&nbsp;&nbsp;Current version 0.6 (2014/08/09)&nbsp;</dt><dd><p>OpenGL binding for Gauche.  Supports most of OpenGL 1.0 to 4.1 APIs
           (including OpenGL Shading Language API),
           and some of GLU and GLUT API.
           Requires Gauche 0.9.4 or later.</p></dd><dt><b>Gauche-gtk2</b> - <a href="https://github.com/shirok/Gauche-gtk2/releases/download/release-0.6.1/Gauche-gtk2-0.6.1.tgz">Download</a>&nbsp;&nbsp;<a href="https://practical-scheme.net/vault/README.Gauche-gtk.txt">Document</a>&nbsp;&nbsp;Current version 0.6.1 (2022/3/20)&nbsp;</dt><dd><p>GTK2 binding for Gauche.</p></dd></dl>


    <p><a name="docs"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Documents</h2>

      <dl>
        <dt> <a href="https://practical-scheme.net/wiliki/schemexref.cgi">Scheme Cross Reference</a>
        </dt><dd> <p>A cross reference of library procedures of various
             Scheme implementations.   Updated constantly.
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/jlugm2000.html">Shooting A Moving Target---
             An Experience In Developing A Production Tracking Database</a>
        </dt><dd> <p>An application of CommonLisp in practice.
             (yeah, it's not Scheme... anyway, I put it here).

        </p></dd><dt> <a href="https://practical-scheme.net/docs/gdc2002.html">
             Tracking Assets in the Production of 'Final Fantasy : The Spirits Within'</a>
        </dt><dd> <p>A follow-up of the article above, a kind of post-mortem
             of the production.</p>

        </dd><dt> <a href="https://practical-scheme.net/docs/ILC2002.html">Gluing Things Together -
             Scheme in the Real-time CG Content Production</a>
        </dt><dd> <p>A paper presented at International Lisp Conference 2002
             at San Francisco, October 2002.
             (there's also a <a href="https://practical-scheme.net/docs/ILC2002.pdf">pdf version</a>).
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/DLS2008.pdf">Efficient floating-point number handling for dynamically typed scripting languages (pdf)</a>
        </dt><dd> <p>A paper presented at Dynamic Language Symposium 2008.
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/schemersway.html">Schemer's Way</a>
        </dt><dd> <p>Trying to explain Scheme's merits to non-Scheme programmers.
             </p>
      </dd></dl>

    <p><a name="links"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2><a name="otherresources">Other Resources</a></h2>

      <p>This list no way covers everything, but you can follow
        links in those links.
      </p>

      <dl>
        <dt> <a href="http://www.schemers.org/">Schemers.org</a>
        </dt><dd> A good anchor point to collect information of Scheme.
             You can get <a href="http://www.schemers.org/Documents/Standards/">R*RS</a>, the language standard.
             The site is also a center of
             <a href="http://srfi.schemers.org/">SRFI's</a>---
             Scheme Request For Implementation---which provides
             common interface of libraries across various implementations.

        </dd><dt> <a href="http://www-swiss.ai.mit.edu/~jaffer/SCM.html">SCM</a>
        </dt><dd> A compact, fast and portable implementation of Scheme
             interpreter.

        </dd><dt> <a href="http://www-swiss.ai.mit.edu/~jaffer/SLIB.html">SLIB</a>
        </dt><dd> A large collection of portable Scheme libraries.
             The contents spans from small utilities complements the
             standard conformance, to the full-featured relational
             database.

        </dd><dt> <a href="http://www4.ocn.ne.jp/~inukai/artificial.html">
             Programming Languages</a> by Dai Inukai
        </dt><dd> Scheme-related documents by Dai Inukai, the author of
             "Nyuumon Scheme (Scheme Primer)" in Japan.
             Check this out if you're interested in processing
             Japanese in Scheme.

        </dd><dt> <a href="http://kaolin.unice.fr/Bigloo/">Bigloo</a>
        </dt><dd> A scheme system with compiler and integrated development
             environment.  If you're planning to write an enterprise
             software rather than just a bunch of scripts, look at it.

        </dd><dt> <a href="http://www.gnu.org/software/guile/guile.html">Guile</a>
        </dt><dd> GNU adopted Scheme for the base of extension language
             several years ago.  The effort became Guile.
             If you have one of popular Linux distributions, you may
             already have it.

        </dd><dt> <a href="http://www.swiss.ai.mit.edu/ftpdir/scsh/">scsh</a>
        </dt><dd> I haven't used this one much, but looks good if you're
             looking for a tool to do syste programming.

        </dd><dt> <a href="http://www.cs.indiana.edu/scheme-repository/">
             The Internet Scheme Repository</a>
        </dt><dd> As the name suggests.

        </dd><dt> <a href="http://www.gnu.org/software/kawa/">Kawa - the Java-based Scheme System</a>
        </dt><dd> A Scheme environment written in Java by Per Bothner.
             Scheme code is compiled to Java bytecode, hence has
             the property "write once run everywhere".

      </dd></dl>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[60k kids have avoided peanut allergies due to 2015 advice, study finds (189 pts)]]></title>
            <link>https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</link>
            <guid>45652307</guid>
            <pubDate>Tue, 21 Oct 2025 03:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/">https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</a>, See on <a href="https://news.ycombinator.com/item?id=45652307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                                                                                                                                
                                                                            
<article id="article-0" data-index="0" data-path="/news/peanut-allergies-60000-kids-avoided-2015-advice/">
  <!-- lil observer -->
      <span></span>
    
        



  <div id="article-header" data-sort-time="1761003592000" data-update-time="1761003592000">
    <header>
                                      
          

      

      
      
      <div>
                  
  



        
        <p>
            <time datetime="2025-10-20T19:39:52-0400">Updated on:  October 20, 2025 / 7:39 PM EDT</time>
            / CBS/AP
          </p>
      </div>

    </header>
  </div>


  <section>
    <p>A decade after a landmark study proved that feeding peanut products to young babies could prevent development of life-threatening allergies, new research finds the change has made a big difference in the real world. </p><p>About 60,000 children have avoided developing peanut allergies after guidance first issued in 2015 upended medical practice by recommending introducing the allergen to infants starting as early as 4 months. </p><p>"That's a remarkable thing, right?" said Dr. David Hill, an allergist and researcher at Children's Hospital of Philadelphia, and author of a study published Monday in the medical journal Pediatrics. Hill and colleagues analyzed electronic health records from dozens of pediatric practices to track diagnoses of food allergies in young children before, during and after the guidelines were issued. </p><p>"I can actually come to you today and say there are less kids with food allergy today than there would have been if we hadn't implemented this public health effort," he added.</p><p>"Our findings have relevance from those of us who treat patients to those caring for infants, and more awareness, education and advocacy could further increase the positive results we observed in this study," he continued. "Future studies could potentially explore specific feeding practices that help us better understand the timing, frequency and dose of foods that optimize protection against food allergies."</p>

    

<p>The researchers found that peanut allergies in children ages 0 to 3 declined by more than 27% after guidance for high-risk kids was first issued in 2015 and by more than 40% after the recommendations were expanded in 2017. </p><p>The effort hasn't yet reduced an overall increase in food allergies in the U.S. in recent years. About 8% of children are affected, including more than 2% with a peanut allergy. </p><p>Peanut allergy is caused when the body's immune system mistakenly identifies proteins in peanuts as harmful and releases chemicals that trigger allergic symptoms, including hives, respiratory symptoms and, sometimes, life-threatening anaphylaxis. </p><p>For decades, doctors had recommended delaying feeding children peanuts and other foods likely to trigger allergies until age 3. But in 2015, Gideon Lack at King's College London published the groundbreaking Learning Early About Peanut Allergy, or LEAP, trial.  </p>

    
    

<p>Lack and colleagues showed that introducing peanut products in infancy reduced the future risk of developing food allergies by more than 80%. Later analysis showed that the protection persisted in about 70% of kids into adolescence.  </p><p>The study immediately sparked new guidelines urging early introduction of peanuts â€” but putting them into practice has been slow. </p><p>Only about 29% of pediatricians and 65% of allergists reported following the expanded guidance issued in 2017, surveys found. </p><p>Confusion and uncertainty about the best way to introduce peanuts early in life led to the lag, according to a commentary that accompanied the study. Early on, medical experts and parents alike questioned whether the practice could be adopted outside of tightly controlled clinical settings.  </p><p>The data for the analysis came from a subset of participating practice sites and may not represent the entire U.S. pediatric population, noted the commentary, led by Dr. Ruchi Gupta, a child allergy expert at Northwestern University.  </p><p>However, the new research offers "promising evidence that early allergen introduction is not only being adopted but may be making a measurable impact," the authors concluded.  </p><p>Advocates for the 33 million people in the U.S. with food allergies welcomed signs that early introduction of peanut products is catching on. </p>

    
    

<p>"This research reinforces what we already know and underscores a meaningful opportunity to reduce the incidence and prevalence of peanut allergy nationwide," said Sung Poblete, chief executive of the nonprofit group Food Allergy Research &amp; Education, or FARE.  </p><p>The new study emphasizes the current guidance, updated in 2021, which calls for introducing peanuts and other major food allergens between four and six months, without prior screening or testing, Hill said. Parents should consult their pediatricians about any questions.  </p><p>"It doesn't have to be a lot of the food, but little tastes of peanut butter, milk-based yogurt, soy-based yogurts and tree butters," he said. "These are really good ways to allow the immune system exposure to these allergenic foods in a safe way." </p><p>Tiffany Leon, 36, a Maryland registered dietician and director at FARE, introduced peanuts and other allergens early to her own sons, James, 4, and Cameron, 2. </p><p>At first, Leon's own mother was shocked at the advice to feed babies such foods before the age of 3, she said. But Leon explained how the science had changed. </p><p>"As a dietician, I practice evidence-based recommendations," she said. "So when someone told me, 'This is how it's done now, these are the new guidelines,' I just thought, 'OK, well, this is what we're going to do.'"</p>
  </section>

  

                
        
      
                  
    <!-- data-recirc-source="queryly" -->
    



    
    
  <section>
  <h2>In:</h2>
  <ul>
          <li><a href="https://www.cbsnews.com/tag/allergies/">Allergies</a></li>
          <li><a href="https://www.cbsnews.com/tag/peanuts/">Peanuts</a></li>
      </ul>
</section>

  

  
  </article>
            

                                                                                            </div></div>]]></description>
        </item>
    </channel>
</rss>