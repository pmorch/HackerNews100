(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 24 Jul 2024 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[X redesigns water pistol emoji back to a firearm (143 pts)]]></title>
            <link>https://blog.emojipedia.org/x-redesigns-water-pistol-emoji-back-to-a-firearm/</link>
            <guid>41060813</guid>
            <pubDate>Wed, 24 Jul 2024 19:20:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.emojipedia.org/x-redesigns-water-pistol-emoji-back-to-a-firearm/">https://blog.emojipedia.org/x-redesigns-water-pistol-emoji-back-to-a-firearm/</a>, See on <a href="https://news.ycombinator.com/item?id=41060813">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    <header>
	

<center>
<!-- Tag ID: Emojipedia_Leaderboard_topcentre -->

</center>


        

        <p>X (fka Twitter) has quietly redesigned its   🔫  Water Pistol emoji to display as a firearm. This diverges from the cross-platform conversion of this emoji from firearm to water pistol from 2016 to 2018. </p>

        <section>
                <ul>
                    <li>
                        <a href="https://blog.emojipedia.org/author/keithbroni/">
                            <img src="https://blog.emojipedia.org/content/images/size/w100/2019/02/keithbroni-emojipedia.png" alt="Keith Broni">
                        </a>
                    </li>
                </ul>
                <div>
                    
                    <p><time datetime="2024-07-23">Jul 23, 2024</time>
                        <span><span>•</span> 2 min read</span>
                    </p>
                </div>
            </section>

        <figure>
            <img srcset="https://blog.emojipedia.org/content/images/size/w300/2024/07/X-Twitter-Water-Pistol-Emoji-Header-2.jpg 300w,
                        https://blog.emojipedia.org/content/images/size/w600/2024/07/X-Twitter-Water-Pistol-Emoji-Header-2.jpg 600w,
                        https://blog.emojipedia.org/content/images/size/w1000/2024/07/X-Twitter-Water-Pistol-Emoji-Header-2.jpg 1000w,
                        https://blog.emojipedia.org/content/images/size/w2000/2024/07/X-Twitter-Water-Pistol-Emoji-Header-2.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://blog.emojipedia.org/content/images/size/w2000/2024/07/X-Twitter-Water-Pistol-Emoji-Header-2.jpg" alt="X Redesigns Water Pistol Emoji Back To A Firearm">
        </figure>
    </header>

    <section>
        <p>The social media platform <a href="https://emojipedia.org/twitter?ref=blog.emojipedia.org" rel="noreferrer">X</a> (formerly known as <a href="https://emojipedia.org/twitter?ref=blog.emojipedia.org" rel="noreferrer">Twitter</a>) has redesigned its   <a href="https://emojipedia.org/pistol?ref=blog.emojipedia.org" rel="noreferrer">🔫  Water Pistol</a> emoji to display as an actual firearm. This redesign diverges from the designs of all other major emoji vendors, inverting the cross-platform conversion of this emoji away from being a firearm between 2016 and 2018. </p><figure><img src="https://blog.emojipedia.org/content/images/2024/07/X-Twitter-Water-Pistol-Emoji-Update-2.jpg" alt="" loading="lazy" width="1400" height="700" srcset="https://blog.emojipedia.org/content/images/size/w600/2024/07/X-Twitter-Water-Pistol-Emoji-Update-2.jpg 600w, https://blog.emojipedia.org/content/images/size/w1000/2024/07/X-Twitter-Water-Pistol-Emoji-Update-2.jpg 1000w, https://blog.emojipedia.org/content/images/2024/07/X-Twitter-Water-Pistol-Emoji-Update-2.jpg 1400w" sizes="(min-width: 720px) 720px"><figcaption><span>Above: comparison between Twitter's Water Pistol design and X's new redesign of this emoji as a firearm.</span></figcaption></figure><p><a href="https://emojipedia.org/twitter/twemoji-15.0.2?ref=blog.emojipedia.org" rel="noreferrer">This update</a> is available through X's web client, which still displays the Twemoji emoji design set. It began its rollout on July 18th, the day after <a href="https://emojipedia.org/world-emoji-day?ref=blog.emojipedia.org" rel="noreferrer">World Emoji Day </a>2024.</p>
<!--kg-card-begin: html-->
<blockquote>— kache (@yacineMTB) <a href="https://twitter.com/yacineMTB/status/1814050624813793729?ref_src=twsrc%5Etfw&amp;ref=blog.emojipedia.org">July 18, 2024</a></blockquote> 
<!--kg-card-end: html-->
<p>In February 2023 the Twemoji set ceased to be used by <a href="https://emojipedia.org/twitter?ref=blog.emojipedia.org" rel="noreferrer">Twitter / X</a> on mobile devices, replaced by the device's native emoji designs. This means this update will not be seen on Android devices.</p><p>The Twitter / X app for iOS has always used&nbsp;the native emojis provided by <a href="https://emojipedia.org/apple?ref=blog.emojipedia.org" rel="noreferrer">Apple</a>.</p><p>However, the X engineer responsible for the change, kache, has since stated that they will be "soon updating the rendering on mobile". </p><p>They also stated that this design is "not the final design (going to make it look more badass)".</p><p>As mentioned above, this change by X in effect reverts a cross-vendor design change for the <a href="https://emojipedia.org/pistol?ref=blog.emojipedia.org" rel="noreferrer">🔫  Water Pistol</a> emoji (fka the <a href="http://emojipedia.org/pistol/?ref=blog.emojipedia.org">🔫 Pistol</a>&nbsp;emoji) that was fully implemented in 2018 following a much-publicized design change by <a href="https://blog.emojipedia.org/apple-and-the-gun-emoji/" rel="noreferrer">Apple in 2016</a>.</p><figure><img src="https://blog.emojipedia.org/content/images/2018/04/google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1.jpg" alt="google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1" loading="lazy" width="2000" height="1417" srcset="https://blog.emojipedia.org/content/images/size/w600/2018/04/google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1.jpg 600w, https://blog.emojipedia.org/content/images/size/w1000/2018/04/google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1.jpg 1000w, https://blog.emojipedia.org/content/images/size/w1600/2018/04/google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1.jpg 1600w, https://blog.emojipedia.org/content/images/2018/04/google-pistol-emojis-emojipedia-2012-2018-updated-microsoft-facebook-1.jpg 2117w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Above: A comparison of pistol emoji designs from major vendors 2013—2018.</em></i></figcaption></figure><p>This is the first update to the version of the original Twemoji emoji design set used by X since July last year, when the designs of the&nbsp;<a href="https://emojipedia.org/face-with-medical-mask?ref=blog.emojipedia.org">😷 Face with Medical Mask</a>,&nbsp;<a href="https://emojipedia.org/pleading-face?ref=blog.emojipedia.org">🥺 Pleading Face</a>, and&nbsp;<a href="https://emojipedia.org/face-holding-back-tears?ref=blog.emojipedia.org">🥹 Face Holding Back Tears</a>&nbsp;emojis were updated.</p><p>Additionally, since October 2022 a separate branch of Twemoji has been maintained on former Twemoji designer&nbsp;<a href="https://github.com/jdecked/twemoji?ref=blog.emojipedia.org">Justine De Caires' Github</a>. This offshoot remains open source and has had contributions made by <a href="https://emojipedia.org/discord?ref=blog.emojipedia.org" rel="noreferrer">Discord</a> designers.</p><p>The <a href="https://emojipedia.org/pistol?ref=blog.emojipedia.org" rel="noreferrer">🔫  Water Pistol</a> emoji in this Discord-used Twemoji offshoot remains as the design originally implemented on Twitter in 2018.</p><h2 id="%F0%9F%93%96-read-more">📖 Read More</h2><ul><li><a href="https://blog.emojipedia.org/all-major-vendors-commit-to-gun-redesign/" rel="noreferrer">All Major Vendors Commit to Gun Redesign</a> (April 2018)</li><li><a href="https://blog.emojipedia.org/google-updates-gun-emoji/" rel="noreferrer">Google Updates Gun Emoji</a> (April 2018)</li><li><a href="https://blog.emojipedia.org/apple-and-the-gun-emoji/" rel="noreferrer">Apple And The Gun Emoji</a> (August 2016)</li></ul>
    </section>


</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CrowdStrike global outage to cost US Fortune 500 companies $5.4B (126 pts)]]></title>
            <link>https://www.theguardian.com/technology/article/2024/jul/24/crowdstrike-outage-companies-cost</link>
            <guid>41060158</guid>
            <pubDate>Wed, 24 Jul 2024 18:28:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/article/2024/jul/24/crowdstrike-outage-companies-cost">https://www.theguardian.com/technology/article/2024/jul/24/crowdstrike-outage-companies-cost</a>, See on <a href="https://news.ycombinator.com/item?id=41060158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The <a href="https://www.theguardian.com/technology/microsoft-it-outage" data-link-name="in body link">global technology outage</a> sparked by CrowdStrike’s faulty update will cost US Fortune 500 companies $5.4bn, insurers estimated, as the cybersecurity firm vowed to make changes to prevent it from happening again.</p><p>The projected financial losses exclude <a href="https://www.theguardian.com/technology/microsoft" data-link-name="in body link" data-component="auto-linked-tag">Microsoft</a>, the tech giant whose systems suffered widespread failures in the crash.</p><figure id="b2112e72-0858-49d3-b9b0-e2eb573f3abf" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:2,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;TechScape: Why CrowdStrike-style chaos is here to stay&quot;,&quot;elementId&quot;:&quot;b2112e72-0858-49d3-b9b0-e2eb573f3abf&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/global/article/2024/jul/23/why-crowdstrike-style-chaos-is-here-to-stay&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}"></gu-island></figure><p>Companies in banking and healthcare are expected to be hit the hardest, according to the insurer Parametrix, as well as major airlines. The <a href="https://www.reuters.com/technology/fortune-500-firms-see-54-bln-crowdstrike-losses-says-insurer-parametrix-2024-07-24/" data-link-name="in body link">total insured losses</a> for the non-Microsoft Fortune 500 companies could be between $540m and $1.08bn.</p><p>A variety of industries are still struggling to rectify the damage from <a href="https://www.theguardian.com/business/live/2024/jul/24/easyjet-record-summer-heathrow-busiest-day-farnborough-air-show-virgin-atlantic-manufacturing-pmi-sterling-ftse-100-business-live?filterKeyEvents=false&amp;page=with:block-66a0eebe8f082b7073589e03#block-66a0eebe8f082b7073589e03:~:text=Share-,55m%20ago,13.21%C2%A0BST,-CrowdStrike%20pledges%20to" data-link-name="in body link">CrowdStrike’s outage</a>, which grounded thousands of flights, caused turmoil at hospitals and crashed payment systems in what experts have described as the largest IT failure in history. The outage exposed how modern tech systems are built on precarious ground, with faulty code in a single update able to bring down operations around the world.</p><p>CrowdStrike – a <a href="https://www.theguardian.com/technology/article/2024/jul/19/what-is-crowdstrike-microsoft-windows-outage" data-link-name="in body link">Texas-based, multibillion-dollar company</a> that has lost about 22% of its stock market value since the outage – has repeatedly apologized for causing the international tech crisis. The company <a href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/" data-link-name="in body link">issued a report</a> on Wednesday detailing what went wrong in the update.</p><p>The primary cause of the failure stemmed from an update that CrowdStrike pushed to its flagship Falcon platform, which functions as a cloud-based service intended to protect businesses from cyber-attacks and disruptions. The update contained a bug which caused 8.5m Windows machines to crash en masse.</p><p>CrowdStrike stated in its postmortem that it plans to increase software testing before issuing updates in the future, and only roll out those updates gradually to prevent the widespread, simultaneous failures that took place last week. The company also plans to issue a more in-depth report on the causes of the outage in the coming weeks.</p><p>CrowdStrike is one of the world’s most <a href="https://www.theguardian.com/technology/article/2024/jul/19/what-is-crowdstrike-microsoft-windows-outage" data-link-name="in body link">prominent cybersecurity firms</a>, and was valued at around $83bn before the outage. It services about 538 of the Fortune 1000 companies, according to its website, and operates around the world. That ubiquity made the consequences of its botched update particularly severe, showcasing how many companies are reliant on the same products to keep operations running.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-9">skip past newsletter promotion</a><p id="EmailSignup-skip-link-9" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>Several companies have had an especially hard time recovering from the outage. Delta Air Lines is still in turmoil days later as it cancels and reschedules hundreds of flights, leaving frustrated passengers without the ability to get home and <a href="https://www.washingtonpost.com/travel/2024/07/23/delta-flight-cancellations-unaccompanied-minors/" data-link-name="in body link">parents scrambling</a> to reach their stranded children. The US Department of Transportation <a href="https://www.theguardian.com/technology/article/2024/jul/23/delta-investigation-crowdstrike-flight-cancellations" data-link-name="in body link">opened an investigation</a> into Delta on Tuesday over its handling of the issue.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anyone can access deleted and private repository data on GitHub (513 pts)]]></title>
            <link>https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github</link>
            <guid>41060102</guid>
            <pubDate>Wed, 24 Jul 2024 18:24:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github">https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github</a>, See on <a href="https://news.ycombinator.com/item?id=41060102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-component-type="RichTextContainer"><p>You can access data from <em>deleted forks</em>, <em>deleted repositories</em> and even <em>private repositories</em> on GitHub. And it is available forever. This is known by GitHub, and intentionally designed that way. </p><p>This is such an enormous attack vector for all organizations that use GitHub that we’re introducing a new term: <strong>Cross Fork Object Reference (CFOR)</strong>. A CFOR vulnerability occurs when one repository fork can access sensitive data from another fork (including data from private and deleted forks). Similar to an Insecure Direct Object Reference, in CFOR users supply commit hashes to directly access commit data that otherwise would not be visible to them. </p><p>Let’s see a few examples.</p><h2>Accessing Deleted Fork Data</h2><p>Consider this common workflow on GitHub:&nbsp;</p><ol><li data-preset-tag="p"><p>You fork a public repository</p></li><li data-preset-tag="p"><p>You commit code to your fork</p></li><li data-preset-tag="p"><p>You delete your fork</p></li></ol><p><img alt="" data-framer-asset="data:framer/asset-reference,msknWhH1EkTt7PchLIRCt3npCI.png" data-framer-height="1646" data-framer-width="2723" height="823" src="https://framerusercontent.com/images/msknWhH1EkTt7PchLIRCt3npCI.png" srcset="https://framerusercontent.com/images/msknWhH1EkTt7PchLIRCt3npCI.png?scale-down-to=512 512w,https://framerusercontent.com/images/msknWhH1EkTt7PchLIRCt3npCI.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/msknWhH1EkTt7PchLIRCt3npCI.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/msknWhH1EkTt7PchLIRCt3npCI.png 2723w" width="1361" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>Is the code you committed to the fork still accessible? It shouldn’t be, right? You deleted it.</p><p>It is. And it’s accessible forever. Out of your control.&nbsp;</p><p>In the video below, you’ll see us fork a repository, commit data to it, delete the fork, and then access the “deleted” commit data via the original repository.</p><p><strong>You might think you’re protected by needing to know the commit hash. You’re not. The hash is discoverable. More on that later.</strong></p><h4>How often can we find data from deleted forks?</h4><p>Pretty often. We surveyed a few (literally 3) commonly-forked public repositories from a large AI company and easily found 40 valid API keys from deleted forks. The user pattern seemed to be this:</p><ol><li data-preset-tag="p"><p>Fork the repo.</p></li><li data-preset-tag="p"><p>Hard-code an API key into an example file.&nbsp;</p></li><li data-preset-tag="p"><p>&lt;Do Work&gt;</p></li><li data-preset-tag="p"><p>Delete the fork.</p></li></ol><p><img alt="" data-framer-asset="data:framer/asset-reference,CIeHAgW971XDzRy61aiZjY8fBqE.png" data-framer-height="1102" data-framer-width="2394" height="551" src="https://framerusercontent.com/images/CIeHAgW971XDzRy61aiZjY8fBqE.png" srcset="https://framerusercontent.com/images/CIeHAgW971XDzRy61aiZjY8fBqE.png?scale-down-to=512 512w,https://framerusercontent.com/images/CIeHAgW971XDzRy61aiZjY8fBqE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/CIeHAgW971XDzRy61aiZjY8fBqE.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/CIeHAgW971XDzRy61aiZjY8fBqE.png 2394w" width="1197" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p><strong>But this gets worse, it works in reverse too:&nbsp;</strong></p><h2>Accessing Deleted Repo Data</h2><p>Consider this scenario:</p><ol><li data-preset-tag="p"><p>You have a public repo on GitHub.</p></li><li data-preset-tag="p"><p>A user forks your repo.</p></li><li data-preset-tag="p"><p>You commit data after they fork it (and they never sync their fork with your updates).</p></li><li data-preset-tag="p"><p>You delete the entire repo.</p></li></ol><p><img alt="" data-framer-asset="data:framer/asset-reference,A7rA45DJNYSMUEPCF6tj4wilVC0.png" data-framer-height="1590" data-framer-width="2647" height="795" src="https://framerusercontent.com/images/A7rA45DJNYSMUEPCF6tj4wilVC0.png" srcset="https://framerusercontent.com/images/A7rA45DJNYSMUEPCF6tj4wilVC0.png?scale-down-to=512 512w,https://framerusercontent.com/images/A7rA45DJNYSMUEPCF6tj4wilVC0.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/A7rA45DJNYSMUEPCF6tj4wilVC0.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/A7rA45DJNYSMUEPCF6tj4wilVC0.png 2647w" width="1323" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>Is the code you committed after they forked your repo still accessible?</p><p>Yep.</p><p>GitHub stores repositories and forks in a <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/about-permissions-and-visibility-of-forks#about-visibility-of-forks" rel="noopener">repository network</a>, with the original “upstream” repository acting as the root node. <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/what-happens-to-forks-when-a-repository-is-deleted-or-changes-visibility#deleting-a-public-repository" rel="noopener">When a public “upstream” repository that has been forked is “deleted”, GitHub reassigns the root node role to one of the downstream forks</a>. However, all of the commits from the “upstream” repository still exist and are accessible via any fork.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,jCEeyZLP33ugahiS5Oc3N9ms.png" data-framer-height="2324" data-framer-width="3686" height="1162" src="https://framerusercontent.com/images/jCEeyZLP33ugahiS5Oc3N9ms.png" srcset="https://framerusercontent.com/images/jCEeyZLP33ugahiS5Oc3N9ms.png?scale-down-to=512 512w,https://framerusercontent.com/images/jCEeyZLP33ugahiS5Oc3N9ms.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/jCEeyZLP33ugahiS5Oc3N9ms.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/jCEeyZLP33ugahiS5Oc3N9ms.png 3686w" width="1843" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>In the video below, we create a repo, fork it and then show how data not synced with the fork can still be accessed by the fork after the original repo is deleted.</p><p>This isn’t just some weird edge case scenario. This unfolded last week:</p><p><em>I submitted a P1 vulnerability to a major tech company showing they accidentally committed a private key for an employee’s GitHub account that had significant access to their entire GitHub organization. They immediately deleted the repository, but since it had been forked, I could still access the commit containing the sensitive data via a fork, despite the fork never syncing with the original “upstream” repository.</em></p><p>The implication here is that any code committed to a public repository may be accessible <em>forever</em> as long as there is at least one fork of that repository.</p><p><strong>It gets worse.</strong></p><h2>Accessing Private Repo Data</h2><p>Consider this common workflow for open-sourcing a new tool on GitHub:</p><ol><li data-preset-tag="p"><p>You create a private repo that will eventually be made public.</p></li><li data-preset-tag="p"><p>You create a private, internal version of that repo (via forking) and commit additional code for features that you’re not going to make public.</p></li><li data-preset-tag="p"><p>You make your “upstream” repository public and keep your fork private.</p></li></ol><p><img alt="" data-framer-asset="data:framer/asset-reference,xImmfuPpiSy9ttCvAMC5G46bGSk.png" data-framer-height="1590" data-framer-width="2481" height="795" src="https://framerusercontent.com/images/xImmfuPpiSy9ttCvAMC5G46bGSk.png" srcset="https://framerusercontent.com/images/xImmfuPpiSy9ttCvAMC5G46bGSk.png?scale-down-to=512 512w,https://framerusercontent.com/images/xImmfuPpiSy9ttCvAMC5G46bGSk.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/xImmfuPpiSy9ttCvAMC5G46bGSk.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/xImmfuPpiSy9ttCvAMC5G46bGSk.png 2481w" width="1240" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>Are your private features and related code (from step 2) viewable by the public?</p><p>Yes. Any code committed between the time you created an internal fork of your tool and when you open-sourced the tool, those commits are accessible on the public repository.&nbsp;</p><p>Any commits made to your private fork <em>after</em> you make the “upstream” repository public are not viewable. That’s because changing the visibility of a private “upstream” repository results in two repository networks - one for the private version, and one for the public version.&nbsp;</p><p><img alt="" data-framer-asset="data:framer/asset-reference,zOeORJBOu7eK4cx0y2qdgtXNW4.png" data-framer-height="2000" data-framer-width="3921" height="1000" src="https://framerusercontent.com/images/zOeORJBOu7eK4cx0y2qdgtXNW4.png" srcset="https://framerusercontent.com/images/zOeORJBOu7eK4cx0y2qdgtXNW4.png?scale-down-to=512 512w,https://framerusercontent.com/images/zOeORJBOu7eK4cx0y2qdgtXNW4.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/zOeORJBOu7eK4cx0y2qdgtXNW4.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/zOeORJBOu7eK4cx0y2qdgtXNW4.png 3921w" width="1960" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>In the video below, we demonstrate how organizations open-source new tools while maintaining private internal forks, and then show how someone could access commit data from the private internal version via the public one.</p><p>Unfortunately, this workflow is one of the most common approaches users and organizations take to developing open-source software. As a result, it’s possible that confidential data and secrets are inadvertently being exposed on an organization's public GitHub repositories.</p><h2>How do you actually access the data?</h2><p>By directly accessing the commit.</p><p>Destructive actions in GitHub’s repository network (like the 3 scenarios mentioned above) remove references to commit data from the standard GitHub UI and normal git operations. However, this data still exists and is accessible (if you know the commit hash). This is the tie-in between CFOR and IDOR vulnerabilities - if you know the commit hash you can directly access data that is not intended for you.</p><p>Commit hashes are SHA-1 values.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png" data-framer-height="602" data-framer-width="2322" height="301" src="https://framerusercontent.com/images/EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png" srcset="https://framerusercontent.com/images/EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png?scale-down-to=512 512w,https://framerusercontent.com/images/EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/EoVCvCLjHvZJCuMrrZ0ZwQd3W8M.png 2322w" width="1161" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>If a user knows the SHA-1 commit hash of a particular commit they want to see, they can directly navigate to that commit at the endpoint: https://github.com<code>/&lt;user/org&gt;/&lt;repo&gt;/commit/&lt;commit_hash&gt;</code>. They’ll see a yellow banner explaining that “[t]his commit does not belong to any branch of this repository, and may belong to a fork outside of the repository.”</p><p><img alt="" data-framer-asset="data:framer/asset-reference,B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png" data-framer-height="1410" data-framer-width="2324" height="705" src="https://framerusercontent.com/images/B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png" srcset="https://framerusercontent.com/images/B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png?scale-down-to=512 512w,https://framerusercontent.com/images/B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/B0wRJU4mjHvmKdy7mpZ3Z3wRV8.png 2324w" width="1162" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p><strong>Where do you get these hash values?</strong></p><p>Commit hashes can be brute forced through GitHub’s UI, particularly because the git protocol permits the use of <a href="https://git-scm.com/book/en/v2/Git-Tools-Revision-Selection#:~:text=to%20any%20commit.-,Short%20SHA%2D1,-Git%20is%20smart" rel="noopener">short SHA-1 values</a> when referencing a commit. A short SHA-1 value is the minimum number of characters required to avoid a collision with another commit hash, with an absolute minimum of 4. The keyspace of all 4 character SHA-1 values is 65,536 (16^4). Brute forcing all possible values can be achieved relatively easily.&nbsp;</p><p>For example, consider this commit in TruffleHog’s repository:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,yPbRdgv9LoasW1BXLK09dZNMSXs.png" data-framer-height="1098" data-framer-width="2320" height="549" src="https://framerusercontent.com/images/yPbRdgv9LoasW1BXLK09dZNMSXs.png" srcset="https://framerusercontent.com/images/yPbRdgv9LoasW1BXLK09dZNMSXs.png?scale-down-to=512 512w,https://framerusercontent.com/images/yPbRdgv9LoasW1BXLK09dZNMSXs.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/yPbRdgv9LoasW1BXLK09dZNMSXs.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/yPbRdgv9LoasW1BXLK09dZNMSXs.png 2320w" width="1160" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>To access this commit, users typically visit the URL containing the full SHA-1 commit hash: <a href="https://github.com/trufflesecurity/trufflehog/commit/07f01e8337c1073d2c45bb12d688170fcd44c637" rel="noopener">https://github.com/trufflesecurity/trufflehog/commit/07f01e8337c1073d2c45bb12d688170fcd44c637</a></p><p>But users don’t need to know the entire 32 character SHA-1 value, they only need to correctly guess the Short SHA-1 value, which in this case is <code>07f01e</code>.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,jji5JQSyL5Bh0OJtpMQDB65DE.png" data-framer-height="1324" data-framer-width="2326" height="662" src="https://framerusercontent.com/images/jji5JQSyL5Bh0OJtpMQDB65DE.png" srcset="https://framerusercontent.com/images/jji5JQSyL5Bh0OJtpMQDB65DE.png?scale-down-to=512 512w,https://framerusercontent.com/images/jji5JQSyL5Bh0OJtpMQDB65DE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/jji5JQSyL5Bh0OJtpMQDB65DE.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/jji5JQSyL5Bh0OJtpMQDB65DE.png 2326w" width="1163" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p><a href="https://github.com/trufflesecurity/trufflehog/commit/07f01e" rel="noopener">https://github.com/trufflesecurity/trufflehog/commit/07f01e</a></p><p>But what’s more interesting; GitHub exposes a public events API endpoint. You can also query for commit hashes in the <a href="https://www.gharchive.org/" rel="noopener">events archive</a> which is managed by a 3rd party, and saves all GitHub events for the past decade outside of GitHub, even after the repos get deleted.</p><h2>GitHub’s Policies</h2><p>We recently submitted our findings to GitHub via their VDP program. This was their response:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,G9xGKRx7gPHauxianQClKVxPE.png" data-framer-height="314" data-framer-width="1874" height="157" src="https://framerusercontent.com/images/G9xGKRx7gPHauxianQClKVxPE.png" srcset="https://framerusercontent.com/images/G9xGKRx7gPHauxianQClKVxPE.png?scale-down-to=512 512w,https://framerusercontent.com/images/G9xGKRx7gPHauxianQClKVxPE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/G9xGKRx7gPHauxianQClKVxPE.png 1874w" width="937" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p>After reviewing the documentation, it’s clear as day that GitHub designed repositories to work like this.&nbsp;</p><p><img alt="" data-framer-asset="data:framer/asset-reference,eE6IuZrodHY2R0pBWcGHKPNxI.png" data-framer-height="1270" data-framer-width="2312" height="635" src="https://framerusercontent.com/images/eE6IuZrodHY2R0pBWcGHKPNxI.png" srcset="https://framerusercontent.com/images/eE6IuZrodHY2R0pBWcGHKPNxI.png?scale-down-to=512 512w,https://framerusercontent.com/images/eE6IuZrodHY2R0pBWcGHKPNxI.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/eE6IuZrodHY2R0pBWcGHKPNxI.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/eE6IuZrodHY2R0pBWcGHKPNxI.png 2312w" width="1156" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p><img alt="" data-framer-asset="data:framer/asset-reference,UpywoiGAzxzDtqcMAzLxKeW6dwQ.png" data-framer-height="612" data-framer-width="2318" height="306" src="https://framerusercontent.com/images/UpywoiGAzxzDtqcMAzLxKeW6dwQ.png" srcset="https://framerusercontent.com/images/UpywoiGAzxzDtqcMAzLxKeW6dwQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/UpywoiGAzxzDtqcMAzLxKeW6dwQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/UpywoiGAzxzDtqcMAzLxKeW6dwQ.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/UpywoiGAzxzDtqcMAzLxKeW6dwQ.png 2318w" width="1159" data-framer-original-sizes="" sizes="(min-width: 1000px) 100vw, (max-width: 999px) 100vw"></p><p><a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/what-happens-to-forks-when-a-repository-is-deleted-or-changes-visibility" rel="noopener">https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/what-happens-to-forks-when-a-repository-is-deleted-or-changes-visibility</a></p><p>We appreciate that GitHub is transparent about their architecture and has taken the time to clearly document what users should expect to happen in the instances documented above.&nbsp;</p><p>Our issue is this:</p><p>The average user views the separation of private and public repositories as a security boundary, and understandably believes that any data located in a private repository cannot be accessed by public users. Unfortunately, as we documented above, that is not always true. Whatsmore, the act of deletion implies the destruction of data. As we saw above, deleting a repository or fork does not mean your commit data is actually deleted.</p><h2>Implications</h2><p>We have a few takeaways from this:</p><ol><li data-preset-tag="p"><p><strong>As long as one fork exists, any commit to that repository network (ie: commits on the “upstream” repo or “downstream” forks) will exist forever.</strong></p><ol><li data-preset-tag="p"><p>This further cements our view that the only way to securely remediate a leaked key on a public GitHub repository is through key rotation. We’ve spent a lot of time documenting how to rotate keys for the most popularly leaked secret types - check our work out here: <a href="https://howtorotate.com/docs/introduction/getting-started/" target="_blank" rel="noopener">howtorotate.com</a>.</p></li></ol></li></ol><ol start="2"><li data-preset-tag="p"><p>GitHub’s repository architecture necessitates these design flaws and unfortunately, the vast <strong>majority of GitHub users will never understand how a repository network actually works and will be less secure </strong>because of it.</p></li></ol><ol start="3"><li data-preset-tag="p"><p>As secret scanning evolves, and we can hopefully scan all commits in a repository network, <strong>we’ll be alerting on secrets that might not be our own</strong> (ie: they might belong to someone who forked a repository). This will require more diligent triaging.</p></li><li data-preset-tag="p"><p>While these three scenarios are shocking, that doesn’t even cover all of the ways GitHub could be storing deleted data from your repositories. Check out our <a href="https://trufflesecurity.com/blog/trufflehog-scans-deleted-git-branches" rel="noopener">recent post</a> (and related TruffleHog update) about how you also need to scan for secrets in deleted branches.&nbsp;</p></li></ol><p>Finally, while our research focused on GitHub, it’s important to note that some of these issues exist on other version control system products.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel confirms oxidation and excessive voltage in 13th and 14th Gen CPUs [video] (173 pts)]]></title>
            <link>https://www.youtube.com/watch?v=OVdmK1UGzGs</link>
            <guid>41058791</guid>
            <pubDate>Wed, 24 Jul 2024 16:33:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=OVdmK1UGzGs">https://www.youtube.com/watch?v=OVdmK1UGzGs</a>, See on <a href="https://news.ycombinator.com/item?id=41058791">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[CrowdStrike offers a $10 apology gift card to say sorry for outage (257 pts)]]></title>
            <link>https://techcrunch.com/2024/07/24/crowdstrike-offers-a-10-apology-gift-card-to-say-sorry-for-outage/</link>
            <guid>41058261</guid>
            <pubDate>Wed, 24 Jul 2024 15:49:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/24/crowdstrike-offers-a-10-apology-gift-card-to-say-sorry-for-outage/">https://techcrunch.com/2024/07/24/crowdstrike-offers-a-10-apology-gift-card-to-say-sorry-for-outage/</a>, See on <a href="https://news.ycombinator.com/item?id=41058261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">CrowdStrike, the cybersecurity firm <a href="https://techcrunch.com/2024/07/19/what-we-know-about-crowdstrikes-update-fail-thats-causing-global-outages-and-travel-chaos/">that crashed millions of computers with a botched update</a> all over the world last week, is offering its partners a $10 Uber Eats gift card as an apology, according to <a rel="nofollow" href="https://x.com/1337ice_cream/status/1815958499496472859?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">several</a> <a rel="nofollow" href="https://x.com/hasminesita/status/1815856664568090836?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">people</a> <a rel="nofollow" href="https://x.com/david__exe/status/1815908023296221372?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">who</a> <a rel="nofollow" href="https://x.com/christappin/status/1816039053357375720?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">say</a> <a rel="nofollow" href="https://x.com/keatonincyber/status/1815955882838221225?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">they</a> received the gift card, as well as a source who also received one.</p>

<p>On Tuesday, a source told TechCrunch that they received an email from CrowdStrike offering them the gift card because the company recognizes “the additional work that the July 19 incident has caused.”&nbsp;</p>

	
	


<p>“And for that, we send our heartfelt thanks and apologies for the inconvenience,” the email read, according to a screenshot shared by the source. The <a rel="nofollow" href="https://x.com/64uni_lions/status/1815928437774995555?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">same email</a> was also posted on X by someone else. “To express our gratitude, your next cup of coffee or late night snack is on us!”</p>

<figure><img decoding="async" width="1858" height="1542" src="https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png 1858w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=150,124 150w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=300,249 300w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=768,637 768w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=680,564 680w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=1200,996 1200w, https://techcrunch.com/wp-content/uploads/2024/07/crowdstrike-ubereats-voucher-gift-card.png?resize=1536,1275 1536w" sizes="(max-width: 1858px) 100vw, 1858px"><figcaption>A screenshot of the email sent to partners by CrowdStrike after the July 19 incident (Image: TechCrunch)</figcaption></figure>

<p>The email was sent in the name of <a rel="nofollow" href="https://www.crowdstrike.com/about-crowdstrike/executive-team/daniel-bernard/">Daniel Bernard</a>, the company’s chief business officer, according to a screenshot of the email seen by TechCrunch According to <a rel="nofollow" href="https://x.com/david__exe/status/1815908023296221372?s=46&amp;t=lLf2YRXhv7kG_Tw6dp_nCA">one post</a> on X, in the United Kingdom the voucher was worth £7.75, or roughly $10 at today’s exchange rate.</p>

<p>On Wednesday, some of the people who posted about the gift card said that when they went to redeem the offer, they got an error message saying the voucher had been canceled. When TechCrunch checked the voucher, the Uber Eats page provided an error message that said the gift card “has been canceled by the issuing party and is no longer valid.”</p>

<p>CrowdStrike did not immediately respond to a request for comment.&nbsp;</p>

<p>On Friday, CrowdStrike released a faulty update that rendered around 8.5 million Windows devices unusable, <a rel="nofollow" href="https://blogs.microsoft.com/blog/2024/07/20/helping-our-customers-through-the-crowdstrike-outage/">according to Microsoft</a>. The update caused the affected computers to be stuck at the infamous “blue screen of death,” or BSOD, a bright blue error screen with a message that is shown when Windows crashed or cannot load because of a critical software failure.</p>

	
	



<p>The outage caused delays at airports in Amsterdam, Berlin, Dubai, and London, and <a rel="nofollow" href="https://x.com/US_Stormwatch/status/1814268813879206397">across the United States</a>. It also caused <a rel="nofollow" href="https://www.beckersspine.com/orthopedic-spine-practices-improving-profits/60138-hospitals-halt-non-urgent-surgeries-amid-global-it-outage.html#:~:text=Hospitals%20across%20the%20U.S.%20postponed,affecting%20computers%20and%20servers%20worldwide.">several</a> <a rel="nofollow" href="https://www.reuters.com/business/healthcare-pharmaceuticals/two-german-hospitals-cancel-elective-operations-citing-global-it-outage-2024-07-19/">hospitals</a> to halt surgeries, and paralyzed countless businesses all over the world.&nbsp;</p>
<div>
		<h4>Contact Us</h4><p>
		Do you have more information about the CrowdStrike outage? From a non-work device, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram and Keybase @lorenzofb, or <a href="mailto:lorenzo@techcrunch.com/">email</a><a href="mailto:lorenzo@techcrunch.com/">.</a> You also can contact TechCrunch via <a href="https://techcrunch.com/got-a-tip/">SecureDrop</a>.	</p></div>
	

<p>Since the outage began on Friday, CrowdStrike has <a rel="nofollow" href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/">regularly published updates</a> on its efforts to figure out what caused the mass outage. In an update on Wednesday, the company said that because of a bug during the process to check that updates are ready to be released to customer devices, the faulty code “passed validation despite containing problematic content data.”</p>

	
	


<p>The company also published apologies from its CEO George Kurtz, as well as its chief security officer Shawn Henry.&nbsp;</p>

<p>“All of CrowdStrike understands the gravity and impact of the situation,” Kurtz <a rel="nofollow" href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/#:~:text=contacting%20CrowdStrike%20Support.-,Statement%20from%20our%20CEO,-Sent%202024%2D07">said in a message</a> published on the company’s site. “Nothing is more important to me than the trust and confidence that our customers and partners have put into CrowdStrike. As we resolve this incident, you have my commitment to provide full transparency on how this occurred and steps we’re taking to prevent anything like this from happening again.”</p>

<p>Henry <a rel="nofollow" href="https://www.linkedin.com/posts/shawn-henry-372bb74b_on-friday-we-failed-you-and-for-that-im-activity-7220983915421806592-VhPP/">wrote on Linkedin</a> that “we failed you, and for that I’m deeply sorry.”</p>

<p>“I’ve been in my professional life for almost 40 years, and my North Star has always been to ‘protect good people from bad things,’” Henry wrote. “The past two days have been the most challenging 48 hours for me over 12+ years. The confidence we built in drips over the years was lost in buckets within hours, and it was a gut punch.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI models collapse when trained on recursively generated data (153 pts)]]></title>
            <link>https://www.nature.com/articles/s41586-024-07566-y</link>
            <guid>41058194</guid>
            <pubDate>Wed, 24 Jul 2024 15:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41586-024-07566-y">https://www.nature.com/articles/s41586-024-07566-y</a>, See on <a href="https://news.ycombinator.com/item?id=41058194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><p>The development of LLMs is very involved and requires large quantities of training data. Yet, although current LLMs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR2" id="ref-link-section-d32410486e578">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. in Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (eds Burstein, J., Doran, C. &amp; Solorio, T.) 4171–4186 (Association for Computational Linguistics, 2019)." href="#ref-CR4" id="ref-link-section-d32410486e581">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Liu, Y. et al. RoBERTa: a Robustly Optimized BERT Pretraining Approach. Preprint at 
                  https://arxiv.org/abs/1907.11692
                  
                 (2019)." href="#ref-CR5" id="ref-link-section-d32410486e581_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Zhang, S. et al. Opt: open pre-trained transformer language models. Preprint at 
                  https://arxiv.org/abs/2205.01068
                  
                 (2022)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR6" id="ref-link-section-d32410486e584">6</a></sup>, including GPT-3, were trained on predominantly human-generated text, this may change. If the training data of most future models are also scraped from the web, then they will inevitably train on data produced by their predecessors. In this paper, we investigate what happens when text produced by, for example, a version of GPT forms most of the training dataset of following models. What happens to GPT generations GPT-{<i>n</i>} as <i>n</i> increases? We discover that indiscriminately learning from data produced by other models causes ‘model collapse’—a degenerative process whereby, over time, models forget the true underlying data distribution, even in the absence of a shift in the distribution over time. We give examples of model collapse for GMMs, VAEs and LLMs. We show that, over time, models start losing information about the true distribution, which first starts with tails disappearing, and learned behaviours converge over the generations to a point estimate with very small variance. Furthermore, we show that this process is inevitable, even for cases with almost ideal conditions for long-term learning, that is, no function estimation error. We also briefly mention two close concepts to model collapse from the existing literature: catastrophic forgetting arising in the framework of task-free continual learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Aljundi, R., Kelchtermans, K. &amp; Tuytelaars, T. Task-free continual learning. in: Proc. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 11254–11263 (IEEE, 2019)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR7" id="ref-link-section-d32410486e594">7</a></sup> and data poisoning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Carlini, N. &amp; Terzis, A. in Proc. Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR8" id="ref-link-section-d32410486e598">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Carlini, N. et al. in Proc. 2024 IEEE Symposium on Security and Privacy (SP) 179 (IEEE, 2024)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR9" id="ref-link-section-d32410486e601">9</a></sup> maliciously leading to unintended behaviour. Neither is able to explain the phenomenon of model collapse fully, as the setting is fundamentally different, but they provide another perspective on the observed phenomenon and are discussed in more depth in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>. Finally, we discuss the broader implications of model collapse. We note that access to the original data distribution is crucial: in learning tasks in which the tails of the underlying distribution matter, one needs access to real human-produced data. In other words, the use of LLMs at scale to publish content on the Internet will pollute the collection of data to train their successors: data about human interactions with LLMs will be increasingly valuable.</p></div><div id="Sec2-section" data-title="What is model collapse?"><h2 id="Sec2">What is model collapse?</h2><div id="Sec2-content">
                <h3 id="FPar1">Definition 2.1 (model collapse)</h3>
                <p>Model collapse is a degenerative process affecting generations of learned generative models, in which the data they generate end up polluting the training set of the next generation. Being trained on polluted data, they then mis-perceive reality. The process is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-07566-y#Fig1">1a</a>. We separate two special cases: early model collapse and late model collapse. In early model collapse, the model begins losing information about the tails of the distribution; in late model collapse, the model converges to a distribution that carries little resemblance to the original one, often with substantially reduced variance.</p>
              <p>This process occurs owing to three specific sources of error compounding over generations and causing deviation from the original model:</p><ul>
                <li>
                  <p><b>Statistical approximation error.</b> This is the primary type of error, which arises owing to the number of samples being finite, and disappears as the number of samples tends to infinity. This occurs because of a non-zero probability that information can get lost at every step of resampling.</p>
                </li>
                <li>
                  <p><b>Functional expressivity error.</b> This is a secondary type of error, arising owing to limited function approximator expressiveness. In particular, neural networks are only universal approximators as their size goes to infinity. As a result, a neural network can introduce non-zero likelihood outside the support of the original distribution or zero likelihood inside the support of the original distribution. A simple example of the expressivity error is if we tried fitting a mixture of two Gaussians with a single Gaussian. Even if we have perfect information about the data distribution (that is, infinite number of samples), model errors will be inevitable. However, in the absence of the other two types of error, this can only occur at the first generation.</p>
                </li>
                <li>
                  <p><b>Functional approximation error.</b> This is a secondary type of error, arising primarily from the limitations of learning procedures, for example, structural bias of stochastic gradient descent<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Mousavi-Hosseini, A., Park, S., Girotti, M., Mitliagkas, I. &amp; Erdogdu, M. A. in Proc. Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR10" id="ref-link-section-d32410486e652">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. &amp; Srebro, N. The implicit bias of gradient descent on separable data. J. Mach. Learn. Res. 19, 1–57 (2018)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR11" id="ref-link-section-d32410486e655">11</a></sup> or choice of objective<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Gu, Y., Dong, L., Wei, F. &amp; Huang, M. in Proc. Twelfth International Conference on Learning Representations (ICLR, 2024)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR12" id="ref-link-section-d32410486e659">12</a></sup>. This error can be viewed as one arising in the limit of infinite data and perfect expressivity at each generation.</p>
                </li>
              </ul><p>Each of the above can cause model collapse to get worse or better. More approximation power can even be a double-edged sword—better expressiveness may counteract statistical noise, resulting in a good approximation of the true distribution, but it can equally compound the noise. More often than not, we get a cascading effect, in which individual inaccuracies combine to cause the overall error to grow. For example, overfitting the density model causes the model to extrapolate incorrectly and assigns high-density regions to low-density regions not covered in the training set support; these will then be sampled with arbitrary frequency. It is worth noting that other types of error exist. For example, computers have limited precision in practice. We now turn to mathematical intuition to explain how the above give rise to the errors observed, how different sources can compound and how we can quantify the average model divergence.</p></div></div><div id="Sec3-section" data-title="Theoretical intuition"><h2 id="Sec3">Theoretical intuition</h2><div id="Sec3-content"><p>Here we provide a theoretical intuition for the phenomenon of model collapse. We argue that the process of model collapse is universal among generative models that recursively train on data generated by previous generations. We quantify the sources of errors discussed in the previous section by examining two mathematical models, which prove to be simple enough to provide analytical expressions for quantities of interest, but also portray the phenomenon of model collapse: a discrete distribution in the absence of functional expressivity and approximation errors, and a multidimensional Gaussian approximation, portraying joint functional expressivity and statistical errors. We further illustrate the impact of all three jointly for a more complex setting of density estimation in Hilbert spaces in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>.</p><p>The overall stochastic process we consider, which we call learning with generational data, is the following. The dataset at generation <i>i</i> is <span>\({{\mathcal{D}}}_{i}\)</span>, comprising independent and identically distributed random variables <span>\({X}_{j}^{i}\)</span> with distribution <i>p</i><sub><i>i</i></sub>, <i>j</i> <span>∈</span> {1,…, <i>M</i><sub><i>i</i></sub>} denotes the size of the dataset. Going from generation <i>i</i> to generation <i>i</i> + 1, we aim to estimate the distribution of samples in <span>\({{\mathcal{D}}}_{i}\)</span>, with an approximation <span>\({p}_{{\theta }_{i+1}}\)</span>. This step is what we refer to as functional approximation, <span>\({p}_{{\theta }_{i+1}}={{\mathcal{F}}}_{\theta }({p}_{i})\)</span>. The dataset <span>\({{\mathcal{D}}}_{i+1}\)</span> is then generated by sampling from <span>\({p}_{i+1}={\alpha }_{i}{p}_{{\theta }_{i+1}}+{\beta }_{i}{p}_{i}+{\gamma }_{i}{p}_{0}\)</span>, with non-negative parameters <i>α</i><sub><i>i</i></sub>, <i>β</i><sub><i>i</i></sub>, <i>γ</i><sub><i>i</i></sub> summing to 1, that is, they represent proportions of data used from different generations. This corresponds to a mixing of data coming from the original distribution (<i>γ</i><sub><i>i</i></sub>), data used by the previous generation (<i>β</i><sub><i>i</i></sub>) and data generated by the new model (<i>α</i><sub><i>i</i></sub>). We refer to this as the sampling step. For the mathematical models to come, we consider <i>α</i><sub><i>i</i></sub> = <i>γ</i><sub><i>i</i></sub> = 0, that is, data only from a single step are used, whereas numerical experiments are performed on more realistic choices of parameters.</p><h3 id="Sec4">Discrete distributions with exact approximation</h3><p>In this subsection, we consider a discrete probability distribution in absence of functional approximation and expressivity errors, that is, <span>\({\mathcal{F}}(p)=p\)</span>. In this case, model collapse arises only because of statistical errors from the sampling step. At first, the tails (low-probability events) begin to disappear as a result of the low probability of sampling them and, over time, support of&nbsp;the distribution shrinks. Denoting the sample size as <i>M</i>, if we consider state <i>i</i> with probability <span>\(q\le \frac{1}{M}\)</span>, the expected number of samples with value <i>i</i> coming from those events will be less than 1. In practice, this would mean that we lose information about them. Considering more generally some state <i>i</i> with probability <i>q</i>, using standard conditional probability, we can show that the probability of losing information (that is, sampling no data at some generation) is equal to 1 − <i>q</i>, implying that the distribution must converge to a delta function positioned at some state, with the probability of ending up at a certain state equal to the probability of sampling said state from the original distribution.</p><p>This can be shown directly by considering the process <span>\({{\bf{X}}}^{i}\to {\mathcal{F}}\,\to \)</span><span>\({p}_{i+1}\to {{\bf{X}}}^{i+1}\)</span> as a Markov chain, as <b>X</b><sup><i>i</i>+1</sup> only depends on <b>X</b><sup><i>i</i></sup>. Furthermore, if all the <span>\({X}_{j}^{i}\)</span> have the same value, then at the next generation, the approximated distribution will be exactly a delta function and therefore all of <span>\({X}_{j}^{i+1}\)</span> will also have the same value. This implies that the Markov chain contains at least one absorbing state and therefore, with probability 1, it will converge to one of the absorbing states. This is a well-known fact, of which a proof is provided in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>. For this chain, the only absorbing states are those corresponding to delta functions. As a result, as we follow the progress of model collapse, we are guaranteed to end up in a constant state, having lost all the information of the original distribution when the chain is absorbed. This argument also works in general owing to floating-point representations being discrete, making the Markov chain over the parameters of the model discrete. Thus, as long as the model parameterization allows for delta functions, we will get to it, because—owing to sampling errors—the only possible absorbing states are delta functions. On the basis of the discussion above, we see how both early model collapse, in which only the low-probability events get cut off, and late stage model collapse, in which the process begins to collapse into a single mode, must arise in the case of discrete distributions with perfect functional approximation.</p><h3 id="Sec5">Multidimensional Gaussian</h3><p>Following the discussion about discrete distributions, we now present a more generic result, which can be shown in the Gaussian approximation setting, in which each generation is approximated using the unbiased estimates of the mean and the variance. A similar result holds more generally, which we detail in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>.</p>
                  <h3 id="FPar2">Theorem 3.1 (Gaussian model collapse)</h3>
                  <p>Assume the original data are sampled from distribution <span>\({{\mathcal{D}}}_{0}\)</span> (not necessarily Gaussian), with non-zero sample variance. Assume <i>X</i><sup><i>n</i></sup> are fit recursively using the unbiased sample mean and variance estimators from the previous generation, <span>\({X}_{j}^{n}| {\mu }_{n},{\Sigma }_{n} \sim {\mathcal{N}}({\mu }_{n},{\Sigma }_{n})\)</span>, with a fixed sample size. Then,</p><div id="Equa"><p><span>$${\mathbb{E}}[{{\mathbb{W}}}_{2}^{2}({\mathcal{N}}({\mu }_{n},{\Sigma }_{n}),{{\mathcal{D}}}_{0})]\to \infty ;\,{\Sigma }_{n}\,\mathop{\to }\limits^{{\rm{a}}.{\rm{s}}.}\,0\,\,{\rm{a}}{\rm{s}}\,\,n\to \infty ,$$</span></p></div><p>in which <span>\({{\mathbb{W}}}_{2}\)</span> denotes the Wasserstein-2 distance between the true distribution and its approximation at generation <i>n</i>.</p>
                <p>In words, this implies that not only does the <i>n</i>th generation approximation diverge arbitrarily far from the original one but it also collapses to be zero variance as the number of generations increases, with probability 1. The results are very analogous to that seen in the discrete case, with this theorem illustrating the effect of late stage model collapse, in which the process begins to collapse to be zero variance. The early stage model collapse can also be seen and the interested reader is referred to the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a> for a more in-depth discussion.</p></div></div><div id="Sec6-section" data-title="Model collapse in language models"><h2 id="Sec6">Model collapse in language models</h2><div id="Sec6-content"><p>In this section, we evaluate the effect of model collapse on language models. We cover more interpretable machine learning models—VAEs and GMMs—in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>. Code is publically available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Shumailov, I. &amp; Shumaylov, Z. Public code for Model Collapse (0.1). Zenodo 
                  https://doi.org/10.5281/zenodo.10866595
                  
                 (2024)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR13" id="ref-link-section-d32410486e1876">13</a></sup>.</p><p>Model collapse is universal across various families of machine learning models. Yet, if small models such as GMMs and VAEs are normally trained from scratch, LLMs are different. They are so expensive to retrain from scratch that they are typically initialized with pre-trained models such as BERT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. in Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (eds Burstein, J., Doran, C. &amp; Solorio, T.) 4171–4186 (Association for Computational Linguistics, 2019)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR4" id="ref-link-section-d32410486e1883">4</a></sup>, RoBERTa<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Liu, Y. et al. RoBERTa: a Robustly Optimized BERT Pretraining Approach. Preprint at 
                  https://arxiv.org/abs/1907.11692
                  
                 (2019)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR5" id="ref-link-section-d32410486e1887">5</a></sup> or GPT-2 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR2" id="ref-link-section-d32410486e1891">2</a></sup>), which are trained on large text corpora. They are then fine-tuned to various downstream&nbsp;tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at 
                  https://arxiv.org/abs/2108.07258
                  
                 (2022)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR14" id="ref-link-section-d32410486e1895">14</a></sup>.</p><p>Here we explore what happens with language models when they are sequentially fine-tuned with data generated by other models. We can easily replicate all experiments covered in this paper with larger language models in non-fine-tuning settings to demonstrate model collapse. Given that training a single moderately large model produces twice the American lifetime’s worth of CO<sub>2</sub> (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Strubell, E., Ganesh, A. &amp; McCallum, A. in Proc. 57th Annual Meeting of the Association for Computational Linguistics (eds Korhonen, A., Traum, D. &amp; Màrquez, L.) 3645–3650 (Association for Computational Linguistics, 2019)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR15" id="ref-link-section-d32410486e1904">15</a></sup>), we opted to not run such an experiment and instead focus on a more realistic setting for a proof of concept. Note that even the language experiments described in this paper took weeks to run. We evaluate the most common setting of training a language model—a fine-tuning setting for which each of the training cycles starts from a pre-trained model with recent data. The data here come from another fine-tuned pre-trained model. Because training is restricted to produce models that are close to the original pre-trained model, and data points generated by the models will generally produce very small gradients, the expectation here may be that the model should only change moderately after fine-tuning. We fine-tune the OPT-125m causal language model made available by Meta through Hugging Face<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Zhang, S. et al. Opt: open pre-trained transformer language models. Preprint at 
                  https://arxiv.org/abs/2205.01068
                  
                 (2022)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR6" id="ref-link-section-d32410486e1908">6</a></sup>.</p><p>We fine-tune it on the wikitext2 dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Merity, S., Xiong, C., Bradbury, J. &amp; Socher, R. in Proc. 5th International Conference on Learning Representations (ICLR, 2017)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR16" id="ref-link-section-d32410486e1915">16</a></sup>. For data generation from the trained models, we use a five-way beam search. We block training sequences to be 64 tokens long; then, for each token sequence in the training set, we ask the model to predict the next 64 tokens. We go through all of the original training dataset and produce an artificial dataset of the same size. Because we go through all of the original dataset and predict all of the blocks, if the model had 0 error, it would produce the original wikitext2 dataset. Training for each generation starts with generation from the original training data. Each experiment is run five times and the results are shown as five separate runs with different randomness seeds. The original model fine-tuned with real wikitext2 data obtains 34 mean perplexity, from the zero-shot baseline of 115, that is, it successfully learns the task. Finally, to be as realistic as possible, we use the best-performing model on the original task, evaluated using the original wikitext2 validation set, as the base model for the subsequent generations, meaning that—in practice—observed model collapse can be even more pronounced. Here we consider two different settings:</p><ul>
                <li>
                  <p>Five epochs, no original training data. Here the model is trained for five epochs starting on the original dataset but with no original data retained for subsequent runs. The overall original task performance is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-07566-y#Fig1">1b</a>. We find that training with generated data allows us to adapt to the underlying task, losing some performance, from 20 to 28 perplexity points.</p>
                </li>
                <li>
                  <p>Ten epochs, 10% of original training data preserved. Here the model is trained for ten epochs on the original dataset and with every new generation of training, a random 10% of the original data points is sampled. The overall original task performance is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-07566-y#Fig1">1c</a>. We find that preservation of the original data allows for better model fine-tuning and leads to only minor degradation of performance.</p>
                </li>
              </ul><p>Both training regimes lead to degraded performance in our models, yet we do find that learning with generated data is possible and models can successfully learn (some of) the underlying task. In particular, from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-07566-y#Fig1">1</a> and their 3D versions in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>, we see that model collapse occurs, as the density of samples with low perplexity begins to accumulate over the generations. This in turn makes it likely that, over the generations, the sampled data will similarly collapse to a delta function.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="The high-level description of the feedback mechanism in the learning process."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: The high-level description of the feedback mechanism in the learning process.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-07566-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-07566-y/MediaObjects/41586_2024_7566_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-07566-y/MediaObjects/41586_2024_7566_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="739"></picture></a></div><p><b>a</b>, Model collapse refers to a degenerative learning process in which models start forgetting improbable events over time, as the model becomes poisoned with its own projection of reality. Here data are assumed to be human-curated and start off clean; then model 0 is trained and data are sampled from it; at step <i>n</i>, data are added to the overall data from step <i>n</i> − 1 and this combination is used to train model <i>n</i>. Data obtained with Monte Carlo sampling should ideally be statistically close to the original, provided that fitting and sampling procedures are perfect. This process depicts what happens in real life with the Internet: model-generated data become pervasive. <b>b</b>,<b>c</b>, Performance of OPT-125m models of different generations evaluated using the original wikitext2 test dataset. Shown on the left are the histograms of perplexities of each individual data training sequence produced by different generations as evaluated by the very first model trained with the real data. Over the generations, models tend to produce samples that the original model trained with real data is more likely to produce. At the same time, a much longer tail appears for later generations. Later generations start producing samples that would never be produced by the original model, that is, they start misperceiving reality based on errors introduced by their ancestors. The same plots are shown in 3D in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>. On the right, average perplexity and its standard deviation are shown for each independent run. The <i>x</i> axis refers to the generation of the model. ‘Real’ refers to the ‘model 0’ trained on the original wikitext2 dataset; model 1 was trained on the data produced by model 0, model 2 was trained on data produced by model 1 and so on, with all generated datasets equal in size. We find that models trained on generated data are able to learn some of the original task, but with errors, as seen from the increase in perplexity.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41586-024-07566-y/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>It is important to note here that the observed behaviour is in line with the general intuition established in the section ‘Theoretical intuition’. To be precise, in all experiments, generational learning is only performed on a finite (usually small) number of generations, whereas claims of the section ‘Theoretical intuition’ are mostly presented in the limit of generations going to infinity. However, as seen from experiments on VAEs and GMMs in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>, convergence to delta functions and specific rates of such convergence are highly related to the specifics of the problem considered, and complete collapse may or may not occur, even after a small number of steps. This is further illustrated theoretically in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>, in which potentially notable divergence from the original model can occur even after a few generations.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-024-07566-y#Fig1">1b,c</a> on the left shows histograms of individual data-point perplexities generated by the models of different generations as is evaluated by the first model developed with real wikitext2 training data. Here, over the generations, models tend to produce more sequences that the original model would produce with the higher likelihood. The observed effect is similar to that described for VAEs and GMMs in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>, in which—over the generations—models started to produce samples that would be produced with higher probabilities by the original model. At the same time, we discover that generated data have much longer tails, suggesting that some of the data would never be produced by the original model—these are the errors that accumulate because of the learning with generational data.</p>
                <h3 id="FPar3">Example 1</h3>
                <p>Example of text outputs of an OPT-125m model affected by model collapse—models degrade over generations, for which each new generation is trained on data produced by the previous generation.</p><ul>
                  <li>
                    <p><b>Input:</b> some started before 1360 — was typically accomplished by a master mason and a small team of itinerant masons, supplemented by local parish labourers, according to Poyntz Wright. But other authors reject this model, suggesting instead that leading architects designed the parish church towers based on early examples of Perpendicular.</p>
                  </li>
                  <li>
                    <p><b>Outputs:</b></p>
                  </li>
                  <li>
                    <p><b>Gen 0:</b> Revival architecture such as St. John’s Cathedral in London. The earliest surviving example of Perpendicular Revival architecture is found in the 18th @-@ century Church of Our Lady of Guernsey, which dates from the late 19th century. There are two types of perpendicular churches : those.</p>
                  </li>
                  <li>
                    <p><b>Gen 1:</b> architecture such as St. Peter’s Basilica in Rome or St. Peter’s Basilica in Buenos Aires. There is no evidence that any of these buildings were built during the reign of Pope Innocent III, but it is possible that they may have been built during the reign of his successor, Pope Innocent.</p>
                  </li>
                  <li>
                    <p><b>Gen 5:</b> ism, which had been translated into more than 100 languages including English, French, German, Italian, Spanish, Portuguese, Dutch, Swedish, Norwegian, Polish, Hungarian, Slovak, Lithuanian, Estonian, Finnish, Romanian, Bulgarian, Turkish, Croatian, Serbian, Ukrainian, Russian, Kazakh, Kyrgyz.</p>
                  </li>
                  <li>
                    <p><b>Gen 9:</b> architecture. In addition to being home to some of the world’s largest populations of black @-@ tailed jackrabbits, white @-@ tailed jackrabbits, blue @-@ tailed jackrabbits, red @-@ tailed jackrabbits, yellow @-.</p>
                  </li>
                </ul>
              <h3 id="Sec7">Ablation: Repetitions</h3><p>We find that data generated by language models in our experiments end up containing a large number of repeating phrases, as in Example 1. The repeating problem has been observed in nearly all text-generation models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C. &amp; Socher, R. CTRL: a conditional transformer language model for controllable generation. Preprint at 
                  https://arxiv.org/abs/1909.05858
                  
                 (2019)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR17" id="ref-link-section-d32410486e2076">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Shumailov, I. et al. in Proc. 2021 IEEE European Symposium on Security and Privacy (EuroS&amp;P) 212–231 (IEEE, 2021)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR18" id="ref-link-section-d32410486e2079">18</a></sup> and, to rule this out as the cause of model collapse, we further provide numerical experiments when models are explicitly encouraged to produce non-repeating sequences with a repeating penalty of 2.0. We find that this causes the models to produce lower score continuations to avoid using repeats, which—as a result—causes the consequent models to perform even worse. Model perplexities shift across the generations towards more probable token sequences, as measured using the model trained on the original real data distribution. Further illustrations are provided in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-024-07566-y#MOESM1">Supplementary Materials</a>. In particular, enforcing this for the LLM experiments causes the perplexity to double compared with the original. Models remain as susceptible to model collapse, if not more.</p><p>The described process demonstrates that fine-tuning of language models does not curb the effects of model collapse and models that are being fine-tuned are also vulnerable. We find that, over the generations, models tend to produce more probable sequences from the original data and start introducing their own improbable sequences, that is, errors.</p></div></div><div id="Sec8-section" data-title="Discussion"><h2 id="Sec8">Discussion</h2><div id="Sec8-content"><p>We now discuss the implications of model collapse on the underlying learning dynamics of LLMs. Long-term poisoning attacks on language models are not new. For example, we saw the creation of click, content and troll farms, a form of human ‘language models’, whose job is to misguide social networks and search algorithms. The negative effect that these poisoning attacks had on search results led to changes in search algorithms. For example, Google downgraded farmed articles<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Google. Finding more high-quality sites in search. Google 
                  https://googleblog.blogspot.com/2011/02/finding-more-high-quality-sites-in.html
                  
                 (2011)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR19" id="ref-link-section-d32410486e2098">19</a></sup>, putting more emphasis on content produced by trustworthy sources, such as education domains, whereas DuckDuckGo removed them altogether<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Mims, C. The search engine backlash against ‘content mills’. MIT Technology Review 
                  https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/
                  
                 (2010)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR20" id="ref-link-section-d32410486e2102">20</a></sup>. What is different with the arrival of LLMs is the scale at which such poisoning can happen once it is automated. Preserving the ability of LLMs to model low-probability events is essential to the fairness of their predictions: such events are often relevant to marginalized groups. Low-probability events are also vital to understand complex systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Taleb, N. N. Black swans and the domains of statistics. Am. Stat. 61, 198–200 (2007)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR21" id="ref-link-section-d32410486e2106">21</a></sup>.</p><p>Our evaluation suggests a ‘first mover advantage’ when it comes to training models such as LLMs. In our work, we demonstrate that training on samples from another generative model can induce a distribution shift, which—over time—causes model collapse. This in turn causes the model to mis-perceive the underlying learning task. To sustain learning over a long period of time, we need to make sure that access to the original data source is preserved and that further data not generated by LLMs remain available over time. The need to distinguish data generated by LLMs from other data raises questions about the provenance of content that is crawled from the Internet: it is unclear how content generated by LLMs can be tracked at scale. One option is community-wide coordination to ensure that different parties involved in LLM creation and deployment share the information needed to resolve questions of provenance. Otherwise, it may become increasingly difficult to train newer versions of LLMs without access to data that were crawled from the Internet before the mass adoption of the technology or direct access to data generated by humans at scale.</p></div></div>
                </div><div>
                <div id="data-availability-section" data-title="Data availability"><h2 id="data-availability">Data availability</h2><p>Data generation code for GMM experiments is available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Shumailov, I. &amp; Shumaylov, Z. Public code for Model Collapse (0.1). Zenodo 
                  https://doi.org/10.5281/zenodo.10866595
                  
                 (2024)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR13" id="ref-link-section-d32410486e2195">13</a></sup>. Data used for VAE experiments are available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="LeCun, Y., Cortes, C. &amp; Burges, C. J. C. The MNIST database of handwritten digits. 
                  http://yann.lecun.com/exdb/mnist/
                  
                 (1998)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR22" id="ref-link-section-d32410486e2199">22</a></sup>. Data used for LLM experiments are available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Merity, S., Xiong, C., Bradbury, J. &amp; Socher, R. in Proc. 5th International Conference on Learning Representations (ICLR, 2017)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR16" id="ref-link-section-d32410486e2203">16</a></sup>.</p></div><div id="code-availability-section" data-title="Code availability"><h2 id="code-availability">Code availability</h2><p>Code for all experiments is publically available in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Shumailov, I. &amp; Shumaylov, Z. Public code for Model Collapse (0.1). Zenodo 
                  https://doi.org/10.5281/zenodo.10866595
                  
                 (2024)." href="https://www.nature.com/articles/s41586-024-07566-y#ref-CR13" id="ref-link-section-d32410486e2215">13</a></sup>.</p></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Radford, A. et al. Language models are unsupervised multitask learners. <i>OpenAI blog</i> <b>1</b>, 9 (2019).</p><p><a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20models%20are%20unsupervised%20multitask%20learners&amp;journal=OpenAI%20blog&amp;volume=1&amp;publication_year=2019&amp;author=Radford%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Brown, T. et al. Language models are few-shot learners. <i>Adv. Neural Inf. Process. Syst.</i> <b>33</b>, 1877–1901 (2020).</p><p><a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20models%20are%20few-shot%20learners&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=33&amp;pages=1877-1901&amp;publication_year=2020&amp;author=Brown%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">OpenAI. GPT-4 Technical Report. <a href="https://cdn.openai.com/papers/gpt-4.pdf" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/papers/gpt-4.pdf</a> (2023).</p></li><li data-counter="4."><p id="ref-CR4">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. in <i>Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i> (eds Burstein, J., Doran, C. &amp; Solorio, T.) 4171–4186 (Association for Computational Linguistics, 2019).</p></li><li data-counter="5."><p id="ref-CR5">Liu, Y. et al. RoBERTa: a Robustly Optimized BERT Pretraining Approach. Preprint at <a href="https://arxiv.org/abs/1907.11692" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a> (2019).</p></li><li data-counter="6."><p id="ref-CR6">Zhang, S. et al. Opt: open pre-trained transformer language models. Preprint at <a href="https://arxiv.org/abs/2205.01068" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2205.01068">https://arxiv.org/abs/2205.01068</a> (2022).</p></li><li data-counter="7."><p id="ref-CR7">Aljundi, R., Kelchtermans, K. &amp; Tuytelaars, T. Task-free continual learning. in: <i>Proc. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i> 11254–11263 (IEEE, 2019).</p></li><li data-counter="8."><p id="ref-CR8">Carlini, N. &amp; Terzis, A. in <i>Proc. Tenth International Conference on Learning Representations</i> (ICLR, 2022).</p></li><li data-counter="9."><p id="ref-CR9">Carlini, N. et al. in <i>Proc. 2024 IEEE Symposium on Security and Privacy (SP)</i> 179 (IEEE, 2024).</p></li><li data-counter="10."><p id="ref-CR10">Mousavi-Hosseini, A., Park, S., Girotti, M., Mitliagkas, I. &amp; Erdogdu, M. A. in <i>Proc. Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="11."><p id="ref-CR11">Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. &amp; Srebro, N. The implicit bias of gradient descent on separable data. <i>J. Mach. Learn. Res.</i> <b>19</b>, 1–57 (2018).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3899772" aria-label="MathSciNet reference 11">MathSciNet</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=19&amp;pages=1-57&amp;publication_year=2018&amp;author=Soudry%2CD&amp;author=Hoffer%2CE&amp;author=Nacson%2CMS&amp;author=Gunasekar%2CS&amp;author=Srebro%2CN">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="12."><p id="ref-CR12">Gu, Y., Dong, L., Wei, F. &amp; Huang, M. in <i>Proc. Twelfth International Conference on Learning Representations</i> (ICLR, 2024).</p></li><li data-counter="13."><p id="ref-CR13">Shumailov, I. &amp; Shumaylov, Z. Public code for Model Collapse (0.1). <i>Zenodo</i> <a href="https://doi.org/10.5281/zenodo.10866595" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.5281/zenodo.10866595">https://doi.org/10.5281/zenodo.10866595</a> (2024).</p></li><li data-counter="14."><p id="ref-CR14">Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at <a href="https://arxiv.org/abs/2108.07258" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a> (2022).</p></li><li data-counter="15."><p id="ref-CR15">Strubell, E., Ganesh, A. &amp; McCallum, A. in <i>Proc. 57th Annual Meeting of the Association for Computational Linguistics</i> (eds Korhonen, A., Traum, D. &amp; Màrquez, L.) 3645–3650 (Association for Computational Linguistics, 2019).</p></li><li data-counter="16."><p id="ref-CR16">Merity, S., Xiong, C., Bradbury, J. &amp; Socher, R. in <i>Proc. 5th International Conference on Learning Representations</i> (ICLR, 2017).</p></li><li data-counter="17."><p id="ref-CR17">Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C. &amp; Socher, R. CTRL: a conditional transformer language model for controllable generation. Preprint at <a href="https://arxiv.org/abs/1909.05858" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1909.05858">https://arxiv.org/abs/1909.05858</a> (2019).</p></li><li data-counter="18."><p id="ref-CR18">Shumailov, I. et al. in <i>Proc. 2021 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</i> 212–231 (IEEE, 2021).</p></li><li data-counter="19."><p id="ref-CR19">Google. Finding more high-quality sites in search. <i>Google</i> <a href="https://googleblog.blogspot.com/2011/02/finding-more-high-quality-sites-in.html" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://googleblog.blogspot.com/2011/02/finding-more-high-quality-sites-in.html">https://googleblog.blogspot.com/2011/02/finding-more-high-quality-sites-in.html</a> (2011).</p></li><li data-counter="20."><p id="ref-CR20">Mims, C. The search engine backlash against ‘content mills’. <i>MIT Technology Review</i> <a href="https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/">https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/</a> (2010).</p></li><li data-counter="21."><p id="ref-CR21">Taleb, N. N. Black swans and the domains of statistics. <i>Am. Stat.</i> <b>61</b>, 198–200 (2007).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1198/000313007X219996" data-track-item_id="10.1198/000313007X219996" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1198%2F000313007X219996" aria-label="Article reference 21" data-doi="10.1198/000313007X219996">Article</a>&nbsp;
    <a data-track="click||click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2393721" aria-label="MathSciNet reference 21">MathSciNet</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Black%20swans%20and%20the%20domains%20of%20statistics&amp;journal=Am.%20Stat.&amp;doi=10.1198%2F000313007X219996&amp;volume=61&amp;pages=198-200&amp;publication_year=2007&amp;author=Taleb%2CNN">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="22."><p id="ref-CR22">LeCun, Y., Cortes, C. &amp; Burges, C. J. C. The MNIST database of handwritten digits. <a href="http://yann.lecun.com/exdb/mnist/" data-track="click||click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> (1998).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-07566-y?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>This paper is dedicated to the memory of Professor Ross J. Anderson, our colleague and friend, who contributed much to this and other works we have produced over the years. We thank A. Thudi, D. Glukhov, P. Zaika, and D. Barak for useful discussions and feedback.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Ilia Shumailov, Zakhar Shumaylov</p></li><li id="na2"><p>Deceased: Ross Anderson</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>OATML, Department of Computer Science, University of Oxford, Oxford, UK</p><p>Ilia Shumailov&nbsp;&amp;&nbsp;Yarin Gal</p></li><li id="Aff2"><p>Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, UK</p><p>Zakhar Shumaylov</p></li><li id="Aff3"><p>Department of Electrical and Electronic Engineering, Imperial College London, London, UK</p><p>Yiren Zhao</p></li><li id="Aff4"><p>University of Toronto, Toronto, Ontario, Canada</p><p>Nicolas Papernot</p></li><li id="Aff5"><p>Vector Institute, Toronto, Ontario, Canada</p><p>Nicolas Papernot</p></li><li id="Aff6"><p>Department of Computer Science and Technology, University of Cambridge, Cambridge, UK</p><p>Ross Anderson</p></li><li id="Aff7"><p>School of Informatics, University of Edinburgh, Edinburgh, UK</p><p>Ross Anderson</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Ilia-Shumailov-Aff1"><span>Ilia Shumailov</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ilia%20Shumailov" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ilia%20Shumailov%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Zakhar-Shumaylov-Aff2"><span>Zakhar Shumaylov</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zakhar%20Shumaylov" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zakhar%20Shumaylov%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Yiren-Zhao-Aff3"><span>Yiren Zhao</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yiren%20Zhao" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yiren%20Zhao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Nicolas-Papernot-Aff4-Aff5"><span>Nicolas Papernot</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nicolas%20Papernot" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nicolas%20Papernot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Ross-Anderson-Aff6-Aff7"><span>Ross Anderson</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ross%20Anderson" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ross%20Anderson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Yarin-Gal-Aff1"><span>Yarin Gal</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yarin%20Gal" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yarin%20Gal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>I.S. and Z.S. proposed and developed the idea, led the research and mathematical modelling and developed the GMM and VAE experiments. I.S. and Y.Z. developed the language-model experiments. N.P., Y.G. and R.A. supervised and guided the project. All authors contributed to writing of the manuscript. Y.G. is supported by a Turing AI Fellowship financed by the UK government’s Office for Artificial Intelligence, through UK Research and Innovation (grant reference EP/V030302/1) and delivered by the Alan Turing Institute.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:ilia.shumailov@chch.ox.ac.uk">Ilia Shumailov</a>, <a id="corresp-c2" href="mailto:zs334@cam.ac.uk">Zakhar Shumaylov</a> or <a id="corresp-c3" href="mailto:yarin@cs.ox.ac.uk">Yarin Gal</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar5">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div><div id="peer-review-section" data-title="Peer review"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar4">Peer review information</h3>
                <p><i>Nature</i> thanks the anonymous reviewers for their contribution to the peer review of this work.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div><div id="Sec10-section" data-title="Supplementary information"><h2 id="Sec10">Supplementary information</h2></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=AI%20models%20collapse%20when%20trained%20on%20recursively%20generated%20data&amp;author=Ilia%20Shumailov%20et%20al&amp;contentID=10.1038%2Fs41586-024-07566-y&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2024-07-24&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1038/s41586-024-07566-y" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-024-07566-y" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Shumailov, I., Shumaylov, Z., Zhao, Y. <i>et al.</i> AI models collapse when trained on recursively generated data.
                    <i>Nature</i> <b>631</b>, 755–759 (2024). https://doi.org/10.1038/s41586-024-07566-y</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-07566-y?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2023-10-20">20 October 2023</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2024-05-14">14 May 2024</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2024-07-24">24 July 2024</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2024-07-25">25 July 2024</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-024-07566-y</span></p></li></ul></div></div></div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Large Enough – Mistral AI (477 pts)]]></title>
            <link>https://mistral.ai/news/mistral-large-2407/</link>
            <guid>41058107</guid>
            <pubDate>Wed, 24 Jul 2024 15:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/mistral-large-2407/">https://mistral.ai/news/mistral-large-2407/</a>, See on <a href="https://news.ycombinator.com/item?id=41058107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-mini.png" alt="Detailed benchmarks" width="5%"></p><p>This latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications.</p><h3 id="mistral-large-2">Mistral Large 2</h3><p>Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash.</p><p>Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node.
We are releasing Mistral Large 2 under the <a href="https://mistral.ai/licenses/MRL-0.1.md">Mistral Research License</a>, that allows usage and modification for research and non-commercial usages.</p><h5 id="general-performance">General performance</h5><p>Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models.</p><h5 id="code--reasoning">Code &amp; Reasoning</h5><p>Following our experience with <a href="https://mistral.ai/news/codestral/">Codestral 22B</a> and <a href="https://mistral.ai/news/codestral-mamba/">Codestral Mamba</a>, we trained Mistral Large 2 on a very large proportion of code. Mistral Large 2 vastly outperforms the previous Mistral Large, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-performance.png" alt="Detailed benchmarks" width="100%"></p><p>A significant effort was also devoted to enhancing the model’s reasoning capabilities. One of the key focus areas during training was to minimize the model’s tendency to “hallucinate” or generate plausible-sounding but factually incorrect or irrelevant information. This was achieved by fine-tuning the model to be more cautious and discerning in its responses, ensuring that it provides reliable and accurate outputs.</p><p>Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills:</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-code-generation.png" alt="Detailed benchmarks" width="100%"></p><p>Performance accuracy on code generation benchmarks (all models were benchmarked through the same evaluation pipeline)</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-multiple.png" alt="Detailed benchmarks" width="100%"></p><p>Performance accuracy on MultiPL-E (all models were benchmarked through the same evaluation pipeline, except for the "paper" row)</p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-code-generation-2.png" alt="Detailed benchmarks" width="100%"></p><p>Performance accuracy on GSM8K (8-shot) and MATH (0-shot, no CoT) generation benchmarks (all models were benchmarked through the same evaluation pipeline)</p></div><h5 id="instruction-following--alignment">Instruction following &amp; Alignment</h5><p>We drastically improved the instruction-following and conversational capabilities of Mistral Large 2. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. Below we report the performance on MT-Bench, Wild Bench, and Arena Hard benchmarks:</p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-instruction.png" alt="Detailed benchmarks" width="100%"></p><p>Performance on general alignment benchmarks (all models were benchmarked through the same evalutation pipeline)</p></div><p>On some benchmarks, generating lengthy responses tends to improve the scores. However, in many business applications, conciseness is paramount – short model generations facilitate quicker interactions and are more cost-effective for inference. This is why we spent a lot of effort to ensure that generations remain succinct and to the point whenever possible. The graph below reports the average length of generations of different models on questions from the MT Bench benchmark:</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-mtbench.png" alt="MT Bench benchmarks" width="100%"></p><h5 id="language-diversity">Language diversity</h5><p>A large fraction of business use cases today involve working with multilingual documents. While the majority of models are English-centric, the new Mistral Large 2 was trained on a large proportion of multilingual data. In particular, it excels in English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi. Below are the performance results of Mistral Large 2 on the multilingual MMLU benchmark, compared to the previous Mistral Large, Llama 3.1 models, and to Cohere’s Command R+.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-mmlu.png" alt="Detailed benchmarks" width="70%"></p><div><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-language-diversity.png" alt="Detailed benchmarks" width="100%"></p><p>Performance on Multilingual MMLU (measured on the base pretrained model)</p></div><h5 id="tool-use--function-calling">Tool Use &amp; Function Calling</h5><p>Mistral Large 2 is equipped with enhanced function calling and retrieval skills and has undergone training to proficiently execute both parallel and sequential function calls, enabling it to serve as the power engine of complex business applications.</p><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-tool-use.png" alt="Detailed benchmarks" width="100%"></p><h5 id="try-mistral-large-2-on-la-plateforme">Try Mistral Large 2 on la Plateforme</h5><p>You can use Mistral Large 2 today via <a href="https://console.mistral.ai/">la Plateforme</a> under the name <code>mistral-large-2407</code>, and test it on le Chat. It is available under the version 24.07 (a YY.MM versioning system that we are applying to all our models), and the API name <code>mistral-large-2407</code>. Weights for the instruct model are <a href="https://models.mistralcdn.com/mistral-large-2407/mistral-large-instruct-2407.tar">available</a> and are also hosted on <a href="https://huggingface.co/mistralai/Mistral-Large-Instruct-2407">HuggingFace</a>.</p><p>we are consolidating the offering on la Plateforme around two general purpose models, Mistral Nemo and Mistral Large, and two specialist models, Codestral and Embed. As we progressively deprecate older models on la Plateforme, all Apache models (Mistral 7B, Mixtral 8x7B and 8x22B, Codestral Mamba, Mathstral) remain available for deployment and fine-tuning using our SDK mistral-inference and mistral-finetune.</p><p>Starting today, we are extending fine-tuning capabilities on la Plateforme: those are now available for Mistral Large, Mistral Nemo and Codestral.</p><h5 id="access-mistral-models-through-cloud-service-providers">Access Mistral models through cloud service providers</h5><p>We are proud to partner with leading cloud service providers to bring the new Mistral Large 2 to a global audience. In particular, today we are expanding our partnership with Google Cloud Platform to bring Mistral AI’s models on <a href="https://cloud.google.com/blog/products/ai-machine-learning/codestral-and-mistral-large-v2-on-vertex-ai?e=48754805&amp;hl=en">Vertex AI</a> via a Managed API. Mistral AI’s best models are now available on Vertex AI, in addition to Azure AI Studio, Amazon Bedrock and IBM watsonx.ai.</p><h5 id="availability-timeline-of-mistral-ai-models">Availability timeline of Mistral AI models</h5><p><img src="https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-availability.png" alt="Detailed benchmarks" width="100%"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crash course in deep learning for computer graphics (130 pts)]]></title>
            <link>https://gpuopen.com/learn/deep_learning_crash_course/</link>
            <guid>41057289</guid>
            <pubDate>Wed, 24 Jul 2024 14:08:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gpuopen.com/learn/deep_learning_crash_course/">https://gpuopen.com/learn/deep_learning_crash_course/</a>, See on <a href="https://news.ycombinator.com/item?id=41057289">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="crash-course-in-deep-learning-for-computer-graphics" role="main" data-id="3cfd864" data-element_type="widget" data-widget_type="theme-post-content.default">
<section id="introduction">
<h2 id="1.-introduction">1. Introduction<a href="#introduction" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>As I recently went through a journey of learning how to make use of deep learning (in the context of computer graphics), I&nbsp;thought it would be good to write down some notes to help others get up to speed quickly. The goal of this article is to make the reader familiar with terms and concepts used in deep learning and to implement a basic deep learning algorithm. This should make it easier to study and understand other deep learning resources. This article comes with the source code and a sample application written in HLSL/DirectX 12 available at <a href="https://github.com/boksajak/Dx12NN" target="_blank" rel="noopener">https://github.com/boksajak/Dx12NN</a>.</p>
</section>
<section id="neural-network">
<h2 id="2.-neural-network">2. Neural Network<a href="#neural-network" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>Deep learning algorithms are powered by artificial neural networks. These are collections of interconnected neurons (also called <strong>units</strong>), usually organized into <strong>layers</strong>. When a neural network has large number of layers, we say that the network is deep, giving us a name <strong>deep learning</strong>. Each neuron has its inputs connected to other neurons via weighted connections, performs a simple computation over them, and passes its output to other neurons.</p>
<p>The type of neural network that we’re going to use in this article is called <strong>multilayer perceptron (MLP)</strong>. It is relatively simple, but also powerful, and is widely used in practice (e.g., for neural radiance cache (NRC) and neural radiance fields (NeRFs)).</p>
<p>The MLP is constructed as follows:</p>
<ul>
<li>
<p>Neurons are organized into layers, where first layer is called <strong>input layer</strong>, last layer is <strong>output layer</strong> and between them are <strong>hidden layers.</strong></p>
</li>
<li>
<p>Each neuron has its inputs connected to outputs of all neurons in the previous layer. Therefore, we say that MLPs are <strong>fully connected networks</strong>, see the Figure 1 for an example.</p>
</li>
<li>
<p>This network architecture forms a directed acyclic graph, meaning that information only flows in one way, starting in the input layer and propagating through hidden layers to the output layer. We say that such network is a <strong>feedforward network</strong>.</p>
<ul>
<li>
<p>Note: Other network types, where information can also flow backwards (via <strong>feedback connections</strong>) are called <strong>recurrent networks</strong>. Some networks also have memory cells to store values from previous evaluations of the network.</p>
</li>
</ul>
</li>
</ul>
<p>Number of neurons and layers in the network determines how powerful the network is. Larger networks have the ability to learn more complex functions (we say that they have a larger <strong>capacity</strong>) but are typically more difficult to train and slower to evaluate. When designing a deep learning algorithm, we need to find a network size which is just right for the task. E.g., the NeRF paper uses MLP with 9 hidden layers consisting of 256 neurons each, and NRC uses 5 hidden layers with 64 neurons.</p>
<p>Note that these networks are small enough that they can be implemented in compute shaders and executed per pixel. In practice, some clever optimizations are necessary but it’s not impossible to have a deep learning algorithm running in real-time on current GPUs. It is also common to run neural networks using types with reduced precision, like FP16 or even smaller.</p>
<p><img width="1166" height="695" decoding="async" alt="invert" src="https://gpuopen.com/docs_images/deep_learning_crash_course/deep_learning_crash_course-html-_images-Multi-layer_perceptron_neural_network.jpg" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='1166'%20height='695'%20viewBox='0%200%201166%20695'%3E%3C/svg%3E" data-src="/docs_images/deep_learning_crash_course/deep_learning_crash_course-html-_images-Multi-layer_perceptron_neural_network.jpg"></p>
<p><em>Figure 1 Architecture of a multi-layer perceptron neural network with 2 neurons in the input layer, 3 neurons per hidden layer and 1 neuron in the output layer.</em></p>
<section id="the-neuron">
<h3 id="2.1-the-neuron">2.1 The Neuron<a href="#the-neuron" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>All the computation that neural network does happens in the neurons. Output value of the neuron (also called its <strong>activation</strong>) is calculated as follows:</p>
<ol>
<li>
<p>First, we sum up activations of all neurons from the previous layer connected to this neuron, weighted by the strength of the connection (this strength is also called the <strong>weight</strong>)</p>
</li>
<li>
<p>We add <strong>bias</strong> of the evaluated neuron to the sum. Bias is a per-neuron value which helps to represent non-linear relationships between network inputs and outputs. Without the bias, output of the network for zero input could only be a zero.</p>
</li>
<li>
<p>We apply the <strong>activation function</strong> to this sum to produce final neuron activation. This must be a non-linear function, which introduces another non-linearity between inputs and outputs. Usually, we use a simple function like (max(0,x)). Without the activation function, outputs of the network could only be a linear combination of the inputs.</p>
</li>
</ol>
<p>More formally, we can write neuron evaluation as:</p>
<p><span data-katex-display="true">O_{i} = \sigma\left( \left( \sum_{j}^{M}{O_{j}*w_{ij}} \right) + b_{i} \right)</span></p>
<p>where <span data-katex-display="false">O_{i}</span> is the activation of the evaluated neuron, <span data-katex-display="false">\sigma</span> is the activation function, <span data-katex-display="false">M</span> is the set of input neurons connected to the evaluated neuron, <span data-katex-display="false">O_{j}</span> is the activation of the input neuron <span data-katex-display="false">j</span> from previous layer, <span data-katex-display="false">w_{ij}</span> is the weight of the connection between neurons <span data-katex-display="false">i</span> and <span data-katex-display="false">j</span> and <span data-katex-display="false">b_{i}</span> is the bias value of the evaluated neuron.</p>
<p>This means, that the output of the network for specific input is given by <strong>weights and biases</strong> and we need to set them accordingly to produce desired results. The process of adjusting weights and biases to make the network do what we want is called <strong>training</strong>.</p>
</section>
<section id="how-to-use-an-mlp">
<h3 id="2.2-how-to-use-an-mlp">2.2 How To Use an MLP<a href="#how-to-use-an-mlp" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>Neural networks based on MLPs are usually used for one of two main tasks: classification and regression:</p>
<ul>
<li>
<p><strong>Classification:</strong> Categorizes the input into one (or more) predefined categories. These neural networks have one output neuron for each category and assign a probability of input belonging to each category as its output. E.g., we can have an input image of a hand-written digit and train the MLP to assign probability of the digit belonging into categories representing digits from 0 to 9. This exercise is a common “hello world” example in deep learning and there is a freely available set of images of hand-written digits called <a href="https://git-disl.github.io/GTDLBench/datasets/mnist_datasets/" target="_blank" rel="noopener">MNIST</a> that can be used for it.</p>
</li>
<li>
<p><strong>Regression:</strong> For given input, the regression calculates a continuous numerical value on the output (also called a <strong>prediction</strong>). The goal is to train the network to perform a desired mapping between inputs and output values. E.g., the NRC algorithm trains the network to map inputs like surface position and normal to radiance values.</p>
</li>
</ul>
<p>In our sample application we will train the network to do regression, specifically it will learn to represent a 2D texture. We will provide UV coordinates of the texture as inputs, and we’ll expect RGB value of the corresponding texel on the output. We want it to learn the mapping:</p>
<p><span data-katex-display="true">\left( u,\ v \right) \rightarrow (R,\ G,\ B)</span></p>
<p>For this example, our network will have 2 neurons in the input layer, corresponding to the <span data-katex-display="false">u</span> and <span data-katex-display="false">v</span> coordinates in the texture. On the output, we will have 3 neurons corresponding to the RGB values of the texel. In practice, it is common to <strong>encode the input</strong> into different representation. A naïve encoding which simply assigns inputs to input neurons as they are is called <strong>identity,</strong> and while it works, some clever encoding schemes usually perform much better. We’ll talk more about input encodings in section 4. Note that the input layer doesn’t perform any computation – instead of calculating activation of the neurons in the input layer, we simply assign input values as their activations.</p>
<p>Before we can use the MLP, it must be trained to perform our desired mapping well. Training is relatively complex and will be described in section 3, so let’s now assume that we have trained the network, obtained the correct weights and biases and we want to calculate the network output (prediction) for given input – this process is also called <strong>inference</strong> and because the information flows forward through the network, it’s sometimes also called the <strong>forward pass</strong>.</p>
</section>
<section id="inference-implementation">
<h3 id="2.3-inference-implementation">2.3 Inference Implementation<a href="#inference-implementation" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>With the architecture of the MLP in mind, let’s now implement the inference. Before we start, it is useful to make it clear how we index data of the neural network in the arrays of weights, biases, activations etc. We need to store activations and biases <em>per neuron</em>, and the weights <em>per connection</em> between neurons. It is easy to make a mistake in indexing when working with graphs (our neural network is a graph), so let’s define a few rules for our indexing scheme:</p>
<ul>
<li>
<p>We will index layers starting from 0: input layer has index 0, and output layer has index <code><span>LAYER_COUNT</span> <span>-</span> <span>1</span></code>. Because the input layer doesn’t do any computation, it might be possible to skip it and start indexing from the first hidden layer, but we want to keep things clear and simple.</p>
</li>
<li>
<p>Connections between neurons will “belong” to the layer they are leading to. This means that layer 0 (input layer) doesn’t have any connections, and layer index <code><span>LAYER_COUNT</span> <span>-</span> <span>1</span></code> has connections from last hidden layer to the output layer.</p>
</li>
<li>
<p>Neurons in each layer are indexed from 0 to <code><span>NUM_NEURONS_PER_LAYER</span></code></p>
</li>
</ul>
<p>With this in mind, let’s define some helper functions to access data in global arrays:</p>
<div>

<pre><code><span>uint</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>uint</span><span> </span><span>layer</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuron</span><span>)</span>
<span>{</span>
<span>  </span><span>return</span><span> </span><span>neuronDataBaseOffsets</span><span>[</span><span>layer</span><span>]</span><span> </span><span>+</span><span> </span><span>neuron</span><span>;</span>
<span>}</span>

<span>uint</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>uint</span><span> </span><span>layer</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuronFrom</span><span>,</span><span> </span><span>uint</span><span> </span><span>neuronTo</span><span>)</span>
<span>{</span>
<span>  </span><span>return</span><span> </span><span>connectionDataBaseOffsets</span><span>[</span><span>layer</span><span>]</span><span> </span><span>+</span><span> </span><span>(</span><span>neuronTo</span><span> </span><span>*</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>])</span><span> </span><span>+</span><span> </span><span>neuronFrom</span><span>;</span>
<span>}</span>
</code></pre>
</div>
<p>Base offsets used in these functions are pre-calculated for each layer based on number of layers and number of neurons per layer in the network. Details can be found in the source code accompanying this article in the function createComputePasses.</p>
<p>With these in place, we can now implement a forward pass which:</p>
<ol>
<li>
<p>Encodes input into <em>activations</em> array.</p>
</li>
<li>
<p>Iterates through all the layers where computation happens (all except the input layer).</p>
<p>a.  Evaluates activation for each neuron in the layer.</p>
</li>
<li>
<p>Reads output from the activations array and returns it.</p>
</li>
</ol>
<div>

<pre><code><span>float</span><span> </span><span>activations</span><span>[</span><span>LAYER_COUNT</span><span> </span><span>*</span><span> </span><span>MAX_NEURONS_PER_LAYER</span><span>];</span>

<span>// Identity encoding passes input as it is</span>
<span>activations</span><span>[</span><span>0</span><span>]</span><span> </span><span>=</span><span> </span><span>input</span><span>.</span><span>x</span><span>;</span>
<span>activations</span><span>[</span><span>1</span><span>]</span><span> </span><span>=</span><span> </span><span>input</span><span>.</span><span>y</span><span>;</span>

<span>// Calculate activations for every layer, going forward through the MLP network</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>layer</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span> </span><span>layer</span><span> </span><span>&lt;</span><span> </span><span>LAYER_COUNT</span><span>;</span><span> </span><span>layer</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronCountCurrentLayer</span><span> </span><span>=</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span>];</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronCountPreviousLayer</span><span> </span><span>=</span><span> </span><span>neuronsPerLayer</span><span>[</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>];</span>

<span>  </span><span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>neuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>neuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountCurrentLayer</span><span>;</span><span> </span><span>neuron</span><span>++</span><span>)</span>
<span>  </span><span>{</span>
<span>    </span><span>const</span><span> </span><span>uint</span><span> </span><span>neuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span>,</span><span> </span><span>neuron</span><span>);</span>

<span>    </span><span>// Evaluate neuron activation</span>
<span>    </span><span>float</span><span> </span><span>neuronValue</span><span> </span><span>=</span><span> </span><span>nnBiases</span><span>[</span><span>neuronDataIndex</span><span>];</span>

<span>    </span><span>// Accumulate weighted contribution from all neurons connected to this neuron in previous layer</span>
<span>    </span><span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>previousNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>previousNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountPreviousLayer</span><span>;</span><span> </span><span>previousNeuron</span><span>++</span><span>)</span>
<span>    </span><span>{</span>
<span>      </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightDataIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>layer</span><span>,</span><span> </span><span>previousNeuron</span><span>,</span><span> </span><span>neuron</span><span>);</span>
<span>      </span><span>const</span><span> </span><span>uint</span><span> </span><span>previousNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>previousNeuron</span><span>);</span>

<span>      </span><span>neuronValue</span><span> </span><span>+=</span><span> </span><span>nnWeights</span><span>[</span><span>weightDataIndex</span><span>]</span><span> </span><span>*</span><span> </span><span>activations</span><span>[</span><span>previousNeuronDataIndex</span><span>];</span>
<span>    </span><span>}</span>

<span>    </span><span>activations</span><span>[</span><span>neuronDataIndex</span><span>]</span><span> </span><span>=</span><span> </span><span>ACTIVATION_FUNCTION</span><span>(</span><span>neuronValue</span><span>);</span>
<span>  </span><span>}</span>
<span>}</span>

<span>const</span><span> </span><span>uint</span><span> </span><span>outputLayerActivationIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>0</span><span>);</span>
<span>const</span><span> </span><span>float3</span><span> </span><span>result</span><span> </span><span>=</span><span> </span><span>float3</span><span>(</span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>0</span><span>],</span>
<span>                </span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>1</span><span>],</span>
<span>                </span><span>activations</span><span>[</span><span>outputLayerActivationIndex</span><span> </span><span>+</span><span> </span><span>2</span><span>]);</span>
</code></pre>
</div>
<p>Note that our code has allocated an array called <strong>activations</strong> where we store activations of all neurons during the forward pass. But for inference we only need to store activations of 2 layers at any time: the one that we are evaluating and the previous layer. As an optimization, we can allocate two smaller arrays with the size <code><span>NUM_MAX_NEURONS_PER_LAYER</span></code> and ping-pong between them. However, during the training we will need to store activations for all neurons from the forward pass to perform a backpropagation pass.</p>
</section>
<section id="activation-functions">
<h3 id="2.4-activation-functions">2.4 Activation Functions<a href="#activation-functions" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>In the previous code listing, we have used the macro called <code><span>ACTIVATION_FUNCTION</span></code> in the place where we want to evaluate the activation function. Let’s now define it – remember that this must be a non-linear function, but we can pick any function assuming that it is differentiable (it has a derivative). The derivative will be needed for training.</p>
<p>Some of the functions commonly used are ReLU (rectified linear unit), leaky ReLU and sigmoid:</p>
<p><span data-katex-display="true">\sigma_{ReLU}(x) = max(0,x)</span></p>
<p><span data-katex-display="true">\sigma_{LeakyReLU}(x) = \left( \begin{matrix} 0.01x, x &lt; 0 \\ x, x \geq 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{Sigmoid} = \frac{1}{1 + e^{-x}}</span></p>
<p>Even though ReLU and leaky ReLU are very simple, they work well and are widely used, e.g., by both NeRF and NRC papers. The 0.01 value (slope) in leaky ReLU implementation is a variable and you can experiment with different values. Usually, the whole network uses the same activation function, but it is not uncommon for output layer to have a different function than hidden layers.</p>
<p>Let’s now look at derivatives of these functions:</p>
<p><span data-katex-display="true">\sigma_{ReLU}^{'} = \left( \begin{matrix} 0,\ &amp; x &lt; 0\  \\ 1,\ &amp; x &gt; 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{LeakyReLU}^{'}\left( x \right) = \left( \begin{matrix} 0.01, x &lt; 0 \\ 1, x &gt; 0 \\ \end{matrix} \right)</span></p>
<p><span data-katex-display="true">\sigma_{Sigmoid}^{'}\left( x \right) = \sigma_{Sigmoid}\left( x \right)(1 - \ \sigma_{Sigmoid}\left( x \right))</span></p>
<p>Note: the derivative of ReLU and leaky ReLU is undefined at point zero, because the function is discontinuous there, but in practice we must use a value of 0 in that point.</p>
<p>For our sample application, we use leaky ReLU by default. Sigmoid function is also available, but the training takes a much longer time compared to ReLU and leaky ReLU.</p>
</section>
</section>
<section id="training">
<h2 id="3.-training">3. Training<a href="#training" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<section id="how-the-neural-network-learns">
<h3 id="3.1-how-the-neural-network-learns">3.1 How the Neural Network Learns<a href="#how-the-neural-network-learns" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>In this article, we implement neural network training based on a common <strong>supervised learning</strong> method. Supervised learning means that we evaluate the neural network on inputs for which we know correct outputs that we’d like to get, and we adjust it to give us results closer to desired outputs. Each input is called a <strong>sample</strong> or an <strong>example</strong>, and all inputs available for training are called a <strong>training set</strong>.</p>
<p>For each sample from the training set, we start by comparing network’s prediction to the correct output using a <strong>cost function</strong>. This function tells us how good the prediction was (lower is better). Then, we will update weights and biases in a way that minimizes this cost function. This is done using an algorithm called <strong>gradient descent</strong>. The gist of it is that we first calculate how the cost function changes if we change individual weights and biases, and then we adjust them in a direction that lowers the cost. In the following text, we will often refer to “weights and biases”, but I’m only going to write “weights” to keep the text more readable. Keep in mind that what applies to weights also applies to biases here.</p>
<p>To know how to adjust the weights to lower the cost we need to calculate a value for each weight that tells us how the cost function changes when that specific weight changes. These are called <strong>partial derivatives</strong> of a cost function wrt. the weight. The vector of all partial derivatives of our weights is called a <strong>gradient</strong>.</p>
<p>Now we can imagine the cost function as a multi-dimensional surface. The gradient points in a direction where the cost function rises most steeply. We can think about stochastic descent as a ball rolling down the hill on this surface (position of the ball is given by the current network state and the cost function value). We want to get the ball as low as possible to minimize the cost. By calculating gradients (which point up the hill on this surface), and moving in opposite direction, we basically roll the ball downhill to some local minimum. Gradient is calculated using the algorithm called <strong>backpropagation</strong> which we’ll discuss in section 3.5. The process of adjusting the weights is also called an <strong>optimization</strong> of the network.</p>
<p>Once we have calculated the gradient, we can simply nudge the weights in an opposite direction by some small amount. This method of minimizing the cost function will eventually get us to a local minimum of the cost function. Note that this method doesn’t find a <em>global</em> minimum (unless it gets very lucky), but in practice this is not a huge issue for highly dimensional problems which typically have many local minima, with values similar to the global minimum.</p>
<p>We repeat this process many times to iteratively lower the cost. In each step, weights are not adjusted by the whole magnitude of the gradient, but we first multiply it by a small number (e.g., 0.0001) to take smaller steps. This multiplication constant is called a <strong>learning rate,</strong> and it tells us how fast the gradient descent should progress along the path to the local minimum. Without the learning rate, gradient descent would be taking very large steps, while being unstable and oscillating around the local minimum. Picking a right learning rate is critical in order to ensure that training will be stable, but also sufficiently fast. In practice, we will often have an <strong>adaptive learning rate.</strong> We will start with highest learning rate, and we lower it after each training step according to some schedule (e.g., linearly or exponentially). In practice, more advanced <strong>optimizers</strong> are used to adjust the weights, like the Adam optimizer that we’ll describe in section 5.</p>
<p>Learning rate is what we call a <strong>hyperparameter</strong> – it’s a parameter of a neural network implementation which is not learned by training, but rather set by a user. More advanced training algorithms have many hyperparameters. Important thing to realize is that we don’t always have to set hyperparameters manually, but we can have other optimizing algorithm (even one based on machine learning) finding the optimal values of hyperparameters for us.</p>
<p>Let’s now look at the overview of how the whole training algorithm with gradient descent works. The single <strong>training step</strong> does the following:</p>
<ol>
<li>
<p>Evaluate the network for each sample from a training set.</p>
<p>a.  Calculate a cost function for each sample, using the calculated and expected outputs.</p>
<p>b.  Calculate a gradient for each sample.</p>
</li>
<li>
<p>Average the gradients calculated in step 1b (so that we obtain one partial derivative value for each weight and bias in the network).</p>
</li>
<li>
<p>Optimize the network: scale the averaged gradient by learning rate and subtract it from the current weights to obtain new weights.</p>
</li>
<li>
<p>Repeat from step 1.</p>
</li>
</ol>
<p>Going through all the inputs in the training set is called an <strong>epoch</strong> and we’ll need many epochs before the network <strong>converges</strong> to a local minimum of a cost function. Going through all the inputs in every step can be cumbersome, as the algorithm goes through the whole data set every time. As an optimization, we can use a modified method called <strong>stochastic</strong> <strong>gradient descent (SGD)</strong>.</p>
</section>
<section id="stochastic-gradient-descent">
<h3 id="3.2-stochastic-gradient-descent">3.2 Stochastic Gradient Descent<a href="#stochastic-gradient-descent" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>In practice, we don’t need to go through the whole training set before updating the weights. We can split the training set into a number of random subsets (<strong>batches</strong>) and update weights after processing every batch. This way, we will perform weight updates more often, achieving faster learning and consuming less memory. The downside is that our gradient is not so precise anymore and it doesn’t guide us to the local minimum along the shortest path. But if our subsets are good representatives of a whole training set, this doesn’t pose a big problem in practice. The process of using batches is also called <strong>mini-batching</strong>. SGD introduces at least two new hyperparameters – batch size and number of epochs to perform per training step.</p>
<p>Remember that our sample application wants to train the network to represent a 2D image, mapping UV coordinates to RGB texel values. We will use SGD for that, taking a batch of 2048 samples per each training step, and we’ll take one training step per frame. If we had to go through all the texels in every training step, the memory requirements and runtime performance would be unusable. For our example, we don’t even need to store the training set explicitly, we can simply generate desired number of random samples in run-time by randomly sampling the texture.</p>
</section>
<section id="when-to-stop-training">
<h3 id="3.3-when-to-stop-training">3.3 When To Stop Training<a href="#when-to-stop-training" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>The training as described above can in theory run indefinitely – improving the predictions and getting the cost function lower and lower. It is unlikely that it will ever reach zero, due to constraints of the neural network architecture and learning algorithm. In practice, we have to decide at some point that training has reached the best state possible and stop. At first it seems that we can set a threshold for the cost function that we want to reach and stop after achieving it, but there is a problem: If we only measure the cost on training data, we won’t know how it will perform on real world data it has not seen before. This is the problem of <strong>generalization</strong>. We want the network to perform well also on general data that it has not seen during the training. As the training progresses, the neural network will start <strong>remembering</strong> the training data, instead of learning some general relationships between inputs and outputs, and it will fail to generalize well to new data. We also say that our model is <strong>overfitting</strong> in this case.</p>
<p>To solve this, we should also track a <strong>validation error</strong> measured on a data set which is separate from the training set. This error will be high at the beginning (just like the training error), and will get lower with training, but after some time it will start to rise again. This rise happens at the point when neural network stops generalizing well to data not seen by training, and it is the point when we should stop training. It is common to create a <strong>validation set</strong> from the training data that we have available by splitting it into 80% training set and 20% validation set. It is, however, necessary to make sure that training and validation sets don’t mix. This method of stopping the training when validation error start rising is called <strong>early stopping</strong>.</p>
<p>Note that some algorithms like NRC don’t ever stop training – they run in real time to make sure that neural network adapts to changes in the scene. Our sample application also runs the training continuously because for our use case, the generalization is not a problem, we simply want it to learn one input image as good as it can.</p>
</section>
<section id="cost-function">
<h3 id="3.4-cost-function">3.4 Cost Function<a href="#cost-function" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>For our sample application, we will use a <strong>mean squared error (MSE)</strong> as a cost function:</p>
<p><span data-katex-display="true">\frac{1}{n}\sum_{i = 1}^{n}\left( target - output \right)^{2}</span></p>
<p>There are also other cost functions used in practice, like the <strong>L1 loss</strong> or <strong>L2 loss</strong>. We can use any function which gives lower score to better outcomes, but the function must have a derivative, which we’ll need for calculating the gradient.</p>
<p>Cost function is often also called a <strong>loss function</strong> and its value simply a <strong>loss</strong>. Cost functions can also have additional <strong>regularization</strong> terms which impose additional cost, e.g., for case when weights and biases get very large (this is called <strong>weight decay</strong>). This will force the weights to be as small as possible, which can help to prevent overfitting.</p>
</section>
<section id="backpropagation">
<h3 id="3.5-backpropagation">3.5 Backpropagation<a href="#backpropagation" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>Let’s now discuss how we calculate the gradient using the <strong>backpropagation</strong> algorithm. As the name suggests, the algorithm starts at the output layer and continues <strong>backwards</strong> through the network. It is therefore also called the <strong>backward pass</strong> through the network.</p>
<p>The goal of this algorithm is to calculate partial derivative for each weight and bias wrt. cost function, which we designate as <span data-katex-display="false">\frac{\partial cost}{\partial weight_{ij}}</span> and <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span>. Before we start let’s formally define some things we’ll need to calculate those partial derivatives:</p>
<p><span data-katex-display="true">Z_{i} = \sum_{}^{}{w_{ij}O_{j} + b_{i}}</span></p>
<p><span data-katex-display="true">O_{i} = \sigma(Z_{i})</span></p>
<p>where <span data-katex-display="false">Z_{i}</span> is the output of the neuron <span data-katex-display="false">i</span> without applying activation function, <span data-katex-display="false">w_{ij}</span> is a weight of the connection to the neuron <span data-katex-display="false">j</span>, <span data-katex-display="false">O_{j}</span> is the activation of connected neuron <span data-katex-display="false">j</span> , <span data-katex-display="false">b_{i}</span> is the bias value, <span data-katex-display="false">O_{i}</span> is the output with activation function applied, and <span data-katex-display="false">\sigma</span> is the activation function.</p>
<p>So how can we calculate these partial derivatives knowing the cost function value? The trick is to split calculations of <span data-katex-display="false">\frac{\partial cost}{\partial weight_{ij}}</span> and <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span> into several derivatives which are easier to calculate and combine them using a chain rule. Let’s start with the simplest, which is <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span>. This is the same as <span data-katex-display="false">\frac{\partial cost}{\partial Z_{i}}</span>, and is also called an <strong>error</strong> of the neuron:</p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial bias_{i}} = \frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}} = \frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} \bullet \frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span></p>
<p>We have now split <span data-katex-display="false">\frac{\partial cost}{\partial bias_{i}}</span> into two simpler derivatives <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> and <span data-katex-display="false">\frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span>. For the neuron in the output layer, <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> is just telling us how the cost changes when its activation changes. This is equal to the partial derivative of the cost function with respect to evaluated neuron. When we use the MSE cost function, this derivative is:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} = c(target_{i} - O_{i})</span></p>
<p>where <span data-katex-display="false">c</span> is the constant coming from the derivation of MSE (in our sample where target is a 3-component vector, the <span data-katex-display="false">c</span> is equal to <span data-katex-display="false">2*\frac{1}{3} = 0.6\overline{66}</span>). In code, we can ignore this constant as it would only scale the whole gradient by the same number, and the gradient scale is already controlled by learning rate.</p>
<p>Next is the <span data-katex-display="false">\frac{\color{#A5A5A5} \partial O_{i}}{\color{LightSalmon} \partial Z_{i}}</span> value. This tells us how the output of the neuron changes when we apply the activation function. This is simply a derivative of the activation function which we described in section 2.4.</p>
<p>We have now calculated the partial derivative of the bias term of the neuron in the output layer, and we can use it to adjust this neuron’s bias. Next, let’s look at the derivatives of weights connecting to this neuron, splitting it again like:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{YellowGreen} \partial weight_{ij}} = \frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}} \bullet \frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}}</span></p>
<p>The term <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{i}}</span> is the same error that we calculated before, and the new term <span data-katex-display="false">\frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}}</span> tells us how the neuron value without activation function changes when we change that weight. This is a partial derivative of <span data-katex-display="false">(w_{ij}O_{j} + b_{i})</span> wrt. <span data-katex-display="false">w_{ij}</span>, which just boils down to the activation of the connected neuron:</p>
<p><span data-katex-display="true">\frac{\color{LightSalmon} \partial Z_{i}}{\color{YellowGreen} \partial weight_{ij}} = \ O_{j}</span></p>
<p>While these derivatives look intimidating at first, they boil down to a very simple calculation:</p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial bias_{i}} = \left( target_{i} - O_{i} \right) \bullet \sigma^{'}(O_{i})</span></p>
<p><span data-katex-display="true">\frac{\partial cost}{\partial weight_{ij}} = \frac{\partial cost}{\partial bias_{i}} \bullet O_{j}</span></p>
<p>With this knowledge, we can now implement gradient calculation for the output layer:</p>
<div>

<pre><code><span>// Gradient of bias</span>
<span>const</span><span> </span><span>uint</span><span> </span><span>neuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>neuron</span><span>);</span>

<span>const</span><span> </span><span>float</span><span> </span><span>neuronActivation</span><span> </span><span>=</span><span> </span><span>activations</span><span>[</span><span>neuronDataIndex</span><span>];</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dCost_O</span><span> </span><span>=</span><span> </span><span>(</span><span>neuronActivation</span><span> </span><span>-</span><span> </span><span>target</span><span>[</span><span>neuron</span><span>]);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dO_Z</span><span> </span><span>=</span><span> </span><span>ACTIVATION_FUNCTION_DERIV</span><span>(</span><span>neuronActivation</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>dCost_Z</span><span> </span><span>=</span><span> </span><span>dCost_O</span><span> </span><span>*</span><span> </span><span>dO_Z</span><span>;</span>
<span>errors</span><span>[</span><span>neuronDataIndex</span><span>]</span><span> </span><span>=</span><span> </span><span>dCost_Z</span><span>;</span>
<span>InterlockedAdd</span><span>(</span><span>gradientBiases</span><span>[</span><span>neuronDataIndex</span><span>],</span><span> </span><span>packFloat</span><span>(</span><span>dCost_Z</span><span>));</span>

<span>// Gradient of weights</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>previousNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>previousNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountPreviousLayer</span><span>;</span><span> </span><span>previousNeuron</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>previousNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>2</span><span>,</span><span> </span><span>previousNeuron</span><span>);</span>
<span>  </span><span>const</span><span> </span><span>float</span><span> </span><span>dCost_weight</span><span> </span><span>=</span><span> </span><span>dCost_Z</span><span> </span><span>*</span><span> </span><span>activations</span><span>[</span><span>previousNeuronDataIndex</span><span>];</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>LAYER_COUNT</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>previousNeuron</span><span>,</span><span> </span><span>neuron</span><span>);</span>
<span>  </span><span>InterlockedAdd</span><span>(</span><span>gradientWeights</span><span>[</span><span>weightIndex</span><span>],</span><span> </span><span>packFloat</span><span>(</span><span>dCost_weight</span><span>));</span>
<span>}</span>
</code></pre>
</div>
<p>What remains is to calculate the same partial derivatives for the hidden layer, which is just slightly more complicated. Because the neurons in the hidden layer are connected to other neurons, their <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> values are not dependent on the cost function directly, but on the connected neurons. Specifically:</p>
<p><span data-katex-display="true">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}} = \sum_{j}^{}\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{j}} \bullet \frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}}</span></p>
<p>Where <span data-katex-display="false">j</span> is the j-th neuron connected to the output of evaluated neuron <span data-katex-display="false">i</span> , <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{LightSalmon} \partial Z_{j}}</span> is the error of the j-th neuron evaluated previously, and <span data-katex-display="false">\frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}}</span> tells us how the output of neuron <span data-katex-display="false">j</span> without activation function changes when the output of the evaluated neuron changes. This is a derivative of <span data-katex-display="false">(w_{ij}O_{j} + b_{i})</span> wrt. <span data-katex-display="false">O_{j}</span>, so simply a strength of the connection between the neurons:</p>
<p><span data-katex-display="true">\frac{\color{LightSalmon} \partial Z_{j}}{\color{#A5A5A5} \partial O_{i}} = weight_{ij}</span></p>
<p>The code for hidden layer gradient is almost the same, except for the <span data-katex-display="false">\frac{\color{DodgerBlue} \partial cost}{\color{#A5A5A5} \partial O_{i}}</span> calculation, which is:</p>
<div>

<pre><code><span>float</span><span> </span><span>dCost_O</span><span> </span><span>=</span><span> </span><span>0.0f</span><span>;</span>
<span>for</span><span> </span><span>(</span><span>uint</span><span> </span><span>nextNeuron</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>nextNeuron</span><span> </span><span>&lt;</span><span> </span><span>neuronCountNextLayer</span><span>;</span><span> </span><span>nextNeuron</span><span>++</span><span>)</span>
<span>{</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>weightIndex</span><span> </span><span>=</span><span> </span><span>getConnectionDataIndex</span><span>(</span><span>layer</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>neuron</span><span>,</span><span> </span><span>nextNeuron</span><span>);</span>
<span>  </span><span>const</span><span> </span><span>uint</span><span> </span><span>nextNeuronDataIndex</span><span> </span><span>=</span><span> </span><span>getNeuronDataIndex</span><span>(</span><span>layer</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>nextNeuron</span><span>);</span>
<span>  </span><span>dCost_O</span><span> </span><span>+=</span><span> </span><span>(</span><span>errors</span><span>[</span><span>nextNeuronDataIndex</span><span>]</span><span> </span><span>*</span><span> </span><span>nnWeights</span><span>[</span><span>weightIndex</span><span>]);</span>
<span>}</span>
</code></pre>
</div>
<p>Notice that in order to implement this, we had to store errors of the previously evaluated neurons. We can say that the error is propagating backwards through the neural network, giving this algorithm its name – backpropagation. We also need to store activations for all neurons, which was not necessary for the forward pass.</p>
</section>
<section id="training-implementation-details">
<h3 id="3.6-training-implementation-details">3.6 Training Implementation Details<a href="#training-implementation-details" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>The code sample implements training as described above running on the GPU using Dx12 compute shaders. The whole training runs in 2 main kernel dispatches: <strong>gradient calculation</strong> and <strong>optimization</strong>. For gradient calculation, we run as many threads in parallel as we have training examples in the batch. Each thread generates a&nbsp;training example by randomly sampling a&nbsp;reference texture. Then it runs the forward pass to evaluate activations for the whole network, followed by the backpropagation pass to calculate the gradient. You may have noticed in the previous listings that we use InterlockedAdd operation to store the gradient. This is because we are interested in average gradient for all training examples in the batch, so we can add them all up and then divide the sum by the batch size before using it. The code for a gradient calculation step is:</p>
<div>

<pre><code><span>// Initialize random numbers generator</span>
<span>uint</span><span> </span><span>rng</span><span> </span><span>=</span><span> </span><span>initRNG</span><span>(</span><span>LaunchIndex</span><span>,</span><span> </span><span>uint2</span><span>(</span><span>1</span><span>,</span><span> </span><span>1</span><span>),</span><span> </span><span>gData</span><span>.</span><span>frameNumber</span><span>);</span>

<span>// Generate a random input (UV coordinates in the image)</span>
<span>const</span><span> </span><span>float2</span><span> </span><span>uvs</span><span> </span><span>=</span><span> </span><span>float2</span><span>(</span><span>rand</span><span>(</span><span>rng</span><span>),</span><span> </span><span>rand</span><span>(</span><span>rng</span><span>));</span>

<span>// Load target value to learn for this input from reference image</span>
<span>const</span><span> </span><span>float3</span><span> </span><span>target</span><span> </span><span>=</span><span> </span><span>targetTexture</span><span>[</span><span>uvs</span><span> </span><span>*</span><span> </span><span>float2</span><span>(</span><span>gData</span><span>.</span><span>outputWidth</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>gData</span><span>.</span><span>outputHeight</span><span> </span><span>-</span><span> </span><span>1</span><span>)].</span><span>rgb</span><span>;</span>

<span>// First run forward pass to evaluate network activations for given input</span>
<span>float</span><span> </span><span>activations</span><span>[</span><span>LAYER_COUNT</span><span> </span><span>*</span><span> </span><span>MAX_NEURONS_PER_LAYER</span><span>];</span>
<span>forwardPass</span><span>(</span><span>uvs</span><span>,</span><span> </span><span>activations</span><span>);</span>

<span>// Run backpropagation on current network state</span>
<span>backpropagation</span><span>(</span><span>target</span><span>,</span><span> </span><span>activations</span><span>);</span>
</code></pre>
</div>
<p>The next step – optimization – runs once after calculating gradient for every batch, reads the gradient and adjusts the weights and biases accordingly.</p>
</section>
<section id="neural-network-initialization">
<h3 id="3.7-neural-network-initialization">3.7 Neural Network Initialization<a href="#neural-network-initialization" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>There is one important aspect of training we haven’t covered yet and that’s the initial state of the network before we start the training. Initial weights and biases influence where our training starts and how fast can it approach the optimal solution, so a&nbsp;good initialization strategy is desirable. If we used some constant for all initial weights (e.g., zero or one), all activations and their gradients would be the same – the network wouldn’t be learning anything as every neuron would follow the same learning path (assuming we have a deterministic learning algorithm).</p>
<p>Because of this, we want to start with different initial weight and bias setting for each neuron to “break the symmetry”. Therefore, we initialize the network with some small random numbers centered around zero. We can take these numbers, e.g., from the uniform or normal distribution. To do that we must pick a range in which to generate random numbers, and in case of normal distribution also the standard deviation. These can either be hyperparameters tuned manually, or better, we can use one of the common strategies for setting them automatically. Such strategies are, e.g., <strong>LeCun uniform</strong>, <strong>Xavier uniform</strong> and their variations using normal distribution. These have been introduced in paper <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a>.</p>
<p>In our code sample we use the Xavier uniform initialization. It generates weights using a uniform random distribution within the range <span data-katex-display="false">\lbrack -x,x\rbrack</span>, where <span data-katex-display="false">x</span> is dependent on number of neurons in layers that the weight is connecting:</p>
<p><span data-katex-display="true">x = \sqrt{\frac{6}{n_{previousLayer} + n_{currentLayer}}}</span></p>
<p>Random initial values for weights are enough to break the symmetry and ensure proper learning, and therefore it is common to initialize biases to zeroes, or some small constant.</p>
</section>
</section>
<section id="improving-the-network-input-encodings">
<h2 id="4.-improving-the-network---input-encodings">4. Improving the Network – Input Encodings<a href="#improving-the-network-input-encodings" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>So far, we have only used the identity input encoding. Meaning, we have simply passed our UV coordinates into two input neurons. In this section we are going to explore a more advanced input encoding, the <strong>frequency encoding</strong>, which greatly improves performance for our sample application.</p>
<section id="frequency-input-encoding">
<h3 id="4.1-frequency-input-encoding">4.1 Frequency Input Encoding<a href="#frequency-input-encoding" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>This encoding was described in the <a href="https://arxiv.org/pdf/2003.08934" target="_blank" rel="noopener">NeRF paper</a> under the name “positional encoding”, but I see it’s often referred to as the “frequency encoding”. Idea is to transform the input into higher-dimensional space using the following formula:</p>
<p><span data-katex-display="true">\gamma\left( p \right) = \left( \sin\left( 2^{0}\pi p \right),\cos\left( 2^{0}\pi p \right), \ldots \sin\left( 2^{L - 1}\pi p \right),\cos\left( 2^{L - 1}\pi p \right) \right)</span></p>
<p>where <span data-katex-display="false">p</span> is the input value we want to encode, <span data-katex-display="false">L</span> is the number of frequencies we want to use (e.g., 8) and <span data-katex-display="false">\gamma</span> is a vector of encoded values. Note that with this encoding, we have greatly increased number of input neurons. For our sample application with 2 input values, and 8 frequencies, we get 32 input neurons.</p>
<p>Having inputs in higher-dimensional space will make it easier for neural network to discover more complex relationships between inputs and outputs. Implementation of this encoding can be found in the sample code in function called frequencyEncoding.</p>
</section>
<section id="other-input-encodings">
<h3 id="4.2-other-input-encodings">4.2 Other Input Encodings<a href="#other-input-encodings" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>In the deep learning literature, you will often encounter a <strong>one-hot encoding</strong>. This is a very simple encoding where input is encoded into a vector of values, where only a single entry is 1, and others are 0 (we say that one value is hot). E.g., we can encode a fact that object belongs to certain category by having a vector of values representing each category, and setting value 1 for the selected category, while zeroing out others.</p>
<p>For more advanced encodings, I recommend looking at the <strong>one-blob encoding</strong>, introduced in <a href="https://arxiv.org/pdf/1808.03856" target="_blank" rel="noopener">Neural Importance Sampling</a> paper, which extends one-hot encoding to have multiple entries activated, instead of just one. This essentially activates or shuts down parts of the network, depending on the input value.</p>
<p>Another useful encoding is a <strong>hash-grid encoding</strong> introduced in <a href="https://tom94.net/data/publications/mueller22instant/mueller22instant.pdf" target="_blank" rel="noopener">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a> paper.</p>
</section>
</section>
<section id="improving-the-network-adam-optimizer">
<h2 id="5.-improving-the-network---adam-optimizer">5. Improving the Network – Adam Optimizer<a href="#improving-the-network-adam-optimizer" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>So far, we have applied gradients during the training using a very simple way – we just multiplied it by the learning rate and subtracted it from current weights and biases. While this works, it is not optimal for several reasons: fixed learning rate is usually suboptimal at the beginning, when we want to take larger steps to proceed faster, but it is also suboptimal at the end, when it can oscillate around the optimal solution because the fixed step is too large to get into the valley of local minimum. In practice, we want to use an adaptive learning rate instead.</p>
<p>In this section we implement an improved optimizer called <em>Adam</em> (adaptive moment estimation), described in the paper <a href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a>. Adam adds two ingredients to the optimization process: adaptive learning rate and momentum.</p>
<p>We have described adaptive learning rate before, so let’s now look at the momentum. Remember our “ball rolling” example where we stated that minimizing the cost function is like rolling the ball on its surface to find a lowest point. Just like in real world, where the ball has certain momentum, we can add momentum to our algorithm by taking into consideration the gradients in previous steps and applying a weighted average of them, instead of just latest gradients. This way, we progress to the optimum faster and we have a higher chance of escaping shallow valleys with local minima to find even lower local minimum.</p>
<section id="adam-implementation">
<h3 id="5.1-adam-implementation">5.1 Adam Implementation<a href="#adam-implementation" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h3>
<p>Adam optimizer works by tracking the first and second moments (mean and variance) of the gradient. This means that for each weight and bias, we have to track mean and variance of their partial derivatives. They are averaged over time and their decay is controlled by new hyperparameters <span data-katex-display="false">\beta_{1}</span> and <span data-katex-display="false">\beta_{2}</span>. The adjustment done to weight or bias is calculated using its gradient, mean, and variance in the following way:</p>
<div>

<pre><code><span>// Update mean and variance for this training step</span>

<span>mean</span><span> </span><span>=</span><span> </span><span>lerp</span><span>(</span><span>gradient</span><span>,</span><span> </span><span>mean</span><span>,</span><span> </span><span>gData</span><span>.</span><span>adamBeta1</span><span>);</span>
<span>variance</span><span> </span><span>=</span><span> </span><span>lerp</span><span>((</span><span>gradient</span><span> </span><span>*</span><span> </span><span>gradient</span><span>),</span><span> </span><span>variance</span><span>,</span><span> </span><span>gData</span><span>.</span><span>adamBeta2</span><span>);</span>

<span>// Calculate weight (or bias) adjustment</span>
<span>const</span><span> </span><span>float</span><span> </span><span>correctedMean</span><span> </span><span>=</span><span> </span><span>mean</span><span> </span><span>/</span><span> </span><span>(</span><span>1.0f</span><span> </span><span>-</span><span> </span><span>gData</span><span>.</span><span>adamBeta1T</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>correctedVariance</span><span> </span><span>=</span><span> </span><span>variance</span><span> </span><span>/</span><span> </span><span>(</span><span>1.0f</span><span> </span><span>-</span><span> </span><span>gData</span><span>.</span><span>adamBeta2T</span><span>);</span>
<span>const</span><span> </span><span>float</span><span> </span><span>weightAdjustment</span><span> </span><span>=</span><span> </span><span>-</span><span>gData</span><span>.</span><span>learningRate</span><span> </span><span>*</span><span> </span><span>(</span><span>correctedMean</span><span> </span><span>/</span><span> </span><span>(</span><span>sqrt</span><span>(</span><span>correctedVariance</span><span>)</span><span> </span><span>+</span><span> </span><span>gData</span><span>.</span><span>adamEpsilon</span><span>));</span>
</code></pre>
</div>
<p>The updated mean and variance are stored next to weights and biases. Note that we still have to set a base learning rate when using this algorithm, the paper suggests a value of 0.001. In practice, the default values suggested in the paper for <span data-katex-display="false">\beta_{1} = 0.9</span> and <span data-katex-display="false">\beta_{2} = 0.999</span> are used as well. The default value for <span data-katex-display="false">\varepsilon</span> which prevents division by zero is <span data-katex-display="false">10^{- 8}</span>. Values <span data-katex-display="false">\beta_{1}^{T}</span> and <span data-katex-display="false">\beta_{2}^{T}</span> are derived from <span data-katex-display="false">\beta_{1}</span> and <span data-katex-display="false">\beta_{2}</span> and adjusted after each training step as follows:</p>
<div>

<pre><code><span>adamBeta1T</span><span> </span><span>=</span><span> </span><span>pow</span><span>(</span><span>adamBeta1</span><span>,</span><span> </span><span>training_steps</span><span> </span><span>+</span><span> </span><span>1</span><span>);</span>
<span>adamBeta2T</span><span> </span><span>=</span><span> </span><span>pow</span><span>(</span><span>adamBeta2</span><span>,</span><span> </span><span>training_steps</span><span> </span><span>+</span><span> </span><span>1</span><span>);</span>
</code></pre>
</div>
</section>
</section>
<section id="problems-with-training">
<h2 id="6.-problems-with-training">6. Problems with Training<a href="#problems-with-training" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>While the method for training described so far is fairly robust and efficient, there are several problems which we can encounter in practice, let’s briefly discuss some of them:</p>
<ul>
<li>
<p><strong>Insufficient training data:</strong> when we have low amount of training data, or when the batch size of stochastic gradient descent is too low, we won’t be able to find a stable solution that generalizes well. Make sure to have sufficient data to get into local minimum of the cost function.</p>
</li>
<li>
<p><strong>Non-deterministic data:</strong> So far, we assumed that there is a deterministic relationship between the training inputs and target outputs. Breaking this assumption by having some random values can make it hard or impossible for training to converge.</p>
</li>
<li>
<p><strong>Wrong learning rate:</strong> Having a learning rate too small will cause the training to proceed very slowly, while having it too large can make the training unstable.</p>
</li>
<li>
<p><strong>Vanishing gradients:</strong> For deep networks, gradients in certain layers can become very small, slowing down or even stopping the training. This happens because for typical activation functions, large changes on some inputs can only cause small changes (or no changes) of the output. E.g., the negative part of the leaky ReLU outputs very small values. In this case, partial derivatives become smaller and smaller, and training doesn’t change the weights much. One of the possible solutions is to use different activation functions.</p>
</li>
<li>
<p><strong>Exploding gradients:</strong> The opposite problem happens when gradients get very large and cause instability. As a solution, we can clamp gradients to some threshold value, limiting the rate at which they can change weights and biases – this is called the <strong>gradient clipping</strong>.</p>
</li>
</ul>
</section>
<section id="next-topics">
<h2 id="7.-next-topics">7. Next Topics<a href="#next-topics" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>There is a lot more to deep learning than what I described so far in this article. Before we wrap up, I want to mention at least two topics worth studying next, that you will encounter often when reading deep learning papers focused on computer graphics: auto-encoders and convolutional networks.</p>
<p>Auto-encoder is a type of neural network consisting of encoder and a decoder. Encoder translates input data into more compact representation (in latent space) and decoder is used to decompress it to original representation. It can be used for applications like data compression, denoising and image reconstruction.</p>
<p>Convolutional networks (CNNs) are specialized networks used mostly in image processing. They contain 2 types of hidden layers: convolutional layers intended to detect certain features in images, and pooling layers which downsample intermediate results into more compact representation.</p>
<p>While I was first learning about deep learning, I found great insight in the paper titled “<a href="https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf" target="_blank" rel="noopener">Multilayer Feedforward Networks are Universal Approximators</a>” from 1989. It says that MLPs with at least one hidden layer are <strong>universal approximators</strong>, meaning they can approximate any function to a degree allowed by the network capacity. It also says that failure to approximate given function can be attributed to inadequate learning, insufficient network capacity, or non-deterministic relation between network inputs and outputs. In practice, we can therefore use MLPs to replace parts of our algorithms which contain difficult functions mapping data from one space to another.</p>
</section>
<section id="conclusion">
<h2 id="8.-conclusion">8. Conclusion<a href="#conclusion" title="Permalink to this heading"><span size="2px"><svg fill="#ED1C24" height="1em" viewBox="0 0 640 512"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"></path></svg></span></a></h2>
<p>In this article, I have described a few concepts and algorithms used in deep learning that I found most interesting for my use case (representing an image using an MLP), but there is much more to explore. With the knowledge provided here, I hope that you’ll have more fun reading deep learning papers and resources and implement your own deep learning ideas.</p>
<p>I recommend looking at the book <em><a href="https://www.glassner.com/portfolio/deep-learning-a-visual-approach/" target="_blank" rel="noopener">Deep Learning: A Visual Approach</a></em> by Glassner, the book <em>Deep Learning</em> by Goodfellow et al. which is <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">freely available online</a>, deep learning videos by <a href="https://www.youtube.com/@3blue1brown" target="_blank" rel="noopener">3Blue1Brown</a>, neural network <a href="https://blog.demofox.org/2017/03/09/how-to-train-neural-networks-with-backpropagation/" target="_blank" rel="noopener">blogs by demofox</a>, paper titled “<a href="https://www.researchgate.net/publication/277411157_Deep_Learning" target="_blank" rel="noopener">Deep learning</a>” by LeCun et al. from 2015, and graphics papers such as <a href="https://arxiv.org/pdf/2003.08934" target="_blank" rel="noopener">NeRF</a> and <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/mueller21realtime.pdf" target="_blank" rel="noopener">NRC</a>. When implementing neural networks, I recommend reading an article <a href="https://gpuopen.com/learn/wmma_on_rdna3/" target="_blank" rel="noopener">How to accelerate AI applications on RDNA 3 using WMMA</a> for insights how to use AI accelerating instructions of current GPUs. Finally, I recommend experimenting with libraries like <a href="https://pytorch.org/" target="_blank" rel="noopener">pyTorch</a> which enable much faster prototyping, than implementing everything from scratch.</p>
<p>I want to thank Daniel Meister for helpful comments and suggestions.</p>
<p><em>This blog was originally published by Jakub at <a href="https://boksajak.github.io/blog/DeepLearning" target="_blank" rel="noopener">https://boksajak.github.io/blog/DeepLearning</a></em>.</p>
</section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google is the only search engine that works on Reddit now thanks to AI deal (325 pts)]]></title>
            <link>https://www.404media.co/google-is-the-only-search-engine-that-works-on-reddit-now-thanks-to-ai-deal/</link>
            <guid>41057033</guid>
            <pubDate>Wed, 24 Jul 2024 13:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/google-is-the-only-search-engine-that-works-on-reddit-now-thanks-to-ai-deal/">https://www.404media.co/google-is-the-only-search-engine-that-works-on-reddit-now-thanks-to-ai-deal/</a>, See on <a href="https://news.ycombinator.com/item?id=41057033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->
  <div>
    <h5>Subscribe</h5>
    <div>
      <p>Join the newsletter to get the latest updates.</p>
      <form data-members-form="subscribe">
        
        
        <div>
          
          <p>
            Great! Check your inbox and click the link.
          </p>
        </div>
        <div>
          
          <p>
            Please enter a valid email address.
          </p>
        </div>
      </form>
    </div>
  </div>

<!--kg-card-end: html-->
<div><p>🖥️</p><p><i><em>404 Media is an independent website whose work is written, reported, and owned by human journalists and whose intended audience is real people, not AI scrapers, bots, or a search algorithm. Sign up to support our work and for free access to this article. </em></i><a href="https://www.404media.co/why-404-media-needs-your-email-address/" rel="noreferrer"><i><em>Learn why we require this here</em></i></a><i><em>.</em></i></p></div><p>Google is now the only search engine that can surface results from Reddit, making one of the web’s most valuable repositories of user generated content exclusive to the internet’s already dominant search engine.</p><p>If you use Bing, DuckDuckGo, Mojeek, Qwant or any other alternative search engine that doesn’t rely on Google’s indexing and search Reddit by using “site:reddit.com,” you will not see any results from the last week. DuckDuckGo is currently turning up seven links when searching Reddit, but provides no data on where the links go or why, instead only saying that “We would like to show you a description here but the site won't allow us.” Older results will still show up, but these search engines are no longer able to “crawl” Reddit, meaning that Google is the only search engine that will turn up results from Reddit going forward. Searching for Reddit still works on <a href="https://www.404media.co/friendship-ended-with-google-now-kagi-is-my-best-friend/" rel="noreferrer">Kagi</a>, an independent, paid search engine that buys part of its search index from Google.</p><p>The news shows how Google’s near monopoly on search is now actively hindering other companies’ ability to compete at a time when Google is facing increasing criticism over the quality of its search results. And while neither Reddit or Google responded to a request for comment, it appears that the exclusion of other search engines is the result of a multi-million dollar deal that gives Google the right to scrape Reddit for data to train its AI products.</p><p>“They’re [Reddit] killing everything for search but Google,” Colin Hayhurst, CEO of the search engine Mojeek told me on a call.&nbsp;</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You got a null result. Will anyone publish it? (187 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-02383-9</link>
            <guid>41056387</guid>
            <pubDate>Wed, 24 Jul 2024 12:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-02383-9">https://www.nature.com/articles/d41586-024-02383-9</a>, See on <a href="https://news.ycombinator.com/item?id=41056387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Evolutionary biologist Natalie Pilakouta thought it would be an easy theory to test: fish living in Iceland’s geothermal hot springs prefer warmer water than do members of the same species that live in cooler lakes nearby. Yet, when she came to the end of her two-year study, what she found was inconclusive — given the choice, both populations of fish preferred the same, cooler waters. Her postdoctoral supervisor urged her to set aside the findings and move on to other studies: “It’s a failed experiment,” she was told. “You must have done something wrong.”</p><p>The words stung because publications are a crucial piece of academic currency, particularly for an early-career researcher, and she knew she’d face an uphill struggle to find a home for her results. Moreover, she felt a sense of urgency to share the counter-intuitive findings, which undermine the assumption that aquatic life might evolve a preference for higher temperatures in response to global warming.</p><p>Pilakouta, who is based at the University of St Andrews, UK, was one of the lucky ones. After submitting her findings to seven journals over six years, her study was finally published<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> in January 2023. But her experience illustrates <a href="https://www.nature.com/articles/d41586-024-01389-7" data-track="click" data-label="https://www.nature.com/articles/d41586-024-01389-7" data-track-category="body text link">academia’s oft-bemoaned ‘file-drawer problem’</a>, in which findings with null or negative results — those that fail to find a relationship between variables or groups, or that go against the preconceived hypothesis — gather dust in favour of studies with positive or significant findings. A 2022 survey of scientists in France, for instance, found that 75% were willing to publish null results they had produced, but only 12.5% were able to do so<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Over time, this bias in publications distorts the scientific record, and a focus on significant results can encourage researchers to selectively report their data or exaggerate the statistical importance of their findings. It also wastes time and money, because researchers might duplicate studies that had already been conducted but not published. Some evidence suggests that the problem is getting worse, with fewer negative results seeing the light of day<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> over time.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-01389-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27154836.jpg"><p>Illuminating ‘the ugly side of science’: fresh incentives for reporting negative results</p></a>
 </article><p>Funders, publishers and researchers are not sitting idle. Many journals now encourage teams to submit plans and protocols for experiments before conducting them, so that the journals can review the proposals and commit to publishing the results, whatever the outcome. Hundreds of journals now offer such ‘registered reports’, and the number of journals adopting this approach has doubled since 2018.</p><p>A laser focus on positive results is not the only way to do science, says Brian Nosek, executive director of the Center for Open Science in Charlottesville, Virginia. Nosek and a constellation of researchers across the world have been pushing to rewrite how research is conducted, challenging the very definition of success. This includes a crackdown on the nefarious side of science — misconduct such as plagiarism — but also a call to curb some of the ‘softer’ transgressions such as selective reporting, with a view to <a href="https://www.nature.com/articles/d41586-023-02876-z" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02876-z" data-track-category="body text link">publishing more negative findings</a>. These changes have started to materialize across the publishing industry, as preprint servers proliferate and publishers adopt new manuscript formats, launch journals dedicated to null results and call for special issues.</p><p>“We can surely do better,” Nosek says.</p><h2><b>Hidden results</b></h2><p>Researchers have noted the file-drawer problem for decades. But the bigger problems it was causing did not become clear until the early 2010s, when they set out to reproduce the findings of several foundational experiments in psychology and medical science, and found that they could not. Scientists began to study the extent of this ‘replication crisis’ and the problem of publication bias.</p><p>Their research laid bare just how often negative results were being buried. In an analysis of more than 300,000 scientific conference presentations, informal posters or talks that scientists often endeavour to turn into papers, fewer than 40% were published in peer-reviewed journals, and negative or null findings were far less likely to be published than positive results<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>.</p><p>The extent of publication bias varies by discipline and by country, but the problem seems to have worsened over time. An analysis of 4,600 papers from 1990 to 2007 found that publication bias had increased by 22% over that period<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>There could be real-world implications to such a skew in publications. Among 74 registered clinical trials evaluating antidepressants, for example, nearly one-third remained unpublished; these trials were much more likely to show negative than positive results<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>. Judging by the publications alone, 94% of the trials looked as if they returned positive results, whereas a drug-approval panel judged that 51% did.</p><p>This selective reporting creates an inflated perception of drug efficacies, one compounded by meta-analyses — surveys of the published literature — that contain mainly studies with positive results.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02876-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27374058.jpg"><p>How early-career researchers can learn to trust negative data: five simple steps</p></a>
 </article><p>The bias is present despite the fact that investigators conducting clinical trials in the United States are mandated by law to report their results, regardless of outcome; there can be billions of dollars at stake and trial participants who have given their time and expect the results to be published. These results illustrate “how high a hill there is to climb”, Nosek says.</p><p>Adding to the likelihood of bias, studies with negative or null findings are often given stricter scrutiny than those with positive findings, especially if the positive findings “confirm something we think is true”, says Steven Goodman, founder of the Stanford Program on Research Rigor and Reproducibility at the Stanford School of Medicine in California.</p><p>Jessica Payne, a cognitive neuroscientist at the University of Notre Dame in South Bend, Indiana, says that there’s still a perception that scientists must have had some flaw in their research design if a study returns negative or null results.</p><p>Indeed, according to a survey of 480 economists<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup>, studies with null results are perceived to be less publishable, of lower quality and less important than studies with large and significant results, even when features such as sample size are held constant — a phenomenon known as the null result penalty. If anything, Goodman says, a study with a large effect size should be scrutinized much more than one with a null finding.</p><h2><b>Cultural bias </b></h2><p>The replication crisis made one fact crystal clear: incentive structures in academia are not always in line with research integrity and reproducibility. That has a large role in why so few negative studies are published, says Anne Scheel, a metascientist at Utrecht University in the Netherlands.</p><p>At the crux of both academic misconduct and publication bias is the same ‘publish or perish’ culture, perpetuated by academic institutions, research funders, scholarly journals and scientists themselves, that rewards researchers when they publish findings in prestigious venues, Scheel says.</p><p>But these academic gatekeepers have biases, say some critics, who argue that funders and top-tier journals often crave novelty and attention-grabbing findings. Journal editors worry that pages full of null results will attract fewer readers, says Simine Vazire, a psychologist at the University of Melbourne in Australia and editor of the journal <i>Psychological Science</i>.</p><p>This creates a tight feedback loop between researchers and journals. To attract journals with findings that seem new and noteworthy, some scientists might be tempted to change their hypothesis after seeing the results, or to release only a portion of the data, or to perform statistical tricks, Nosek says.</p><h2><b>Positive solutions</b></h2><p>To encourage more researchers to report null results, journals and funders are trying several schemes. One of the most significant changes to come out of the replication crisis is the expansion of preregistration (see ‘Registrations on the rise’), in which researchers must state their hypothesis and the outcomes they intend to measure in a public database at the outset of their study (this is already the norm in clinical trials).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365096.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365096.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="REGISTRATIONS ON THE RISE. Chart shows the number of study plans uploaded by researchers is increasing each year." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365096.png">
  <figcaption>
   <p><span>Source: OSF.IO</span></p>
  </figcaption>
 </picture>
</figure><p>The preregistration model nudges researchers to be faithful to the original intent of their study, but it doesn’t address biases that might affect whether they submit their findings to a journal, nor the biases of journal editors and reviewers in deciding what to publish, Nosek says.</p><p>Instead, he and his colleagues have been focusing on promoting and evaluating the registered report model — similar to a preregistered report, but with the initial plan published by a journal, along with a commitment to peer-review and publish the results.</p><p>Preliminary data look promising: when Scheel and her colleagues compared the results of 71 registered reports with a random sample of 152 standard psychology manuscripts, they found that 44% of the registered reports had positive results, compared with 96% of the standard publications<sup><a href="#ref-CR7" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">7</a></sup> (see ‘Intent to publish’). And Nosek and his colleagues found that reviewers scored psychology and neuroscience registered reports higher on metrics of research rigour and quality compared with papers published under the standard model<sup><a href="#ref-CR8" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">8</a></sup>.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365092.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365092.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="INTENT TO PUBLISH. Chart shows registered reports are much more likely to yield negative results" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27365092.png">
  <figcaption>
   <p><span>Source: Ref. 7</span></p>
  </figcaption>
 </picture>
</figure><p>When the format launched in 2012, only a handful of journals published registered reports; now more than 300 offer the format, including <i>PLoS ONE</i> and <i>Nature</i>, which is published by Springer Nature (<i>Nature</i>’s news team is editorially independent from its journal team). Since starting to offer the format in February 2023, <i>Nature</i> has yet to publish any registered reports, but its sibling journal <a href="https://www.nature.com/nathumbehav/research-articles?type=registered-report" data-track="click" data-label="https://www.nature.com/nathumbehav/research-articles?type=registered-report" data-track-category="body text link"><i>Nature Human Behaviour</i> has</a>.</p><p>Although the format has gained in popularity, there are still some kinks to be ironed out, researchers say. Earlier this year, Christine Blume, a sleep researcher at the University of Basel in Switzerland, published her first registered report<sup><a href="#ref-CR9" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">9</a></sup> on how light affects human circadian rhythms in <i>Nature Human Behaviour.</i> Although she liked receiving feedback on her study design before data collection — “it made me feel that I had the best study design to answer the question I set out to address,” she says — she found it frustrating that the feedback process can span months, even though researchers have a limited time in which to spend grant money.</p><p>These practical concerns are important to address, Nosek says. He admits that his own paper about the quality of registered reports<sup><a href="#ref-CR8" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">8</a></sup> was not itself a registered report, because the grant money was expiring and the team didn’t have time to go through a lengthy approval process and complete its analysis. “We cannot dismiss pragmatics, but what we can do is think about how we lower the barrier so that more of these circumstances can be dealt with,” he says.</p><p>Journals that offer registered reports are not spread equally across disciplines; most are in psychology and, more recently, neuroscience. Few physical-science journals offer the format — even though null results, such as the failure of the Large Hadron Collider near Geneva in Switzerland to find new subatomic particles since the Higgs boson, have been an important part of progress. Emily Sena, a translational-medicine researcher and metascientist at the University of Edinburgh, UK, says that few academics in preclinical fields have been keen to try the format, especially when there is already so much red tape before researchers can begin their experiments.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-01347-3" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02383-9/d41586-024-02383-9_27374054.jpg"><p>Disputed dark-matter claim to be tested by new lab in South Korea</p></a>
 </article><p>The format has been slow to catch on among researchers, says Vazire. “We’re not receiving many registered-report submissions.”</p><p>Sena and her colleagues have been spreading the word about registered reports and helping journal editors to feel equipped to review submissions, she says. Some funders are providing cash incentives: in 2022, the <a href="https://www.cos.io/blog/funding-consciousness-research" data-track="click" data-label="https://www.cos.io/blog/funding-consciousness-research" data-track-category="body text link">Center for Open Science</a> offered up to US$50,000 to consciousness researchers willing to publish a registered report for their work.</p><p>It will be important to track how these interventions affect marginalized groups in academia, Sena says. Academics of colour are more likely to be on a fixed contract, so they have less wiggle room to embrace formats that might be better for science overall but less helpful for individual scientists, she says.</p><p>The Center for Open Science is planning to run trials in which researchers are randomly assigned to use either the standard publication model or a registered report, to evaluate the rigour, acceptance rate and timelines of the resulting publications. Results are expected by 2027.</p><p>Not every effort to reduce publication bias has borne fruit. One that has seldom worked, Nosek says, is to set up journals with the express purpose of publishing null results. These efforts are well intentioned, he says, but often do not work because a journal can become identified with studies that weren’t able to be published elsewhere. “It can’t provide the reward that researchers need,” he says.</p><p>Payne was a co-editor at one of these journals, <i>Experimental Results</i>, published by Cambridge University Press. After only three years, the journal ceased publication in 2023 despite carrying the “imprimatur of Cambridge”, she says.</p><p>There’s an increasingly popular do-it-yourself route to publishing negative results: posting a manuscript on a preprint server. Publishing a preprint can offer an opportunity to showcase research without the pressure of journal submission. This option can be especially helpful for early-career researchers, Pilakouta says. Still, it takes time to write up a result, regardless of where it appears, and publishing on preprint servers is unlikely to offer researchers enough of an incentive to justify the time, Goodman says.</p><h2><b>Null nuance</b></h2><p>Advocates acknowledge that not every study that returns a null result is worthy of publishing. Goodman says he encourages researchers to publish null and negative findings that are “informative”, meaning they come from studies and analyses that are designed rigorously, question previous results and open up fresh areas for exploration.</p><p>For example, there is a long-held idea that the womb is sterile — that the uterus and fetus are free of microorganisms. But, beginning in 2010, a series of papers found microbial contamination in the placenta, calling the hypothesis into question and suggesting that some complications of pregnancy could be linked to bacteria. It wasn’t until 2019 that a study of placental samples from 537 women — by far the largest number in an analysis of this kind — rigorously showed the absence of any bacterial signal. That study set a benchmark for investigating the microbiome of tissues that carry few microorganisms and that can therefore give rise to false-positive results, and suggested that bacterial infection is not a common cause of problems in pregnancy<sup><a href="#ref-CR10" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">10</a></sup>.</p><p>Blume says it’s important to extract something insightful from the data, even if they are inconclusive. For example, in 2022 she found that although artificial light suppresses the hormone melatonin, that didn’t equate to a change in sleep quality<sup><a href="#ref-CR11" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">11</a></sup>. The message that melatonin isn’t necessarily a proxy for sleep quality might have helped that study to get published, she says.</p><p>As long as researchers continue to seek publication in prestigious outlets, publication bias won’t go away, Goodman predicts. Still, he is surprised at how much progress has been made in the past decade: top-tier journals pledging to accept rigorous studies, regardless of outcome, would have been “unheard of” even five or ten years ago, he says.</p><p>Pilakouta now leads a laboratory and can set an example for her undergraduate and graduate students. But she’s also seen first-hand how deeply engrained the thirst for positive findings is. “It concerns me how early it starts,” she says. Next time she gets a null result, she says, she’s hopeful that it won’t take seven years to publish it.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Micromouse (133 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Micromouse</link>
            <guid>41055230</guid>
            <pubDate>Wed, 24 Jul 2024 09:36:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Micromouse">https://en.wikipedia.org/wiki/Micromouse</a>, See on <a href="https://news.ycombinator.com/item?id=41055230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Micromouse_maze.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Micromouse_maze.jpg/220px-Micromouse_maze.jpg" decoding="async" width="220" height="165" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Micromouse_maze.jpg/330px-Micromouse_maze.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Micromouse_maze.jpg/440px-Micromouse_maze.jpg 2x" data-file-width="4224" data-file-height="3168"></a><figcaption>Micromouse maze</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Micromouse_Green_Giant_V1.3.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Micromouse_Green_Giant_V1.3.jpg/220px-Micromouse_Green_Giant_V1.3.jpg" decoding="async" width="220" height="147" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Micromouse_Green_Giant_V1.3.jpg/330px-Micromouse_Green_Giant_V1.3.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Micromouse_Green_Giant_V1.3.jpg/440px-Micromouse_Green_Giant_V1.3.jpg 2x" data-file-width="1280" data-file-height="853"></a><figcaption>Micromouse robot</figcaption></figure>
<p><b>Micromouse</b> is an event where small <a href="https://en.wikipedia.org/wiki/Robot" title="Robot">robotic</a> mice compete to solve a 16×16 <a href="https://en.wikipedia.org/wiki/Maze" title="Maze">maze</a>. It began in the late 1970s.<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> Events are held worldwide, and are most popular in the <a href="https://en.wikipedia.org/wiki/United_Kingdom" title="United Kingdom">UK</a>, <a href="https://en.wikipedia.org/wiki/United_States" title="United States">U.S.</a>, <a href="https://en.wikipedia.org/wiki/Japan" title="Japan">Japan</a>, <a href="https://en.wikipedia.org/wiki/Singapore" title="Singapore">Singapore</a>, <a href="https://en.wikipedia.org/wiki/India" title="India">India</a>, <a href="https://en.wikipedia.org/wiki/South_Korea" title="South Korea">South Korea</a> and becoming popular in subcontinent countries such as <a href="https://en.wikipedia.org/wiki/Sri_Lanka" title="Sri Lanka">Sri Lanka</a>.
</p><p>The maze is made up of a 16×16 grid of cells, each 180&nbsp;mm square with walls 50&nbsp;mm high.<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> The mice are completely <a href="https://en.wikipedia.org/wiki/Autonomous_robot" title="Autonomous robot">autonomous robots</a> that must find their way from a predetermined starting position to the central area of the maze unaided. The mouse needs to keep track of where it is, discover walls as it explores, map out the maze and detect when it has reached the goal. Having reached the goal, the mouse will typically perform additional searches of the maze until it has found an optimal route from the start to the finish. Once the optimal route has been found, the mouse will traverse that route in the shortest achievable time.
</p><p>Competitions<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup><sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> and conferences<sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> are run regularly.
</p>
<meta property="mw:PageProp/toc">
<div><h2 id="Half-Size_Micromouse">Half-Size Micromouse</h2><p><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Micromouse&amp;action=edit&amp;section=1" title="Edit section: Half-Size Micromouse"><span>edit</span></a><span>]</span></span></p></div>
<p>A version of Micromouse called the Half-Size Micromouse was introduced at the 30th All Japan Micromouse Competition in 2009.<sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup><sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup> Instead of a 16×16 maze, the Half-Size competition uses up to a 32×32 maze. Cell and wall dimensions have been reduced by half,<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> providing a new challenge.
</p><p>There have been half-size competitions in Europe in Hungary in 2015<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> and the UK in 2018.<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p>


<p>Mice used in competitions employ the fundamental elements of <a href="https://en.wikipedia.org/wiki/Robot_navigation" title="Robot navigation">robot navigation</a>, including mapping, planning, and localization. Additionally, they optimize their path through the maze using various <a href="https://en.wikipedia.org/wiki/Search_algorithm" title="Search algorithm">search algorithms</a>. Common search algorithms use variations of the Bellman <a href="https://en.wikipedia.org/wiki/Flood_fill" title="Flood fill">flood-fill</a> method,<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> <a href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" title="Dijkstra's algorithm">Dijkstra's algorithm</a>, <a href="https://en.wikipedia.org/wiki/A*_search_algorithm" title="A* search algorithm">A* search algorithm</a>, among various <a href="https://en.wikipedia.org/wiki/Graph_traversal" title="Graph traversal">graph traversal</a> and <a href="https://en.wikipedia.org/wiki/Tree_traversal" title="Tree traversal">tree traversal</a> algorithms.
</p>

<p>Mice can run at over three meters per second, depending on the maze design. Some of the best micromouse builders are Yusuke Kato,<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> Ng Beng Kiat<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> and Fumitaka Nakashima.<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> The current world record is 3.921 seconds<sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> and is held by Ng Beng Kiat.
</p><p>Performance in recent years has improved considerably. As of 2015, winning mice are likely to run with forward acceleration and braking well over 1g.<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> Cornering with <a href="https://en.wikipedia.org/wiki/Acceleration" title="Acceleration">centripetal acceleration</a> as high as 2g is possible. Micromice are among the highest-performing autonomous robots.
</p><p>Most recently, robots are being equipped with a fan to create a partial vacuum under the mouse while it is running.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup><sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup><sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> The additional downforce available has made possible a huge improvement in performance. Compared to a non-fan mouse, the newer robots are likely to be able to achieve centripetal accelerations of 6g or more. Straight line accelerations can easily exceed 2.5g.
</p>

<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.micromouseonline.com/micromouse-book/history/">"History"</a>. <i>Micromouse Online</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Micromouse+Online&amp;rft.atitle=History&amp;rft_id=http%3A%2F%2Fwww.micromouseonline.com%2Fmicromouse-book%2Fhistory%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.cs.york.ac.uk/micromouse/Rules/Maze_Solver_Rules.pdf">"UK Micromouse Maze Solver Rules"</a> <span>(PDF)</span>. <i>University of York Department of Computer Science</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=University+of+York+Department+of+Computer+Science&amp;rft.atitle=UK+Micromouse+Maze+Solver+Rules&amp;rft_id=https%3A%2F%2Fwww.cs.york.ac.uk%2Fmicromouse%2FRules%2FMaze_Solver_Rules.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite><a rel="nofollow" href="http://micromouseusa.com/">"Micromouse USA - USA Micromouse Fans Site"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Micromouse+USA+-+USA+Micromouse+Fans+Site&amp;rft_id=http%3A%2F%2Fmicromouseusa.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite><a rel="nofollow" href="https://ukmars.org/index.php/Main_Page">"UK Micromouse and Robotics Society"</a>. <i>ukmars.org</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ukmars.org&amp;rft.atitle=UK+Micromouse+and+Robotics+Society&amp;rft_id=https%3A%2F%2Fukmars.org%2Findex.php%2FMain_Page&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://ukmars.org/index.php/Minos">"Minos - UK Micromouse and Robotics Society"</a>. <i>ukmars.org</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ukmars.org&amp;rft.atitle=Minos+-+UK+Micromouse+and+Robotics+Society&amp;rft_id=https%3A%2F%2Fukmars.org%2Findex.php%2FMinos&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFrobolaboN">robolaboN. <a rel="nofollow" href="https://www.youtube.com/watch?v=bszRuwK3yIs">"MicroMouse All Japan contest 2009 half size preliminary"</a>. <a rel="nofollow" href="https://ghostarchive.org/varchive/youtube/20211212/bszRuwK3yIs">Archived</a> from the original on 2021-12-12 – via YouTube.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=MicroMouse+All+Japan+contest+2009+half+size+preliminary&amp;rft.au=robolaboN&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DbszRuwK3yIs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.youtube.com/watch?v=aN5vYrrSdKQ"><i>Japan 2009 half-size micromouse contest final</i></a>. <i><a href="https://en.wikipedia.org/wiki/YouTube" title="YouTube">YouTube</a></i>. <a rel="nofollow" href="https://ghostarchive.org/varchive/youtube/20211210/aN5vYrrSdKQ">Archived</a> from the original on 2021-12-10.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Japan+2009+half-size+micromouse+contest+final&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaN5vYrrSdKQ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.ntf.or.jp/mouse/micromouse2010/rulehalf-EN.html">"NTF -New Technology Foundation-Micromouse2010"</a>. <i>www.ntf.or.jp</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.ntf.or.jp&amp;rft.atitle=NTF+-New+Technology+Foundation-Micromouse2010&amp;rft_id=http%3A%2F%2Fwww.ntf.or.jp%2Fmouse%2Fmicromouse2010%2Frulehalf-EN.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.pcbway.com/project/sponsor/The_first_Half_size_Micromouse_competition_in_Europe.html">"The first Half-size Micromouse competition in Europe- Sponsor - PCBWay"</a>. <i>www.pcbway.com</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.pcbway.com&amp;rft.atitle=The+first+Half-size+Micromouse+competition+in+Europe-+Sponsor+-+PCBWay&amp;rft_id=https%3A%2F%2Fwww.pcbway.com%2Fproject%2Fsponsor%2FThe_first_Half_size_Micromouse_competition_in_Europe.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><a rel="nofollow" href="https://www.youtube.com/watch?v=jsHbhUYqG0I"><span>UK Half size MicroMouse contest?????</span></a> on <a href="https://en.wikipedia.org/wiki/YouTube_video_(identifier)" title="YouTube video (identifier)">YouTube</a></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.micromouseonline.com/micromouse-book/mazes-and-maze-solving/solving-the-maze/#axzz1uapduejO">"Solving the maze"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Solving+the+maze&amp;rft_id=http%3A%2F%2Fwww.micromouseonline.com%2Fmicromouse-book%2Fmazes-and-maze-solving%2Fsolving-the-maze%2F%23axzz1uapduejO&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite><a rel="nofollow" href="http://blog.livedoor.jp/robolabo/">"ロボット工作研究室 - livedoor Blog（ブログ）"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%E3%83%AD%E3%83%9C%E3%83%83%E3%83%88%E5%B7%A5%E4%BD%9C%E7%A0%94%E7%A9%B6%E5%AE%A4+-+livedoor+Blog%EF%BC%88%E3%83%96%E3%83%AD%E3%82%B0%EF%BC%89&amp;rft_id=http%3A%2F%2Fblog.livedoor.jp%2Frobolabo%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite><a rel="nofollow" href="https://sites.google.com/site/ngbengkiat/">"Ng Beng Kiat"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Ng+Beng+Kiat&amp;rft_id=https%3A%2F%2Fsites.google.com%2Fsite%2Fngbengkiat%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20130605025335/http://homepage1.nifty.com/hfd01577/index.html">"第４実験室"</a>. Archived from <a rel="nofollow" href="http://homepage1.nifty.com/hfd01577/index.html">the original</a> on 2013-06-05<span>. Retrieved <span>2013-05-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%E7%AC%AC%EF%BC%94%E5%AE%9F%E9%A8%93%E5%AE%A4&amp;rft_id=http%3A%2F%2Fhomepage1.nifty.com%2Fhfd01577%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><cite><a rel="nofollow" href="https://spectrum.ieee.org/automaton/robotics/diy/meet-the-new-worlds-fastest-micromouse">"Meet the New World's Fastest Micromouse Robot"</a>. 21 November 2011.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Meet+the+New+World%27s+Fastest+Micromouse+Robot&amp;rft.date=2011-11-21&amp;rft_id=https%3A%2F%2Fspectrum.ieee.org%2Fautomaton%2Frobotics%2Fdiy%2Fmeet-the-new-worlds-fastest-micromouse&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFHarrison2017">Harrison, Peter (3 August 2017). <a rel="nofollow" href="http://www.micromouseonline.com/2017/08/03/micromouse-hard-acceleration/">"Micromouse Hard Acceleration"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Micromouse+Hard+Acceleration&amp;rft.date=2017-08-03&amp;rft.aulast=Harrison&amp;rft.aufirst=Peter&amp;rft_id=http%3A%2F%2Fwww.micromouseonline.com%2F2017%2F08%2F03%2Fmicromouse-hard-acceleration%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFHarrison2017">Harrison, Peter (10 October 2017). <a rel="nofollow" href="http://www.micromouseonline.com/2017/10/11/taiwan-micromouse-intelligent-robot-contest-2017/">"Taiwan Micromouse Contest 2017"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Taiwan+Micromouse+Contest+2017&amp;rft.date=2017-10-10&amp;rft.aulast=Harrison&amp;rft.aufirst=Peter&amp;rft_id=http%3A%2F%2Fwww.micromouseonline.com%2F2017%2F10%2F11%2Ftaiwan-micromouse-intelligent-robot-contest-2017%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFHarrison2018">Harrison, Peter (18 February 2018). <a rel="nofollow" href="http://www.micromouseonline.com/2018/02/18/more-suck-less-slip/">"More suck, less slip"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=More+suck%2C+less+slip&amp;rft.date=2018-02-18&amp;rft.aulast=Harrison&amp;rft.aufirst=Peter&amp;rft_id=http%3A%2F%2Fwww.micromouseonline.com%2F2018%2F02%2F18%2Fmore-suck-less-slip%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFBy2008">By (27 November 2008). <a rel="nofollow" href="https://hackaday.com/2008/11/26/vacuum-micromouse/">"Vacuum micromouse"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Vacuum+micromouse&amp;rft.date=2008-11-27&amp;rft.au=By&amp;rft_id=https%3A%2F%2Fhackaday.com%2F2008%2F11%2F26%2Fvacuum-micromouse%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMicromouse"></span></span>
</li>
</ol></div>

<ul><li><a rel="nofollow" href="https://www.youtube.com/watch?v=ZMQbHMgK2rw"><span>The Fastest Maze-Solving Competition On Earth</span></a> on <a href="https://en.wikipedia.org/wiki/YouTube_video_(identifier)" title="YouTube video (identifier)">YouTube</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw‐api‐int.eqiad.main‐ccc4f6dcd‐5hkxv
Cached time: 20240724102302
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.445 seconds
Real time usage: 0.534 seconds
Preprocessor visited node count: 1177/1000000
Post‐expand include size: 26213/2097152 bytes
Template argument size: 1220/2097152 bytes
Highest expansion depth: 18/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 62178/5000000 bytes
Lua time usage: 0.288/10.000 seconds
Lua memory usage: 4940426/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  480.738      1 -total
 68.47%  329.153      1 Template:Reflist
 51.26%  246.437     17 Template:Cite_web
 20.41%   98.112      1 Template:Short_description
 11.96%   57.512      2 Template:Pagetype
  9.27%   44.556      2 Template:YouTube
  5.87%   28.237      1 Template:See_also
  4.85%   23.309      5 Template:Main_other
  4.09%   19.682      1 Template:SDcat
  4.04%   19.443      2 Template:Replace
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:266014-0!canonical and timestamp 20240724102302 and revision id 1236374847. Rendering was triggered because: api-parse
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Doors" in Solaris: Lightweight RPC Using File Descriptors (1996) (105 pts)]]></title>
            <link>http://www.kohala.com/start/papers.others/doors.html</link>
            <guid>41053761</guid>
            <pubDate>Wed, 24 Jul 2024 05:02:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.kohala.com/start/papers.others/doors.html">http://www.kohala.com/start/papers.others/doors.html</a>, See on <a href="https://news.ycombinator.com/item?id=41053761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<img src="http://www.kohala.com/start/gifs/sun_masthead.art.gif"><br clear="RIGHT">
<img src="http://www.kohala.com/start/gifs/sun_tch_fcs.gif" width="187" height="65">

<h2>"Doors" in Solaris<sup><small>TM</small></sup>: Lightweight RPC using File Descriptors</h2>	

<blockquote>
<b>Jim Voll<br>
Senior Staff Engineer<br>
Solaris Products Group</b>
</blockquote>

<em>This new lightweight RPC mechanism, adapted from Sun's Spring O/S
project, will be inforporated into a future release of the
Solaris<sup><small>TM</small></sup> operating environment.  Developers
can refer to Solaris documentation for additional library and SPI
information.</em>

<h3>An Overview of Doors</h3>

<p>
A door is a "file" descriptor used to describe a procedure in a process
and optionally some additional state associated with the procedure.
Doors were initially designed as Spring object descriptors
used to encapsulate the state and capabilities of C++ objects. A
process which obtains a door is free to pass it, along with its
capabilities, to other processes in the system.
</p><p>
A server normally creates a door for some service it plans on
providing, and exports it to clients. Clients who obtain the door may
then invoke the service associated with the door using the synchronous
RPC semantics of a door <em>call</em> operation.
</p><p>
Conceptually, during a door invocation the client thread that issues
the door procedure call migrates to the server process associated with the door, and starts executing the procedure while in the address space of the server. When the service procedure is finished, a door <em>return</em> operation is performed and the thread migrates back to the client's address space with the results, if any, from the procedure call.
</p><p>
A client may pass data, including other doors, as arguments during a
door invocation, and the server procedure may return data, including
other doors, as results from an invocation.
</p><p>
Normally each door invocation from a client results in an active thread executing in the server (a server may choose to limit the number of threads available for incoming door invocations). The server must therefore be a MT-safe application. The doors interface provides a convenient mechanism for enabling high performance multithreaded programs since the creation and dispatching of threads is handled by the interface based on load.

</p><h3>Doors as File Descriptors</h3>

Using file descriptors to encapsulate doors allows many existing
UNIX<sup><small>(R)</small></sup> paradigms to be easily adopted for
use with doors.  Inventing a new name space for doors was widely viewed
as poor design alternative (e.g SVR4 IPC) and would have resulted in a
duplication of functionality already provided in UNIX.
<p>
File descriptors also provide a secure mechanism to encapsulate a door since the state associated with the door (procedure, process) is kept in the kernel. A user cannot construct a fake file descriptor to use as a door.
</p><p>
Some of the other important capabilities of file descriptors and how
they related to doors include the naming facility available with
<em>fattach(3c)</em>.

</p><h3>Naming</h3>

The <em>namefs</em> facility in SVR4 allows a process to bind a
STREAMS-based file descriptor to an object in filesystem name space
using <em>fattach(3)</em>. A logical extension to this facility allows
a door to be named in a similar manner (see Figure 1).
<p>
The protection mode of the file associated with a named door does not
provide strict access protection since a client may obtain access to
the door through means other than open(). (Example: A client is free to
pass a door to another process with weaker credentials.) This is also
true of STREAMS which are named via <em>fattach()</em> (see Figure 1).
</p><p>

<br>
<img src="http://www.kohala.com/start/gifs/sun_doors.fig1.gif">

FIGURE 1. Named Doors

</p><h3>Doors Implementation</h3>

The doors interface is exported from a user level shared library. The
library also controls the default server threads that are created in
response to incoming door requests on the server.
<p>
A new synchronization object, called a <em>shuttle</em>, was engineered to encapsulate the state associated with a door invocation. A shuttle provides the necessary state for process control such as signals and procfs operations, while allowing a direct scheduler hand-off operation between two threads.
</p><p>
Rather than placing a client thread on a sleep queue, signaling the
server and asking the scheduler to pick another thread to run, a
shuttle marks the current thread as sleeping, marks the server thread as running, and passes control directly to the server thread.

</p><h3>Server Threads</h3>

Server threads are normally created on demand by the doors library.
These server threads are created using the Solaris threads library with bound threads (each server thread runs on its own lightweight
process).

The first server thread is created automatically when the server issues a door <em>create</em> call. The server thread will inherit the scheduling class, and signal disposition of the thread issuing the door create. Door server threads use the Solaris thread library's default stack size. Once created, a server thread will place itself in a pool of
available threads and wait for a door invocation. New server threads
are created on demand when the available pool is depleted.

<h3>Benchmark Results</h3>

A simple benchmark was used to compare doors in Solaris 2.5 to other existing UNIX IPC mechanisms. The benchmark measures the round trip time associated with transferring control from a client thread in one process, to a server thread in another, and then back again. In the case of pipes and SVR4 messages, a single byte of data is passed.
<p>
<b>TABLE 1. SPARCstation 10<sup><small>TM</small></sup> (dual processor, 40 MHz SuperSPARC<sup><small>TM</small></sup> processors)</b>
</p><p>

<table>
<tbody><tr>
<td><b>IPC Mechanism</b><br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td> <b>usecs</b><br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>
<tr>
<td>Doors<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td>66<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>
<tr>
<td>SVR4 Semaphores<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td>142<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>
<tr>
<td>Pipes<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td>175<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>
<tr>
<td>SVR4 Messages<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td>270<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>
<tr>
<td>ONC-RPC<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td><td>1020<br>
<img src="http://www.kohala.com/start/papers.others/black.gif" width="150" height="1"></td></tr>


</tbody></table>

</p><h3>Door Uses</h3>

A name service cache will be implemented in future Solaris versions, similar in nature to the DCE CDS clerk. A daemon process runs on the client machine and caches information from the configured name service such as files, NIS, or NIS+. The name service request interfaces, such as <em>getpwnam()</em>, communicate with the local name service cache daemon using a door named <em>fattach()</em>. Although most of the performance advantage of the name service cache is due to avoiding the setup and tear down cost associated with communicating with the name server (the connection to the name server is kept open by the daemon), doors do provide the fastest RPC mechanism for moving the results from daemon to the requestor. The automatic multithreaded dispatching provided by the doors interface makes a high performance implementation straightforward.

<h3>Future Work at SunSoft</h3>

To date, most of the performance work on Solaris doors concerns
transferring control from one address space to another. Future work
will optimize the transfer of data through such techniques as
copy-on-write transfers. Plans are also underway to use doors as a
general upcall mechanism in the kernel, enabling efficient
implementations of work such as scheduler activations and user level
file systems based on the <em>vnode</em> interface.
<p>
Initially developed as Spring object descriptors, doors are being used
as a part of a future high performance CORBA compliant ORB
implementation. As part of this work, a remote proxy service is being
investigated to allow the doors programming model to extend over the network. Doors can also be used as a local transport mechanism for ONC-RPC.
</p><p>
For more information, see:<br>
<a href="http://www.sun.com/tech/projects/spring/"><em>http://www.sun.com/tech/projects/spring/</em></a>.
</p><hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MPPP – The first 'designer drug' disaster (2023) (144 pts)]]></title>
            <link>https://www.chm.bris.ac.uk/motm/mppp/mppph.htm</link>
            <guid>41053383</guid>
            <pubDate>Wed, 24 Jul 2024 03:32:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chm.bris.ac.uk/motm/mppp/mppph.htm">https://www.chm.bris.ac.uk/motm/mppp/mppph.htm</a>, See on <a href="https://news.ycombinator.com/item?id=41053383">Hacker News</a></p>
<div id="readability-page-1" class="page">

<table id="centre">
<tbody><tr>
<td>
<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="400" height="3" alt=""></p> 

<h3>1-methyl-4-phenyl-4-propionoxypiperidine,<br>or Desmethylprodine or ‘reversed meperidine’</h3>

<h3>The first ‘designer drug’ disaster</h3>

<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="400" height="3" alt=""></p> 

<p id="centre"><a href="mailto:s.cotton@bham.ac.uk">Simon Cotton</a><br>
<a href="http://www.birmingham.ac.uk/schools/chemistry/index.aspx">University of Birmingham</a></p>

<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="400" height="3" alt=""></p> 

<p id="centre">Molecule of the Month August 2023<br><span>Also available: <a href="https://www.chm.bris.ac.uk/motm/mppp/mpppjs.htm">JSMol</a> version.</span></p>

<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="400" height="3" alt=""></p> 

</td><td><img src="https://www.chm.bris.ac.uk/motm/mppp/structure.gif" width="362" height="354" alt="Structure of MPPP" title="Structure of MPPP"></td>

</tr>
</tbody></table>

<h2><img src="https://www.chm.bris.ac.uk/motm/mppp/dd.jpg" width="350" height="" alt="Designer drugs" title="Designer drugs">What is a designer drug?</h2>

<p>The origin of the term is credited to Dr Gary L. Henderson, of the University of California at Davis. A designer drug is based on the structure of an existing drug – which may be naturally sourced from a plant (like <a href="http://www.chm.bris.ac.uk/motm/cocaine/cocaineh.htm">cocaine</a> or <a href="http://www.chm.bris.ac.uk/motm/morphine/Morphine.htm">morphine</a>) or be synthetic (like <a href="http://www.chm.bris.ac.uk/motm/methamphetamine/methh.htm">amphetamine</a>) - but with a slightly different structure (often done by changing a side-chain) designed to make it undetectable in routine drug tests. These slight changes to its structure also change the properties of the drug in the body, sometimes dangerously. Thus, recently this has meant the ‘bath salts’ such as mephedrone in place of cathinone (khat), with amphetamine-like effects, or the ‘spice’ drugs, which affect the ‘<a href="http://www.chm.bris.ac.uk/motm/cannabidiol/cannabidiolh.htm">cannabinoid</a>’ receptor and may produce similar symptoms to marijuana.</p>

<h2>So how long has this ‘designer drug’ been around?</h2>

<p>What we call MPPP was first reported in the <i>Journal of Organic Chemistry</i> in 1947 by Albert Ziering and John Lee of the pharmaceutical company Hofmann-La Roche. They were trying to make better painkillers, though MPPP turned out to be no better than existing ones. They tested MPPP on lab rats and found that it seemed to be safe. To make it, you first react 1-methyl-4-piperidone with phenyllithium, then esterify the resulting alcohol with propionic anhydride.</p> 

<p id="centre"><img src="https://www.chm.bris.ac.uk/motm/mppp/synthesis.gif" width="944" height="350" alt="Synthesis of MPPP" title="Synthesis of MPPP"></p>

<h2>Why is it called reversed meperidine?</h2>

<p>In comparison with the meperidine (a.k.a. pethidine or Demerol, the first synthetic opioid painkiller, originally reported in 1939), the ester group in MPPP is the other way round – it is an isomer of meperidine.</p>

<p id="centre"><img src="https://www.chm.bris.ac.uk/motm/mppp/reversed.gif" width="760" height="350" alt="MPPP and meperidene" title="MPPP and meperidene"></p>

<h2>So who did find out the nasty side to the molecule?</h2>

<p>A young chemistry graduate named Barry Kidston suddenly developed Parkinson’s disease in 1976, when he was only 23. For nearly a decade, he had been abusing a wide range of drugs, such as marijuana, amphetamines and barbiturates, before finally focussing upon opiates like meperidine and codeine. During the summer of 1976 he discovered the 1947 report of MPPP and decided to try that, successfully making it and testing it on himself, finding that it had an opiate-like ‘high’. He then repeated this synthesis several times. Whether he got over-confident in his ability or not, he seems to have taken some shortcuts in the synthesis and in 1976 developed a state of ‘muteness, severe rigidity, weakness, tremor, flat facial expression, and altered sensorium’, being unable to speak. He was admitted to a psychiatric ward.</p>

<h2>So what happened next?</h2>

<p>A number of treatments were tried. After treatments with a number of drugs including <i>levodopa</i> (a drug which the body converts into dopamine, used to treat Parkinson’s disease), his condition improved, and he received a number of medications until in September 1978 he was found dead from an overdose of cocaine and codeine, having  continued on a path of drug abuse. Post-mortem examination of the brain revealed major destruction of nerve cells in the <i>substantia nigra</i> region, something also observed with Parkinson’s patients. They examined his laboratory in 1976 and found not just the MPPP but also an impurity formed at high temperatures, MPTP.  This was tested on rats (and also on hamsters and guinea pigs), but without revealing any Parkinson’s symptom, so the cause remained a mystery. The medics studying the case wrote it up for publication in a journal called <i>Psychiatry Research</i>, and the medical world largely ignored it for a few years.</p>

<p id="centre"><a href="https://www.chm.bris.ac.uk/motm/mppp/mptp.mol"><img src="https://www.chm.bris.ac.uk/motm/mppp/mptp.gif" width="400" height="132" alt="MPTP" title="MPTP"></a><br>
MPTP</p>

<table>
<tbody><tr>
<td id="centre"><img src="https://www.chm.bris.ac.uk/motm/mppp/langston.jpg" width="250" height="297" alt="Dr J. William Langston" title="Dr J. William Langston"><br>
Dr J. William Langston<br>
<small>[Photo: <a href="https://www.journalofparkinsonsdisease.com/blog/neuroscientists_corner/profile-j-william-langston"><i>J. Parkinson's Dis</i>.</a>]</small></td>

<td><h2>So what changed?</h2>

<p>In 1982, Dr J. William Langston, the Director of Neurology of the Santa Clara Valley Medical Centre in San José, was called in to examine George Carillo, a 42-year-old heroin addict who had just been admitted into the centre’s locked psychiatry unit. Carillo was ‘frozen’ – he could not move or talk. They spotted limited finger movement and after a while he was able to write very slowly: ‘I’m not sure what is happening to me. I only know I can’t function normally. I can’t move right.’ These symptoms of Parkinson’s disease had suddenly come on, over just a few weeks. And then he said he’d been taking heroin. The man’s 30-year-old girlfriend Juanita had the same symptoms. This puzzled Langston. People in their 30s and 40s were not supposed to get Parkinson’s. Certainly not from taking heroin. Then Langston found out about Bill and David, a young pair of brothers some 30 miles away in Santa Cruz, who had taken a ‘synthetic heroin’ and displayed similar ‘frozen’ symptoms. Through publicising these four cases, they learned of a drug dealer named Toby and his 21-year-old girlfriend Connie, with exactly the same history. At that point, someone recalled the article that had appeared in <i>Psychiatry Research</i>.</p></td>
<td id="centre"><img src="https://www.chm.bris.ac.uk/motm/mppp/carillo.jpg" width="241" height="350" alt="George Carillo" title="George Carillo"><br>
George Carillo - a 'frozen addict'</td>
</tr>
</tbody></table>

<h2>And?</h2>

<p>Samples of the drugs which they had taken were sent for analysis, and found that what was supposed to be MPPP was largely made up of the impurity MPTP. Animal testing was extended to monkeys, and it was found that MPTP selectively kills cells in the <i>substantia nigra</i>, an area of the brain important in areas like motor control and learning, in rhesus and squirrel monkeys, just as in humans, so it affects the brains of primates in a different way to other animals.</p>

<p>Eventually George, Juanita and Connie were able to receive an experimental surgery in Sweden, during which they underwent transplanting <i>substantia nigra</i> cells from aborted foetuses into the damaged parts of their brains, and this helped them recover much of their motor function.</p>

<h2>How had this happened?</h2>

<p>Kidston seemed to have taken some short cuts in his synthesis in 1976, as had the West-Coast entrepreneur in 1982. If you try to carry out the esterification to make MPPP at too high a temperature, or too low a pH, an elimination reaction occurs. When Langston followed up the original 1947 synthesis of MPPP by going to the library of Stanford University, he found that the paper by Ziering and Lee had been cut out from the volume of <i>Journal of Organic Chemistry</i> with a razor blade. Obviously the chemist making this ‘new heroin’ did not want others in on the game.</p>

<p id="centre"><img src="https://www.chm.bris.ac.uk/motm/mppp/mppp-plus.gif" width="900" height="588" alt="Synthesis of MPP+" title="Synthesis of MPP+"></p>

<p>MPPP contains a tertiary carbon, which readily eliminates a carboxylate (a good ‘leaving group’) to generate a tertiary carbocation; this in turn loses H<sup>+</sup> and is converted into 1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine (MPTP). MPTP can cross the blood-brain barrier and, once in the brain, is mistaken for dopamine by monoamine oxidase (MAO) enzymes. Thus MPTP undergoes oxidation, forming MPP<sup>+</sup>, the species which is responsible for the neuronal damage.</p>

<h2>Did any good come from this?</h2>

<p>It has led to some new understandings about possible causes of Parkinson’s disease. For one thing, people spotted that MPP<sup>+</sup> had a similar structure to the herbicide <i>Paraquat</i>, and scientists are working on a link between Parkinson’s and herbicide use.</p> 



<p>But - above all - the story of MPPP is another warning, a horrible one, about the dangers of dabbling in the wrong sort of chemistry.</p> 

<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="400" height="3" alt=""></p> 

<h2>Bibliography</h2>

<ul>
<li>A. Ziering and J. Lee, <i>J. Org. Chem</i>., 1947, <b>12</b>, 911-914 (synthesis of MPPP).</li>
<li>G. C. Davis, A. C. Williams, S. P. Markey, M. H. Ebert, E. D. Caine, C. M. Reichert and I. J. Kopin, 
<i>Psychiatry Research</i>, 1979, <b>1</b>, 249-254 (Barry Kidston diagnosed).</li>
<li>J. W. Langston, P. Ballard, J. W. Tetrud and I. Irwin, <i>Science</i>, 1983, 
<b>219</b>, 979-980 (chronic Parkinsonism in humans due to a product of meperidine-analogue synthesis).</li>
<li>R. S. Burns, C. C. Chieh, S. P. Markey, M. H., Ebert, D. M. Jacobowitz and I. J. Kopin, 
<i>Proc. Natl. Acad. Sci. USA</i>, 1983, <b>80</b>, 4546-4550 (testing MPTP in rhesus monkeys).</li>
<li>J. W. Langston, I Irwin, E. B. Langston and L. S. Forno, <i>Science</i>, 1984, 
<b>225</b>, 1480-1482 (pargyline prevents MPTP-induced Parkinsonism in primates).</li>
<li>J. W. Langston, I. Irwin, E. B. Langston and L. S. Forno, <i>Neurosci. Lett</i>., 1984, 
<b>48</b>, 87-92. (MPP<sup>+</sup> as a toxin selective to the <i>substantia nigra</i>).</li>
<li>J. N. Johannessen and S. P. Markey, <i>Drug and Alcohol Dependence</i>, 1984, 
<b>13</b>, 367-374 (analysis of MPTP/MPPP mixture).</li>
<li>P. A. Ballard, J. W. Tetrud and J. W. Langston, <i>Neurology</i>, 1985, <b>35</b>, 949-956. (human Parkinsonism due to MPTP).</li>
<li>R. M. Baum, <i>Chem. Eng. News</i>, 1985, <b>63</b>, 7–16 (designer drugs)</li>
<li>I. J. Kopin, <i>Environmental Health Perspectives</i>, 1987, <b>75</b>, 45-51 (MPTP and Parkinson’s disease).</li>
<li>G. L. Henderson, <i>J. Forensic Sci</i>., 1988, <b>33</b>, 569-575 (designer drugs)</li>
<li>H. Widner, J. Tetrud, S. Rehncrona, B. Snow, P. Brundin, B. Gustavii, A. Björklund, O. Lindvall and J. W. Langston, <i>N. Engl. J. Med</i>., 1992, <b>327</b>, 1556-1563 (transplantation of fetal dopaminergic neurons as a treatment for MPTP-induced Parkinsonism).</li>
<li>D. M. Perrine, <i>The Chemistry of Mind-Altering Drugs</i>, Washington DC, American Chemical Society, 1996, pp 78-81. (MPPP and the MPTP story).</li>
<li>J. W. Langston and J. Palfreman, <i>The case of the frozen addicts: how the solution of an extraordinary medical mystery spawned a revolution in the understanding and treatment of Parkinson's disease</i>, New York, Pantheon, 1996.</li>
<li>C. M. Tanner, F. Kamel <i>et al., Environ. Health Perspect.</i>, 2011, <b>119</b>, 866-872 (Rotenone, Paraquat, and Parkinson’s Disease).</li>
<li>F. Kamel, <i>Science</i>, 2013, <b>341</b>, 722-723 (herbicide-Parkinson’s link).</li>
<li>J. W. Langston, <i>Journal of Parkinson’s Disease</i>, 2017, <b>7</b>, S11–S22 (MPPP and the MPTP story)</li>
<li>C. Vaccari, R. El Dib. and J. L. V. de Camargo, <i>Systematic Reviews</i>, 2017, <b>6</b>, 98 (Paraquat and Parkinson’s disease: a systematic review protocol).</li>
<li><a href="https://neuwritesd.org/2016/08/18/mind-your-ps-and-ts-how-tainted-drugs-revolutionized-parkinsons-research/">How tainted drugs revolutionized Parkinsons research</a></li> 
<li><a href="https://www.journalofparkinsonsdisease.com/blog/neuroscientists_corner/profile-j-william-langston">Profile of Dr William Langston</a></li>
<li><a href="https://www.scientificamerican.com/article/parkinsons-disease-and-pesticides-whats-the-connection/">Parkinsons disease and pesticides - what's the connection?</a></li>
</ul>

<p><img src="https://www.chm.bris.ac.uk/clrline.gif" width="560" height="3" alt=""></p> 

<p><img src="https://www.chm.bris.ac.uk/cgi-bin/Count.cgi?df=mppph.dat|md=7|pad=Y|dd=D" alt="counter"><img src="https://www.chm.bris.ac.uk/backto.gif" width="29" height="29" alt=""> <a href="https://www.chm.bris.ac.uk/motm/motm.htm">Back to Molecule of the Month page</a>.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[DOI:<a href="http://dx.doi.org/10.6084/m9.figshare.22013048">10.6084/m9.figshare.22013048</a>]</p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[What "consent" looks like for the DEA and TSA (219 pts)]]></title>
            <link>https://papersplease.org/wp/2024/07/23/what-consent-really-looks-like-for-the-dea-and-tsa/</link>
            <guid>41053329</guid>
            <pubDate>Wed, 24 Jul 2024 03:16:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://papersplease.org/wp/2024/07/23/what-consent-really-looks-like-for-the-dea-and-tsa/">https://papersplease.org/wp/2024/07/23/what-consent-really-looks-like-for-the-dea-and-tsa/</a>, See on <a href="https://news.ycombinator.com/item?id=41053329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary" role="main">	
			
<article id="post-18773">
			
	
	<p><span>Jul</span>
		<span>23</span>
		<span>2024</span>
	</p>
		
	<div>
		<p>The Drug Enforcement Agency (DEA) and the Transportation Security Administration (TSA) have been <a href="https://papersplease.org/wp/2015/05/01/secondary-inspection-used-as-pretext-for-airport-drug-searches/">working together</a> for years to steal travelers’ money.</p>
<p>The DEA <a href="https://papersplease.org/wp/2016/08/10/dea-recruits-airline-travel-industry-staff-to-inform-on-travelers/">pays informers</a> to finger people who might be flying with large amounts of cash, and gets the TSA to identify these people when they go through TSA checkpoints at airports, claims that they “consent” to be searched, and then finds any money they are carrying and seizes it through “civil forfeiture”.</p>
<p>The DEA carries out <a href="https://papersplease.org/wp/2016/10/03/how-the-dea-uses-travel-company-spies-to-confiscate-travelers-cash/">similar cash-seizure operations on Amtrak trains</a> — mostly domestic trains that don’t cross the US border — in collaboration with US Customs and Border Protection (CBP).</p>
<p>A new <a href="https://www.youtube.com/watch?v=0XBzV0bDZdQ">video</a> released by the <a href="https://ij.org/press-release/new-institute-for-justice-video-exposes-unconstitutional-airport-interdiction-tactic">Institute for Justice</a> shows how this “consent” works in practice.</p>
<p>In the w <a href="https://www.youtube.com/watch?v=0XBzV0bDZdQ">video</a>, a DEA agent won’t take “<a href="https://www.youtube.com/watch?v=0XBzV0bDZdQ">I don’t consent to a search</a>” for an answer. The agent follows an airline passenger onto their plane (without objection by airline staff), snatches the passenger’s carry-on bag, carries it off the plane, and refuses to return it. The agent claims the right to keep the passenger’s bag as long as it takes to get a warrant (although they don’t have that right, and don’t actually get a warrant).</p>
<p>This is not meaningful “consent”, and it’s not a valid legal basis for a search.</p>
<p>An ongoing <a href="https://www.courtlistener.com/docket/16702479/brown-v-transportation-security-adminstration/">class-action lawsuit</a> by the <a href="https://ij.org/case/dea-tsa-forfeitures/">Institute for Justice</a> on behalf&nbsp; of air travelers who have been searched without probable cause on the pretextual claim of “consent” in order to find, seize, and “forfeit” their cash has shown just how common this pattern of illegal search and seizure is.</p>
<p>We reported on the <a href="https://papersplease.org/wp/2020/01/17/is-the-tsa-screening-for-threats-to-aviation-or-for-cash-and-drugs/">filing&nbsp; of this lawsuit</a> in 2020, and on the <a href="https://papersplease.org/wp/2021/04/05/can-tsa-checkpoints-be-used-as-a-general-law-enforcement-dragnet/">first substantive ruling</a> in the case, in favor of the plaintiffs and allowing the case to move forward, in 2021.</p>
<p>Since then, the case has <a href="https://www.courtlistener.com/docket/16702479/brown-v-transportation-security-adminstration/">bogged down</a> in foot-dragging by the DEA and TSA, <a href="https://storage.courtlistener.com/recap/gov.uscourts.pawd.263087/gov.uscourts.pawd.263087.127.0.pdf">resisting discovery</a> of their records of&nbsp; searches and seizures of cash from travelers at airports.</p>
<p>The DEA and TSA continue to claim — despite the <a href="https://papersplease.org/wp/2021/04/05/can-tsa-checkpoints-be-used-as-a-general-law-enforcement-dragnet/">initial ruling against them</a> on this point —&nbsp; that they don’t have an actionable “policy” of targeting travelers with cash for searches because they haven’t put this policy in writing. But the latest <a href="https://storage.courtlistener.com/recap/gov.uscourts.pawd.263087/gov.uscourts.pawd.263087.127.0.pdf">status report</a> on discovery to date indicates that the DEA and TSA have made thousands of seizures of “bulk currency” from air travelers in recent years. This is clearly a routine and officially sanctioned agency practice, whether or not anyone has put it in writing.</p>
<p>The DEA and TSA claim that the volume of records of these searches and seizures would make producing them unduly burdensome. But the volume of these records is symptomatic of the scale and systemic nature of the problem — which is what the plaintiffs are trying to prove. The plaintiffs have suggested examining a statistical sample of the records of airport searches and seizures, but the DEA and TSA are resisting even that.</p>
<p>We wish the plaintiffs in this case and their lawyers success in their pursuit of justice for travelers.</p>
			</div>
</article><!-- #post-18773 -->
	<!-- #nav-below -->
	

	<!-- #comments .comments-area -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama 3.1 in C (182 pts)]]></title>
            <link>https://github.com/trholding/llama2.c/blob/master/runq.c</link>
            <guid>41053201</guid>
            <pubDate>Wed, 24 Jul 2024 02:49:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trholding/llama2.c/blob/master/runq.c">https://github.com/trholding/llama2.c/blob/master/runq.c</a>, See on <a href="https://news.ycombinator.com/item?id=41053201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Copilot&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>

                  <li>
      
      <div>
              <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Resources&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Resources&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Resources&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Resources&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Enterprise&quot;,&quot;action&quot;:&quot;click to go to Enterprise platform&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise platform;&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:trholding/llama2.c" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="IZvnxnjD4Vd_v2KOcFzS-gPxR3zfAwmBnem7joVbuavmz6NE-6R4sBLm4nC9Kz-DNtEsKAG-NJQkk8S__2CfZg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="trholding/llama2.c" data-current-org="" data-current-owner="trholding" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=trholding%2Fllama2.c" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/trholding/llama2.c/blob/master/runq.c&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="aa82c133db6d523ea13008f4930b35ea9001ce8a896581a81164d1b902248a90" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We made glhf.chat – run almost any open-source LLM, including 405B (152 pts)]]></title>
            <link>https://glhf.chat/landing/home</link>
            <guid>41052934</guid>
            <pubDate>Wed, 24 Jul 2024 01:52:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glhf.chat/landing/home">https://glhf.chat/landing/home</a>, See on <a href="https://news.ycombinator.com/item?id=41052934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Run<br> <!-- -->(almost)<br> <!-- -->any<br> <!-- -->language<br> <!-- -->model</h2><div><p>We use</p><!-- --> <p><a href="https://docs.vllm.ai/en/stable/">vLLM</a></p><!-- --><p>and a custom-built, autoscaling GPU scheduler to run</p><!-- --> <p><span>(almost)</span></p><!-- --><p>any open-source large language model for you: just paste a link to the Hugging Face repo. You can use our chat UI, or our OpenAI-compatible API. We'll let you use up to eight Nvidia A100 80Gb GPUs.</p></div><div><p>Works with any full-weight or 4-bit AWQ repo on Hugging Face that vLLM supports, including:</p><ul><li>Meta Llama 3.1 405b Instruct (and 70b, and 8b)</li><li>Qwen 2 72b</li><li>Mixtral 8x22b</li><li>Gemma 2 27b</li><li>Deepseek V2 Coder Lite (support for the full model is in the works)</li><li>Phi-3</li></ul><p>And many more. We'll run full-weight finetunes as well, like those from Nous Research or uncensored anti-refusal abliterated models.</p></div><p>For the most popular models, we proxy to always-on inference providers for you automatically. For the more bespoke models, we'll spin up a cluster for you on-demand, and spin it down once you're done using it.</p><p>It's free during the beta period, while we work out the kinks and figure out how to price it. Once the beta is over, we expect to significantly beat pricing of the major cloud GPU vendors due to our ability to run the models multi-tenant.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Origin of Emacs in 1976 (191 pts)]]></title>
            <link>https://onlisp.co.uk/On-the-Origin-of-Emacs-in-1976.html</link>
            <guid>41052593</guid>
            <pubDate>Wed, 24 Jul 2024 00:49:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onlisp.co.uk/On-the-Origin-of-Emacs-in-1976.html">https://onlisp.co.uk/On-the-Origin-of-Emacs-in-1976.html</a>, See on <a href="https://news.ycombinator.com/item?id=41052593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="On-the-Origin-of-Emacs-in-1976">

<hr>


<p>(Date: 23 Jul 2024)
</p>
<p>Summary:
EMACS was developed at the MIT AI Lab in 1976.  The specifics of the
origin have been documented by different people in various places.
</p>
<p>There is an interesting thread which was discussed on the blog of the
late Dan Weinreb, and preserved by archive.org.  Ultimately, Guy
Steele pulled up his records (in the form of printed emails).  The
below is an extract which is of historical interest and includes
emails from the first couple months of Emacs in 1976.  I quote some
sections and include the verbatim text at the bottom, which starts
with an ITS email from RMS to GLS.
</p>
<p>Dan Weinreb introduces
</p><blockquote>
<p>And now, here’s the mail from Guy Steele. I think this is the
best information we are ever going to get, and that this is
the last word on the topic.
</p></blockquote>


<p>The summary from Guy Steele is:-
</p><blockquote>
<p>My conclusions: (1) Clearly, by the end of 1976 and thereafter, RMS was
doing the bulk of EMACS development work, but it was not an “overnight”
takeover. For a period of seven weeks, anyway, he had some implementation
help from others (at least GLS, MOON, and JLK), and certainly had help
with design and debugging from these and others (DLW, EAK, ED).
He may have become the “principal hacker” more quickly than that,
however, perhaps in the space of a week or less; but remember that
in the AI lab culture, what I here call “principal hacker” (not a term used
at the time) was a “first among equals”, not an exclusive owner.
</p>
<p>(2) Moon’s involvement was not “hidden”.
</p>
<p>(3) RMS is responsible for the names “E” and “EMACS”.
RMS still deserves 99% or 99.9% or 99.99% or 99.999% of the credit
for taking a package of TECO macros and turning it into the most
powerful editor on the planet, twice (first in TECO and then with ELISP),
pouring in enormous amounts of effort and creativity over many years.
He also deserves credit for working with the early user community to
work out the initial set of key bindings and command names. I don’t
think RMS has any reason to deny the people who helped him out during
the first few months their due share of credit. They gave of their time
and creativity freely, in the best spirit of contributing to the community.
</p>
<p>–GLS
</p></blockquote>



<p>The sources are:
Beginning of thread
<a href="https://web.archive.org/web/20121107150708/http://danweinreb.org/blog/rebuttal-to-stallmans-story-about-the-formation-of-symbolics-and-lmi#comments">https://web.archive.org/web/20121107150708/http://danweinreb.org/blog/rebuttal-to-stallmans-story-about-the-formation-of-symbolics-and-lmi#comments</a>
<a href="https://web.archive.org/web/20121107150522/http://danweinreb.org/blog/why-did-symbolics-fail">why did symbolics fail</a>
<a href="https://web.archive.org/web/20121107150708/http://danweinreb.org/blog/rebuttal-to-stallmans-story-about-the-formation-of-symbolics-and-lmi#comment-65">rebuttal</a>
</p>



<blockquote>
<p>Moon
Moon Says:
November 15th, 2007 at 9:08 pm
</p>
<p>All true, so far as I can remember.
</p>
<p>But in all fairness I have to say that Stallman greatly improved Emacs
after he “liberated” it from Guy and me.
</p></blockquote>



<p>The early history of Emacs was followed-up compiled by Adrienne:
</p>
<blockquote>
<p>The question of Steele’s role in the creation and development of EMACS appears to be an old issue that continues to rear its head, although Stallman addressed this as far back as 1987 in his article “Emacs the Full Screen Editor” [8] and just this year in the comp.lang.lisp thread “teco, rms, gosling, mocklisp” on 28 July: [2, viper-2]
</p>
<p>| Guy Steel played a role in starting the development of Emacs in
| 1975. He developed the key bindings, I designed the internal
| platform, and we worked together for the first night of
| implementation. After that he dropped out.
</p>
<p>Guy Steele concurred: [3, Pitman 8 August]
</p>
<p>| Except for the minor misspelling of my name :-) , I concur with
| everything in RMS’ response. Feel free to post this reply to the
| discussion thread.
</p></blockquote>



<blockquote>
<p>Adrienne Says:
November 25th, 2007 at 4:13 pm
</p>
<p>Dan Weinreb:
</p>
<p>I contacted Stallman drawing his attention to the comments posted in this blog. With regard to your assertions that Guy L. Steele Jr. and David Moon were the authors of the original TECO-based Emacs, Stallman has requested that I inform you that your claim is false.
</p>
<p>In an email message to me dated Thursday, 22 November 2007, Stallman stated:
</p>
<p>| Weinreb is wrong. Moon was never involved in developing Emacs.
</p>
<p>In a further message dated Saturday, 24 November 2007, Stallman reiterated:
</p>
<p>| Please post that I told you that this claim is false. Steele and I
| worked together for the first night of writing code for Emacs, and
| then Steele dropped out. Moon was not involved.
</p>
<p>Sincerely
Adrienne
</p>
<p>cc
* Richard M. Stallman
* Guy L. Steele, Jr.
</p></blockquote>



<blockquote>
<p>My conclusions: (1) Clearly, by the end of 1976 and thereafter, RMS was
doing the bulk of EMACS development work, but it was not an “overnight”
takeover. For a period of seven weeks, anyway, he had some implementation
help from others (at least GLS, MOON, and JLK), and certainly had help
with design and debugging from these and others (DLW, EAK, ED).
He may have become the “principal hacker” more quickly than that,
however, perhaps in the space of a week or less; but remember that
in the AI lab culture, what I here call “principal hacker” (not a term used
at the time) was a “first among equals”, not an exclusive owner.
</p>
<p>(2) Moon’s involvement was not “hidden”.
</p>
<p>(3) RMS is responsible for the names “E” and “EMACS”.
RMS still deserves 99% or 99.9% or 99.99% or 99.999% of the credit
for taking a package of TECO macros and turning it into the most
powerful editor on the planet, twice (first in TECO and then with ELISP),
pouring in enormous amounts of effort and creativity over many years.
He also deserves credit for working with the early user community to
work out the initial set of key bindings and command names. I don’t
think RMS has any reason to deny the people who helped him out during
the first few months their due share of credit. They gave of their time
and creativity freely, in the best spirit of contributing to the community.
</p>
<p>–GLS
</p></blockquote>





<p>Full verbatim quotation from Dan Weinreb:
</p>
<pre>dlweinreb Says:
November 28th, 2007 at 2:37 am

I wasn’t going to keep this origin-of-Emacs topic going, but today,
Guy Steele today sent a very long piece of mail to me, Richard
Stallman, and Adrienne Thompson, in reply to mail from Stallman, who
asked Guy:

Weinreb says that Moon did something important (though he is vague
about what) in starting Emacs. If Moon was involved, he must have
hidden it from me. Do you know what’s going on here?

First some preliminary comments from me:

- Secret decoder: GLS is Guy Steele, RMS is of course Richard
Stallman, DLW is Dan Weinreb (me), Moon is David Moon, Ed is Ed
Schwalenberg who was beta-testing along with me, CBF is Charles
Frankston, EAK is Earl Killian, ECC is Gene Cicarelli, RMF is Bob
Frankston, and JLK is John Kulp.

- I agree that I’m somewhat vague on precisely which things Moon
did. I remember that there was a part of “?” called the “loader” that
was part of the underlying infrastructure, and I recall that Moon
developed this initially. Evidently Stallman improved upon it later
(see first email below), as he surely improved upon everything as time
went by. Moon also worked on the “MM macros”, which meant “commands
that had descriptive English names instead of being one or two
keystrokes”, corresponding to today’s Meta-X commands. The MM macros
feature was brought into “?” from one of the Emacs predecessors,
sometimes called TMACS, that was developed and used by Moon,
CBF, EAK, and ECC. RMF had already figured out how to extract the MM commands
from TMACS and insert them into TECMAC, one of the other Emacs
predecessors, and I borrowed (copied) that code from him. It soon
became clear that what we had here was a mess, and the right thing to
do was to all join forces and come up with a single code base that had
the best of all the ideas in it. Thus was the project born that
turned into Emacs.

- The reason the mail looks funny is that it predates the Internet;
some of what you see is Arpanet mail, and some (like the first one) is
internal ITS mail.

- Guy sent PDF files of scans, which I have no way of posting here,
but he quotes all the important stuff below.

- It would be even better had there been email from the previous
week, but, gee, you can’t have everything.

And now, here’s the mail from Guy Steele. I think this is the
best information we are ever going to get, and that this is
the last word on the topic.

——

I think all of us have been relying on our memories, which can
fail in various ways. Last time around I checked my file folder
of notes about Emacs, which has some useful information, but
not a lot about who did what. Now I have some more information
to offer. I’m going to quote email I received during the last
part of 1976. The attached PDF files are scans I made today from
my paper archives of that email. I may have committed typographical
errors in quoting the email below; if in doubt, consult the scans.

On October 23, 1976, RMS sent this email to GLS:

RMS@@MIT-AI 10/23/76 02:11:39
To: GLS at MIT-AI
I HAVE HACKED ?MACS A LOT. IT NOW HAS
AN IMPROVED LOADER MACRO AND SUITABLE PURIFY MACRO.
THE PURIFY MACRO HAS BEEN DEBUYGGED, AND WINS;
I HAVEN’T TESTED THE LOADER ON THE RESULT THOUGH.

Comments: (a) At this point the new proposed consolidated set of
TECO macros for real-time editing was called “?”. I had chosen this
name as a kind of stubborn joke, because a non-alphabetical character
as the name of a program was just a little harder to invoke from DDT;
also, it followed the example of the @ program, which had just recently
taken over from the @ command in TECO for creating program listings
—another project that I started and then RMS markedly improved over
the years. However, the @ program was useless without command-line
arguments, so no one ever wanted to type @^K to start it, whereas
it was desirable to start a frequently-used editor by typing a single-character
name and then ^K, and I knew it, and I was being a bit mulish about it.

(b) This email was sent just to GLS. From the fact that he was
reporting progress to me, I infer from this that RMS did not yet regard
himself as the “owner” or “principal hacker” of this project. (While the
AI Lab culture did support the notion that in principle anyone could hack
on any program, in practice it was also well-understood that certain
people had superior knowledge about certain programs, and that superior
knowledge was consulted and paid due respect. I wouldn’t have dreamed
of hacking on TECO without consulting RMS, and he would not have hacked
on LISP without consulting JONL or me.)

On October 29, 1976, GLS sent this email to RMS:

GLS@@MIT-AI 10/29/76 15:20:31
To: RMS at MIT-AI
CC: GLS at MIT-AI
See .TECO.;?VARS &gt; for a ? variables macro.
It has some hair for pushing and popping
variables as well as getting and setting them.
Suggestions appreciated for reducing hair.

Comment: Six days later, I am still working on the implementation.
I think that explodes the pretty myth that the project was handed over
to RMS literally overnight. (However, as we will see, it did occur fairly
quickly, as such things go.)

On October 31, 1976, RMS sent this email to GLS:

RMS@@MIT-AI 10/31/76 01:15:37
To: GLS at MIT-AI
I MOVED ?VARS INTO ?MACS
UNDER THE NAME ^^ VARIABLES (THAT’S 2 UPARROWS).
I PARTIALLY DEBUGGED IT; READING AND WRITING WORK BUT
NOT PUSHING AND POPPING.
TO GET A ?, DO :XT ?;
THEN DO MMLIST COMMANDS$$ AND MMLIST REDEFINITIONS$$.

Comment: I believe that by this point I thought of RMS as principal
hacker on the project, or at least the most active contributor; I’m feeding
him little chunks of code as I am able, and he does the integration.

The next day:

RMS@@MIT-AI 11/01/76 03:53:45
To: GLS at MIT-AI
m.v now works completely.
List commands implemented.
lisp indentation command works (meta-I).
MIDAS, TECO and LISP editing modes defined.

Comments: RMS still sends me reports on his progress. (The Lisp indentation
macro was the “big one” that he and I worked on together in a single ten-hour
hack session.)

On November 10:

RMS@@MIT-AI 11/10/76 21:46:03
To: EAK at MIT-AI, CBF at MIT-AI, GLS at MIT-AI, ED at MIT-AI
To: DLW at MIT-AI, MOON at MIT-AI
Unless anyone can think of a better idea, I think we should
rename ? to E.

DLW@@MIT-AI 11/10/76 21:49:07
To: MOON ay MIT-AI, DLW at MIT-AI, ED at MIT-AI, GLS at MIT-AI
To: CBF at MIT-AI, EAK at MIT-AI, RMS at MIT-AI
Another idea is to call it formally “QMARK” with a link
existing for “QM” .

Comment: Note that MOON is among the interested parties. Most of
these addressees were implementors of macro packages that were
predecessors of ?MACS and had user constituencies.

GLS@@MIT-AI 11/11/76 14:43:03
To: MOON ay MIT-AI, DLW at MIT-AI, ED at MIT-AI, GLS at MIT-AI
To: CBF at MIT-AI, EAK at MIT-AI, RMS at MIT-AI
Well, for hack value TS ? ought to exist (yes, you CAN
get DDT to load it under that name!), but E is a good
abbreviation.

Comment: Finally, I capitulate on the name (thank goodness).

Later that day:

GLS@@MIT-AI 11/11/76 16:39:50
To: CBF at MIT-AI, EAK at MIT-AI, ED at MIT-AI, MOON ay MIT-AI
To: DLW at MIT-AI, RMS at MIT-AI
CC: GLS at MIT-AI
My current tentative suggestions for ? command placement
are in TGQ;?CHARS &gt; on AI. (They aren’t even completely
what I want, now that I have talked with RMS, but at
least some desirable features are listed even if they aren’t
where we want them to be.)

Comment: The key bindings are still in flux, and I’m still
involved in determining those key bindings.

MOON@@MIT-AI 11/11/76 21:28:51
To: INFO-E at MIT-AI
You are now on the INFO-E @ AI mailing list. (Used to be called INFO-?).

Comment: Moon creates the INFO-E mailing list. Looks like the name
change has been agreed upon.

RMS@@MIT-AI 11/12/76 03:53:31
To: INFO-E at MIT-AI
LOTS OF COMMANDS MOVED.
NEW PURIFIER (USING FO) NOW UP, GIVING
TREMENDOUD INCREASE IN SPEED, ESPECIALLY FOR DOCUMENTATION
MACROS.

DLW sends several messages to (BUG EMACS) and (BUG E); he and
Moon are the principal testers of the new editor, shaking out many
bugs.

MOON@@MIT-AI 11/14/76 04:40:49 Re: Changes
To: INFO-E at MIT-AI
[1] RMS’s many bug fixes and changes of this afternoon compiled and installed.
[2] MM LIST FILES renamed to MM LIST LOADED FILES
[3] New MM macros:
LIST FILES compact directory listing
LIST DIRECTORIES compact, sorted listing of M.F.D.
LIST TECO FS FLAGS compact, sorted listing of Teco FS flags
DUMP RMAIL don’t try it!
RMAIL temporary access to rmail – seems to have
a few bugs. In particular, don’t try
to get the minibuffer inside rmail’s ^R
command – you’ll be sorry!
EDIT ..D edit the delimiter table
VIEW Q-REGISTER try to view any type of Q-register
[4] Note that MM LIST should be an acceptable
abbreviation for most such commands. Note that RMAIL needs
to be rewritten.
[5] For those who don’t know MM DIRED has worked for a few days.

Comment: From the fact that Moon first cites RMS’s work in item [1]
and then goes on to cite other changes to EMACS, this seems to imply
that these other items are things Moon was working on (and they strike
me as his style of things to work on).

MOON@@MIT-AI 11/14/76 19:14:06
To: (BUG E) at MIT-AI
M.I lossage – if you are using a multicharacter command, e.g. ^XB..Z,
and type part of it fast, you end up seeing at the bottom of the screen
something like “.:z” – it seems the right thing would be to save up all
non-echoed chars in a string in ..0 (which q-r gets reset at the right times)
and echo them all when echoing starts. And flush the colon. This would
also allow hairy commands to use long prompts by putting a string in ..0
before calling .I the first time.

Comment: This message testifies to Moon’s intimate knowledge of the inner
workings of TECO and the fledgling EMACS.

RMS@@MIT-AI 11/16/76 22:05:41
To: INFO-E at MIT-AI
EMACS^K and E^K now exist, and run links to EMACS;TS &gt;.
:NT EMACS; will still load up from scratch.
Note that the file [PRFY] is no longer loaded by default.

Comment: the birth of EMACS as a stand-alone program
under that name (and the name E)! (Though note that the
(BUG EMACS) mailing list had already existed for a couple
of days, and that the previous way to start the macros was
to say “:NT EMACS”.)

gls@@MIT-AI (Sent by BRS@@MIT-AI) 11/17/76 12:44:06
To: (BUG E) at MIT-AI
Grumble! If CTRL-META-[ is gobbled, then I can’t use it to insert Q!

Comment: This was a reference to the Crunchly cartoon of 5/19/1973.
(You can see it in _The New Hacker’s Dictionary_.)

MOON@@MIT-AI 11/17/76 23:32:45 Re: Featurama EMACS
To: INFO-EMACS at MIT-AI
MM TECORD $ teco command $
^R puts current line at top of screen, ^U^R at bottom,
^U^U^R puts top of current defun, paragraph, etc. at top of screen.
Warning- this may get moved to another character.
Multiple consecutive deletes act like one as far as the ..K ring
is concerned; thus one ^Y will get it all back.
^K accepts negative arguments just like meta and control-meta versions.
A few bugs fixed.

Comment: Moon is still involved.

RMS@@MIT-AI 11/19/76 04:49:41
To: GLS at MIT-AI, MOON at MIT-AI, DLW at MIT-AI, ED at MIT-AI
I have just written some winning Meta, Control, and Control-Meta
prefix characters, and I am desperately in need of a good idea
of where to put them. The Meta and Control-Meta prefixes should
be easy to type on ordinary terminals. One idea is to put them
on ^W and ^L, but then 1) where to put ^R Kill Region, and
2) does ^L^L clear the screen or move left?
A possible place for the Control-Meta prefix is ^C,
which has the advantace of being easy to remember for
an ex-TECMAC user.

Comment: RMS confers with me, DLW, Ed, and Moon on design.

On 11/27/76, RMS sends out a long message to INFO-E reporting
mane changes he has made. By this point RMS appears to be doing
most of the work, and I think Moon is doing much less implementation
work.

On 11/30/76, RMS send a message to INFO-E, and two more on 12/05/76,
and three more on 12/11/76.

On 12/10/76, JLK sent a message to INFO-E announcing 18 or 19
new features. Comment: John Kulp (an implementor of one of
the predecessor macro packages) was actively involved in EMACS
development as late as December 10.

My conclusions: (1) Clearly, by the end of 1976 and thereafter, RMS was
doing the bulk of EMACS development work, but it was not an “overnight”
takeover. For a period of seven weeks, anyway, he had some implementation
help from others (at least GLS, MOON, and JLK), and certainly had help
with design and debugging from these and others (DLW, EAK, ED).
He may have become the “principal hacker” more quickly than that,
however, perhaps in the space of a week or less; but remember that
in the AI lab culture, what I here call “principal hacker” (not a term used
at the time) was a “first among equals”, not an exclusive owner.

(2) Moon’s involvement was not “hidden”.

(3) RMS is responsible for the names “E” and “EMACS”.

RMS still deserves 99% or 99.9% or 99.99% or 99.999% of the credit
for taking a package of TECO macros and turning it into the most
powerful editor on the planet, twice (first in TECO and then with ELISP),
pouring in enormous amounts of effort and creativity over many years.
He also deserves credit for working with the early user community to
work out the initial set of key bindings and command names. I don’t
think RMS has any reason to deny the people who helped him out during
the first few months their due share of credit. They gave of their time
and creativity freely, in the best spirit of contributing to the community.

–GLS
</pre>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pnut: A C to POSIX Shell Compiler you can Trust (155 pts)]]></title>
            <link>https://pnut.sh</link>
            <guid>41052446</guid>
            <pubDate>Wed, 24 Jul 2024 00:22:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pnut.sh">https://pnut.sh</a>, See on <a href="https://news.ycombinator.com/item?id=41052446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
                <p>
                    <h5>Write C</h5>
                </p>
                <p>
                    Write your script in plain-old C, no new language to learn.
                </p>
                <p><img src="https://pnut.sh/code.png" alt="Write Image">
            </p></div>
            <div>
                <p>
                    <h5>Human readable</h5>
                </p>
                <p>
                    Pnut's output is designed to be human-readable, making it
                    easy to inspect, debug and maintain code.
                </p>
                <p><img src="https://pnut.sh/code-shell.png" alt="Compile Image">
                </p>
            </div>
            <div>
                <p>
                    <h5>Runs everywhere</h5>
                </p>
                <p>
                    Pnut's output runs on any POSIX-compliant shell, from bash to
                    zsh, across all major operating systems including Linux,
                    macOS, and Windows.
                </p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scrapscript: A functional, content-addressable programming language (178 pts)]]></title>
            <link>https://github.com/tekknolagi/scrapscript</link>
            <guid>41052371</guid>
            <pubDate>Wed, 24 Jul 2024 00:08:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tekknolagi/scrapscript">https://github.com/tekknolagi/scrapscript</a>, See on <a href="https://news.ycombinator.com/item?id=41052371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Scrapscript Interpreter</h2><a id="user-content-scrapscript-interpreter" aria-label="Permalink: Scrapscript Interpreter" href="#scrapscript-interpreter"></a></p>
<p dir="auto">See <a href="https://scrapscript.org/" rel="nofollow">scrapscript.org</a> for some more information. Keep
in mind that the syntax on the website will change a little bit in the coming
weeks to match this repository.</p>
<p dir="auto">Take a look inside <a href="https://github.com/tekknolagi/scrapscript/blob/trunk/scrapscript.py">scrapscript.py</a> and all of its tests to get
an idea for how the language works.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">We support python3.8+.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# With a file
python3 scrapscript.py eval examples/0_home/factorial.scrap

# With a string literal
python3 scrapscript.py apply &quot;1 + 2&quot;

# With a REPL
python3 scrapscript.py repl"><pre><span><span>#</span> With a file</span>
python3 scrapscript.py <span>eval</span> examples/0_home/factorial.scrap

<span><span>#</span> With a string literal</span>
python3 scrapscript.py apply <span><span>"</span>1 + 2<span>"</span></span>

<span><span>#</span> With a REPL</span>
python3 scrapscript.py repl</pre></div>
<p dir="auto">or with <a href="https://justine.lol/cosmopolitan/index.html" rel="nofollow">Cosmopolitan</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./build-com

# With a file
./scrapscript.com eval examples/0_home/factorial.scrap

# With a string literal
./scrapscript.com apply &quot;1 + 2&quot;

# With a REPL
./scrapscript.com repl"><pre>./build-com

<span><span>#</span> With a file</span>
./scrapscript.com <span>eval</span> examples/0_home/factorial.scrap

<span><span>#</span> With a string literal</span>
./scrapscript.com apply <span><span>"</span>1 + 2<span>"</span></span>

<span><span>#</span> With a REPL</span>
./scrapscript.com repl</pre></div>
<p dir="auto">(if you have an exec format error and use Zsh, either upgrade Zsh or prefix
with <code>sh</code>)</p>
<p dir="auto">or with Docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# With a file (mount your local directory)
docker run --mount type=bind,source=&quot;$(pwd)&quot;,target=/mnt -i -t ghcr.io/tekknolagi/scrapscript:trunk eval /mnt/examples/0_home/factorial.scrap

# With a string literal
docker run -i -t ghcr.io/tekknolagi/scrapscript:trunk apply &quot;1 + 2&quot;

# With a REPL
docker run -i -t ghcr.io/tekknolagi/scrapscript:trunk repl"><pre><span><span>#</span> With a file (mount your local directory)</span>
docker run --mount type=bind,source=<span><span>"</span><span><span>$(</span>pwd<span>)</span></span><span>"</span></span>,target=/mnt -i -t ghcr.io/tekknolagi/scrapscript:trunk <span>eval</span> /mnt/examples/0_home/factorial.scrap

<span><span>#</span> With a string literal</span>
docker run -i -t ghcr.io/tekknolagi/scrapscript:trunk apply <span><span>"</span>1 + 2<span>"</span></span>

<span><span>#</span> With a REPL</span>
docker run -i -t ghcr.io/tekknolagi/scrapscript:trunk repl</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">The experimental compiler:</h3><a id="user-content-the-experimental-compiler" aria-label="Permalink: The experimental compiler:" href="#the-experimental-compiler"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Normal ELF</h4><a id="user-content-normal-elf" aria-label="Permalink: Normal ELF" href="#normal-elf"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="./scrapscript.py compile some.scrap  # produces output.c
./scrapscript.py compile some.scrap --compile  # produces a.out"><pre>./scrapscript.py compile some.scrap  <span><span>#</span> produces output.c</span>
./scrapscript.py compile some.scrap --compile  <span><span>#</span> produces a.out</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Cosmopolitan</h4><a id="user-content-cosmopolitan" aria-label="Permalink: Cosmopolitan" href="#cosmopolitan"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="CC=~/Downloads/cosmos/bin/cosmocc ./scrapscript.py compile some.scrap  --compile # produces a.out"><pre>CC=<span>~</span>/Downloads/cosmos/bin/cosmocc ./scrapscript.py compile some.scrap  --compile <span><span>#</span> produces a.out</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Wasm</h4><a id="user-content-wasm" aria-label="Permalink: Wasm" href="#wasm"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="CC=/opt/wasi-sdk/bin/clang \
CFLAGS=-D_WASI_EMULATED_MMAN \
LDFLAGS=-lwasi-emulated-mman \
./scrapscript.py compile some.scrap --compile  # produces a.out"><pre>CC=/opt/wasi-sdk/bin/clang \
CFLAGS=-D_WASI_EMULATED_MMAN \
LDFLAGS=-lwasi-emulated-mman \
./scrapscript.py compile some.scrap --compile  <span><span>#</span> produces a.out</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Tests</h2><a id="user-content-running-tests" aria-label="Permalink: Running Tests" href="#running-tests"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 scrapscript.py test"><pre>python3 scrapscript.py <span>test</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Taking my diabetes treatment into my own hands (2024) (332 pts)]]></title>
            <link>https://martin.janiczek.cz/2024/07/23/taking-my-diabetes-treatment-into-my-own-hands.html</link>
            <guid>41052365</guid>
            <pubDate>Wed, 24 Jul 2024 00:06:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martin.janiczek.cz/2024/07/23/taking-my-diabetes-treatment-into-my-own-hands.html">https://martin.janiczek.cz/2024/07/23/taking-my-diabetes-treatment-into-my-own-hands.html</a>, See on <a href="https://news.ycombinator.com/item?id=41052365">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody">
    <p>First of all, this blogpost is kinda long. Let me prove to you reading it <em>will</em> actually have some payoff:</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/04-original-and-best.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/04-original-and-best.png" alt="See, I wrote something!"></a></p>

<p>OK, now that you’ll stay, let’s start from the beginning…</p>

<hr>

<p>I’m a Type 1 diabetic. This means my pancreas doesn’t produce insulin (which allows cells to use blood glucose for energy) and I have to provide it externally.</p>

<p>This is a finnicky process, because you need to balance your glucose in the right “zone” - not too high (hyperglycemia, &gt;10 mmol/l, is a long-term danger to your body) and not too low (hypoglycemia, &lt;4 mmol/l, is a short-term danger to your body). If it’s too high, you need to inject insulin, and if it’s too low, you need to eat some sugars.</p>

<p>A commonly used metaphor for this is flying a plane. There are games illustrating the process as well: click blue button, bird flies down, click orange button, bird flies up. Too high bad, too low bad, just right good.</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/icarus.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/icarus.png" alt="A game showing the process"></a></p>

<p>The issues with manually managing this process you’ve inherited from your douchebag pancreas are manifold:</p>

<ul>
  <li>the insulin doesn’t act immediately, there’s around 20min delay (depending on the brand) before your blood glucose goes down</li>
  <li>the <em>food</em> doesn’t act immediately, there’s around 20min delay (depending on the food!) before your blood glucose goes up
    <ul>
      <li>simple sugars (eg. fruit) are faster (and stop acting faster as well)</li>
      <li>complex sugars (rice / potatoes / …) are slower. You usually want your blood glucose as constant and flat as possible. The worst thing you can do is to go from hypo to hyper to hypo to hyper in huge amplitude swings.</li>
      <li>fats also somehow affect the digestion of sugars. I can’t be bothered to remember how, I just ignore them honestly.</li>
    </ul>
  </li>
  <li>injecting insulin ~15min before you start eating would do <em>wonders</em> for neutralizing the BG spike, the issue is, nobody does it, because what if you then get a smaller serving at the restaurant or it gets delayed? What if you get called somewhere urgently and can’t finish your meal? People usually inject right before the meal / after the meal as a result.</li>
  <li>there’s no generic formula (that I know of) for estimating how much will a gram of sugar increase your blood glucose, nor how much will an unit of insulin decrease your blood glucose. <em>It’s all vibes.</em></li>
  <li>mathematical models <em>do</em> exist but you need to find your body’s parameters - I’ll get to it below!</li>
  <li>the body has its own emergency reserves of glucose, which it can sometimes decide to use (though beware, this system turns off when you’re drunk), so <em>maybe</em> the Snickers bar you just ate to save your life wasn’t actually needed anymore, and you end up with a hyper</li>
  <li>you’re not quite yourself during a hypo (you get slower, dumber, I’ve heard of people getting stuck in thought loops in front of an open fridge), and so even though you intellectually know you just ate enough to get back into the correct levels <em>eventually</em>, your brain is screaming at you <strong>“EAT! YOU’RE DYING! I AM LOW ON SUGAR <em>NOW!!</em>“</strong> and in my case this leads to overcompensating quite knowingly and willingly. <em>Yeah one more yoghurt can’t hurt.</em> Well…</li>
  <li>insulin doesn’t work when eaten, you need to inject yourself with it (though nowadays we do it under skin, not into veins, I sure am glad I’m not living 50 years ago) or inhale it. Since injections aren’t the most pleasant thing in the world, the dosage is usually limited to 4x a day (to counteract the three main meals + a long-acting different type of insulin once a day), even though you’d be more stable if you injected more often, with smaller doses.
    <ul>
      <li>(no experience with inhalations here, so I’ll skip this)</li>
      <li>(insulin pumps do exist, I’ll mention those briefly in a moment… maybe)</li>
    </ul>
  </li>
  <li>measuring your blood glucose level is painful if you are using test strips and need to prick your fingers to provide a blood drop, so measuring your sugar is usually limited to 4-6x a day – again, even though it would be better to have more data points.
    <ul>
      <li>this is less of a problem nowadays with Continuous Glucose Monitoring systems like Freestyle Libre, which you install into your arm once every 14 days and get a measurement every minute through Bluetooth to your phone</li>
    </ul>
  </li>
  <li>things like physical effort, illness, stress, <em>heck, even seasons of the year</em> do all affect how your body behaves and reacts to sugar / insulin</li>
  <li>there’s this damn thing called <a href="https://en.wikipedia.org/wiki/Dawn_phenomenon">dawn phenomenon</a> that some diabetics, me included, experience: in the morning your sugar will just start going up and up. If you wanted to sleep in on the weekend, well tough luck, you’re now in the 15 mmol/l range.</li>
</ul>

<p>I hope this incomplete list gives you an idea of how wonky the process of trying to make your blood glucose stay in the right levels is.</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/phone.jpeg">
<img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/phone.jpeg" alt="My 7-day average">
</a>
</p>

<p>My treatment is usually: keep the Freestyle Libre app on my phone open as much as possible and when I see my BG’s getting high, I inject a small amount of insulin. How much? No idea. <em>IT’S ALL VIBES.</em></p>

<p>But sometimes the app is yelling at you: “you’re at 15 mmol/l for an hour now, idiot!” and you just don’t (want to) pay attention. Alert fatigue is a real thing.</p>

<p>I have some recommended insulin dosages that we’ve settled on with my diabetologist, whom I visit every three months. So my regimen can look like:</p>

<ul>
  <li>Breakfast: inject 18 units, eat 24g of sugars</li>
  <li>Lunch: inject 22 units, eat 60g of sugars</li>
  <li>Dinner: inject 21 units, eat 60g of sugars</li>
  <li>Before sleep: inject 32 units of long-acting insulin</li>
</ul>

<p>And then I see my diabetologist, she looks at the 7d / 14d / 30d averages and says “maybe you can try fixing the 15:00 hypers you’re getting quite regularly, by injecting more insulin before lunch. You should also lose weight, when you started coming here you had 80kg, now you’re a centurion. Like seriously, WTF. OK cool bye, see you in 3 months!”</p>

<p>Lovely.</p>

<p>If you can’t tell, the thing that irks me the most about the whole thing is: <em><span>Ï̷̛̛̮̏̀̊͠T̵̨̡̏͝’̵̧͍̐̂̑̈́͐̐͜S̸͉̖͒̈̀̕͝ ̴̢̺̤̜͎͚̙́Ạ̷͕̱͖͙̉̊L̴̼̞̺̤̞̬̟̅̈́L̷̠̔̏̐̃̚ ̴̞̊͛͝V̸͇͚̱͑̄̈̌̊Ḯ̴̧̧͚̰̞̈́͝B̸̡̬̪͊̌͂̓̐E̵͙̼̰̞̹͇̎̽̈́̓Ş̷̱͍͖̼͍̯̉̾͊̂̾͝</span>.</em> I’d seriously appreciate it if my diabetologist used a model, or a simulation of some kind, got my body’s parameters there somehow, and found the improvements to my schedule <em>that way</em>. Maybe she has some expert knowledge in her head but from my perspective it’s all guessworks. Err, I mean <span>v̴̼̂i̴̥̇b̸̠̌e̶͙̕ś̴̲</span>.</p>

<p>And this is where my programmer mind comes in.</p>

<hr>

<p>There are people who take <strong>insulin pumps</strong> (which provide insulin in very small very frequent doses and are ~permanently injected into your body, but are otherwise dumb as a brick) and combine them with <strong>continuous glucose monitors,</strong> and make the glucose measurements inform and control the pump. This is called “closed loop” or “artificial pancreas”, and getting one officially is very hard or impossible: not FDA approved yet / you need to be part of an university study to get one / … It’s one of those things that “will be here in 5 years”, <em>they say every year for the past 30 years.</em></p>

<blockquote>
  <p>Aside: I try not to be too butthurt about it: CGMs have just recently started being available and even fully sponsored by the Czech health insurance companies, and having a 1440-datapoints-a-day graph is a <em>MASSIVE</em> improvement compared to pricking your finger 4x a day and getting 4 datapoints for your blood glucose graph with nothing in between. So, the artificial pancreas is slowly coming. Unlike nuclear fusion.</p>
</blockquote>

<p>The most prominent of these people hacking their devices together, in my social bubble at least, is <a href="https://www.hanselman.com/">Scott Hanselman</a>, the Microsoft programmer guy. (Check out his <a href="https://www.youtube.com/watch?v=uNhYhlBQoEY">talk</a>.) He’s a T1DM as well and has been promoting the <a href="https://wearenotwaiting.net/">#WeAreNotWaiting</a> initiative where people take their own pump and their own CGM and hack them together despite the healthcare companies’ pleas that it’s not approved and not safe etc. #TheyAreNotWaiting.</p>

<p>And that is really inspirational.</p>

<p>I don’t have a pump myself (and to have a chance of getting one I’d first have to find another diabetologist, which makes this into a <em>“too much work, can’t be bothered”</em> issue for me), so I can’t currently do quite what they are doing, but I can go with the high-level idea and #NotWait in my own way.</p>

<hr>

<p>A few days ago I was fumbling down the stairs to our kitchen at ~3:00 in the morning to fix my hypo. (Night hypoglycemias are <em>especially</em> bad: what if you don’t wake up?) On the stairs I had the thought: why the hell is there no app into which I’d put my past X blood glucose values, my usual daily schedule, my weight, height, gender, age, whatever, and it would let me play with some kind of prediction (interactively!) and find good dosages / meal times / injection times? Then I would have a potentially good target to get to, and over the course of a few days I could gradually adjust my real dosage to that level and see how it behaves, and hopefully stay in the 4-10 mmol/l range much more easily.</p>

<p>Why doesn’t such a thing exist?</p>

<p>(Coincidentally I’ve also started chatting about this and other app ideas with <a href="https://twitter.com/lambdapriest">John Pavlick</a>. Thanks for your encouragement, John!)</p>

<p>So I started googling. Turns out there are <em>many</em> papers detailing differential equations for a model of how glucose and insulin interact, and how an artificial pancreas could get you to the correct range automatically. The issue is, I DON’T HAVE AN ARTIFICIAL PANCREAS. I have four daily injections. Give me something useful for those!</p>

<p>There’s not too much to pick from.</p>

<blockquote>
  <p>To be honest, I also don’t really know how to translate those differential equations into a simulation algorithm, even if I found a good model. I’m guessing I need to write an <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ODE</a> solver like <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods">Runge-Kutta</a> for the specific set of equations and somehow find <em>my</em> specific parameters for it. It’s been a while since I studied this in uni.</p>
</blockquote>

<p>One of the links led me to <a href="https://diabetes.zcu.cz/">diabetes.zcu.cz</a> though, and in particular their <a href="https://diabetes.zcu.cz/smartcgms/">SmartCGMS</a> app (open-source too!). From the screenshots it seemed kinda relevant, or at least similar to what I had in mind for my dream app.</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/smartcgms.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/smartcgms.png" alt="SmartCGMS"></a></p>

<p>So I sent an email to the authors. Found one maintainer on GitHub and wrote them an email detailing my woes and the app I’d love to write, and whether they could point me to some existing software or good papers on modelling multiple-dose-injection treatment (as opposed to a pump / artificial pancreas).</p>

<p>What they gave me was better than I could imagine. (Thanks again, <a href="https://github.com/MartinUbl">Martin</a>!) They had a wrapper for the core SmartCGMS engine (which contains <a href="https://github.com/SmartCGMS/core/blob/dffdd89a274144d0e9ecbe9f581db9eca0e4b8ed/model/src/bergman/bergman.cpp#L82">implementations of some of these models</a> already) for the C# language. The API was quite simple:</p>

<ul>
  <li><code>.Create(...)</code> - initialize the simulation</li>
  <li><code>.Step()</code> - step one time-unit (typically a minute) in the simulation</li>
  <li><code>.ScheduleInsulinBasalRate(double unitsPerHour)</code> - schedule a (pump, sadly) insulin dosage</li>
  <li><code>.ScheduleInsulinBolus(double units)</code> - schedule a short-acting insulin injection</li>
  <li><code>.ScheduleCarbohydratesIntake(double grams)</code> - schedule food consumption</li>
  <li><code>.Terminate()</code> - stop the simulation</li>
</ul>

<p>And then there was the current simulation state:</p>

<ul>
  <li><code>.BloodGlucose</code> - current blood sugar, in mmol/l</li>
  <li><code>.InterstitialGlucose</code> - glucose in your interstitial fluid - let’s skip it, not important</li>
  <li><code>.InsulinOnBoard</code> - how much insulin is still left to be absorbed</li>
  <li><code>.CarbohydratesOnBoard</code> - how much sugar is still left to be absorbed</li>
</ul>

<p>As you can see, with some caveats this would let me make a simulation for my daily schedule.</p>

<p>So a few moments later I had something like this (in C# but here presented as Elm)</p>

<pre><code>type IntakeType
  = BasalInsulin -- long-acting
  | BolusInsulin -- short-acting
  | Carbs        -- fooooooooooooood!

type alias Intake =
  { intakeType : IntakeType
  , amount : Float
  , timeMinutes : Int
  }

type alias Input =
  { basalInsulin : Intake
  , bolusInsulins : List Intake
  , carbs : List Intake
  }

type alias OutputRow =
  { minute : Float
  , bloodGlucose : Float
  , carbohydratesOnBoard : Float
  , insulinOnBoard : Float
  , interstitialGlucose : Float
  }

simulate : Input -&gt; Int -&gt; List OutputRow
simulate input days =
  ...

mySchedule : Input
mySchedule =
  { basalInsulin = Intake BasalInsulin (22 * 60) 32
  , bolusInsulins =
      [ Intake BolusInsulin (10 * 60) 18
      , Intake BolusInsulin (13 * 60) 22
      , Intake BolusInsulin (19 * 60) 21
      ]
  , carbs =
      [ Intake Carbs (10 * 60) 24
      , Intake Carbs (13 * 60) 60
      , Intake Carbs (19 * 60) 60
      , Intake Carbs (22 * 60) 24
      ]
  }

myPrediction : List OutputRow
myPrediction =
  simulate mySchedule 3
</code></pre>

<p>Never have I copied the resulting CSV into Google Sheets so fast. Tada:</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/00-accidental-art.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/00-accidental-art.png" alt="I'm the artist now."></a></p>

<p>Oh, wait, that’s not it. Cool piece of accidental art though! Now let me use the correct column for the X axis.</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/01-google-sheets.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/01-google-sheets.png" alt="Google Sheets"></a></p>

<p>Well hot damn. The graph actually makes sense!</p>

<p>Granted, it’s not <em>me</em> in the graph but I can simulate <em>someone</em> now!</p>

<p><em>Let’s stash away “I need to actually simulate</em> me, <em>you know” as a TODO for future Martin and continue.</em></p>

<hr>

<p>Next I made a Windows Forms application with an OxyPlot chart so that I don’t need to write a CSV to a file and manually copy it to Google Sheets.</p>

<blockquote>
  <p>Aside: what do you .NET folks use nowadays? MAUI? WPF? Xamarin Forms? It’s kinda confusing for an outsider.</p>
</blockquote>

<p>This took me a while to figure out (I’m not a C# guy; honestly I’ve thought about rewriting this into F# instead the moment I had to start learning about event handlers and delegates), but I succeeded:</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/02-initial-graph.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/02-initial-graph.png" alt="Initial graph"></a></p>

<p>There’s one little issue with seeing just the first day of the simulation though, and that’s the fact that I inject my basal (long-acting) insulin at 22:00. So for the majority of the first day the glucose will just be higher because I haven’t injected the long-acting insulin yet. So let’s simulate more days just to see what happens.</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/03-ranges.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/03-ranges.png" alt="Ranges"></a></p>

<p>Oh wow, it gets periodic by day 3! Cool!</p>

<p>As you can see I’ve also added the hypo- and hyperglycemic ranges so that it’s clearer where the “Goldilocks zone” lies. As it turns out, my insulin dosage is absolutely inappropriate for the person simulated by SmartCGMS right now. Hitting such severe hypoglycemia, they’d probably be dead by now (or their liver has to work overtime on dosing that emergency glucose).</p>

<p>OK, well then, now it’s just a small step from simulating a hardcoded</p>

<pre><code>mySchedule : Input
mySchedule =
  { basalInsulin = Intake BasalInsulin (22 * 60) 32
  , bolusInsulins =
      [ Intake BolusInsulin (10 * 60) 18
      , Intake BolusInsulin (13 * 60) 22
      , Intake BolusInsulin (19 * 60) 21
      ]
  , carbs =
      [ Intake Carbs (10 * 60) 24
      , Intake Carbs (13 * 60) 60
      , Intake Carbs (19 * 60) 60
      , Intake Carbs (22 * 60) 24
      ]
  }
</code></pre>

<p>to automatically finding a more optimal dosage!</p>

<hr>

<p>I’ve opened the NuGet package manager, wrote <code>genetic</code> and installed the most popular package: <a href="https://github.com/giacomelli/GeneticSharp">GeneticSharp</a>. Turns out it’s <em>really solid.</em> It needed me to provide the usual stuff: chromosomes, crossover, selection, population size, termination criteria… and the fitness function.</p>

<p>The fitness function actually deserves a fuller description. It looks like this:</p>

<pre><code>fitness : Input -&gt; Float
fitness input =
  let
    output : List OutputRow
    output = simulate input 3

    longtermHypos = output |&gt; List.takeLast (24*60) |&gt; List.count (\row -&gt; row.bloodGlucose &lt; 4)
    longtermHypers = output |&gt; List.takeLast (24*60) |&gt; List.count (\row -&gt; row.bloodGlucose &gt;= 10)
    ...

    longtermHyposNormalized = longtermHypos / List.length output
    longtermHypersNormalized = longtermHypers / List.length output
    ...

    longtermHyposWeight = 15
    longtermHypersWeight = 12
    ...

    weightSum = longtermHyposWeight + longtermHypersWeight + ...
  in
  ( longtermHyposNormalized * longtermHyposWeight
  + longtermHypersNormalized * longtermHypersWeight
  + ...
  ) / weightSum
</code></pre>

<p><em>(Yes I know stuff is computed needlessly here; in the real C# code I’m doing all the intermediate result reuse you wish I did here, but I’m optimizing for understanding instead here.)</em></p>

<p>Turns out I care about many things. The following list evolved gradually but I’m only giving you the final version:</p>
<ul>
  <li>minimize # of hypoglycemic readings in the (stabilized) last day</li>
  <li>minimize # of hyperglycemic readings in the last day</li>
  <li>as small amplitude between min and max glucose reading as possible in the last day</li>
  <li>minimize the sum of bolus insulin dosages</li>
  <li>minimize the basal insulin dosage</li>
  <li>minimize # of hypoglycemic readings in the stabilization phase (first 2 days)</li>
  <li>minimize # of hyperglycemic readings in the stabilization phase</li>
</ul>

<p>Note that I don’t care about all of those equally. I’ve actually sorted the above list by priority: long-term hypos are the most urgent, the temporary hypers while the system settles I care about the least. I’m encoding that via the weights. Each of the normalized measurements is a number <code>0..1</code>, which then gets multiplied by the weight.</p>

<p>I’m not quite sure whether this is the right way to encode multiple concerns into a single number, but it’s the best I could come up with without consulting math books, and it seems to work well. At least I couldn’t find a case where an input with lower (better) fitness was less preferable to me (according to my brain’s fuzzy intuition) than another input with higher (worse) fitness.</p>

<p>If I was able to construct all the values and sort them, I’d probably do something like</p>

<pre><code>allCombinations
  |&gt; List.sortBy (\output -&gt;
       [ longtermHypos output
       , longtermHypers output
       , amplitude output
       , ...
       ]
     )
</code></pre>

<p>(that is, I can order outputs pairwise), but that’s not how the genetic programming libraries work. I believe explicitly not going through the whole space is one of their very top priorities :)</p>

<blockquote>
  <p>Aside: how many inputs are there? This to me looks like permutations with repetitions (order does matter), <code>n^r</code> , so in my case <code>injectionPossibilities ^ injectionCount</code>, and for my specific example schedule, <code>51^4</code>  (assuming I can inject <code>0..50</code> units of insulin). That’s around 6.7 million.</p>
</blockquote>

<p>So this could be bruteforceable. But I have to run the whole simulation inside the fitness function, and it takes around a second or two.</p>

<p>At least it’s parallelizable then! (And believe me, I’m making use of my 16 cores.)</p>

<p>Given it’s a pure function (basically… the results vary around the 10th decimal digit - negligible), I can memoize the fitness function for the input of four ints. The genetic algorithm ends up repeating some guesses quite a lot near the end of the simulation, so this actually saves a lot of time.</p>

<p>Writing a memoization function in C# was a breath of fresh air, coming from the pure FP world of Elm:</p>

<pre><code>private Dictionary&lt;List&lt;Intake&gt;, double&gt; fitnessCache = new(new IntakesSameAmount());

// ...

if (fitnessCache.TryGetValue(amounts, out var cachedFitness))
    return cachedFitness;

// ...

var fitness = Fitness(newInput);
fitnessCache.Add(amounts, fitness);
return fitness;
</code></pre>

<p><em>Amazing what you can do with a bit of mutation. Yeah I’ll go return my Elm badge now.</em></p>

<hr>

<p>So, with this fitness function created, I can now run the genetic algorithm. Let’s add a button and some more info to the UI, and start it off!</p>

<p><a href="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/04-original-and-best.png"><img src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/04-original-and-best.png" alt="Intakes on the side, and a button"></a></p>

<p><em>WELL HOT DAMN.</em> It has optimized the insulin intakes and the patient (again, sadly not <em>me</em>) is now stable inside the 4-10 mmol/l range.</p>

<p>That’s really magical. I feel like I’ve solved diabetes. Of course it’s not as simple. There will always  be unexpected things and changes you need to react to. You’ll have hypers, you’ll have hypos and it’s OK. But still. It managed to squeeze the blood glucose into the correct range.</p>

<hr>

<p>So, what do I need to make this useful <em>to me?</em></p>

<p>I feel like interactivity would go a long way. Being able to add injections or meals, move them up and down, left and right, and seeing the graph change based on that, would give the diabetic much better understanding than the <em>“OK I’m gonna inject more before lunch today and see what happens two hours from now”</em> feedback loop, or even worse, the <em>“consult a doctor every three months”</em> one. Did I mention I’m a big fan of short feedback loops?</p>

<p>Of course, it needs to simulate <em>me</em> instead of somebody else. I can’t use these optimized dosages because my body reacts differently. I need to consult this with the SmartCGMS folks, but the process will likely involve me downloading my historical blood glucose data off my Freestyle Libre sensor and somebody somewhere fitting the model parameters to that data. The math escapes me but it can be done.</p>

<p>Another issue is that the algorithm is optimized for pumps, and my basal (long-acting) insulin will need to be tracked into the model a bit differently. Right now it’s as if I was injecting 1/24-th of the dosage every hour, instead of the full dosage once a day. But once the specific insulin brand and its behaviour is tracked in the software, I will be able to call <code>.ScheduleInsulinBasal(double units)</code> and all should be well (and more precise), hopefully.</p>

<p>So, there’s a bunch of stuff yet to be done and collaborate with the SmartCGMS folks on, but I’m <em>really</em> excited by this, and feel empowered taking care of my diabetes better than I thought I could. WE’VE GOT THE TECHNOLOGY! Genetic algorithms and Markov Chains, baby!</p>

<hr>

<p>What’s that? I didn’t mention Markov Chains <em>once</em> in the article?</p>

<p>Oh yeah, well, I experimented with a bunch of stuff. To end off the blogpost, here’s a stupid random walk arriving at a value iteratively. (Change each intake by a random value <code>-2..+2</code>, and if the fitness of that tweaked input is better, keep the change, otherwise rollback.)</p>

<video src="https://martin.janiczek.cz/assets/images/2024-07-23-taking-my-diabetes-treatment-into-my-own-hands/recording.mp4" controls="">
</video>

    <hr>





  </div>

  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can opt out of airport face scans (263 pts)]]></title>
            <link>https://www.vox.com/future-perfect/360952/summer-travel-airport-facial-recognition-scan</link>
            <guid>41051327</guid>
            <pubDate>Tue, 23 Jul 2024 21:50:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vox.com/future-perfect/360952/summer-travel-airport-facial-recognition-scan">https://www.vox.com/future-perfect/360952/summer-travel-airport-facial-recognition-scan</a>, See on <a href="https://news.ycombinator.com/item?id=41051327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here’s something I’m embarrassed to admit: Even though I’ve been reporting on <a href="https://www.vox.com/future-perfect/2019/4/27/18518598/ai-facial-recognition-ban-apple-amazon-microsoft">the problems</a> with <a href="https://www.vox.com/future-perfect/2019/5/16/18625137/ai-facial-recognition-ban-san-francisco-surveillance">facial recognition</a> for <a href="https://www.theatlantic.com/international/archive/2018/08/china-surveillance-technology-muslims/567443/">half a dozen years</a>, I have allowed my face to be scanned at airports. Not once. Not twice. Many times. </p><p>There are lots of reasons for that. For one thing, traveling is stressful. I feel time pressure to make it to my gate quickly and social pressure not to hold up long lines. (This alone makes it feel like I’m not truly consenting to the face scans so much as being coerced into them.) Plus, I’m always getting “randomly selected” for additional screenings, maybe because of my Middle Eastern background. So I get nervous about doing anything that might lead to extra delays or interrogations.</p><p>But the main reason I haven’t declined airport face scans is actually very simple: I had no idea I could opt out. </p><p>It turns out that saying no is not only doable, but <a href="https://keepbeyond.com/optout/">surprisingly easy</a> — at least in theory. Everyone, regardless of citizenship, can opt out when it comes to domestic flights in the US. (For international flights, US citizens can opt out but foreign nationals have to participate in face scanning, <a href="https://www.cbp.gov/about/congressional-resources/testimony/statement-record-assessing-cbps-use-facial-recognition-technology">with some exceptions</a>.) Simply stand away from the camera or keep your face covered with a mask, present your ID, and say, “I opt out of biometrics. I want the standard verification process.” </p><div><p>In theory, an officer is then supposed to manually look over your ID and compare it to your face, as they used to do before facial recognition. But in practice, there have been <a href="https://www.washingtonpost.com/technology/2023/07/11/tsa-airport-security-facial-recognition/">reports of passengers — even a senator — facing resistance or intimidation</a> when they try to go this route. </p></div><p>The Transportation Security Administration (TSA) and Customs and Border Protection (CBP) are also supposed to have clear signs informing passengers of the right to opt out. But at many airports, you have to look really, really hard to spot that message. Be prepared to crane your neck at an unnatural angle or squint at a very small font!</p><p>This is why the Algorithmic Justice League, a nonprofit that sheds light on AI harms, launched a campaign this month called <a href="https://www.ajl.org/campaigns/fly">“Freedom Flyers”</a> to raise awareness of your right to opt out. The timing is perfect: The TSA <a href="https://thehill.com/regulation/transportation/4738416-transportation-security-administration-record-air-travel-day/">recorded</a> an all-time record day for air travel on June 23, with nearly 3 million people screened at the country’s airports as summer vacation season picked up. </p><p>Now is the ideal time to make sure you know your rights when you pass through airport security — and understand exactly what’s at stake. The implications go way beyond air travel. </p><div><p id="how-facial-recognition-works-at-the-airport"><h2>How facial recognition works at the airport</h2></p></div><p>In the US, <a href="https://apnews.com/article/facial-recognition-tsa-airport-security-privacy-7b97462591c49184d1cb19cda9c95211">over 80 airports</a> are currently piloting facial recognition technology. The TSA’s goal is to roll out the tech in all of the more than 430 airports that it covers, <a href="https://www.tsa.gov/sites/default/files/tsa_biometrics_roadmap.pdf">arguing</a> that this kind of automation would reduce “friction” at airports — meaning, presumably, how long it takes passengers to move through security. </p><p>That should raise some eyebrows, because there are known risks with this AI technology, from the possibility that your face data will be <a href="https://www.oversight.gov/report/dhs/review-cbps-major-cybersecurity-incident-during-2019-biometric-pilot">stolen due to breaches</a> to the chance that you’ll be <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">misidentified as a criminal suspect — and jailed</a>. Neither of these are hypothetical scenarios; the former has happened due to CBP system vulnerabilities and the latter has happened at the hands of police. And then, of course, there’s <a href="https://www.vox.com/future-perfect/22916602/ai-bias-fairness-tradeoffs-artificial-intelligence">AI bias</a>; facial recognition tech is known to disproportionately <a href="https://www.vox.com/future-perfect/2019/4/19/18412674/ai-bias-facial-recognition-black-gay-transgender">misidentify people of color</a>. (A CBP spokesperson insisted that the agency’s facial comparison algorithm “shows virtually no measurable differential performance in results based on demographic factors.”)</p><p>But as dangerous as face recognition can be if it goes wrong, a greater concern could be what happens if it’s seen to work as intended. When I asked <a href="https://www.vox.com/future-perfect/23365558/future-perfect-50-ai-joy-buolamwini-founder-algorithmic-justice-league">Joy Buolamwini</a>, the founder of the Algorithmic Justice League, what worries her about the use of this tech in airports, she said, “The big one for me is normalizing surveillance.”</p><p>Buolamwini argued that airport face recognition is a way of acclimating the public to having more and more sensitive information taken. “I see this on a longer trajectory,” she said. “And they’ve shown you the trajectory.”</p><p>She was referring to <a href="https://www.tsa.gov/sites/default/files/tsa_biometrics_roadmap.pdf">a roadmap released in 2018 by the TSA</a>. It distinguishes between two types of facial recognition: There’s one-to-one matching, where the TSA compares the photo in your passport with the photo they take of you at the airport, to make sure that the photos match. (If you ever use your face to unlock your iPhone, this is the kind of facial recognition you’re using.)</p><p>Then there’s one-to-many matching, where your image is compared with images of others. One-to-many matching is already in use by CBP and airline partners in that they compare a passenger’s photo to a database of government documents (like US passports) for verification, TSA press secretary Carter Langston told me by email.</p><p>A particularly worrisome form of one-to-many matching is live biometrics. “Live biometrics is the <em>Minority Report</em> kind of thing — where you’re just walking around and they can identify you,” Buolamwini said. And if everyone’s face becomes fair game for live biometrics, your likeness could one day be checked against a criminal database any time you walk through a drug store or show up at a protest, which may <a href="https://www.vox.com/future-perfect/2019/5/16/18625137/ai-facial-recognition-ban-san-francisco-surveillance">create a dangerous chilling effect</a> across society. </p><p>The TSA’s own 2018 roadmap says they aim to use “live biometrics” in the future. However, Langston disputed Buolamwini’s interpretation of that term. “That interpretation of TSA’s use case is nothing that I have heard anyone involved in the program indicate. TSA’s use case is and continues to be about identity verification,” he told me.</p><p>For now, Buolamwini said, “You might hear people say ‘Oh, we’re only doing one-to-one matching. You show us your ID, you show us your face, and we delete the data.’” But, she stressed, the full story is more complicated.</p><div><p id="do-airports-really-delete-your-photo-after-taking-it"><h2>Do airports really delete your photo after taking it?</h2></p></div><p>The first thing to know is that if you’re not a US citizen, you have no guarantee that your photo will be deleted. </p><p>In fact, <a href="https://www.cbp.gov/sites/default/files/assets/documents/2022-Sep/CPE%20Final%20Report%20Traveler%20Verification%20Service%2020220815%20Final_%20Redacted_0.pdf">according to CBP documents</a>, “Facial images for in-scope [noncitizen] travelers are also transmitted to the Department’s Automated Biometric Identification System (IDENT) and Homeland Advanced Recognition Technology System (HART). All biometrics of in-scope travelers are transmitted to IDENT/HART as encounters and are retained for 75 years in support of immigration, border management, and law enforcement activities.”</p><p>That means your photo could end up in the database for the rest of your life. What’s more, CBP <a href="https://www.dhs.gov/sites/default/files/publications/privacy-pia-cbp056-tvs-february2021.pdf">notes</a> that “CBP may share information with federal, state, and local authorities, which may be authorized to use the information for purposes beyond the scope of CBP’s mission.”</p><p>If you’re a US citizen, you might breathe a bit easier upon reading on the CBP <a href="https://www.cbp.gov/travel/biometrics/biometric-privacy-policy">website</a>, “CBP retains U.S. citizen photos for no more than 12 hours after identity verification, and only for continuity of operations purposes.” But even so, Buolamwini says, there’s reason to wonder whether all your data is really deleted after those 12 hours.</p><p>When you submit to facial recognition, the tech analyzes a photo of your face and creates what’s called a “face print” or “face template.” This is not an image — it comes in the form of a series of numbers. You can think of it as your face’s metadata. </p><p>The problem is, even if airports do delete your photo, that does not necessarily mean they’re deleting your face print. And that face print is the real informational gold. Researchers have shown that <a href="https://ieeexplore.ieee.org/document/8338413">they can reconstruct an image of your actual face</a> as long as they’ve got the face print. </p><p>I asked CBP what happens to that precious series of numbers. A CBP spokesperson did not answer the question about whether face prints get deleted in time for publication. After we published this story, the CBP spokesperson said that “CBP does not store or share the templates generated during the matching process, for either US citizens or non-citizens.”</p><div><p id="if-youve-already-let-airports-scan-your-face-is-there-a-point-in-saying-no-next-time"><h2>If you’ve already let airports scan your face, is there a point in saying no next time?</h2></p></div><p>Maybe you’re in the same situation as me. Maybe you’ve already let airports scan your face. And maybe you’re wondering whether saying no in the future will make any difference, given that your face data is probably already in a database — or two — or three. (Separate from TSA, your individual airline may also scan your face instead of your boarding pass before letting you on the plane, though airlines <a href="https://www.pcmag.com/opinions/you-can-say-no-to-face-scans-for-airplane-boarding">say</a> you can opt out for domestic flights.)</p><p>Buolamwini’s opinion? It’s definitely still worth declining the face scan next time you fly. “Every opt-out opportunity is a way to vote for your biometric rights,” she said.</p><p>We’ve already seen that when there’s enough of a public outcry, it can lead to deletion of face data. After Facebook’s facial recognition system sparked a class-action lawsuit, government investigations, and public furor, the company ended up <a href="https://www.nytimes.com/2021/11/02/technology/facebook-facial-recognition.html">deleting the face prints of more than a billion users</a> in 2021. </p><p>“Face purges can and do happen,” Buolamwini said. </p><p>Remember, the TSA’s stated reason for rolling out facial recognition in airports is to minimize friction. If you’re unhappy about the use of the tech, you can consider generating more friction next time you fly. </p><div><p><em>A version of this story originally appeared in the </em><a href="https://www.vox.com/future-perfect"><em><strong>Future Perfect</strong></em></a><em> newsletter. </em><a href="https://www.vox.com/pages/future-perfect-newsletter-signup"><em><strong>Sign up here!</strong></em></a></p></div><p><em><strong>Update, July 19, 2:30 pm ET: </strong>This story was originally published July 17 and has been updated with new comment from US Customs and Border Protection.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ESA report shows unsustainable levels of orbital debris (107 pts)]]></title>
            <link>https://payloadspace.com/esa-report-shows-unsustainable-levels-of-orbital-debris/</link>
            <guid>41051257</guid>
            <pubDate>Tue, 23 Jul 2024 21:41:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://payloadspace.com/esa-report-shows-unsustainable-levels-of-orbital-debris/">https://payloadspace.com/esa-report-shows-unsustainable-levels-of-orbital-debris/</a>, See on <a href="https://news.ycombinator.com/item?id=41051257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><p><span>Related Stories</span></p><div><div><h2> <a href="https://payloadspace.com/asteroid-watchers-fret-about-politicians-facing-dangerous-neos/" data-wpel-link="internal">Asteroid Watchers Fret About Politicians Facing Dangerous NEOs</a></h2><p>“I know what I would prefer [to do], but Congress will tell us to wait,”</p> <p><span><time datetime="2024-07-01T08:14:49-04:00">July 1, 2024</time><time datetime="2024-07-01T18:25:23-04:00">July 1, 2024</time></span></p></div><div><h2> <a href="https://payloadspace.com/spacex-will-destroy-the-iss/" data-wpel-link="internal">SpaceX Will Destroy The ISS</a></h2><p>NASA will pay SpaceX $843M to develop the US Deorbit Vehicle.</p> <p><span><time datetime="2024-06-27T08:13:19-04:00">June 27, 2024</time><time datetime="2024-06-27T08:13:21-04:00">June 27, 2024</time></span></p></div><div><h2> <a href="https://payloadspace.com/kayhan-space-releases-satcat-space-intelligence-tool/" data-wpel-link="internal">Kayhan Space Releases Satcat Space Intelligence Tool</a></h2><p>The Satcat database catalogs 60,000+ objects in orbit, from active satellites to drifting debris</p> <p><span><time datetime="2024-06-14T08:45:39-04:00">June 14, 2024</time><time datetime="2024-06-14T09:21:50-04:00">June 14, 2024</time></span></p></div><div><h2> <a href="https://payloadspace.com/astroscale-shares-surge-in-market-debut/" data-wpel-link="internal">Astroscale Shares Surge in Market Debut</a></h2><p>Astroscale’s IPO proves there’s value in junk stocks—space junk stocks, that is.</p> <p><span><time datetime="2024-06-06T08:23:46-04:00">June 6, 2024</time><time datetime="2024-06-06T08:23:48-04:00">June 6, 2024</time></span></p></div></div></div></div>]]></description>
        </item>
    </channel>
</rss>