<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 08 Apr 2024 08:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Groq CEO: 'We No Longer Sell Hardware' (149 pts)]]></title>
            <link>https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/</link>
            <guid>39964590</guid>
            <pubDate>Sun, 07 Apr 2024 22:40:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/">https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/</a>, See on <a href="https://news.ycombinator.com/item?id=39964590">Hacker News</a></p>
Couldn't get https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Xkcd 1928: Seven Years (2017) (117 pts)]]></title>
            <link>https://xkcd.com/1928/</link>
            <guid>39963643</guid>
            <pubDate>Sun, 07 Apr 2024 20:33:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xkcd.com/1928/">https://xkcd.com/1928/</a>, See on <a href="https://news.ycombinator.com/item?id=39963643">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="topRight">
<p><span><a href="https://xkcd.com/"><img src="https://xkcd.com/s/0b7742.png" alt="xkcd.com logo" height="83" width="185"></a></span>
<span id="slogan">A webcomic of romance,<br> sarcasm, math, and language.</span>
</p>
<p>
Becky Beaton, sister of fellow cartoonist Kate Beaton, has also been diagnosed with cancer. You can support her treatment <a href="https://www.youcaring.com/beckybeaton-1008390">here</a>.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook banned website that links to critical article, claims phishing (105 pts)]]></title>
            <link>https://bsky.app/profile/mosseri.bsky.social/post/3kpimfpxjkh2r</link>
            <guid>39962644</guid>
            <pubDate>Sun, 07 Apr 2024 18:24:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/mosseri.bsky.social/post/3kpimfpxjkh2r">https://bsky.app/profile/mosseri.bsky.social/post/3kpimfpxjkh2r</a>, See on <a href="https://news.ycombinator.com/item?id=39962644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Xemu: Original Xbox Emulator (160 pts)]]></title>
            <link>https://xemu.app/</link>
            <guid>39962184</guid>
            <pubDate>Sun, 07 Apr 2024 17:18:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xemu.app/">https://xemu.app/</a>, See on <a href="https://news.ycombinator.com/item?id=39962184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><div><p><img id="xbox-logo" src="https://xemu.app/xbox_logo.png" width="450"></p><div><h2><canvas id="logo-canvas"></canvas> <img alt="xemu logo" id="logo-fallback" src="https://xemu.app/logo-green-jumbotron.svg"></h2><h4>Original Xbox Emulator</h4><p>A free and open-source application that emulates the original Microsoft Xbox game console, enabling people to play their original Xbox games on Windows, macOS, and Linux systems.</p><div><p>Version 0.7.120 (Mar 23, 2024) <br></p></div></div></div><div><p><img src="https://xemu.app/linux_title_bar_dark_2x.png"></p></div><div><div><h4><i></i>Open Source</h4><p>The source code for xemu is available on <a href="https://github.com/xemu-project/xemu">GitHub</a>. You are invited to help improve the project! Learn more <a href="https://xemu.app/docs/dev/">here</a>.</p></div><div><h4><i></i>Cross Platform</h4><p>xemu runs natively on Windows, macOS, and Linux platforms. Binaries are available for all platforms, or you can build from source if desired. Learn more <a href="https://xemu.app/docs/download/">here</a>.</p></div><div><h4><i></i>Low Level Emulation</h4><p>xemu emulates the hardware of the original Xbox, providing superior compatibility with kernels, titles, and homebrew applications.</p></div><div><h4><i></i>Controller Support</h4><p>Built on <a href="https://www.libsdl.org/">SDL2</a>, xemu supports virtually all controllers. Connect up to 4 controllers at any time, just like a real Xbox. Learn more <a href="https://xemu.app/docs/input/">here</a>.</p></div><div><h4><i></i>Snapshots (Save States)</h4><p>No need to wait for game checkpoints. xemu supports saving the current machine state and loading it back up at any time. Learn more <a href="https://xemu.app/docs/snapshots/">here</a>.</p></div><div><h4><i></i>Render Scaling</h4><p>Breathe new life into your original Xbox games by easily increasing the resolution that games render at, on the fly. Scale up from 480p to 1080p at the click of a button.</p></div><div><h4><i></i>Networking</h4><p>Connect to other instances of xemu and real Xboxes, locally or over the Internet. Supports tunneling services and Xbox Live recreation projects. Learn more <a href="https://xemu.app/docs/networking/">here</a>.</p></div><div><h4><i></i>Community</h4><p>xemu has a thriving online community of original Xbox fans. Set up multiplayer matches, get help running xemu, and more by joining our community on <a href="https://discord.gg/ayyjsuM">Discord</a>!</p></div></div><div><h2 id="compatibility">Compatibility</h2><div><p><b>Note:</b> Title compatibility status is provided by volunteer reporters in the community, as the reporter experienced the title in the current version of xemu on their computer at time of reporting. As the project evolves, reports may need to be updated. You are invited to help improve the project by submitting an updated compatibility report. Join the Discord server to learn how to contribute!</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PumpkinOS, a Re-Implementation of PalmOS (303 pts)]]></title>
            <link>https://github.com/migueletto/PumpkinOS</link>
            <guid>39962023</guid>
            <pubDate>Sun, 07 Apr 2024 16:55:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/migueletto/PumpkinOS">https://github.com/migueletto/PumpkinOS</a>, See on <a href="https://news.ycombinator.com/item?id=39962023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">PumpkinOS</h2><a id="user-content-pumpkinos" aria-label="Permalink: PumpkinOS" href="#pumpkinos"></a></p>
<p dir="auto">PumpkinOS is a re-implementation of PalmOS that runs on modern architectures (x86, ARM, etc).
It is not your average PalmOS emulator (it does NOT require a PalmOS ROM), but it can run m68K PalmOS applications.
For a series of articles describing various aspects of PumpkinOS, look here: <a href="https://pmig96.wordpress.com/category/palmos/" rel="nofollow">https://pmig96.wordpress.com/category/palmos/</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/migueletto/PumpkinOS/blob/master/screenshots/pumpkin.png"><img src="https://github.com/migueletto/PumpkinOS/raw/master/screenshots/pumpkin.png" alt=""></a></p>
<p dir="auto">Launcher is the first application that runs when PumpkinOS starts. It shows a panel from which you can start other applications.
Preferences will eventually contain all preference options for configuring PumpkinOS.
Command is a command shell, still experimental.</p>
<p dir="auto">This release contains the four PIM applications found on PalmOS: AddressBook, MemoPad, ToDoList and DateBook. The source code for these applications
were distributed in one or more PalmOS SDks and were adapted for correct compilation on PumpkinOS.
Records created by AddressBook and MemoPad should be compatible with their PalmOS counterparts. Because of differences in
word size en endianness, however, records created by ToDoList and DateBook are not compatible.
These applications were tested just to the point where I could create and edit a few records. There are still some quirks, and some functions were not tested at all.
The goal here is just to offer a view of what to expect from PumpkinOS in the future.</p>
<p dir="auto">I am planing to setup a bug tracker to document enhancements and bugs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Licensing</h2><a id="user-content-licensing" aria-label="Permalink: Licensing" href="#licensing"></a></p>
<p dir="auto">PumpkinOS is licensed under the GPL v3.
The license directory contains information on specific licenses of the various components used in PumpkinOS.
If you think something is missing and/or incorrect, please let me know.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">You have to build PumpkinOS from source. No IDE is required, you can build from the command line.
If you use 64-bits Windows, you can use MSYS2 (<a href="https://www.msys2.org/" rel="nofollow">https://www.msys2.org/</a>). Download the installer and follow the instructions there.
Open a MINGW64 terminal (the one with the blue 'M' icon) and install these additional packages:</p>
<div data-snippet-clipboard-copy-content="pacman -S gcc binutils make git"><pre><code>pacman -S gcc binutils make git
</code></pre></div>
<p dir="auto">Next clone the PumpkinOS repository:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/migueletto/PumpkinOS.git"><pre><code>git clone https://github.com/migueletto/PumpkinOS.git
</code></pre></div>
<p dir="auto">Finally go to the source directory of the PumpkinOS repository you have just cloned and run the make script:</p>
<div data-snippet-clipboard-copy-content="cd PumpkinOS/src
./mk.sh Msys 64"><pre><code>cd PumpkinOS/src
./mk.sh Msys 64
</code></pre></div>
<p dir="auto">If everything goes well, you will have a pumpkin.exe in the root directory, some DLLs in the bin directory, and some PRC files in the vfs/app_install directory.</p>
<p dir="auto">There is also experimental support for 32-bits Windows (Vista or later. It will not work on Windows XP).
Open a MINGW32 terminal (the one with the gray 'M' icon) and install this additional package:</p>
<div data-snippet-clipboard-copy-content="pacman -S mingw-w64-i686-gcc"><pre><code>pacman -S mingw-w64-i686-gcc
</code></pre></div>
<p dir="auto">From there, compile using (note that now argument is 32, for 32-bits):</p>
<div data-snippet-clipboard-copy-content="cd PumpkinOS/src
./mk.sh Msys 32"><pre><code>cd PumpkinOS/src
./mk.sh Msys 32
</code></pre></div>
<p dir="auto">If you are using a 64-bits Linux-based OS (like Debian, Ubuntu, etc), you also need gcc, binutils, make and git. If you are a developer,
there is a chance you already have those. If they are not installed, follow the instructions to download additional packages on your specific Linux distribution.
You must also install the SDL2 development package (the package that contains the libraries and the headers). On a Debian distribution, it is probably something like:</p>
<div data-snippet-clipboard-copy-content="sudo apt install gcc binutils make git libsdl2-dev"><pre><code>sudo apt install gcc binutils make git libsdl2-dev
</code></pre></div>
<p dir="auto">Again, you must clone the repository and compile it using:</p>
<div data-snippet-clipboard-copy-content="cd PumpkinOS/src
./mk.sh GNU/Linux 64"><pre><code>cd PumpkinOS/src
./mk.sh GNU/Linux 64
</code></pre></div>
<p dir="auto">On Windows 11 and recent releases of Windows 10, it is also possible to build PumpkinOS on WSL (Windows Subsystem for Linux, version 2).
Open a WSL terminal and follow the same instructions for a Linux build.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running</h2><a id="user-content-running" aria-label="Permalink: Running" href="#running"></a></p>
<p dir="auto">On 64-bits Windows, run pumpkin.bat. On 32-bits Windows, run pumpkin32.bat. On Linux or WSL, run pumpkin.sh. PumpkinOS will open on a new window.
On WSL you may need to run a X-Window Manager, otherwise the PumpkinOS window will not have a border.</p>
<p dir="auto">When you run PumpkinOS, all PRCs inside vfs/app_install will be removed and expanded into folders inside vfs/app_storage.
Please keep in mind that everything is pretty much experimental at this stage, so expect a few issues here and there.
After either a successful or an unsuccessful run, you will find a pumpkin.log file on the root directory.
If something goes wrong, look for lines marked with an "E" on the third column of this file.
You can reach me for questions (and send me your log file if you wish).</p>
<p dir="auto">The Windows version implements Drag &amp; Drop functionality. You can drag a PalmOS PRC over the PumpkinOS window and hopefully
it will be installed and show up in the Launcher. The Linux version lacks this functionality. For now, you have to manually copy PRCs
to the vfs/app_install directory and restart PumpkinOS.</p>
<p dir="auto">If you really want to, you can debug PumpkinOS with gdb on Windows, Linux and WSL. On Windows, edit pumpkin.bat and change the last line to (you should also add the Windows equivalent of the /usr/bin directory of your MSYS2 installation the the PATH):</p>
<div data-snippet-clipboard-copy-content="gdb.exe --args .\pumpkin.exe -d 1 -f pumpkin.log -s libscriptlua.dll script\pumpkin_windows.lua"><pre><code>gdb.exe --args .\pumpkin.exe -d 1 -f pumpkin.log -s libscriptlua.dll script\pumpkin_windows.lua
</code></pre></div>
<p dir="auto">On Linux and WSL edit pumpkin.sh and change the last line to:</p>
<div data-snippet-clipboard-copy-content="gdb --args ./pumpkin -d 1 -f pumpkin.log -s libscriptlua.so ./script/pumpkin_linux.lua"><pre><code>gdb --args ./pumpkin -d 1 -f pumpkin.log -s libscriptlua.so ./script/pumpkin_linux.lua
</code></pre></div>
<p dir="auto">I am writing a full Wiki article on source level debuging PumpkinOS.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rpgp: Pure Rust implementation of OpenPGP (148 pts)]]></title>
            <link>https://github.com/rpgp/rpgp</link>
            <guid>39961994</guid>
            <pubDate>Sun, 07 Apr 2024 16:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rpgp/rpgp">https://github.com/rpgp/rpgp</a>, See on <a href="https://news.ycombinator.com/item?id=39961994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">rPGP</h2><a id="user-content-rpgp" aria-label="Permalink: rPGP" href="#rpgp"></a></p>
<p dir="auto"><a href="https://crates.io/crates/pgp" rel="nofollow"><img src="https://camo.githubusercontent.com/10e1dfae1eaa5abb346385ac94190b36ba0a4a7537ef6e8d320369e3233c8ce2/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f7067702e7376673f7374796c653d666c61742d737175617265" alt="crates.io" data-canonical-src="https://img.shields.io/crates/v/pgp.svg?style=flat-square"></a>
<a href="https://docs.rs/crate/pgp/" rel="nofollow"><img src="https://camo.githubusercontent.com/b5f09ebb64a35fd5c5c38e5e7a4727df4b0b833e987a210d5250fee2d62b7227/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6f6e6c696e652d626c75652e7376673f7374796c653d666c61742d737175617265" alt="Documentation" data-canonical-src="https://img.shields.io/badge/docs-online-blue.svg?style=flat-square"></a>
<a href="https://github.com/rpgp/rpgp/actions?query=workflow%3ACI+branch%3Amaster"><img src="https://github.com/rpgp/rpgp/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/46a8c8f591a66778ad0fb692f11b6efe5362f28b03c7f9c70e6cf1f221f61c90/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72757374632d312e37302b2d626c75652e737667"><img src="https://camo.githubusercontent.com/46a8c8f591a66778ad0fb692f11b6efe5362f28b03c7f9c70e6cf1f221f61c90/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72757374632d312e37302b2d626c75652e737667" alt="minimum rustc 1.70" data-canonical-src="https://img.shields.io/badge/rustc-1.70+-blue.svg"></a>
<a href="https://deps.rs/repo/github/rpgp/rpgp" rel="nofollow"><img src="https://camo.githubusercontent.com/9bd706a0e53959f32a739b515a2472a4a86590d086051da0c98804d51e15ed68/68747470733a2f2f646570732e72732f7265706f2f6769746875622f727067702f727067702f7374617475732e737667" alt="dependency status" data-canonical-src="https://deps.rs/repo/github/rpgp/rpgp/status.svg"></a>
<a href="https://github.com/rpgp/rpgp/blob/master/LICENSE.md"><img src="https://camo.githubusercontent.com/d3e815ecf6a689482215a733c6e5a9075c240b4498711cbce7c67ff59c971ff6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d4954253246417061636865322e302d677265656e2e7376673f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/badge/License-MIT%2FApache2.0-green.svg?style=flat-square"></a></p>
<blockquote>
<p dir="auto">OpenPGP implemented in pure Rust, permissively licensed</p>
</blockquote>
<p dir="auto">rPGP is the only pure Rust implementation of OpenPGP, following <a href="https://tools.ietf.org/html/rfc4880.html" rel="nofollow">RFC4880</a> and <a href="https://tools.ietf.org/html/rfc2440" rel="nofollow">RFC2440</a>. It offers a minimal low-level API and does not prescribe trust schemes or key management policies. It fully supports all functionality required by the <a href="https://autocrypt.org/level1.html" rel="nofollow">Autocrypt 1.1 e-mail encryption specification</a>.</p>
<p dir="auto">rPGP is regularly published as <a href="https://crates.io/crates/pgp/" rel="nofollow">the <code>pgp</code> Crate</a> and its <a href="https://crates.io/crates/rsa" rel="nofollow">RSA</a> implementation
lives under the collective <a href="https://github.com/RustCrypto/RSA">RustCrypto umbrella</a>.
For ECC crypto support we are using <a href="https://crates.io/crates/curve25519-dalek" rel="nofollow">Curve25519-dalek</a>.</p>
<blockquote>
<p dir="auto">Please note that the API is not well documented yet. You may check out
the tests which exercise the API. Please open issues here if if you are
attempting to use rPGP and need help.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status (Last updated: October 2019)</h2><a id="user-content-status-last-updated-october-2019" aria-label="Permalink: Status (Last updated: October 2019)" href="#status-last-updated-october-2019"></a></p>
<p dir="auto">rPGP and its RSA dependency got an independent security audit mid 2019,
see here for the <a href="https://delta.chat/assets/blog/2019-first-security-review.pdf" rel="nofollow">full report from IncludeSecurity</a>.
No critical flaws were found and we have fixed most high, medium and low risk ones.</p>
<p dir="auto">rPGP is used in production by <a href="https://delta.chat/" rel="nofollow">Delta Chat, the e-mail based messenger app suite</a>, successfully running on Windows, Linux, macOS, Android and iOS in 32bit (only Windows and Android) and 64 bit builds (for the other platforms).</p>
<p dir="auto">More details on platform and OpenPGP implementation status:</p>
<ul dir="auto">
<li><a href="https://github.com/rpgp/rpgp/blob/master/STATUS.md">OpenPGP Status document</a> which describes what of OpenPGP is supported</li>
<li><a href="https://github.com/rpgp/rpgp/blob/master/PLATFORMS.md">Platform status document</a> which describes current platform support.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Experimental WASM Support</h3><a id="user-content-experimental-wasm-support" aria-label="Permalink: Experimental WASM Support" href="#experimental-wasm-support"></a></p>
<p dir="auto">When enabeling the <code>wasm</code> feature, rpgp can be compiled to run using WASM in Node.js and the supported Browsers. Experimental bindings for this can be found in <a href="https://github.com/rpgp/rpgp-js">rpgp/rpgp-js</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developement</h2><a id="user-content-developement" aria-label="Permalink: Developement" href="#developement"></a></p>
<p dir="auto">To run the stress tests,</p>
<div dir="auto" data-snippet-clipboard-copy-content="> git submodule update --init --recursive
> cargo test --release -- --ignored"><pre><span>&gt;</span> git submodule update --init --recursive
<span>&gt;</span> cargo <span>test</span> --release -- --ignored</pre></div>
<p dir="auto">To enable debugging, add</p>
<div dir="auto" data-snippet-clipboard-copy-content="use pretty_env_logger;
let _ = pretty_env_logger::try_init();"><pre><span>use</span> pretty_env_logger<span>;</span>
<span>let</span> _ = pretty_env_logger<span>::</span><span>try_init</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">And then run tests with <code>RUST_LOG=pgp=info</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How is rPGP different from Sequoia?</h2><a id="user-content-how-is-rpgp-different-from-sequoia" aria-label="Permalink: How is rPGP different from Sequoia?" href="#how-is-rpgp-different-from-sequoia"></a></p>
<p dir="auto">Some key differences:</p>
<ul dir="auto">
<li>
<p dir="auto">rPGP has a more permissive license than Sequoia, which allows a broader usage</p>
</li>
<li>
<p dir="auto">rPGP is a library with a well-defined, relatively small feature-set
where Sequoia also tries to be a replacement for the GPG command line tool</p>
</li>
<li>
<p dir="auto">All crypto used in rPGP is implemented in pure Rust,
whereas Sequoia by default uses Nettle, which is implemented in C.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimum Supported Rust Version (MSRV)</h2><a id="user-content-minimum-supported-rust-version-msrv" aria-label="Permalink: Minimum Supported Rust Version (MSRV)" href="#minimum-supported-rust-version-msrv"></a></p>
<p dir="auto">All crates in this repository support Rust 1.70 or higher. In future minimally supported version of Rust can be changed, but it will be done with a minor version bump.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">LICENSE</h2><a id="user-content-license" aria-label="Permalink: LICENSE" href="#license"></a></p>
<p dir="auto">MIT or Apache 2.0</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution</h2><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">Unless you explicitly state otherwise, any contribution submitted
for inclusion in rPGP by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What John von Neumann did at Los Alamos (2020) (181 pts)]]></title>
            <link>https://3quarksdaily.com/3quarksdaily/2020/10/what-john-von-neumann-really-did-at-los-alamos.html</link>
            <guid>39961910</guid>
            <pubDate>Sun, 07 Apr 2024 16:42:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://3quarksdaily.com/3quarksdaily/2020/10/what-john-von-neumann-really-did-at-los-alamos.html">https://3quarksdaily.com/3quarksdaily/2020/10/what-john-von-neumann-really-did-at-los-alamos.html</a>, See on <a href="https://news.ycombinator.com/item?id=39961910">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-187398"><div><p><strong>by Ashutosh Jogalekar</strong></p><figure id="attachment_187408" aria-describedby="caption-attachment-187408"><img fetchpriority="high" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/1_NAhEjpkDIARExylyZ9CQ7w-360x241.png" alt="" width="360" height="241" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/1_NAhEjpkDIARExylyZ9CQ7w-360x241.png 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_NAhEjpkDIARExylyZ9CQ7w-768x514.png 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_NAhEjpkDIARExylyZ9CQ7w-300x201.png 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_NAhEjpkDIARExylyZ9CQ7w.png 999w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187408"><em>John von Neumann (Image: Life Magazine)</em></figcaption></figure><p><span>During a wartime visit to England in early 1943, John von Neumann wrote a letter to his fellow mathematician Oswald Veblen at the Institute for Advanced Study in Princeton, saying:</span></p><p><i><span>“I think I have learned a great deal of experimental physics here, particularly of the gas dynamical variety, and that I shall return a better and impurer man. I have also developed an obscene interest in computational techniques…”</span></i></p><p><span>This seemingly mundane communication was to foreshadow a decisive effect on the development of two overwhelmingly important aspects of 20th and 21st century technology – the development of computing and the development of nuclear weapons.</span></p><p><span>Johnny von Neumann was the multifaceted intellectual diamond of the 20th century. He contributed so many seminal ideas to so many fields so quickly that it would be impossible for any one person to summarize, let alone understand them. He may have been the last universalist in mathematics, having almost complete command of both pure and applied mathematics. But he didn’t stop there. After making fundamental contributions to operator algebra, set theory and the foundations of mathematics, he revolutionized at least two different and disparate fields – economics and computer science – and made contributions to a dozen others, each of which would have been important enough to enshrine his name in scientific history.</span></p><p><span>But at the end of his relatively short life which was cut down cruelly by cancer, von Neumann had acquired another identity – that of an American patriot who had done more than almost anyone else to make sure that his country was well-defended and ahead of the Soviet Union in the rapidly heating Cold War. Like most other contributions of this sort, this one had a distinctly Faustian gleam to it, bringing both glory and woe to humanity’s experiments in self-elevation and self-destruction. </span><span id="more-187398"></span></p><p><span>The origins of Johnny’s far-reaching accomplishments lay in the Manhattan Project. But this fact alone is curious: Von Neumann was never part of the regular cast of characters at Los Alamos which included Robert Oppenheimer, Hans Bethe, Edward Teller, Richard Feynman, Enrico Fermi and other world-renowned scientists; he features as a relatively minor player in most standard histories of the project. He visited as a consultant a few times a year. And yet in some sense, his contribution equalled or even exceeded in the long-term the contributions made by his fellow scientists. This discrepancy between his role in the Manhattan Project and the importance of his work speaks to both his awesome intellect and, more interestingly, to a core element in the history and philosophy of science in which ideas and technologies unexpectedly intersect and piggyback on each other. To understand this more fully, it’s important to understand von Neumann’s origins.</span></p><p><b>From Budapest to Los Alamos via Princeton</b></p><figure id="attachment_187409" aria-describedby="caption-attachment-187409"><img decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg-360x274.png" alt="" width="360" height="274" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg-360x274.png 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg-768x584.png 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg-1024x779.png 1024w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg-300x228.png 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_s7FpypggCfGVlPRp2l8Sfg.png 1039w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187409"><em>Johnny at age 11 with his cousin (Image: Michael Vonneumann and Cantor’s Paradise)</em></figcaption></figure><p><span>Von Neumann was the quintessential product of turn-of-the-century Jewish affluence and intellectual achievement in the Austro-Hungarian empire. Born in 1903 to a wealthy banker and his wife in Budapest, Johnny was one of history’s great child prodigies, speaking half a dozen languages and learning calculus by the time he was barely past the first decade of his life. He had an amazing photographic memory and an intense interest in history that stayed with him all his life and dazzled his friends and colleagues; as the story goes, by the time he was eight he had read and annotated all forty-three volumes of a comprehensive world history written by the German historian Wilhelm Oncken, and he used to stun his parents’ friends by reciting entire pages from the phone directory as a child. As he grew up he collected around himself some of the great Hungarian minds of the 20th century – Eugene Wigner, Leo Szilard, Edward Teller. Later all of them became émigrés to the United States, and their superior intelligence led others to joke that they were Martians who had learnt to perfectly mimic human beings. Toward the end of his life, Eugene Wigner who won a Nobel Prize for his work on nuclear structure was asked why a tiny country like Hungary produced so many scientific geniuses. Wigner said that Hungary had produced only one genius – Johnny von Neumann.</span></p><p><span>After getting two degrees in Zurich and Budapest, one in mathematics and one in chemical engineering – the latter to to please his father who, like many Jewish fathers, worried that his son might not be able to get a job in spite of his great intellect but due to his Jewish background – von Neumann became an assistant to David Hilbert, one of the 20th century’s greatest mathematicians. While his early forays were in set theory and operator algebra, in a short time he left a blazing trail of contributions whose depth and breadth would be unequalled in the annals of 20th century mathematics and science. By his 30th birthday, he had solved Hilbert’s <a href="https://en.wikipedia.org/wiki/Hilbert%27s_fifth_problem">fifth problem</a> for compact groups, proved the <a href="https://en.wikipedia.org/wiki/Ergodic_theory#Mean_ergodic_theorem">mean ergodic theorem</a>, provided a mathematical <a href="https://www.abebooks.com/servlet/BookDetailsPL?bi=4096840786&amp;searchurl=fe%3Don%26kn%3DMathematische%2BGrundlagen%2Bder%2BQuantenmechanik%26sortby%3D17&amp;cm_sp=snippet-_-srp1-_-title8">foundation</a> for quantum mechanics, proved the <a href="https://mathworld.wolfram.com/MinimaxTheorem.html#:~:text=The%20fundamental%20theorem%20of%20game,John%20von%20Neumann%20in%201928.&amp;text=It%20also%20turns%20out%20that,strategy%2C%20there%20are%20infinitely%20many.">minimax theorem</a>,&nbsp;started laying the foundation for game theory in economics and done important work in the foundations of mathematics.</span></p><p><span>He had already become acutely prescient about the deteriorating political situation in Europe. After Hitler became chancellor of Germany in 1933, Johnny started making trips to the United States where he had been invited to lecture at the Institute for Advanced Study. The institute had been set up as a kind of pure thinkers’ heaven by the renowned educator Abraham Flexner, and there were few fields more suited to pure thought than mathematics, philosophy and theoretical physics. Flexner went to great lengths to snag the greatest fish of them all – Albert Einstein. By 1933 Einstein had already been the target of virulent anti-Semitic attacks and was looking for a new home. After visiting a few universities in the United States and England, Flexner lured him to Princeton with the promise of a very generous salary and complete freedom to explore his interests without any administrative or teaching responsibilities. But even by 1933, when Einstein was fifty-four, the twenty-nine-year-old von Neumann was considered important enough to be made one of the first professors at the institute, along with Einstein, Hermann Weyl and Oswald Veblen, a mathematician who was as adept at hacking away at the brush in the Princeton woods as he was at problems in geometry and topology. Veblen was to become an important mentor to Johnny. When the institute’s plans to hire Weyl fell through, Johnny was hired as a permanent faculty member.</span></p><p><span>Ever since he moved to the United States, Johnny showed a great love for his adopted country. His home was the scene of weekly raucous parties, and he loved fast cars, expensive suits and hobnobbing with the rich and famous. His friend Eugene Wigner who had shipped from Hungary as a joint appointment with von Neumann recalled how Johnny was refreshed by the new world and by the youthful enthusiasm he saw there which rejected the reactionary and orthodox ideas of old world Europe, a motivation not uncommon among American émigrés. As Europe crumbled in the face of fascism in the next few years, Johnny was joined by European scientists like Bethe, Fermi and Weisskopf who became household names, started great schools of science and catapulted America into the front ranks of science and technology, a position that it continues to hold in no small part due to such immigrants.</span></p><figure id="attachment_187406" aria-describedby="caption-attachment-187406"><img decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/game-theory-360x156.jpg" alt="" width="360" height="156" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/game-theory-360x156.jpg 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/game-theory-768x332.jpg 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/game-theory-300x130.jpg 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/game-theory.jpg 898w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187406"><em>Oskar Morgenstern and von Neumann published “Theory of Games and Economic Behavior” in 1944, when Johnny was already deep into defense work (Image: ThatsMaths)</em></figcaption></figure><p><span>Because of his intense interest and background in history, Johnny could already see in 1935 that there would be some kind of war between Germany and other countries. By 1937 he had started expressing interest in the more applied parts of mathematics. His work on the minimax theorem and the foundations of quantum mechanics was already dazzling, especially for a pure mathematician, and in Princeton he had struck up a friendship with another Austrian emigre, the economist Oskar Morgenstern. During the next few years he would embark on the writing “<a href="https://www.abebooks.com/servlet/BookDetailsPL?bi=22795883348&amp;searchurl=fe%3Don%26sortby%3D17%26tn%3Dtheory%2Bgames%2Beconomic%2Bbehavior&amp;cm_sp=snippet-_-srp1-_-title7">The Theory of Games and Economic Behavior</a>” with Morgenstern which is now considered the foundation of game theory. </span></p><p><span>What accounted for Johnny’s forays into applied mathematics and away from pure mathematics? One reason was certainly his capacious mind that roamed over diverse domains out of sheer burning intellectual curiosity, but I also personally feel that it was partly a disillusionment with the foundations of mathematics, a disillusionment that was made acutely clear by the work of his friend and institute colleague Kurt Gödel who in 1932 had struck a blow at the foundations of mathematics through his famous <a href="https://plato.stanford.edu/entries/goedel-incompleteness/">incompleteness theorems</a>. Johnny had extended Gödel’s first theorem in short order, only to find that Gödel had already gotten there. Later he did what he could to ensure Gödel’s appointment at the institute; the two friends <a href="https://3quarksdaily.com/3quarksdaily/2019/05/life-and-death-in-new-jersey.html">now lie</a> only a few feet from each other. Gödel’s work might have convinced Johnny that making deep contributions to pure mathematics was perhaps not his forte. Fortunately for him, the escalating situation in Europe brought a new opportunity that would make full use of his intellectual powers.</span></p><p><b>Shaped charges and the Aberdeen Proving Ground</b></p><figure id="attachment_187401" aria-describedby="caption-attachment-187401"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe-270x360.jpg" alt="" width="270" height="360" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe-270x360.jpg 270w, https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe-768x1025.jpg 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe-767x1024.jpg 767w, https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe-225x300.jpg 225w, https://3quarksdaily.com/wp-content/uploads/2020/10/1024px-Charles_Edward_Munroe.jpg 1024w" sizes="(max-width: 270px) 100vw, 270px"><figcaption id="caption-attachment-187401"><em>Charles Munroe (Image: Wikipedia)</em></figcaption></figure><p><span>In 1888, an American chemist working for the navy named Charles Munroe discovered something quite wonderful and intriguing. Monroe found that when he detonated a block of explosive with the company’s name engraved on it with raised letters near a metal plate, the raised letters somehow got “transferred” into the metal plate. What was happening was that the explosive blast from the block took the shape of the letters – and stayed that way for some time. Later Munroe demonstrated this effect even more convincingly with a metal safe. As he <a href="https://en.wikisource.org/wiki/Popular_Science_Monthly/Volume_56/February_1900/The_Applications_of_Explosives_II">described it</a> in a 1900 ‘Popular Science’ article,</span></p><p><em><span>“Among the experiments made … was one upon a safe twenty-nine inches cube, with walls four inches and three quarters thick, made up of plates of iron and steel … When a hollow charge of dynamite nine pounds and a half in weight and untamped was detonated on it, a hole three inches in diameter was blown clear through the wall … The hollow cartridge was made by tying the sticks of dynamite around a tin can, the open mouth of the latter being placed downward.”</span></em></p><p><span>Thus was born the principle of the Munroe Effect and the shaped charge, a phenomenon which still has a little bit of a whiff of magic to it. The shaped charge essentially enables a precisely geometrically-shaped explosive to ephemerally create a small pocket of air that is packed with deadly force – hell in a puff of air. When this puff hits a target, even a metal safe cannot help but be cast in its shadow. Shaped charges soon started seeing use in many peaceful applications of explosives to create controlled, precisely shaped, powerful cavities in materials like rocks and metals. By the time the 1930s came along, scientists in many countries had gotten interested in them. A particularly important contribution was made by R. W. Wood, a Johns Hopkins physicist whose initial interest stemmed from morbid accidental deaths by shaped charges.</span></p><p><span><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM-360x309.png" alt="" width="360" height="309" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM-360x309.png 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM-768x659.png 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM-1024x879.png 1024w, https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM-300x257.png 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-25-at-8.39.04-PM.png 1156w" sizes="(max-width: 360px) 100vw, 360px">An<a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a220095.pdf"> illuminating history</a> of shaped charges tells the story of how the technology gradually percolated from America to Britain and back to America. But there was an ominous detour in Germany. An Austrian physicist named Rudolf Thomanek had become interested in the British and American work and was trying to develop an anti-tank gun. By 1932, he had become convinced that the Austrian authorities were “halfwits” who were uninterested – why not try to sell the concept to the much more enthusiastic German authorities? In fact, why not go straight to the top? By the end of 1935, Thomanek had pulled enough strings to find himself presenting the idea to an audience consisting of Hitler, Himmler, Goering and the General Staff of the Third Reich’s armed forces. Contrary to his later, well-deserved image as a strategist who routinely came up with unworkable and outlandish schemes, Hitler was extremely detail-oriented and well-versed when it came to weaponry. Thomanek had put a dented plate on Hitler’s desk.</span></p><figure id="attachment_187404" aria-describedby="caption-attachment-187404"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/HandTool-360x300.jpg" alt="" width="360" height="300" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/HandTool-360x300.jpg 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/HandTool-300x250.jpg 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/HandTool.jpg 600w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187404"><em>The German version of the American bazooka – the ‘Panzerfaust’ (Image: HistoryNet)</em></figcaption></figure><p><em><span>“I am in a hurry and will express my opinion immediately”, </span></em><span>Hitler</span><span>&nbsp;said. </span><em><span>“This proposal could be the solution I have always wanted to give the individual soldier a weapon to defeat tanks. Maybe the same principle could be used in bombs and torpedoes.”</span></em></p><p><span>The Führer had grasped the significance of the new idea immediately. So did scientists, engineers and politicians in Britain, the Soviet Union and the United States. In the United States the shaped charge saw incarnation as the fabled anti-tank weapon the Bazooka, a weapon whose detailed workings were kept secret until 1945 when the core principle was revealed by a reprint of Munroe’s 1900 Popular Science article. The point of this slight digression is to indicate that by 1937, shaped charges were a well-studied form of anti-tank weaponry. They were also ideally suited to von Neumann’s particular skill set, and it was at the Aberdeen Proving Ground that they found fertile landfall.</span></p><p><span>The Aberdeen Proving ground in Maryland had been set up during World War 1 for ballistics research. Ballistics research was a clear priority for any army utilizing artillery. The main area of research for ballistics specialists was firing tables that indicated the velocity and the angle at which to fire a projectile to hit both stationary and moving targets. In 1918, Oswald Veblen had been hired to oversee scientific work at the site, and among others had recruited von Neumann’s fellow mathematical prodigy Norbert Wiener to work with him. By the late 1930s, Aberdeen was in a good position to contribute to the war effort and Veblen invited von Neumann to join him. The newly naturalized Johnny was eager enough that he applied to take an exam as a lieutenant in the army reserve in 1939. Amusingly, even though he passed the exam, he was turned down because of an age cutoff. Senator William Smathers from New Jersey expressed his perplexed reaction to FDR’s Secretary of War, Harry Woodring: <em>“Mr. von Neumann is internationally known as a mathematician, and I cannot understand how a man with so much potential value to the American armed forces should be turned down for a technicality.”&nbsp;</em></span></p><p><span>The reason Smathers and Veblen considered Johnny so valuable to a potential war effort was simple. Although von Neumann had no training in war-related work, his singular skill as a universalist mathematician would be invaluable in analyzing the nonlinear phenomena involved in shaped charge detonation. Crudely speaking, nonlinear phenomena deal with cases in which a given stimulus produces a disproportionate response. Most complex phenomena in the real world of interest to scientists are nonlinear, and in that sense the name ‘nonlinear’ is a misnomer: As von Neumann’s best friend Stanislaw Ulam who had escaped Poland before the Nazis invaded it once said, <em>“</em></span><span><em>Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.”</em> Nonlinear phenomena required mathematical manipulation at the highest level because of the complexity of the equations which usually involved partial differential equations in multiple dimensions. Both the versatility and speed of von Neumann’s mind made him an ideal candidate for analyzing these. As we will see later, Johnny’s interest in nonlinear equations was to have an unexpected and revolutionary impact on modern technology.</span></p><p><span>As the United States entered World War 2 after Pearl Harbor, most European émigré scientists wanted to do their part to help the war effort. Johnny’s point of departure into what became his most important contribution arose from Hans Bethe and Edward Teller’s work on shock waves. In 1941, Teller and Bethe had <a href="https://fas.org/sgp/othergov/doe/lanl/lib-www/la-pubs/00367149.pdf">analyzed</a> how the change in conditions around a shock wave, both in its front and back, can be analyzed in terms of characteristics like the medium’s temperature, pressure etc. To a simplifying approximation, Teller and Bethe found that the conditions at the front of the developing wave can be represented accurately by knowing the conditions at the back of the wave. Johnny was aware of this work, and in May 1941 wrote a letter to Bethe in which he asked the physicist if he and Teller could extend their work to air at 25,000 degrees centigrade. The scientists were finding out, interestingly, that the so-called equations of state for materials that relate pressure, temperature and density to each other were actually simplified at higher temperatures compared to lower ones because of a kind of equalizing behavior of different materials at those temperatures. Johnny’s main contribution had been to figure out that the behavior of the detonation of a shaped charge could be interpreted as the propagation of a shock wave followed by chemical reaction of the materials in the medium. We thus know that even before Pearl Harbor, Johnny was already making himself an expert in the behavior of shock waves at high temperatures.</span></p><p><b>From shaped charges to implosion</b></p><p><span>Curiously, von Neumann wasn’t involved in the early developments leading to the Manhattan Project, perhaps he already had much on his plate at that point. Not only was he consulting for multiple departments of the army and navy, but he was also making trips to England as a consultant. The early steps leading to the bomb project were all taken by physicists like Leo Szilard, Edward Teller, Robert Oppenheimer and Enrico Fermi. After the news of fission came to the United States in 1939, Szilard convinced his friend Einstein to write a famous <a href="http://wavefunction.fieldofscience.com/2014/08/celebrating-1939-leo-szilard-letter-to.html">letter</a> to FDR impressing on him the urgency of starting research into uranium fission. By the summer of 1942, Oppenheimer had been picked by Leslie Groves, the head of the project, to oversee a preliminary theoretical effort into a fission bomb. At Berkeley that summer, Oppenheimer, Bethe, Teller and others convinced themselves that theoretically, a simple fission bomb would be possible. By the end of 1942, Enrico Fermi and his team had done a proof-of-concept experiment and built the world’s first nuclear reactor at the University of Chicago. With Fermi’s experiments the Manhattan Project kicked into high gear, and by March 1943 an isolated site set among the majestic mesas of Los Alamos in New Mexico had been selected for the top-secret project.</span></p><p><span>As has been well documented, the basic principle of building a fission bomb was clear by 1943. You had to fire two subcritical pieces of uranium into one another, and once the critical mass had been exceeded there would be an explosion. It was also understood that this process needed to be exceptionally fast to prevent pre-detonation or a fizzle which would give suboptimal yield. The main challenge was always considered the separation of uranium isotopes and the production of plutonium, a novel material which would be even more fissile. By the time the scientists settled in at Los Alamos, Oppenheimer’s close collaborator and confidant Robert Serber had given a set of <a href="https://www.amazon.com/Los-Alamos-Primer-Lectures-Atomic/dp/0520075765">lectures</a> summarizing the state of knowledge.</span></p><p><span>It was then that a physicist named Seth Neddermeyer came up with an entirely novel approach to detonating a bomb – an approach called implosion. Instead of crashing two pieces of fissile material together, implosion called for bringing together those pieces inward, preferably in three dimensions to maximize efficiency. It says something about how novel and speculative this approach was that Neddermeyer’s idea met with fierce opposition from the likes of scientists like Bethe, Fermi and Feynman, men who weren’t known for a deficiency of new ideas. Deke Parsons, a naval physicist who was called in to help with ordnance, derisively called it the “beer can experiment”: the challenge was to symmetrically implode or squeeze a beer can from all sides without having the beer squirt out. But Oppenheimer was more prescient and he asked Neddermeyer to conduct preliminary experiments. Stymied by complex three dimensions, Neddermeyer started working with cylinders in two dimensions. By September 1943, the work was trudging along and Neddermeyer was not a particularly harmonious individual to work with, sticking to his ideas and seldom entertaining others. More importantly, what had been already suspected by Oppenheimer’s team in 1942 now became clear – that when it came to building a bomb, understanding the hydrodynamics or flow of shock waves was as important as understanding the nuclear physics and flow of radiation and particles like neutrons. Enter Johnny von Neumann.</span></p><p><span>Oppenheimer had written a letter to von Neumann in July 1943, and the contents of that letter make clear both the high regard in which he held Johnny and the difficulties he was facing:</span></p><p><i><span>“We are in what can only be described as a desperate need of your help. We have a good many theoretical people working here, but I think that if your usual shrewdness is a guide to you about the probable nature of our problems you will see why even this staff is in some respects critically inadequate…I would like you to come as a permanent, and let me assure you, honored member of our staff. A visit will give you a better idea of this somewhat Buck Rogers project than any amount of correspondence.”</span></i></p><p><span>At this point it’s worth noting von Neumann’s salient qualities that were considered to be helpful in such a project. I have noted these qualities in detail before in <a href="http://wavefunction.fieldofscience.com/2020/05/what-john-von-neumann-really-did-for.html">an essay</a> about his major contributions to computing. In a nutshell, while Johnny’s mind was as original as anyone else’s, his main strength was being able to jump ten steps ahead of everyone with his lightning fast mind, look at the big picture and connect disparate ideas together. This enabled him, in his friend Edward Teller’s words, <em>“to identify solutions where most people didn’t even notice the problems.”</em> All this not only made von Neumann a supremely reliable mind to seek advice from but lent his words enormous prestige. Johnny agreed to visit Los Alamos, not as a permanent member but as a consultant in September 1943. When not at Los Alamos, he would work out of the office of the National Academy of Sciences in Washington, D.C.&nbsp;</span></p><p><span>What Johnny essentially did in his critical visit was anoint implosion with his blessings. He confirmed that the best way to achieve implosion would be to spherically place shaped charges around a uranium or plutonium core; the Munroe Effect operating in three dimensions would then squeeze the core to the unearthly densities required to bring about the fission reaction. He also made the critical suggestion that implosion could be made much faster by increasing the ratio of the explosive in the shaped charges to the nuclear material. The faster assembly would not only prevent pre-detonation or a fizzle by making sure that the material has already reached critical mass before the nuclear fission reaction has been initiated, but the higher explosive to fissile material ratio would mean that you would need to use less of exceedingly precious enriched uranium or plutonium. In fact, as Johnny discovered because of his friend Teller who was very knowledgeable about the behavior of materials at high pressure, at the pressures that would exist in the core, materials would get squeezed so hard that they would become not just critical but supercritical, greatly accelerating the efficiency of the resulting explosion.</span></p><p><span>Because of his prestige and confidence in his calculations, von Neumann breathed fresh air into what was until then a creaky, uncertain program that Neddermeyer had been stubbornly sticking to. Fermi, Bethe and other senior scientists who had until then been skeptical of Neddermeyer’s scheme not only trusted von Neumann but regarded his dazzling mind with an awe that would be extraordinary for men who themselves were some of the leading scientific minds of the 20th century. Perhaps the ultimate tribute to von Neumann’s intelligence was paid by Bethe when he remarked after the war that <em>“Von Neumann’s mind seemed to indicate that he belonged to a new species, an evolution beyond man.”</em>, and Fermi once told his student Richard Garwin that von Neumann left him feeling he knew no mathematics at all. After von Neumann’s trip Charles Critchfield, a theoretical physicist on Oppenheimer’s team, said, <em>“Johnny woke everyone up. He was a very resourceful person, at least twenty years ahead of his time.”</em></span></p><p><span>Once von Neumann made his suggestions, it was quite clear that the implosion program had to be given top priority. There were two main challenges associated with implementing it. One was to work out the complicated nonlinear hydrodynamical equations associated with the shock wave. The other was to find explosive materials which would sustain an imploding wave and have it impinge on the center of the fission target uniformly. To aid with the latter task, Oppenheimer invited George Kistiakowsky from Harvard who was a world-class expert in explosives; in his spare time Kistiakowsky would use his explosives knowledge to raze trees and create ski slopes on the mesa for recreation for the scientists. He also set aside separate groups that would develop the diagnostics and assembly techniques for implosion, including the development of novel detonators that would assure an instantaneous detonation on multiple points of a sphere. None of these innovations or work was trivial, but von Neumann left everyone with the feeling that it was possible.</span></p><p><span>The biggest challenge in putting implosion into practice was to design an assembly of explosives that would ensure a symmetric shock wave – an asymmetry as little as 5% would lead to a “beer can experiment”. The problem with a standard explosion is that it sets off a diverging shock wave. To ensure that this wave impinged on the uranium or plutonium at the center, it had to somehow be turned from diverging to converging. How to achieve this? At this point another valuable character showed up, from England. James Tuck was part of the British contingent to Los Alamos, a small cadre of highly accomplished physicists who were to help with the bomb. The proof-of-concept for fission and critical mass had actually been worked out in England before the United States, so it was natural for British scientists to be involved. Tuck had experience working with explosive materials of differing densities. College physics says that when a light wave passes through materials of varying densities, it bends or refracts differently. Similarly a shock wave would “refract” or bend differently with different densities. And just like a material that refracts and transmits light is a lens, so an explosive that bends and transmits shock waves is an “explosive lens”. Tuck suggested an ingenious arrangement in which a diverging shock wave could be turned into a converging one by layering explosives burning at different rates together. By starting the detonation in fast-burning explosive and then maneuvering the resulting wave inside using shaped charges of slow-burning explosives, thirty-two different waves from thirty-two different points of detonation could be made to converge to a pinpoint at the center, squeezing the material to unearthly densities and triggering the fission reaction. It was again von Neumann who worked out the mathematics of explosive lenses and played a major role in their design.</span></p><figure id="attachment_187410" aria-describedby="caption-attachment-187410"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/2560px-Implosion_Nuclear_weapon.svg_-360x205.png" alt="" width="360" height="205" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/2560px-Implosion_Nuclear_weapon.svg_-360x205.png 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/2560px-Implosion_Nuclear_weapon.svg_-768x437.png 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/2560px-Implosion_Nuclear_weapon.svg_-1024x582.png 1024w, https://3quarksdaily.com/wp-content/uploads/2020/10/2560px-Implosion_Nuclear_weapon.svg_-300x171.png 300w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187410"><em>An explosive lens in an implosion weapon (Image: Wikipedia)</em></figcaption></figure><p><span>The real role played by von Neumann, Tuck, Kistiakowsky and others became apparent when a critical observation made it clear that implosion would be the only possible way to assemble the plutonium weapon. In the fall of 1944, Emilio Segrè, a protege of Fermi’s, <a href="http://wavefunction.fieldofscience.com/2017/02/born-otd-physicist-emilio-segre-who.html">discovered</a> that the plutonium-239 being used in the bomb contained an isotope, plutonium-240, that was an inevitable byproduct of the reactor-bred plutonium in the reactors at Hanford, Washington state. Pu-240 underwent spontaneous fission without any initiation, and it turned out that the gun method that was being used for the uranium bomb would be too slow to prevent pre-detonation or a fizzle. Implosion would be the only possible way to go for plutonium. Without implosion the plutonium bomb would likely have been abandoned at this stage. Implosion was still considered such a novel concept that the design was tested in New Mexico’s Alamogordo desert on July 16, 1945.</span></p><p><span>Von Neumann wasn’t done with his ideas though. A crucial albeit rather dark contribution that he made was to simplify the calculations used to estimate the pressures from a bomb. Von Neumann knew that for large explosions, the effects of the detonation are not dictated by the length of the pulse but only by the excess pressure that results; this made the relationship between bomb yield and effects a much simpler one. More importantly, using his knowledge of shock wave reflections, Johnny helped to estimate the optimal height at which a nuclear bomb should be exploded to cause maximum damage. On August 6, 1945, the Little Boy uranium bomb exploded at a height of about 1900 feet on top of Hiroshima. About 80,000 to 100,000 people were instantly killed, providing a real-life demonstration of von Neumann’s calculations.</span></p><figure id="attachment_187407" aria-describedby="caption-attachment-187407"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w-360x214.jpeg" alt="" width="360" height="214" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w-360x214.jpeg 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w-768x457.jpeg 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w-1024x610.jpeg 1024w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w-300x179.jpeg 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/1_EITY1jXNBfn2CN_VXIDH0w.jpeg 1149w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187407"><em>Von Neumann, Feynman and Ulam at Los Alamos: Their individual attire seems consonant with their personalities (Image: Atomic Heritage)</em></figcaption></figure><p><span>Johnny enjoyed being at Los Alamos. He reconnected with his old friend Edward Teller and invited a new friend, Stan Ulam, to help with the hydrodynamics calculations. Ulam was almost as versatile a mathematician as Johnny and the two were best friends – significant parts of Ulam’s wonderful memoir, “Adventures of a Mathematician”, are dedicated to describing conversations and travels with Johnny. They tried to recreate the famous cafe atmosphere in Eastern Europe that had fueled late night mathematical bull sessions in the early 20th century before it all came crashing down. In keeping with his predilection for wearing expensive, tailored three-piece suits, Johnny once wore one even when riding a mule in the Jemez mountains near the lab. He also went for walks with Richard Feynman, and in his own memoir “Surely You’re Joking Mr. Feynman”, Feynman points out that it was von Neumann who planted the idea in his mind that knowing how uncertain the consequences of our actions are, you should not have them weigh too heavily on your conscience. Knowing the rather grim endeavor that the physicists were involved with, this was psychologically soothing advice for Feynman.</span></p><p><b>“I am thinking about something much more important than bombs; I am thinking about computers.”</b></p><p><span>For all his critical ideas, the most important and far-reaching thing to come out of von Neumann’s work at Los Alamos had little to do with bombs. Once Johnny suggested using spherical implosion, it quickly became clear that the calculations involved would be too complex for individuals to perform. In England and before, Johnny had already been introduced to early computing machines, and he had been impressed with Alan Turing’s seminal paper on Turing machines and even tried to recruit Turing as his assistant when Turing visited Princeton to finish his PhD before the war. Now when it became clear that computing machines might be needed to handle the complex physics of implosion, Johnny was again in the right place at the right time. Stan Ulam who came to Los Alamos on Johnny’s invitation captured the problem well: <em>“The hydrodynamical problem was simply stated but very difficult to calculate, not just in its details but even in order of magnitude.”</em> He remembered a discussion in which von Neumann and others had suggested all kinds of ingenious shortcuts and theoretical simplifications, and he wondered whether<em> “more simpleminded brute force, that is, more realistic, numerical work”</em> might not be better.&nbsp;</span></p><p><span>To do this kind of work, the first IBM computing machines arrived in April, 1944 to help with the calculations. The implosion problem involved the integration of a hyperbolic partial differential equation in one space and one time coordinate. The integration was carried out using punched cards; Richard Feynman was in charge of this effort. Johnny himself spent two weeks programming the machines and getting a feel for the calculations. He found rewiring the tabulators especially cumbersome, and this frustration had an impact on his later crucial thinking about general-purpose computers. Johnny also knew that Howard Aiken at Harvard had had IBM build one of the first programmable computers, and he arranged for a numerical integration of a second-order partial differential equation to be run simultaneously on the Los Alamos computer and the Harvard computer as a contest; the Los Alamos computer finished first, but the Harvard computer calculated to more decimal places.</span></p><p><span>His experience at Los Alamos immediately suggested to the thinking-ten-steps-ahead Johnny that computers were going to become an invaluable asset in doing complex math and science. Quite fortuitously, around the same time he also came in contact with the pioneering engineers Presper Eckert and John Mauchly who were building one of the first pioneering general-purpose computers, the ENIAC, at the University of Pennsylvania. <a href="http://wavefunction.fieldofscience.com/2020/05/what-john-von-neumann-really-did-for.html">I have described</a> Johnny’s accidental introduction to computers and his subsequent work as one of the fathers of modern computing in a separate essay, but it is clear that it was his work at Los Alamos that crystallized in his mind the value of computers as an enabling tool for science and technology. It was characteristically far-sighted of him that even before the war ended he was writing to a colleague, <em>“I am thinking about something much more important than bombs; I am thinking about computers.”</em></span></p><p><b>Johnny’s&nbsp;legacy</b></p><p><span>Johnny von Neumann’s work at Los Alamos had a direct impact on the weapons of mass destruction that leveled Hiroshima and Nagasaki and that still decorate the world’s nuclear arsenals. Implosion is still the predominant technique used in fission bombs, and even in thermonuclear weapons it’s used as the primary fission device that ignites the fusion secondary. After the war Johnny kept on advising the United States government as a high-level consultant; at one point he advised every agency except the Coast Guard. He made valuable contributions not just to fission weapons but to the thermonuclear weapons that are today capable of killing hundreds of millions in a matter of minutes. Just before his death, he became one of the prime movers behind the <a href="https://en.wikipedia.org/wiki/Teapot_Committee">recommendation</a> to accelerate the United States’s nuclear ICBM program, a development that immediately made the entire world vulnerable to the threat of nuclear armageddon. He was convinced that the Soviet Union posed an existential danger to the security of the United States and advocated not just a preemptive but a preventive strike on that country; thankfully that part of his advice was not taken. </span></p><figure id="attachment_187412" aria-describedby="caption-attachment-187412"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/di_03376-360x294.jpg" alt="" width="360" height="294" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/di_03376-360x294.jpg 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/di_03376-300x245.jpg 300w, https://3quarksdaily.com/wp-content/uploads/2020/10/di_03376.jpg 500w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187412"><em>Von Neumann receiving the Medal of Freedom from President Eisenhower (Image: University of Texas)</em></figcaption></figure><p><span>He remained a steadfast patriot, often politically at odds with his more liberal colleagues but always friends with everyone, and was held in such esteem that during the last year of his life when cancer unexpectedly struck him, he received the Medal of Freedom from President Eisenhower and spent his last days in a special, guarded suite at Walter Reed Hospital that Eisenhower had specially requisitioned for him. On his deathbed lay a manuscript of a <a href="https://www.amazon.com/Computer-Brain-Silliman-Memorial-Lectures/dp/0300181116">set of lectures</a> comparing the computer with the brain. In it lay ideas whose seeds germinated at Los Alamos and whose branches now extend into artificial intelligence, robotics and neuroscience. Perhaps his most enduring idea in this regard, and one which is very much still waiting to play out, was the idea of self-reproducing automata which could be <a href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft">sent out</a> into space and would populate the cosmos with a diversity of exploding life.</span></p><figure id="attachment_187411" aria-describedby="caption-attachment-187411"><img loading="lazy" decoding="async" src="https://3quarksdaily.com/wp-content/uploads/2020/10/1-1-360x288.jpg" alt="" width="360" height="288" srcset="https://3quarksdaily.com/wp-content/uploads/2020/10/1-1-360x288.jpg 360w, https://3quarksdaily.com/wp-content/uploads/2020/10/1-1-768x614.jpg 768w, https://3quarksdaily.com/wp-content/uploads/2020/10/1-1-1024x818.jpg 1024w, https://3quarksdaily.com/wp-content/uploads/2020/10/1-1-300x240.jpg 300w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-187411"><em>Von Neumann and Oppenheimer in front of the Institute for Advanced Study computer in Princeton (Image: IAS)</em></figcaption></figure><p><span>As the eminent historian of science George Dyson put it in his superb book “<a href="https://www.amazon.com/Turings-Cathedral-Origins-Digital-Universe-ebook/dp/B005IEGK5C/ref=sr_1_1?dchild=1&amp;keywords=turing%27s+cathedral&amp;qid=1603690526&amp;s=books&amp;sr=1-1">Turing’s Cathedral</a>”, <em>“Bombs made computers, and computers made bombs.”</em> If designing the implosion lens for nuclear weapons were to be Johnny’s biggest legacy from Los Alamos, it would be a morally dubious one. But his unexpected recognition of the value of computers takes his contributions to a completely new level. While initially he did encourage using computers to simulate the workings of first fission and then fusion weapons, he made seminal contributions to charting out the stored-program concept, random access memory and what is today called the von Neumann architecture. With a talented team of engineers he designed a pioneering computer at the Institute for Advanced Study. Using this computer, his team made forays into a remarkable number of important and fascinating topics: weather simulation, artificial life, fundamental mathematical research, geophysics. The branches that these explorations sent out continue to thrive.</span></p><p><span>Johnny von Neumann’s Los Alamos story shows us that often, the most important impact of a new technology is another new technology. Often this new technology is wholly unanticipated. When Johnny came to Los Alamos, he and his colleagues thought they would be designing bombs, but what they didn’t know was that they would need computers to design those bombs. And that the computers would be far more important than the bombs even in the short term. Ultimately a hundred or a thousand years from now, if humanity still survives and the world’s nuclear arsenals are a dim memory in a history textbook, we will still be using computers, and computers will still be using us.</span></p><p><em>Recommended further reading:</em></p><ol><li>John von Neumann and the Origins of Modern Computing – William Aspray</li><li>Critical Assembly: A Technical History of Los Alamos during the Oppenheimer Years, 1943-1945 – Lillian Hoddeson, Paul Henriksen, Roger Meade and Catherine Westfall</li><li>John von Neumann: Selected Letters – Edited by Miklós Rédei</li><li>The Making of the Atomic Bomb – Richard Rhodes</li><li>Adventures of a Mathematician – S. M. Ulam</li><li>John von Neumann – Norman Macrae</li><li>Prisoner’s Dilemma – William Poundstone</li><li><a href="http://wavefunction.fieldofscience.com/2020/05/what-john-von-neumann-really-did-for.html">What John von Neumann really did for modern computing</a> – Ashutosh Jogalekar</li><li><a href="https://3quarksdaily.com/3quarksdaily/2020/06/von-neumann-in-1955-and-2020-musings-of-a-cheerful-pessimist-on-technological-survival.html">Von Neumann in 1955 and 2020: Musings of a cheerful pessimist</a> – Ashutosh Jogalekar</li><li><a href="https://medium.com/cantors-paradise/the-unparalleled-genius-of-john-von-neumann-791bb9f42a2d">The Unparalleled Genius of John von Neumann</a> – Jørgen Veisdal</li><li><a href="https://youtu.be/Y2jiQXI6nrE">John von Neumann</a> – 1966 documentary by the Mathematical Association of America that includes great recollections by Johnny’s eminent colleagues (Eugene Wigner, Hans Bethe, Paul Halmos, Herman Goldstine, Edward Teller and others)</li></ol></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adventures Making Vegemite (160 pts)]]></title>
            <link>https://daveon.design/adventures-making-vegemite.html</link>
            <guid>39960788</guid>
            <pubDate>Sun, 07 Apr 2024 13:55:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daveon.design/adventures-making-vegemite.html">https://daveon.design/adventures-making-vegemite.html</a>, See on <a href="https://news.ycombinator.com/item?id=39960788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
<article><section>
<p><span><span>To non-Australians, Vegemite</span> is one of the strangest food products you’re likely to ever encounter.</span> <span>It’s a dark brown, almost black, shiny paste with a pungent scent, and a very strong, almost indescribable taste that is strongly umami and very salty.</span> </p><figure>
<img src="https://daveon.design/images/vegemite.webp">
</figure>
<p><span>It is a little like soy sauce in solid form.</span> </p><p><span>We tend to eat it on toast for breakfast.</span> <span><span></span> <span><span>The trick is not to spread it thickly like peanut butter; instead, you take about a quarter of a teaspoon’s worth and scrape it thinly over the entire slice.</span> <span>It also goes very nicely in a toasted cheese sandwich.</span> </span> <span></span> </span><span> </span></p><p><span>It sounds unique, but perhaps the strangest thing about it is that it is <em>not:</em> the <span>British have Marmite, which tastes similar but is runnier, Australians have a <em>second</em> yeast extract spread called <span>Promite,<span><span></span> <span><span>In my personal opinion, even better than Vegemite.</span> </span> <span></span> </span><span>  the <span>Germans and Swiss have Cenovits and Vitam-R,<span><span></span> <span><span>I haven’t tried either of these.</span> </span> <span></span> </span><span>  and a <span>Latvian friend once gave me a jar of a tan-coloured, honey-consistency thick liquid he told me was like Vegemite and I (sadly) found inedible.</span> </span></span></span></span></span></span></p><p><span>This leads to spirited debates about which is best and, like a true Australian, I mock Marmite to the British friend with whom I made Vegemite for this article,<span><span></span> <span><span>Marmite is the <em>inferior</em> product: it is not a solid paste but is runny, and has a slightly less strong taste.</span> </span> <span></span> </span><span>  even as he keeps <span>Marmite in his cupboard and would never touch Vegemite.</span> <span>Both of us quietly admit they’re very similar and I have many times bought Marmite when Vegemite was not available.</span> <span><span></span> <span><span>When you live overseas, you make do with what you can get.</span> <span>In Australia, you’d never admit to eating Marmite.</span> <span>But among fraternity of overseas Aussie citizens, there’s an unspoken knowledge that we’ve all lowered ourselves to Marmite from time to time.</span> </span> <span></span> </span><span> </span></span></span></p><p><span>But even if thick black pungent salted umami spreads are common, <strong>how are they made?</strong></span> <span>And <strong>what is such a strange spread really made of?</strong></span> </p><p><span>The jar claims to be a ‘yeast extract’, and Aussie rumour had it that it was made from leftovers from making beer.</span> <span>Very Australian.</span> <span>Since it was invented in the 1920s, I personally had a mental image of a specific strain of yeast fermenting to produce some precursor substance, <a href="https://worldbuilding.stackexchange.com/questions/56560/can-you-make-arbitrary-food-out-of-yeast"><span>Asimov-style</span></a>.</span> </p><p><span>Then my Marmite-loving British friend sent me <a href="https://www.youtube.com/watch?v=ukd3lg3Z_Pg">this video</a>,<span><span></span> <span><span>From the <a href="https://www.youtube.com/@HowToCookThat"><em><span>How To Cook That</span></em></a> <span>Youtube channel by Ann Reardon.</span> </span> <span></span> </span>  and one afternoon we made it.</span><span> <span><span></span> <span><span>By ‘we made it’, it’s more that I’d casually mentioned I’d like to try, and then at the pub the previous evening he said he had all the ingredients and would I like to join him the next afternoon?</span> <span>In other words: his effort, his ingredients, his kitchen, and I took photos.</span> </span> <span></span> </span><span> </span></span></span></p><figure>
<img src="https://daveon.design/images/vegemite-results-toast.webp"><span></span>
</figure>
<section><h2 id="ingredients">Ingredients</h2>
<p><span>A homebrewing friend provided:</span></p><figure>
<img src="https://daveon.design/images/vegemite-yeast-slurry.webp">
</figure>
<ul>
<li><span>The yeast slurry from the bottom of a batch of beer (from a plain lager)</span></li><li><span>A fermented malt wort made from dry malt mixed with water</span></li></ul>
<p><span>The malt was very liquid and we boiled it down to a thick syrup.</span> <span>You could likely use supermarket malt.</span> </p><figure>
<img src="https://daveon.design/images/vegemite-slurry-liquid-boiling.webp"><figcaption>Boiling the skimmed yeast slurry liquid.</figcaption><span></span>
</figure>
<p><span>The yeast slurry in Ann Reardon’s video was separated into yeast and liquids via a home-made washing-machine centrifuge.</span> <span>But our slurry had been sitting in my friend’s fridge for a few days, and he simply poured off the top.</span> <span>This liquid, a pale clear tan, we boiled down into a thick, dark brown syrup too.</span> <span>Initially, the kitchen smelled like hot beer, but it soon changed to smell like something recognisable as a Vegemite-like scent.</span> <span><span></span> <span><span>It was not unpleasant—if you like Vegemite—but it was strong, and at some point despite the winter outside every window in the apartment was opened.</span> </span> <span></span> </span><span> </span></p><figure>
<img src="https://daveon.design/images/vegemite-boiling-down.webp"><figcaption role="complementary "><span>Left: the malt reducing to a syrup; right: the slurry liquid also being boiled down.</span> </figcaption><span><span></span>
</span></figure><p><span>A good half-liter or more of yeast-slurry-liquid turned into only a couple of tablespoons of dense brown syrup.</span> <span>This is definitely a recipe where large starting volumes are needed, and you won’t get a large quantity result.</span> </p><figure>
<img src="https://daveon.design/images/vegemite-boiled-malt-and-slurry-liquid-2.webp"><figcaption role="complementary "><span>The resulting two syrups: on the left, heavily reduced malt wort to a very dark, sweet, flavourful syrup; on the right, the main Vegemite flavour comes from the boiled-down reduction from the yeast slurry.</span> <br><span>A good cooking blog would show these two dishes very clean, with no drip remnants, but all this stuff is thick, sticky, and messy.</span> </figcaption><span><span></span>
</span></figure><p><span>The final major ingredient is nutritional yeast.</span> <span>We mixed several tablespoons with warm water to make a light yellow sludge.</span> <span><span></span> <span><span>The names for the ingredients—slurries, sludges—are as appetizing as many non-Australians claim Vegemite to be.</span> </span> <span></span> </span><span> </span></p><h3 id="taste-testing">Taste Testing</h3>
<p><span>A taste test gave:</span></p><ul>
<li><span>Boiled slurry liquid: Vegemite or Marmite overtones, but surprisingly bitter (perhaps the result of hops?)</span> </li><li><span>Boiled, thick malt: unexpectedly, a strong ‘this taste is not Vegemite but is in Vegemite’ response.</span> <span><span></span> <span><span>Vegemite does not taste like malt, and you would never eat Vegemite and think of malt, yet boiled-down malt wort syrup distinctly resembles very sweet Vegemite.</span> </span> <span></span> </span><span> </span></li></ul>
</section><section><h2 id="mixing">Mixing</h2><span></span>
<div>
<p><img src="https://daveon.design/images/vegemite-mixing-1.webp">
</p>
<p><img src="https://daveon.design/images/vegemite-mixing-2.webp">
<br>Drag ⇢ 
</p></div>
<p><span>The Youtube video has no proportions, and even with large quantities of starting liquids we had only teaspoons of boiled-down results.</span> <span>Taking the entire boiled-down yeast slurry runoff liquid, perhaps three teaspoons or a little more, we added about half a teaspoon of malt, a generous dash of salt<span><span></span> <span><span>Vegemite is very salty.</span> </span> <span></span> </span><span>  and about a teaspoon of nutritional yeast sludge.</span> <span>Some vigorous mixing, some adjustment (a tiny bit more malt, plus a bit more salt)<span><span></span> <span><span>Vegemite is <em>very</em> salty.</span> </span> <span></span> </span><span>  and we deemed it done.</span> </span></span></p><figure>
<img src="https://daveon.design/images/vegemite-result.webp"><figcaption role="complementary "><span>The appetizingly-coloured result!</span> </figcaption><span><span></span>
</span></figure><p><span>It was not the right colour.</span> <span>Nor did it taste exactly like Vegemite, though it was very close.</span> <span>But the taste was <em>exactly</em> like <span>Marmite!</span> </span></p></section><section><h2 id="taste-tests">Taste Tests</h2>
<p><span>British friend:</span></p><blockquote>
<span>Bang on.</span> <span>Yeah.</span> <span>Feels like you should make it with beer that’s not very hoppy.</span> </blockquote>
<p><span>Friend’s partner:</span></p><blockquote>
<span>Really really close.</span> <span>A little more bitter maybe.</span> </blockquote>
<p><span>My wife:</span></p><blockquote>
<span>It’s softer, and more bitter, than Marmite.</span> <span>Marmite is very sticky, and this lacks the stickiness of Marmite.</span> <span>But it still tastes like Marmite.</span> </blockquote>
<p><span>To me: There was a strong hit of salt and then it fades into distinct Marmite flavour.</span> <span>Then a slight unwanted bitter note.</span> <span>The aftertaste is just like Vegemite.</span> <span>The next day, cold from the fridge, it seemed more sharp in flavour than the previous day.</span> <span>Overall, very in the Vegemite/Marmite family.</span> </p></section></section><section><h2 id="variations">Variations</h2>
<p><span><span>In the same taste-test,</span> we also tried a variant made by our Czech friend who supplied the beer slurry.</span> <span>He was not interested in recreating Vegemite <em>per se,</em> but in creating an umami stock he could use for cooking.</span> <span>So he’d boiled it down with stock cubes and a lot more malt: the colour was right (very dark due to the malt) but it was sweet and very much not Vegemite.</span> </p><p><span>For ours, the main consensus is that there is a hint more bitterness in our Mite than in Vegemite.</span> <span>This may be due to the beer that provided the slurry: it came from a well-hopped lager.</span> <span>Perhaps we need to add more malt to offset the bitterness; it’s a strong flavour though and risks overpowering the key Vegemite notes that come from the boiled-down beer slurry.</span> <span>I’d like to try again using slurry from making a stout or one of the sweeter porters, which may have fewer hops, a darker colour from the roasted grains, and could give a less bitter starting point.</span> </p><p><span>The majority of Australian beer is lager, and if Vegemite uses the most widely available beer remnants, it likely comes from some kind of light, hopped beer.</span> <span>I am curious how they get such a dark colour.</span> </p><p><span>Finally, Wikipedia claims Vegemite is made with celery and onion extracts.</span> <span>I’d like to try again using a stout/sweet porter as basis, and add celery salt and onion powder.</span> </p></section><section><h2 id="making-vegemite">Making Vegemite</h2>
<p><span><span>What did I get for an afternoon’s</span> worth of watching beery liquids boil down in my friend’s kitchen?</span> <span>I came home with three quarters of a teaspoon of brown runny goop… and somewhat of a fascination for <a href="https://en.wikipedia.org/wiki/Justus_von_Liebig#Marmite">how people invented this stuff in the first place</a>.</span> </p><figure>
<img src="https://daveon.design/images/vegemite-results-toast.webp"><figcaption role="complementary "><span>Left to right: Marmite, our spread, our Czech friend’s umami stock spread, and some nutritional yeast paste to round out the last slice of toast.</span> </figcaption><span><span></span>
</span></figure></section></article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mixture-of-Depths: Dynamically allocating compute in transformers (232 pts)]]></title>
            <link>https://arxiv.org/abs/2404.02258</link>
            <guid>39960717</guid>
            <pubDate>Sun, 07 Apr 2024 13:42:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.02258">https://arxiv.org/abs/2404.02258</a>, See on <a href="https://news.ycombinator.com/item?id=39960717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2404.02258">View PDF</a>
    <a href="https://arxiv.org/html/2404.02258v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\% faster to step during post-training sampling.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Adam Santoro [<a href="https://arxiv.org/show-email/7e3a6283/2404.02258">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 2 Apr 2024 19:28:11 UTC (1,763 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blog posts, sorted by sleep (184 pts)]]></title>
            <link>https://breckyunits.com/sleepWriting.html</link>
            <guid>39960645</guid>
            <pubDate>Sun, 07 Apr 2024 13:32:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breckyunits.com/sleepWriting.html">https://breckyunits.com/sleepWriting.html</a>, See on <a href="https://news.ycombinator.com/item?id=39960645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><span>April 5, 2024 — </span>Have you ever examined the correlation between your writing behavior and sleep?</p>
<p>I've written some things in my life that make me cringe. I might cringe because I see some past writing was naive, mistaken, locked-in, overconfident, unkind, insensitive, aggressive, or grandiose.</p>
<p>I now have a pretty big dataset to identify my secret trick to write more cringe: less sleep.</p>
<p>For this post I combined 2,500 nights of sleep data with 58 blog posts. A 7 year experiment to see how sleep affects my writing.</p>
<figure><a href="https://breckyunits.com/less-sleeping-less-thinking.png" target="_blank"><img src="https://breckyunits.com/less-sleeping-less-thinking.png" width="1240" height="942" loading="lazy"></a><figcaption><p><a href="https://www.datawrapper.de/_/YsKWU/">Interactive version.</a></p></figcaption></figure>
<div><h2>~7 Hours is the Cutoff</h2>
<p>Most posts above 7 hours of sleep do not need a sleep disclaimer. Most posts below 7 hours do. Not to say there is no value in the posts made with under 7 hours of sleep, it's just less rigorous writing (and thinking). On the plus side, writing with little sleep can be more concise at times. It might exaggerate the key ideas, but nevertheless identify them fearlessly and concisely.</p>
</div>

<figure><a href="https://breckyunits.com/sleeping-less-is-slightly-correlated-with-posting-more.png" target="_blank"><img src="https://breckyunits.com/sleeping-less-is-slightly-correlated-with-posting-more.png" width="1240" height="1006" loading="lazy"></a></figure>
<div><h2>More Posts. Similar Word Counts. Higher Scatteredness. Similar IQ. Higher Confidence.</h2>
<p>I actually post slightly more when I sleep less (Pearson correlation coefficient of -.14), but fewer words per post, which is indicative of a more "scattered" thinking state. I was surprised to see that I don't generally generate a whole lot more words in deprived sleep states. I <em>perceive</em> my writing to be smarter during those times, but looking back it's clearly not.</p>
</div>
<div><h2>Other Social Media</h2>
<p>Besides this blog, I have long written and posted content to HackerNews, Reddit, other discussion forums, and at times Twitter, Instagram, Facebook, YouTube, and LinkedIn. I haven't done the data grunt work, but if my memory serves me correctly I am confident my publishing behavior on those platforms mirrors the same patterns as my blogging behavior, with regards to sleep.</p>
</div>
<div><h2>Public vs Private Writing</h2>
<p>There have been stretches where I published little publicly but was generating a similar amount of tokens, just in private groups. My writing patterns in private groups also mirrors my patterns on this blog, with regards to sleep.</p>
</div>
<p>Tangent: when I've been lucky to be a part of brainiac private organizations (such as Microsoft, YCombinator, Our World in Data, academia, and so on), I got to read so much brilliant writing by people who rarely post publicly, and every time I think about that I am humbled. There is so much well written content on the public web, and to think it is <em>only a fraction</em> of the great content ever written, is humbling.</p>
<div><h2>Sleep Disclaimers</h2>
<p>I realize I already have an unofficial "sleep disclaimer" policy. I have de-indexed (but kept published) at least a couple of sleep-deprived posts, and added a disclaimer/correction to at least 2 others. Now with this dataset I am sure I will append a few more sleep disclaimers.</p>
</div>
<p>With sleep disclaimers, I can say, "hey, might be interesting ideas here, but don't train too heavily on this".</p>
<div><h2>Grateful for Git</h2>
<p>I am happy with my decision to use git for this blog so that I always keep an honest history, while still being free to down weight sleep deprived content and try and keep my more thought-out out ideas front and center.</p>
</div>
<div><h2>Benefits of Peer Review</h2>
<p>I don't have a column for it (yet), but it does seem my better posts often were the ones where I took the time to get friends and/or colleagues to review, IRL. Sleep deprived posts I would generally blast out without talking to anyone.</p>
</div>
<p>Peer review is a great filter, and a great forcing function to put more effort in.</p>
<p>On the other hand, because the importance of ideas varies by so many orders of magnitude (there are "black swan" ideas), you could make an argument that spending too much time in one area of ideas isn't the optimal strategy, and publishing things as you go, improving them later, is an approach with merit.</p>
<div><h2>Writing data reflects the current phenomena in your brain</h2>
<p>It seems when I sleep less, my brain is in more of a pleasure seeking state, has a bias to action ("don't think, just do"), and feels less pain than in a more rested state. Less sleep means less critical thinking. Less sleep seems to make me less willing to invest the time in rewiring my brain to correct mistaken thought habits. </p>
</div>
<div><h2>Grateful for FitBit</h2>
<p>I started wearing a Microsoft Band when it first came out in November 2014. Then a Band 2, then FitBit Charge, Ionic, Versa, and now Sense 2. I am grateful for all the people involved with creating these things. I think continued progress in the wearable sensor field is the best bet for improving human health.</p>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I open-sourced the in-memory PostgreSQL I built at work for E2E tests (276 pts)]]></title>
            <link>https://github.com/stackframe-projects/pgmock</link>
            <guid>39960537</guid>
            <pubDate>Sun, 07 Apr 2024 13:13:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/stackframe-projects/pgmock">https://github.com/stackframe-projects/pgmock</a>, See on <a href="https://news.ycombinator.com/item?id=39960537">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">pgmock</h2><a id="user-content-pgmock" aria-label="Permalink: pgmock" href="#pgmock"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://stackframe-projects.github.io/pgmock" rel="nofollow">Demo</a> —
  <a href="https://discord.gg/pD4nyYyKrb" rel="nofollow">Discord</a>
</h3><a id="user-content---demo---discord" aria-label="Permalink: Demo —
  Discord" href="#--demo---discord"></a></p>
<p dir="auto"><code>pgmock</code> is an in-memory PostgreSQL mock server for unit and E2E tests. It requires no external dependencies and runs entirely within WebAssembly on both Node.js and the browser.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>

<p dir="auto">If you'd like to run <code>pgmock</code> in a browser, see the <a href="#browser-support">Browser support</a> section for detailed instructions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">You can run an in-memory server like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { PostgresMock } from &quot;pgmock&quot;;

const mock = await PostgresMock.create();
const connectionString = await mock.listen(5432);"><pre><span>import</span> <span>{</span> <span>PostgresMock</span> <span>}</span> <span>from</span> <span>"pgmock"</span><span>;</span>

<span>const</span> <span>mock</span> <span>=</span> <span>await</span> <span>PostgresMock</span><span>.</span><span>create</span><span>(</span><span>)</span><span>;</span>
<span>const</span> <span>connectionString</span> <span>=</span> <span>await</span> <span>mock</span><span>.</span><span>listen</span><span>(</span><span>5432</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Recommended: If you use <code>node-postgres</code> (<code>pg</code> on npm), <code>pgmock</code> provides you with a configuration object that doesn't require you to serve on a port (and also works in the browser):</p>
<div dir="auto" data-snippet-clipboard-copy-content="import * as pg from &quot;pg&quot;;

const mock = await PostgresMock.create();
const client = new pg.Client(mock.getNodePostgresConfig());

await client.connect();
console.log(await client.query('SELECT $1::text as message', ['Hello world!']));"><pre><span>import</span> <span>*</span> <span>as</span> <span>pg</span> <span>from</span> <span>"pg"</span><span>;</span>

<span>const</span> <span>mock</span> <span>=</span> <span>await</span> <span>PostgresMock</span><span>.</span><span>create</span><span>(</span><span>)</span><span>;</span>
<span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>pg</span><span>.</span><span>Client</span><span>(</span><span>mock</span><span>.</span><span>getNodePostgresConfig</span><span>(</span><span>)</span><span>)</span><span>;</span>

<span>await</span> <span>client</span><span>.</span><span>connect</span><span>(</span><span>)</span><span>;</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>client</span><span>.</span><span>query</span><span>(</span><span>'SELECT $1::text as message'</span><span>,</span> <span>[</span><span>'Hello world!'</span><span>]</span><span>)</span><span>)</span><span>;</span></pre></div>
<p dir="auto">It is considered good practice to destroy the mock server after you are done with it to free up resources:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Check the <a href="https://github.com/stackframe-projects/pgmock/blob/main/src/postgres-mock.ts">PostgresMock source file</a> for a list of all available methods and their documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Browser support</h2><a id="user-content-browser-support" aria-label="Permalink: Browser support" href="#browser-support"></a></p>
<p dir="auto"><code>pgmock</code> fully supports browser environments. While webapps can't listen to TCP ports, you can still use <code>PostgresMock.createSocket</code> and the <code>node-postgres</code> configuration. However, if your bundler statically analyzes imports, the default configuration may show a warning because of missing (optional) Node.js modules. Check <code>examples/web-demo/next.config.mjs</code> for an example on how to configure Webpack for bundling.</p>
<p dir="auto">If you're only looking to run a database in the browser, you might want to consider <a href="https://github.com/electric-sql/pglite">pglite</a> instead. It is more performant and lightweight, but only has a limited feature set. <code>pgmock</code> is designed for feature parity with production PostgreSQL environments, as you would want in a testing environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">There are two approaches to run Postgres in WebAssembly; by <a href="https://github.com/electric-sql/postgres-wasm">forking it to support WASM natively</a> or by <a href="https://supabase.com/blog/postgres-wasm" rel="nofollow">emulating the Postgres server in an x86 emulator</a>. The former is more performant and uses considerably less memory, but only supports single-user mode (no connections), and no extensions.</p>
<p dir="auto">To prevent discrepancies between testing and production, and because performance is not usually a concern in tests, <code>pgmock</code> currently uses the latter approach. In the mid-term future, once native Postgres WASM forks mature, we plan to make both options available, and eventually, switch to native WASM as default. We don't expect there to be many breaking changes besides the APIs inside <code>PostgresMock.subtle</code>.</p>
<p dir="auto"><code>pgmock</code> differs from previous Postgres-in-the-browser projects by providing full feature-compatibility entirely inside the JavaScript runtime, without depending on a network proxy for communication. We did this by simulating a network stack in JavaScript that behaves like a real network, that can simulate TCP connections even on platforms that do not allow raw socket access.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Wanna contribute?</h2><a id="user-content-wanna-contribute" aria-label="Permalink: Wanna contribute?" href="#wanna-contribute"></a></p>
<p dir="auto">Great! We have a <a href="https://discord.gg/pD4nyYyKrb" rel="nofollow">Discord server</a> where you can talk to us.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Can this run other Docker images or databases?</h2><a id="user-content-can-this-run-other-docker-images-or-databases" aria-label="Permalink: Can this run other Docker images or databases?" href="#can-this-run-other-docker-images-or-databases"></a></p>
<p dir="auto">In theory, yes. I just haven't tested them. Ping me on our <a href="https://discord.gg/pD4nyYyKrb" rel="nofollow">Discord server</a> if you're interested.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://github.com/copy/v86">v86</a>, the x86 emulator which makes this possible</li>
<li><a href="https://supabase.com/blog/postgres-wasm" rel="nofollow">Supabase &amp; Snaplet</a> for building their own approach of running Postgres inside WebAssembly, which this is based on</li>
<li><a href="https://stackframe.co/" rel="nofollow">Stackframe</a> for keeping me on a payroll while I was building <code>pgmock</code></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ClangQL: A tool to run SQL-like query on C/C++ Code (108 pts)]]></title>
            <link>https://github.com/AmrDeveloper/ClangQL</link>
            <guid>39960535</guid>
            <pubDate>Sun, 07 Apr 2024 13:13:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AmrDeveloper/ClangQL">https://github.com/AmrDeveloper/ClangQL</a>, See on <a href="https://news.ycombinator.com/item?id=39960535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ClangQL - Clang AST Query Language</h2><a id="user-content-clangql---clang-ast-query-language" aria-label="Permalink: ClangQL - Clang AST Query Language" href="#clangql---clang-ast-query-language"></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AmrDeveloper/ClangQL/blob/master/media/clangql_logo.svg"><img src="https://github.com/AmrDeveloper/ClangQL/raw/master/media/clangql_logo.svg" width="20%" height="20%"></a>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1531c20d64a49760c4bfd70a9689861e5b3bd0b56f15c3f32d3da2b099d750df/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f636c616e67716c3f7374796c653d666c61742d737175617265"><img alt="Crates.io" src="https://camo.githubusercontent.com/1531c20d64a49760c4bfd70a9689861e5b3bd0b56f15c3f32d3da2b099d750df/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f636c616e67716c3f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/crates/v/clangql?style=flat-square"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6302052a609fdfc518cb92d7a05c93f1008386210057e1be2e5b3618f5410f26/68747470733a2f2f646570732e72732f7265706f2f6769746875622f616d72646576656c6f7065722f636c616e67716c2f7374617475732e737667"><img alt="Deps" src="https://camo.githubusercontent.com/6302052a609fdfc518cb92d7a05c93f1008386210057e1be2e5b3618f5410f26/68747470733a2f2f646570732e72732f7265706f2f6769746875622f616d72646576656c6f7065722f636c616e67716c2f7374617475732e737667" data-canonical-src="https://deps.rs/repo/github/amrdeveloper/clangql/status.svg"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/108ab52c954f90085e23ddb1da4751a08b3cc4591c9cda5319afadd162da3819/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f616d72646576656c6f7065722f636c616e67716c"><img alt="GitHub issues" src="https://camo.githubusercontent.com/108ab52c954f90085e23ddb1da4751a08b3cc4591c9cda5319afadd162da3819/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f616d72646576656c6f7065722f636c616e67716c" data-canonical-src="https://img.shields.io/github/issues/amrdeveloper/clangql"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a41fce30cc4544cc983c190f01021defdd17d1af49f62882537e28a582192022/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f616d72646576656c6f7065722f636c616e67716c"><img alt="GitHub" src="https://camo.githubusercontent.com/a41fce30cc4544cc983c190f01021defdd17d1af49f62882537e28a582192022/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f616d72646576656c6f7065722f636c616e67716c" data-canonical-src="https://img.shields.io/github/license/amrdeveloper/clangql"></a>
</p>
<p dir="auto">
ClangQL is a tool that allow you to run SQL-like query on C/C++ Code instead of database files using the GitQL SDK.
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/AmrDeveloper/ClangQL/blob/master/media/clangql_demo.PNG"><img src="https://github.com/AmrDeveloper/ClangQL/raw/master/media/clangql_demo.PNG" alt="animated" width="100%"></a>
</p>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Samples</h3><a id="user-content-samples" aria-label="Permalink: Samples" href="#samples"></a></p>
<p dir="auto">Note that all Keywords in ClangQL are case-insensitive, similar to SQL.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT 1
SELECT 1 + 2
SELECT LEN(&quot;File Query Language&quot;)
SELECT &quot;One&quot; IN (&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;)
SELECT &quot;File Query Language&quot; LIKE &quot;%Query%&quot;

SELECT * FROM functions
SELECT COUNT(name) from functions WHERE return_type = &quot;int&quot;
SELECT DISTINCT name AS function_name FROM functions"><pre><span>SELECT</span> <span>1</span>
<span>SELECT</span> <span>1</span> <span>+</span> <span>2</span>
<span>SELECT</span> LEN(<span><span>"</span>File Query Language<span>"</span></span>)
<span>SELECT</span> <span><span>"</span>One<span>"</span></span> <span>IN</span> (<span><span>"</span>One<span>"</span></span>, <span><span>"</span>Two<span>"</span></span>, <span><span>"</span>Three<span>"</span></span>)
<span>SELECT</span> <span><span>"</span>File Query Language<span>"</span></span> <span>LIKE</span> <span><span>"</span>%Query%<span>"</span></span>

<span>SELECT</span> <span>*</span> <span>FROM</span> functions
<span>SELECT</span> <span>COUNT</span>(name) <span>from</span> functions <span>WHERE</span> return_type <span>=</span> <span><span>"</span>int<span>"</span></span>
<span>SELECT DISTINCT</span> name <span>AS</span> function_name <span>FROM</span> functions</pre></div>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Functions table structure</h3><a id="user-content-functions-table-structure" aria-label="Permalink: Functions table structure" href="#functions-table-structure"></a></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>Text</td>
<td>Function or Method name</td>
</tr>
<tr>
<td>signature</td>
<td>Text</td>
<td>Parameters and return type literal</td>
</tr>
<tr>
<td>args_count</td>
<td>Integer</td>
<td>Number of arguments</td>
</tr>
<tr>
<td>return_type</td>
<td>Text</td>
<td>Return type literal</td>
</tr>
<tr>
<td>is_method</td>
<td>Boolean</td>
<td>True if it's a method</td>
</tr>
</tbody>
</table>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download or Install</h3><a id="user-content-download-or-install" aria-label="Permalink: Download or Install" href="#download-or-install"></a></p>
<p dir="auto">Note that Building from source or installing from Cargo.io requires LibClang 17 to be installed</p>
<ul dir="auto">
<li>Install from Cargo.io</li>
</ul>

<ul dir="auto">
<li>Build from source code</li>
</ul>
<div data-snippet-clipboard-copy-content="git clone https://github.com/AmrDeveloper/clangql.git
cd clangql
cargo build"><pre><code>git clone https://github.com/AmrDeveloper/clangql.git
cd clangql
cargo build
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Run ClangQL</h3><a id="user-content-run-clangql" aria-label="Permalink: Run ClangQL" href="#run-clangql"></a></p>
<div data-snippet-clipboard-copy-content="ClangQL is a SQL like query language to run on local files
Usage: ClangQL [OPTIONS]

Options:
  -f,  --files <paths>        Path for local files to run query on
  -q,  --query <GQL Query>    ClangQL query to run on selected files
  -p,  --pagination           Enable print result with pagination
  -ps, --pagesize             Set pagination page size [default: 10]
  -o,  --output               Set output format [render, json, csv]
  -a,  --analysis             Print Query analysis
  -h,  --help                 Print ClangQL help
  -v,  --version              Print ClangQL Current Version"><pre><code>ClangQL is a SQL like query language to run on local files
Usage: ClangQL [OPTIONS]

Options:
  -f,  --files &lt;paths&gt;        Path for local files to run query on
  -q,  --query &lt;GQL Query&gt;    ClangQL query to run on selected files
  -p,  --pagination           Enable print result with pagination
  -ps, --pagesize             Set pagination page size [default: 10]
  -o,  --output               Set output format [render, json, csv]
  -a,  --analysis             Print Query analysis
  -h,  --help                 Print ClangQL help
  -v,  --version              Print ClangQL Current Version
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<div data-snippet-clipboard-copy-content="MIT License

Copyright (c) 2024 Amr Hesham

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE."><pre><code>MIT License

Copyright (c) 2024 Amr Hesham

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Public DNS's approach to fight against cache poisoning attacks (180 pts)]]></title>
            <link>https://security.googleblog.com/2024/03/google-public-dnss-approach-to-fight.html</link>
            <guid>39960125</guid>
            <pubDate>Sun, 07 Apr 2024 11:46:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2024/03/google-public-dnss-approach-to-fight.html">https://security.googleblog.com/2024/03/google-public-dnss-approach-to-fight.html</a>, See on <a href="https://news.ycombinator.com/item?id=39960125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Command Injection and Backdoor Account in D-Link NAS Devices (198 pts)]]></title>
            <link>https://github.com/netsecfish/dlink</link>
            <guid>39960107</guid>
            <pubDate>Sun, 07 Apr 2024 11:39:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/netsecfish/dlink">https://github.com/netsecfish/dlink</a>, See on <a href="https://news.ycombinator.com/item?id=39960107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Command Injection and Backdoor Account in D-Link NAS Devices</h2><a id="user-content-command-injection-and-backdoor-account-in-d-link-nas-devices" aria-label="Permalink: Command Injection and Backdoor Account in D-Link NAS Devices" href="#command-injection-and-backdoor-account-in-d-link-nas-devices"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Vulnerability Summary:</strong></h2><a id="user-content-vulnerability-summary" aria-label="Permalink: Vulnerability Summary:" href="#vulnerability-summary"></a></p>
<p dir="auto">The described vulnerability affects multiple D-Link NAS devices, including models DNS-340L, DNS-320L, DNS-327L, and DNS-325, among others. The vulnerability lies within the <strong><code>nas_sharing.cgi</code></strong> uri, which is vulnerable due to two main issues: a backdoor facilitated by hardcoded credentials, and a command injection vulnerability via the <strong><code>system</code></strong> parameter. This exploitation could lead to arbitrary command execution on the affected D-Link NAS devices, granting attackers potential access to sensitive information, system configuration alteration, or denial of service, by specifying a command,affecting over 92,000 devices on the Internet.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/netsecfish/dlink/blob/main/fofa-result.png"><img src="https://github.com/netsecfish/dlink/raw/main/fofa-result.png" alt="Untitled"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Corresponding CWE</h2><a id="user-content-corresponding-cwe" aria-label="Permalink: Corresponding CWE" href="#corresponding-cwe"></a></p>
<p dir="auto">CWE-77 (Command Injection) and CWE-798 (Use of Hard-coded Credentials).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Affected Devices:</h2><a id="user-content-affected-devices" aria-label="Permalink: Affected Devices:" href="#affected-devices"></a></p>
<ul dir="auto">
<li>DNS-320L Version 1.11, Version 1.03.0904.2013, Version 1.01.0702.2013</li>
<li>DNS-325 Version 1.01</li>
<li>DNS-327L Version 1.09, Version 1.00.0409.2013</li>
<li>DNS-340L Version 1.08</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Vulnerability Details:</h2><a id="user-content-vulnerability-details" aria-label="Permalink: Vulnerability Details:" href="#vulnerability-details"></a></p>
<p dir="auto">The vulnerability exists in the <code>nas_sharing.cgi</code> CGI script, which leads to:</p>
<ol dir="auto">
<li><strong>Backdoor through Username and Password Exposure</strong>: The request includes parameters for a username (<strong><code>user=messagebus</code></strong>) and an empty password field (<strong><code>passwd=</code></strong>). This indicates a backdoor allowing unauthorized access without proper authentication.</li>
<li><strong>Command Injection through the System Parameter</strong>: The <strong><code>system</code></strong> parameter within the request carries a base64 encoded value that, when decoded, appears to be a command.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Exploitation:</h2><a id="user-content-exploitation" aria-label="Permalink: Exploitation:" href="#exploitation"></a></p>
<p dir="auto">Craft Malicious HTTP Request: Prepare an HTTP GET request targeting the <code>/cgi-bin/nas_sharing.cgi</code> endpoint.</p>
<div data-snippet-clipboard-copy-content="GET /cgi-bin/nas_sharing.cgi?user=messagebus&amp;passwd=&amp;cmd=15&amp;system=<BASE64_ENCODED_COMMAND_TO_BE_EXECUTED>"><pre><code>GET /cgi-bin/nas_sharing.cgi?user=messagebus&amp;passwd=&amp;cmd=15&amp;system=&lt;BASE64_ENCODED_COMMAND_TO_BE_EXECUTED&gt;
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Actual Result</strong></h2><a id="user-content-actual-result" aria-label="Permalink: Actual Result" href="#actual-result"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/netsecfish/dlink/blob/main/dns-320.jpg"><img src="https://github.com/netsecfish/dlink/raw/main/dns-320.jpg" alt="Untitled"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Impact:</h2><a id="user-content-impact" aria-label="Permalink: Impact:" href="#impact"></a></p>
<p dir="auto">Successful exploitation of this vulnerability could allow an attacker to execute arbitrary commands on the system, potentially leading to unauthorized access to sensitive information, modification of system configurations, or denial of service conditions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Fix Recommendation:</strong></h2><a id="user-content-fix-recommendation" aria-label="Permalink: Fix Recommendation:" href="#fix-recommendation"></a></p>
<ul dir="auto">
<li>Apply available patches and updates from the device manufacturer.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Court approves 3M multi-billion dollar settlement over PFAS in drinking water (190 pts)]]></title>
            <link>https://www.cbsnews.com/minnesota/news/3m-pfas-drinking-water-settlement/</link>
            <guid>39960069</guid>
            <pubDate>Sun, 07 Apr 2024 11:26:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbsnews.com/minnesota/news/3m-pfas-drinking-water-settlement/">https://www.cbsnews.com/minnesota/news/3m-pfas-drinking-water-settlement/</a>, See on <a href="https://news.ycombinator.com/item?id=39960069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                                                                
                                                        
<article id="article-0" data-index="0" data-path="/minnesota/news/3m-pfas-drinking-water-settlement/">

  <div id="article-header" data-sort-time="1712029688000" data-update-time="1712029688000">
    <header>

      
      <div>

        <!-- show SVG here -->
                            <p><a href="https://www.cbsnews.com/minnesota/" data-invalid-url-rewritten-http="">
            <img width="60" height="60" alt="minnesota" src="https://www.cbsnews.com/assets/show/minnesota/logo-square-32.svg" loading="lazy"></a></p><!-- end show SVG -->

              <p>
    By
                        
              , 
                              
              <span>The Associated Press</span>
              </p>
  
          

        <p>
          <time datetime="2024-04-01T22:48:08-0500">Updated on:  April 1, 2024 / 10:48 PM CDT</time>
          / CBS/AP
        </p>
        


</div>

          </header></div>

      




  
  
      
        
  <figure data-ads="{&quot;extraWordCount&quot;:50}"><div>
      
      
                      



  <svg xmlns="http://www.w3.org/2000/svg" style="position:absolute;width:0;height:0"><defs><symbol id="player-icon-pause" viewBox="0 0 32 32"><path d="M4 4h10v24h-10zM18 4h10v24h-10z"></path></symbol><symbol id="player-icon-play" viewBox="0 0 32 32"><path d="M6 4l20 12-20 12z"></path></symbol><symbol id="player-icon-close" viewBox="0 0 32 32"><line stroke-width="6" x1="3" y1="3" x2="29" y2="29"></line><line stroke-width="6" x1="29" y1="3" x2="3" y2="29"></line></symbol><symbol id="player-icon-fullscreen" viewBox="0 0 32 32"><path d="M32 0h-13l5 5-6 6 3 3 6-6 5 5z"></path><path d="M32 32v-13l-5 5-6-6-3 3 6 6-5 5z"></path><path d="M0 32h13l-5-5 6-6-3-3-6 6-5-5z"></path><path d="M0 0v13l5-5 6 6 3-3-6-6 5-5z"></path></symbol><symbol id="player-icon-drag" viewBox="0 0 40 55"><g fill="#f5f5f5"><circle cx="5" cy="5" r="5"></circle><circle cx="20" cy="5" r="5"></circle><circle cx="35" cy="5" r="5"></circle><circle cx="5" cy="20" r="5"></circle><circle cx="20" cy="20" r="5"></circle><circle cx="35" cy="20" r="5"></circle><circle cx="5" cy="35" r="5"></circle><circle cx="20" cy="35" r="5"></circle><circle cx="35" cy="35" r="5"></circle><circle cx="5" cy="50" r="5"></circle><circle cx="20" cy="50" r="5"></circle><circle cx="35" cy="50" r="5"></circle></g></symbol></defs></svg><div data-theme="default" data-component="viewability" data-viewability-options="true">
                  <svg><use xlink:href="#player-icon-drag"></use></svg><p><span>Report finds some water filters can reduce PFAS from tap water </span>
        
        

      </p></div>
          </div>
          <figcaption><a href="https://www.cbsnews.com/minnesota/video/report-finds-some-water-filters-can-reduce-pfas-from-tap-water/" data-invalid-url-rewritten-http="">
          
          <span>
            </span>

          <span>Report finds some water filters can reduce PFAS from tap water</span>

          <span>01:54</span>

                      </a>
                  
        
        
              </figcaption></figure><section><p><strong>MINNEAPOLIS —</strong> Minnesota-based chemical manufacturer 3M will begin payments this summer to many U.S. public drinking water systems as part of a multi-billion-dollar settlement over contamination with potentially dangerous chemicals, the company said.</p><p>Communities in the east metro are especially impacted by the contamination.</p><p>3M announced Monday that last year's lawsuit settlement received final approval from the U.S. District Court in Charleston, South Carolina.</p><p>The agreement called for payouts through 2036. Depending on what additional contamination is found, the amount paid out will range from $10.5 billion to $12.5 billion.</p><p><strong>RELATED:&nbsp;</strong><span><a href="https://www.cbsnews.com/minnesota/news/minnesota-pfas-forever-chemicals-drinking-water/" target="_blank" data-invalid-url-rewritten-http=""><strong>Despite historic 3M PFAS payout, Minnesota communities need millions more for cleanup</strong></a></span></p>

    

<p>"This is yet another important step forward for 3M as we continue to deliver on our priorities. The final approval of this settlement and continued progress toward exiting all PFAS manufacturing by the end of 2025 will further our efforts to reduce risk and uncertainty as we move forward," 3M's chairman and CEO, Mike Roman, said in a news release.</p><p>Six years ago, 3M settled with the state for $850 million for disposing the chemicals and contaminating drinking water and our environment —&nbsp;<span><a href="https://www.cbsnews.com/minnesota/news/3m-agrees-to-10-3-billion-in-pfas-settlement/" target="_blank" data-invalid-url-rewritten-http="">one of the largest settlements of its kind in the country</a></span>.</p><p>The deal compensates water providers for pollution with per- and polyfluorinated substances, known collectively as PFAS — a broad class of chemicals used in nonstick, water- and grease-resistant products such as clothing and cookware.</p><p>PFAS have been described as "forever chemicals" because they don't degrade naturally in the environment. They've been linked to a variety of health problems, including liver and immune-system damage and some cancers.</p>

    
    

<p>The compounds have been detected at varying levels in drinking water nationwide. The Environmental Protection Agency in March 2023 proposed strict limits on two common types, PFOA and PFOS, and said it wanted to regulate four others. Water providers would be responsible for monitoring their systems for the chemicals.</p><p><strong>RELATED:&nbsp;<span><a href="https://www.cbsnews.com/minnesota/news/pfas-in-minnesota-how-forever-chemicals-changed-the-state-of-water/" target="_blank" data-invalid-url-rewritten-http="">PFAS in Minnesota: How "forever chemicals" changed the state of water</a></span></strong></p><p><span><a href="https://www.cbsnews.com/minnesota/news/minnesota-house-approves-environment-package-that-includes-ban-on-forever-chemicals-pfas/" target="_blank" data-invalid-url-rewritten-http="">A new state law passed last year</a></span>&nbsp;will ban PFAS in some consumer products starting in 2025 with a full ban in 2032. But prevention is only one part of the solution. Some communities like Woodbury and St. Louis County also want lawmakers to approve funding for PFAS mitigation in their infrastructure package this year.</p><p>The <span><a href="https://www.cbsnews.com/minnesota/news/3m-agrees-to-10-3-billion-in-pfas-settlement/" target="_blank" data-invalid-url-rewritten-http="">3M settlement first announced in June</a></span> came in a lawsuit by Stuart, Florida, one of about 300 communities that had filed similar suits against companies that produced firefighting foam or the PFAS it contained. The payment will help cover the costs of filtering PFAS from systems.</p><p>Some of the settlement money will help additional water systems test for contamination from PFAS, said Scott Summy, one of the lead attorneys for those suing 3M and other manufacturers. They have until June 2026 to apply for compensation if contamination is found. </p><p>"That's great news for American citizens who drink from that water," Summy said. "It'll help rid our public drinking water systems of PFAS, and that's the most important thing about the settlement."</p><p>3M&nbsp;<span><a href="https://www.cbsnews.com/minnesota/news/3m-plans-to-end-manufacturing-and-use-of-pfas-by-end-of-2025/" target="_blank" data-invalid-url-rewritten-http="">pledged</a></span>&nbsp;in late 2022 that the company would stop manufacturing and using PFAS by the end of 2025.</p>
                  
        
      
                  
    <!-- data-recirc-source="queryly" -->

    <!-- tags --><ul>In:
          <li><a href="https://www.cbsnews.com/minnesota/tag/health/">Health</a></li>
          <li><a href="https://www.cbsnews.com/minnesota/tag/charleston/">Charleston</a></li>
          <li><a href="https://www.cbsnews.com/minnesota/tag/lawsuit/">Lawsuit</a></li>
      </ul><div>
      <p><a href="https://www.cbsnews.com/minnesota/search/author/wcco-staff/" data-invalid-url-rewritten-http="">WCCO Staff</a></p><div>
                  <p><a href="https://www.cbsnews.com/minnesota/search/author/wcco-staff/" data-invalid-url-rewritten-http="">
              <span><img src="https://www.cbsnews.com/minnesota/news/3m-pfas-drinking-water-settlement/512-appicon-minnesota.png" alt="512-appicon-minnesota.png " height="80" width="80" data-srcset="https://assets3.cbsnewsstatic.com/hub/i/r/2023/04/14/3f903c1c-7834-4b45-9e2a-d1e23d837478/thumbnail/80x80/e370a104780595f243857bf615c869d6/512-appicon-minnesota.png?v=95af720165ffeea582866d60dd9b1b18 1x" srcset="data:image/svg+xml,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2080%2080'%2F%3E"></span>
            </a>
          </p>
                  <div>
            <p>The WCCO Staff is a group of experienced journalists who bring you the content on WCCO.com.</p>
                          <p>
                                                                                              <a href="https://twitter.com/wcco" rel="nofollow noopener">
                      
                      Twitter
                    </a>
                
                                                                        
                  <a href="https://facebook.com/CBSMinnesota" rel="nofollow noopener">
                    
                    Facebook
                  </a>
                
                
                                                                
                <a href="https://instagram.com/wcco" rel="nofollow noopener">
                  
                  Instagram
                </a>
                        </p>
                </div>
      </div>
    </div>
      </section>
  </article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Did any processor implement an integer square root instruction? (213 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/29787/did-any-processor-implement-an-integer-square-root-instruction</link>
            <guid>39959946</guid>
            <pubDate>Sun, 07 Apr 2024 10:55:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/29787/did-any-processor-implement-an-integer-square-root-instruction">https://retrocomputing.stackexchange.com/questions/29787/did-any-processor-implement-an-integer-square-root-instruction</a>, See on <a href="https://news.ycombinator.com/item?id=39959946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p><strong>It's not that easy.</strong></p>
<p>The most efficient method to calculate square root is to calculate inverse/reciprocal of the square root using Newton-Raphson iterations, and then multiply it with the original.</p>
<p>This is best known as the <a href="https://thatonegamedev.com/math/fast-square-root-quake-iii/" rel="noreferrer">"Quake method"</a> (see also <a href="https://retrocomputing.stackexchange.com/q/4615/79">Where did Fast InvSqrt() come from?</a>). The more modern version used by contemporary CPU and GPUs are generalized into two instructions, one for estimating the initial guess (e.g. <a href="https://developer.arm.com/documentation/ddi0602/2022-03/SIMD-FP-Instructions/FRSQRTE--Floating-point-Reciprocal-Square-Root-Estimate-?lang=en" rel="noreferrer">frsqrte of ARMv8</a>), another to run the following iterations (e.g. <a href="https://developer.arm.com/documentation/ddi0602/2022-03/SIMD-FP-Instructions/FRSQRTS--Floating-point-Reciprocal-Square-Root-Step-?lang=en" rel="noreferrer">frsqrts of ARMv8</a>). Single-instruction version of sqrt is a micro-coded or pseudo-instruction version of these two instructions.</p>
<p><strong>The prerequisite for all of this is a multiplier.</strong></p>
<p>If you want to calculate FP (i)sqrt, then you need a (fast) FP multiplier, which all FPUs have.</p>
<p>If you want to calculate integer (i)sqrt, then you need a (fast) integer multiplier, which most CPUs <strong>don't</strong> have (historically). Otherwise it would be called a <strong>DSP</strong>.</p>
<p>To make it better, you need a (fast) multiplier that is twice the width of your input to have sufficient precision, which most CPUs definitely don't have until "relatively" recently (relative to RetroComputing).</p>
<p><strong>And precision matters, or not?</strong></p>
<p>If you look at the "Quake method" closely, you notice that one of the iterations was commented out.</p>
<p>There are a lot of use cases where the extreme precision isn't necessary and it'll be better to leave the choice of precision/speed trade off to programmers. <code>isqrt</code> was intentionally separated into <code>fsqrte</code> and <code>fsqrts</code> on ARMv8 exactly for this reason: so that the programmer can adjust the number of <code>fsqrts</code> for the desired speed and accuracy tradeoff.</p>
<p>So I don't quite agree to the statement that single instruction sqrt is very common. It's there because the IEEEE754 and the C stand math library requires it (for the flag bits and exceptions), but that doesn't mean it's frequently used.</p>
<p><strong>Further reading</strong></p>
<ul>
<li><a href="https://www.ti.com/tool/SPRC542" rel="noreferrer">SPRC542</a> TI's math library for C64x DSP (8-issue VLIW CPU with two 32x32=64 multipliers). In this library <code>_iq _IQisqrt</code> is implemented using Newton-Raphson iterations and <code>_iq _IQsqrt</code> is calculated by multiplying the original with the isqrt. The source code is available on request.</li>
<li><a href="https://www.ti.com/lit/ug/sprugg9/sprugg9.pdf" rel="noreferrer">SPRUGG9</a> TMS320C64x+ IQmath Library User's Guide.The user guide for SPRC542.</li>
<li>My implementation of square root using binary search, that doesn't depend on a multiplier. Only basic ALU instructions are used. It is vigorously undocumented. I have no idea what I wrote but it seems to work.</li>
</ul>
<p>.</p>
<pre><code>unsigned int usqrt(unsigned int x){
    unsigned int a=0;
    unsigned int masksq=0,mask=0;
    unsigned int mask_shift=15;
    for(masksq=1u&lt;&lt;(mask_shift&lt;&lt;1),mask=1u&lt;&lt;(mask_shift);
        mask!=0;
        masksq=masksq&gt;&gt;2,mask=mask&gt;&gt;1,mask_shift--){
        if(x&gt;=masksq){
            a=mask;
            break;
        }
    }
    x-=masksq;//masksq==a*a;
    mask=mask&gt;&gt;1;
    mask_shift--;
    while(mask&gt;0){
        unsigned int step=(mask&lt;&lt;mask_shift)+(a&lt;&lt;(mask_shift+1));
        if(x&gt;=step){
            a|=mask;
            x-=step;
        }
        mask=mask&gt;&gt;1;
        mask_shift--;
    }
    return a;
}
</code></pre>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SentenceTransformers: Python framework for sentence, text and image embeddings (178 pts)]]></title>
            <link>https://www.sbert.net/index.html</link>
            <guid>39959790</guid>
            <pubDate>Sun, 07 Apr 2024 10:23:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sbert.net/index.html">https://www.sbert.net/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39959790">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="#">Sentence-Transformers</a>
        
      </nav>


      <div itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
            
  
<div id="installation">
<h2>Installation<a href="#installation" title="Permalink to this headline">¶</a></h2>
<p>You can install it using pip:</p>
<div><pre><span></span><span>pip</span> <span>install</span> <span>-</span><span>U</span> <span>sentence</span><span>-</span><span>transformers</span>
</pre></div>
<p>We recommend <strong>Python 3.8</strong> or higher, and at least <strong>PyTorch 1.11.0</strong>. See <a href="https://www.sbert.net/docs/installation.html">installation</a> for further installation options, especially if you want to use a GPU.</p>
</div>
<div id="usage">
<h2>Usage<a href="#usage" title="Permalink to this headline">¶</a></h2>
<p>The usage is as simple as:</p>
<div><pre><span></span><span>from</span> <span>sentence_transformers</span> <span>import</span> <span>SentenceTransformer</span>
<span>model</span> <span>=</span> <span>SentenceTransformer</span><span>(</span><span>"all-MiniLM-L6-v2"</span><span>)</span>

<span># Our sentences to encode</span>
<span>sentences</span> <span>=</span> <span>[</span>
    <span>"This framework generates embeddings for each input sentence"</span><span>,</span>
    <span>"Sentences are passed as a list of string."</span><span>,</span>
    <span>"The quick brown fox jumps over the lazy dog."</span>
<span>]</span>

<span># Sentences are encoded by calling model.encode()</span>
<span>embeddings</span> <span>=</span> <span>model</span><span>.</span><span>encode</span><span>(</span><span>sentences</span><span>)</span>

<span># Print the embeddings</span>
<span>for</span> <span>sentence</span><span>,</span> <span>embedding</span> <span>in</span> <span>zip</span><span>(</span><span>sentences</span><span>,</span> <span>embeddings</span><span>):</span>
    <span>print</span><span>(</span><span>"Sentence:"</span><span>,</span> <span>sentence</span><span>)</span>
    <span>print</span><span>(</span><span>"Embedding:"</span><span>,</span> <span>embedding</span><span>)</span>
    <span>print</span><span>(</span><span>""</span><span>)</span>
</pre></div>
</div>
<div id="performance">
<h2>Performance<a href="#performance" title="Permalink to this headline">¶</a></h2>
<p>Our models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at <a href="https://www.sbert.net/docs/pretrained_models.html">Pre-Trained Models</a> for an overview of available models and the respective performance on different tasks.</p>
</div>

<div id="citing-authors">
<h2>Citing &amp; Authors<a href="#citing-authors" title="Permalink to this headline">¶</a></h2>
<p>If you find this repository helpful, feel free to cite our publication <a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>:</p>
<blockquote>
<div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2019-sentence-bert</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>"Reimers, Nils and Gurevych, Iryna"</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>"11"</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>"2019"</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>"Association for Computational Linguistics"</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>"https://arxiv.org/abs/1908.10084"</span><span>,</span>
<span>}</span>
</pre></div></blockquote>
<p>If you use one of the multilingual models, feel free to cite our publication <a href="https://arxiv.org/abs/2004.09813">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a>:</p>
<blockquote>
<div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2020-multilingual-sentence-bert</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation"</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>"Reimers, Nils and Gurevych, Iryna"</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>"11"</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>"2020"</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>"Association for Computational Linguistics"</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>"https://arxiv.org/abs/2004.09813"</span><span>,</span>
<span>}</span>
</pre></div></blockquote>
<p>If you use the code for <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/data_augmentation">data augmentation</a>, feel free to cite our publication <a href="https://arxiv.org/abs/2010.08240">Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a>:</p>
<blockquote>
<div><pre><span></span><span>@inproceedings</span><span>{</span><span>thakur-2020-AugSBERT</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks"</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna"</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>jun</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>"2021"</span><span>,</span>
<span>  </span><span>address</span><span> </span><span>=</span><span> </span><span>"Online"</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>"Association for Computational Linguistics"</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>"https://www.aclweb.org/anthology/2021.naacl-main.28"</span><span>,</span>
<span>  </span><span>pages</span><span> </span><span>=</span><span> </span><span>"296--310"</span><span>,</span>
<span>}</span>
</pre></div></blockquote>
<div>
<p><span>Overview</span></p>
<ul>
<li><a href="https://www.sbert.net/docs/installation.html">Installation</a><ul>
<li><a href="https://www.sbert.net/docs/installation.html#install-sentencetransformers">Install SentenceTransformers</a></li>
<li><a href="https://www.sbert.net/docs/installation.html#install-pytorch-with-cuda-support">Install PyTorch with CUDA support</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/quickstart.html">Quickstart</a><ul>
<li><a href="https://www.sbert.net/docs/quickstart.html#comparing-sentence-similarities">Comparing Sentence Similarities</a></li>
<li><a href="https://www.sbert.net/docs/quickstart.html#pre-trained-models">Pre-Trained Models</a></li>
<li><a href="https://www.sbert.net/docs/quickstart.html#training-your-own-embeddings">Training your own Embeddings</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/pretrained_models.html">Pretrained Models</a><ul>
<li><a href="https://www.sbert.net/docs/pretrained_models.html#model-overview">Model Overview</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_models.html#semantic-search">Semantic Search</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models">Multi-Lingual Models</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_models.html#image-text-models">Image &amp; Text-Models</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_models.html#other-models">Other Models</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html">Pretrained Cross-Encoders</a><ul>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#ms-marco">MS MARCO</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#squad-qnli">SQuAD (QNLI)</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#stsbenchmark">STSbenchmark</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#quora-duplicate-questions">Quora Duplicate Questions</a></li>
<li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#nli">NLI</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/publications.html">Publications</a></li>
<li><a href="https://www.sbert.net/docs/hugging_face.html">Hugging Face 🤗</a><ul>
<li><a href="https://www.sbert.net/docs/hugging_face.html#the-hugging-face-hub">The Hugging Face Hub</a></li>
<li><a href="https://www.sbert.net/docs/hugging_face.html#using-hugging-face-models">Using Hugging Face models</a></li>
<li><a href="https://www.sbert.net/docs/hugging_face.html#sharing-your-models">Sharing your models</a></li>
<li><a href="https://www.sbert.net/docs/hugging_face.html#sharing-your-embeddings">Sharing your embeddings</a></li>
<li><a href="https://www.sbert.net/docs/hugging_face.html#additional-resources">Additional resources</a></li>
</ul>
</li>
</ul>
</div>
<div>
<p><span>Usage</span></p>
<ul>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html">Computing Sentence Embeddings</a><ul>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#prompt-templates">Prompt Templates</a></li>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#input-sequence-length">Input Sequence Length</a></li>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#storing-loading-embeddings">Storing &amp; Loading Embeddings</a></li>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#multi-process-multi-gpu-encoding">Multi-Process / Multi-GPU Encoding</a></li>
<li><a href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers">Sentence Embeddings with Transformers</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a></li>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html">Embedding Quantization</a><ul>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html#binary-quantization">Binary Quantization</a></li>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html#scalar-int8-quantization">Scalar (int8) Quantization</a></li>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html#additional-extensions">Additional extensions</a></li>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html#demo">Demo</a></li>
<li><a href="https://www.sbert.net/examples/applications/embedding-quantization/README.html#try-it-yourself">Try it yourself</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html">Semantic Search</a><ul>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#background">Background</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">Symmetric vs. Asymmetric Semantic Search</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#python">Python</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#util-semantic-search">util.semantic_search</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#speed-optimization">Speed Optimization</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#elasticsearch">Elasticsearch</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#approximate-nearest-neighbor">Approximate Nearest Neighbor</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#retrieve-re-rank">Retrieve &amp; Re-Rank</a></li>
<li><a href="https://www.sbert.net/examples/applications/semantic-search/README.html#examples">Examples</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a><ul>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#retrieve-re-rank-pipeline">Retrieve &amp; Re-Rank Pipeline</a></li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#retrieval-bi-encoder">Retrieval: Bi-Encoder</a></li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#re-ranker-cross-encoder">Re-Ranker: Cross-Encoder</a></li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#example-scripts">Example Scripts</a></li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#pre-trained-bi-encoders-retrieval">Pre-trained Bi-Encoders (Retrieval)</a></li>
<li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html#pre-trained-cross-encoders-re-ranker">Pre-trained Cross-Encoders (Re-Ranker)</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/clustering/README.html">Clustering</a><ul>
<li><a href="https://www.sbert.net/examples/applications/clustering/README.html#k-means">k-Means</a></li>
<li><a href="https://www.sbert.net/examples/applications/clustering/README.html#agglomerative-clustering">Agglomerative Clustering</a></li>
<li><a href="https://www.sbert.net/examples/applications/clustering/README.html#fast-clustering">Fast Clustering</a></li>
<li><a href="https://www.sbert.net/examples/applications/clustering/README.html#topic-modeling">Topic Modeling</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/paraphrase-mining/README.html">Paraphrase Mining</a></li>
<li><a href="https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html">Translated Sentence Mining</a><ul>
<li><a href="https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html#marging-based-mining">Marging Based Mining</a></li>
<li><a href="https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html#examples">Examples</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html">Cross-Encoders</a><ul>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html#bi-encoder-vs-cross-encoder">Bi-Encoder vs. Cross-Encoder</a></li>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html#when-to-use-cross-bi-encoders">When to use Cross- / Bi-Encoders?</a></li>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html#cross-encoders-usage">Cross-Encoders Usage</a></li>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html#combining-bi-and-cross-encoders">Combining Bi- and Cross-Encoders</a></li>
<li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html#training-cross-encoders">Training Cross-Encoders</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/applications/image-search/README.html">Image Search</a><ul>
<li><a href="https://www.sbert.net/examples/applications/image-search/README.html#installation">Installation</a></li>
<li><a href="https://www.sbert.net/examples/applications/image-search/README.html#usage">Usage</a></li>
<li><a href="https://www.sbert.net/examples/applications/image-search/README.html#examples">Examples</a></li>
</ul>
</li>
</ul>
</div>
<div>
<p><span>Training</span></p>
<ul>
<li><a href="https://www.sbert.net/docs/training/overview.html">Training Overview</a><ul>
<li><a href="https://www.sbert.net/docs/training/overview.html#network-architecture">Network Architecture</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#creating-networks-from-scratch">Creating Networks from Scratch</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#training-data">Training Data</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#loss-functions">Loss Functions</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#evaluators">Evaluators</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#loading-custom-sentencetransformer-models">Loading Custom SentenceTransformer Models</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#multitask-training">Multitask Training</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#adding-special-tokens">Adding Special Tokens</a></li>
<li><a href="https://www.sbert.net/docs/training/overview.html#best-transformer-model">Best Transformer Model</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/docs/training/loss_overview.html">Loss Overview</a><ul>
<li><a href="https://www.sbert.net/docs/training/loss_overview.html#loss-modifiers">Loss modifiers</a></li>
<li><a href="https://www.sbert.net/docs/training/loss_overview.html#distillation">Distillation</a></li>
<li><a href="https://www.sbert.net/docs/training/loss_overview.html#commonly-used-loss-functions">Commonly used Loss Functions</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html">Matryoshka Embeddings</a><ul>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html#use-cases">Use Cases</a></li>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html#results">Results</a></li>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html#inference">Inference</a></li>
<li><a href="https://www.sbert.net/examples/training/matryoshka/README.html#code-examples">Code Examples</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html">Adaptive Layers</a><ul>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html#use-cases">Use Cases</a></li>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html#results">Results</a></li>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html#inference">Inference</a></li>
<li><a href="https://www.sbert.net/examples/training/adaptive_layer/README.html#code-examples">Code Examples</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html">Multilingual-Models</a><ul>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#available-pre-trained-models">Available Pre-trained Models</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#usage">Usage</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#performance">Performance</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#extend-your-own-models">Extend your own models</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#data-format">Data Format</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#loading-training-datasets">Loading Training Datasets</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#sources-for-training-data">Sources for Training Data</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#evaluation">Evaluation</a></li>
<li><a href="https://www.sbert.net/examples/training/multilingual/README.html#citation">Citation</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/distillation/README.html">Model Distillation</a><ul>
<li><a href="https://www.sbert.net/examples/training/distillation/README.html#knowledge-distillation">Knowledge Distillation</a></li>
<li><a href="https://www.sbert.net/examples/training/distillation/README.html#speed-performance-trade-off">Speed - Performance Trade-Off</a></li>
<li><a href="https://www.sbert.net/examples/training/distillation/README.html#dimensionality-reduction">Dimensionality Reduction</a></li>
<li><a href="https://www.sbert.net/examples/training/distillation/README.html#quantization">Quantization</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/cross-encoder/README.html">Cross-Encoders</a><ul>
<li><a href="https://www.sbert.net/examples/training/cross-encoder/README.html#examples">Examples</a></li>
<li><a href="https://www.sbert.net/examples/training/cross-encoder/README.html#training-crossencoders">Training CrossEncoders</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html">Augmented SBERT</a><ul>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#motivation">Motivation</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#extend-to-your-own-datasets">Extend to your own datasets</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#methodology">Methodology</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#scenario-1-limited-or-small-annotated-datasets-few-labeled-sentence-pairs">Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs)</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#scenario-2-no-annotated-datasets-only-unlabeled-sentence-pairs">Scenario 2: No annotated datasets (Only unlabeled sentence-pairs)</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/data_augmentation/README.html#citation">Citation</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/datasets/README.html">Training Datasets</a><ul>
<li><a href="https://www.sbert.net/examples/training/datasets/README.html#datasets-on-the-hugging-face-hub">Datasets on the Hugging Face Hub</a></li>
</ul>
</li>
</ul>
</div>
<div>
<p><span>Training Examples</span></p>
<ul>
<li><a href="https://www.sbert.net/examples/training/sts/README.html">Semantic Textual Similarity</a><ul>
<li><a href="https://www.sbert.net/examples/training/sts/README.html#training-data">Training data</a></li>
<li><a href="https://www.sbert.net/examples/training/sts/README.html#loss-function">Loss Function</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/nli/README.html">Natural Language Inference</a><ul>
<li><a href="https://www.sbert.net/examples/training/nli/README.html#data">Data</a></li>
<li><a href="https://www.sbert.net/examples/training/nli/README.html#softmaxloss">SoftmaxLoss</a></li>
<li><a href="https://www.sbert.net/examples/training/nli/README.html#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/paraphrases/README.html">Paraphrase Data</a><ul>
<li><a href="https://www.sbert.net/examples/training/paraphrases/README.html#datasets">Datasets</a></li>
<li><a href="https://www.sbert.net/examples/training/paraphrases/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/paraphrases/README.html#pre-trained-models">Pre-Trained Models</a></li>
<li><a href="https://www.sbert.net/examples/training/paraphrases/README.html#work-in-progress">Work in Progress</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a><ul>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#pretrained-models">Pretrained Models</a></li>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#dataset">Dataset</a></li>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#usage">Usage</a></li>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#training">Training</a></li>
<li><a href="https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/training/ms_marco/README.html">MS MARCO</a><ul>
<li><a href="https://www.sbert.net/examples/training/ms_marco/README.html#bi-encoder">Bi-Encoder</a></li>
<li><a href="https://www.sbert.net/examples/training/ms_marco/README.html#cross-encoder">Cross-Encoder</a></li>
<li><a href="https://www.sbert.net/examples/training/ms_marco/README.html#cross-encoder-knowledge-distillation">Cross-Encoder Knowledge Distillation</a></li>
</ul>
</li>
</ul>
</div>
<div>
<p><span>Unsupervised Learning</span></p>
<ul>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html">Unsupervised Learning</a><ul>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#tsdae">TSDAE</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#simcse">SimCSE</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#ct">CT</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#ct-in-batch-negative-sampling">CT (In-Batch Negative Sampling)</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#masked-language-model-mlm">Masked Language Model (MLM)</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#genq">GenQ</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#gpl">GPL</a></li>
<li><a href="https://www.sbert.net/examples/unsupervised_learning/README.html#performance-comparison">Performance Comparison</a></li>
</ul>
</li>
<li><a href="https://www.sbert.net/examples/domain_adaptation/README.html">Domain Adaptation</a><ul>
<li><a href="https://www.sbert.net/examples/domain_adaptation/README.html#domain-adaptation-vs-unsupervised-learning">Domain Adaptation vs. Unsupervised Learning</a></li>
<li><a href="https://www.sbert.net/examples/domain_adaptation/README.html#adaptive-pre-training">Adaptive Pre-Training</a></li>
<li><a href="https://www.sbert.net/examples/domain_adaptation/README.html#gpl-generative-pseudo-labeling">GPL: Generative Pseudo-Labeling</a></li>
</ul>
</li>
</ul>
</div>

</div>


           </div>

    </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The lifecycle of a code AI completion (152 pts)]]></title>
            <link>https://sourcegraph.com/blog/the-lifecycle-of-a-code-ai-completion</link>
            <guid>39959380</guid>
            <pubDate>Sun, 07 Apr 2024 08:55:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sourcegraph.com/blog/the-lifecycle-of-a-code-ai-completion">https://sourcegraph.com/blog/the-lifecycle-of-a-code-ai-completion</a>, See on <a href="https://news.ycombinator.com/item?id=39959380">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Generative AI, whether for code, text, images, or other use cases, appears as a magic black box to many users. Users typically navigate to a website, install an app, or set up an extension and start seeing the results of the AI tool. But, have you ever wondered what goes into this magic black box or how it really works?</p>
<p>In this post, we want to demystify what goes into a code AI completion for <a href="https://cody.dev/">Cody</a>, our code AI assistant that knows your entire codebase. Leveraging a Large Language Model (LLM) to generate a code AI response is fairly trivial, but doing so in a production-grade application that serves many different use cases, coding languages, workflows, and other variables while achieving a high-level of completion acceptance and developer happiness is a whole other thing. We’ll cover the importance of the underlying LLM but also expand the implementation to a fully featured AI engineering system that features various pre and post processing steps, discuss the role of context and how to retrieve it, and more as we explore the lifecycle of a code AI completion. Let’s dive in!</p>
<h2 id="code-completions-101">Code completions 101</h2>
<p>In its minimal form, a code autocomplete request takes the current code inside the editor and asks an LLM to complete it. You can do this with ChatGPT too! Consider the following example:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-light"><code data-language="ts" data-theme="github-light"><span data-line=""><span>// sort.js</span></span>
<span data-line=""><span>function</span><span> bubbleSort</span><span>(</span><span>array</span><span>) {</span></span>
<span data-line=""><span>     |</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>From this limited file we already have a lot of information to work with:</p>
<ul>
<li>The cursor is inside the function body so the user is most likely going to continue writing code at this position</li>
<li>We know that the file is called <code>sort.js</code></li>
<li>The code before the cursor (which we refer to as prefix) has strong hints about what code we want to write</li>
<li>The code after the cursor (postfix) is empty and consists only of a closing bracket.</li>
</ul>
<p>We can easily convert this into a prompt for ChatGPT and have it generate the implementation for us: <a href="https://chat.openai.com/share/27aeb581-2d68-4ac8-94eb-3c64af91f0c6">https://chat.openai.com/share/27aeb581-2d68-4ac8-94eb-3c64af91f0c6</a></p>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/chatgpt-bubblesort.png" alt="ChatGPT bubblesort"></p>
<p>Congratulations, you just wrote a code completion AI!</p>
<p>In fact, this is pretty much how we started out with Cody autocomplete back in March! All you need to make this into a full-blown VS Code extension, is to implement this API interface:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-light"><code data-language="ts" data-theme="github-light"><span data-line=""><span>/**</span></span>
<span data-line=""><span> * Provides inline completion items for the given position and document.</span></span>
<span data-line=""><span> * If inline completions are enabled, this method will be called whenever the user stopped typing.</span></span>
<span data-line=""><span> * It will also be called when the user explicitly triggers inline completions or explicitly asks for the next or previous inline completion.</span></span>
<span data-line=""><span> * In that case, all available inline completions should be returned.</span></span>
<span data-line=""><span> * `context.triggerKind` can be used to distinguish between these scenarios.</span></span>
<span data-line=""><span> *</span></span>
<span data-line=""><span> * </span><span>@param</span><span> document</span><span> The document inline completions are requested for.</span></span>
<span data-line=""><span> * </span><span>@param</span><span> position</span><span> The position inline completions are requested for.</span></span>
<span data-line=""><span> * </span><span>@param</span><span> context</span><span> A context object with additional information.</span></span>
<span data-line=""><span> * </span><span>@param</span><span> token</span><span> A cancellation token.</span></span>
<span data-line=""><span> * </span><span>@return</span><span> An array of completion items or a thenable that resolves to an array of completion items.</span></span>
<span data-line=""><span> */</span></span>
<span data-line=""><span>provideInlineCompletionItems</span><span>(document: TextDocument, position: Position, context: InlineCompletionContext, token: CancellationToken): ProviderResult</span><span>&lt;</span><span>InlineCompletionItem[] </span><span>|</span><span> InlineCompletionList</span><span>&gt;</span><span>;</span></span></code></pre></figure>
<p>However, our trivial implementation has a few shortcomings: In a real world application, this would be too slow, it would not have understanding of the right syntactic boundaries, and it would lack contextual awareness of your codebase. The interaction with the LLM is important, but only a small piece of a much larger AI engineering system. Let’s dig a bit deeper and see what it takes to make Cody, a production ready AI application.</p>
<h2 id="how-to-get-great-ai-completions">How to get great AI completions</h2>
<p>Before we dive into the specifics, let’s outline a few basics principles for getting great AI completions. In fact, the principles are the same as if you’re asking someone new on the team to do great work! In order to do their work, the new dev (or the AI assistant) needs to have an understanding of the task at hand. We refer to this knowledge as context. The more <em>context</em> you have, the more effective you’ll be in a project.</p>
<p>For code completions, we can use the current code file as the basis for our context. When writing code, you start by pointing the cursor at a specific position inside the document. From that position, we can define the <em><strong>prefix</strong></em> as the text before and the <em><strong>suffix</strong></em> as the text below that cursor. When coding, your lowest level task is to insert code between the prefix and the suffix.</p>
<p>However, a developer will also look at other files in the project and try to understand relationships between them: Some of this extended <em><strong>context might</strong></em> come from introduction material during their onboarding, their own mental model, existing code and API interfaces, and so much more.</p>
<p>To get great AI completions, we need to think along the same lines and must be able to extract relevant context for the current problem. Modern LLMs already come with a lot of context from the data they were trained on. They know the programming language and are familiar with a lot of the open source libraries that are commonly associated with it. So our task is to fill in the gaps and add context that is specific to the project at hand.</p>
<p>In AI engineering, we call this process RAG (retrieval augmented generation). We <em><strong>retrieve</strong></em> specific knowledge, like code snippets and documentation, from any external knowledge source (which may or may not be included in the model training set) and use it to guide the generative process. If I point you to an arbitrary file in an arbitrary codebase and ask you to “write some code”, you’d also appreciate some context about that codebase. RAG is about automating this process.</p>
<p>When working on code completions inside the editor, we can use APIs available in the editor to get as much context as possible. For example: What repo are you working on? What are other files that you have recently edited? Are you trying to write a docstring, implement a function body, or work out the right arguments for a method call?</p>
<p>With Cody, we use a two step process for retrieving context. We first have a <em><strong>planning</strong></em> step that is packed with heuristics to categorize the type of code completion that is required and then, based on that, retrieve context that works best for the problem at hand.</p>
<p>Once we have a collection of context, we build a prompt that is optimized for the underlying LLM. In our ChatGPT example we would ask it to “complete the following code”. Then it’s up for the GPUs to roll the dice and give you some text back. This step is usually referred to as the <em><strong>generation</strong></em>.</p>
<p>Lastly, we want to do some processing on the generated content. In the ChatGPT example above, there is a lot of text that we do not want in the text editor, for example. We refer to this step as <em><strong>post-processing</strong></em>.</p>
<p>To summarize, every Cody code completion currently goes through these four steps:</p>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/4-step-diagram.png" alt="Steps of a Cody Completion"></p>
<h2 id="planning">Planning</h2>
<p>The first step is all about preparing the best possible execution plan for the autocomplete request. We must decide on what context we believe would work best and what parameters to use for the generation process. At the moment, all of these steps are rule based (that is, they do not invoke any AI system yet and are usually very fast to complete) and based on heuristics that we’ve gathered over time. You can compare this a lot to a database that does a query planning step before it does any of the heavier work. It allows us to divide the problem space into different categories and optimize for them individually, instead of trying to create a one-size-fits-all solution.</p>
<p>Let’s dive into some of the heuristics we currently use in production during this step:</p>
<h3 id="single-line-vs-multi-line-requests">Single-line vs. Multi-line requests</h3>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/single%20vs%20multiline.png" alt="Single vs MultiLine Completion"></p>
<p>The first learning we had is that there are situations where a user would only expect the current line to be completed and situations where users are willing to wait longer in order to receive a completion that fills out a whole function definition. To detect which type of request is needed, we use a mixture of language heuristics (by looking at indentation and specific symbols) and precise language information (guided by Tree-sitter, more on that later).</p>
<p>Multi-line requests run through the same pipeline but have additional logic during post-processing to make sure the response fits well into the existing document. One interesting learning was that if a user is willing to wait longer for a multi-line request, it usually is worth it to increase latency slightly in favor of quality. For our production setup this means we use a more complex language model for multi-line completions than we do for single-line completions.</p>
<p>Because of the language-specific nature of this heuristic, we generally do not support multi-line completions for all languages. However, we’re always happy to extend our list of supported languages and, since Cody is open-source, you can also <a href="https://github.com/sourcegraph/cody/blob/main/vscode/src/completions/language.ts">contribute and improve the list</a>.</p>
<h3 id="syntactic-triggers">Syntactic triggers</h3>
<p>The position of the cursor relative to elements of code like the beginning of an expression or the current block scope offers insight into the user's intent and desired completion behavior. The first version of Cody used regular expressions to approximate these syntactic clues, but there is only so much information that you can extract from plaintext pattern matching. The current version of Cody uses a great tool to obtain concrete syntax trees for each file: <a href="https://tree-sitter.github.io/tree-sitter/">Tree-sitter</a>.</p>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/syntactic-triggers.png" alt="Syntactic triggers"></p>
<p>At Sourcegraph, we are long-time users of Tree-sitter for improving our code search experience and it felt natural to extend the usage for our autocomplete pipeline. More specifically we use custom-built WASM bindings to parse the current document state and use that to trigger syntax-specific branches–For example, to detect if the cursor is currently within a comment.</p>
<p>Tree-sitter is great for this use case because it is extremely fast, supports incremental parsing (so after a document is parsed, changes can be applied with very low latency) and it’s robustness allows us to use it even when the document is currently being worked on and contains syntax errors.</p>
<p>During the planning step, we use Tree-sitter to categorize the autocomplete request into different syntactic actions like implementing a function body, writing a docstring, or implementing a method call. We can then use this information to focus on different types of contexts or modify the parameters for the generation phase.</p>
<h3 id="suggestion-widget-interaction">Suggestion widget interaction</h3>
<p>If you’ve worked with VS Code you’re probably familiar with the suggestion widget. It pops up when you’re trying to call a method on a class and will list you all of the methods that the class implements and is powered by the mighty IntelliSense system. In the context of autocomplete, VS Code gives us some hints to create better interoperability between AI suggestions and the suggest widget as part of the <code>InlineCompletionContext</code>, the range of the document that is going to be replaced with the suggestion and the currently selected suggestion.</p>
<p>Using the suggest widget to steer the LLM results is absolutely magical:</p>
<figure><video width="1280" height="720" autoplay="" muted="" loop="" playsinline="" title="Using the suggeststion widget to drive the autocomplete result" data-cookieconsent="ignore"><source type="video/webm" src="https://storage.googleapis.com/sourcegraph-assets/undefined.webm" data-cookieconsent="ignore"><source type="video/mp4" src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/suggest-widget.mp4" data-cookieconsent="ignore"></video></figure>
<h2 id="retrieval">Retrieval</h2>
<p>Depending on the model being used, there are varying limitations for how long such a context window can be but regardless of these limitations, finding the right code examples and prompting them in the correct way will have a huge impact on the quality of the autocomplete result, as outlined above.</p>
<p><strong>One of the biggest constraints on the retrieval implementation is <em>latency</em></strong>: Retrieval happens before any of the generation work can start and is thus in the hot path of the life cycle. We generally want the end to end latency (that is, the time between the keystroke and the autocomplete becoming visible) to be as fast as possible, definitely under one second and since this must account for network latency and inference speed, there’s not a lot of room for expensive retrieval.</p>
<p>From the first version on, Cody’s main retrieval mechanism was to look at <em>editor context</em>. This takes into account other tabs you have open or files that you recently looked at. The result of such retrieval processes are a few example code snippets that are sorted by relevance. We currently use a sliding window <a href="https://philippspiess.com/note/engineering/ml/jaccard-similarity">Jaccard similarity</a> search to do that: We take a few lines above the current cursor position as the “reference” and then start a sliding window over relevant files to find the best possible matches.</p>
<p>In order to reduce client CPU pressure, we limit the files to the most relevant ones. These are usually the files you looked at very recently and are generally written in the same programming language.</p>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/class-implementation-as-context.png" alt="Class implementation as context"></p>
<p>Over the past few months we’ve experimented with various other context improvements. One thing that seemed very promising was to reuse our existing <a href="https://about.sourcegraph.com/whitepaper/cody-context-architecture.pdf">embeddings</a> index that we already use for other Cody features. We’ve started to move away from this approach as we’re working on improving the accuracy of embeddings responses and removing the need to do extensive caching to make this work.</p>
<p>Editor context is only one possible source for information though and having <a href="https://sourcegraph.com/search">one of the world’s largest code graphs</a>, there’s a lot more that we can do. One overarching problem that we’re working on right now is how do we rank information from different sources and only include the relevant information (we have learned from internal experimentation that adding irrelevant context can make the response quality worse).</p>
<h2 id="generation">Generation</h2>
<p>As we move to the next stage, let’s dive into the heart of the autocomplete process: the Large Language Model (LLM). The LLM is responsible for taking the prompt and generating a completion that is relevant, accurate, and fast.</p>
<p>Sourcegraph has been a vivid early adopter of Anthropic’s Claude. Because of this, our Autocomplete journey started with early experiments in prompting Claude Instant (for its faster response times) to create code completions similar to the ChatGPT example we explored above. We quickly learned that a simple prompt resulted in a lot of frustration for our users:</p>
<ul>
<li><strong>No Fill in the Middle support</strong>: Without adding information from the document suffix, the LLM would often repeat code that is already in the next line. In the terminology of LLMs, this use case is often described as fill in the middle (FITM) or infilling, as the problem is to insert text in the middle of existing text.</li>
<li><strong>Latency</strong>: We measured that a significant number of requests came back with no response at all (so the LLM decided to terminate the request early).</li>
<li><strong>Quality</strong>: Slight variants in the prompt could have a huge impact on quality. E.g. When we ran an experiment with a prompt that tried to improve the accuracy of comments, we learned that mentioning the term comment caused an increase in comments being generated rather than actual code.</li>
</ul>
<p>Over the past months, we have made a lot of improvements to the Claude Instant prompt, let me highlight some in particular:</p>
<ul>
<li>
<p>The first major update to the prompt changed three things which caused the quality, and more specifically the number of no responses to improve dramatically:</p>
<ul>
<li>
<p><strong>We moved from markdown backtick tags for code segments to XML tags as <a href="https://docs.anthropic.com/claude/docs/constructing-a-prompt#mark-different-parts-of-the-prompt">suggested by Anthropic</a></strong>. Since Claude has been fine tuned to pay special attention to the structure created by XML tags, we found an improvement in response quality with this easy change. It pays off to read the docs!</p>
</li>
<li>
<p><strong>We found that including whitespace at the end of the prompt would cause significantly worse responses.</strong> In the <code>bubbleSort</code> example above, we would end the prompt in all of the whitespace that lead to the cursor so it would end in <code>\n</code> followed by four spaces. In real world applications, the indentation would often be higher resulting in even more whitespace. We achieved a significant reduction of empty responses by trimming the prompt and accounting for the whitespace differences in post-processing.</p>
</li>
<li>
<p>We also started to <strong>lay words in Claude’s mouth</strong> by omitting information in the initial question and then leading with this in the assistant prompt. An example for this could be a completion for these two lines:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-light"><code data-language="ts" data-theme="github-light"><span data-line=""><span>const</span><span> array</span><span> =</span><span> [</span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>];</span></span>
<span data-line=""><span>console.</span><span>log</span><span>(</span><span>|</span></span></code></pre></figure>
<p>Which would translate into a prompt like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="jsx" data-theme="github-light"><code data-language="jsx" data-theme="github-light"><span data-line=""><span>Human</span><span>: Complete the following code</span></span>
<span data-line=""><span>&lt;</span><span>code</span><span>&gt;</span></span>
<span data-line=""><span>const array = [1, 2, 3];</span></span>
<span data-line=""><span>&lt;/</span><span>code</span><span>&gt;</span></span>
<span data-line=""> </span>
<span data-line=""><span>Assistant</span><span>: Sure</span><span>!</span><span> Here is the </span><span>completion</span><span>:</span></span>
<span data-line=""><span>&lt;</span><span>code</span><span>&gt;</span></span>
<span data-line=""><span>console.log(</span></span></code></pre></figure>
</li>
</ul>
</li>
<li>
<p>The second major update was to add support for <strong>Fill in the Middle</strong>: Instead of only quoting the prefix, we also added information about the code after the cursor into the prompt. This was not trivial since simple implementations often caused the LLM to simply repeat from the suffix without generating new code. We ended up using a combination of XML tags and the extended reasoning capabilities of Claude Instant 1.2 to our advantage here.</p>
</li>
</ul>
<h3 id="the-strive-for-faster-latencies">The strive for faster latencies</h3>
<p>A general purpose model like Claude Instant is great as it allows you to extend the capabilities of the system by writing better instructions. There is, however, a catch: These advanced reasoning capabilities require a much larger model to work and as a result, end-to-end latencies (as measured from the keystroke until the completion is visible) have not been great which significantly impacted the UX of our service. This was also reflected in a lot of early adopter feedback so it’s become an obsession for us to try and improve the status quo.</p>
<p>Latencies apply throughout every step in the autocomplete lifecycle but the generation part is the definitely the slowest since it also requires routing a request to the backend and the LLM provider. In our quest to improve the UX, we had to be pedantic about every step in this process. This, of course, meant that we had to add tracing to every step in the pipeline and then critically think about how we can improve all of these segments. Oh and what interesting things you’ll find when you do that!</p>
<p>Here’s a number of improvements that we applied in order to reduce 75th percentile of end-to-end latencies for single line completion from 1.8 seconds to under 900 milliseconds over the past months:</p>
<ul>
<li>
<p><strong>Token limits and stop words:</strong> The most time in our request was spent waiting for the LLM to respond (that's somewhat expected). The number of tokens in the prompt and in the output makes a huge difference in these delays, though. After some tweaking and especially the addition of stop-words, we were able to speed up inference times by a ton.</p>
</li>
<li>
<p><strong>Streaming:</strong> Later, we'd even start to use streaming (so the LLM can return the response token-by-token) and our client could implement more advanced mechanisms to terminate a completion request early.</p>
<p>For example if you are looking to complete a function definition and the LLM response starts to define another function after the current one is finished, chances are you don't even want to show the second function--So why block the response until the request is finished?</p>
</li>
<li>
<p><strong>TCP connection reusing:</strong> Autocomplete requires a lot of requests. Roughly a request for every few keystrokes. We don't think about this often, but every new request requires a handshake between the client and server which adds latency.</p>
<p>Luckily there is a solution here and that is to keep the TCP connection open. What we didn't know: Different HTTP clients have different defaults here and since a Cody autocomplete request is routed from the Client to the Sourcegraph server and then the LLM, we needed to make sure that TCP connections are reused for every step in this pipeline.</p>
</li>
<li>
<p><strong>Backend improvements:</strong> The story wouldn't be complete with a few obvious improvements. Once we found out that our logging on the Sourcegraph server does a synchronous write to BigQuery for example, it didn't take long for us to notice that this is probably not the best way. Safe to say our server side logging steps are now no longer blocking the critical paths. Whoops!</p>
</li>
<li>
<p><strong>Parallel request limits:</strong> Early on, Cody autocomplete was triggering multiple generations for every request. This was added in order to mitigate shortcomings of the initial prompts: If we have a sample of two or three completions to use, we can improve the quality by picking the best one. The catch for this though: The latency is now defined as the longest duration of any of the three requests. We were able to reduce this level and currently only request multiple variants for multi-line completions (which are generally more error prone and less latency sensitive).</p>
</li>
<li>
<p><strong>Recycling prior completion requests:</strong> This is a client level improvement that was able to improve latencies in some cases quite dramatically. Imagine you're trying to write <code>console.log(</code>. However, while typing, you make a short break between the <code>console.</code> and the <code>log(</code>. This happens all the time as devs think about how to proceed.</p>
<p>The small delay would cause Cody to make an autocomplete request, however if you're quick in resuming the typing, that result might not make it to the screen yet as the document state keeps changing.</p>
<p>However, chances are that the initial request (the one with <code>console.</code>) would already be enough information for the LLM to generate the desired completion. In practice we have measured this to be the case in quite a few cases (about every tenth request). We have added additional bookkeeping to the clients to detect these cases and recycle such prior completion requests.</p>
</li>
<li>
<p><strong>Following up on downstream performance regressions:</strong> Our extensive latency logging setup was also helpful when the downstream inference provider introduced latency regressions. We're proud of our collaboration with Anthropic on these and the data we could share with them was always helpful to fix the regression quickly.</p>
</li>
</ul>
<p>This is not the end of the journey though and we know there’s still a lot of room for improvement left on the table. One limitation right now is that our backends (the Sourcegraph server and the inference endpoint) are only hosted in one region which is not ideal for users of other parts of the world. There’s also the possibility to improve the raw inference speed, especially as new hardware and algorithms become available.</p>
<h3 id="a-use-case-specific-llm">A use-case specific LLM</h3>
<p>Regardless of how fast we make our Claude Instant implementation, we still have to deal with the fact that it's a general purpose model and is thus a lot larger than it needs to be. To avoid falling into a local maxima, we started evaluations of use-case specific LLMs that are only helpful for generating code. Our hypothesis was that:</p>
<ul>
<li>
<p><strong>Use-case specific LLMs</strong> can be better at the their trained use case while having a reduced size (so they are faster to run)</p>
</li>
<li>
<p>We can take advantage of state-of-the-art models that are trained specifically for the <strong>Fill in the Middle</strong> use case to further improve our response quality.</p>
</li>
<li>
<p><strong>Tokenization for a coder model</strong> is likely going to be in our favor more which means we will be able to generate more characters using smaller token counts.</p>
</li>
<li>
<p>Being able to leverage <strong>open source LLMs</strong> is going to help us futureproof the system while allowing us to have more control over the deployment (if we want to spin up an inference end point at a specific location, we'll need to be in control of this).</p>
</li>
</ul>
<p><a href="https://huggingface.co/bigcode/starcoder">StarCoder</a> has always been a model that we found particularly interesting given that it is built especially for our use case, it has multiple variants (based on the parameters size) so we can run faster models for use cases where we do not need the full accuracy. We can even rely on <a href="https://huggingface.co/docs/optimum/concept_guides/quantization">quantized versions</a> (the name of a technique to reduce the precision of a model to reduce its size), that have almost no visible quality difference while being even faster to run.</p>
<p>After a long evaluation period against other models, we began a broad A/B test on our community user group and, after a few bug fixes and improvements, have recently finished the rollout for community users to this model, resulting in much reduced latencies and an increase in acceptance rate for our users.</p>
<p>At Sourcegraph, we've always believed that our strength does not come from being tied and hyper-optimized around a specific LLM (heck, the one that you optimize for can be outdated in months anyways!) but that we need to be flexible to use the best tooling available and feed it the most relevant context. This unlocks quite a few opportunities where we can easily move to a better model for our users and even support <a href="https://github.com/sourcegraph/cody/pull/905">local-only inference with tools like Ollama</a>. After all, the AI journey has only just started!</p>
<h2 id="post-processing">Post-processing</h2>
<p>Once we have a string from the backend we're done, ...right? Well, almost. The reality is that sometimes responses aren't quite what you expect them to be but since we've gone through all of this effort to create these strings, we'll go to lengths to salvage whatever we got back.</p>
<p>With Cody, this step is called post-processing and we employ a number of tricks to make sure the text that is being displayed at the screen is as relevant as possible:</p>
<ul>
<li>
<p><strong>Avoiding repeated content:</strong> If there's one quality that LLMs have it's that they're really good at repeating content. Unfortunately this sometimes leads to undesired results when the completion contains a line that was already written above or below the cursor. There's only so much we can do with instruction tuning to avoid this so we're also employing rule based systems to guard against this failure case (via algorithms like Levenshtein edit distances).</p>
</li>
<li>
<p><strong>Truncating multi-line completions:</strong> Ever since the first implementation of multi-line completions, we identified the need to have language specific rules to know 1) when to trigger a multi-line completion and 2) when to cut it off.</p>
<p>LLMs are really good at continuing to produce output so when you ask it to fill out a function body, chances are that the LLM continues to implement another function and another function and another.... 🙃</p>
<p>To prevent this from happening, we use a combination of two techniques to find out exactly when we want to truncate the completion:</p>
<ul>
<li>
<p><strong>Indentation based:</strong> The idea for indentation based systems is to leverage code indentations to find out when the response leaves the current indentation level. The handy bit about this is that it's mostly language-agnostic, we only need to handle a few special cases like closing brackets to get usable results.</p>
</li>
<li>
<p><strong>Syntactical:</strong> I've already touched on our use of Tree-sitter above but this is another problem area where syntactic knowledge of the code provides us with a great opportunity. Insteading having to guess where a block ends, Tree-sitter can be used to be precise about it. We've seen great improvements in truncation quality by moving to this approach and will likely extend this to support more programming languages in the near future.</p>
</li>
</ul>
</li>
<li>
<p><strong>Estimating the relevance of a completion:</strong> Once we have a completion, it is handy to be able to score how relevant it is. We employ techniques for this mostly for multi-line completions at the moment (where we have more than one candidate completion available during post-processing and can use this information to select the most probable):</p>
<ul>
<li>
<p><strong>Using syntactic parsing:</strong> Leveraging Tree-sitter again, we can automatically devalue completions with syntax errors.</p>
</li>
<li>
<p><strong>Using probabilities returned by the LLM:</strong> One additional benefit of moving away from Claude Instant is that we now have access to the underlying probabilities that the LLM used to generate the completion. We can sum up probabilities to understand how certain the model is about a specific generation.</p>
</li>
</ul>
</li>
<li>
<p><strong>Filtering out obvious bad suggestions:</strong> While this is not as big of a problem anymore than it was in our early days, we also have a regex that highlights obvious bad completions. One such example is that in our initial Claude prompt, we'd sometimes get git diff style patches back.</p>
</li>
</ul>
<p>One overarching learning from this step is that we do not want to filter out too many completions. If we err on the side of not showing completions, our users have given us the feedback that the product does not work and it's really unclear for a user as to why. Hence, it's better to focus on generating relevant completions.</p>
<h2 id="data-data-data">Data, data, data…</h2>
<p>At Sourcegraph, we’re strong believers in the saying that “If you cannot measure it, you can't improve it” and as a result of this, analytics has always played a key role in how we improve Cody autocomplete. Over time, this system has become quite advanced as there’s a ton of additional bookkeeping needed to account for all of the VS Code APIs oddities and growing demands. Let’s dive into some specifics.</p>
<h3 id="what-metrics-do-we-track">What metrics do we track?</h3>
<ul>
<li>
<p><strong>Suggestions:</strong> At the heard of our telemetry is an event for every completion that was suggested to a user</p>
<ul>
<li>
<p>This includes the number of lines and characters of that completion in addition to the execution plan so we know what's being suggested. We also attach latency information and other debugging information to this event.</p>
</li>
<li>
<p>Every completion has a unique UUID so we can combine various data sources to get a more complete picture of the completion.</p>
</li>
<li>
<p>Knowing when VS Code decides to show a completion is unfortunately a hard problem (that is, <a href="https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.inlineCompletionsAdditions.d.ts">unless you are GitHub and can implement specific VS Code APIs that no one else can use in production</a>). So in order to understand when VS Code is deciding to show a completion we have a user-space implementation of their display criteria. Even then, another completion provider could be fast to respond to a completion, in which case we don't know if theirs or our completion is shown. We try to guard against this by logging if the user also has one of a list of known completion extensions enabled for their VS Code instance.</p>
</li>
<li>
<p>We also measure for how long a completion was visible on screen. This, however, is not precise as there are also no VS Code APIs so we approximate this based on a few VS Code specific heuristics.</p>
</li>
</ul>
</li>
<li>
<p><strong>Acceptances:</strong> A straightforward success criteria: If a user uses tab to insert a completion, this is a strong signal that a completion is indeed helpful.</p>
</li>
<li>
<p><strong>Partial acceptances:</strong> VS Code specifically has UI to only accept one word or one line from a completion. For a partial acceptance we also log how much (in number of characters) of the completion was added and we only log a partial acceptance when at least one full word of the completion was inserted.</p>
</li>
<li>
<p><strong>Completion retention:</strong> To better understand how useful the completions are, we also track how completions were changed over time after they were inserted. For this, we have bookkeeping that detects document changes to update the initial range that a completion was inserted at and then uses Levenshtein edit distances at specific polling intervals to capture how much of the initial completion is still present.</p>
</li>
</ul>
<p>Based on these events, we can compute our most important metric and that is <strong>completion acceptance rate</strong>. A metric that combines a lot of criteria like latency and quality into a single number.</p>
<p><img src="https://storage.googleapis.com/sourcegraph-assets/blog/the-lifecycle-of-a-code-ai-completion/Starcoder-CAR.png" alt="Starcoder completion acceptance rate"></p>
<p>The good news is that our users use Cody autocomplete a lot and that we can use this telemetry to get rapid feedback for improvements and use that to run A/B tests. To showcase how sensitive our logging is: We noticed a 50ms regression to latency in only a few hours of logging. In fact our logging was so advanced that we were able to provide valuable insight and fix performance regressions caused by Anthropic for a while.</p>
<p>By adding a lot of metadata from the previous steps to every autocomplete event, we're able to categorize requests into areas that work well and areas that need more improvement. The combination of Tree-sitter syntax information has been really helpful to identify issues in this category.</p>
<p>One such example is to reduce the frequency of completions on positions where we know that they are unhelpful. One example is if you're at the end of a line but the statement on that line is already complete:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-light"><code data-language="ts" data-theme="github-light"><span data-line=""><span>console.</span><span>log</span><span>();</span><span>|</span></span>
<span data-line=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// ^ showing an autocomplete at this point is likely not very useful 😅</span></span></code></pre></figure>
<h2 id="reliability">Reliability</h2>
<p>There is one area that, in my opinion, is often overlooked in software development: Reliability. More specifically, we need to ensure that our system not only works on paper, but that it also does not regress in functionality over time (this can happen by us pushing faulty updates or by the infrastructure failing us in production).</p>
<p>There are a few basics for reliability like unit testing and tracking production errors that every project should implement. Since we’re working with a very flaky environment though (the LLMs indeterministic nature), we’ve had to add a lot more safeguards though.</p>
<h3 id="autocomplete-tests">Autocomplete tests</h3>
<p>I won’t go into detail about this and I don’t think this is controversial anymore but automating your tests allows you to move faster. Most of the day to day improvements on heuristics outlined above rely on a large integration test suite that calls directly into the <code>provideInlineCompletionItems</code> API that VSCode uses. By running through the whole autocomplete architecture, we can write tests by defining a document and potential LLM responses and make assertions on all steps along the way. Here’s an example of such a test:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-light"><code data-language="ts" data-theme="github-light"><span data-line=""><span>it</span><span>(</span><span>'properly truncates multi-line responses for python'</span><span>, </span><span>async</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span data-line=""><span>    const</span><span> items</span><span> =</span><span> await</span><span> getInlineCompletionsInsertText</span><span>(</span></span>
<span data-line=""><span>        params</span><span>(</span></span>
<span data-line=""><span>            dedent</span><span>`</span></span>
<span data-line=""><span>                for i in range(11):</span></span>
<span data-line=""><span>                    if i % 2 == 0:</span></span>
<span data-line=""><span>                        █</span></span>
<span data-line=""><span>            `</span><span>,</span></span>
<span data-line=""><span>            [</span></span>
<span data-line=""><span>                completion</span><span>`</span></span>
<span data-line=""><span>                        ├print(i)</span></span>
<span data-line=""><span>                    elif i % 3 == 0:</span></span>
<span data-line=""><span>                        print(f"Multiple of 3: {i}")</span></span>
<span data-line=""><span>                    else:</span></span>
<span data-line=""><span>                        print(f"ODD {i}")</span></span>
<span data-line=""> </span>
<span data-line=""><span>                for i in range(12):</span></span>
<span data-line=""><span>                    print("unrelated")┤`</span><span>,</span></span>
<span data-line=""><span>            ],</span></span>
<span data-line=""><span>            {</span></span>
<span data-line=""><span>                languageId: </span><span>'python'</span><span>,</span></span>
<span data-line=""><span>            }</span></span>
<span data-line=""><span>        )</span></span>
<span data-line=""><span>    )</span></span>
<span data-line=""><span>    expect</span><span>(items[</span><span>0</span><span>]).</span><span>toMatchInlineSnapshot</span><span>(</span><span>`</span></span>
<span data-line=""><span>            "print(i)</span></span>
<span data-line=""><span>                elif i % 3 == 0:</span></span>
<span data-line=""><span>                    print(f</span><span>\\</span><span>"Multiple of 3: {i}</span><span>\\</span><span>")</span></span>
<span data-line=""><span>                else:</span></span>
<span data-line=""><span>                    print(f</span><span>\\</span><span>"ODD {i}</span><span>\\</span><span>")"</span></span>
<span data-line=""><span>        `</span><span>)</span></span>
<span data-line=""><span>})</span></span></code></pre></figure>
<p>In addition to a broad suite of integration tests, we also have E2E tests for our VS Code extensions that fires up a headless version of VS Code and instruments it via Playwright to ensure it’s working properly.</p>
<h3 id="llm-inference-test-suite">LLM inference test suite</h3>
<p>So we know that our implementation works for statically defined LLM responses. But how do we evaluate that changes we make actually have a positive impact on the overall user experience? One way of thinking about this is by looking at your production metric, but even in scenarios where you have lots of data, this usually results in a slow feedback cycle since you need to push a change, run an experiment and wait for it to conclude, evaluate it, and start again…</p>
<p>To improve our feedback cycles, we started very early to collect static examples of specific document states to automatically run our whole autocomplete stack against it. These only consisted of prefix and suffix pairs and were mostly evaluated manually using a small web UI that we built. It’s been super helpful to hook up new models, work on prompt changes, and tweak the generation parameters.</p>
<p>Over time, the manual evaluation became more and more work as we’ve added more examples and focusing only on one file was not a good replication of how a user works in their IDE. Testing LLMs created for code generation is a known problem and so we looked at existing solutions like the famous HumanEval tests. Those tests are usually also constrained to a single input file but they do have tests associated that can be run to validate the solution for correctness. These tests are great to validate the underlying LLM but they still do not capture the big picture of a user using their IDE to write code.</p>
<p>We knew we had to do more to build the best autocomplete experience and so we’ve recently overhauled our LLM inference test suite to document more and more cases of how code completion is used in the editor. Examples that encapsulate a whole workspace configuration like when you are writing a class, and then move to a different file and try to write a unit test for this class. We also added a system to run automated tests against the generated completion to measure its correctness. This allows us to test changes across the whole autocomplete stack quickly and without the need to deploy them, and get a sense of whether they improve the experience or not… And it’s only the beginning!</p>
<h2 id="summary">Summary</h2>
<p>In this post we looked at the lifecycle of a code AI completion for Cody. To summarize, every Cody completion goes through four steps:</p>
<ul>
<li>
<p><strong>Planning</strong> - analyzing the code context to determine the best approach for generating completions, such as using single vs. multi-line completions.</p>
</li>
<li>
<p><strong>Retrieval</strong> - finding relevant code examples from the codebase to provide the best possible context for the LLM.</p>
</li>
<li>
<p><strong>Generation</strong> - using the LLM to generate code completions based on the prompt and context provided.</p>
</li>
<li>
<p><strong>Post-processing</strong> - refining and filtering the raw AI-generated completions to deliver the most relevant suggestions.</p>
</li>
</ul>
<p>The goal of Cody is to provide high-quality completions that integrate seamlessly into a developer's workflow. Creating an effective code AI assistant requires the right context, prompt, and LLM. Through syntactic analysis, smart prompt engineering, proper LLM selection, and the right telemetry we are continuously iterating and improving code completion quality and acceptance rate for Cody. Latest numbers show Cody completion acceptance rate to be as high as 30%.</p>
<p>Curious to see Cody in action for yourself? <a href="https://cody.dev/">Get started for free today</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to found a company in Germany: 14 "easy" steps and lots of pain (341 pts)]]></title>
            <link>https://eidel.io/how-to-found-a-company-in-germany-14-easy-steps-and-lots-of-pain/</link>
            <guid>39959368</guid>
            <pubDate>Sun, 07 Apr 2024 08:51:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eidel.io/how-to-found-a-company-in-germany-14-easy-steps-and-lots-of-pain/">https://eidel.io/how-to-found-a-company-in-germany-14-easy-steps-and-lots-of-pain/</a>, See on <a href="https://news.ycombinator.com/item?id=39959368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>While Estonia has famously introduced its e-residency which enables future founders to incorporate their company in minutes, things are still quite different here in Good Old Germany.</p>



<p>Germany has a reputation for precision and adhering to rules.</p>



<p>I’m not sure how any of these aspects apply to the founding of companies though. It’s just damn slow, painful and sometimes borderline broken.</p>



<p>Regardless, I have performed this feat successfully in the past (twice!), so I thought I’d share my experience with you and hopefully save you a huge chunk of time. Also, the top-ranked Google search results for this topic are usually semi – content marketing posts of bookkeeping software or lawyers, and I thought a more rant-focused post was <em>exactly</em> the thing which the world needs right now.</p>



<p>The usual disclaimer applies: I’m a doctor, not a tax advisor. If in doubt, search out a real tax advisor (and/or doctor).</p>



<h2 id="step-0-reminding-yourself-that-you-can-do-it-15-minutes">Step 0: Reminding Yourself That You Can Do It (15 minutes)</h2>



<p>I was scared of founding a company for a very long time because I thought I didn’t know enough about it. Here’s what changed my mind and gave me self-confidence: I ate a Kebab in Berlin. While eating it and staring at people, I looked at the Kebab store and its owner dude. That made me realize that, yes, also this Kebab store owner dude seems to have gone through this process successfully. And with great success, too! Because he had customers (me) and was making revenue (my money).</p>



<p>So if a Kebab owner dude can navigate the painful bureaucratic system in Germany&nbsp;<em>while working grueling hours behind a Kebab grill</em>, I might be able to do so, too, sitting in my comfy home office while constantly overthinking things.</p>



<p>As a side note, probably another data point that migrants might be better entrepreneurs.</p>



<p>So! Step 0: Remember that you can do this, and everyone started out clueless. This is learning by doing.</p>



<h2 id="step-1-do-you-really-need-a-company">Step 1: Do You Really Need a Company?</h2>



<p>In simplified terms (I like those), you have two options if you want to launch a business in Germany: A sole proprietorship (Einzelunternehmen), or a limited liability company (GmbH).</p>



<p>In even more simplified terms, here’s a super-simplified “flowchart” of how to decide what to choose:</p>



<ul>
<li>You want to hire people right now, or you’re in a super risky industry where people get constantly sued: Choose a GmbH.</li>



<li>Everything else: Choose an Einzelunternehmen</li>
</ul>



<p>If you need more details, here’s a quick comparison table. I won’t go into much detail here though (let me know if you’re interested) because this article is about founding the GmbH. If the GmbH is not for you, you can congratulate yourself an avoiding a lot of complexity (good job!) and stop reading here.</p>



<figure><table><thead><tr><th>&nbsp;</th><th>Einzelunternehmen</th><th>GmbH</th></tr></thead><tbody><tr><td>Initial share capital required</td><td>0€</td><td>12.5k€</td></tr><tr><td>Founding costs</td><td>Nearly free</td><td>~900€</td></tr><tr><td>Yearly bookkeeping costs</td><td>Nearly free</td><td>1.000€ – 2.500€</td></tr><tr><td>Monthly bookkeeping costs</td><td>0-100€</td><td>100€ – 400€</td></tr><tr><td>Liability</td><td>You are personally liable</td><td>Liability limited to company</td></tr><tr><td>Yearly profits taxed at</td><td>Your tax rate (up to 45%)</td><td>Corporate tax rate (30%)</td></tr><tr><td>General complexity and pain</td><td>Low</td><td>High</td></tr></tbody></table></figure>



<p>Now, assuming you&nbsp;<em>still</em>&nbsp;want to move forward with the GmbH, let’s start (finally)!</p>



<h2 id="step-1-choose-a-name-but-dont-consult-the-only-people-youre-allowed-to-consult">Step 1: Choose a Name, But Don’t Consult The Only People You’re Allowed To Consult</h2>



<p>You might think that choosing a name for your company would be trivial. This might be true in startup-friendly and digitalized countries. Germany is neither.</p>



<p>The first and somewhat obvious aspect is that you can’t choose a name which already exists – so if someone else has founded Magic Software GmbH, you can’t choose that name. Look it up in the&nbsp;<a href="https://www.handelsregister.de/">company registry (Handelsregister) here</a>&nbsp;(be warned: Windows 95 vibes, and the website is down sometimes).</p>



<p>The second and much more confusing aspect is that, theoretically, your company name shouldn’t consist of words which are commonly used. I noticed this when I attempted to found OpenRegulatory GmbH – the fact that “Open” and “Regulatory” are both commonly used words makes this an invalid company name.</p>



<p>Yes, seriously!</p>



<p>So the obvious question is “what are commonly used words”. The answer is simple, pretty much any word in a dictionary. Yeah, great.</p>



<p>So, if you want to name your company Magic Software GmbH, that’s technically not possible, because “magic” and “software” are entries in a dictionary. However, if you name it Magic42 Software GmbH, that’s totally fine, because “magic42” is not in the dictionary.</p>



<p>And now the next obvious question is “why does this apparently not apply to other people”, because there are lots of companies with “invalid” names which apparently exist: ResearchGate GmbH, Delivery Hero GmbH, GetYourGuide Deutschland GmbH.</p>



<p>The answer is that the enforcement of this rule is very random, and it literally depends on whoever is in charge of entering your company name into the registry. Different people have different interpretations of this rule, apparently.</p>



<p>So can you get some sort of consultation whether your planned company name is okay? Yes, you can! There’s one institution, called the IHK, which offers this service.</p>



<p>Every company (includes yours in the future) has to become a member of the IHK and pay its fees, there’s no way to opt out. If you think this sounds really shady, you might be on to something! The IHK prides itself on supporting businesses etc., but then again, you can’t opt out, so I guess you just have to accept that they exist and do mostly-average-sometimes-very-mediocre work. The likelihood that you’ll ever get in touch with them and benefit from their work is around 1%.</p>



<p>.. And that 1% is right now, because the IHK is the only institution which has the authority to tell you whether your planned company name might be acceptable.</p>



<p>But – stop right there – don’t ask them, because their interpretation will be the most unpragmatic interpretation you can imagine. Here’s my abbreviated dialogue with them when attempting to found OpenRegulatory GmbH:</p>



<p><strong>Oliver:</strong>&nbsp;I want to found a company called OpenRegulatory GmbH. Does that work?<br><strong>IHK:</strong>&nbsp;No. Because the name consists of common words.<br><strong>O:</strong>&nbsp;But what about ToolTime GmbH, JustWatch GmbH, RIDE GmbH, BioWink GmbH, etc.?<br><strong>IHK:</strong>&nbsp;That would depend on whoever entered it into the registry that, it’s subjective. You could add a number or a phantasy word (yes, seriously) to your name.<br><strong>O:</strong>&nbsp;What about spaces? OpenRegulatory123 vs. OpenRegulatory 123?<br><strong>IHK:</strong>&nbsp;Spaces don’t matter.<br><strong>O:</strong>&nbsp;How about OpenRegulatory.com GmbH?<br><strong>IHK:</strong>&nbsp;Domains are not allowed.</p>



<p>And that, dear reader, is the story how I settled on “OpenReg GmbH”. A daily and painful reminder of arbitrary German bureaucracy.</p>



<p>Don’t ask the IHK about approval for your company name. Just go for it.</p>



<p>By the way, what happens if the person who enters it in the registry declines to enter your chosen company name? Then you have to repeat step 2 again and you’ll lose a few more weeks, see below.</p>



<h2 id="step-2-find-a-notary-public-who-picks-up-the-phone">Step 2: Find a Notary Public Who Picks Up The Phone</h2>



<p>Your next step is to actually incorporate your company. You might think there’s an online form for that. No, not in Germany. You have to make an in-person appointment with a notary public – that’s a person who reads out documents for a living and ensures it’s actually you who appears at their place, not e.g. a con artist who secretly incorporates companies under other people’s names (beware).</p>



<p>So time to pick up your phone, open Google Maps and search for a notary public (Notar) close to you.</p>



<p>I’ve collected some criteria how to select a suitable Notar:</p>



<ul>
<li>They pick up the phone.</li>



<li>They actually are willing to give you an appointment and are not booked out.</li>
</ul>



<p>High, expectations, I know. Once you’ve found a suitable Notar, they might send you an email with one of those crappy PDF forms to fill out (yeah, those without text fields). You’ll enter your address, company name (how you chose a good one), articles of association and share capital. Talking about those two things..</p>



<h3 id="step-2a-articles-of-association-use-the-musterprotokoll">Step 2a: Articles of Association: Use The Musterprotokoll</h3>



<p>Your articles of association (Gesellschaftervertrag) contain the rules of your company, e.g. who owns how many shares and who can make decisions.</p>



<p>You don’t have to draft one yourself though: You can use the official German template (Musterprotokoll) which covers the most simple cases, e.g. if you’re a solo founder.</p>



<p>In all other cases, e.g. if you have Venture Capital investors (ugh), you might have to use whatever your investors want and/or engage a lawyer to draft one for you which opens yet another gigantic can of worms. I’ll just skip that possibility here.</p>



<p>Instead, the magic word is Musterprotokoll.</p>







<p>I skipped over this before in the spirit of simplification, but will have to give you a brief summary on share capital now. In short, a GmbH needs 25k€ initial share capital, so that’s the sum you’ll be paying into the company bank account from your private savings. Luckily, the money doesn’t disappear – you can use it within the GmbH to pay bills (most likely the ~1k€ notary and incorporation costs which will become due soon!). But you can’t use the 25k€ privately privately anymore as it’s part of the company now.</p>



<p>There are two small hacks here which I’ll just briefly mention:</p>



<ul>
<li>You could either only pay in half of it (12.5k€) and transfer the other 12.5k€ some time later;</li>



<li>Or, you could&nbsp;<em>not</em>&nbsp;found a GmbH but a UG instead which can have a share capital of less than 25k€, e.g. 4k€ (choose a sum which is above the initial incorporation costs because otherwise your company will be bankrupt after a few days – not a good idea).</li>
</ul>



<p>For simplicity’s sake, I assume you’re rich and have 25k€ lying around which you’ll use to found your GmbH. I did found a UG in the past, too, with 4k€, so that’s your alternative.</p>



<h2 id="step-3-go-to-the-notary-public-notar">Step 3: Finally Go To The Notary Public (Notar)</h2>



<p>Now, let’s hope that you get an appointment at the Notar within the next two weeks or so. Let’s also hope you spent the time in a productive manner and not just sitting around and watching YouTube videos like I did.</p>



<p>Go to the Notar. Don’t forget your ID or passport because that’s their foolproof way of identifying you and ensuring you are you and not a mysterious con man setting up companies in your name.</p>



<p>The Notar will read out your company’s articles of association to you. Yes, they will read it out even though both of you already know it. And yes, even if you use the official German template, they will read it out.</p>



<p>It might seem ridiculous, two grown-up people sitting in a room and reading out a company incorporation template provided by the state. If you’re currently learning German, you can treat it like a listening exercise (a very expensive one).</p>



<p>After that’s done, you will sign various documents.</p>



<p>And that’s it! Your company is incorporated.. kind of. But you are not done at all and many days still lie ahead on your journey, young hobbit. On to the next step.</p>



<h2 id="step-4-receive-documents-from-notar">Step 4: Receive Documents From Notar</h2>



<p>The Notar will now finalize the documents you just signed and send them to you. If they’re super digital, they’ll send you scans of the paper sheets you signed via email. And if they’re super fast, they’ll send them on the same day. Remember: These are the documents you just signed, so the critical path here is about one of the Notar assistants putting your just-signed documents in a scanner and sending them to you via email.</p>



<p>They might forget about it though, or they might only send it via snail mail.</p>



<h2 id="step-5-find-a-bank-account">Step 5: Find a Bank Account</h2>



<p>Your company is now in the weird situation of being partially incorporated. This is resembled by the fact that you refer to your company now as “Magic Software GmbH i.G.” where the i.G. means it’s currently being incorporated (in Gründung). In theory, you could now sign contracts and send out invoices with this weird name, but no one does that, because it seems a bit awkward.</p>



<p>What you should do instead is set up a bank account, because you now have a chicken-and-egg situation:</p>



<p><strong>Situation:</strong> Your Notar needs proof that your company has a bank account and the share capital (25k€) has been paid in to finish the official company registration in the Handelsregister (= the Windows 95 website).</p>



<p><strong>Problem:</strong> To open a bank account, your company needs to be in the Handelsregister.</p>



<p>The easy solution to this is that most German banks allow you to open a company bank account <em>even though you’re not in the Handelsregister yet</em>. Just be sure to keep the “i.G.” thing behind your company name for the time being.</p>



<p>Two important points on how to save time here!</p>



<ol>
<li>International “startup” banks like Wise generally don’t support this, so don’t even try! You’ll lose multiple days going back and forth with their support, like I did. You can open a Wise bank account later when you’re properly incorporated.</li>



<li>All the old-school banks in Germany (those with brick-and-mortar branches) generally are super slow and will take multiple weeks to open your bank account.</li>
</ol>



<p>Instead, choose a startup bank which openly supports German companies which are currently being founded. In the past, I used Penta for that but they got bought by Qonto. From what I hear, the same should now be possible with Qonto.</p>



<p>If they’re fast, you can get all of this done in a day (including the awkward KYC video call with a friendly person in an Eastern European country).</p>







<p>The next step, for a change, doesn’t involve waiting for a slow institution and is instead completely up to you: Pay in your share capital – those are the 25k€ I mentioned earlier. Your notary might have given you some hints on which text they’d like to see in the transaction description, e.g. “Stammeinlage Gesellschafter &lt;your name&gt;”.</p>



<h2 id="step-7-forward-bank-account-paperwork-to-notary">Step 7: Forward Bank Account Paperwork To Notary</h2>



<p>Once that’s done and the share capital has arrived in your fresh new company bank account, you need two confirmations which you’ll send to your notary:</p>



<ul>
<li>A confirmation that your company now has a bank account at whatever bank you chose earlier.</li>



<li>The most recent account statement which shows that you paid in the share capital of 25k€.</li>
</ul>



<p>Send those PDFs to your notary via email.</p>



<h2 id="step-8-waiting-for-the-notary-and-registration-in-the-commercial-registry-handelsregister">Step 8: Waiting For The Notary And Registration In The Commercial Registry (Handelsregister)</h2>



<p>With that confirmation in hand, your notary will now begin a highly sophisticated digital manoeuvre: They will perform an “XML upload” (yes) to the German commercial registry (Handelsregister) to finalize your company registration. Ah, and they will charge you for that, too, adding to your total incorporation costs of approximately 1k€ (remember your listening exercise?).</p>



<p>Now begins another waiting period, because this upload has to be manually approved by those famous people who approve company names (remember?. Not the IHK people (phew), but the actual people who work at the registry. As noted above, these people have highly subjective and varying ideas about company names, so the big question now is whether your name gets rejected if you used commonly-used words.</p>



<p>This usually takes another 1-2 weeks, but it might take longer based on how busy or understaffed the commercial registry is.</p>



<p>Two possible outcomes:</p>



<ul>
<li>Your name gets rejected. Sad dog face. Move back to step 2 and lose multiple weeks. Yes, seriously.</li>



<li>Your name gets approved. You will receive a “preliminary invoice” from the commercial registry.</li>
</ul>



<h2>Step 9: Pay Preliminary Invoice</h2>



<p>The Handelsregister will send you a preliminary invoice (150€ for me at the time). You have to pay it so that your registration moves forward.</p>



<p>You can use your company bank account (with the 25k€ in it) for that, because these are, well, company-operational costs.</p>



<p>If you want to save a few days, you can in theory go there in person and pay in cash (yes, seriously). Not sure if it’s worth it though.</p>



<h2 id="step-9-download-confirmation-from-commercial-registry">Step 10: Download Confirmation From Commercial Registry</h2>



<p>The waiting game now entails to check the&nbsp;<a href="https://www.handelsregister.de/">Handelsregister</a>&nbsp;daily by entering your company name as a search term. After 1-2 weeks, it should be listed and you can download a PDF confirmation of its listing which is your official proof that your company exists!</p>



<p>Your notary will likely also be checking this and they’ll also send you a confirmation at some stage.</p>



<p>So are you done? Can you write invoices now? No. You still need your tax ID – to be exact, two tax IDs. But before we get those, an important word of warning.</p>



<h2>Step 11: Danger: Brace For Handelsregister Spam</h2>



<p>This is crazy and might make you feel like you’re in a developing country. In short, there are very shady organizations which monitor new entries in the Handelsregister, and they will send your company invoices which <em>look</em> like invoices of the Handelsregister, but they are not (remember, you already paid the 150€). They call these services “registry services” or so and attempt to exploit the confusion of founders like you (can’t blame you, the whole process is confusing).</p>



<p>Ignore them and don’t pay them.</p>



<p>Onwards to the two tax IDs.</p>



<h2 id="step-10-filling-out-the-fragebogen-zur-steuerlichen-erfassung">Step 12: Filling Out The Fragebogen Zur Steuerlichen Erfassung</h2>



<p>Take a deep breath, because this is just painful. Yes, your company is incorporated, but until now, the tax authorities don’t know much about it. Wait, you thought that your data was transmitted there automatically? Think again.</p>



<p>It’s now up to you to fix that by filling out a form called “Fragebogen zur steuerlichen Erfassung”. It can best be translated as “questionnaire for tax registration”.</p>



<p>It is 10 pages long. Yes, 10 pages! Even I as a German thought some of the questions were quite tricky because they were written in bureaucratic German. Here’s an example:</p>



<blockquote>
<p>Die Gesellschaft ist in nur einem EU-Mitgliedsstaat ansässig und der Gesamtbetrag – ohne Umsatzsteuer – der oben bezeichneten Umsätze an in anderen EU-Mitgliedstaaten außerhalb des Ansässigkeitsstaats ansässige Nichtunternehmer überschreitet im laufenden Kalenderjahr nicht 10.000 € und hat dies auch im vorangegangenen Kalenderjahr nicht getan.</p>
</blockquote>



<p>DeepL translation attempt:</p>



<blockquote>
<p>The company is domiciled in only one EU Member State and the total amount – excluding VAT – of the above-mentioned sales to non-entrepreneurs domiciled in other EU Member States outside the country of domicile does not exceed € 10,000 in the current calendar year and did not do so in the previous calendar year.</p>
</blockquote>



<p>Good luck. Here are some quick pointers:</p>



<ul>
<li>Most important: Tick the checkbox to request a EU VAT ID – you’ll need that for registering with online payment processors (Stripe, Paddle, etc.), and for writing invoices for customers in the EU (outside of Germany).</li>



<li>Estimate your revenue at 0€ for now unless you have real reason to believe otherwise (do you have paying customers lined up already? probably not). That way, you only have to pay corporate tax at the end of the year based on your actual profit (vs. having to prepay tax every quarter).</li>



<li>Tick “Bargründung” – you founded your company by paying in 25k€ in cash, not by putting in random items (e.g. property).</li>



<li>Tick “Sollversteuerung” which means you pay taxes based on the&nbsp;<em>invoices</em>&nbsp;you send and receive, not based on&nbsp;<em>money</em>&nbsp;you actually send and receive (requirement for GmbH, I think).</li>
</ul>



<p>Still, the form is a pain. Good luck. The good news is that if you screw it up, your tax advisor can likely talk to the authorities and fix it. Talking about tax advisors..</p>



<h2 id="step-10a-finding-a-tax-advisor-and-bookkeeper">Step 12a: Finding a Tax Advisor And Bookkeeper</h2>



<p>If you haven’t noticed yet, dealing with the financial and tax authorities in Germany is a huge pain – not because they’re evil people (they’re often actually really nice!), but because of all the clunky systems and processes around it. Communication with them is often via physical snail mail. Some forms can be submitted electronically, others can not.</p>



<p>Additionally, your company is required to do monthly bookkeeping to, among other things, calculate the total VAT you charged your customers and transferring that to the authorities. I still haven’t fully gotten into how the hell all of this bookkeeping works, and neither should you – you have better things to do, like running your company and generating revenue.</p>



<p>So my somewhat unfortunate recommendation is to look for a tax advisor who also offers bookkeeping and outsource everything to them. Unfortunate, because I feel like this creates yet another middleman who profits from the complexity of the system.</p>



<p>The benefit is that you only have to upload your invoice PDFs every month to whatever broken web portal your tax advisor offers, and they’ll take care of the rest, including handling questions of the tax authorities if they come up.</p>



<p>The costs for this differ, but a good rule is to select a tax advisor who charges you a flat fee based on your company revenue. Other tax advisors charge 100-150€ / hr based on the hours they spend, not recommended.</p>



<p>As a side note, I’m currently re-evaluating this – some “new generation” tax advisors offer more innovative approaches of enabling you to do your own bookkeeping in fancy SaaS software, while they only review it.</p>



<h2 id="step-11-receiving-your-tax-id">Step 13: Receiving Your Tax ID</h2>



<p>After your tax form (step 10) has been processed, you should receive your company’s German tax ID in the snail mail. This tax ID will finally enable you to write invoices. It usually has the format of xx/xxx/xxxxx where x is a number. Add those to your invoices (“Steuernummer: xx/xxx/xxxxx”) and you’re now ready to create invoices!</p>



<p>Don’t confuse this with the EU VAT ID though which usually arrives slightly later.</p>



<h2 id="step-12-receiving-your-vat-id">Step 14: Receiving Your VAT ID</h2>



<p>A few days later, your EU VAT ID should arrive, also via snail mail. Its format is usually DEXXXXXXXXX where X is a number. You should add it to all invoices because it’s required for non-German EU customers. It’s also required to sign up for payment processors like Stripe and Paddle.</p>



<h2 id="done">Done!</h2>



<p>You’re done already! Only 12 steps and around 6 weeks in total, and your German GmbH is set up.</p>



<p>Now you’re ready for business! Hopefully your prospective customers haven’t disappeared while you were busy setting up your GmbH.</p>



<h2 id="optional-steps">Optional Optimizations</h2>



<p>Here are a few optional ideas about what to do next:</p>



<h3>Switch Banks?</h3>



<p>You could move to a bank which is better and/or cheaper than your current bank, because now your search is no longer limited to banks which allow partially-incorporated companies. I use Wise as their offering is pretty good. That being said, they have a weird habit of automatically deactivating accounts from time to time and their support is sometimes ridiculously bad, so I can’t fully recommend them.</p>



<p>In a very ironic turn of events, you could even consider moving to a brick-and-mortar bank now! You’re no longer in a rush as your company is already incorporated and has a bank account, and the benefit of brick-and-mortar banks is that they usually don’t lock you out of your account randomly every now and then. If you’re undecided, that’s also fine, because a company can have multiple bank accounts. Why not have both?</p>



<h3>Liquidity Planning</h3>



<p><strong>Liquidity (money) planning:</strong>&nbsp;You’ve got 25k€ in your bank account, but that might not last you very long. At the very least, all the fees including the notary will reduce that by around 1k€, and if you pay yourself a salary or hire someone, money will disappear fast – an average software engineer in Germany can easily cost you 6-8k€ / month. You could transfer more money to your company by giving it a loan – make sure to create all the paperwork for that though, i.e. create a loan contract (yes, with yourself) before you transfer the money.</p>



<h2 id="tax-considerations">Tax Considerations</h2>



<p>Some quick thoughts about taxes.</p>



<h3>Corporate Taxes &amp; VAT In Germany</h3>



<p><strong>Germany taxes company profits at approximately 30%.</strong>&nbsp;In simplified terms this means that, at the end of the year, whatever money is left over, gets taxed at 30% – so make sure to not spend it (nervous chuckle). Here’s an example: You receive 200k€ in money over the course of a year (revenue) and you spend 100k€ on costs like salaries and hardware. Your profit at the end of the year is 100k€ which gets taxed at 30%, so you have to pay 30k€ in taxes and only keep the remaining 70k€. Quite a lot of taxes here.</p>



<p><strong>You charge your customers VAT and pay it at the end of the month:</strong>&nbsp;On your invoices, you add VAT, e.g. when selling to German customers (19% currently). You don’t get to keep that though. Instead, you transfer the total VAT of each month to the tax authorities – so make sure you don’t spend it.<br>Things get more complex if you sell stuff to customers within the EU, but outside of Germany. I’ll skip that complexity here and maybe write about it another time.</p>



<h2>Holding Company?</h2>



<p>Talking about taxes, there’s one rather huge optimization you can do: Instead of owning your company yourself, you can create <em>another</em> company first which holds it.</p>



<p>So, specifically, the first setup I described above would look like this, let’s call it <strong>Scenario 1 (direct ownership)</strong>:</p>



<p>You (private person) own 100% of shares of Magic Software GmbH.</p>



<p>Instead, you could do this holding thing, let’s call it <strong>Scenario 2 (holding)</strong>:</p>



<p>You (private person) own 100% of shares of Magic Holding GmbH, and Magic Holding GmbH owns 100% of shares of Magic Software GmbH.</p>



<p>What’s the benefit? Let’s look at how things are taxed with the 100k€ profit I mentioned earlier.</p>



<h3>Scenario 1: Direct Ownership</h3>



<p>Magic Software GmbH makes 100k€ in profits. Those get taxed at 30%, so 70k€ remain. If you want to pay out these profits to yourself (as you own 100% of the company), you’ll have to pay an <em>additional </em>~26% capital gains tax on the 70k€, so you end up with around 51k€.</p>



<h3>Scenario 2: Holding</h3>



<p>Magic Software GmbH makes 100k€ in profits. Those get taxed at 30%, so 70k€ remain. If you want to pay those out, the first step is now to pay it out to Magic Holding GmbH, because <em>that</em> company is the owner of Magic Software GmbH, not you.</p>



<p>And here’s the benefit: In simplified terms, you don’t pay taxes then. So you take 70k€ out of Magic Software GmbH and pay it out into Magic Holding GmbH and still have 70k€.</p>



<p>In the Magic Holding GmbH, you could e.g. invest it in ETFs and leave it there until you really need it.</p>



<p>And then, if you pay it out from Magic Holding GmbH to you personally, then yes, you pay ~26% capital gains tax – but the huge benefit here is that it could accrue investment returns in the meantime!</p>



<p>There are also some pretty huge other optimizations here – e.g. you couldn’t even take it out of your holding company and instead <em>buy stuff within your holding company</em>, like property or airplanes (yes, seriously) and then <em>rent that stuff out to you as a person</em>. But this, dear reader, is pretty much exactly where my knowledge ends.</p>



<p>I did end up going for the holding thing because the benefits really are great, and if you haven’t noticed by now, if there’s one thing Germans love, it’s optimizing their taxes. The drawback of holding construct is that you have to found your holding company <em>first</em>, so you have to go through all of this twice.</p>



<p>Hope I saved you some time. Good luck!</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>