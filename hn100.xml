<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 16 Aug 2023 02:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How Is LLaMa.cpp Possible? (232 pts)]]></title>
            <link>https://finbarr.ca/how-is-llama-cpp-possible/</link>
            <guid>37140013</guid>
            <pubDate>Tue, 15 Aug 2023 22:18:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finbarr.ca/how-is-llama-cpp-possible/">https://finbarr.ca/how-is-llama-cpp-possible/</a>, See on <a href="https://news.ycombinator.com/item?id=37140013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
    
    
    
    <p><em>If you want to read more of my writing, I have a <a href="https://finbarrtimbers.substack.com/">Substack</a>. Articles will be posted simultaneously to both places.</em></p>

<p>Recently, a <a href="https://github.com/ggerganov/llama.cpp">project</a> rewrote the <a href="https://github.com/facebookresearch/llama">LLaMa inference code</a> in raw C++. With some optimizations and quantizing the weights, this allows running a LLM locally on a wild variety of hardware:</p>

<ul>
  <li>On a <a href="https://twitter.com/rgerganov/status/1635604465603473408">Pixel5</a>, you can run the 7B parameter model at 1 tokens/s.</li>
  <li>On a <a href="https://simonwillison.net/2023/Mar/11/llama/">M2 Macbook Pro</a>, you can get ~16 tokens/s with the 7B parameter model</li>
  <li>You can <a href="https://twitter.com/miolini/status/1634982361757790209">even run the 7B model on a 4GB RAM Raspberry Pi</a>, albeit at 0.1 tokens/s.</li>
</ul>

<p>If you are like me, you saw this and thought: What? How is this possible? Don‚Äôt large models require expensive GPUs? I took my confusion and dove into the math surrounding inference requirements to understand the constraints we‚Äôre dealing with.</p>

<p>Let‚Äôs start with GPUs. GPUs have two main benefits for deep learning:</p>

<ol>
  <li>They have a large amount of memory bandwidth (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100</a>: 1935 GB/s, <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf">4090</a>: 1008 GB/s)</li>
  <li>They have a large amount of compute (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100</a>: 312 TFLOPS of FP16, <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf">4090</a>: 82.6 TFLOPS of FP16)</li>
</ol>

<p>When we talk about memory bandwidth, we‚Äôre talking about how long it takes to move things from the HBM memory (i.e. the RAM) into the on-chip memory. To actually do math with the GPU, we need to move the matrices in question into the on-chip memory, which is quite small (40MB on an A100, compared to 40-80GB of RAM). Note that the memory bandwidth is ~2 orders of magnitude smaller than the compute performance‚Äî this will matter later, as the memory bandwidth tends to be the bottleneck for inference.</p>

<p>What does this mean in the context of serving LLaMa? Let‚Äôs start with some <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">inference arithmetic</a>. We can do some rough calculations on the inference performance of a LLM using <a href="https://kipp.ly/blog/transformer-param-count/">Kipply‚Äôs article</a><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. First, some notation on the dimensions of the model:</p>

<ul>
  <li>The \(Q\), \(K\), and \(V\) weight matrices are all shape [ \(d_{\text{model}}\), \(d_{\text{head}}\)], and we have \(n_{\text{heads}}\) of them per layer; the attention output matrix has the same shape, for a total of  \(4 \times\) [ \(d_{\text{model}}\), \(n_{\text{heads}} \cdot d_{\text{head}}\)]. By convention, GPT-style networks have \(d_{\text{head}} \cdot n_{\text{heads}} = d_{\text{model}}\).</li>
  <li>The MLP has two weight matrices, of shape [ \(d_{\text{model}}\), \(4 \cdot d_{\text{model}}\)] and [ \(4\cdot d_{\text{model}}\), \(d_{\text{model}}\)]</li>
  <li>The embeddings matrix is of size [ \(d_{\text{vocab}}\), \(d_{\text{model}}\)].</li>
</ul>

<p>This gives us a handy equation for the number of parameters in a GPT-style model:<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p><p>

\[P = n_{\text{blocks}} \left( 4 \cdot d_{\text{model}}^2 + 2 \cdot 4 \cdot d_{\text{model}}^2\right) + n_{\text{vocab}} \cdot d_{\text{model}}\]

</p><p>For the duration of the post, I‚Äôm going to focus on the case where we‚Äôre running a ChatGPT style service locally, which is what LLaMa.cpp does, letting me assume a batch size of 1.</p>

<p>For efficient inference, the KV cache has to be stored in memory; the KV cache requires storing the KV values for every layer, which is equal to storing:</p><p>

\[n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

</p><p>I use \(n_{\text{bytes}}\) here to indicate the number of bytes per param; for float32s, this is 4, for float16s, this is 2, etc. The 2 in the middle is because we have to store one set of weights for the K values, and one for the Vs.</p>

<p>Given a model with n layers, the total memory for the KV cache is:</p><p>

\[n_{\text{blocks}} \cdot n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

</p><p>In addition to storing the KV cache in memory, we also need to store the weights themselves in memory; this requires \(n_{\text{bytes}} \cdot P\) bytes.</p>

<p><img src="https://finbarr.ca/static/images/llama-memory-weights.png" alt="Screenshot of table showing the memory required for LLaMa weights"></p>

<p>This is the advantage of quantization. By using less precision, we can radically decrease the amount of memory needed to store our models in memory. Note that, with int4 precision, <em>all of these models fit into memory on an A100</em> (which is the standard datacenter GPU right now), and all of them, except for the biggest model, fit into memory on high-end consumer GPUs (3090s/4090s, which have 24GB of RAM).</p>

<p>It takes approximately \(2P\) FLOPS to run inference on our model for a single token, because we are doing a bunch of matmuls with a total of \(P\) parameters, and multiplying a matrix of size \((m, n)\) with a vector of size \((n,)\) has a cost of \(2mn\).<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>With all that math out of the way, let‚Äôs calculate the requirements for running inference with LLaMa. The main requirements when it comes to sampling are:</p>

<ol>
  <li>Keep the KV cache in memory, in addition to all the parameters.</li>
  <li>Read all the weights from HBM into the on-chip memory. Because we sample auto-regressively, we have to repeat this for each token we sample.</li>
  <li>Do the actual matmuls to calculate the output of our network.</li>
</ol>

<p>The latency is the maximum of either the compute or the memory latency, as reading parameters into on-chip memory happens asynchronously in all modern tensor programming libraries. As a result, we write:</p><p>

\[\begin{align*}
\text{latency}_\text{model} &amp;= \text{max}(\text{latency}_\text{compute}, \text{latency}_\text{memory})\\
\text{latency}_\text{memory} &amp;= \dfrac{2 \cdot P \cdot n_{\text{bytes}}\cdot B}{n_{\text{memory bandwidth}}},\\
\text{latency}_\text{compute} &amp;= \dfrac{2 \cdot P}{n_{\text{flops}}},
\end{align*}\]

</p><p>where \(B\) is the batch size. As \(n_{\text{memory bandwidth}} = 1.935e12\), and  \(n_{\text{flops}} = 3.12e14,\) as long as the batch size is less than 161, the model is memory-bound.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>With a batch size of 1, this is the same equation, as on most hardware (e.g. Nvidia GPUs), there is a linear speedup as you decrease the precision (you get twice the FLOPS when using fp16 vs fp32, which doubles again as you go to int8, and doubles once more as you go to int4s).</p>

<p>As LLaMa.cpp uses int4s, the RAM requirements are reduced to 1.33GB of memory for the KV cache, and 16.25GB of VRAM for the model parameters. That‚Äôs pretty good!</p>

<p>As the memory bandwidth is almost always<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup> much smaller than the number of FLOPS, memory bandwidth is the binding constraint.</p>

<p><img src="https://finbarr.ca/static/images/llama-inference-times.png" alt="Screenshot fo table showing the inference times to run the varying LLaMa models with varying precision levels on an A100"></p>

<h2 id="running-llama-on-an-a100">Running LLaMa on an A100</h2>

<p>On an A100 (80GB PCIe), the memory bandwidth is 1935GB/s. The int4 compute is 1248 TOPS. As such, the model is (heavily) memory-bound. We should expect to see inferences as given in the table; roughly 30 tokens/s with the 65B model, and 277 tokens/s with the 7B model.</p>

<h2 id="running-llama-on-a-m1-macbook-air">Running LLaMa on a M1 Macbook Air</h2>

<p>The M1 GPU has a bandwidth of <a href="https://www.macworld.com/article/783678/m2-vs-m1-chip-performance-graphics-ram.html">68.25 GB/s</a>, while the M1 GPU can do up to <a href="https://tlkh.dev/benchmarking-the-apple-m1-max#heading-gpu-matrix-multiplication-gemm-performance">5.5 TFLOPS</a> of fp16 compute. As such, we should expect a ceiling of ~1 tokens/s for sampling from the 65B model with int4s, and 10 tokens/s with the 7B model.</p>

<p>As the M2 Pro has 200 GB/s of bandwidth, and the M2 Max has 400 GB/s of bandwidth, we should expect massive improvements with them, going up to 6 tokens/s with the M2 Max with the 65B model. That‚Äôs pretty darn good for a laptop.</p>

<h2 id="running-llama-on-a-raspberry-pi-4">Running LLaMa on a Raspberry Pi 4</h2>

<p>A Raspberry Pi 4 has <a href="https://web.eece.maine.edu/~vweaver/group/green_machines.html">13.5 GFLOPS of compute</a>, and <a href="https://forums.raspberrypi.com/viewtopic.php?t=281183">~4GB/s of memory bandwidth</a>. Given this, we‚Äôd expect to see ~2 tokens/s with the 7B model if it was memory bound. Given that we‚Äôre currently seeing ~0.1 tokens/s, I suspect we‚Äôre actually compute-bound (although this is a stab in the dark‚Äî I can‚Äôt find enough information about the specs for a Raspberry Pi to determine this with any precision).</p>

<h2 id="summary">Summary</h2>

<p>Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them <em>much</em> easier to serve‚Äî like quantization! This is yet another reason why distillation, or just <a href="https://finbarr.ca/llms-not-trained-enough/">training smaller models for longer</a>, is really important.</p>

<p><em>Note: I‚Äôm not an expert in CUDA, so I probably have errors in my math. If so, please shoot me an <a href="mailto:finbarrtimbers@gmail.com">email</a> and let me know- I‚Äôd love to hear from you so I can learn more about how this works and update this post.</em></p>

<p>Resources on transformer inference performance:</p>

<ul>
  <li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization</a></li>
  <li><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Transformer inference arithmetic</a></li>
  <li><a href="https://kipp.ly/blog/transformer-param-count/">LLM parameter counting</a></li>
  <li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers</a></li>
</ul>

<p><em>Thank you to <a href="https://twitter.com/kaushikpatnaik?lang=en">Kaushik Patnaik</a>, <a href="https://twitter.com/arthurallshire">Arthur Allshire</a>, <a href="https://twitter.com/stanislavfort">Stanislav Fort</a>, and <a href="https://twitter.com/banburismus_">Tom McGrath</a> for reading early drafts of this.</em></p>



    
    <p>PS if you want to read more of my writing, subscribe to my <a href="https://finbarrtimbers.substack.com/">Substack</a>.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You're a cyclist who was just struck by a car driver. Why it was your fault (105 pts)]]></title>
            <link>https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault</link>
            <guid>37139980</guid>
            <pubDate>Tue, 15 Aug 2023 22:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault">https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault</a>, See on <a href="https://news.ycombinator.com/item?id=37139980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="o-wrapper">
    <main>
        <header>
    <div>
      <div>
          <p><span><img role="none" src="https://edge-assets.mcsw.net/assets/search-614fbdcc4e71f0730ad039e484ec78a1085f24294fa0b4514da70b0a930b2dce.svg"></span></p>        </div>
      <div>
        <ul>
          <li><a href="https://www.mcsweeneys.net/">Internet Tendency</a></li>
          <li><a href="https://store.mcsweeneys.net/">The Store</a></li>
          <li><a href="https://store.mcsweeneys.net/t/categories/books">Books Division</a></li>
          <li><a href="https://store.mcsweeneys.net/t/categories/timothy-mcsweeneys-quarterly-concern">Quarterly Concern</a></li>
          <li><a href="https://thebeliever.net/">The Believer</a></li>
          <li><a href="https://www.mcsweeneys.net/donate">Donate</a></li>
        </ul>
      </div>
    </div>
    
  </header>


      
  <div>
    <h6>The Believer has returned</h6>
    
  </div>


      
<article>
    
   
    <div>
      <p><strong>You were riding during rush hour.</strong><br>
Why were you riding then? There are way too many cars on the road. If you were commuting, you should have contacted your boss and politely asked to work from 3:00 a.m. to 11:00 a.m. instead.</p>
<p><strong>You were riding at night or in the early morning.</strong><br>
There‚Äôs no way drivers can see you. Remember: if you‚Äôre one of those people who rides bikes because it keeps the mental darkness at bay, the best time to do so is in the middle of the workday.</p>
<p><strong>You were riding in the middle of the workday.</strong><br>
The only people who should ride their bikes during the workday are bike messengers, who I also dislike. They weave, they bob‚Äîit‚Äôs inappropriate. Bike messengers need to do what drivers do: go straight, get pissed off, and hate everyone.</p>
<p><strong>You were riding on a back road.</strong><br>
Those roads are narrow and have a lot of twists and turns. There are hardly any cyclists on them. Drivers weren‚Äôt expecting you!</p>
<p><strong>You were riding on a main road.</strong><br>
Again, too much traffic. We‚Äôve been over this.</p>
<p><strong>You were riding in the morning, or at night, or on a quiet road, or a main road.</strong><br>
Do I honestly have to spell it out for you? The only appropriate time and place to ride a bike is a time beyond time and a place beyond place, where the space-time continuum is bent so strangely you are both everywhere and nowhere, eternal and nonexistent. You must become the smoke that comes from shadow, the sound of blue, the smell that emanates from the number twelve.</p>
<p><strong>You didn‚Äôt signal properly.</strong><br>
I mean, no, I don‚Äôt have any ‚Äúevidence‚Äù for that, but you must have done something wrong for an upstanding citizen like the driver of a Ford Focus that looks like it got into a fight with a forklift to strike you. The stats are on my side. Sixty-six percent of drivers <a href="https://www.forbes.com/sites/carltonreid/2019/05/10/cyclists-break-far-fewer-road-rules-than-motorists-finds-new-video-study/?sh=2d9fafbd4bfa">routinely commit moving violations</a>, compared with 5 percent of cyclists when they have somewhere safe to ride. That‚Äôs why I believe drivers.</p>
<p><strong>Your bike isn‚Äôt an <span>SUV</span>.</strong><br>
If your bike were an <span>SUV</span>, we wouldn‚Äôt be having this conversation. You‚Äôd be fine. In fact, it would be the Ford Focus driver who‚Äôd be all messed up. And that‚Äôs why SUVs are considered safe.</p>
<p><strong>You forgot to go back in time and tell people that subsidizing the oil industry might be a bad idea.</strong><br>
When the oil and auto industries teamed up to bend public policy to their will, making a system of roads and parking lots that now function as a continuous subsidy and magnificent symbol of the normalization of injury and pollution, you had a lot of options. You could have objected. You could have shifted public opinion. Instead, you weren‚Äôt even born yet. And, rather than go back in time, all you‚Äôve been doing is riding to get groceries and occasionally saying, ‚ÄúPlease stop killing us.‚Äù On the effort scale? 1/10.</p>
<p><strong>Frankly, I‚Äôm not sure a driver even hit you.</strong><br>
Maybe you were just <a href="https://www.nytimes.com/2023/07/29/health/ebikes-safety-teens.html">clipped by a Nissan van</a>. Was there a driver in the van? Has the passive voice historically functioned to deflect responsibility and consolidate unjust power arrangements? These are all fascinating questions that, sadly, we will never know the answers to.</p>
<p><strong>Oops, looks like you died.</strong><br>
Miraculously, the driver has been arrested and will face involuntary vehicular homicide. For killing you, the driver will get <a href="https://chi.streetsblog.org/2015/11/17/driver-who-killed-cyclist-hector-avalos-sentenced-to-only-100-days-in-prison">a hundred days in prison</a>. Apparently, killing someone is <a href="https://www.outsideonline.com/culture/essays-culture/justice-drivers-hit-cyclists/">basically legal</a> if you do it with a car. Also, our liberal city council has decided to make positive change. As I write, they‚Äôre enforcing strict new rules to ensure no one can ride their bikes in this part of town ever again.</p>
    </div>
    

    <div>
    <p>
        Please help support our writers and keep our site ad-free by becoming a patron today!
    </p>
    
  </div>

</article>

    


      <div>
        <h5>Suggested Reads</h5>
        <ul>
            <li>
    <a href="https://www.mcsweeneys.net/articles/the-great-sag">
      <p>October  4, 2000</p>
      <p>The Great Sag</p>
</a>    
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/evidence-that-automakers-predicted-senate-hearings-but-not-the-outcome-of-the-2008-presidential-election">
      <p>December  5, 2008</p>
      <p>Evidence That Automakers Predicted Senate Hearings but Not the Outcome of the 2008 Presidential Election</p>
</a>    <p><span>by </span>Elizabeth Worthington</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/variations-on-the-spelling-of-vehicles-submitted-by-my-6th-graders-attempting-to-earn-extra-credit-on-a-weekly-spelling-test">
      <p>February 18, 2002</p>
      <p>Variations on the Spelling of ‚ÄòVehicles,‚Äô Submitted By My 6th Graders Attempting to Earn Extra Credit on a Weekly Spelling Test</p>
</a>    <p><span>by </span>Andre Theisen</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/our-rv-has-a-kitchen-bedroom-bathroom-and-plenty-of-space-for-our-seven-children">
      <p>June 12, 2023</p>
      <p>Our RV Has a Kitchen, Bedroom, Bathroom, and Plenty of Space for Our Seven Children</p>
</a>    <p><span>by </span>Bobbie Armstrong<span> and&nbsp;</span>Madeline Goetz</p>
  </li>

        </ul>
      </div>


  <section>
        <div>
      <h5>Trending üî•</h5>
      <ol>
          <li>
    <a href="https://www.mcsweeneys.net/articles/youre-a-cyclist-who-was-just-struck-by-a-car-driver-heres-why-it-was-your-fault">
      <p>August 11, 2023</p>
      <p>You‚Äôre a Cyclist Who Was Just Struck by a Car Driver. Here‚Äôs Why It Was Your Fault</p>
</a>    <p><span>by </span>Chas Gillespie</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/how-to-ensure-your-annual-beach-vacation-destroys-your-relationship-with-your-extended-family">
      <p>July 26, 2023</p>
      <p>How to Ensure Your Annual Beach Vacation Destroys Your Relationship with Your Extended Family</p>
</a>    <p><span>by </span>Talia Argondezzi<span> and&nbsp;</span>Jeff Bender</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/i-regret-to-announce-that-i-will-not-be-canceling-my-plans-with-you-tonight">
      <p>August  4, 2023</p>
      <p>I Regret to Announce That I Will Not Be Canceling My Plans with You Tonight</p>
</a>    <p><span>by </span>Sam Shafaghi</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/what-your-favorite-80s-band-says-about-you">
      <p>July  5, 2011</p>
      <p>What Your Favorite ‚Äô80s Band Says About You</p>
</a>    <p><span>by </span>John K. Peck</p>
  </li>

      </ol>
    </div>

      <div>
    <h5>Recently</h5>
    <ul>
        <li>
    <a href="https://www.mcsweeneys.net/articles/im-racketeering-charges-and-im-here-to-rock-this-presidential-indictment-fest-like-you-wouldnt-believe">
      <p>August 15, 2023</p>
      <p>I‚Äôm Racketeering Charges, and I‚Äôm Here to Rock This Presidential Indictment-Fest Like You Wouldn‚Äôt Believe</p>
</a>    <p><span>by </span>Jess Keefe</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/welcome-to-your-new-city-in-the-northwest-where-recycling-is-so-simple">
      <p>August 15, 2023</p>
      <p>Welcome to Your New City in the Northwest, Where Recycling Is So Simple</p>
</a>    <p><span>by </span>Tori Multon</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/can-i-get-away-with-this-on-the-bus-an-faq-for-the-modern-commuter">
      <p>August 14, 2023</p>
      <p>Can I Get Away with This on the Bus? An <span>FAQ</span> for the Modern Commuter</p>
</a>    <p><span>by </span>Seif Drywater</p>
  </li>
  <li>
    <a href="https://www.mcsweeneys.net/articles/predictive-texts-for-the-conflict-averse">
      <p>August 14, 2023</p>
      <p>Predictive Texts for the Conflict-Averse</p>
</a>    <p><span>by </span>Tom Ellison<span> and&nbsp;</span>Caitlin Kunkel</p>
  </li>

    </ul>
  </div>

  </section>


  

    
  
  
  




        

    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How should I read type system notation? (153 pts)]]></title>
            <link>https://langdev.stackexchange.com/questions/2692/how-should-i-read-type-system-notation</link>
            <guid>37138807</guid>
            <pubDate>Tue, 15 Aug 2023 20:15:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://langdev.stackexchange.com/questions/2692/how-should-i-read-type-system-notation">https://langdev.stackexchange.com/questions/2692/how-should-i-read-type-system-notation</a>, See on <a href="https://news.ycombinator.com/item?id=37138807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>The notation used to describe type systems varies from presentation to presentation, so giving a comprehensive overview is impossible. However, most presentations share a large, common subset, so this answer will attempt to provide a foundation of enough of the basics to understand variations on the common theme.</p>
<h2>Syntax and grammars</h2>
<p>Type systems as applied to programming language are <em>syntactic</em> systems. That is, a type system is a set of rules that operate on the (abstract) syntax of a programming language. For this reason, comprehensive treatments of type systems begin by providing the <a href="https://en.wikipedia.org/wiki/Formal_grammar" rel="noreferrer">grammar</a> of all the syntactic constructs considered by the type system using <a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form" rel="noreferrer">BNF</a> notation. In the simplest typed languages, syntax is needed for precisely two things: <em>expressions</em> and <em>types</em>.</p>
<p>For example, let‚Äôs consider the grammar for an extremely simple language of booleans and integers:
<span>$$
\begin{array}{rcll}
e \hskip{-10mu}
  &amp;::=&amp;\hskip{-10mu} \mathsf{true}\hskip{10mu}|\hskip{10mu}\mathsf{false} &amp;\textrm{boolean literal}\\
  &amp; | &amp;\hskip{-10mu} 0 \hskip{5mu}|\hskip{5mu} 1 \hskip{5mu}|\hskip{5mu} {-1} \hskip{5mu}|\hskip{5mu} 2 \hskip{5mu}|\hskip{5mu} {-2} \hskip{5mu}|\hskip{5mu} \ldots &amp;\textrm{integer literal}\\
  &amp; | &amp;\hskip{-10mu} \mathbf{if}\ e\ \mathbf{then}\ e\ \mathbf{else}\ e &amp;\textrm{conditional}\\
  &amp; | &amp;\hskip{-10mu} e + e \hskip{10mu}|\hskip{10mu} e - e \hskip{10mu}|\hskip{10mu} e √ó e &amp;\textrm{arithmetic}\\
  &amp; | &amp;\hskip{-10mu} e = e \hskip{10mu}|\hskip{10mu} e &lt; e \hskip{10mu}|\hskip{10mu} e &gt; e &amp;\textrm{comparison}\\[8pt]
\tau \hskip{-10mu}
  &amp;::=&amp;\hskip{-10mu} \mathsf{Bool} &amp;\textrm{booleans}\\
  &amp; | &amp;\hskip{-10mu} \mathsf{Int} &amp;\textrm{integers}
\end{array}
$$</span>
Here, <span>$e$</span> corresponds to an expression and <span>$\tau$</span> corresponds to a type, which is a standard notational convention. Some presentations use other symbols for types, such as <span>$t$</span>, <span>$T$</span>, <span>$\sigma$</span>, or other lowercase Greek letters, but the overall structure will look roughly the same.</p>
<p>More complex languages will naturally have more complex grammars: imperative languages will include the grammar of statements, languages with pattern-matching will include the grammar of patterns, and so on. This language is so simple that it doesn‚Äôt even have variables! However, this core syntactic division between <em>terms</em> (things that have types) and <em>types</em> is essential, as defining the relationship between them is what type systems are all about.</p>
<h2>Relations, judgments, axioms, and inference rules</h2>
<p>Once the grammar has been specified, the next step is to define the <em>typing relation</em>, which is generally written <span>$e : \tau$</span> and can be read ‚Äú<span>$e$</span> has type <span>$\tau$</span>‚Äù. Intuitively, we understand that some statements of this form ‚Äúmake sense‚Äù and others do not:</p>
<ul>
<li><p><span>$1 + 2 : \mathsf{Int}$</span> means ‚Äú<span>$1 + 2$</span> has type <span>$\mathsf{Int}$</span>‚Äù, which certainly makes sense.</p>
</li>
<li><p><span>$1 + 2 : \mathsf{Bool}$</span> means ‚Äú<span>$1 + 2$</span> has type <span>$\mathsf{Bool}$</span>‚Äù, which does <em>not</em> make sense.</p>
</li>
<li><p><span>$\mathsf{true} + 2 : \mathsf{Int}$</span> means ‚Äú<span>$\mathsf{true} + 2$</span> has type <span>$\mathsf{Int}$</span>‚Äù, which makes even <em>less</em> sense, as the expression <span>$\mathsf{true} + 2$</span> is nonsense and does not have any type at all.</p>
</li>
</ul>
<p>We‚Äôd like to write down some rules that precisely capture our intuitions about which statements ‚Äúmake sense‚Äù and which do not. To do this, we‚Äôll define the <em>typing judgment</em>, which is written using the following notation:
<span>$$
‚ä¢ e : \tau
$$</span>
Here, the <span>$‚ä¢$</span> can be read to mean ‚Äúthe following statement is true‚Äù. (The <span>$‚ä¢$</span> might seem a bit unnecessary, and indeed it is sometimes omitted in simple systems like this one, but it will play a more significant role later.) Using this notation, we can write down the some of the <em>typing rules</em> for our type system:
<span>$$
\begin{array}{l}\hline ‚ä¢ \mathsf{true} : \mathsf{Bool}\end{array} \quad \quad
\begin{array}{l}\hline ‚ä¢ \mathsf{false} : \mathsf{Bool}\end{array}
$$</span>
The horizontal bar over each of these rules with nothing on top means that they are <em>always</em> true, which makes them <em>axioms</em>. We also have an infinite number of similar axioms for integer literals:
<span>$$
\begin{array}{l}\hline ‚ä¢ 0 : \mathsf{Int}\end{array} \quad
\begin{array}{l}\hline ‚ä¢ 1 : \mathsf{Int}\end{array} \quad
\begin{array}{l}\hline ‚ä¢ -1 : \mathsf{Int}\end{array} \quad
\begin{array}{l}\hline ‚ä¢ 2 : \mathsf{Int}\end{array} \quad \cdots
$$</span></p>
<p>Of course, the typing rules for literals are fairly boring. Things get much more interesting when we consider rules for expressions that have subexpressions! Here are the typing rules for <span>$+$</span> and <span>$&lt;$</span>:
<span>$$
\begin{array}{l}
‚ä¢ e_1 : \mathsf{Int} \\
‚ä¢ e_2 : \mathsf{Int} \\
\hline
‚ä¢ e_1 + e_2 : \mathsf{Int}
\end{array} \quad \quad \quad \begin{array}{l}
‚ä¢ e_1 : \mathsf{Int} \\
‚ä¢ e_2 : \mathsf{Int} \\
\hline
‚ä¢ e_1 &lt; e_2 : \mathsf{Bool}
\end{array}
$$</span>
Now we have things both above <em>and</em> below the horizontal bars, which makes these <em>inference rules</em>. These represent conditional typing rules: the statement under the bar is true <strong>if</strong> all of the statements above the bar are true. For example, the first rule can be read as ‚Äúif <span>$e_1 : \mathsf{Int}$</span> and <span>$e_2 : \mathsf{Int}$</span> are true, then <span>$e_1 + e_2 : \mathsf{Int}$</span> is true,‚Äù which should hopefully make intuitive sense.</p>
<p>Rules for <span>$-$</span>, <span>$√ó$</span>, <span>$=$</span>, and <span>$&gt;$</span> are nearly identical to the ones given above, but the rule for <span>$\mathbf{if} \ldots \mathbf{then} \ldots \mathbf{else}$</span> must be slightly more complex. This is because the branches of these expressions can be any type, as long as they agree. That is, both
<span>$$ \mathbf{if}\ \mathsf{true}\ \mathbf{then}\ 1\ \mathbf{else}\ 2 $$</span>
and
<span>$$ \mathbf{if}\ \mathsf{true}\ \mathbf{then}\ \mathsf{false}\ \mathbf{else}\ \mathsf{true} $$</span>
are legal, but
<span>$$
\mathbf{if}\ \mathsf{true}\ \mathbf{then}\ 1\ \mathbf{else}\ \mathsf{true}
$$</span>
is not. To capture this, the typing rule uses a variable to stand for the type of the branches:
<span>$$
\begin{array}{l}
‚ä¢ e_1 : \mathsf{Bool} \\
‚ä¢ e_2 : \tau \\
‚ä¢ e_3 : \tau \\
\hline
‚ä¢ \mathbf{if}\ e_1\ \mathbf{then}\ e_2\ \mathbf{else}\ e_3 : \tau
\end{array}
$$</span>
When the rule is applied, any type may be picked for <span>$\tau$</span> as long as the choice is used consistently.</p>
<p>This notation originates in formal logic, and in particular, the style used to specify type systems most closely resembles <a href="https://en.wikipedia.org/wiki/Natural_deduction" rel="noreferrer">natural deduction</a>. Though I will not go into the formal details of the notation in this answer, rules expressed in this way can be used to construct formal proofs about the system‚Äôs properties, which is important for proving things like <a href="https://en.wikipedia.org/wiki/Type_safety" rel="noreferrer">type soundness</a>.</p>
<h2>Judgments as an algorithmic specification</h2>
<p>So far, I‚Äôve intentionally refrained from saying anything about the computational interpretation of typing judgments. In general, judgments are just logical rules, and some type systems specified this way do not directly correspond to a decidable typechecking algorithm. However, if you are not used to thinking about proof systems, this perspective can be extremely difficult to wrap your head around.</p>
<p>Fortunately, in many cases, there <em>is</em> a way to read typing rules that directly yields a typechecking algorithm: it‚Äôs possible to interpret <span>$‚ä¢ e : \tau$</span> as a <em>function</em> from an expression <span>$e$</span> to its type <span>$\tau$</span>. This is because there is usually exactly one rule defined for each case in the grammar of expressions, which makes it possible to think of each typing rule as a distinct case in a recursive typechecking function.</p>
<p>For example, consider the rules for our little language given above. They correspond directly to an <span>$\mathrm{infer}$</span> function with the following shape:
<span>$$
\begin{array}{l}
\mathrm{infer} : \mathsf{Expr} ‚Üí \mathsf{Type} \\
\mathrm{infer}(e) = \mathbf{match}\ e\ \mathbf{where} \\
\quad \begin{alignedat}{2}
  &amp;\mathsf{true}\ |\ \mathsf{false} &amp;&amp;‚Ü¶ \mathsf{Bool} \\
  &amp;0\ |\ 1\ |\ {-1}\ |\ 2\ |\ \ldots &amp;&amp;‚Ü¶ \mathsf{Int} \\
  &amp;e_1 + e_2 &amp;&amp;‚Ü¶ \mathbf{assert}\: \mathrm{infer}(e_1) = \mathsf{Int}; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathbf{assert}\: \mathrm{infer}(e_2) = \mathsf{Int}; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathsf{Int} \\
  &amp;e_1 &lt; e_2 &amp;&amp;‚Ü¶ \mathbf{assert}\: \mathrm{infer}(e_1) = \mathsf{Int}; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathbf{assert}\: \mathrm{infer}(e_2) = \mathsf{Int}; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathsf{Bool} \\
  &amp;\mathbf{if}\ e_1\ \mathbf{then}\ e_2\ \mathbf{else}\ e_3
    &amp;&amp;‚Ü¶ \mathbf{assert}\: \mathrm{infer}(e_1) = \mathsf{Bool}; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathbf{let}\: \tau = \mathrm{infer}(e_2); \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathbf{assert}\: \mathrm{infer}(e_3) = \tau; \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \tau
\end{alignedat}
\end{array}
$$</span></p>
<p>Even when it isn‚Äôt possible to perform such a direct translation from typing rules to typechecking algorithm, it can be extremely helpful to consider information flow when reasoning about logical judgments. That is, for the <span>$‚ä¢ e : \tau$</span> judgment we defined above, <span>$e$</span> can be considered an ‚Äúinput‚Äù to the judgment and <span>$\tau$</span> considered an ‚Äúoutput‚Äù. This strict directionality does not always apply to every rule in a type system, but it often applies to many of them, and it is a useful way to wrap your head around what the rules represent.</p>
<h2>Variables, contexts, and environments</h2>
<p>The language we‚Äôve been using as a running example so far is exceptionally simple. One complication I‚Äôve intentionally chosen to avoid so far is <em>variables</em>, but we need to consider them if we want to be able to write typing rules for any useful programming language. Let‚Äôs therefore expand our example language by adding functions, making it a variant of the <a href="https://en.wikipedia.org/wiki/Simply_typed_lambda_calculus" rel="noreferrer">simply typed lambda calculus</a> (STLC). This requires the following additions to the language‚Äôs grammar:
<span>$$
\begin{array}{rcll}
e \hskip{-10mu}
  &amp;::=&amp;\hskip{-10mu}\ldots\\
  &amp; | &amp;\hskip{-10mu} x &amp;\textrm{variable}\\
  &amp; | &amp;\hskip{-10mu} Œªx{:}\tau. e &amp;\textrm{function abstraction}\\
  &amp; | &amp;\hskip{-10mu} e\ e &amp;\textrm{function application}\\[8pt]
\tau \hskip{-10mu}
  &amp;::=&amp;\hskip{-10mu}\ldots\\
  &amp; | &amp;\hskip{-10mu} \tau ‚Üí \tau &amp;\textrm{functions}
\end{array}
$$</span>
Here, <span>$x$</span> stands for ‚Äúsome variable‚Äù. If you are unfamiliar with the lambda calculus, the notation used here might appear somewhat exotic, but it is likely not as foreign as it seems: the STLC syntax <span>$Œªx{:}\tau. e$</span> corresponds directly to <code>(x:œÑ) =&gt; e</code> in TypeScript, and <span>$f\ x$</span> corresponds to <code>f(x)</code>.</p>
<p>With these additions to our grammar, the notation used for our typing relation does not change‚Äîit‚Äôs still just <span>$e : \tau$</span>‚Äîbut the structure of our typing <em>judgment</em> must be extended. The trouble appears when we attempt to write a typing rule for variables:
<span>$$
\begin{array}{l}
\hline
‚ä¢ x : \text{???}
\end{array}
$$</span>
The problem is that the type of a variable depends on the <em>context</em> it appears in. Therefore, we need to extend the typing judgment to keep track of the types of all variables that happen to be in scope, which we do using the following notation:
<span>$$
Œì ‚ä¢ e : \tau
$$</span>
<span>$Œì$</span> is called ‚Äúthe context‚Äù or ‚Äúthe type environment‚Äù, and the role of the <span>$‚ä¢$</span> now becomes clearer: it separates contextual assumptions from the statement to be proved. The extended judgment can therefore be pronounced ‚Äúunder the context <span>$Œì$</span>, the expression <span>$e$</span> has type <span>$\tau$</span>‚Äù, and algorithmically, <span>$Œì$</span> can be considered an additional ‚Äúinput‚Äù to the judgment of type <code>Map&lt;Variable, Type&gt;</code>. However, formally speaking, every typing rule must be defined syntactically, so contexts are explicitly represented in typing rules as syntactic constructs with the following shape:
<span>$$
\begin{array}{rcll}
Œì \hskip{-10mu}
  &amp;::=&amp;\hskip{-10mu} \varnothing &amp;\textrm{empty context}\\
  &amp; | &amp;\hskip{-10mu} Œì, x{:}\tau &amp;\textrm{variable binding}
\end{array}
$$</span>
Sometimes <span>$\bullet$</span> is used instead of <span>$\varnothing$</span> to represent the empty context. Under this representation, a context is essentially an <a href="https://en.wikipedia.org/wiki/Association_list" rel="noreferrer">association list</a> mapping variable names to types.</p>
<p>Most typing rules have no reason to care about the context. Most inference rules just pass it along unaltered, and most axioms ignore it altogether. For example, here are a couple typing rules from above adapted to our new judgment:
<span>$$
\begin{array}{l}\hline
Œì ‚ä¢ \mathsf{true} : \mathsf{Bool}
\end{array} \quad \quad \begin{array}{l}
Œì ‚ä¢ e_1 : \mathsf{Int} \\
Œì ‚ä¢ e_2 : \mathsf{Int} \\
\hline
Œì ‚ä¢ e_1 + e_2 : \mathsf{Int}
\end{array}
$$</span>
However, the context is essential for two new typing rules, which handle variable uses and lambda expressions:
<span>$$
\begin{array}{l}
x{:}\tau \in Œì \\
\hline
Œì ‚ä¢ x : \tau
\end{array} \quad \quad
\begin{array}{l}
Œì,x{:}\tau_1 ‚ä¢ e : \tau_2 \\
\hline
Œì ‚ä¢ (Œªx{:}\tau_1. e) : \tau_1 ‚Üí \tau_2
\end{array}
$$</span>
The second of these two rules is the more complex one, as it‚Äôs where all the magic happens: while typechecking the body <span>$e$</span> of the lambda expression, the context is extended with a new binding <span>$x{:}\tau_1$</span>. This information is then utilized by the first rule, which says that if there is a variable binding <span>$x$</span> with type <span>$\tau$</span> in the current context (and therefore in scope), then <span>$x$</span> has type <span>$\tau$</span>. In other words, the context is used as a communication mechanism to propagate information between these two rules.</p>
<p>(Observant readers may note that this model does not handle variable shadowing. As a simplifying assumption, type systems specified this way usually assume that all variables have already been resolved and made unique.)</p>
<p>If this still seems a bit confusing to you, it may help to consider the way these additions affect our <span>$\mathrm{infer}$</span> function from earlier:
<span>$$
\begin{array}{l}
\mathrm{infer} : (\mathsf{Context}, \mathsf{Expr}) ‚Üí \mathsf{Type} \\
\mathrm{infer}(Œì, e) = \mathbf{match}\ e\ \mathbf{where} \\
\quad \begin{alignedat}{2}
  &amp;\ldots\\
  &amp;x &amp;&amp;‚Ü¶ \mathrm{lookup}(Œì, x) \\
  &amp;(Œªx{:}\tau_1. e') &amp;&amp;‚Ü¶ \mathbf{let}\: Œì' = \mathrm{extend}(Œì, x, \tau_1); \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathbf{let}\: \tau_2 = \mathrm{infer}(Œì', e'); \\
    &amp;&amp;&amp;\phantom{{}‚Ü¶{}} \mathsf{\tau_1 ‚Üí \tau_2}
\end{alignedat}
\end{array}
$$</span></p>
<p>The only remaining rule we need to add to cope with the addition of functions is a rule for function application:
<span>$$
\begin{array}{l}
Œì ‚ä¢ e_1 : \tau_1 ‚Üí \tau_2 \\
Œì ‚ä¢ e_2 : \tau_1 \\
\hline
Œì ‚ä¢ e_1\ e_2 : \tau_2
\end{array}
$$</span>
That‚Äôs it!</p>
<h2>Other common notation and considerations</h2>
<p>The above describes the large majority of notation used to specify type systems if measured by volume, but modifications and extensions to this foundation are extremely common. It would be impossible to cover all of them, but fortunately, good papers usually explain whatever nonstandard notation they choose to introduce. However, some conventions are common enough that they are often used without explanation; this section attempts to provide a basic survey and describe a few notational quirks.</p>
<p>Since this is not and can never be an exhaustive list, please open a separate question if you find some notation not covered here!</p>
<h2>Inference rule layout</h2>
<p>So far, all of the examples in this answer have laid out inference rules in a very regular, vertical way. However, placing each condition on its own line is not by any means required, so several conditions may appear side-by-side:
<span>$$
\begin{array}{c}
Œì ‚ä¢ e_1 : \mathsf{Int} \hskip{25mu}
Œì ‚ä¢ e_2 : \mathsf{Int} \\
\hline
Œì ‚ä¢ e_1 + e_2 : \mathsf{Int}
\end{array}
$$</span>
Vertical and horizontal arrangement may even be combined within the same rule.</p>
<h2>Side conditions</h2>
<p>Usually, the conditions that appear above the horizontal bar in an inference rule are themselves judgments that must be satisfied by some combination of inference rules and axioms. However, this is not always the case: rules may also include arbitrary boolean expressions known as <em>side conditions</em>, which must all be true in order for the rule to be applied. The <span>$x{:}\tau \in Œì$</span> condition in our typing rule for variables is an example of a side condition.</p>
<p>A special type of side condition that sometimes appears in algorithmic type systems is written <span>$\alpha\ \mathbf{fresh}$</span>. This means that <span>$\alpha$</span> should be a fresh type variable, i.e. a type variable distinct from all other type variables.</p>
<h2>Subtyping</h2>
<p>Subtyping introduces a weaker notion of consistency between types than strict equality, and the subtyping relation must be explicitly defined. It is usually denoted <span>$\tau_1 &lt;: \tau_2$</span>, which can be read as ‚Äú<span>$\tau_1$</span> is a subtype of <span>$\tau_2$</span>‚Äù.</p>
<p>The relation is often defined using the same syntax used to define judgments. For example, a very simple subtyping relation might introduce two special types, <span>$\top$</span> (pronounced ‚Äútop‚Äù) and <span>$\bot$</span> (pronounced ‚Äúbottom‚Äù), which are the supertype and subtype of all types, respectively. This relation can be expressed using three simple axioms:
<span>$$
\begin{array}{l}\hline \tau &lt;: \tau \end{array} \quad \quad
\begin{array}{l}\hline \tau &lt;: \top \end{array} \quad \quad
\begin{array}{l}\hline \bot &lt;: \tau \end{array}
$$</span>
The first rule is the reflexive rule, often abbreviated to ‚Äúrefl‚Äù, which states that every type is a subtype of itself and ensures that subtyping is strictly weaker than equality.</p>
<p>The defined subtyping relation must then be used explicitly in every rule that permits subtyping. For example, a system that supports subtyping might use the following rule for function application:
<span>$$
\begin{array}{l}
Œì ‚ä¢ e_1 : \tau_2 ‚Üí \tau_3 \\
Œì ‚ä¢ e_2 : \tau_1 \\
\tau_1 &lt;: \tau_2 \\
\hline
Œì ‚ä¢ e_1\ e_2 : \tau_3
\end{array}
$$</span></p>
<h2>Multiple contexts</h2>
<p>Some type systems define typing judgments involving more than one context. The second context is usually named <span>$Œî$</span>. Common notations for multi-context rules are <span>$Œì;Œî ‚ä¢ e : \tau$</span> (used when both contexts are morally ‚Äúinputs‚Äù) and <span>$Œì ‚ä¢ e : \tau ‚ä£ Œî$</span> (used when <span>$Œî$</span> is morally an ‚Äúoutput‚Äù).</p>
<p>The second context may be used for any number of different things. Perhaps certain variables can be referenced from inside certain expressions but not others, or perhaps an output context is used to keep track of which variables are ‚Äúconsumed‚Äù in a resource-aware programming language.</p>
<h2>Bidirectional typechecking</h2>
<p><a href="https://arxiv.org/abs/1908.05839" rel="noreferrer">Bidirectional typechecking</a> is an approach for performing a limited form of nonlocal type inference without the need to use a constraint solver. A bidirectional system splits the usual <span>$Œì ‚ä¢ e : \tau$</span> typing judgment into two specialized judgments:</p>
<ul>
<li><p><span>$Œì ‚ä¢ e ‚áê \tau$</span> is the <em>checking</em> judgment, which checks that <span>$e$</span> has some expected type <span>$\tau$</span>. Algorithmically, <span>$\tau$</span> is an <em>input</em> to this judgment.</p>
</li>
<li><p><span>$Œì ‚ä¢ e ‚áí \tau$</span> is the <em>inference</em> judgment, which is used whenever expected type information is not available. Algorithmically, <span>$\tau$</span> is an <em>output</em> from this judgment.</p>
</li>
</ul>
<p>The two judgments are defined in a mutually-recursive way to propagate type information bidirectionally, which allows some type annotations to sometimes be omitted. For example, the checking variant of the rule for lambda abstraction may omit the annotation on the variable binder since it can be determined from the expected type:
<span>$$
\begin{array}{c}
Œì,x{:}\tau_1 ‚ä¢ e ‚áê \tau_2 \\
\hline
Œì ‚ä¢ (Œªx. e) ‚áê \tau_1 ‚Üí \tau_2
\end{array}
$$</span></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Requiring ink to scan a document‚Äìyet another insult from the printer industry (150 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/08/the-printers-that-require-ink-to-scan-and-fax/</link>
            <guid>37138691</guid>
            <pubDate>Tue, 15 Aug 2023 20:06:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/08/the-printers-that-require-ink-to-scan-and-fax/">https://arstechnica.com/gadgets/2023/08/the-printers-that-require-ink-to-scan-and-fax/</a>, See on <a href="https://news.ycombinator.com/item?id=37138691">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hp-printer-e1692118118328-800x819.jpg" alt="HP ENVY 6455e printer">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hp-printer-e1692118118328.jpg" data-height="1245" data-width="1216">Enlarge</a> <span>/</span> Don't bother hitting the scan button if you're out of ink.</p></figcaption>  </figure>

  




<!-- cache hit 4:single/related:e295cebb7bd5fad088ecd7ff3995225e --><!-- empty -->
<p>How much ink does an all-in-one printer need in order to fax a document? Or to scan one to your computer? The obvious answer is "none." But if you own certain printers from companies like HP and Canon, you won't be able to use core features unless the device has ink‚Äîeven if those features <em>have nothing to do with ink.&nbsp;</em></p>
<p>Unfortunately, all-in-one printers arbitrarily demanding ink to perform non-printing functions <a href="https://www.consumerreports.org/consumerist/if-you-want-to-scan-without-an-ink-cartridge-maybe-dont-buy-an-all-in-one-at-all/">isn't a new frustration</a>. And that's despite some companies having printers that can <a href="https://support.brother.com/g/b/faqend.aspx?c=us&amp;lang=en&amp;prod=mfcj4335dw_us_eu&amp;faqid=faq00002608_007">scan without ink</a>. Clearly, scanning or faxing without requiring an ink cartridge would improve users' experience‚Äîand they've illustrated that through class-action lawsuits. But this hasn't stopped printer makers from fighting to keep the nettlesome practice.</p>
<h2>No ink, no scan</h2>
<p>Since mid-2022, HP has been fighting a class-action lawsuit alleging that certain all-in-one printer models won't scan or fax without ink and that HP doesn't properly disclose this to shoppers. On January 13, 2023, the complaint was dismissed but allowed to be amended (you can view the amended complaint here: [<a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hp-complaint.pdf">PDF</a>]), and on August 10, a Northern District of California judge dismissed HP's motion to dismiss the amended complaint [<a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/HP-dismissed.pdf">PDF</a>].</p>
<p>HP Envy 6455e and HP Deskjet 2655 purchasers Gary Freund and Wayne McMath filed the complaint, which states that HP printers are designed to enter an error state when low or out of ink, preventing usage until the installment of a new ink cartridge. The plaintiffs are also peeved that HP marketing and advertising doesn't clearly disclose this, the complaint says. The complaint also notes that an HP support agent has <a href="https://h30434.www3.hp.com/t5/Scanning-Faxing-Copying/Scanning-without-a-working-ink-cartridge/td-p/6777739">said</a> that HP printers are "designed in such a way that with the empty cartridge or without the cartridge the printer will not function."</p>                                            
                                                        
<p>"HP‚Äôs All-in-One Printers do not work as advertised. Ink is <em>not</em> a necessary component to scan or to fax a document," the complaint reads.</p>
<p>It adds:</p>
<blockquote><p>Tying the scan or fax capabilities of the All-In-One Printers to ink contained in the devices offers no benefit and only serves to disadvantage and harm consumers financially. However, tying the scan or fax capabilities of the All-In-One Printers to ink contained in the devices does, however [sic], serve to benefit HP.</p></blockquote>
<p>Anyone who's owned an inkjet printer knows how expensive ink can be. That suggests a reason to push people to buy ink through tactics like blocking core features if no ink is present and <a href="https://www.consumerreports.org/electronics-computers/printers/why-is-printer-ink-so-expensive-a2101590645/">reportedly</a> selling printers below cost. Ink-buying programs have also become cash cows. HP in 2021, for example, said its Instant Ink subscription business was worth $500 million, per <a href="https://www.crn.com/news/components-peripherals/hp-says-instant-ink-is-a-500m-business-with-30-percent-growth">CRN</a>. In its Q2 2023 financial report, HP named Instant Ink a key growth area.</p>
<p>The complaint against HP says:</p>
<blockquote><p>Indeed, HP designs its All-in-One printer products so they will not work without ink. Yet, HP does not disclose this fact to consumers. ‚Ä¶ Even were it technically possible to scan a document without all ink cartridges present, HP does not disclose any 'workaround'&nbsp; to consumers in any of the product packaging nor in any of HP‚Äôs advertising and marketing materials regarding its multi-function devices.</p></blockquote>
<p>The complaint seeks monetary damages as well as the end of HP's "misleading advertising and marketing campaign" and for HP to "engage in a corrective campaign to inform consumers of the misleading advertising."</p>
<p>Here are all the HP printer models listed in the complaint:</p>
<ul>
<li aria-level="4">HP Deskjet 2755e</li>
<li aria-level="4">HP DeskJet 3755</li>
<li aria-level="4">HP DeskJet 4155e</li>
<li aria-level="4">HP ENVY 6055e</li>
<li aria-level="4">HP ENVY 6075</li>
<li aria-level="4">HP ENVY 6455</li>
<li aria-level="4">HP ENVY Pro 6475</li>
<li aria-level="4">HP OfficeJet 250 Mobile</li>
<li aria-level="4">HP OfficeJet Pro 7740 Wide Format</li>
<li aria-level="4">HP OfficeJet Pro 8025</li>
<li aria-level="4">HP DeskJet 2622</li>
<li aria-level="4">HP DeskJet 2655</li>
</ul>
<p>HP declined to comment on this story.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ISPs complain that listing every fee is too hard, urge FCC to scrap new rule (267 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/08/isps-complain-that-listing-every-fee-is-too-hard-urge-fcc-to-scrap-new-rule/</link>
            <guid>37138681</guid>
            <pubDate>Tue, 15 Aug 2023 20:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/08/isps-complain-that-listing-every-fee-is-too-hard-urge-fcc-to-scrap-new-rule/">https://arstechnica.com/tech-policy/2023/08/isps-complain-that-listing-every-fee-is-too-hard-urge-fcc-to-scrap-new-rule/</a>, See on <a href="https://news.ycombinator.com/item?id=37138681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/getty-internet-bills-800x533.jpg" alt="Dollar signs superimposed on a photo of a person's hands typing on a laptop keyboard.">
      <figcaption><p>Getty Images | anyaberkut</p></figcaption>  </figure>

  




<!-- cache hit 4:single/related:c2dc09635dc8106c544e0dc4acf983a2 --><!-- empty -->
<p>The US broadband industry is united in opposition to a requirement that Internet service providers list all of their monthly fees. Five lobby groups representing cable companies, fiber and DSL providers, and mobile operators have repeatedly urged the Federal Communications Commission to eliminate the requirement before new broadband labeling rules take effect.</p>
<p>The trade associations <a href="https://www.fcc.gov/ecfs/document/10117331109471/1">petitioned the FCC</a> in January to change the rules and renewed their call last week in a <a href="https://www.fcc.gov/ecfs/document/10811085828256/1">filing</a> and in a meeting with FCC officials. The requirement that ISPs list all their monthly fees "would add unnecessary complexity and burdens to the label for consumers and providers and could result in some providers having to create many labels for any given plan," the groups said in the filing on Friday.</p>
<p>The trade groups said the FCC should instead "require providers to include an explanatory statement that such fees may apply and that they vary by jurisdiction, similar to the Commission's treatment of government-imposed taxes," or require "the display of the maximum level of government-imposed fees that might be passed through, so that consumers would not experience bill shock with respect to such fees."</p>
<p>The filing was submitted by NCTA-The Internet &amp; Television Association, which represents Comcast, Charter, Cox, and other cable companies. The NCTA's <em>ex parte</em> filing described a meeting with FCC officials that also included wireless industry trade group CTIA and USTelecom, which represents telcos including AT&amp;T, Verizon, Lumen (formerly CenturyLink), Frontier, and Windstream.</p>
<p>The meeting was attended by two other groups representing smaller ISPs: NTCA-The Rural Broadband Association and ACA Connects-America's Communications Association. The trade groups met on Wednesday with the legal advisors to FCC Chairwoman Jessica Rosenworcel and Commissioner Brendan Carr, according to the filing.</p>                                            
                                                        
<h2>Comcast accused of ‚Äútrying to create loopholes‚Äù</h2>
<p>Comcast submitted its own filing urging the FCC to scrap the rules in June. The calls to weaken the FCC's truth-in-billing rules angered consumer advocates, as we <a href="https://arstechnica.com/tech-policy/2023/06/comcast-complains-to-fcc-that-listing-all-of-its-monthly-fees-is-too-hard/">wrote at the time</a>. "The label hasn't even reached consumers yet, but Comcast is already trying to create loopholes. This request would allow the big ISPs to continue hiding the true cost of service and frustrating customers with poor service," Joshua Stager, policy director at media advocacy group Free Press, told Ars.</p>
<p>Congress required the FCC to implement broadband labels with <a href="https://arstechnica.com/tech-policy/2022/01/fcc-aims-to-stop-broadband-bill-shock-reviving-plan-nixed-by-ajit-pai/">exact prices</a> for Internet service plans in a <a href="https://arstechnica.com/tech-policy/2021/11/congress-oks-42-billion-to-deploy-100mbps-broadband-in-unserved-areas/">2021 law</a>, but gave the FCC some leeway in how to structure the rules. The <a href="https://www.fcc.gov/document/fcc-requires-broadband-providers-display-labels-help-consumers">FCC adopted specific</a> label rules in November 2022.</p>
<p>The labels must be displayed to consumers at the point of sale and include monthly price, additional charges, speeds, data caps, additional charges for data, and other information. The FCC rules aren't in force yet because they are subject to a federal Office of Management and Budget (OMB) review under the US Paperwork Reduction Act.</p>
<p>ISPs object to a portion of the FCC order that says, "providers must list all recurring monthly fees" including "all charges that providers impose at their discretion, <em>i.e.</em>, charges not mandated by a government." The five trade groups complain that this would require ISPs "to display the pass-through of fees imposed by federal, state, or local government agencies on the consumer broadband label."</p>
<p>But just because an ISP says a fee is related to a government charge doesn't mean that ISPs have to break them out separately. ISPs could instead include all costs in their advertised rates to give potential customers a clearer idea of how much they would have to pay each month.</p>
<p>"A provider that opts to combine all of its monthly discretionary fees with its base monthly price may do so and list that total price. In that case, the provider need not separately itemize those fees in the label," the FCC order said.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asteroid ZTm0038 with a >3% impact probability (193 pts)]]></title>
            <link>https://newton.spacedys.com/neodys2/NEOScan/risk_page/ZTm0038/index_summary_ZTm0038.html</link>
            <guid>37138102</guid>
            <pubDate>Tue, 15 Aug 2023 19:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newton.spacedys.com/neodys2/NEOScan/risk_page/ZTm0038/index_summary_ZTm0038.html">https://newton.spacedys.com/neodys2/NEOScan/risk_page/ZTm0038/index_summary_ZTm0038.html</a>, See on <a href="https://news.ycombinator.com/item?id=37138102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainContent2">

	<!-- #CONTENT-WARNING-NOSIG -->
		
	<table>
	  <tbody><tr>     
	    <!--
		<th align="center" width="50">NEOCP name            </th>
		-->
	    <th>Asteroid name            </th>
	    <th>Number of observations</th>
	    <th>Arc length (h)        </th>
	    <th title="Epoch of the orbit with minimum œá">Epoch (MJD)         </th>
	    <th title="Residuals RMS of the orbit with minimum œá">RMS          </th>
	    <th title="Absolute magnitude of the orbit with minimum œá">H            </th>
	    <th title="MOID of the orbit with minimum œá">&nbsp;MOID&nbsp; (au)    </th>
	    <th title="Closest approach distance of the orbit with minimum œá">CA dist. (LD)</th>
	    <th title="Velocity at infinity of the orbit with minimum œá">V_inf (km/s)</th>
	  </tr>
	  <!-- #CONTENT-GEN -->
<tr>
<td>ZTm0038</td>
<td>8</td>
<td>24.23</td>
<td>60169.002</td>
<td>1.183</td>
<td>19.90</td>
<td>0.0000296</td>
<td></td>
<td>15.84</td>
</tr>
	</tbody></table>

	<br>
	<!--
	    !-------------!
	    ! Scores data !
	    !-------------!
	  -->
	<h3> Probabilities and scores </h3> 
	<center>
	  <table>
	    <tbody><tr>
	      <td>
		<table>
		  <tbody><tr>     
		    <th>NEO score</th>
		    <th>MBO score</th>
		    <th>&nbsp;&nbsp;DO&nbsp;&nbsp; score</th>
		    <th>&nbsp;&nbsp;SO&nbsp;&nbsp; score</th>
		  </tr>
		  <!-- #CONTENT-SCORE-OBJ -->
<tr>
<td>1.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00
</td>
</tr>
		</tbody></table>
	      </td>
	      <td></td><td></td><td>
	      </td><td>
		<table>
		  <tbody><tr>     
		    <th>PHA score</th>
		  </tr>
		  <!-- #CONTENT-SCORE-PHA -->
<tr>
<td>1.00</td>
</tr>
		</tbody></table>
	      </td>
	      <td></td><td></td><td>
	      </td><td>
		<table>
		  <tbody><tr>     
		    <th>Geocentric score</th>
		  </tr>
		  <!-- #CONTENT-SCORE-GEOC -->
<tr>
<td> 0.028</td>
</tr>
		</tbody></table>
	      </td>
	      <td></td><td></td><td>
	      </td><td>
		<table>
		  <tbody><tr>     
		    <th>Impact probability</th>
		  </tr>
		  <!-- #CONTENT-IP -->
<tr>
<td>0.034</td>
</tr>
		</tbody></table>
	      </td>
	    </tr>
	  </tbody></table>
	</center>

	<br>
	<!--
	    !-----------------------!
	    ! Auxiliary information !
	    !-----------------------!
	  -->
	<h3> Auxiliary information </h3> 
	<!-- #CONTENT-AUXILIARY-INFO -->
<p>Absolute magnitude of impactors: H_min = 18.84 and H_max = 20.84</p>
<p>Closest approach time of impactors: t_min = 2023/08/14 04:48 TDB and t_max = 2023/08/15 12:22 TDB</p>
<p>Velocity at infinity of impactors: v_inf_min =   7.668 [km/s] and v_inf_max =  55.785 [km/s]</p>
<p>Run started at 2023-08-14 23:51 UTC and ended at 2023-08-15 00:06 UTC</p>

	<br>
	<!-- 
	     !----------!
	     ! Arc data !
	     !----------!
	  -->
	<h3> Arc data </h3>	
	<table>
	  <tbody><tr>     
	    <th rowspan="2">Arc type                 </th>
	    <th colspan="3">Significance of curvature</th>
	  </tr>
  	  <tr>     
	    <th>Geodesic    </th>
	    <th>Acceleration</th>
	    <th>Overall     </th>
	  </tr>
	  <!-- #CONTENT-ARC -->
<tr>
<td>1</td>
<td>-1.93</td>
<td>-3.08</td>
<td>3.65</td>
</tr>
	</tbody></table>

	<br>
	<!--
	    !-------------------!
	    ! MOV sampling data !
	    !-------------------!
	  -->
	<h3> MOV sampling data </h3>
	<table>
	  <tbody><tr>     
	    <th>Number of sample points       </th>
	    <th>Number of MOV orbits          </th>
	    <th>Number of impacting MOV orbits</th>
	  </tr>
	  <!-- #CONTENT-SAMPLING -->
<tr>
<td>6400</td>
<td>969</td>
<td>204</td>
</tr>
	</tbody></table>

	<br>
	
	<!-- #CONTENT-BACK-TO-MAIN -->
<h3><a href="https://newton.spacedys.com/neodys2/NEOScan/index_risk.html">BACK to the Risk Page</a></h3>      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Probabilistic Machine Learning: Advanced Topics (105 pts)]]></title>
            <link>https://probml.github.io/pml-book/book2.html</link>
            <guid>37137810</guid>
            <pubDate>Tue, 15 Aug 2023 18:57:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://probml.github.io/pml-book/book2.html">https://probml.github.io/pml-book/book2.html</a>, See on <a href="https://news.ycombinator.com/item?id=37137810">Hacker News</a></p>
<div id="readability-page-1" class="page">
by <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Patrick Murphy</a>.
<br>
MIT Press, 2023.


	<p>
	  <img src="https://raw.githubusercontent.com/probml/pml2-book/main/cover2.jpg" alt="Book cover">
</p><h2>Key links</h2>

<ul>
	  
	<li> <a href="#toc">Short table of contents</a>
		
	    </li><li> <a href="https://github.com/probml/pml2-book/blob/main/toc2-long-2023-01-19.pdf">
		    Long table of  contents</a>
		    
</li><li> <a href="https://github.com/probml/pml2-book/blob/main/preface2-2023-01-02.pdf">Preface</a>

  </li><li> <a href="https://github.com/probml/pml2-book/releases/latest/download/book2.pdf">
    Draft pdf of the main book</a>, 2023-08-15.  CC-BY-NC-ND license. (Please cite the official reference below.)
       		    
</li><li> <a href="https://github.com/probml/pml-book/blob/main/supp2.md">Supplementary material</a>
	
  </li><li> <a href="https://github.com/probml/pml2-book/issues">Issue tracker</a>. 
	   
	  </li><li> <a href="https://github.com/probml/pyprobml/tree/master/notebooks/book2">
	    Code to reproduce most of the figures</a> 
		 
	  
</li><li> <a href="#ack">Acknowledgements</a>
	
	</li><li> <a href="#endorsements">Endorsements</a>
	
</li></ul>

If you use this book, please be sure to cite
<pre><code>
 @book{pml2Book,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: Advanced Topics",
 publisher = "MIT Press",
 year = 2023,
 url = "http://probml.github.io/book2"
}
</code></pre>


<p> Downloads since 2022-02-28. 
  <img src="https://img.shields.io/github/downloads/probml/pml2-book/total" alt="download stats shield">
  
</p><h2><a id="toc">Table of contents</a></h2><a id="toc">


    <img src="https://raw.githubusercontent.com/probml/pml2-book/main/toc2-short-2023-01-02.png" alt="TOC">
    
	  
</a><h2><a id="toc"></a><a id="endorsements"></a>Endorsements</h2>

<ul>
	<li>
			"Kevin Murphy has distilled the vast and confusing literature on machine learning and neural networks
			into a beautifully written 
			and extremely clear textbook that will be a wonderful resource for both new students 
			and seasoned researchers who are trying to keep up with this fast moving field.  
			The chapter on generative models is a masterpiece."
			-- <a href="https://www.cs.toronto.edu/~hinton/">Geoff Hinton</a>, U. Toronto/ Google.
		  
	</li><li> "Kevin Murphy had already impressed and greatly benefited the machine learning community with his introductory
		book on probabilistic ML
		and I am delighted to see the depth and breadth of material in his new sequel on advanced probabilistic ML.
		The book covers topics which I believe are at the heart of past and upcoming advances in our field,
		while often lacking in the training of graduate students in computer science, 
		and I therefore recommend it highly to all of them."
		-- <a href="https://yoshuabengio.org/">Yoshua Bengio</a>, U. Montreal
		
		</li><li> "This book is an amazing tour de force: Murphy and his co-authors have described and systematized virtually 
		all of the important advances in machine learning over the past 30 years. Pick any topic, and they provide a 
		crisp description of the state-of-the-art methods in a common, well-chosen notation and using a set of core concepts 
		in modeling, statistics, and optimization. This book will be a valuable starting point for students entering 
		the field and a wonderful reference for seasoned researchers. It is hard to imagine a better antidote to the vast, 
		confusing, and voluminous literature in machine learning.  This is such an amazing book! I learned things even in the sections
		where I‚Äôm fairly up to date with the literature. " --- <a href="https://web.engr.oregonstate.edu/~tgd/">Tom Dietterich</a>,
		Oregon State University
		
	</li><li> "The prior version of Dr. Murphy's book was amongst the 3-4 books I recommended to students and machine learning colleagues 
		at all levels as being essential to own, and, perhaps more importantly, to always have readily at hand. 
		As machine learning has matured and evolved, no other comprehensive resource of this nature has even remotely
		kept pace with modern methodological developments.  This new version is a blessing for the machine learning community and frankly, 
		at this moment in time, is the only truly necessary machine learning book to own. 
		Very few people in the world can do what Dr. Murphy has done here and the world owes him its thanks." 
		-- <a href="https://www.cs.ubc.ca/~fwood/">Frank Wood</a>, UBC. 
		
	</li><li> "Whether teaching machine learning to undergrads, master students, or PhD students, 
		I found myself time and time again choosing the 2012 "Machine Learning: A Probabilistic Perspective" as the primary textbook. 
		When I heard about the new "Probabilistic Machine Learning" series, I was thrilled to see the expanded and modernized set of topics;
		this will be the go-to book for my ML courses at Stanford.  Kevin Murphy has a phenomenal ability to go deep while making topics 
		digestible to a broad audience.  His writing is clear and concise with great visuals throughout.  I highly recommend this as "the book" 
		for anyone wanting to become a well-versed ML expert." -- <a href="https://emilybfox.su.domains/">Emily Fox</a>, Stanford.
	
		
		</li><li> "Murphy‚Äôs book is certainly the most comprehensive resource on machine learning available today. 
			With the growing body of research in the field, 
			it is a daunting challenge to provide an organized perspective of the current state of  knowledge. 
			This book achieves this feat by integrating classic material, like MCMC inference, 
			with very recent developments like denoising diffusion models. 
			The material is organized and presented in a very accessible and intuitive manner,
			making the book an asset for any researcher or practitioner in the field." --
			<a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson</a>, Tel Aviv University.
			
		</li><li> "This new Advanced Topics volume will provide a crucial path from the foundations of machine
		learning to state-of-the-art probabilistic models, many of which have only been described in the research literature.  
		I particularly like the way it draws parallels between variational and Monte Carlo inference,
		and between graphical models and (Bayesian) deep learning, so one can see the deep links between 
		methods that are sometimes cast as competitors.  
		I look forward to the next generations of probabilistic machine learning that this volume inspires." 
		--- <a href="https://www.ics.uci.edu/~sudderth/">Erik Sudderth</a>, UC Irvine.
		
	
		
		</li><li> "This book provides an outstanding and deep tour of the most foundational ideas in probabilistic machine learning. 
		It explains the essential mathematical and computational tools that a student needs to move beyond the basics. 
		With this book, 
		Murphy provides a comprehensive resource that will be great not just to learn from, but also as a reference 
		to return to again and again." -- <a href="https://www.cs.princeton.edu/~rpa/">Ryan Adams</a>, Princeton.
	
	</li><li> "Kevin Murphy's book is a landmark achievement in machine learning. It provides an in-depth coverage of
		a wide range of topics in probabilistic machine learning, from inference methods to generative models
		and decision making. It gives a modern perspective on these topics, bringing them up to date with recent
		advances in deep learning and representation learning. The insights in this 
		book are essential for a solid understanding of the field, 
		and I highly recommend it to students and experts alike." 
		-- <a href="http://mlg.eng.cam.ac.uk/zoubin/">Zoubin Ghahramani</a>, Cambridge/Google.
		
	</li><li> "As a
researcher trained by Murphy's 2012 book, I was excited to read
its sequel "Probabilistic Machine Learning: Advanced Topics." This new
book brings us to the forefront of cutting-edge probabilistic ML,
distilling the recent advances into a systemic exposition. It
communicates the core ideas of these recent advances in an
impressively clear and intuitive way, while managing to convey the
depth of these ideas by situating them in a broad context. It will 
		become a major reference that I constantly return to."
		-- <a href="https://yixinwang.github.io/">Yixin Wang</a>, U. Michigan.
	</li></ul>
		
	
  <h2><a id="ack"></a>Acknowledgements</h2>

I would like to thank the following people for helping with this book.

<ul>

<li> People who helped to write various sections and chapters:
	Alex Alemi (Google),
 Jeff Bilmes (U. Washington), 
	Peter Chang,
 Marco Cuturi (Apple, work done at Google), 
	Alexander D'Amour (Google),
	Finale Doshi-Velez (Harvard),
 Roy Frostig (Google), 
	Justin Gilmer (Google),
	Giles Harper-Donnelly,
	Been Kim (Google),
	Durk Kingma (Google),
	Simon Kornblith (Google),
   Balaji Lakshminarayanan (Google),  
	Lihong Li (Amazon, work done at Google), 
 Xinglong Li (UBC),
	Shakir Mohamed (Deepmind),
	George Papamakarios (Deepmind),
	Zeel Patel (IIT Gandhinagar),
	Ben Poole (Google),
	Mihaela Rosca (Deepmind / UCL),
  Vinayak Rao (Purdue), 
	 Yang Song (Stanford),
	Victor Veitch (Google / U. Chicago),
Andrew Wilson (NYU).
	
		      
      </li><li> Many people who helped make or improve the figures, including:
 Vishal Ghoniya,  Ankita Kumari Jain,  Aleyna Kara,  Zeel Patel, Karm Patel,
  Dhruv Patel, Nitish Sharma, Mahmoud Soliman.
    
</li><li> Participants in the Google Summer of Code (GSOC) for 2021,
including
Ming Liang Ang,
Aleyna Kara, Gerardo Duran-Martin,
Srikar Reddy Jilugu,  Drishti Patel,
and co-mentor Mahmoud Soliman.

</li><li> Participants in the Google Summer of Code (GSOC) for 2022,
  including
  Peter Chang, Giles Harper-Donnelly,
  Xinglong Li,
  Zeel Patel, Karm Patel, Qingyao Sun,
and co-mentors Nipun Batra and Scott Linderman.


</li><li> Many other people who contributed code
	(see auto-generated list <a href="https://github.com/probml/pyprobml#acknowledgements">here</a>).
	
	</li><li> Many people who helped with proof reading (see book preface for list).
	 
  
    </li></ul>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Future of Terraform Must Be Open (126 pts)]]></title>
            <link>https://blog.gruntwork.io/the-future-of-terraform-must-be-open-ab0b9ba65bca?gi=c07f9cd96456</link>
            <guid>37137781</guid>
            <pubDate>Tue, 15 Aug 2023 18:54:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.gruntwork.io/the-future-of-terraform-must-be-open-ab0b9ba65bca?gi=c07f9cd96456">https://blog.gruntwork.io/the-future-of-terraform-must-be-open-ab0b9ba65bca?gi=c07f9cd96456</a>, See on <a href="https://news.ycombinator.com/item?id=37137781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="c567">Our plan and pledge to keep Terraform open source</h2><div><a href="https://medium.com/@brikis98?source=post_page-----ab0b9ba65bca--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Yevgeniy Brikman" src="https://miro.medium.com/v2/resize:fill:88:88/1*7IJ4hkSmGgjeibEpJfbMGQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://blog.gruntwork.io/?source=post_page-----ab0b9ba65bca--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Gruntwork" src="https://miro.medium.com/v2/resize:fill:48:48/1*d6HE86wu0vvHrTQi_FZ3NQ.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><figure></figure><p id="f23e">On August 10, 2023, HashiCorp announced that after ~9 years of Terraform being open source under the MPL v2 license, they were suddenly switching it to a non open source BSL v1.1 license. We believe the BSL license is a poison pill for Terraform which threatens the entire community and ecosystem, and in this blog post, we‚Äôll introduce <a href="https://opentf.org/" rel="noopener ugc nofollow" target="_blank">OpenTF</a>, our plan for keeping Terraform open source‚Äîforever.</p><h2 id="7b5c">Why the BSL license is a poison pill for Terraform</h2><h2 id="b3c6">The virtuous cycle of open source</h2><p id="5ada">Terraform was originally released under the <a href="https://www.mozilla.org/en-US/MPL/2.0/FAQ/" rel="noopener ugc nofollow" target="_blank">Mozilla Public License</a> (MPL) v2.0, which is a well-known, trusted, and permissive open source license: MPL allows you to use Terraform for just about any purpose and incorporate it into any product, including copying, modifying, and redistributing the code. The only limitation is that if you modify the source code of Terraform itself, you have to release those modifications under the same MPL license.</p><p id="a2a2">The permissive terms of this license were a key factor in why the community adopted Terraform:</p><ul><li id="055b">Companies were comfortable in adopting Terraform, as the license ensured there would be no unexpected fees or IP problems.</li><li id="2be6">Developers were comfortable in contributing to <a href="https://github.com/hashicorp/terraform/" rel="noopener ugc nofollow" target="_blank">Terraform core</a> (which had 1,700+ contributors as of this writing) and the hundreds of Terraform providers (the <a href="https://github.com/hashicorp/terraform-provider-aws" rel="noopener ugc nofollow" target="_blank">AWS provider</a> had 2,800+ contributors and the <a href="https://github.com/hashicorp/terraform-provider-azurerm" rel="noopener ugc nofollow" target="_blank">Azure provider</a> had 1,300+ contributors as of this writing), as the license ensured they‚Äôd be able to use their work in both their current jobs and any future ones.</li><li id="355c">Vendors created tools, modules, and extensions for Terraform (e.g., at Gruntwork, we created <a href="https://terragrunt.gruntwork.io/" rel="noopener ugc nofollow" target="_blank">Terragrunt</a>, <a href="https://terratest.gruntwork.io/" rel="noopener ugc nofollow" target="_blank">Terratest</a>, and the <a href="https://gruntwork.io/infrastructure-as-code-library/" rel="noopener ugc nofollow" target="_blank">IaC Library</a>), as the license ensured you‚Äôd be able to use this work, whether for fun (e.g., as part of a side project) or profit (e.g., as part of a proprietary project).</li></ul><p id="a02b">This led to a virtuous cycle: as more companies adopt Terraform, more developers start using it; those developers contribute to Terraform, fixing bugs, improving security, adding new features, creating new tools and extensions, etc.; that makes Terraform even more compelling, so even more companies adopt it, and the cycle continues. As a result, Terraform has become one of the most popular infrastructure as code (IaC) tools on the market.</p><p id="3112"><strong>But none of this would have happened if Terraform hadn‚Äôt been released under a truly open source license</strong>. Those companies wouldn‚Äôt have adopted it; those developers wouldn‚Äôt have contributed to it; I can say for certain we would‚Äôve never built Terragrunt, Terratest, and the IaC Library, or even written <a href="https://www.terraformupandrunning.com/" rel="noopener ugc nofollow" target="_blank"><em>Terraform: Up &amp; Running</em></a>, if Terraform hadn‚Äôt been open source.</p><h2 id="0e18">The move to BSL</h2><p id="920b">After ~9 years of seeing the virtuous cycle of open source in action, we were shocked to hear HashiCorp‚Äôs <a href="https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license" rel="noopener ugc nofollow" target="_blank">announcement</a> that they were suddenly switching Terraform to the <a href="https://www.hashicorp.com/bsl" rel="noopener ugc nofollow" target="_blank">Business Source License</a> (BSL) v1.1, which is <em>not</em> an open source license (<a href="https://www.hashicorp.com/license-faq#details-on-nomenclature" rel="noopener ugc nofollow" target="_blank">HashiCorp‚Äôs own FAQ</a> even says the BSL doesn‚Äôt meet the definition of open source). In fact, not only Terraform, but all other core HashiCorp products are moving to BSL too: e.g., Vault, Consul, Nomad, etc.</p><p id="a744">To some extent, we understand what led HashiCorp in this direction. They are trying to maintain a delicate balance: on the one hand, they are creating amazing value by providing high quality, free, open source software to a community of thousands of developers; on the other hand, they are trying to run a sustainable business, so they need to capture and monetize some of the value they create.</p><p id="0c3f">The tricky part is deciding what to make open source and what to commercialize. If you make too much open source, then you don‚Äôt have enough to sell, so you can‚Äôt make enough money to pay the very employees who are creating and maintaining these amazing open source projects, so everyone loses; but if you make too little open source, then you never build up a community, and without that community, you can‚Äôt sell enough, can‚Äôt pay employees, and again, everyone loses. It‚Äôs only when you get this balance just right that everyone wins.</p><p id="ed96">Up until last Friday, it seemed like HashiCorp had this balance just right. They did so by making most functionality open source, but reserving some functionality solely for their commercial products, such as Terraform Cloud and Terraform Enterprise. With this approach, the Terraform community grew, companies that used Terraform grew, and, of course, HashiCorp grew into a public, multi-billion dollar company. Everyone wins!</p><p id="d078">That‚Äôs why we‚Äôre so stumped about the sudden move away from an open source license to a non open source BSL license. We believe that this move away from open source will upset the delicate balance and will lead to a scenario where everyone loses: the community will lose, companies that use Terraform will lose, and ultimately, even HashiCorp will lose. Let‚Äôs discuss why.</p><h2 id="44fe">The problems with BSL</h2><p id="8645">The BSL license, along with the additional use grants HashiCorp wrote for it, is generally fairly permissive, allowing you to copy, modify, and redistribute the code. However, there is an exception. The license does <em>not</em> allow you to use Terraform if you meet both of the following conditions:</p><ol><li id="979e">You are building a product that is competitive with HashiCorp.</li><li id="43b3">You embed or host Terraform in your product.</li></ol><p id="e46f"><strong>The first problem is that the terms of the BSL and the use grants are vague.</strong> What does ‚Äúcompetitive with HashiCorp‚Äù mean and who decides that? And what does ‚Äúembed or host‚Äù mean? If you‚Äôre a lawyer and you see this, you start to sweat. The number of questions is infinite. For example, if you‚Äôre an independent software vendor (ISV) or managed service provider (MSP) in the DevOps space, and you use Terraform with your customers (but not necessarily Terraform Cloud/Enterprise), are you a competitor? If your company creates a CI / CD product, is that competitive with Terraform Cloud or Waypoint? If your CI / CD product natively supports running Terraform as part of your CI / CD builds, is that embedding or hosting? If you built a wrapper for Terraform, is that a competitor? Is it embedding only if you include the source code or does using the Terraform CLI count as embedding? What if the CLI is installed by the customer? Is it hosting if the customer runs your product on their own servers?</p><p id="a5e5">It‚Äôs not an accident that these terms are vague. This is a common practice used by many legal teams to implicitly force you to ask the company for permission, so they can decide on a case-by-case basis: <a href="https://www.hashicorp.com/license-faq#details-on-nomenclature" rel="noopener ugc nofollow" target="_blank">HashiCorp‚Äôs FAQ</a> even says that to get clarification on whether what you‚Äôre doing is competitive or embedding or hosting, you need to reach out to <a href="mailto:licensing@hashicorp.com" rel="noopener ugc nofollow" target="_blank">licensing@hashicorp.com</a>.</p><p id="500d"><strong>And that leads to the second problem with the BSL: whether your usage complies with the license isn‚Äôt determined by the legal terms, but instead is entirely at the whim of HashiCorp.</strong> If HashiCorp doesn‚Äôt think you‚Äôre a competitor, you‚Äôre in the clear. But if they feel threatened by your company: no Terraform for you. If they want to define your usage as embedding or hosting: no Terraform for you. Or perhaps they grant you a license to use Terraform, but only at a fee, and if you can‚Äôt pay that fee: no Terraform for you.</p><p id="ee62"><strong>And that brings us to the third, and worst problem with the BSL: HashiCorp can change its decisions at any time. </strong>Perhaps HashiCorp told you your usage was safe before, but a year later, you launched a new product and now they think you‚Äôre a competitor, so suddenly, your usage no longer complies with the license. Or perhaps you didn‚Äôt change anything on your end, but it‚Äôs HashiCorp that launched a new product, which just so happens to be in your market, so now you‚Äôre a competitor, and your usage is no longer allowed. Or maybe HashiCorp decides that all the usages that were OK before will now cost money. Or maybe you‚Äôve already been paying them for a license, but suddenly, they decide to raise prices. And no matter what else happens, or what previous decisions or agreements you‚Äôve reached, remember, HashiCorp can just change the license again at any time!</p><p id="7011"><strong>As a result of the change to BSL, there is now no certainty with using Terraform:</strong></p><ul><li id="1fd2">If you‚Äôre a CTO, and you‚Äôre picking an IaC tool to use at your company, if you see that Terraform is BSL licensed, why take the risk? You‚Äôre now much more likely to go with alternative tools that are truly open source and have no licensing headaches.</li><li id="3f81">If you‚Äôre on the legal team at a company and reviewing the licenses your dev team wants to use, and you see BSL, why take the risk? You‚Äôre now much more likely to push back and put BSL on the banned license list.</li><li id="ae6c">If you‚Äôre a developer and considering contributing to open source, why contribute to a BSL project with no guarantee you‚Äôd be able to use your own work in the future? You‚Äôre now much more likely to contribute to something else.</li><li id="68d6">If you‚Äôre a vendor and considering building a product or tooling in the DevOps space, why build it around Terraform, and take the risk that HashiCorp will, now or in the future, consider you competitive? That‚Äôs just too shaky of a foundation to build on, so you‚Äôre now much more likely to build around something else.</li></ul><p id="3dcf"><strong>In short, BSL breaks the virtuous open source cycle that got Terraform to where it is today.</strong> And once you break that virtuous cycle, everyone loses: adoption will slow, contributions will decrease, and the community and ecosystem will dwindle and wither.</p><h2 id="5e3b">OpenTF: keeping Terraform open source</h2><p id="9908">We believe that the essential building blocks of the modern Internet‚Äîtools such as Linux, Kubernetes, and Terraform‚Äîmust be truly open source. That is the only way to ensure that we are building our industry on top of solid and predictable underpinnings.</p><p id="37f0">Therefore, we have joined forces with a number of other companies to create the <a href="https://opentf.org/" rel="noopener ugc nofollow" target="_blank">OpenTF manifesto</a>. <strong>The goal of the manifesto is to ensure Terraform remains truly open source ‚Äî always.</strong></p><p id="324e">To this end, the manifesto lays out the following two-step plan:</p><ol><li id="2d67"><strong>Ask HashiCorp to switch Terraform back to open source. </strong>The manifesto is a public request‚Äîa petition with signatures‚Äîfor HashiCorp to do the right thing here and switch Terraform back to a truly open source license (e.g., go back to MPL). Moreover, we want to be sure that it stays that way, so we are asking HashiCorp to commit to keeping Terraform under an open source license forever in the future.</li><li id="59ba"><strong>If they refuse, we will fork Terraform into an open source foundation.</strong> If HashiCorp decides to keep Terraform under the BSL license, then we will fork the MPL-licensed Terraform (Terraform version 1.5.5 and all versions before that are still MPL licensed), and put it into an open source foundation maintained by the community. We are one of many companies who have pledged resources to maintaining this fork, if it comes to it.</li></ol><p id="64a6"><strong>Either way, the future of Terraform is open source</strong>. If you also believe that Terraform should remain open source, please lend us your support, and let HashiCorp know how you feel by signing the OpenTF manifesto at <a href="https://opentf.org/" rel="noopener ugc nofollow" target="_blank">https://opentf.org/</a>!</p><h2 id="0e44">What this means for Gruntwork customers</h2><p id="7d02"><strong>As long as you use Terraform 1.5.5 or older, you can keep using all our commercial and open source products with no changes</strong>. Terraform 1.5.5 and all older versions are still MPL licensed, so you can keep using that version of Terraform with Terragrunt, Terratest, the IaC Library, the Reference Architecture, Gruntwork Pipelines, etc., with no changes of any kind.</p><p id="f16d"><strong>For future versions of Terraform, Gruntwork will use open source Terraform</strong>. For versions of Terraform that come out after 1.5.5, we will switch all our commercial and open source products to work only with open source Terraform: that is, if HashiCorp chooses to switch Terraform back to an open source license, we will use that, and if they don‚Äôt, then we will use our open source fork. We are currently waiting to see how HashiCorp responds to OpenTF, and we will share more details once we have them.</p><p id="9dba"><strong>But rest assured: no matter what happens, we are committed to keeping Terraform open source and ensuring all our products will continue to work with it.</strong></p><p id="6f20">For more information, and to join the OpenTF movement, please see <a href="https://opentf.org/" rel="noopener ugc nofollow" target="_blank">https://opentf.org/</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We reduced the cost of building Mastodon at Twitter-scale by 100x (690 pts)]]></title>
            <link>https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/</link>
            <guid>37137110</guid>
            <pubDate>Tue, 15 Aug 2023 17:54:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/">https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/</a>, See on <a href="https://news.ycombinator.com/item?id=37137110">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-1075">
	<!-- .entry-header -->

	<div>
		




<div><p>I‚Äôm going to cover a lot of ground in this post, so here‚Äôs the TLDR:</p>
<ul>
<li>We built a Twitter-scale <a href="https://joinmastodon.org/">Mastodon</a> instance from scratch in only 10k lines of code. This is <strong>100x less code</strong> than the ~1M lines Twitter wrote to build and scale their original consumer product, which is very similar to Mastodon. Our instance is located at <a href="https://mastodon.redplanetlabs.com/" rel="nofollow">https://mastodon.redplanetlabs.com</a> and open for anyone to use. The instance has 100M bots posting 3,500 times per second at 403 average fanout to demonstrate its scale.</li>
<li>Our implementation is built on top of a new platform called Rama that we at <a href="https://redplanetlabs.com/">Red Planet Labs</a> have developed over the past 10 years. This is the first time we‚Äôre talking about Rama publicly. Rama unifies computation and storage into a coherent model capable of building end-to-end backends at any scale in 100x less code than otherwise. Rama integrates and generalizes data ingestion, processing, indexing, and querying. Rama is a generic platform for building application backends, not just for social networks, and is programmed with a pure Java API. I will be exploring Rama in this post through the example of our Mastodon implementation.</li>
<li>We spent nine person-months building our scalable Mastodon instance. Twitter spent ~200 person-years to build and scale their original consumer product, and <a href="https://www.washingtonpost.com/technology/2023/07/29/meta-threads-mark-zuckerberg-rival-twitter-musk/">Instagram spent ~25 person-years building Threads</a>, a recently launched Twitter competitor. In their effort Instagram was able to leverage infrastructure already powering similar products.</li>
<li>Our scalable Mastodon implementation is also significantly less code than Mastodon‚Äôs <a href="https://github.com/mastodon/mastodon">official implementation</a>, which cannot scale anywhere near Twitter-scale.</li>
<li>In one week we will release a version of Rama that anyone can download and use. This version simulates Rama clusters within a single process and can be used to explore the full Rama API and build complete prototypes. We will also release the Rama documentation at that time.</li>
<li>In two weeks we will fully open-source our Mastodon implementation.</li>
<li>Red Planet Labs will be starting a private beta soon to give companies access to the full version of Rama. We will release more details on the private beta later, but companies can <a href="https://docs.google.com/forms/d/e/1FAIpQLSfrhmBwI0YAeaL8u4XmgfscW4UIUUDp2ZHSs4KmPH_TaDt1QQ/viewform">apply here</a> in the meantime.</li>
</ul>
<p>We recognize the way we‚Äôre introducing Rama is unusual. We felt that since the 100x cost reduction claim sounds so unbelievable, it wouldn‚Äôt do Rama justice to introduce it in the abstract. So we took it upon ourselves to directly demonstrate Rama‚Äôs 100x cost reduction by replicating a full application at scale in all its detail.</p>
</div>



<h2>Table of contents</h2>
<div><ul>
<li><a href="#Our_Mastodon_instance">Our Mastodon instance</a></li>
<li><a href="#Performance_and_scalability">Performance and scalability</a></li>
<li><a href="#Rama">Rama</a></li>
<li><a href="#Mastodon_on_Rama">Mastodon on Rama</a>
<ul>
<li><a href="#Following_hashtags">Following hashtags</a></li>
<li><a href="#Social_graph">Social graph</a></li>
<li><a href="#Computing_and_rendering_home_timelines">Computing and rendering home timelines</a></li>
<li><a href="#Personalized_follow_suggestions">Personalized follow suggestions</a></li>
</ul>
</li>
<li><a href="#DevOps_with_Rama">DevOps with Rama</a></li>
<li><a href="#Simple_Rama_code_example">Simple Rama code example</a></li>
<li><a href="#Sample_code_from_our_Mastodon_implementation">Sample code from our Mastodon implementation</a>
<ul>
<li><a href="#Representing_data">Representing data</a></li>
<li><a href="#Following_hashtags_code">Following hashtags code</a></li>
<li><a href="#Social_graph_code">Social graph code</a></li>
<li><a href="#Timeline_fanout_code">Timeline fanout code</a></li>
</ul>
</li>
<li><a href="#Conclusion">Conclusion</a></li>
</ul>
</div>



<h2 id="Our_Mastodon_instance">Our Mastodon instance</h2>



<div><p>First off, we make no comment about whether Mastodon should be scalable. There are good reasons to limit the size of an individual Mastodon instance. It is our belief, however, that such decisions should be product decisions and not forced by technical limitations. What we are demonstrating with our scalable Mastodon instance is that building a complex application at scale doesn‚Äôt have to be a costly endeavor and can instead be easily built and managed by individuals or small teams. There‚Äôs no reason the tooling you use to most quickly build your prototype should be different from what you use to build your application at scale.</p>
<p>Our Mastodon instance is hosted at <a href="https://mastodon.redplanetlabs.com/">https://mastodon.redplanetlabs.com</a>. We‚Äôve implemented every feature of Mastodon from scratch, including:</p>
<ul>
<li>Home timelines, account timelines, local timeline, federated timeline</li>
<li>Follow / unfollow</li>
<li>Post / delete statuses</li>
<li>Lists</li>
<li>Boosts / favorites / bookmarks</li>
<li>Personalized follow suggestions</li>
<li>Hashtag timelines</li>
<li>Featured hashtags</li>
<li>Notifications</li>
<li>Blocking / muting</li>
<li>Conversations / direct messages</li>
<li>Filters</li>
<li>View followers / following in order (paginated)</li>
<li>Polls</li>
<li>Trending hashtags and links</li>
<li>Search (status / people / hashtags)</li>
<li>Profiles</li>
<li>Image/video attachments</li>
<li>Scheduled statuses</li>
<li>ActivityPub API to integrate with other Mastodon instances</li>
</ul>
<p>There‚Äôs huge variety between these features, and they require very different kinds of implementations for how computations are done and how indexes are structured. Of course, every single aspect of our Mastodon implementation is scalable.</p>
<p>To demonstrate the scale of our instance, we‚Äôre also running 100M bot accounts which continuously post statuses (Mastodon‚Äôs analogue of a ‚Äútweet‚Äù), replies, boosts (‚Äúretweet‚Äù), and favorites. 3,500 statuses are posted per second, the average number of followers for each post is 403, and the largest account has over 22M followers. As a comparison, Twitter serves 7,000 tweets per second at 700 average fanout (according to the numbers I could find). With the click of a button we can scale our instance up to handle that load or much larger ‚Äì&nbsp;it would just cost us more money in server costs. We used the OpenAI API to generate 50,000 statuses for the bots to choose from at random.</p>
<p>Since our instance is just meant to demonstrate Rama and costs money to run, we‚Äôre not planning to keep it running for that long. So we don‚Äôt recommend using this instance for a primary Mastodon account.</p>
<p>One feature of Mastodon that needed tweaking because of our high rate of new statuses was global timelines, as it doesn‚Äôt make sense to flood the UI with thousands of new statuses per second. So for that feature we instead <a href="https://mastodon.redplanetlabs.com/timeline/local">show a small sample</a> of all the statuses on the platform.</p>
<p>The implementation of our instance looks like this:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" width="634" height="285" data-attachment-id="1080" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/overall-structure/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?fit=634%2C285&amp;ssl=1" data-orig-size="634,285" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="overall-structure" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?fit=300%2C135&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?fit=634%2C285&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?resize=634%2C285&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?w=634&amp;ssl=1 634w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?resize=300%2C135&amp;ssl=1 300w" sizes="(max-width: 634px) 100vw, 634px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?w=634&amp;ssl=1 634w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?resize=300%2C135&amp;ssl=1 300w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/overall-structure.png?resize=634%2C285&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>The Mastodon backend is implemented as Rama modules (explained later on this page), which handles all the data processing, data indexing, and most of the product logic. On top of that is our implementation of the <a href="https://docs.joinmastodon.org/api/">Mastodon API</a> using Spring/Reactor. For the most part, the API implementation just handles HTTP requests with simple calls to the Rama modules and serves responses as JSON. We use <a href="https://soapbox.pub/">Soapbox</a> to serve the frontend since it‚Äôs built entirely on top of the Mastodon API.</p>
<p><a href="https://aws.amazon.com/s3/">S3</a> is used only for serving pictures and videos. Though we could serve those from Rama, static content like that is better served via a <a href="https://en.wikipedia.org/wiki/Content_delivery_network">CDN</a>. So we chose to use S3 to mimic that sort of architecture. All other storage is handled by the Rama modules.</p>
<p>Our implementation totals 10k lines of code, about half of which is the Rama modules and half of which is the API server. We will be fully open-sourcing the implementation in two weeks.</p>
<p>Our implementation is a big reduction in code compared to the <a href="https://github.com/mastodon/mastodon">official Mastodon implementation</a>, which is built with Ruby on Rails. That codebase doesn‚Äôt always have a clear distinction between frontend and backend code, but just adding up the code for clearcut backend portions (models, workers, services, API controllers, ActivityPub) totals 18k lines of Ruby code. That doesn‚Äôt include any of the database schema definition code, configurations needed to run Postgres and Redis, or other controller code, so the true line count for the official Mastodon backend is higher than that. And unlike our Rama implementation, it can‚Äôt achieve anywhere near Twitter-scale.</p>
<p>This isn‚Äôt a criticism of the Mastodon codebase ‚Äì&nbsp;building products with existing technologies is just plain expensive. The reason we‚Äôve worked on Rama for so many years is to enable developers to build applications much faster, with much greater quality, and to never have to worry about scaling ever again.</p>
</div>



<h2 id="Performance_and_scalability">Performance and scalability</h2>



<p>Here‚Äôs a chart showing the scalability of the most intensive part of Mastodon, processing statuses:</p>



<figure><img data-lazy-fallback="1" decoding="async" width="656" height="407" data-attachment-id="1306" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/mastodon-perf-1-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?fit=1934%2C1202&amp;ssl=1" data-orig-size="1934,1202" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mastodon-perf-1-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?fit=300%2C186&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?fit=656%2C407&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=656%2C407&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1024%2C636&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=768%2C477&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1536%2C955&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1200%2C746&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?w=1934&amp;ssl=1 1934w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1024%2C636&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=768%2C477&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1536%2C955&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=1200%2C746&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?w=1934&amp;ssl=1 1934w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-1-2.png?resize=656%2C407&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>As you can see, increasing the number of nodes increases the statuses/second that can be processed. Most importantly, the relationship is linear. Processing statuses is so intensive because of fanout&nbsp;‚Äì&nbsp;if you have 15M followers, each of your statuses has to be written to 15M timelines. Each status on our instance is written to an average of 403 timelines (plus additional work to handle replies, lists, and conversations).</p>
<p>Twitter operates at 7,000 tweets per second at 700 average fanout, which is equivalent to about 12,200 tweets / second at 403 average fanout. So you can see we tested our Mastodon instance well above Twitter-scale.</p>
<p>Here‚Äôs a chart showing the latency distribution for the time from a status being posted to it being available on follower timelines:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="411" data-attachment-id="1084" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/mastodon-perf-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?fit=1672%2C1046&amp;ssl=1" data-orig-size="1672,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mastodon-perf-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?fit=300%2C188&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?fit=656%2C411&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=656%2C411&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1024%2C641&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=300%2C188&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=768%2C480&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1536%2C961&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1200%2C751&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?w=1672&amp;ssl=1 1672w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1024%2C641&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=300%2C188&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=768%2C480&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1536%2C961&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=1200%2C751&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?w=1672&amp;ssl=1 1672w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-2.png?resize=656%2C411&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>These numbers are a bit better than <a href="https://youtu.be/WEgCjwyXvwc?t=255">Twitter‚Äôs numbers</a>. Because of how unbalanced the social graph is, getting performance this good and this reliable is <a href="https://twitter.com/elonmusk/status/1624660886572126209">not easy</a>. For example, when someone with 20M followers posts a status, that creates a huge burst of load which could delay other statuses from fanning out. How we handled this is described more below.</p>
<p>Lastly, here‚Äôs a chart showing the latency distribution for fetching the data to render a user‚Äôs home timeline:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="410" data-attachment-id="1085" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/mastodon-perf-3/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?fit=1674%2C1046&amp;ssl=1" data-orig-size="1674,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mastodon-perf-3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?fit=300%2C187&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?fit=656%2C410&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=656%2C410&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1024%2C640&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=300%2C187&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=768%2C480&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1536%2C960&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1200%2C750&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?w=1674&amp;ssl=1 1674w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1024%2C640&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=300%2C187&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=768%2C480&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1536%2C960&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=1200%2C750&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?w=1674&amp;ssl=1 1674w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/mastodon-perf-3.png?resize=656%2C410&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Rendering a home timeline requires a lot of data from the backend: a page of statuses to render that aren‚Äôt muted/filtered, stats on each status (number of replies, boosts, and favorites), as well as information on the accounts that posted each status (username, display name, profile pic). Getting all this done in an average of 87ms is extremely efficient and a result of Rama being such an integrated system.</p>



<h2 id="Rama">Rama</h2>



<div><p>The numbers I‚Äôve shared here should be hard to believe: a Twitter-scale Mastodon implementation with extremely strong performance numbers in only 10k lines of code, which is less code than Mastodon‚Äôs current backend implementation and 100x less code than Twitter‚Äôs scalable implementation of a very similar product? How is it possible that we‚Äôve reduced the cost of building scalable applications by multiple orders of magnitude?</p>
<p>You can begin to understand this by starting with a simple observation: you can describe Mastodon (or Twitter, Reddit, Slack, Gmail, Uber, etc.) in total detail in a matter of hours. It has profiles, follows, timelines, statuses, replies, boosts, hashtags, search, follow suggestions, and so on. It doesn‚Äôt take that long to describe all the actions you can take on Mastodon and what those actions do. So the real question you should be asking is: given that software is entirely abstraction and automation, why does it take so long to build something you can describe in hours?</p>
<p>At its core Rama is a coherent set of abstractions for expressing backends end-to-end. All the intricacies of an application backend can be expressed in code that‚Äôs much closer to how you describe the application at a high level. Rama‚Äôs abstractions allow you to sidestep the mountains of complexity that blow up the cost of existing applications so much. So not only is Rama inherently scalable and fault-tolerant, it‚Äôs also far less work to build a backend with Rama than any other technology.</p>
<p>Let‚Äôs now dive into Rama. We‚Äôll start with a high-level overview of Rama‚Äôs concepts. Then we‚Äôll look at how some of the most important parts of our Mastodon instance are implemented in terms of these concepts. Finally, we‚Äôll look at some code from our Mastodon implementation.</p>
<p>Rama is programmed entirely with a Java API, and Rama‚Äôs programming model has four main concepts:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="354" data-attachment-id="1087" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/rama-concepts/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?fit=1622%2C874&amp;ssl=1" data-orig-size="1622,874" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rama-concepts" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?fit=300%2C162&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?fit=656%2C354&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=656%2C354&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1024%2C552&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=300%2C162&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=768%2C414&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1536%2C828&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1200%2C647&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?w=1622&amp;ssl=1 1622w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1024%2C552&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=300%2C162&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=768%2C414&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1536%2C828&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=1200%2C647&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?w=1622&amp;ssl=1 1622w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/rama-concepts.png?resize=656%2C354&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>On the left are ‚Äúdepots‚Äù, which are distributed, durable, and replicated logs of data. All data coming into Rama comes in through depot appends. Depots are like <a href="https://kafka.apache.org/">Apache Kafka</a> except integrated with the rest of Rama.</p>
<p>Next are "ETL"s, extract-transform-load topologies. These process incoming data from depots as it arrives and produce indexed stores called ‚Äúpartitioned states‚Äù. Rama offers two types of ETL, streaming and microbatching, which have different performance characteristics. Most of the time spent programming Rama is spent making ETLs. Rama exposes a Java dataflow API for coding topologies that is extremely expressive.</p>
<p>Next are ‚Äúpartitioned states‚Äù, which we usually call ‚ÄúPStates‚Äù. PStates are how data is indexed in Rama, and just like depots they‚Äôre partitioned across many nodes, durable, and replicated. PStates are one of the keys to how Rama is such a general-purpose system. Unlike existing databases, which have rigid indexing models (e.g. ‚Äúkey-value‚Äù, ‚Äúrelational‚Äù, ‚Äúcolumn-oriented‚Äù, ‚Äúdocument‚Äù, ‚Äúgraph‚Äù, etc.), PStates have a flexible indexing model. In fact, they have an indexing model already familiar to every programmer: <strong>data structures</strong>. A PState is an arbitrary combination of data structures, and every PState you create can have a different combination. With the ‚Äúsubindexing‚Äù feature of PStates, nested data structures can efficiently contain hundreds of millions of elements. For example, a ‚Äúmap of maps‚Äù is equivalent to a ‚Äúdocument database‚Äù, and a ‚Äúmap of subindexed sorted maps‚Äù is equivalent to a ‚Äúcolumn-oriented database‚Äù. Any combination of data structures and any amount of nesting is valid ‚Äì&nbsp;e.g. you can have a ‚Äúmap of lists of subindexed maps of lists of subindexed sets‚Äù. I cannot emphasize enough how much interacting with indexes as regular data structures instead of magical ‚Äúdata models‚Äù liberates backend programming.</p>
<p>The last concept in Rama is ‚Äúquery‚Äù. Queries in Rama take advantage of the data structure orientation of PStates with a ‚Äúpath-based‚Äù API that allows you to concisely fetch and aggregate data from a single partition. In addition to this, Rama has a feature called ‚Äúquery topologies‚Äù which can efficiently do real-time distributed querying and aggregation over an arbitrary collection of PStates. These are the analogue of ‚Äúpredefined queries‚Äù in traditional databases, except programmed via the same Java API as used to program ETLs and far more capable.</p>
<p>Individually, none of these concepts are new. I‚Äôm sure you‚Äôve seen them all before. You may be tempted to dismiss Rama‚Äôs programming model as just a combination of <a href="https://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> and <a href="https://en.wikipedia.org/wiki/Materialized_view">materialized views</a>. But what Rama does is integrate and generalize these concepts to such an extent that you can build entire backends end-to-end without any of the impedance mismatches or complexity that characterize and overwhelm existing systems.</p>
<p>All these concepts are implemented by Rama in a linearly scalable way. So if your application needs more resources, you can add them at the click of a button. Rama also achieves fault-tolerance by replicating all data and implementing automatic failover.</p>
<p>Here‚Äôs an example of how these concepts fit together. These are all the depots, ETLs, PStates, and query topologies for the portion of our Mastodon implementation handling profiles, statuses, and timelines:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="577" data-attachment-id="1088" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timelines-diagram/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?fit=1457%2C1282&amp;ssl=1" data-orig-size="1457,1282" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timelines-diagram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?fit=300%2C264&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?fit=656%2C577&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=656%2C577&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=1024%2C901&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=768%2C676&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=1200%2C1056&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?w=1457&amp;ssl=1 1457w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=1024%2C901&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=768%2C676&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=1200%2C1056&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?w=1457&amp;ssl=1 1457w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram.png?resize=656%2C577&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>This looks intimidating, but this part of the codebase only totals 1,100 lines of code. And it implements a ton of functionality, all scalably: statuses, timelines, boosts, conversations, favorites, bookmarks, mutes, account registration, profile edits, federation, and more. Notice that the PStates are a diverse collection of data structure combinations, and there are 33 of them here. Many of the ETLs produce multiple PStates and consume multiple depots. Making depots, ETLs, and PStates is inexpensive in Rama and can be done liberally.</p>
<p>What you‚Äôre seeing in this diagram is a total inversion of control compared to how applications are typically architected today. For example, consider the ‚Äúfanout‚Äù ETL in this diagram which processes incoming statuses and sends them to follower timelines (you‚Äôll see the code for this <a href="#Timeline_fanout_code">later in this post</a>). There are a bunch of rules dictating which statuses go to which followers ‚Äì&nbsp;boosts never go back to the original author, replies only go to followers who also follow the account being replied to, and so on. Traditionally, that‚Äôs accomplished with a ‚Äúdatabase layer‚Äù handling storage and a separate ‚Äúapplication layer‚Äù implementing the product logic. The ‚Äúapplication layer‚Äù does reads and writes to the ‚Äúdatabase layer‚Äù, and the two layers are deployed, scaled, and managed separately. But with Rama, the product logic exists inside the system doing the indexing. Computation and storage are colocated. Rama does everything a database does, but it also does so much more.</p>
<p>When building a backend with Rama, you begin with all the use cases you need to support. For example: fetch the number of followers of a user, fetch a page of a timeline, fetch ten follow suggestions, and so on. Then you determine what PState layouts (data structures) you need to support those queries. One PState could support ten of your queries, and another PState may support just one query.</p>
<p>Next you determine what your source data is, and then you make depots to receive that data. Source data usually corresponds to events happening in your application, like ‚ÄúAlice follows Bob‚Äù, ‚ÄúJames posted the status ‚ÄòHello world‚Äô‚Äù, or ‚ÄúBob unfollows Charlie‚Äù. You can represent your data however you want, whether Java objects, frameworks like <a href="https://thrift.apache.org/">Thrift</a> or <a href="https://protobuf.dev/">Protocol Buffers</a>, or even unstructured formats like JSON (however, we recommend using structured formats as much as possible).</p>
<p>The last step is writing the ETL topologies that convert source data from your depots into your PStates. When deployed, the ETLs run continuously keeping your PStates up to date. Rama‚Äôs ETL API, though just Java, is like a ‚Äúdistributed programming language‚Äù with the computational capabilities of any Turing-complete language along with facilities to easily control on which partition computation happens at any given point. You‚Äôll see many examples of this API later in this post.</p>
</div>



<h2 id="Clusters_and_modules">Clusters and modules</h2>



<div><p>Rama is deployed onto a cluster of nodes. There‚Äôs a central daemon called the ‚ÄúConductor‚Äù which coordinates deploys, updates, and scaling operations. Every node has a ‚ÄúSupervisor‚Äù daemon which manages the launch/teardown of user code.</p>
<p>Applications are deployed onto a Rama cluster as ‚Äúmodules‚Äù. A ‚Äúmodule‚Äù contains an arbitrary combination of depots, ETLs, PStates, and query topologies. Unlike traditional architectures, where the corresponding analogues exist in separate processes and usually on separate nodes, in Rama these are colocated in the same set of processes. This colocation enables fantastic efficiency which has never been possible before. Modules can also consume data from depots and PStates in other modules just as easily as they can from their own. A module runs forever, continuously processing new data from depots, unless you choose to destroy it.</p>
<p>A module is deployed by giving the Conductor a .jar file with user code. Additionally, configuration is provided for the number of nodes to allocate to the module, replication parameters, as well as any other tuning parameters. A module is updated a similar way: a new .jar is provided with the new code, and the Conductor orchestrates an update sequence that launches new processes and transfers depots and PStates to the new module version.</p>
<p>Here‚Äôs what a Rama cluster could look like with two modules deployed, ‚ÄúSocialGraphModule‚Äù and ‚ÄúTimelineModule‚Äù:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="338" data-attachment-id="1091" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-overview/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?fit=1548%2C796&amp;ssl=1" data-orig-size="1548,796" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-overview" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?fit=300%2C154&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?fit=656%2C338&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=656%2C338&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1024%2C527&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=300%2C154&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=768%2C395&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1536%2C790&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1200%2C617&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?w=1548&amp;ssl=1 1548w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1024%2C527&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=300%2C154&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=768%2C395&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1536%2C790&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=1200%2C617&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?w=1548&amp;ssl=1 1548w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-overview.png?resize=656%2C338&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>For testing and development, Rama provides a class called

<code>InProcessCluster</code>

for simulating Rama clusters within a single process.</p>



<h2 id="Mastodon_on_Rama">Mastodon on Rama</h2>



<p>Let‚Äôs look at some of the key parts of how Mastodon is implemented on top of Rama. In this section we‚Äôll focus on the design of Mastodon ‚Äì&nbsp;what PStates are created, the flow of how data is processed, and how everything is organized. You‚Äôll see how Rama‚Äôs capabilities enable some seriously novel ways to architect applications. In the next section we‚Äôll look at some of the code from our implementation.</p>



<h3 id="Following_hashtags">Following hashtags</h3>



<div><p>Let‚Äôs start with an extremely simple part of the implementation, tracking followers for hashtags. The implementation for this totals 11 lines of code and supports the following queries:</p>
<ul>
<li>Does user A follow hashtag H?</li>
<li>Who follows hashtag H (paginated)?</li>
<li>How many followers does hashtag H have?</li>
</ul>
</div>



<p>Only a single PState is needed for this, called

<code>$$hashtagToFollowers</code>

(PState names in Rama always begin with

<code>$$</code>

). It is a map from a hashtag to a set of account IDs. Here‚Äôs a visualization of what this PState could look like across two partitions. Keep in mind that PStates are distributed across many partitions, with each partition of a PState being the specified data structure:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="536" height="241" data-attachment-id="1120" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/hashtagtofollowers/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?fit=536%2C241&amp;ssl=1" data-orig-size="536,241" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hashtagToFollowers" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?fit=300%2C135&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?fit=536%2C241&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?resize=536%2C241&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?w=536&amp;ssl=1 536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?resize=300%2C135&amp;ssl=1 300w" sizes="(max-width: 536px) 100vw, 536px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?w=536&amp;ssl=1 536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?resize=300%2C135&amp;ssl=1 300w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/hashtagToFollowers.png?resize=536%2C241&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>There are two events that change this PState: following a hashtag, and unfollowing a hashtag. In our implementation, these are represented by the types

<code>FollowHashtag</code>

and

<code>RemoveFollowHashtag</code>

.</p>



<p>A good way to visualize how data is processed to produce this PState is via a dataflow graph, as you can see how data moves from the start of processing (one or more depots) to the end results of processing (updates to PStates):</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="372" height="472" data-attachment-id="1121" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/follow-hashtags/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?fit=372%2C472&amp;ssl=1" data-orig-size="372,472" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="follow-hashtags" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?fit=236%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?fit=372%2C472&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?resize=372%2C472&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?w=372&amp;ssl=1 372w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?resize=236%2C300&amp;ssl=1 236w" sizes="(max-width: 372px) 100vw, 372px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?w=372&amp;ssl=1 372w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?resize=236%2C300&amp;ssl=1 236w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags.png?resize=372%2C472&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>The logic here is trivial, which is why the implementation is only 11 lines of code. You don‚Äôt need to worry about things like setting up a database, establishing database connections, handling serialization/deserialization on each database read/write, writing deploys just to handle this one task, or any of the other tasks that pile up when building backend systems. Because Rama is so integrated and so comprehensive, a trivial feature like this has a correspondingly trivial implementation.</p>
<p>You may be wondering why the follow and unfollow events go onto the same depot instead of separate depots. Though you could implement it that way, it would be a mistake to do so. Data is processed off a depot partition in the order in which it was received, but there are no ordering guarantees across different depots. So if a user was spamming the ‚ÄúFollow‚Äù and ‚ÄúUnfollow‚Äù buttons on the UI and those events were appended to different depots, it‚Äôs possible a later unfollow could be processed before a prior follow. This would result in the PState ending up in the incorrect state according to the order by which the user made actions. By putting both events in the same depot, the data is processed in the same order in which it was created.</p>
<p>As a general rule, Rama guarantees <strong>local ordering</strong>. Data sent between two points are processed in the order in which they were sent. This is true for processing data off a depot, and it‚Äôs also true for intra-ETL processing when your processing jumps around to different partitions as part of computation.</p>
<p>This dataflow diagram is literally how you program with Rama, by specifying dataflow graphs in a pure Java API. As you‚Äôll see below, the details of specifying computations like this involve variables, functions, filters, loops, branching, and merging. It also includes fine-grained control over which partitions computation is executing at any given point.</p>
</div>







<div><p>Let‚Äôs now look at a slightly more involved part of the implementation, the social graph. The social graph totals 105 lines of code and supports the following queries which power various aspects of Mastodon:</p>
<ul>
<li>Does user A follow user B?</li>
<li>How many followers does user A have?</li>
<li>How many accounts does user A follow?</li>
<li>Who are user A‚Äôs followers in the order in which they followed (paginated)?</li>
<li>Who does user A follow in the order in which they followed them (paginated)?</li>
<li>Who is currently requesting to follow user A in the order in which they requested (paginated)?</li>
<li>Does user A block user B?</li>
<li>Does user A mute user B?</li>
<li>Does user A wish to see boosts from user B?</li>
</ul>
<p>Even though the implementation is so simple with Rama, it‚Äôs worth noting that Twitter had to write a <a href="https://github.com/twitter-archive/flockdb">custom database from scratch</a> to build their scalable social graph.</p>
</div>



<p>There are four main PStates produced by our social graph implementation, named

<code>$$followerToFollowees</code>

,

<code>$$followeeToFollowers</code>

,

<code>$$accountIdToFollowRequests</code>

, and

<code>$$accountIdToSuppressions</code>

. The first three PStates have the same structure, which is a map from account ID to a linked set of account IDs plus additional information needed about the relationship. For example, for

<code>$$followeeToFollowers</code>

we track whether a follower wants to see boosts from that account in their home timeline (which is one of Mastodon‚Äôs features). This PState is also used to compute: whether an account already follows another account, the order in which accounts were followed (which is why a linked set is used rather than a regular set), and the number of followers for an account (which is just the size of the inner set, something Rama computes in fast constant time even if the set has millions of elements).</p>



<p>The

<code>$$accountIdToSuppressions</code>

PState tracks blocks and mutes for each account and has a different structure. It is a map from account ID to a map containing two keys: ‚Äúblocked‚Äù and ‚Äúmuted‚Äù. The ‚Äúblocked‚Äù key maps to the set of account IDs the top-level account ID has blocked. The ‚Äúmuted‚Äù key is similar but stores a map from account ID to the options for that mute (like expiration time). This PState is used wherever statuses are rendered (e.g. home timeline, notifications) to filter out statuses a user doesn‚Äôt want to see.</p>



<p>Here‚Äôs a visualization of what the

<code>$$followeeToFollowers</code>

PState could look like in a deployed module:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="261" data-attachment-id="1125" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/followeetofollowers/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?fit=1208%2C480&amp;ssl=1" data-orig-size="1208,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="followeeToFollowers" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?fit=300%2C119&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?fit=656%2C261&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=656%2C261&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=1024%2C407&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=300%2C119&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=768%2C305&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=1200%2C477&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?w=1208&amp;ssl=1 1208w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=1024%2C407&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=300%2C119&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=768%2C305&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=1200%2C477&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?w=1208&amp;ssl=1 1208w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/followeeToFollowers.png?resize=656%2C261&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>Here you can see that account ‚Äú1‚Äù is followed by accounts ‚Äú2‚Äù, ‚Äú7‚Äù, and ‚Äú5‚Äù, with ‚Äú2‚Äù and ‚Äú7‚Äù having boosts enabled and ‚Äú5‚Äù having boosts disabled. Account ‚Äú4‚Äù is only followed by account ‚Äú1‚Äù, and ‚Äú1‚Äù has boosts enabled for that relationship.</p>
<p>The social graph is constructed based on follow, unfollow, block, unblock, mute, and unmute events. In Mastodon‚Äôs design, blocking a user also unfollows in both directions (if currently followed). So the block and unblock events go on the same depot as the follow-related events, while the mute and unmute events go on a separate depot.</p>
<p>Here‚Äôs the dataflow graph showing how these PStates are computed based on the source data:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="704" data-attachment-id="1126" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?fit=1136%2C1219&amp;ssl=1" data-orig-size="1136,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?fit=280%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?fit=656%2C704&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=656%2C704&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=954%2C1024&amp;ssl=1 954w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=280%2C300&amp;ssl=1 280w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=768%2C824&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?w=1136&amp;ssl=1 1136w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=954%2C1024&amp;ssl=1 954w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=280%2C300&amp;ssl=1 280w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=768%2C824&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?w=1136&amp;ssl=1 1136w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph.png?resize=656%2C704&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This is more involved than the hashtag follows dataflow graph since this ETL supports so many more features. Yet it‚Äôs still only 105 lines of code to implement. You can see how this ETL makes use of conditionals, branching, and merging in its implementation. Here‚Äôs a few notes on the logic within this ETL:</p>



<ul>
<li>When a user follows another user, the

<code>$$followerToFollowees</code>

and

<code>$$followeeToFollowers</code>

PStates are updated. Unfollowing updates the same PStates but removes the relationship instead of adding it.</li>
<li>Blocking is handled specially by implicitly emitting additional unfollow events as part of processing (to unfollow in both directions), as well as tracking who blocks who in the

<code>$$accountIdToSuppressions</code>

PState.</li>
<li>A locked account (where each follower must be individually approved) receives

<code>FollowLockedAccount</code>

events, and unlocked accounts receive

<code>Follow</code>

events.</li>
<li>Attributes of a relationship (e.g. ‚Äúshow boosts‚Äù, ‚Äúnotify‚Äù) are updated via appending another

<code>Follow</code>

or

<code>FollowLockedAccount</code>

event to the depot. This is why the

<code>FollowLockedAccount</code>

checks if the follow relationship already exists so it can determine whether to update the follow relationship or add a new follow request.</li>
<li>A

<code>Follow</code>

event also removes the corresponding follow request if it exists. This is why an

<code>AcceptFollowRequest</code>

event just converts to a

<code>Follow</code>

event.</li>
</ul>



<p>This ETL interacts with many PStates at the same time. Because of Rama‚Äôs integrated nature, these PStates are colocated with one another within the same processes that are executing the ETL logic. So whereas you always have to do a network operation to access most databases, PState operations are local, in-process operations with Rama ETLs. As you‚Äôll see later, you utilize the network in an ETL via ‚Äúpartitioner‚Äù operations to get to the right partition of a module, but once you‚Äôre on a partition you can perform as many PState operations to as many colocated PStates as you need. This is not only extremely efficient but also liberating due to the total removal of impedance mismatches that characterizes interacting with databases.</p>



<h3 id="Computing_and_rendering_home_timelines">Computing and rendering home timelines</h3>



<p>Next let‚Äôs look at the core of Mastodon, computing and rendering home timelines. This powers the primary page of the Mastodon experience:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="411" data-attachment-id="1157" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/home-timeline-screenshot-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?fit=3382%2C2122&amp;ssl=1" data-orig-size="3382,2122" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="home-timeline-screenshot-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?fit=300%2C188&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?fit=656%2C411&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=656%2C411&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1024%2C642&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=300%2C188&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=768%2C482&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1536%2C964&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=2048%2C1285&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1200%2C753&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1024%2C642&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=300%2C188&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=768%2C482&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1536%2C964&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=2048%2C1285&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=1200%2C753&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/home-timeline-screenshot-1.png?resize=656%2C411&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This use case is a great example of how to think about building data-intensive systems not just with Rama, but in general. For any backend feature you want to implement, you have to balance what gets precomputed versus what gets computed on the fly at query-time. The more you can precompute, the less work you‚Äôll have to do at query-time and the lower latencies your users will experience. This basic structure looks like this:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="243" data-attachment-id="1133" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/backend-model/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?fit=1642%2C608&amp;ssl=1" data-orig-size="1642,608" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="backend-model" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?fit=300%2C111&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?fit=656%2C243&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=656%2C243&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1024%2C379&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=300%2C111&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=768%2C284&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1536%2C569&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1200%2C444&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?w=1642&amp;ssl=1 1642w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1024%2C379&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=300%2C111&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=768%2C284&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1536%2C569&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=1200%2C444&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?w=1642&amp;ssl=1 1642w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/backend-model.png?resize=656%2C243&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>This of course is the programming model of Rama you‚Äôve already seen, and a big part of designing Rama applications is determining what computation goes in the ETL portion versus what goes in the query portion. Because both the ETL and query portions can be arbitrary distributed computations, and since PStates can be any structure you want, you have total flexibility when it comes to choosing what gets precomputed versus what gets computed on the fly.</p>
<p>It‚Äôs generally a good idea to work backwards from the needed queries to learn how to best structure things. In the case of querying for a page of a timeline, you need the following information:</p>
<ul>
<li>Content for each status</li>
<li>Stats for each status (number of replies, boosts, and favorites)</li>
<li>Information about the account that posted each status (username, display name, profile pic)</li>
<li>Whether the author of each status is blocked or muted</li>
<li>For boosts, the username of the booster</li>
<li>ID to use to fetch the next page of statuses</li>
</ul>
<p>Since statuses can be edited and profile information can be changed, almost all this information is dynamic and must be fetched for each render of a timeline page.</p>
<p>Let‚Äôs consider a typical way to go about this with non-Rama technologies, even scalable ones, which unfortunately is extremely inefficient:</p>
<ul>
<li>Fetch the list of status IDs for a page of the timeline</li>
<li>In parallel, send database requests to fetch:
<ul>
<li>Content for each status</li>
<li>Stats for each status</li>
<li>Information for each author</li>
</ul>
</li>
</ul>
<p>For a page of 20 statuses, this could easily require over 100 database calls with a lot of overhead in the sheer amount of back and forth communication needed to fetch data from each database partition.</p>
<p>We handled this use case with Rama by making use of Rama‚Äôs facilities for colocation of PStates. The module is organized such that <strong>all information</strong> for a status and the account who posted it are colocated in the same process. So instead of needing separate requests for status content, status stats, and author information, only one request is needed per status.</p>
</div>



<h4>Structuring the PStates</h4>



<p>Here are all the depots, PStates, and query topologies in the module implementing timelines and profiles (repeated from above):</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="577" data-attachment-id="1136" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timelines-diagram-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?fit=1457%2C1282&amp;ssl=1" data-orig-size="1457,1282" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timelines-diagram-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?fit=300%2C264&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?fit=656%2C577&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=656%2C577&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=1024%2C901&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=768%2C676&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=1200%2C1056&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?w=1457&amp;ssl=1 1457w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=1024%2C901&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=768%2C676&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=1200%2C1056&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?w=1457&amp;ssl=1 1457w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timelines-diagram-1.png?resize=656%2C577&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Let‚Äôs look at these PStates in closer detail, as the way they‚Äôre structured and partitioned is extremely interesting. Let‚Äôs start with

<code>$$accountIdToAccount</code>

. This is a map from account ID to profile information, including username, display name, and profile pic. Here‚Äôs a picture of what this PState could look like across four partitions:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="401" data-attachment-id="1137" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/accountidtoaccount/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?fit=786%2C481&amp;ssl=1" data-orig-size="786,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="accountIdToAccount" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?fit=300%2C184&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?fit=656%2C401&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=656%2C401&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?w=786&amp;ssl=1 786w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=300%2C184&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=768%2C470&amp;ssl=1 768w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?w=786&amp;ssl=1 786w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=300%2C184&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=768%2C470&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToAccount.png?resize=656%2C401&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This PState is partitioned by the account ID. When I say ‚Äúpartitioned by‚Äù, I mean how the Mastodon implementation chooses on which partition to store data for any particular account. The most common way to partition a PState is to hash a partitioning key and modulo the hash by the number of partitions. This deterministically chooses a partition for any particular partitioning key, while evenly spreading out data across all partitions (this ‚Äúhash/mod‚Äù technique is used by most distributed databases). For this PState, the partitioning key is the same as the key in the map, the account ID (as you‚Äôll see soon, it doesn‚Äôt have to be).</p>



<p>Next is

<code>$$accountIdToStatuses</code>

. This is a map from account ID to a map from status ID to a list of status content versions. A list of status content versions is stored to capture the edit history of a status, which Mastodon lets you view in its UI. You can visualize this PState like so:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="480" height="554" data-attachment-id="1141" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/accountidtostatuses/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?fit=480%2C554&amp;ssl=1" data-orig-size="480,554" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="accountIdToStatuses" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?fit=260%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?fit=480%2C554&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?resize=480%2C554&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?w=480&amp;ssl=1 480w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?resize=260%2C300&amp;ssl=1 260w" sizes="(max-width: 480px) 100vw, 480px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?w=480&amp;ssl=1 480w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?resize=260%2C300&amp;ssl=1 260w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/accountIdToStatuses.png?resize=480%2C554&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Whenever a status is edited, a new version is prepended to its respective inner list. Since this PState and

<code>$$accountIdToAccount</code>

are in the same module, each partition is colocated on the same thread within the same process. You can visualize this colocation like so:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="656" data-attachment-id="1143" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/colocated-account-pstates/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?fit=1077%2C1078&amp;ssl=1" data-orig-size="1077,1078" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="colocated-account-pstates" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?fit=656%2C656&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=656%2C656&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=768%2C769&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?w=1077&amp;ssl=1 1077w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=768%2C769&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?w=1077&amp;ssl=1 1077w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/colocated-account-pstates.png?resize=656%2C656&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Because of this colocation queries can look at the partitions for both PStates at the same time within the same event instead of needing two roundtrips. You can also have multiple threads per worker process, or multiple worker processes per node.</p>



<p>Now let‚Äôs see how things get really interesting.

<code>$$statusIdToFavoriters</code>

is a map from a status ID to a linked set of account IDs. Here‚Äôs a visualization of this PState:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="439" height="480" data-attachment-id="1145" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/statusidtofavoriters/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?fit=439%2C480&amp;ssl=1" data-orig-size="439,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="statusIdToFavoriters" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?fit=274%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?fit=439%2C480&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?resize=439%2C480&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?w=439&amp;ssl=1 439w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?resize=274%2C300&amp;ssl=1 274w" sizes="(max-width: 439px) 100vw, 439px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?w=439&amp;ssl=1 439w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?resize=274%2C300&amp;ssl=1 274w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/statusIdToFavoriters.png?resize=439%2C480&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Similar to the social graph PStates, this PState is used to compute: the order in which accounts favorited a status (paginated), whether a particular account already favorited a status, and the number of favorites for a status.</p>



<p>What‚Äôs interesting is how this PState is partitioned. It is <strong>not</strong> partitioned by the status ID, which is the key of the map. It is instead partitioned by the account ID of the user who posted that status, which is not even in the map! This same partitioning scheme is used for all other PStates tracking status information, like

<code>$$statusIdToReplies</code>

,

<code>$$statusIdToBoosters</code>

, and

<code>$$statusIdToMuters</code>

. This means all information for a user and all information for that user‚Äôs statuses exist on the same partition, and performing a query to fetch all the information needed to render a status only needs to perform a single roundtrip.</p>



<p>Here‚Äôs a visualization of how all the PStates described could exist in a module deployment:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="528" height="1024" data-attachment-id="1148" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/all-colocated-pstates/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?fit=1074%2C2084&amp;ssl=1" data-orig-size="1074,2084" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="all-colocated-pstates" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?fit=155%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?fit=528%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=528%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=528%2C1024&amp;ssl=1 528w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=155%2C300&amp;ssl=1 155w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=768%2C1490&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=792%2C1536&amp;ssl=1 792w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=1055%2C2048&amp;ssl=1 1055w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?w=1074&amp;ssl=1 1074w" sizes="(max-width: 528px) 100vw, 528px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=528%2C1024&amp;ssl=1 528w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=155%2C300&amp;ssl=1 155w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=768%2C1490&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=792%2C1536&amp;ssl=1 792w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=1055%2C2048&amp;ssl=1 1055w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?w=1074&amp;ssl=1 1074w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/all-colocated-pstates.png?resize=528%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Notice, for example, how status ID ‚Äú3‚Äù for account ID ‚Äú10‚Äù exists both as a subvalue within

<code>$$accountIdToStatuses</code>

and also as a top-level key for the status-specific states on the same partition.</p>



<p>This use case is a great example of the power of Rama‚Äôs integrated approach, achieving incredible efficiency and incredible simplicity. Each of these PStates exists in exactly the perfect shape and partitioning for the use cases it serves.</p>



<h4 id="Home_timelines">Home timelines</h4>



<div><p>Next, let‚Äôs explore how home timelines are stored. Materializing home timelines is by far the most intensive part of the application, since statuses are fanned out to all followers. Since the average fanout on our instance is 403, there are over 400x more writes to home timelines than any of the other PStates involved in processing statuses.</p>
<p>PStates are durable, replicated, high-performance structures. They are easy to reason about and ideal for most use cases. In our initial implementation of Mastodon, we stored home timelines in a PState and it worked fine.</p>
<p>However, writing to the home timelines PState was clearly the overwhelming bottleneck in the application because of the sheer amount of data that had to be written to disk and replicated. We also realized that home timelines are unique in that they are somewhat redundant with other PStates. You can reconstruct a home timeline by looking at the statuses of everyone you follow. This can involve a few hundred PState queries across the cluster, so it isn‚Äôt cheap, but it‚Äôs also not terribly expensive.</p>
<p>As it turns out, what we ended up doing to optimize home timelines is very similar to how Twitter did it for their chronological timelines, and for the exact same reasons. In our revised implementation, we store home timelines in-memory and unreplicated. So instead of needing to persist each timeline write to disk and replicate it to followers, a timeline write is an extremely cheap write to an in-memory buffer. This optimization increased the number of statuses we could process per second by 15x.</p>
<p>Solely storing the timeline in-memory is not sufficient though, as it provides no fault-tolerance in the event of a power loss or other node failure. So a new leader for the partition would not have the timeline info since it‚Äôs unreplicated and not persisted to disk. To solve this, we reconstruct the timeline on read if it‚Äôs missing or incomplete by querying the recent statuses of all follows. This provides the same fault-tolerance as replication, but in a different way.</p>
<p>Implementing fault-tolerance this way is a tradeoff. For the benefit of massively reduced cost on timeline write, sometimes reads will be much more expensive due to the cost of reconstructing lost timelines. This tradeoff is overwhelmingly worth it because timeline writes are way, way more frequent than timeline reads and lost partitions are rare.</p>
<p>Whereas Twitter stores home timelines in a dedicated in-memory database, in Rama they‚Äôre stored in-memory in the same processes executing the ETL for timeline fanout. So instead of having to do network operations, serialization, and deserialization, the reads and writes to home timelines in our implementation are literally just in-memory operations on a hash map. This is dramatically simpler and more efficient than operating a separate in-memory database. The timelines themselves are stored like this:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br></p></td><td><p><span>public</span> <span>static</span> <span>class</span> Timeline <span>{</span><br>
&nbsp; <span>public</span> <span>long</span><span>[</span><span>]</span> buffer<span>;</span><br>
&nbsp; <span>public</span> <span>int</span> startIndex <span>=</span> <span>0</span><span>;</span> <span>// index within buffer that contains oldest timeline element</span><br>
&nbsp; <span>public</span> <span>int</span> numElems <span>=</span> <span>0</span><span>;</span> <span>// number of elements in this timeline</span><br>
<span>}</span></p></td></tr></tbody></table></div>




<div><p>To minimize memory usage and GC pressure, we use a ring buffer and Java primitives to represent each home timeline. The buffer contains pairs of author ID and status ID. The author ID is stored along with the status ID since it is static information that will never change, and materializing it means that information doesn‚Äôt need to be looked up at query time. The home timeline stores the most recent 600 statuses, so the buffer size is 1,200 to accommodate each author ID and status ID pair. The size is fixed since storing full timelines would require a prohibitive amount of memory (the number of statuses times the average number of followers).</p>
<p>Each user utilizes about 10kb of memory to represent their home timeline. For a Twitter-scale deployment of 500M users, that requires about 4.7TB of memory total around the cluster, which is easily achievable.</p>
<p>The in-memory home timelines and other PStates are put together to render a page of a timeline with the following logic:</p>
</div>



<ul>
<li>First, query the in-memory home timeline to fetch a page of

<code>[author ID, statusID]</code>

pairs.</li>
<li>Next, invoke a query topology (a predefined distributed query) that takes in a list of

<code>[author ID, statusID]</code>

pairs and returns all information needed to render each status ‚Äì&nbsp;status content, status stats, and author information. The query topology goes to all partitions containing requested status IDs in parallel and fetches all needed information with colocated PState queries.</li>
</ul>



<h4 id="timeline-fanout">Implementing fanout</h4>



<div><p>So that‚Äôs the story on how timelines are rendered, but how are home timelines and these various PStates computed? If you look back at <a href="#Performance_and_scalability">the performance numbers</a>, you can see our Mastodon implementation has high throughput that scales linearly while also having great, consistent latencies for delivering statuses to follower timelines.</p>
<p>Let‚Äôs focus on how statuses are handled, particularly how timelines are materialized. Whenever someone posts a status, that status must be fanned out to all followers and appended to their home timelines.</p>
<p>The tricky part is dealing with bursty load arising from how unbalanced the social graph can get. In our Mastodon instance, for example, the average fanout is 403, but the most popular user has over 22M followers. 3,500 statuses are posted each second, meaning that every second the system usually needs to perform 1.4M timeline writes to keep up. But if a user with 20M followers posts a status, then the number of timeline writes blows up by 15x to about 21.4M. With a naive implementation this can significantly delay other statuses from reaching follower timelines. And since the latency from posting a status to it reaching follower timelines is one of the most important metrics for this product, that‚Äôs really bad!</p>
<p>This is essentially a problem of fairness. You don‚Äôt want very popular users to hog all the resources on the system whenever they post a status. The key to solving this issue is to limit the amount of resources a status from a popular user can use before allocating those resources to other statuses. The approach we take in our implementation is:</p>
<ul>
<li>For each iteration of timeline processing, fan out each status to at most 64k followers.</li>
<li>If there are more followers left to deliver to for a status, add that status to a PState to continue fanout in the next iteration of processing.</li>
</ul>
<p>With this approach a status from a user with 20M followers will take 312 iterations of processing to complete fanout (about 3 minutes), and fairness is achieved by giving all statuses equal access to resources at any given time. Since the vast majority of users have less than 64k followers, most users will see their statuses delivered in one iteration of processing. Statuses from popular users take longer to deliver to all followers, but that is the tradeoff that has to be made given that the amount of resources is fixed at any given time.</p>
<p>As a side note, Twitter also has special handling for users with lots of followers to address the exact same issue.</p>
<p>With the basic approach understood, let‚Äôs look specifically at how statuses are processed in our implementation to materialize timelines. Here‚Äôs a dataflow diagram showing the logic:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="549" height="1024" data-attachment-id="1284" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-9/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?fit=742%2C1384&amp;ssl=1" data-orig-size="742,1384" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-9" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?fit=161%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?fit=549%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=549%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=549%2C1024&amp;ssl=1 549w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=161%2C300&amp;ssl=1 161w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?w=742&amp;ssl=1 742w" sizes="(max-width: 549px) 100vw, 549px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=549%2C1024&amp;ssl=1 549w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=161%2C300&amp;ssl=1 161w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?w=742&amp;ssl=1 742w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-9.png?resize=549%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>This dataflow diagram is a little bit different than the social graph one, as this ETL is implemented with microbatching while the social graph is implemented with streaming. Streaming processes data directly off of depots as it arrives, while microbatching processes data in small batches. ‚ÄúStart iteration‚Äù in this diagram signifies the beginning of a microbatch. The tradeoffs between streaming and microbatching are too much to cover for this post, but you‚Äôll be able to learn more about that next week when we release the documentation for Rama.</p>
<p>In this diagram you can see all the steps involved in delivering statuses to followers. There are many product rules implemented here, such as: only fan out statuses with the correct visibility, only send replies to followers who also follow the account being replied to, respect ‚Äúshow boost‚Äù settings, and so on.</p>
<p>This dataflow diagram only shows the computation of home timelines, whereas in reality this ETL also handles lists, hashtag timelines, and conversations. Those all work similarly to home timelines and are just additional branches of dataflow computation with slightly differing logic.</p>
</div>



<h4>Processing skew from unbalanced social graph</h4>



<div><p>Fairness issues aren‚Äôt the only problem caused by an unbalanced social graph. Another problem is skew: a naive implementation that handles all fanout for a single user from a single partition will lead to some partitions of a module having a lot more overall work to do than others. Without balanced processing, throughput is lowered since some resources are idle while others are being overwhelmed.</p>
<p>In our Mastodon implementation, we put significant effort into balancing fanout computation regardless of how unbalanced a social graph gets. This is a deep topic, so we will explore this further in a subsequent blog post. The optimizations we did in this area increased throughput by about 15%.</p>
</div>



<h3 id="Personalized_follow_suggestions">Personalized follow suggestions</h3>



<p>Let‚Äôs now look at another part of Mastodon which works completely differently than timelines: personalized follow suggestions. This powers this section on Mastodon:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="608" height="438" data-attachment-id="1159" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/follow-suggestions-screenshot/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?fit=608%2C438&amp;ssl=1" data-orig-size="608,438" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="follow-suggestions-screenshot" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?fit=300%2C216&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?fit=608%2C438&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?resize=608%2C438&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?w=608&amp;ssl=1 608w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?resize=300%2C216&amp;ssl=1 300w" sizes="(max-width: 608px) 100vw, 608px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?w=608&amp;ssl=1 608w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?resize=300%2C216&amp;ssl=1 300w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-screenshot.png?resize=608%2C438&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>Everything about how data is processed and indexed for follow suggestions is different from what we looked at in the previous sections. Taken together, the implementations of timelines, the social graph, and follow suggestions demonstrate how expressive Rama is as a system. This generality is a result of the total arbitrariness with which you can write ETL computations, structure indexes, and compute queries.</p>
<p>Unlike the social graph and timelines, the behavior of personalized follow suggestions could be specified in very different ways. We‚Äôve chosen to determine follow suggestions like so:</p>
<ul>
<li>Rank accounts to suggest based on who‚Äôs most followed by the accounts you already follow</li>
<li>Don‚Äôt suggest accounts you already follow</li>
<li>If this doesn‚Äôt produce enough suggestions (e.g. because you don‚Äôt follow many accounts yet), suggest the most followed accounts on the platform</li>
</ul>
<p>We took a different approach than Mastodon for follow suggestions ‚Äì&nbsp;the <a href="https://docs.joinmastodon.org/methods/suggestions/">API docs</a> describe follow suggestions as ‚ÄúAccounts that are promoted by staff, or that the user has had past positive interactions with, but is not yet following.‚Äù We chose our approach because it‚Äôs much more difficult to implement and thus a better demonstration of Rama. Our implementation of personalized follow suggestions totals 141 lines of code.</p>
</div>



<p>The main PState underlying follow suggestions is called

<code>$$whoToFollow</code>

, a map from account ID to a list of up to 80 account ID suggestions. The interesting part of the follow suggestions implementation is how this PState is computed and maintained.</p>



<div><p>Follow suggestions can‚Äôt be computed fully incrementally, at least not practically. That is, you can‚Äôt receive a new follow or unfollow event and incrementally update all the follow suggestions for affected accounts. Computing follow suggestions is a batch operation that needs to look at everyone an account follows, and everyone they follow, at the same time in a single computation.</p>
<p>With that in mind, there are a few pieces to our implementation. First, everyone‚Äôs follow suggestions are recomputed on a regular basis. The ETL for follow suggestions recomputes the suggestions for 1,280 accounts every 30 seconds. Since there are 100M accounts, this means each account has its suggestions updated every 27 days.</p>
<p>In addition to this, we have special handling for new users. When a new user signs up, you want to provide good follow suggestions as soon as possible in order to increase engagement and increase the chance they‚Äôll continue to use the service. So you don‚Äôt want to wait 27 days to compute personalized suggestions for a new user. At the same time, you can‚Äôt produce good personalized suggestions until the user has followed at least a few accounts. So our implementation tracks milestones which trigger immediate recomputation of follow suggestions: when a user follows 10 accounts and when a user follows 100 accounts.</p>
</div>



<h4>Structuring the PStates</h4>



<p>Here are all the depots, PStates, and query topologies related to the follow suggestions implementation:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="135" data-attachment-id="1161" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/follow-suggestions-diagram/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?fit=1417%2C290&amp;ssl=1" data-orig-size="1417,290" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="follow-suggestions-diagram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?fit=300%2C61&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?fit=656%2C135&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=656%2C135&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=1024%2C210&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=300%2C61&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=768%2C157&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=1200%2C246&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?w=1417&amp;ssl=1 1417w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?w=1312&amp;ssl=1 1312w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=1024%2C210&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=300%2C61&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=768%2C157&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=1200%2C246&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?w=1417&amp;ssl=1 1417w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?w=1312&amp;ssl=1 1312w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-diagram.png?resize=656%2C135&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Notice that this ETL also consumes

<code>followAndBlockAccountDepot</code>

, which is the same depot as consumed by the social graph ETL to produce the social graph PStates. Depots are sources of truth that can be consumed by as many topologies as you need.</p>



<p>Here‚Äôs what each PState for follow suggestions is used for:</p>



<ul>
<li>

<code>$$whoToFollow</code>

: As described above, this stores a list of up to 80 suggestions for each user.</li>
<li>

<code>$$nextId</code>

: This keeps track of the next group of accounts for which to recompute follow suggestions. This stores a

<code>Long</code>

on each partition that points to a key within the colocated

<code>$$followerToFollowees</code>

PState partition.</li>
<li>

<code>$$followCounts</code>

: This is a map from account ID to the number of follow actions that account has taken. This is used to track when a user has passed 10 or 100 follows and trigger an immediate recompute of their follow suggestions.</li>
<li>

<code>$$forceRecomputeUsers</code>

: This is the set of users (a set per partition) that have recently passed the 10 or 100 follows milestone. Accounts chosen for follow suggestion recomputes come from this PState as well as where

<code>$$nextId</code>

is pointing in

<code>$$followerToFollowees</code>

.</li>
<li>

<code>$$topFollowedUsers</code>

: This is a global list of the top followed users on the platform. These are used to supplement a user‚Äôs follow suggestions when not enough are computed via the personalized method.</li>
</ul>



<p>Notice how some of these PStates are not maps at the top-level, which may feel unusual given that pretty much every database that‚Äôs ever existed is map-based (with a ‚Äúkey‚Äù being the central concept to identify a record or row). But just as data structures other than maps are useful for everyday programming, data structures other than maps are useful for backend programming as well.</p>



<h4>Computing follow suggestions</h4>



<p>Let‚Äôs explore in more detail how follow suggestions are recomputed. Unlike the other ETLs we‚Äôve described, this one initiates computations based on time and not on the receipt of data. Every 30 seconds it needs to recompute follow suggestions for a rotating subset of all users and for any users specified in the

<code>$$forceRecomputeUsers</code>

PState.</p>



<div><p>You can think of time as an infinite stream of events, with each event being an instant in time. Rama exposes a special depot for time called a ‚Äútick depot‚Äù which emits events according to a specified frequency.</p>
<p>For follow suggestions, a tick depot is used to trigger processing every 30 seconds. The subsequent computation then uses the PStates described to recompute follow suggestions for a subset of users. The dataflow looks like this:</p>
</div>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="648" height="1013" data-attachment-id="1165" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/follow-suggestions-dataflow/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?fit=648%2C1013&amp;ssl=1" data-orig-size="648,1013" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="follow-suggestions-dataflow" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?fit=192%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?fit=648%2C1013&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?resize=648%2C1013&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?w=648&amp;ssl=1 648w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?resize=192%2C300&amp;ssl=1 192w" sizes="(max-width: 648px) 100vw, 648px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?w=648&amp;ssl=1 648w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?resize=192%2C300&amp;ssl=1 192w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-suggestions-dataflow.png?resize=648%2C1013&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>The key PState is

<code>$$followerToFollowees</code>

. The ETL selects a subset of the keys in that map for processing, and it stores the last key chosen in

<code>$$nextId</code>

. The next iteration will start from that key. When it gets to the end of the PState, it starts over again from the beginning.</p>



<p>The rest of the processing uses

<code>$$followerToFollowees</code>

to fetch follows, fetch the follows of those follows, and aggregate a list of candidates along with how many times each candidate is followed among that subset of users. After filtering out candidates the starting account already follows, the

<code>$$whoToFollow</code>

PState is updated.</p>



<p>Every step of this is done in parallel. So when a subset of users is selected from

<code>$$followerToFollowees</code>

, it‚Äôs actually selecting a subset of users from each partition of that PState.</p>



<p>In between recomputes, a user may have followed some of the users in their list of suggestions. This is handled with a query topology that filters a user‚Äôs follow suggestions to exclude users they already follow.</p>



<h2 id="DevOps_with_Rama">DevOps with Rama</h2>



<p>
Let‚Äôs briefly take a look at how we do DevOps with Rama: managing the deployment, monitoring, and operation of modules in production. Since the steps are the same for all modules, we‚Äôll use as an example how we manage the module handling statuses, timelines, and profiles. The module is implemented in the class

<code>com.rpl.mastodon.modules.Core</code>

and is deployed to a Rama cluster like so:
</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br></p></td><td><p>rama deploy \<br>
--action launch \<br>
--jar target/mastodon.jar \<br>
--module com.rpl.mastodon.modules.Core \<br>
--tasks 128 \<br>
--threads 32 \<br>
--workers 16 \<br>
--replicationFactor 3 \<br>
--configOverrides overrides.yaml</p></td></tr></tbody></table></div>




<div><p>This submits the module and its code to the cluster with the given parallelism, and Rama then launches worker processes around the cluster to run the module. The same cluster is shared by all modules. Once the workers finish starting up, they start reading from depots, executing ETLs, updating PStates, serving query requests, and so on.</p>
<p>The ‚Äúreplication factor‚Äù specifies to how many nodes each depot and PState should replicate its data. Replication happens completely behind the scenes and provides automatic failover in case of failures (e.g. hardware issues). Rama provides very strong guarantees with replication ‚Äì&nbsp;data is not made visible for consumption from depots or PStates until it has been successfully replicated.</p>
</div>



<p>The referenced

<code>overrides.yaml</code>

file has only two lines and just registers with Rama how to serialize/deserialize the custom types used by our implementation (defined using <a href="https://thrift.apache.org/">Thrift</a>, described more below).</p>



<p>After the module finishes launching, the Cluster UI for Rama starts displaying telemetry on the module:</p>



<figure data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/blog.redplanetlabs.com\/2023\/08\/15\/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x\/&quot;}">
<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="373" data-attachment-id="1173" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-modules-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?fit=3290%2C1870&amp;ssl=1" data-orig-size="3290,1870" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-modules" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?fit=656%2C373&amp;ssl=1" data-id="1173" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=656%2C373&amp;ssl=1" alt="" title="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=768%2C437&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1536%2C873&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=2048%2C1164&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=768%2C437&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1536%2C873&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=2048%2C1164&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-modules.png?resize=656%2C373&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="374" data-attachment-id="1174" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-1-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?fit=3280%2C1870&amp;ssl=1" data-orig-size="3280,1870" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?fit=656%2C374&amp;ssl=1" data-id="1174" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=656%2C374&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1024%2C584&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1536%2C876&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=2048%2C1168&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1024%2C584&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1536%2C876&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=2048%2C1168&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-1.png?resize=656%2C374&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="374" data-attachment-id="1179" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-2-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?fit=3286%2C1874&amp;ssl=1" data-orig-size="3286,1874" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?fit=656%2C374&amp;ssl=1" data-id="1179" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=656%2C374&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1024%2C584&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1536%2C876&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=2048%2C1168&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1024%2C584&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1536%2C876&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=2048%2C1168&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-2.png?resize=656%2C374&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="375" data-attachment-id="1177" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-3-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?fit=3278%2C1876&amp;ssl=1" data-orig-size="3278,1876" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?fit=300%2C172&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?fit=656%2C375&amp;ssl=1" data-id="1177" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=656%2C375&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1024%2C586&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=300%2C172&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=768%2C440&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1536%2C879&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=2048%2C1172&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1200%2C687&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1024%2C586&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=300%2C172&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=768%2C440&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1536%2C879&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=2048%2C1172&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=1200%2C687&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-3.png?resize=656%2C375&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="373" data-attachment-id="1178" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-6-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?fit=3288%2C1870&amp;ssl=1" data-orig-size="3288,1870" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-6" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?fit=656%2C373&amp;ssl=1" data-id="1178" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=656%2C373&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=768%2C437&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1536%2C874&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=2048%2C1165&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=768%2C437&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1536%2C874&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=2048%2C1165&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-6.png?resize=656%2C373&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="375" data-attachment-id="1175" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-5-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?fit=3272%2C1874&amp;ssl=1" data-orig-size="3272,1874" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?fit=300%2C172&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?fit=656%2C375&amp;ssl=1" data-id="1175" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=656%2C375&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1024%2C586&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=300%2C172&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=768%2C440&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1536%2C880&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=2048%2C1173&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1200%2C687&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1024%2C586&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=300%2C172&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=768%2C440&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1536%2C880&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=2048%2C1173&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=1200%2C687&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-5.png?resize=656%2C375&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="373" data-attachment-id="1176" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/cluster-ui-telemetry-4-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?fit=3282%2C1870&amp;ssl=1" data-orig-size="3282,1870" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cluster-ui-telemetry-4" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?fit=300%2C171&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?fit=656%2C373&amp;ssl=1" data-id="1176" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=656%2C373&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1024%2C583&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1536%2C875&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=2048%2C1167&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?w=1968&amp;ssl=1 1968w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1024%2C583&amp;ssl=1 1024w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=768%2C438&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1536%2C875&amp;ssl=1 1536w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=2048%2C1167&amp;ssl=1 2048w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=1200%2C684&amp;ssl=1 1200w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?w=1312&amp;ssl=1 1312w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?w=1968&amp;ssl=1 1968w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/cluster-ui-telemetry-4.png?resize=656%2C373&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
</figure>



<div><p>Rama tracks and displays telemetry for the module as a whole, as well as specific telemetry for each topology, depot, and PState. The telemetry is extremely useful for understanding the performance of a module and when it needs to be scaled. Rama uses itself to implement telemetry ‚Äì&nbsp;a built-in module collects telemetry data from all modules into a depot, processes that data with an ETL, and indexes the results into PStates arranged in a time-series structure.</p>
<p>When we want to update the module to add a feature (e.g. add a new PState) or fix a bug, we run a command like the following:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br></p></td><td><p>rama deploy \<br>
--action update \<br>
--jar target/mastodon.jar \<br>
--module com.rpl.mastodon.modules.Core \<br>
--configOverrides overrides.yaml</p></td></tr></tbody></table></div>




<div><p>This launches a carefully coordinated automated procedure to launch new worker processes and handoff responsibility for depots and PStates to the new version of the module. Clients of the module doing depot appends and PState queries don‚Äôt need to be updated and automatically transition themselves to the new module version.</p>
<p>Similarly, when we want to scale the module to have more resources, we run a command like the following:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br></p></td><td><p>rama scaleExecutors \<br>
--module com.rpl.mastodon.modules.Core \<br>
--workers 24</p></td></tr></tbody></table></div>




<div><p>This launches a similar procedure as module update to transition the module to the new version.</p>
<p>And that‚Äôs all there is to DevOps with Rama ‚Äì&nbsp;it‚Äôs just a few commands at the terminal to manage everything. You don‚Äôt need to invest huge amounts of time writing piles of shell scripts to coordinate changes across dozens of systems. Since Rama is such a cohesive, integrated system it‚Äôs able to automate deployment entirely, and it‚Äôs able to provide deep and detailed runtime telemetry without needing to lift a finger.</p>
</div>



<h2 id="Simple_Rama_code_example">Simple Rama code example</h2>



<div><p>Let‚Äôs look at some code! Before diving into the Mastodon code, let‚Äôs look at a simple example of coding with Rama to gain a feeling for what it‚Äôs like.</p>
<p>I‚Äôm not going to explain every last detail in this code ‚Äì&nbsp;the API is so rich that it‚Äôs too much to explain for this post. Instead, I‚Äôll do my best to summarize what the code is doing. In one week we will be releasing all the documentation for Rama, and this includes a six part tutorial that gently introduces everything. We will also be releasing a build of Rama that anyone can download and use. This build will be able to simulate Rama clusters within a single process but will not be able to run distributed clusters. It has the full Rama API and can be used to experiment with Rama. Once we open-source our Mastodon implementation in two weeks, you‚Äôll be able to run it within a single process using this build.</p>
<p>With that said, let‚Äôs look at a simple example. Here‚Äôs the complete definition for a ‚Äúword count module‚Äù, which accepts sentences as input and produces a single PState containing the count of all words in those sentences:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br></p></td><td><div><p><span>public</span> <span>class</span> WordCountModule <span>implements</span> RamaModule <span>{</span><br>
&nbsp; &nbsp; @Override<br>
&nbsp; &nbsp; <span>public</span> <span>void</span> define<span>(</span>Setup setup, Topologies topologies<span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; setup.<span>declareDepot</span><span>(</span><span>"*sentenceDepot"</span>, Depot.<span>random</span><span>(</span><span>)</span><span>)</span><span>;</span></p><p>

&nbsp; &nbsp; &nbsp; &nbsp; StreamTopology wordCount <span>=</span> topologies.<span>stream</span><span>(</span><span>"wordCount"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; wordCount.<span>pstate</span><span>(</span><span>"$$wordCounts"</span>, PState.<span>mapSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a>.<span>class</span>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span><span>)</span><span>)</span><span>;</span></p><p>

&nbsp; &nbsp; &nbsp; &nbsp; wordCount.<span>source</span><span>(</span><span>"*sentenceDepot"</span><span>)</span>.<span>out</span><span>(</span><span>"*sentence"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>each</span><span>(</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> sentence, OutputCollector collector<span>)</span> <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span>for</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> word<span>:</span> sentence.<span>split</span><span>(</span><span>" "</span><span>)</span><span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;collector.<span>emit</span><span>(</span>word<span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span>}</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span>}</span>, <span>"*sentence"</span><span>)</span>.<span>out</span><span>(</span><span>"*word"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>hashPartition</span><span>(</span><span>"*word"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>compoundAgg</span><span>(</span><span>"$$wordCounts"</span>, CompoundAgg.<span>map</span><span>(</span><span>"*word"</span>, Agg.<span>count</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>}</span><br>
<span>}</span></p></div></td></tr></tbody></table></div>




<p>This module has one ETL named

<code>wordCount</code>

, one depot named

<code>*sentenceDepot</code>

, and one PState named

<code>$$wordCounts</code>

. The ETL receives new sentences from the depot, tokenizes those sentences into words, and then updates the counts for those words in the PState. The PState partitions are updated within a few milliseconds of appending a sentence to the depot.</p>



<p>A module implements the interface

<code>RamaModule</code>

that has a single method

<code>define</code>

on it.

<code>setup</code>

is used to declare depots and any dependencies to depots or PStates in other modules, and

<code>topologies</code>

is used to declare all ETL and query topologies.</p>



<p>The first line of

<code>define</code>

declares the depot. Depot names always begin with a

<code>*</code>

. Strings beginning with

<code>*</code>

are interpreted as variables in Rama code, and they can be passed around and used just like variables in any programming language. The second argument

<code>Depot.random()</code>

specifies the partitioning scheme of the depot. In this case the partitioning scheme causes appended sentences to go to a random partition of the depot. When local ordering is important, like for follow and unfollow events, the partitioning scheme would be set appropriately so events for the same entity go to the same partition.</p>



<p>The next line declares the ETL

<code>wordCount</code>

as a streaming topology.</p>



<p>After that is the declaration of the PState

<code>$$wordCounts</code>

. The PState is declared with a schema that specifies what it stores and how it stores it. In this case it‚Äôs just a simple map, but you can specify whatever structure you want here (e.g. a map of subindexed maps of lists of subindexed sets).</p>



<p>Lastly is the definition of the ETL. The line

<code>wordCount.source("*sentenceDepot").out("*sentence")</code>

subscribes the ETL to

<code>*sentenceDepot</code>

and binds any new sentences received to the variable

<code>*sentence</code>

.</p>



<p>The next line tokenizes each sentence into words. Java code is inserted with a lambda to split each sentence on whitespace and emit each word individually as the variable

<code>*word</code>

. Inserting arbitrary Java code into topologies like this is extremely common.</p>



<p>The next line

<code>.hashPartition("*word")</code>

relocates the dataflow to the partition of the module storing the counts for that word. The code before that line and after that line can execute on different machines, and Rama takes care of all the serialization and network transfer involved in moving the computation.</p>



<div><p>Finally, now that the computation is on the correct partition, the last line updates the count for the word in the PState. This PState update is specified in the form of an aggregation template ‚Äì&nbsp;in this case it says it‚Äôs aggregating a map where the key is the word and the value is the count of all events seen for that word.</p>
<p>This is such a basic example that it doesn‚Äôt really do justice to the expressive power of Rama. However, it does demonstrate the general workflow of declaring modules, depots, PStates, and topologies. Some of the functionality not shown here includes: consuming depots/PStates from other modules, query topologies, microbatching, branching/merging, joins, loops, shadowing variables, conditionals, and decomposing code with macros.</p>
<p>Let‚Äôs now take a look at interacting with Rama modules as a client outside the cluster, similar to how you interact with a database using a database client. Here‚Äôs code that connects to a remote cluster, creates handles to the depot and PState of the module, appends some sentences, and then does some PState queries:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br></p></td><td><div><p><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+map"><span>Map</span></a> config <span>=</span> <span>new</span> <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+hashmap"><span>HashMap</span></a><span>(</span><span>)</span><span>;</span><br>
config.<span>put</span><span>(</span><span>"conductor.host"</span>, <span>"1.2.3.4"</span><span>)</span><span>;</span><br>
RamaClusterManager manager <span>=</span> RamaClusterManager.<span>open</span><span>(</span>config<span>)</span><span>;</span><br>
Depot depot <span>=</span> manager.<span>clusterDepot</span><span>(</span><span>"rama.examples.wordcount.WordCountModule"</span>, <span>"*sentenceDepot"</span><span>)</span><span>;</span><br>
depot.<span>append</span><span>(</span><span>"hello world"</span><span>)</span><span>;</span><br>
depot.<span>append</span><span>(</span><span>"hello world again"</span><span>)</span><span>;</span><br>
depot.<span>append</span><span>(</span><span>"say hello to the planet"</span><span>)</span><span>;</span><br>
depot.<span>append</span><span>(</span><span>"red planet labs"</span><span>)</span><span>;</span></p><p>

PState wc <span>=</span> manager.<span>clusterPState</span><span>(</span><span>"rama.examples.wordcount.WordCountModule"</span>, <span>"$$wordCounts"</span><span>)</span><span>;</span><br>
<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'hello' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"hello"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'world' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"world"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'planet' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"planet"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'red' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"red"</span><span>)</span><span>)</span><span>)</span><span>;</span></p></div></td></tr></tbody></table></div>




<p>

<code>RamaClusterManager</code>

is used to connect to a cluster and retrieve handles to depots and PStates. Depots and PStates are identified by their module name (the class name of the module definition) and their name within the module. By default, depot appends block until all colocated streaming topologies have finished processing the appended data. This is why the PState queries can be executed immediately following the depot appends without further coordination.</p>



<p>The PState queries here fetch the values for the specified keys. PStates are queried using Rama‚Äôs ‚ÄúPath‚Äù API, and this example barely scratches the surface of what you can do with paths. They allow you to easily reach into a PState, regardless of its structure, and retrieve precisely what you need ‚Äì&nbsp;whether one value, multiple values, or an aggregation of values. They can also be used for updating PStates within topologies. Mastering paths is one of the keys to mastering Rama development.</p>



<p>Let‚Äôs now take a look at how you would run

<code>WordCountModule</code>

in a unit test environment:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br></p></td><td><div><p><span>public</span> <span>void</span> wordCountTest<span>(</span><span>)</span> <span>throws</span> <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+exception"><span>Exception</span></a> <span>{</span><br>
&nbsp; &nbsp; <span>try</span> <span>(</span>InProcessCluster cluster <span>=</span> InProcessCluster.<span>create</span><span>(</span><span>)</span><span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; cluster.<span>launchModule</span><span>(</span><span>new</span> WordCountModule<span>(</span><span>)</span>, <span>new</span> LaunchConfig<span>(</span><span>4</span>, <span>2</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> moduleName <span>=</span> WordCountModule.<span>class</span>.<span>getName</span><span>(</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; Depot depot <span>=</span> cluster.<span>clusterDepot</span><span>(</span>moduleName, <span>"*sentenceDepot"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; depot.<span>append</span><span>(</span><span>"hello world"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; depot.<span>append</span><span>(</span><span>"hello world again"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; depot.<span>append</span><span>(</span><span>"say hello to the planet"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; depot.<span>append</span><span>(</span><span>"red planet labs"</span><span>)</span><span>;</span></p><p>

&nbsp; &nbsp; &nbsp; &nbsp; PState wc <span>=</span> cluster.<span>clusterPState</span><span>(</span>moduleName, <span>"$$wordCounts"</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'hello' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"hello"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'world' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"world"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'planet' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"planet"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>out</span>.<span>println</span><span>(</span><span>"'red' count: "</span> <span>+</span> wc.<span>selectOne</span><span>(</span>Path.<span>key</span><span>(</span><span>"red"</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>}</span><br>
<span>}</span></p></div></td></tr></tbody></table></div>




<p>Running this code prints:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br></p></td><td><p>'hello' count: 3<br>
'world' count: 2<br>
'planet' count: 2<br>
'red' count: 1</p></td></tr></tbody></table></div>




<p>

<code>InProcessCluster</code>

simulates a Rama cluster completely in-process and is ideal for unit testing modules. Here you can see how

<code>InProcessCluster</code>

is used to launch the module and then fetch depots/PStates just like with

<code>RamaClusterManager</code>

. There‚Äôs no difference in the functionality available with

<code>InProcessCluster</code>

versus a real cluster, and you‚Äôll be able to try out

<code>InProcessCluster</code>

next week when we release the non-production build of Rama.</p>



<h2 id="Sample_code_from_our_Mastodon_implementation">Sample code from our Mastodon implementation</h2>



<div><p>Now let‚Äôs look at some code from our Mastodon implementation. We‚Äôll be looking at bigger code samples in this section utilizing a lot more of the Rama API, so even more than the last section I won‚Äôt be able to explain all the details of the code. I‚Äôll summarize what the key parts are, and you‚Äôll be able to learn all the details next week when we release the documentation.</p>
<p>Please don‚Äôt be too intimidated by this code. There are a lot of concepts and API methods at work here, and no one could possibly understand this code completely at a first glance. This is especially true without the accompanying documentation. I‚Äôm showing this code because it ties together the high-level concepts I‚Äôve discussed in this post by making them real instead of abstract.</p>
</div>



<h3 id="Representing_data">Representing data</h3>



<p>Let‚Äôs start by looking at an example of how data is defined. We chose to represent data using <a href="https://thrift.apache.org/">Thrift</a> since it has a nice schema definition language and produces efficient serialization, but you can just as easily use plain Java objects, <a href="https://protobuf.dev/">Protocol Buffers</a>, or anything else you want. We use Thrift-defined objects in both depots and PStates. Here‚Äôs how statuses are defined:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br></p></td><td><div><p>typedef i64 AccountId<br>
typedef i64 StatusId<br>
typedef i64 Timestamp</p><p>

enum StatusVisibility {<br>
&nbsp; Public = 1,<br>
&nbsp; Unlisted = 2,<br>
&nbsp; Private = 3,<br>
&nbsp; Direct = 4<br>
}</p><p>

enum AttachmentKind {<br>
&nbsp; Image = 1,<br>
&nbsp; Video = 2<br>
}</p><p>

struct StatusPointer {<br>
&nbsp; 1: required AccountId authorId;<br>
&nbsp; 2: required StatusId statusId;<br>
&nbsp; 3: optional Timestamp timestamp;<br>
&nbsp; 4: optional bool shouldExclude;<br>
}</p><p>

struct PollContent {<br>
&nbsp; 1: list&lt;string&gt; choices;<br>
&nbsp; 2: required Timestamp expiration;<br>
&nbsp; 3: required bool multipleChoice;<br>
}</p><p>

struct Attachment {<br>
&nbsp; 1: required AttachmentKind kind;<br>
&nbsp; 2: required string extension;<br>
&nbsp; 3: required string description;<br>
}</p><p>

struct AttachmentWithId {<br>
&nbsp; 1: required string uuid;<br>
&nbsp; 2: required Attachment attachment;<br>
}</p><p>

struct NormalStatusContent {<br>
&nbsp; 1: required string text;<br>
&nbsp; 2: required StatusVisibility visibility;<br>
&nbsp; 3: optional PollContent pollContent;<br>
&nbsp; 4: optional list&lt;AttachmentWithId&gt; attachments;<br>
&nbsp; 5: optional string sensitiveWarning;<br>
}</p><p>

struct ReplyStatusContent {<br>
&nbsp; 1: required string text;<br>
&nbsp; 2: required StatusVisibility visibility;<br>
&nbsp; 3: required StatusPointer parent;<br>
&nbsp; 4: optional PollContent pollContent;<br>
&nbsp; 5: optional list&lt;AttachmentWithId&gt; attachments;<br>
&nbsp; 6: optional string sensitiveWarning;<br>
}</p><p>

struct BoostStatusContent {<br>
&nbsp; 1: required StatusPointer boosted;<br>
}</p><p>

union StatusContent {<br>
&nbsp; 1: NormalStatusContent normal;<br>
&nbsp; 2: ReplyStatusContent reply;<br>
&nbsp; 3: BoostStatusContent boost;<br>
}</p><p>

struct Status {<br>
&nbsp; 1: required AccountId authorId;<br>
&nbsp; 2: required StatusContent content;<br>
&nbsp; 3: required Timestamp timestamp;<br>
&nbsp; 4: optional string remoteUrl;<br>
&nbsp; 5: optional string language;<br>
}</p></div></td></tr></tbody></table></div>




<p>Every type of status, including boosts, replies, and statuses with polls is represented by this definition. Being able to represent your data using normal programming practices, as opposed to restrictive database environments where you can‚Äôt have nested definitions like this, goes a long way in avoiding impedance mismatches and keeping code clean and comprehensible.</p>



<h3 id="Following_hashtags_code">Following hashtags code</h3>



<p>Next, let‚Äôs look at the entire definition of following hashtags (described <a href="#Following_hashtags">earlier</a> in this post). As a reminder, here‚Äôs the dataflow diagram for the following hashtags ETL:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="372" height="472" data-attachment-id="1194" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/follow-hashtags-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?fit=372%2C472&amp;ssl=1" data-orig-size="372,472" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="follow-hashtags-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?fit=236%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?fit=372%2C472&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?resize=372%2C472&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?w=372&amp;ssl=1 372w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?resize=236%2C300&amp;ssl=1 236w" sizes="(max-width: 372px) 100vw, 372px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?w=372&amp;ssl=1 372w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?resize=236%2C300&amp;ssl=1 236w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/follow-hashtags-1.png?resize=372%2C472&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Here‚Äôs the implementation, which is a direct translation of the diagram to code:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br></p></td><td><div><p>setup.<span>declareDepot</span><span>(</span><span>"*followHashtagDepot"</span>, Depot.<span>hashBy</span><span>(</span>ExtractToken.<span>class</span><span>)</span><span>)</span><span>;</span></p><p>

StreamTopology stream <span>=</span> topologies.<span>stream</span><span>(</span><span>"relationshipsStream"</span><span>)</span><span>;</span></p><p>

stream.<span>pstate</span><span>(</span><span>"$$hashtagToFollowers"</span>, PState.<span>mapSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a>.<span>class</span>, PState.<span>setSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span><span>)</span>.<span>subindexed</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span></p><p>

stream.<span>source</span><span>(</span><span>"*followHashtagDepot"</span>, StreamSourceOptions.<span>retryAllAfter</span><span>(</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; .<span>subSource</span><span>(</span><span>"*data"</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SubSource.<span>create</span><span>(</span>FollowHashtag.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*token"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>localTransform</span><span>(</span><span>"$$hashtagToFollowers"</span>, Path.<span>key</span><span>(</span><span>"*token"</span><span>)</span>.<span>voidSetElem</span><span>(</span><span>)</span>.<span>termVal</span><span>(</span><span>"*accountId"</span><span>)</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SubSource.<span>create</span><span>(</span>RemoveFollowHashtag.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*token"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>localTransform</span><span>(</span><span>"$$hashtagToFollowers"</span>, Path.<span>key</span><span>(</span><span>"*token"</span><span>)</span>.<span>setElem</span><span>(</span><span>"*accountId"</span><span>)</span>.<span>termVoid</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span></p></div></td></tr></tbody></table></div>




<p>This is extremely simple.

<code>subSource</code>

branches the dataflow graph based on the type of an object. In this code the object in

<code>*data</code>

can be one of two types, and there is a separate branch of dataflow for each type. When a

<code>FollowHashtag</code>

event is received, that account is added to the set of followers for that hashtag. When a

<code>RemoveFollowHashtag</code>

event is received, that account is removed from the set of followers for that hashtag. Because the nested sets are subindexed, they can efficiently contain hundreds of millions of elements or more.</p>



<p>

<code>extractFields</code>

is a helper function in the Mastodon implementation for extracting fields out of Thrift objects by name and binding them to corresponding Rama variables of the same name. So

<code>extractFields("*data", "*accountId", "*token"))</code>

extracts the fields ‚ÄúaccountId‚Äù and ‚Äútoken‚Äù from the Thrift object in

<code>*data</code>

and binds them to the variables

<code>*accountId</code>

and

<code>*token</code>

.</p>



<p>

<code>extractFields</code>

is implemented as a Rama macro, which is a utility for inserting a snippet of dataflow code into another section of dataflow code. It is a mechanism for code reuse that allows the composition of any dataflow elements: functions, filters, aggregation, partitioning, etc.</p>



<div><p>Unlike word count, this code uses paths instead of aggregators to define the writes to the PStates, which is the same API used to read from PStates. You‚Äôll be able to learn more next week when we release Rama‚Äôs documentation about the differences between aggregators and paths and when to prefer one over the other.</p>
<p>Note that this code defines a parallel computation just like the word count example earlier. The code runs across many nodes to process data off each partition of the depot and update the PState. Any failures (e.g. a node dying) are handled transparently and Rama guarantees all depot data will be fully processed.</p>
</div>



<p>The partitioning is defined at the depot level (

<code>Depot.hashBy(ExtractToken.class)</code>

), so when the ETL begins processing a piece of data, the computation is already located on the partition of the module storing followers for that hashtag. So no further partitioning is needed in the ETL definition.</p>







<p>Next, let‚Äôs look at the entire definition of the social graph as described <a href="#Social_graph">earlier</a>. Here was the dataflow diagram for the social graph ETL:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="704" data-attachment-id="1198" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?fit=1136%2C1219&amp;ssl=1" data-orig-size="1136,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?fit=280%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?fit=656%2C704&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=656%2C704&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=954%2C1024&amp;ssl=1 954w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=280%2C300&amp;ssl=1 280w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=768%2C824&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?w=1136&amp;ssl=1 1136w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=954%2C1024&amp;ssl=1 954w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=280%2C300&amp;ssl=1 280w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=768%2C824&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?w=1136&amp;ssl=1 1136w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1.png?resize=656%2C704&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Like hashtag follows, the social graph implementation is also a direct translation of the diagram to code. Since the code for this is longer, let‚Äôs look at it section by section in the order in which it‚Äôs written. The first part is the declaration of the depots, topology, and PStates:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br></p></td><td><div><p>setup.<span>declareDepot</span><span>(</span><span>"*followAndBlockAccountDepot"</span>, Depot.<span>hashBy</span><span>(</span>ExtractAccountId.<span>class</span><span>)</span><span>)</span><span>;</span><br>
setup.<span>declareDepot</span><span>(</span><span>"*muteAccountDepot"</span>, Depot.<span>hashBy</span><span>(</span>ExtractAccountId.<span>class</span><span>)</span><span>)</span><span>;</span></p><p>

StreamTopology stream <span>=</span> topologies.<span>stream</span><span>(</span><span>"relationshipsStream"</span><span>)</span><span>;</span></p><p>

KeyToLinkedEntitySetPStateGroup accountIdToFollowRequests <span>=</span> <span>new</span> KeyToLinkedEntitySetPStateGroup<span>(</span><span>"$$accountIdToFollowRequests"</span>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, FollowLockedAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; .<span>entityIdFunction</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, req <span>-&gt;</span> <span>(</span><span>(</span>FollowLockedAccount<span>)</span> req<span>)</span>.<span>requesterId</span><span>)</span><br>
&nbsp; &nbsp; .<span>descending</span><span>(</span><span>)</span><span>;</span><br>
accountIdToFollowRequests.<span>declarePStates</span><span>(</span>stream<span>)</span><span>;</span></p><p>

KeyToLinkedEntitySetPStateGroup followerToFollowees <span>=</span> <span>new</span> KeyToLinkedEntitySetPStateGroup<span>(</span><span>"$$followerToFollowees"</span>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, Follower.<span>class</span><span>)</span><br>
&nbsp; &nbsp; .<span>entityIdFunction</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, <span>new</span> ExtractAccountId<span>(</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; .<span>descending</span><span>(</span><span>)</span><span>;</span><br>
KeyToLinkedEntitySetPStateGroup followeeToFollowers <span>=</span> <span>new</span> KeyToLinkedEntitySetPStateGroup<span>(</span><span>"$$followeeToFollowers"</span>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, Follower.<span>class</span><span>)</span><br>
&nbsp; &nbsp; .<span>entityIdFunction</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, <span>new</span> ExtractAccountId<span>(</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; .<span>descending</span><span>(</span><span>)</span><span>;</span><br>
followerToFollowees.<span>declarePStates</span><span>(</span>stream<span>)</span><span>;</span><br>
followeeToFollowers.<span>declarePStates</span><span>(</span>stream<span>)</span><span>;</span></p><p>

stream.<span>pstate</span><span>(</span><span>"$$accountIdToSuppressions"</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; PState.<span>mapSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, PState.<span>fixedKeysSchema</span><span>(</span><span>"muted"</span>, PState.<span>mapSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span>, MuteAccountOptions.<span>class</span><span>)</span>.<span>subindexed</span><span>(</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>"blocked"</span>, PState.<span>setSchema</span><span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>.<span>class</span><span>)</span>.<span>subindexed</span><span>(</span><span>)</span><span>)</span><span>)</span><span>)</span><span>;</span></p></div></td></tr></tbody></table></div>




<p>Note that both hashtag follows and the social graph are part of the same stream topology. The social graph implementation consumes different depots than hashtag follows does, so the code is otherwise completely independent.</p>



<p>The

<code>$$followerToFollowees</code>

and

<code>$$followeeToFollowers</code>

PStates are defined with

<code>KeyToLinkedEntitySetPStateGroup</code>

, which defines the ‚Äúmap to linked set‚Äù data structure abstraction as the composition of multiple, more primitive PStates underneath the hood. Its implementation is only 68 lines of code.</p>



<p>The next part defines the root of processing where the branching occurs at the start of the dataflow diagram:</p>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>stream.<span>source</span><span>(</span><span>"*followAndBlockAccountDepot"</span>, StreamSourceOptions.<span>retryAllAfter</span><span>(</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*initialData"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; .<span>anchor</span><span>(</span><span>"SocialGraphRoot"</span><span>)</span></p></td></tr></tbody></table></div>




<p>As we build up the code for the social graph, let‚Äôs also take a look visually at how the dataflow diagram is filled out. This code starts off the dataflow diagram like this:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="100" height="69" data-attachment-id="1201" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-1-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1-1.png?fit=100%2C69&amp;ssl=1" data-orig-size="100,69" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1-1.png?fit=100%2C69&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1-1.png?fit=100%2C69&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1-1.png?resize=100%2C69&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-1-1.png?resize=100%2C69&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>

<code>anchor</code>

defines a location in a dataflow graph that can later be hooked onto with

<code>hook</code>

. The next section defines the first branch of processing:</p>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>.<span>each</span><span>(</span>Ops.<span>IDENTITY</span>, <span>"*initialData"</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
.<span>anchor</span><span>(</span><span>"Normal"</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="100" height="485" data-attachment-id="1203" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?fit=100%2C485&amp;ssl=1" data-orig-size="100,485" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?fit=62%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?fit=100%2C485&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?resize=100%2C485&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?w=100&amp;ssl=1 100w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?resize=62%2C300&amp;ssl=1 62w" sizes="(max-width: 100px) 100vw, 100px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?w=100&amp;ssl=1 100w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?resize=62%2C300&amp;ssl=1 62w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-2.png?resize=100%2C485&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This branch passes all data through to the anchor ‚ÄúNormal‚Äù, which will later be merged with other branches as you can see in the dataflow diagram.</p>



<p>Rama provides the

<code>Ops</code>

class which has commonly used functions to use within dataflow code. This includes math operations, comparators, and other utilities. Here

<code>Ops.IDENTITY</code>

is used which emits its input unchanged.</p>



<p>The next section defines the branch handling implicit unfollow events for block events:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br></p></td><td><p>.<span>hook</span><span>(</span><span>"SocialGraphRoot"</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_INSTANCE_OF</span>, BlockAccount.<span>class</span>, <span>"*initialData"</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>BlockAccount data, OutputCollector collector<span>)</span> <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; collector.<span>emit</span><span>(</span><span>new</span> RemoveFollowAccount<span>(</span>data.<span>getAccountId</span><span>(</span><span>)</span>, data.<span>getTargetId</span><span>(</span><span>)</span>, data.<span>getTimestamp</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; collector.<span>emit</span><span>(</span><span>new</span> RemoveFollowAccount<span>(</span>data.<span>getTargetId</span><span>(</span><span>)</span>, data.<span>getAccountId</span><span>(</span><span>)</span>, data.<span>getTimestamp</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; collector.<span>emit</span><span>(</span><span>new</span> RejectFollowRequest<span>(</span>data.<span>getAccountId</span><span>(</span><span>)</span>, data.<span>getTargetId</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span><br>
<span>}</span>, <span>"*initialData"</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
.<span>anchor</span><span>(</span><span>"ImplicitUnfollow"</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="248" height="457" data-attachment-id="1206" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-3/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?fit=248%2C457&amp;ssl=1" data-orig-size="248,457" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?fit=163%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?fit=248%2C457&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?resize=248%2C457&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?w=248&amp;ssl=1 248w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?resize=163%2C300&amp;ssl=1 163w" sizes="(max-width: 248px) 100vw, 248px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?w=248&amp;ssl=1 248w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?resize=163%2C300&amp;ssl=1 163w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-3.png?resize=248%2C457&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This uses

<code>hook</code>

to create a branch off the root of processing for this depot that was defined in the previous code section. The

<code>keepTrue</code>

line continues processing on this branch only for block events. It then generates implicit events to unfollow in both directions and remove a follow request if it exists. Lastly, the ‚ÄúImplicitUnfollow‚Äù anchor is declared which will later be used to merge this branch together with ‚ÄúNormal‚Äù and other branches.</p>



<p>The next section defines the branch handling accepting a follow request:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br></p></td><td><p>.<span>hook</span><span>(</span><span>"SocialGraphRoot"</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_INSTANCE_OF</span>, AcceptFollowRequest.<span>class</span>, <span>"*initialData"</span><span>)</span><span>)</span><br>
.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*initialData"</span>, <span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span><br>
.<span>localSelect</span><span>(</span><span>"$$accountIdToFollowRequests"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span><span>)</span>.<span>must</span><span>(</span><span>"*requesterId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*followRequestId"</span><span>)</span><br>
.<span>localSelect</span><span>(</span><span>"$$accountIdToFollowRequestsById"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"*followRequestId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*followRequest"</span><span>)</span><br>
.<span>hashPartition</span><span>(</span><span>"*requesterId"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>AcceptFollowRequest data, FollowLockedAccount req<span>)</span> <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; FollowAccount follow <span>=</span> <span>new</span> FollowAccount<span>(</span>data.<span>getRequesterId</span><span>(</span><span>)</span>, data.<span>getAccountId</span><span>(</span><span>)</span>, data.<span>getTimestamp</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>if</span> <span>(</span>req.<span>isSetShowBoosts</span><span>(</span><span>)</span><span>)</span> follow.<span>setShowBoosts</span><span>(</span>req.<span>showBoosts</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>if</span> <span>(</span>req.<span>isSetNotify</span><span>(</span><span>)</span><span>)</span> follow.<span>setNotify</span><span>(</span>req.<span>notify</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>if</span> <span>(</span>req.<span>isSetLanguages</span><span>(</span><span>)</span><span>)</span> follow.<span>setLanguages</span><span>(</span>req.<span>languages</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>return</span> follow<span>;</span><br>
<span>}</span> , <span>"*initialData"</span>, <span>"*followRequest"</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
.<span>anchor</span><span>(</span><span>"CompleteFollowRequest"</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="378" height="457" data-attachment-id="1209" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-4/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?fit=378%2C457&amp;ssl=1" data-orig-size="378,457" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-4" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?fit=248%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?fit=378%2C457&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?resize=378%2C457&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?w=378&amp;ssl=1 378w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?resize=248%2C300&amp;ssl=1 248w" sizes="(max-width: 378px) 100vw, 378px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?w=378&amp;ssl=1 378w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?resize=248%2C300&amp;ssl=1 248w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-4.png?resize=378%2C457&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This code is structured just like the previous sections by hooking onto the root and then filtering for the data type of interest. Then, this code checks to see if that follow request still exists since a user could retract their follow request at the same time it was accepted. The

<code>must</code>

navigator stops this branch of computation if the follow request no longer exists.</p>



<p>After that, the code generates the implicit

<code>Follow</code>

event which will later perform the actual logic of updating the

<code>$$followerToFollowees</code>

and

<code>$$followeeToFollowers</code>

PStates.</p>



<p>The next section handles follows to a locked account. As a reminder, a locked account requires all followers to be manually approved.</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br></p></td><td><p>.<span>hook</span><span>(</span><span>"SocialGraphRoot"</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_INSTANCE_OF</span>, FollowLockedAccount.<span>class</span>, <span>"*initialData"</span><span>)</span><span>)</span><br>
.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*initialData"</span>, <span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span><br>
.<span>localSelect</span><span>(</span><span>"$$followeeToFollowers"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*existingFollowerId"</span><span>)</span><br>
.<span>ifTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_NOT_NULL</span>, <span>"*existingFollowerId"</span><span>)</span>,<br>
&nbsp; &nbsp;Block.<span>each</span><span>(</span><span>(</span>FollowLockedAccount data<span>)</span> <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; FollowAccount follow <span>=</span> <span>new</span> FollowAccount<span>(</span>data.<span>requesterId</span>, data.<span>accountId</span>, data.<span>timestamp</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span><span>(</span>data.<span>isSetShowBoosts</span><span>(</span><span>)</span><span>)</span> follow.<span>setShowBoosts</span><span>(</span>data.<span>isShowBoosts</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span><span>(</span>data.<span>isSetNotify</span><span>(</span><span>)</span><span>)</span> follow.<span>setNotify</span><span>(</span>data.<span>isNotify</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span><span>(</span>data.<span>isSetLanguages</span><span>(</span><span>)</span><span>)</span> follow.<span>setLanguages</span><span>(</span>data.<span>getLanguages</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>return</span> follow<span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <span>}</span>, <span>"*initialData"</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; .<span>hashPartition</span><span>(</span><span>"*requesterId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; .<span>anchor</span><span>(</span><span>"UpdatePrivateFollow"</span><span>)</span>,<br>
&nbsp; &nbsp;Block.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*initialData"</span>, <span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; .<span>macro</span><span>(</span>accountIdToFollowRequests.<span>addToLinkedSet</span><span>(</span><span>"*accountId"</span>, <span>"*initialData"</span><span>)</span><span>)</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="539" height="1024" data-attachment-id="1211" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-5/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?fit=618%2C1175&amp;ssl=1" data-orig-size="618,1175" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?fit=158%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?fit=539%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=539%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=539%2C1024&amp;ssl=1 539w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=158%2C300&amp;ssl=1 158w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?w=618&amp;ssl=1 618w" sizes="(max-width: 539px) 100vw, 539px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=539%2C1024&amp;ssl=1 539w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=158%2C300&amp;ssl=1 158w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?w=618&amp;ssl=1 618w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-5.png?resize=539%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>When a follow request is done in the UI to a locked account, a

<code>FollowLockedAccount</code>

event is appended to the depot. Otherwise, a

<code>FollowAccount</code>

event is appended.</p>



<p>Follow relationships contain additional information such as whether the follower wants to see boosts from the followee and whether they only want to see statuses in a certain language from the followee (another one of Mastodon‚Äôs features). Updating these settings is done via another

<code>Follow</code>

or

<code>FollowLockedAccount</code>

event.</p>



<p>This code uses

<code>ifTrue</code>

to determine if the follower already follows the followee.

<code>ifTrue</code>

works just like

<code>if</code>

in any programming language, with a ‚Äúthen‚Äù block and an optional ‚Äúelse‚Äù block. If the follow relationship exists, it creates an implicit

<code>FollowAccount</code>

event to update the options on the relationship. The ‚ÄúUpdatePrivateFollow‚Äù anchor is used later to merge that branch just like the previous sections.</p>



<div><p>If the follow relationship does not already exist, then the PState tracking follow requests is updated.</p>
<p>The next section merges the prior branches together and begins processing for the rest of the events, whether they came directly off the depot or were created implicitly by one of the branches:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>.<span>unify</span><span>(</span><span>"Normal"</span>, <span>"ImplicitUnfollow"</span>, <span>"CompleteFollowRequest"</span>, <span>"UpdatePrivateFollow"</span><span>)</span><br>
.<span>subSource</span><span>(</span><span>"*data"</span>,</p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="512" height="1024" data-attachment-id="1213" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-6/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?fit=587%2C1175&amp;ssl=1" data-orig-size="587,1175" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-6" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?fit=150%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?fit=512%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=512%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=512%2C1024&amp;ssl=1 512w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=150%2C300&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?w=587&amp;ssl=1 587w" sizes="(max-width: 512px) 100vw, 512px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=512%2C1024&amp;ssl=1 512w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=150%2C300&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?w=587&amp;ssl=1 587w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-6.png?resize=512%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>

<code>unify</code>

merges the specified branches together so they share subsequent computation. Any variables that are in scope in all specified branches are in scope in the code following the

<code>unify</code>

call.</p>



<p>The

<code>subSource</code>

call dispatches subsequent code on the type of the object in

<code>*data</code>

. The following code defines the handling for

<code>FollowAccount</code>

events:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br></p></td><td><p>SubSource.<span>create</span><span>(</span>FollowAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span>, <span>"*followerSharedInboxUrl"</span>, <span>"*showBoosts"</span>, <span>"*notify"</span>, <span>"*languages"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$followerToFollowees"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span><span>)</span>.<span>view</span><span>(</span>Ops.<span>SIZE</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*followeeCount"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>LESS_THAN</span>, <span>"*followeeCount"</span>, relationshipCountLimit<span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$followerToFollowees"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"*targetId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*followeeId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$followerToFolloweesById"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"*followeeId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*existingFollowee"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>each</span><span>(</span>Relationships<span>::</span>makeFollower, <span>"*targetId"</span>, <span>"*showBoosts"</span>, <span>"*languages"</span>, <span>"*followerSharedInboxUrl"</span>, <span>"*existingFollowee"</span><span>)</span>.<span>out</span><span>(</span><span>"*followee"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>followerToFollowees.<span>addToLinkedSet</span><span>(</span><span>"*accountId"</span>, <span>"*followee"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>hashPartition</span><span>(</span><span>"*targetId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$followeeToFollowers"</span>, Path.<span>key</span><span>(</span><span>"*targetId"</span>, <span>"*accountId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*followerId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$followeeToFollowersById"</span>, Path.<span>key</span><span>(</span><span>"*targetId"</span>, <span>"*followerId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*existingFollower"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>each</span><span>(</span>Relationships<span>::</span>makeFollower, <span>"*accountId"</span>, <span>"*showBoosts"</span>, <span>"*languages"</span>, <span>"*followerSharedInboxUrl"</span>, <span>"*existingFollower"</span><span>)</span>.<span>out</span><span>(</span><span>"*follower"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>followeeToFollowers.<span>addToLinkedSet</span><span>(</span><span>"*targetId"</span>, <span>"*follower"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>accountIdToFollowRequests.<span>removeFromLinkedSetByEntityId</span><span>(</span><span>"*targetId"</span>, <span>"*accountId"</span><span>)</span><span>)</span>,</p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="974" data-attachment-id="1215" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-7/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?fit=844%2C1252&amp;ssl=1" data-orig-size="844,1252" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-7" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?fit=202%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?fit=656%2C974&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=656%2C974&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=690%2C1024&amp;ssl=1 690w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=202%2C300&amp;ssl=1 202w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=768%2C1139&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?w=844&amp;ssl=1 844w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=690%2C1024&amp;ssl=1 690w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=202%2C300&amp;ssl=1 202w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=768%2C1139&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?w=844&amp;ssl=1 844w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-7.png?resize=656%2C974&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This code adds the relationship to the

<code>$$followerToFollowees</code>

and

<code>$$followerToFollowees</code>

PStates. If the relationship already exists, it updates the options on the relationship. It also removes any corresponding follow request from

<code>$$accountIdToFollowRequests</code>

if it exists.</p>



<p>

<code>relationshipCountLimit</code>

puts an upper limit on the number of follows someone can have and is set to a very conservative limit of 100,000. It exists to prevent abuse of the system.</p>



<p>The next section handles

<code>RemoveFollowAccount</code>

events:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br></p></td><td><p>SubSource.<span>create</span><span>(</span>RemoveFollowAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span>, <span>"*followerSharedInboxUrl"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>hashPartition</span><span>(</span><span>"*accountId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>followerToFollowees.<span>removeFromLinkedSetByEntityId</span><span>(</span><span>"*accountId"</span>, <span>"*targetId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>hashPartition</span><span>(</span><span>"*targetId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>followeeToFollowers.<span>removeFromLinkedSetByEntityId</span><span>(</span><span>"*targetId"</span>, <span>"*accountId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>accountIdToFollowRequests.<span>removeFromLinkedSetByEntityId</span><span>(</span><span>"*targetId"</span>, <span>"*accountId"</span><span>)</span><span>)</span>,</p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="960" data-attachment-id="1218" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-8/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?fit=833%2C1219&amp;ssl=1" data-orig-size="833,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?fit=205%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?fit=656%2C960&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=656%2C960&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=700%2C1024&amp;ssl=1 700w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=205%2C300&amp;ssl=1 205w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=768%2C1124&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?w=833&amp;ssl=1 833w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=700%2C1024&amp;ssl=1 700w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=205%2C300&amp;ssl=1 205w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=768%2C1124&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?w=833&amp;ssl=1 833w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-8.png?resize=656%2C960&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<div><p>This code just removes the relationship from all relevant PStates.</p>
<p>Here is the next section:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br></p></td><td><div><p>SubSource.<span>create</span><span>(</span>FollowLockedAccount.<span>class</span><span>)</span>,</p><p>

SubSource.<span>create</span><span>(</span>AcceptFollowRequest.<span>class</span><span>)</span>,</p></div></td></tr></tbody></table></div>




<p>This handles events coming off the depot that were handled already in one of the top-level branches we already looked at.

<code>subSource</code>

requires every data type it sees to have a handler, so this code says to do nothing for those types.</p>



<p>The next section handles

<code>RejectFollowRequest</code>

:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br></p></td><td><p>SubSource.<span>create</span><span>(</span>RejectFollowRequest.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>accountIdToFollowRequests.<span>removeFromLinkedSetByEntityId</span><span>(</span><span>"*accountId"</span>, <span>"*requesterId"</span><span>)</span><span>)</span>,</p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="960" data-attachment-id="1221" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-9/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?fit=833%2C1219&amp;ssl=1" data-orig-size="833,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-9" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?fit=205%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?fit=656%2C960&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=656%2C960&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=700%2C1024&amp;ssl=1 700w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=205%2C300&amp;ssl=1 205w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=768%2C1124&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?w=833&amp;ssl=1 833w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=700%2C1024&amp;ssl=1 700w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=205%2C300&amp;ssl=1 205w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=768%2C1124&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?w=833&amp;ssl=1 833w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-9.png?resize=656%2C960&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This just removes the follow request from the PState.</p>



<p>The next section handles

<code>BlockAccount</code>

:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br></p></td><td><p>SubSource.<span>create</span><span>(</span>BlockAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span>, <span>"*timestamp"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localSelect</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"blocked"</span><span>)</span>.<span>view</span><span>(</span>Ops.<span>SIZE</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*blockeeCount"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>LESS_THAN</span>, <span>"*blockeeCount"</span>, relationshipCountLimit<span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localTransform</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"blocked"</span><span>)</span>.<span>voidSetElem</span><span>(</span><span>)</span>.<span>termVal</span><span>(</span><span>"*targetId"</span><span>)</span><span>)</span>,</p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="914" data-attachment-id="1223" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-10/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?fit=875%2C1219&amp;ssl=1" data-orig-size="875,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-10" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?fit=215%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?fit=656%2C914&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=656%2C914&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=735%2C1024&amp;ssl=1 735w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=215%2C300&amp;ssl=1 215w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=768%2C1070&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?w=875&amp;ssl=1 875w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=735%2C1024&amp;ssl=1 735w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=215%2C300&amp;ssl=1 215w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=768%2C1070&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?w=875&amp;ssl=1 875w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-10.png?resize=656%2C914&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>

<code>BlockAccount</code>

was handled in one of the initial branches to generate the implicit unfollows between the two accounts. Here,

<code>BlockAccount</code>

is handled again to record the block relationship in the

<code>$$accountIdToSuppressions</code>

PState.</p>



<p>The next section handles

<code>RemoveBlockAccount</code>

events:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br></p></td><td><p>SubSource.<span>create</span><span>(</span>RemoveBlockAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localTransform</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"blocked"</span><span>)</span>.<span>setElem</span><span>(</span><span>"*targetId"</span><span>)</span>.<span>termVoid</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="914" data-attachment-id="1226" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-11/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?fit=875%2C1219&amp;ssl=1" data-orig-size="875,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-11" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?fit=215%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?fit=656%2C914&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=656%2C914&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=735%2C1024&amp;ssl=1 735w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=215%2C300&amp;ssl=1 215w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=768%2C1070&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?w=875&amp;ssl=1 875w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=735%2C1024&amp;ssl=1 735w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=215%2C300&amp;ssl=1 215w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=768%2C1070&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?w=875&amp;ssl=1 875w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-11.png?resize=656%2C914&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>All this does is remove the block relationship from the

<code>$$accountIdToSuppressions</code>

PState.</p>



<p>That‚Äôs all the code for handling social graph updates from events on

<code>followAndBlockAccountDepot</code>

. The next section contains all the logic for handling events from

<code>muteAccountDepot</code>

:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br></p></td><td><p>stream.<span>source</span><span>(</span><span>"*muteAccountDepot"</span>, StreamSourceOptions.<span>retryAllAfter</span><span>(</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; .<span>subSource</span><span>(</span><span>"*data"</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SubSource.<span>create</span><span>(</span>MuteAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span>, <span>"*options"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>localSelect</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"muted"</span><span>)</span>.<span>view</span><span>(</span>Ops.<span>SIZE</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*muteeCount"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>LESS_THAN</span>, <span>"*muteeCount"</span>, relationshipCountLimit<span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>localTransform</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"muted"</span>, <span>"*targetId"</span><span>)</span>.<span>termVal</span><span>(</span><span>"*options"</span><span>)</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SubSource.<span>create</span><span>(</span>RemoveMuteAccount.<span>class</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*accountId"</span>, <span>"*targetId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>localTransform</span><span>(</span><span>"$$accountIdToSuppressions"</span>, Path.<span>key</span><span>(</span><span>"*accountId"</span>, <span>"muted"</span>, <span>"*targetId"</span><span>)</span>.<span>termVoid</span><span>(</span><span>)</span><span>)</span><span>)</span><span>;</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="656" height="685" data-attachment-id="1229" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/social-graph-12/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?fit=1167%2C1219&amp;ssl=1" data-orig-size="1167,1219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="social-graph-12" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?fit=287%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?fit=656%2C685&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=656%2C685&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=980%2C1024&amp;ssl=1 980w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=287%2C300&amp;ssl=1 287w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=768%2C802&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?w=1167&amp;ssl=1 1167w" sizes="(max-width: 656px) 100vw, 656px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=980%2C1024&amp;ssl=1 980w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=287%2C300&amp;ssl=1 287w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=768%2C802&amp;ssl=1 768w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?w=1167&amp;ssl=1 1167w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/social-graph-12.png?resize=656%2C685&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This is really simple, as all it does is add the mute relationship for

<code>MuteAccount</code>

events and remove the relationship for

<code>RemoveMuteAccount</code>

events.</p>



<p>That‚Äôs the complete implementation of the social graph! As you can see, it‚Äôs expressed exactly as you saw in the dataflow diagram with branches, merges, and dispatching on the types of events.</p>



<p>It‚Äôs worth noting that dataflow code compiles to efficient bytecode when deployed, as efficient as regular Java code. So Rama variables like

<code>*accountId</code>

and

<code>*targetId</code>

become actual variables in the generated bytecode.</p>



<div><p>The code for this ETL is also a great example of why it‚Äôs so beneficial to interact with your data layer with an API in a general-purpose language instead of a custom language (like SQL). This code makes use of normal programming practices to factor out reusable functionality or to separate code into separate functions to make it easier to read. This code also demonstrates how easy it is to intermix logic written in Java with logic written in Rama‚Äôs dataflow API. Method references, lambdas, and macros are facilities for combining the two.</p>
<p>Let‚Äôs look at some of the Mastodon API implementation related to the social graph so you can see how you interact with a Rama cluster to serve the frontend. Here‚Äôs how a new unfollow event is added:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br></p></td><td><p><span>public</span> CompletableFuture<span>&lt;</span>Boolean<span>&gt;</span> postRemoveFollowAccount<span>(</span><span>long</span> followerId, <span>long</span> followeeId, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> sharedInboxUrl<span>)</span> <span>{</span><br>
&nbsp; &nbsp; RemoveFollowAccount removeFollowAccount <span>=</span> <span>new</span> RemoveFollowAccount<span>(</span>followerId, followeeId, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>currentTimeMillis</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>if</span> <span>(</span>sharedInboxUrl <span>!=</span> <span>null</span><span>)</span> removeFollowAccount.<span>setFollowerSharedInboxUrl</span><span>(</span>sharedInboxUrl<span>)</span><span>;</span><br>
&nbsp; &nbsp; <span>return</span> followAndBlockAccountDepot.<span>appendAsync</span><span>(</span>removeFollowAccount<span>)</span>.<span>thenApply</span><span>(</span>res <span>-&gt;</span> <span>true</span><span>)</span><span>;</span><br>
<span>}</span></p></td></tr></tbody></table></div>




<p>This uses the async API for depots to append unfollow data to

<code>followAndBlockAccountDepot</code>

. Rama‚Äôs async API is used almost exclusively in the Mastodon API implementation so as not to block any threads (which would be an inefficient use of resources).</p>



<p>Here‚Äôs how a follow event is handled, conditioning the type of data appended depending on if the account is locked or not:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br></p></td><td><p><span>public</span> CompletableFuture<span>&lt;</span>Boolean<span>&gt;</span> postFollowAccount<span>(</span><span>long</span> followerId, <span>long</span> followeeId, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> sharedInboxUrl, PostFollow params<span>)</span> <span>{</span><br>
&nbsp; &nbsp; <span>return</span> getAccountWithId<span>(</span>followeeId<span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; .<span>thenCompose</span><span>(</span><span>(</span>followee<span>)</span> <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>followee <span>!=</span> <span>null</span> <span>&amp;&amp;</span> followee.<span>account</span> <span>!=</span> <span>null</span> <span>&amp;&amp;</span> followee.<span>account</span>.<span>locked</span><span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; FollowLockedAccount req <span>=</span> <span>new</span> FollowLockedAccount<span>(</span>followeeId, followerId, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>currentTimeMillis</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params <span>!=</span> <span>null</span><span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>reblogs</span> <span>!=</span> <span>null</span><span>)</span> req.<span>setShowBoosts</span><span>(</span>params.<span>reblogs</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>notify</span> <span>!=</span> <span>null</span><span>)</span> req.<span>setNotify</span><span>(</span>params.<span>notify</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>languages</span> <span>!=</span> <span>null</span><span>)</span> req.<span>setLanguages</span><span>(</span>params.<span>languages</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>}</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>return</span> followAndBlockAccountDepot.<span>appendAsync</span><span>(</span>req<span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>}</span> <span>else</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; FollowAccount req <span>=</span> <span>new</span> FollowAccount<span>(</span>followerId, followeeId, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+system"><span>System</span></a>.<span>currentTimeMillis</span><span>(</span><span>)</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params <span>!=</span> <span>null</span><span>)</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>reblogs</span> <span>!=</span> <span>null</span> <span>)</span> req.<span>setShowBoosts</span><span>(</span>params.<span>reblogs</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>notify</span> <span>!=</span> <span>null</span><span>)</span> req.<span>setNotify</span><span>(</span>params.<span>notify</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>params.<span>languages</span> <span>!=</span> <span>null</span><span>)</span> req.<span>setLanguages</span><span>(</span>params.<span>languages</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>}</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>sharedInboxUrl <span>!=</span> <span>null</span><span>)</span> req.<span>setFollowerSharedInboxUrl</span><span>(</span>sharedInboxUrl<span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>return</span> followAndBlockAccountDepot.<span>appendAsync</span><span>(</span>req<span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>}</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; <span>}</span><span>)</span>.<span>thenApply</span><span>(</span>res <span>-&gt;</span> <span>true</span><span>)</span><span>;</span><br>
<span>}</span></p></td></tr></tbody></table></div>




<p>This first calls the helper function

<code>getAccountWithId</code>

which uses a query topology to get all information about that account. If the account is locked, a

<code>FollowLockedAccount</code>

is appended with any options appropriately set. Otherwise, a

<code>FollowAccount</code>

events is appended.</p>



<p>The helper function

<code>getAccountWithId</code>

is implemented like this:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br></p></td><td><div><p><span>public</span> CompletableFuture<span>&lt;</span>AccountWithId<span>&gt;</span> getAccountWithId<span>(</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a> requestAccountIdMaybe, <span>long</span> accountId<span>)</span> <span>{</span><br>
&nbsp; &nbsp; <span>return</span> getAccountsFromAccountIds.<span>invokeAsync</span><span>(</span>requestAccountIdMaybe, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+arrays"><span>Arrays</span></a>.<span>asList</span><span>(</span>accountId<span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .<span>thenApply</span><span>(</span>accounts <span>-&gt;</span> <span>{</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>if</span> <span>(</span>accounts.<span>size</span><span>(</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>null</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>return</span> accounts.<span>get</span><span>(</span><span>0</span><span>)</span><span>;</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span>}</span><span>)</span><span>;</span><br>
<span>}</span></p><p>

<span>public</span> CompletableFuture<span>&lt;</span>AccountWithId<span>&gt;</span> getAccountWithId<span>(</span><span>long</span> accountId<span>)</span> <span>{</span><br>
&nbsp; &nbsp; <span>return</span> <span>this</span>.<span>getAccountWithId</span><span>(</span><span>null</span>, accountId<span>)</span><span>;</span><br>
<span>}</span></p></div></td></tr></tbody></table></div>




<p>The query topology can optionally be invoked with a ‚Äúrequesting account ID‚Äù, because in some circumstances an account should not be visible to another account (e.g. the requesting account is blocked by that user). In this case, it just needs to check if the account is locked or not so it passes

<code>null</code>

for the requesting account ID. The query topology client is fetched like so:</p>




<div><table><tbody><tr><td><p>1<br></p></td><td><p>QueryTopologyClient<span>&lt;</span>List<span>&gt;</span> getAccountsFromAccountIds <span>=</span> cluster.<span>clusterQuery</span><span>(</span><span>"com.rpl.mastodon.modules.Core"</span>, <span>"getAccountsFromAccountIds"</span><span>)</span><span>;</span></p></td></tr></tbody></table></div>




<p>As you can see, a query topology client is fetched just like how you fetch a handle to a depot or PState. Invoking a query topology is like invoking a regular function ‚Äì&nbsp;you pass it some arguments and you get a result back. Unlike a regular function, a query topology executes on a cluster across potentially many nodes. Here the result is received asynchronously, but you can also do a blocking call with

<code>invoke</code>

.</p>



<h3 id="Timeline_fanout_code">Timeline fanout code</h3>



<p>Lastly, let‚Äôs look at the code implementing timeline fanout, as described <a href="#timeline-fanout">earlier</a>. This implementation is only 51 lines of code. As a reminder, here‚Äôs the dataflow diagram for timeline fanout:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="549" height="1024" data-attachment-id="1285" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-10/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?fit=742%2C1384&amp;ssl=1" data-orig-size="742,1384" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-10" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?fit=161%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?fit=549%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=549%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=549%2C1024&amp;ssl=1 549w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=161%2C300&amp;ssl=1 161w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?w=742&amp;ssl=1 742w" sizes="(max-width: 549px) 100vw, 549px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=549%2C1024&amp;ssl=1 549w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=161%2C300&amp;ssl=1 161w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?w=742&amp;ssl=1 742w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-10.png?resize=549%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Once again, since this is a longer piece of code let‚Äôs look at it section by section. Let‚Äôs start with the subscription to the depot containing all the statuses:</p>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>fan.<span>source</span><span>(</span><span>"*statusWithIdDepot"</span><span>)</span>.<span>out</span><span>(</span><span>"*microbatch"</span><span>)</span><br>
&nbsp; &nbsp;.<span>anchor</span><span>(</span><span>"FanoutRoot"</span><span>)</span></p></td></tr></tbody></table></div>




<p>Like in the social graph example, let‚Äôs see visually how the dataflow diagram gets filled out. This code starts off the dataflow diagram like this:</p>



<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="83" height="68" data-attachment-id="1239" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-1-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-1-1.png?fit=83%2C68&amp;ssl=1" data-orig-size="83,68" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-1-1.png?fit=83%2C68&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-1-1.png?fit=83%2C68&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-1-1.png?resize=83%2C68&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-1-1.png?resize=83%2C68&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Unlike the previous examples, this topology is implemented with microbatching. Microbatching guarantees exactly-once processing semantics even in the case of failures. That is, even if there are node or network outages and computation needs to be retried, the resulting PState updates will be as if each depot record was processed exactly once.</p>



<p>The variable

<code>*microbatch</code>

represents a batch of data across all partitions of the depot. This code simply binds that variable and marks the root of computation with the label ‚ÄúFanoutRoot‚Äù. As you can see in the dataflow diagram, there are two branches off the root of processing.</p>



<p>The next section implements the first branch, which handles continuing fanout for statuses with too many followers from the previous iteration:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br></p></td><td><p>.<span>allPartition</span><span>(</span><span>)</span><br>
.<span>localSelect</span><span>(</span><span>"$$statusIdToLocalFollowerFanouts"</span>, Path.<span>all</span><span>(</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*keyAndVal"</span><span>)</span><br>
.<span>each</span><span>(</span>Ops.<span>EXPAND</span>, <span>"*keyAndVal"</span><span>)</span>.<span>out</span><span>(</span><span>"*statusId"</span>, <span>"*followerFanouts"</span><span>)</span><br>
.<span>localTransform</span><span>(</span><span>"$$statusIdToLocalFollowerFanouts"</span>, Path.<span>key</span><span>(</span><span>"*statusId"</span><span>)</span>.<span>termVoid</span><span>(</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span>Ops.<span>EXPLODE</span>, <span>"*followerFanouts"</span><span>)</span>.<span>out</span><span>(</span><span>"*followerFanout"</span><span>)</span><br>
.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*followerFanout"</span>, <span>"*authorId"</span>, <span>"*nextIndex"</span>, <span>"*fanoutAction"</span>, <span>"*status"</span>, <span>"*task"</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span>FanoutAction<span>::</span>getValue, <span>"*fanoutAction"</span><span>)</span>.<span>out</span><span>(</span><span>"*fanoutActionValue"</span><span>)</span><br>
.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*status"</span>, <span>"*content"</span>, <span>"*language"</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>RamaFunction2<span>&lt;</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, StatusPointer<span>&gt;</span><span>)</span> StatusPointer<span>::</span><span>new</span>, <span>"*authorId"</span>, <span>"*statusId"</span><span>)</span>.<span>out</span><span>(</span><span>"*statusPointer"</span><span>)</span><br>
.<span>directPartition</span><span>(</span><span>"$$partitionedFollowers"</span>, <span>"*task"</span><span>)</span><br>
.<span>anchor</span><span>(</span><span>"LocalFollowerFanoutContinue"</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="331" height="258" data-attachment-id="1241" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-2/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?fit=331%2C258&amp;ssl=1" data-orig-size="331,258" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?fit=300%2C234&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?fit=331%2C258&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?resize=331%2C258&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?w=331&amp;ssl=1 331w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?resize=300%2C234&amp;ssl=1 300w" sizes="(max-width: 331px) 100vw, 331px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?w=331&amp;ssl=1 331w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?resize=300%2C234&amp;ssl=1 300w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-2.png?resize=331%2C258&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Whenever a status has too many followers for one iteration of fanout, it is added to the

<code>$$statusIdToLocalFollowerFanouts</code>

PState. This PState is a map from status ID to the type

<code>FollowerFanout</code>

, which contains the information needed to continue fanout starting with the next unhanded follower. As you can see in this code, it reads everything from that PState using

<code>Path.all()</code>

and then deletes everything from that PState.</p>



<p>As you‚Äôll see later, adding to

<code>$$statusIdToLocalFollowerFanouts</code>

is done on whatever partition followers were read for that status. So this code uses

<code>allPartition</code>

to access every partition of the PState.

<code>allPartition</code>

is a partitioner like

<code>hashPartition</code>

, except instead of the subsequent code executing on one partition, the subsequent code executes on all partitions. This allows the ETL to fetch all statuses that required continued fanout from the last iteration. You have to be careful when using

<code>allPartition</code>

as you can create non-scalable topologies if you were to use it for every piece of data on a high throughput depot. In this case

<code>allPartition</code>

is used just once per iteration, so it doesn‚Äôt affect the scalability of the topology.</p>



<p>At the end of this block of code is some handling related to

<code>$$partitionedFollowers</code>

. We didn‚Äôt mention this PState in the earlier discussion of fanout, but it‚Äôs an additional optimization to balance load for handling of users with large amounts of followers. In short, this is an additional view of the social graph where users with more than 1,000 followers have their followers spread among multiple partitions of this PState. This balances the load of processing for fanout by reducing variance among partitions. We will be publishing another blog post in the future exploring this optimization and others.</p>



<p>The next section begins the other branch of processing at the root of the dataflow diagram:</p>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>.<span>hook</span><span>(</span><span>"FanoutRoot"</span><span>)</span><br>
.<span>explodeMicrobatch</span><span>(</span><span>"*microbatch"</span><span>)</span>.<span>out</span><span>(</span><span>"*data"</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="537" height="263" data-attachment-id="1244" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-3/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?fit=537%2C263&amp;ssl=1" data-orig-size="537,263" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?fit=300%2C147&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?fit=537%2C263&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?resize=537%2C263&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?w=537&amp;ssl=1 537w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?resize=300%2C147&amp;ssl=1 300w" sizes="(max-width: 537px) 100vw, 537px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?w=537&amp;ssl=1 537w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?resize=300%2C147&amp;ssl=1 300w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-3.png?resize=537%2C263&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This creates the branch and reads all new statuses for this iteration from the microbatch.

<code>explodeMicrobatch</code>

here reads all data from the microbatch across all partitions and binds each piece of data to the variable

<code>*data</code>

. This operation emits across all partitions of the module.</p>



<p>The next section begins processing of new statuses:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br></p></td><td><p>.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*data"</span>, <span>"*statusId"</span>, <span>"*status"</span><span>)</span><span>)</span><br>
.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*status"</span>, <span>"*authorId"</span>, <span>"*content"</span>, <span>"*language"</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span>MastodonHelpers<span>::</span>getStatusVisibility, <span>"*status"</span><span>)</span>.<span>out</span><span>(</span><span>"*visibility"</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>NOT_EQUAL</span>, <span>"*visibility"</span>, StatusVisibility.<span>Direct</span><span>)</span><span>)</span><br>
.<span>each</span><span>(</span>Ops.<span>IDENTITY</span>, <span>-</span>1L<span>)</span>.<span>out</span><span>(</span><span>"*nextIndex"</span><span>)</span><br>
.<span>each</span><span>(</span>Ops.<span>IDENTITY</span>, FanoutAction.<span>Add</span>.<span>getValue</span><span>(</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*fanoutActionValue"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>RamaFunction2<span>&lt;</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, StatusPointer<span>&gt;</span><span>)</span> StatusPointer<span>::</span><span>new</span>, <span>"*authorId"</span>, <span>"*statusId"</span><span>)</span>.<span>out</span><span>(</span><span>"*statusPointer"</span><span>)</span><br>
.<span>each</span><span>(</span>HomeTimelines<span>::</span>addTimelineItem, <span>"*homeTimelines"</span>, <span>"*authorId"</span>, <span>"*statusPointer"</span>, <span>new</span> Expr<span>(</span>Ops.<span>CURRENT_MICROBATCH_ID</span><span>)</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="547" height="1024" data-attachment-id="1287" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-4-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?fit=733%2C1372&amp;ssl=1" data-orig-size="733,1372" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-4-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?fit=160%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?fit=547%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=547%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=547%2C1024&amp;ssl=1 547w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=160%2C300&amp;ssl=1 160w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?w=733&amp;ssl=1 733w" sizes="(max-width: 547px) 100vw, 547px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=547%2C1024&amp;ssl=1 547w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=160%2C300&amp;ssl=1 160w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?w=733&amp;ssl=1 733w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-4-1.png?resize=547%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This code specifies to only perform fanout for statuses with visibility other than

<code>Direct</code>

, which is for direct messages and handled elsewhere. It then adds the status to the author‚Äôs own home timeline, which implements self-fanout.</p>



<p>The next section reads a batch of followers for the status:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br></p></td><td><div><p>.<span>select</span><span>(</span><span>"$$partitionedFollowersControl"</span>, Path.<span>key</span><span>(</span><span>"*authorId"</span><span>)</span><span>)</span>.<span>out</span><span>(</span><span>"*tasks"</span><span>)</span><br>
.<span>each</span><span>(</span>Ops.<span>EXPLODE_INDEXED</span>, <span>"*tasks"</span><span>)</span>.<span>out</span><span>(</span><span>"*i"</span>, <span>"*task"</span><span>)</span><br>
.<span>ifTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>NOT_EQUAL</span>, <span>0</span>, <span>"*i"</span><span>)</span>, Block.<span>directPartition</span><span>(</span><span>"$$partitionedFollowers"</span>, <span>"*task"</span><span>)</span><span>)</span><br>
.<span>anchor</span><span>(</span><span>"NormalFanout"</span><span>)</span></p><p>

.<span>unify</span><span>(</span><span>"NormalFanout"</span>, <span>"LocalFollowerFanoutContinue"</span><span>)</span><br>
.<span>macro</span><span>(</span>safeFetchMapLocalFollowers<span>(</span><span>"$$partitionedFollowers"</span>, <span>"*authorId"</span>, <span>"*nextIndex"</span>, rangeQueryLimit, <span>"*fetchedFollowers"</span>, <span>"*nextFollowerId"</span><span>)</span><span>)</span><br>
.<span>ifTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_NOT_NULL</span>, <span>"*nextFollowerId"</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; Block.<span>each</span><span>(</span><span>(</span>RamaFunction5<span>&lt;</span><a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a>, FanoutAction, Status, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+integer"><span>Integer</span></a>, FollowerFanout<span>&gt;</span><span>)</span> FollowerFanout<span>::</span><span>new</span>, <span>"*authorId"</span>, <span>"*nextFollowerId"</span>, <span>new</span> Expr<span>(</span>FanoutAction<span>::</span>findByValue, <span>"*fanoutActionValue"</span><span>)</span>, <span>"*status"</span>, <span>"*task"</span><span>)</span>.<span>out</span><span>(</span><span>"*followerFanout"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>localTransform</span><span>(</span><span>"$$statusIdToLocalFollowerFanouts"</span>, Path.<span>key</span><span>(</span><span>"*statusId"</span><span>)</span>.<span>nullToList</span><span>(</span><span>)</span>.<span>afterElem</span><span>(</span><span>)</span>.<span>termVal</span><span>(</span><span>"*followerFanout"</span><span>)</span><span>)</span><span>)</span></p></div></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="512" height="1024" data-attachment-id="1288" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-5-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?fit=690%2C1381&amp;ssl=1" data-orig-size="690,1381" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-5-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?fit=150%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?fit=512%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=512%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=512%2C1024&amp;ssl=1 512w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=150%2C300&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?w=690&amp;ssl=1 690w" sizes="(max-width: 512px) 100vw, 512px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=512%2C1024&amp;ssl=1 512w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=150%2C300&amp;ssl=1 150w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?w=690&amp;ssl=1 690w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-5-1.png?resize=512%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>As mentioned earlier, in a future post we‚Äôll explore the

<code>$$partitionedFollowers</code>

optimization. As will be explored in that future post, the first section of this code determines from which tasks to read followers in parallel for the status‚Äôs author.</p>
<p>The

<code>unify</code>

call merges processing for statuses from both last iteration and new statuses from this iteration.

<code>safeFetchMapLocalFollowers</code>

is a small helper function reading up to

<code>rangeQueryLimit</code>

followers from this partition for that author (

<code>rangeQueryLimit</code>

is a constant set to 1,000). Since followers are read in parallel across many partitions for users with more than 1,000 followers, and since we deployed this module with 64 partitions, this means up to 64k followers are read per status per iteration.</p>
<p>The

<code>ifTrue</code>

line writes to the

<code>$$statusIdToLocalFollowerFanouts</code>

PState to continue fanout next iteration if there are still more followers to handle. This is the same PState you saw used in the earlier section.</p>



<p>The next section handles follower-specified options on the types of statuses they wish to see from this author:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br></p></td><td><p>.<span>each</span><span>(</span>Ops.<span>EXPLODE</span>, <span>"*fetchedFollowers"</span><span>)</span>.<span>out</span><span>(</span><span>"*follower"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>Follower follower<span>)</span> <span>-&gt;</span> follower.<span>accountId</span>, <span>"*follower"</span><span>)</span>.<span>out</span><span>(</span><span>"*followerId"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>Follower follower<span>)</span> <span>-&gt;</span> follower.<span>sharedInboxUrl</span>, <span>"*follower"</span><span>)</span>.<span>out</span><span>(</span><span>"*followerSharedInboxUrl"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>Follower follower<span>)</span> <span>-&gt;</span> follower.<span>isShowBoosts</span><span>(</span><span>)</span>, <span>"*follower"</span><span>)</span>.<span>out</span><span>(</span><span>"*showBoosts"</span><span>)</span><br>
.<span>each</span><span>(</span><span>(</span>Follower follower<span>)</span> <span>-&gt;</span> follower.<span>getLanguages</span><span>(</span><span>)</span>, <span>"*follower"</span><span>)</span>.<span>out</span><span>(</span><span>"*languages"</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_NULL</span>, <span>"*followerSharedInboxUrl"</span><span>)</span><span>)</span> <span>// skip remote followers</span><br>
.<span>ifTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_INSTANCE_OF</span>, BoostStatusContent.<span>class</span>, <span>"*content"</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; Block.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*content"</span>, <span>"*boostedAuthorId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>NOT_EQUAL</span>, <span>"*boostedAuthorId"</span>, <span>"*followerId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>keepTrue</span><span>(</span><span>"*showBoosts"</span><span>)</span><span>)</span><br>
.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span><span>(</span>List<span>&lt;</span>String<span>&gt;</span> languages, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+string"><span>String</span></a> statusLanguage<span>)</span> <span>-&gt;</span> languages <span>==</span> <span>null</span> <span>||</span> statusLanguage <span>==</span> <span>null</span> <span>||</span> languages.<span>contains</span><span>(</span>statusLanguage<span>)</span>, <span>"*languages"</span>, <span>"*language"</span><span>)</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="489" height="1024" data-attachment-id="1289" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-6-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?fit=659%2C1381&amp;ssl=1" data-orig-size="659,1381" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-6-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?fit=143%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?fit=489%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=489%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?w=659&amp;ssl=1 659w" sizes="(max-width: 489px) 100vw, 489px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?w=659&amp;ssl=1 659w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-6-1.png?resize=489%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This filters this follower out of fanout for this status if: it‚Äôs a boost and they don‚Äôt wish to see boosts from this author, it‚Äôs a boost and they‚Äôre the original author of the status, or they specified they only wish to see certain languages from this author and the status doesn‚Äôt match.</p>



<p>Notice how all the information needed to do the filtering is on the

<code>Follower</code>

structure that was retrieved as part of fetching followers for fanout. No extra PState queries need to be done for this information, which is one of the reasons our implementation has such high throughput. The average amount of fanout per status on our instance is 403, so any work post-fanout (after the

<code>Ops.EXPLODE</code>

call, which emits once per follower in the

<code>*fetchedFollowers</code>

list) is multiplied by 403 compared to work pre-fanout. This is why we went out of our way to materialize as much information on the follow relationship as possible to minimize the work post-fanout.</p>



<p>The next section handles additional filtering required for replies:</p>




<div><table><tbody><tr><td><p>1<br>2<br>3<br>4<br>5<br>6<br></p></td><td><p>.<span>ifTrue</span><span>(</span><span>new</span> Expr<span>(</span>Ops.<span>IS_INSTANCE_OF</span>, ReplyStatusContent.<span>class</span>, <span>"*content"</span><span>)</span>,<br>
&nbsp; &nbsp; &nbsp; &nbsp; Block.<span>macro</span><span>(</span>extractFields<span>(</span><span>"*content"</span>, <span>"*parentAuthorId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>hashPartition</span><span>(</span><span>"*followerId"</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>macro</span><span>(</span>fetchBloomMacro<span>(</span><span>"*followerId"</span>, <span>"*rbloom"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>keepTrue</span><span>(</span><span>new</span> Expr<span>(</span><span>(</span>RBloomFilter rbloom, <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Adocs.oracle.com+javase+docs+api+long"><span>Long</span></a> accountId<span>)</span> <span>-&gt;</span> rbloom.<span>bloom</span>.<span>isPresent</span><span>(</span><span>""</span> <span>+</span> accountId<span>)</span>, <span>"*rbloom"</span>, <span>"*parentAuthorId"</span><span>)</span><span>)</span><br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.<span>select</span><span>(</span><span>"$$followerToFollowees"</span>, Path.<span>key</span><span>(</span><span>"*followerId"</span><span>)</span>.<span>must</span><span>(</span><span>"*parentAuthorId"</span><span>)</span><span>)</span><span>)</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="489" height="1024" data-attachment-id="1290" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-7-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?fit=659%2C1381&amp;ssl=1" data-orig-size="659,1381" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-7-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?fit=143%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?fit=489%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=489%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?w=659&amp;ssl=1 659w" sizes="(max-width: 489px) 100vw, 489px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?w=659&amp;ssl=1 659w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-7-1.png?resize=489%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Replies are delivered to a follower only if they also follow the account being replied to. This code queries

<code>$$followerToFollowees</code>

to perform that check, with the

<code>must</code>

navigator only emitting if the follow relationship exists.</p>



<div><p>Before the PState query, there‚Äôs a bloom filter check to minimize the amount of PState queries done here. This is another optimization that we didn‚Äôt mention in the earlier discussion of fanout, and we‚Äôll discuss it more in a future post. In short, a bloom filter is materialized and cached in-memory on this module for each account with all follows for the account. If the bloom filter returns false, the follow relationship definitely does not exist and no PState query is necessary. If it returns true, the PState query is done to weed out false positives. The bloom filter reduces PState queries for replies by 99%.</p>
<p>The next section completes this ETL by writing to the home timelines of followers that have passed each of the preceding filters:</p>
</div>




<div><table><tbody><tr><td><p>1<br>2<br></p></td><td><p>.<span>hashPartition</span><span>(</span><span>"*followerId"</span><span>)</span><br>
.<span>each</span><span>(</span>HomeTimelines<span>::</span>addTimelineItem, <span>"*homeTimelines"</span>, <span>"*followerId"</span>, <span>"*statusPointer"</span>, <span>new</span> Expr<span>(</span>Ops.<span>CURRENT_MICROBATCH_ID</span><span>)</span><span>)</span><span>;</span></p></td></tr></tbody></table></div>




<figure><img data-lazy-fallback="1" decoding="async" loading="lazy" width="489" height="1024" data-attachment-id="1291" data-permalink="https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/timeline-fanout-8-1/" data-orig-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?fit=659%2C1381&amp;ssl=1" data-orig-size="659,1381" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="timeline-fanout-8-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?fit=143%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?fit=489%2C1024&amp;ssl=1" src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=489%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?w=659&amp;ssl=1 659w" sizes="(max-width: 489px) 100vw, 489px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=489%2C1024&amp;ssl=1 489w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=143%2C300&amp;ssl=1 143w, https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?w=659&amp;ssl=1 659w" data-lazy-src="https://i0.wp.com/blog.redplanetlabs.com/wp-content/uploads/2023/07/timeline-fanout-8-1.png?resize=489%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>This simply routes to the appropriate partition hosting each follower‚Äôs home timeline and then adds to it. That

<code>.hashPartition</code>

call is actually the most expensive part of this ETL because of the huge volume of messages that flow through it. Due to the average of 403 fanout on our instance and our incoming rate of 3,500 statuses / second, 1.4M messages go across that partitioner every second.</p>



<div><p>When we open-source our Mastodon instance in two weeks, you‚Äôll see that this ETL also handles hashtags, lists, conversations, and federation. We excluded those from this code example since they‚Äôre all pretty similar to home timeline fanout, with slightly different rules. They‚Äôre just additional branches of computation on this ETL.</p>
<p>That‚Äôs all we‚Äôll show for now. As mentioned, in two weeks we‚Äôll be open-sourcing our entire Mastodon implementation.</p>
</div>



<h2 id="Conclusion">Conclusion</h2>



<div><p>I‚Äôve covered a lot in this post, but I‚Äôve barely scratched the surface on Rama and our Mastodon implementation. For example, I didn‚Äôt mention ‚Äúfine-grained reactivity‚Äù, a new capability provided by Rama that‚Äôs never existed before. It allows for true incremental reactivity from the backend up through the frontend. Among other things it will enable UI frameworks to be fully incremental instead of doing expensive diffs to find out what changed. We use reactivity in our Mastodon implementation to power much of <a href="https://docs.joinmastodon.org/methods/streaming/">Mastodon‚Äôs streaming API</a>.</p>
<p>I also didn‚Äôt mention Rama‚Äôs integration API. Because of my description of Rama as being able to build an entire backend on its own, you may have the impression that Rama is an ‚Äúall-or-nothing‚Äù tool. However, just because Rama can do so much doesn‚Äôt mean it has to be used to do everything. We‚Äôve designed Rama to be able to seamlessly integrate with any other tool (e.g. databases, queues, monitoring systems, etc.). This allows Rama to be introduced gradually into any architecture.</p>
</div>



<p>To reiterate what‚Äôs to come: in one week we will be releasing the full Rama documentation as well as a build of Rama that exposes the full API for use with

<code>InProcessCluster</code>

, and in two weeks we will be fully open-sourcing our Mastodon implementation (which can run on

<code>InProcessCluster</code>

). Additionally, we will be publishing more posts exploring Rama and our Mastodon implementation in greater depth.</p>



<div><p>You can keep track of developments with Rama by joining <a href="https://redplanetlabs.us13.list-manage.com/subscribe?u=68cd3e63bd4533d0db57922c5&amp;id=fae70c7c5b">our newsletter</a> or following us on Twitter at <a href="https://twitter.com/redplanetlabs">@redplanetlabs</a>. We‚Äôve also started the Google group <a href="https://groups.google.com/u/1/g/rama-user">rama-user</a>, where you can discuss Rama or ask questions.</p>
<p>Lastly, Red Planet Labs will be starting a private beta in the coming months to give companies access to the full version of Rama. We plan to work closely with our private beta users to help them build new systems or reimplement existing systems at massively reduced cost. We will be releasing more details on the private beta later, but you can <a href="https://docs.google.com/forms/d/e/1FAIpQLSfrhmBwI0YAeaL8u4XmgfscW4UIUUDp2ZHSs4KmPH_TaDt1QQ/viewform">apply here</a> in the meantime.</p>
</div>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Opendream: A layer-based UI for Stable Diffusion (301 pts)]]></title>
            <link>https://github.com/varunshenoy/opendream</link>
            <guid>37136898</guid>
            <pubDate>Tue, 15 Aug 2023 17:38:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/varunshenoy/opendream">https://github.com/varunshenoy/opendream</a>, See on <a href="https://news.ycombinator.com/item?id=37136898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Opendream: A Web UI For the Rest of Us <g-emoji alias="thought_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ad.png">üí≠</g-emoji> <g-emoji alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png">üé®</g-emoji></h2>
<p dir="auto">Opendream brings much needed and familiar features, such as layering, non-destructive editing, portability, and easy-to-write extensions, to your Stable Diffusion workflows. Check out our <a href="https://twitter.com/varunshenoy_/status/1691506322360201216?s=20" rel="nofollow">demo video</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/varunshenoy/opendream/blob/main/images/hero.png"><img src="https://github.com/varunshenoy/opendream/raw/main/images/hero.png" alt="hero"></a></p>
<h2 tabindex="-1" dir="auto">Getting started</h2>
<ol dir="auto">
<li>Clone this repository.</li>
<li>Navigate to this project within your terminal and run <code>sh ./run_opendream.sh</code>. After ~30 seconds, both the frontend and backend of the Opendream system should be up and running.</li>
</ol>
<h2 tabindex="-1" dir="auto">Features</h2>
<p dir="auto">Diffusion models have emerged as powerful tools in the world of image generation and manipulation. While they offer significant benefits, these models are often considered black boxes due to their inherent complexity. The current diffusion image generation ecosystem is defined by tools that allow one-off image manipulation tasks to control these models - text2img, in-painting, pix2pix, among others.</p>
<p dir="auto">For example, popular interfaces like <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111</a>, <a href="https://midjourney.com/" rel="nofollow">Midjourney</a>, and <a href="https://beta.dreamstudio.ai/generate" rel="nofollow">Stability.AI's DreamStudio</a> only support destructive editing: each edit "consumes" the previous image. This means users cannot easily build off of previous images or run multiple experiments on the same image, limiting their options for creative exploration.</p>
<h3 tabindex="-1" dir="auto">Layering and Non-destructive Editing</h3>
<p dir="auto">Non-destructive editing is a method of image manipulation that preserves the original image data while allowing users to make adjustments and modifications without overwriting previous work. This approach facilitates experimentation and provides more control over the editing process by using layers and masks. When you delete a layer, all layers after it also get deleted. This guarantees that all layers currently on the canvas are a product of other existing layers. This also allows one to deterministically "replay" a workflow.</p>
<p dir="auto">Like Photoshop, Opendream supports non-destructive editing out of the box. Learn more about the principles of non-destructive editing in Photoshop <a href="https://helpx.adobe.com/photoshop/using/nondestructive-editing.html" rel="nofollow">here</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/varunshenoy/opendream/blob/main/images/editing.png"><img src="https://github.com/varunshenoy/opendream/raw/main/images/editing.png" alt="layers"></a></p>
<h3 tabindex="-1" dir="auto">Save and Share Workflows</h3>
<p dir="auto">Users can also save their current workflows into a portable file format that can be opened up at a later time or shared with collaborators. In this context, a "state" is just a JSON file describing all of the current layers and how they were created.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/varunshenoy/opendream/blob/main/images/workflow.png"><img src="https://github.com/varunshenoy/opendream/raw/main/images/workflow.png" alt="workflow"></a></p>
<h3 tabindex="-1" dir="auto">Support Simple to Write, Easy to Install Extensions</h3>
<p dir="auto">As the open-source ecosystem flourishes around these models and tools, extensibility has also become a major concern. While Automatic1111 does offer extensions, they are often difficult to program, use, and install. It is far from being as full-featured as an application like Adobe Photoshop.</p>
<p dir="auto">As new features for Stable Diffusion, like ControlNet, are released, users should be able to seamlessly integrate them into their artistic workflows with minimal overload and time.</p>
<p dir="auto">Opendream makes writing and using new diffusion features as simple as writing a Python function. Keep reading to learn how.</p>
<h2 tabindex="-1" dir="auto">Extensions</h2>
<p dir="auto">From the get-go, Opendream supports two key primitive operations baked into the core system: <code>dream</code> and <code>mask_and_inpaint</code>. In this repository, extensions for <code>instruct_pix2pix</code>, <code>controlnet_canny</code>, <code>controlnet_openpose</code>, and <code>sam</code> (Segment Anything) are provided.</p>
<p dir="auto">Any image manipulation logic can be easily written as an extension. With extensions, you can also decide how certain operations work. For example, you can override the <code>dream</code> operation to use OpenAI's DALL-E instead or call a serverless endpoint on a service like AWS or Replicate. <a href="https://gist.githubusercontent.com/varunshenoy/f029c55536bb7e4fac61a595e836d930/raw/f7e693c8aa42a814d05198c28a843a97c8f6a4c6/baseten_stable_diffusion.py" rel="nofollow">Here's an example using Baseten</a>.</p>
<h3 tabindex="-1" dir="auto">Loading an Existing Extension</h3>
<p dir="auto">There are two ways to load extensions.</p>
<ol dir="auto">
<li>Install a pre-written one through the Web UI.</li>
<li><em>(Manual)</em> Download a valid extension file (or write one yourself!) and add it to the <code>opendream/extensions</code> folder. Instructions for writing your own extension are below.</li>
</ol>
<p dir="auto">Here is a sampling of currently supported extensions. You can use the links to install any given extension through the Web UI.</p>
<table>
<thead>
<tr>
<th><strong>Extension</strong></th>
<th><strong>Link</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI's DALL-E</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/4a9a6bbfedfa7def28178a8f0563320a/raw/d2d10faa0fad8c2d251e599d962b0c7f62c06db0/dalle.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>Serverless Stable Diffusion</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/f029c55536bb7e4fac61a595e836d930/raw/f7e693c8aa42a814d05198c28a843a97c8f6a4c6/baseten_stable_diffusion.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>Instruct Pix2Pix</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/894c7a723de6b4651380dd7fa2a81724/raw/fa678d8d6c430421fb481f7023ad76898dd27ad6/instruct_pix2pix.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>ControlNet Canny</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/0b0455449454e5856021fe2971b78352/raw/1c08b376b499c25c84976eade71db9aa355dba47/controlnet_canny.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>ControlNet Openpose</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/380722906b8ff184569af57e06fd37b7/raw/728832370db0448bc2807ffc9e267635749e6a9f/controlnet_openpose.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>Segment Anything</td>
<td><a href="https://gist.githubusercontent.com/varunshenoy/5fbc883360e5ab2a3c023ce1e286ddd5/raw/efbc92d27ae2209b15948fb52f657e88c185b349/sam.py" rel="nofollow">File</a></td>
</tr>
<tr>
<td>PhotoshopGPT</td>
<td><a href="https://gist.github.com/varunshenoy/63054e7a479f256974416ef45a51e6a0">Gist</a></td>
</tr>
</tbody>
</table>
<p dir="auto">Note that extensions may have their own requirements you would need to include in the <code>requirements.txt</code> file. For example, you would need to add <code>openai</code> if you want to use the DALL-E extension.</p>
<p dir="auto">Feel free to make a PR if you create a useful extension!</p>
<h3 tabindex="-1" dir="auto">Writing Your Own Extension</h3>
<p dir="auto">Users can write their own extensions as follows:</p>
<ol dir="auto">
<li>Create a new Python file in the <code>opendream/extensions</code> folder.</li>
<li>Write a method with type hints and a <code>@opendream.define_op</code> decorator. This decorator registers this method with the Opendream backend.</li>
</ol>
<p dir="auto">The method has a few requirements:</p>
<ul dir="auto">
<li>Parameters must have type hints. These enable the backend to generate a schema for the input which is parsed into form components on the frontend. Valid types include: <code>str</code>, <code>int</code>, <code>float</code>, <code>Layer</code>, <code>MaskLayer</code>, or <code>ImageLayer</code>.</li>
<li>The only valid return types are a <code>Layer</code> or a list of <code>Layer</code> objects.</li>
</ul>
<h2 tabindex="-1" dir="auto">Contributions and Licensing</h2>
<p dir="auto"><em>Opendream was built by Varun Shenoy, Eric Lou, Shashank Rammoorthy, and Rahul Shiv as a part of Stanford's <a href="https://cs348k.stanford.edu/" rel="nofollow">CS 348K</a>.</em></p>
<p dir="auto">Feel free to provide any contibutions you deem necessary or useful. This project is licensed under the MIT License.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Musk‚Äôs X is throttling traffic to news and websites he dislikes (164 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/</link>
            <guid>37136858</guid>
            <pubDate>Tue, 15 Aug 2023 17:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/">https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/</a>, See on <a href="https://news.ycombinator.com/item?id=37136858">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA["About 67k‚Äù sites are banned from submission on HN, the list is kept secret (104 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37130147</link>
            <guid>37136111</guid>
            <pubDate>Tue, 15 Aug 2023 16:41:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ycombinator.com/item?id=37130147">https://news.ycombinator.com/item?id=37130147</a>, See on <a href="https://news.ycombinator.com/item?id=37136111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37130398"><td></td></tr>
                <tr id="37137757"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37137757" href="https://news.ycombinator.com/vote?id=37137757&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>HN operates, based on a number of reasons, on numerous dynamics of friction and nudges.  Mostly for the better.  I've had my disagreements about things in the past, though as I watch the site and have studied it (particularly over the past few months, see: &lt;<a href="https://news.ycombinator.com/item?id=36843900">https://news.ycombinator.com/item?id=36843900</a>&gt;) I mostly agree with it.<p>The parts that <i>don't</i> work especially well, most particularly discussion of difficult-but-important topics (in my view) ... have also been acknowledged by its creator pg (Paul Graham) and mods (publicly, dang, though there are a few others).</p><p>In general:  if you submit a story and it doesn't go well, drop a note to the moderators:  hn@ycombinator.com.  They typically reply within a few hours, perhaps a day or if things are busy or for complex.</p><p>You can verify that a submission did or didn't go through by checking on the link from an unauthenticated (logged-out) session.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37130484"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130484" href="https://news.ycombinator.com/vote?id=37130484&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>What is it if the information is freely available, to anyone asking, for a single domain they are trying to post at that time?<p>It‚Äôs not secret, because they‚Äôll be provided an answer if they email the mod team.</p><p>It‚Äôs not free as in open source, because it isn‚Äôt available for anyone to download and study in full.</p><p>So, since it‚Äôs not secret, is it public, or private? Since it‚Äôs not published in full but any query of LIMIT 1 is answered, is that open, closed, or other?</p><p>Restrictions to publication don‚Äôt necessarily equate to secrecy, but the best I‚Äôve got is ‚Äúavailable upon request‚Äù, which isn‚Äôt quite right either. Suggestions welcome.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130600"><td></td></tr>
                <tr id="37131149"><td></td></tr>
                <tr id="37134237"><td></td></tr>
                  <tr id="37130675"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130675" href="https://news.ycombinator.com/vote?id=37130675&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>There‚Äôs a protection system in place that can result in that; I don‚Äôt have the details at hand (since I‚Äôm not associated with HN/YC) but I remember seeing it once before on a highly contentious post, and an email to the mods helped explain/correct whatever was up.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130613"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130613" href="https://news.ycombinator.com/vote?id=37130613&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; Suggestions welcome<p>"This domain is not allowed on HN" as an error message upon submission.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130643"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130643" href="https://news.ycombinator.com/vote?id=37130643&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>That‚Äôs not going to work, because now that‚Äôs an API for spammers to bulk process against a domain list. The only available API must be human communication to the mod team, or the spammers will overcome it with automation.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130730"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130730" href="https://news.ycombinator.com/vote?id=37130730&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Spammers can already do that API call and see if the domain shows up. This only puts human users at the same level of consideration as spammer automation.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130772"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130772" href="https://news.ycombinator.com/vote?id=37130772&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Seriously dedicated spammers can, yes! But antispam is about reducing the noise threshold, and eliminating low-effort spam opportunities that can be done to a single HTTP endpoint with a bash script is a big win. Simply having to access two pages is already too much to bother with for the vast majority.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37130705"><td></td></tr>
                        <tr id="37130606"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130606" href="https://news.ycombinator.com/vote?id=37130606&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>There's a lot of user hostile moderation practices that occur on this site, manual and automatic. They're not often, or really at all, discussed. Some of them don't work well, and haven't for as long as they've existed.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37131198"><td></td></tr>
                <tr id="37133548"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37133548" href="https://news.ycombinator.com/vote?id=37133548&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>I would consider any moderation action that isn't visible to users to be user hostile.<p>If you're going to censor someone, you owe it to them to be honest about what you're doing to them.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37136341"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37136341" href="https://news.ycombinator.com/vote?id=37136341&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>You possibly haven't experienced how devious and determined and dishonest and unpleasant some bad actors are, including SPAMmers.<p>(Even when doing the RightThing(TM) would probably be easier...)</p><p>And, BTW, I occasionally get blocked by the mechanisms here, even though not doing anything bad, but understand that there is a trade-off.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37137135"><td></td></tr>
                <tr id="37139801"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37139801" href="https://news.ycombinator.com/vote?id=37139801&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>And it is so high that over the ~25Y+* that I have been running my own sites I have not had UGC on any of them, other then a very brief experiment, which showed me what utter relentless turds the bad actors are.<p>Congrats to HN for striking a reasonable pragmatic balance.</p><p>*I had some of the first live (non-academic) Internet connectivity in the UK, and the very very first packets were hacking attempts...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37137674"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37137674" href="https://news.ycombinator.com/vote?id=37137674&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>The HN moderation policies are clearly effective, because the site is mostly full of useful information that attracts a wide audience of readers.<p>I really like this take on moderation:</p><p>"The essential truth of every social network is that the product is content moderation, and everyone hates the people who decide how content moderation works. Content moderation is what Twitter makes ‚Äî it is the thing that defines the user experience."</p><p>From Nilay Patel in <a href="https://www.theverge.com/2022/10/28/23428132/elon-musk-twitter-acquisition-problems-speech-moderation" rel="nofollow noreferrer">https://www.theverge.com/2022/10/28/23428132/elon-musk-twitt...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37137731"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37137731" href="https://news.ycombinator.com/vote?id=37137731&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>I agree with you both. The only thing I'd add is that it's a tradeoff - if we do it this way, it's only because the alternative would be even more user-hostile.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37138692"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_37138692" href="https://news.ycombinator.com/vote?id=37138692&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Does HN ever show a user that their comment was submitted, but the comment is not visible for anyone else? Or it‚Äôs not visible for most people? Without having the flagged tag</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37138887"><td></td></tr>
                                    <tr id="37137040"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37137040" href="https://news.ycombinator.com/vote?id=37137040&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt;If you're going to censor someone<p>unless HN is suddenly the government what you've misnomered is moderation, not censorship. Calling censorship just exaggerates your opinion and makes you look unhinged. It's a private website not national news.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37137128"><td></td></tr>
                <tr id="37137680"><td></td></tr>
            <tr id="37137214"><td></td></tr>
                <tr id="37139254"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_37139254" href="https://news.ycombinator.com/vote?id=37139254&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>I like Scott Alexander's definition[1]. Quoting directly:<p>&gt; Moderation is the normal business activity of ensuring that your customers like using your product. If a customer doesn‚Äôt want to receive harassing messages, or to be exposed to disinformation, then a business can provide them the service of a harassment-and-disinformation-free platform.</p><p>&gt; Censorship is the abnormal activity of ensuring that people in power approve of the information on your platform, regardless of what your customers want. If the sender wants to send a message and the receiver wants to receive it, but some third party bans the exchange of information, that‚Äôs censorship.</p><p>Censorship is somewhat subjective, something that you might find offensive and want moderated might not be considered so by others. Therefore, Alexander further argues that the simplest mechanism that turns censorship into moderation is a switch that, when enabled, lets you see the banned content, which is exactly what HN does. Alexander further argues that there are kinds of censorship that aren't necessarily bad, by this definition, disallowing pedophiles from sharing child porn with each other is censorship, but it's something that we should still do.</p><p>[1] <a href="https://astralcodexten.substack.com/p/moderation-is-different-from-censorship" rel="nofollow noreferrer">https://astralcodexten.substack.com/p/moderation-is-differen...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37138738"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_37138738" href="https://news.ycombinator.com/vote?id=37138738&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>In my head, censorship is the removal of an idea that is offensive to a particular ideology but isn‚Äôt objectively harmful.<p>Moderation is the removal of content that objectively doesn‚Äôt belong in context, eg spam</p><p>Obviously that moderation definition is nuanced bc some could argue that Marxist ideas don‚Äôt belong in the context of a site with a foundation in startups. And indeed Marxist ideas often get flagged here
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37137828"><td></td></tr>
                                          <tr id="37131304"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37131304" href="https://news.ycombinator.com/vote?id=37131304&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>I've twice had some "user hostile moderation practice" used against me on HN. Both times an email to the right person cleared it up - and one of those times in fact I had crossed a boundary that I shouldn't have crossed. Any long-time community member here knows what to do.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37137642"><td></td></tr>
            <tr id="37136798"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37136798" href="https://news.ycombinator.com/vote?id=37136798&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Understandable, but I think there should be some discriminating system for another class of sites, the "you can submit but not discuss" ones.<p>For example, a recent submission (of mine):</p><p>"Luis Bu√±uel: The Master of Film Surrealism"</p><p>it had no discussion space because (I guess) it comes from fairobserver.com . Now, I understand that fairobserver.com may had been an hive of dubious publishing historically, but it makes little sense we cannot discuss Bu√±uel...</p><p>Maybe a rough discriminator (function approximator, Bayesian etc.) could try and decide (based at least on the title) whether a submission from "weak editorial board" sites seems to be material to allow posts or not.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37137830"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37137830" href="https://news.ycombinator.com/vote?id=37137830&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Oh I agree - <a href="https://news.ycombinator.com/item?id=36924205">https://news.ycombinator.com/item?id=36924205</a> was a fine submission. Can you please email  hn@ycombinator.com so I can send you a repost invite for it?<p>That domain is a borderline case. Sometimes the leopard really changes its spots, i.e. a site goes from offtopic or spam to one that at least occasionally produces good-for-HN articles. In such cases we simply unban it. Other times, the general content is still so bad for HN that we have to rely on users to vouch for the occasional good submission, or to email us and get us to restore it. I can't quite tell where fairobserver.com is on this spectrum because the most recent submission (yours) is good, the previous one (from 7 months ago) is borderline, and before that it was definitely not good. But I've unbanned it now and moved it into the downweighted category, i.e. one notch less penalized.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37136419"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37136419" href="https://news.ycombinator.com/vote?id=37136419&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Do you have get-out-of-jail or N-strikes-and-you're-out policies? What if someone's legitimate website gets caught in this?
I've also long wondered about user specific shadow bans. Can you please shed light on this?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37137747"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37137747" href="https://news.ycombinator.com/vote?id=37137747&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>There's no automatic unban. That would require writing code that knows how to tell a good (for HN) site apart from a bad one, and if we could write such code, we wouldn't need to keep a list of banned sites in the first place. However, we're always happy to unban a site when we notice that it's actually fine for HN, or when someone points this out to us.<p>Re shadowbanning (i.e. banning a user without telling them), see the past explanations at <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=by%3Adang%20shadowb&amp;sort=byDate&amp;type=comment" rel="nofollow noreferrer">https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;que...</a> and let me know if you still have questions. The short version is that when an account has an established history, we tell them we're banning them and why. We only shadowban when it's a spammer or a new account that we have reason to guess is a serial abuser.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37138012"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37138012" href="https://news.ycombinator.com/vote?id=37138012&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Thanks for the thoughtful reply. Is it also true that users with certain karma count or special permissions have more significant - and potentially lasting - downvoting weight that impacts to the downvoted party's long term reputation?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37139032"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37139032" href="https://news.ycombinator.com/vote?id=37139032&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>I'm interpreting your question as "are there privileged HN members with supervotes", excluding moderators, and who can single-handedly kill submissions or comments.<p>So far as I'm aware, no, and there are comments from dang and pg going back through the site history which argue strongly <i>against</i> distinguishing groups of profiles in any way.</p><p>The one possible exception is that YC founder's handles appear orange to one another at one point in time (pg discusses this in January 2013: &lt;<a href="https://news.ycombinator.com/item?id=5025168">https://news.ycombinator.com/item?id=5025168</a>&gt;).  The feature was disabled <i>for performance reasons</i>.</p><p>Dang mentions the feature still being active as of a year ago:  &lt;<a href="https://news.ycombinator.com/item?id=31727636">https://news.ycombinator.com/item?id=31727636</a>&gt;</p><p>I seem to recall a pg or dang discussion where showing this <i>publicly</i> created a social tension on the site, as in, one set of people distinguished from another.</p><p>dang discusses the (general lack of) secret superpowers here:  &lt;<a href="https://news.ycombinator.com/item?id=22767204">https://news.ycombinator.com/item?id=22767204</a>&gt;, which reiterates what's in the FAQ:</p><p><i>HN gives three features to YC: job ads (see above) and startup launches get placed on the front page, and YC founder names are displayed to other YC alumni in orange.</i></p><p>&lt;<a href="https://news.ycombinator.com/newsfaq.html">https://news.ycombinator.com/newsfaq.html</a>&gt;</p><p>Top-100 karma lands you on the leaderboard:  &lt;<a href="https://news.ycombinator.com/leaders">https://news.ycombinator.com/leaders</a>&gt;.  That's currently 41,815+ karma.  There are also no special privileges here other than occasionally being contacted by someone.  (I've had inquiries about dealing with the head-trip of being on the leaderboard, and a couple of requests to boost submissions, which I forward to the moderation team).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37138283"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37138283" href="https://news.ycombinator.com/vote?id=37138283&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>I'm afraid I don't understand your question but here are the basics: HN has downvotes (on comments, not submissions). The ability to downvote requires &gt; 500 karma. When a comment gets downvoted, both its point score and the commenter's karma go down (in most cases - it's more complicated than that but this is the principle). Does that help?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37137929"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37137929" href="https://news.ycombinator.com/vote?id=37137929&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>You forgot to mention that you are also shadowbanning the ability of users to upvote or downvote things when you dislike their upvotes or downvotes‚Äîinstances that you perceive as not contributing to the discussion or that are escalating the conversation.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37138287"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37138287" href="https://news.ycombinator.com/vote?id=37138287&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>I didn't forget to mention that - it's simply not what the word shadowban means, as I've always understood and used it.<p>This is a big problem with trying to explain these things - people mean very different things by the same words, and it leads to misunderstanding.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37138736"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37138736" href="https://news.ycombinator.com/vote?id=37138736&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Which other word do you think would be suitable here? In my view, 'shadowban' aligns with the definition in this context, as you aren't notifying people about it (hence 'shadow') and their actions of upvoting or downvoting have no impact (so same as shadowbanning comments or submissions etc).</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37134742"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37134742" href="https://news.ycombinator.com/vote?id=37134742&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>That's a lot of domains! Did you source that from some other list, or is that a result of 67k individual entries? Either way, I appreciate it.<p>Out of curiosity, what's the rationale for blocking archive.is? Legal reasons I assume?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37136514"><td></td></tr>
            <tr id="37138324"><td></td></tr>
            <tr id="37136281"><td></td></tr>
                <tr id="37136526"><td></td></tr>
                <tr id="37136690"><td></td></tr>
                <tr id="37139215"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37139215" href="https://news.ycombinator.com/vote?id=37139215&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>0.02% of a big number ... is still a big number<p>0.02% of 10,000 is 2 - pretty small</p><p>0.02% of 1,000,000,000 is 200,000 ... kinda big :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37131476"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37131476" href="https://news.ycombinator.com/vote?id=37131476&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Well that explains why all those links I posted to maximizedlivingdrlabrecque.com never got any traction‚Ä¶</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37135813"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37135813" href="https://news.ycombinator.com/vote?id=37135813&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Maybe "major media" should include tech media like The Register, Ars Technica, Tech Dirt, etc.. Unlike with media like the NYT, Bloomberg or Reuters, I've never seen a story for which these sites were the best source and much of what they publish is blogspam summarizing stories that have already been posted on HN, usually with a votebait title.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37139781"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37139781" href="https://news.ycombinator.com/vote?id=37139781&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Yes, those sites are all downweighted. Whether they count as "major media" or not, they're classified the same way by HN's software, for more or less the same reason: they produce a lot of derivative and/or sensational and/or otherwise not-great-for-HN content, and they also produce substantive articles that are good for HN.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37136386"><td></td></tr>
                  <tr id="37136474"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37136474" href="https://news.ycombinator.com/vote?id=37136474&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>So, is there an algorithm to be features in the front page? ‚Äîother than upvotes. If a site can be banned, can another one be promoted?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37139764"><td></td></tr>
            <tr id="37136594"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37136594" href="https://news.ycombinator.com/vote?id=37136594&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>I could be wrong, but I was always under the impression that companies that are in ycombinator get an inital boast in the jobs posts but also quickly fall off as such links don't allow comments.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130472"><td></td></tr>
            <tr id="37130834"><td></td></tr>
                <tr id="37131157"><td></td></tr>
            <tr id="37131153"><td></td></tr>
            <tr id="37131215"><td></td></tr>
                  <tr id="37130185"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130185" href="https://news.ycombinator.com/vote?id=37130185&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Would be nice if the lists were published though with a link to the list from the submission form.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130234"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130234" href="https://news.ycombinator.com/vote?id=37130234&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>The problem is that if you publish the lists it leads to more abuses. For example if spammers find out which sites are banned then they just post other ones.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37131669"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37131669" href="https://news.ycombinator.com/vote?id=37131669&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>I think there are two different types of sites you are blocking: (1) those which are just pure spam; (2) news/opinion/etc websites that you‚Äôve decided are not suitable for HN for various reasons (such as being low quality and tending to produce more ideological flame-wars than curiosity), for example Breitbart<p>I agree that publishing case (1) causes harm (spammers will just use a different domain if they know you‚Äôve blocked theirs.) But case (2) is rather different. I don‚Äôt think the same justification for lack of transparency exists in this case. And I think shadow-banning the submission in case (2) is not very user-friendly. It would be better to just display an error, e.g. ‚Äúsubmissions from this site are blocked because we do not believe it is suitable for HN‚Äù (or whatever). A new user might post stuff like (2) out of misunderstanding what the site is about rather than malevolence, so better to directly educate them than potentially leave them ignorant. Also, while Breitbart is rather obviously garbage, since we don‚Äôt know everything in category (2) on the list, maybe there are some sites on it whose suitability is more debatable or mixed, and its inappropriateness may be less obvious to someone than Breitbart‚Äôs (hopefully) is
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37137883"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37137883" href="https://news.ycombinator.com/vote?id=37137883&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>That's a good argument and subtle enough that I'm not sure whether I agree or disagree.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130270"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130270" href="https://news.ycombinator.com/vote?id=37130270&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; For example if spammers find out which sites are banned then they just post other ones.<p>I don't think that makes sense. The supposed spammers can just try looking up whether their submissions show up or not when not logged in.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130279"><td></td></tr>
                <tr id="37130516"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130516" href="https://news.ycombinator.com/vote?id=37130516&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Increasing cost of attacks is effective against good faith people, not spammers.<p>Even Cory Doctorow made this case in "Como is Infosec" [1].</p><p>The only problem with Cory's argument is, he points people to the SC Principles [2]. The SCP contain exceptions for not notifying about "spam, phishing or malware." But anything can be considered spam, and transparency-with-exceptions has always been platforms' position. They've always argued they can secretly remove content when it amounts to "spam." Nobody has challenged them on that point. The reality is, platforms that use secretive moderation lend themselves to spammers.</p><p>[1] <a href="https://doctorow.medium.com/como-is-infosec-307f87004563" rel="nofollow noreferrer">https://doctorow.medium.com/como-is-infosec-307f87004563</a></p><p>[2] <a href="https://santaclaraprinciples.org/" rel="nofollow noreferrer">https://santaclaraprinciples.org/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37136408"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37136408" href="https://news.ycombinator.com/vote?id=37136408&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>In my experience, increasing cost or delay even a <i>little</i> bit cuts out a disproportionate amount of bad stuff.<p>I once had the domain 'moronsinahurry' registered, though not with this group in mind...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37131955"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37131955" href="https://news.ycombinator.com/vote?id=37131955&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span><i>platforms that use secretive moderation lend themselves to spammers</i><p>how is that? i can understand it not being useful, but how would it help spammers?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37132012"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37132012" href="https://news.ycombinator.com/vote?id=37132012&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Spammers game the system while good-faith users get edged out. Spammers are determined actors who perceive threats everywhere, whereas good-faith users never imagine that a platform would secretly remove their content. Today, you see low quality content on social media, not because the world is dumb, but because the people who get their message out know the secret tricks.<p>Secret suppression is extremely common [1].</p><p>Many of today's content moderators say exceptions for shadowbans are needed [2]. They think lying to users promotes reality. That's bologna.</p><p>[1] <a href="https://www.removednews.com/p/hate-online-censorship-its-way-worse" rel="nofollow noreferrer">https://www.removednews.com/p/hate-online-censorship-its-way...</a></p><p>[2] <a href="https://twitter.com/rhaksw/status/1689887293002379264" rel="nofollow noreferrer">https://twitter.com/rhaksw/status/1689887293002379264</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37133790"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_37133790" href="https://news.ycombinator.com/vote?id=37133790&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>so to spammers shadowbanning makes no difference, but good-faith users somehow get discouraged even if they don't know they are shadowbanned just because they get no reaction to their posts? how is an explicit ban any less discouraging?<p>i can't see how shadowbanning makes things worse for good-faith users. and evidently it does work against spammers here on HN (though we don't know if it is the shadow or the banning that makes it effective, but i'll believe dang when he says that it does help)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37135288"><td><table>  <tbody><tr>    <td indent="8"><img src="https://news.ycombinator.com/s.gif" height="1" width="320"></td><td>
      <center><a id="up_37135288" href="https://news.ycombinator.com/vote?id=37135288&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; how is an explicit ban any less discouraging?<p>It's about whose messages are sidelined, not who gets discouraged.</p><p>With shadow removals, good-faith users' content is elbowed out without their knowledge. Since they don't know about it, they don't adjust behavior and do not bring their comments elsewhere.</p><p>Over 50% of Reddit users have removed content they don't know about. Just look at what people say when they find out [1].</p><p>&gt; and evidently it does work against spammers here on HN</p><p>It doesn't. It benefits people who know how to work the system. The more secret it is, the more special knowledge you need.</p><p>[1] <a href="https://www.reveddit.com/#say" rel="nofollow noreferrer">https://www.reveddit.com/#say</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                          <tr id="37130814"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130814" href="https://news.ycombinator.com/vote?id=37130814&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>It has made sense since the internet was invented, spammers need everything thrown at them because they will abuse every nook and cranny of your system to get paid 1 cent more</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130460"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130460" href="https://news.ycombinator.com/vote?id=37130460&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>You're correct again. Spammers and bots are the most determined actors, so these secretive measures don't impact them.<p>In fact, such secrecy <i>benefits</i> spammers. Good-faith users never imagine that platforms would secretly action content. So when you look at overall trends, bots, spammers and trolls are winning while genuine users are being pushed aside.</p><p>I argued that secrecy benefits trolls in a blog post, but I don't want to spam links to my posts in the comments.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130494"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130494" href="https://news.ycombinator.com/vote?id=37130494&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Most spammers aren‚Äôt that competent. Hiding their posts without telling them used to be very effective on Reddit (now Reddit tells them). I guess it‚Äôs the same on HN.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130519"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130519" href="https://news.ycombinator.com/vote?id=37130519&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Spammers are more competent than genuine users. They are advertisers, so they are more likely to be tracking metrics.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37131311"><td></td></tr>
                <tr id="37131678"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37131678" href="https://news.ycombinator.com/vote?id=37131678&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>So you think secretive measures more often defeat spammers than trusting users? I'd argue HN's content could be a lot better than it currently is.<p>Content curation is necessary, but shadow moderation is not helping. When a forum removes visible consequences, it does not prepare its users to learn from their mistakes.</p><p>I'll admit, I find HN to be more transparently moderated than Reddit and Twitter, but let's not pretend people have stopped trying to game the system. The more secret the rules (and how they are applied), the more a system serves a handful of people who have learned the secret tricks.</p><p>Meanwhile, regular users who are not platform experts trust these systems to be transparent. Trustful users spend more time innovating elsewhere, and they are all disrupted by unexpected secretive tricks.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37136670"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37136670" href="https://news.ycombinator.com/vote?id=37136670&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; <i>So you think secretive measures more often defeat spammers than trusting users?</i><p>Yes. And it's really not a close question.</p><p>"Regular users" don't have to be platform experts and learn tricks and stuff. They just post normal links and comments and never run into moderation at all.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                                <tr id="37132430"><td></td></tr>
            <tr id="37130261"><td></td></tr>
                <tr id="37131212"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37131212" href="https://news.ycombinator.com/vote?id=37131212&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>By that logic, the fact that penispowerworldwide.com is banned on HN* means we're biased against your politics.<p>Of the 67k sites banned on HN I would guess that fewer than 0.1% are "news sources", left- or right- or any wing. Why would you expect them to show up in a random sample of 10?</p><p>* which it is! I've unkilled <a href="https://news.ycombinator.com/item?id=1236054">https://news.ycombinator.com/item?id=1236054</a> for the occasion.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37130315"><td></td></tr>
                <tr id="37131261"><td></td></tr>
                <tr id="37136828"><td></td></tr>
                        <tr id="37130365"><td></td></tr>
            <tr id="37130597"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130597" href="https://news.ycombinator.com/vote?id=37130597&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>That's what "random 10 out of 67k sites" gives you. Your set was cherry-picked, dang's wasn't.<p>That said, dailykos.com seems to be banned. Happy now?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130645"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130645" href="https://news.ycombinator.com/vote?id=37130645&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; Your set was cherry-picked<p>Not exactly cherry-picked, these were from things I submitted myself and noticed that were shadow flagged.</p><p>&gt; That said, dailykos.com seems to be banned. Happy now?</p><p>No, I'd be happy when archive.is, Federalist and the rest of the non-spammy ones are unbanned. (Also, even if "balanced" censorship was the desired goal, having a single unreliable left-wing source banned vs a ton of right-wing ones doesn't really achieve that.)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37135652"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37135652" href="https://news.ycombinator.com/vote?id=37135652&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Turn on show dead, browse <a href="https://news.ycombinator.com/newest">https://news.ycombinator.com/newest</a> and take the affirmative community moderation steps of vouching for those that are good.<p>Archive.is shouldn't ever need to be the primary site.  Post a link to the original and then a comment to the archive site if there's the possibility of take down or issues with paywalls.</p><p>It is likely that people were using archive.is for trying to avoid posting the original domain and masking the content that it presented.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37130688"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130688" href="https://news.ycombinator.com/vote?id=37130688&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; Not exactly cherry-picked, these were from things I submitted myself and noticed that were shadow flagged.<p>Definitely not random, in any case.</p><p>&gt; Also, even if "balanced" censorship was the desired goal,</p><p>Nobody claimed that. You merely stated that "I don't see a single left-wing new source in there." and I offered a counter-point.</p><p>&gt; having one left-wing source vs a ton of right-wing one doesn't achieve that</p><p>I didn't do an exhaustive search for "left-wing domains" that are banned to present you a complete list, this was attempt 1 of 1.</p><p>Following your model, I could claim that 100% of left-wing domains are banned, but I won't.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37130701"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130701" href="https://news.ycombinator.com/vote?id=37130701&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>The people behind Federalist can just start a new site where they produce content that‚Äôs more interesting to HN users and won‚Äôt get banned. It‚Äôs a free world. Nobody‚Äôs entitled to get their pet domain promoted on another private web site.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130849"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130849" href="https://news.ycombinator.com/vote?id=37130849&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><br><div>
                  <p><span>Have you considered that may speak more to your biases than the site's? How many far left-wing news sites do you regular?</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37131422"><td></td></tr>
                  <tr id="37130150"><td></td></tr>
                <tr id="37130210"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130210" href="https://news.ycombinator.com/vote?id=37130210&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>The word censorship has so many meanings that I have to ask what you mean by it before I can say whether I see it that way.<p>Is it censorship that the rules of chess say you can't poke someone's queen off the board? We're trying to play a particular game here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130254"><td></td></tr>
            <tr id="37131166"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37131166" href="https://news.ycombinator.com/vote?id=37131166&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>Censorship has a very defined meaning. To take the chess analogy, it would be like allowing one side to poke the queen off the board, and not allow the other. This is very much like what happens at HN today.<p>You're dang right, trying to play a particular [rigged] game here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37136705"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37136705" href="https://news.ycombinator.com/vote?id=37136705&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>What is the "very defined meaning" that you're thinking of?<p>The one that I think makes the most clear sense is "censorship" by a state power. But you must be thinking of something different, because HN is not a state power.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37131329"><td></td></tr>
                  <tr id="37130250"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130250" href="https://news.ycombinator.com/vote?id=37130250&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; The word censorship has so many meanings that I have to ask what you mean by it before I can say whether I see it that way.<p>Perhaps its one of those things that are hard to define. [1] But that doesn't mean clear cases don't exist.</p><p>&gt; Is it censorship that the rules of chess say you can't poke someone's queen off the board? We're trying to play a particular game here.</p><p>No, but it is clearly political censorship if you only apply the unwritten and secret "rules" of the game to a particular political faction. Also, banning entire domain names is definitely heavy-handed.</p><p>[1]: <a href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/I_know_it_when_I_see_it</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130414"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130414" href="https://news.ycombinator.com/vote?id=37130414&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; political faction<p>I remember some words that succinctly express something I often observe. To paraphrase:</p><p>&gt; Left-wing and Right-wing are terms which make a lot of people falsely believe that they disagree with each other.</p><p>It is worth trying to find common ground with people ‚Äúon the other side‚Äù.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37130427"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130427" href="https://news.ycombinator.com/vote?id=37130427&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>&gt; censorship if you only apply the unwritten and secret "rules"<p>I mostly agree. I argued in an article [1] that it's only censorship if the author of the content is not told about the action taken against the content.</p><p>These days though, mods and platforms will generally argue that they're being transparent by telling you that it happens. <i>When</i> it happens is another story altogether that is often not shared.</p><p>[1] <a href="https://www.removednews.com/p/twitters-throttling-of-what-is-a" rel="nofollow noreferrer">https://www.removednews.com/p/twitters-throttling-of-what-is...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37130160"><td></td></tr>
                <tr id="37138549"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37138549" href="https://news.ycombinator.com/vote?id=37138549&amp;how=up&amp;goto=item%3Fid%3D37130147"></a></center>    </td><td><p><span>That's true, but it's a bit of an interesting question because "free speech" has different meanings in different contexts. The thing to understand about HN is that we're trying to optimize for one thing: intellectual curiosity (<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;sort=byDate&amp;type=comment&amp;query=curiosity%20optimiz%20by:dang" rel="nofollow noreferrer">https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;sor...</a>). Given that, we're obviously not "free speech" in the sense of "post anything about anything" - we have to moderate things like spam, flamewar, and lame comments like "ok boomer" or whatever, because those detract from intellectually curious discussion.<p>On the other hand, no single political or ideological position has a monopoly on intellectual curiosity either‚Äîso by the same principle, HN can't be moderated for political or ideological position.</p><p>It's tricky because working this way conflicts with how everyone's mind works. When people see a politically charged post X that they don't like, or when they see a politically charged post Y that they do like, but which we've moderated, it's basically irresistible not to jump to the conclusion "the mods are biased against Y in favor of X". This is because what people see in the first place is conditioned by their own preferences - we're more likely to notice and to put weight on things we dislike (<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=by%3Adang%20notice%20dislike&amp;sort=byDate&amp;type=story&amp;type=comment" rel="nofollow noreferrer">https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;que...</a>). People with opposite preferences notice opposite data points and therefore "see" opposite biases. It's the same mechanism at work in each case.</p><p>In reality, though, we're just trying to solve an optimization problem: how can you operate a public internet forum to maximize intellectual curiosity? That's basically all there is to it. It's not so easy to solve though.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ball milling destroys PFAS in contaminated soil (120 pts)]]></title>
            <link>https://phys.org/news/2023-08-chemicals.html</link>
            <guid>37134563</guid>
            <pubDate>Tue, 15 Aug 2023 14:40:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2023-08-chemicals.html">https://phys.org/news/2023-08-chemicals.html</a>, See on <a href="https://news.ycombinator.com/item?id=37134563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/forever-chemicals-mayb.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2023/forever-chemicals-mayb.jpg" data-sub-html="Heavily modified shipping containers at EDL's research and development facility in Henderson, Auckland, house the company's patented Mechanochemical Destruction (MCD) reactors. Credit: EDL Ltd">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/forever-chemicals-mayb.jpg" alt="`Forever chemicals'? Maybe not" title="Heavily modified shipping containers at EDL's research and development facility in Henderson, Auckland, house the company's patented Mechanochemical Destruction (MCD) reactors. Credit: EDL Ltd" width="800" height="530">
             <figcaption>
                Heavily modified shipping containers at EDL's research and development facility in Henderson, Auckland, house the company's patented Mechanochemical Destruction (MCD) reactors. Credit: EDL Ltd
            </figcaption>        </figure>
    </div>
<p>Dangerous "forever chemicals" left in the soil from firefighting foam could be destroyed by grinding, according to a proof-of-concept study by University of Auckland scientists collaborating with the U.S. Environmental Protection Agency.

										  
											        </p>
										 
										 											  
<p>"Ball milling" appears viable for decontaminating soil from <a href="https://phys.org/tags/military+bases/" rel="tag">military bases</a>, airports, and refineries around the world where the foam was used over decades, according to the University and Environmental Decontamination (NZ) Limited (EDL).
</p><p>Contaminant chemicals called PFAS (per- and polyfluoroalkyl substances) don't break down naturally and, at certain levels, have been linked to cancers, reduced fertility, liver damage and other adverse health effects.
</p><p>"Cleaning up PFAS from the environment is a massive task that will require our continuous and dedicated investment in the coming years," US President Joe Biden's White House said in March. Individual sites can have thousands of tons of contaminated soil, with the US Department of Defense estimating in 2021 that its clean-up could cost $31 billion.
</p><p>Ball milling in a University of Auckland chemistry laboratory destroyed 99.88 percent to 100 percent of PFAS in soil from a decommissioned New Zealand Defence Force firefighting training site and in firefighting foam.
</p><p>Intense grinding at an extremely high speed by metal balls left a safe by-product, according to Dr. Kapish Gobindlal, an honorary academic at the University and the chief scientist for the company EDL.
</p><p>Published in the <a href="https://phys.org/tags/academic+journal/" rel="tag">academic journal</a> <i>Environmental Science: Advances</i>, the research was by Gobindlal and his Ph.D. supervisors, Professor Jon Sperry and Dr. Cameron Weber, of the University's Centre for Green Chemical Science. Collaborating were scientists Erin Shields and Andrew Whitehill of the US EPA.
</p><p>"We've established proof-of-concept and believe this method can be scaled up faster and cheaper than alternatives," says Gobindlal. "There is a massive need‚Äîthe US alone has thousands of contaminated sites and regulation is shifting toward mandating remediation of these sites."
</p><p>It's exactly what Sperry hoped to achieve when the University set up the Centre for Green Chemical Science, which puts the environment at the forefront.
</p><p>"Work in the lab is flowing quickly toward real-world benefits," Sperry said. "This is an example of green chemistry that can help communities, the environment and, in fact, the world."


											  													    </p>
											  
											  <p>Numbering in their thousands, forever chemicals resist water, oil and heat, famously featuring in Teflon non-stick pans but also in everything from burger wrappers and pizza boxes to waterproof clothing.
</p><p>Firefighters used "aqueous film-forming foam" containing PFAS to blanket and smother flammable liquid fires.
</p><p>PFAS are in animals‚Äîeven plankton‚Äîand in humans' blood and <a href="https://phys.org/tags/breast+milk/" rel="tag">breast milk</a> because of carbon-fluorine bonds which prevent the chemicals breaking down. Like microplastics, they are ubiquitous, turning up in drinking water and even rain.
</p><p>While contaminated soil is only part of the problem, it's a big part.
</p><p>In some ways, "ball milling" is not all that different from the grinding of a mortar and pestle, but at an extremely high intensity, with the balls moving at incredible speeds to degrade the PFAS at a <a href="https://phys.org/tags/molecular+level/" rel="tag">molecular level</a>, says Gobindlal.
</p><p>Crucial to ramping up is cost, including whether the grinding process requires expensive additives. Affordable and easy-to-source quartz sand was used as part of the treatment for firefighting foam, says Gobindlal, while no additive was needed for soil.
</p><p>Laboratory benchtop experiments at the University from 2018 to 2023 typically involved 10 to 30 small metal balls colliding to destroy PFAS in soil, in firefighting foam, and in media such as activated carbon, which is used to remove PFAS from water. The process left an inert powder suitable for being a grinding additive or non-hazardous fill.
</p><p>Heavily modified shipping containers at EDL's research and development facility in Henderson, Auckland, house the company's patented Mechanochemical Destruction (MCD) reactors, intended to treat contaminated soil at speed and scale‚Äîpotentially dealing with several tons per hour.
</p><p>In New Zealand, PFAS soil contamination occurred at locations such as Royal New Zealand Air Force bases Woodbourne (west of Blenheim) and Ohakea (near Palmerston North). Banned in New Zealand in 2011, the firefighting foams were still found at sites including airports years later, according to New Zealand's Environmental Protection Agency.
</p><p>Last year, Channel Infrastructure NZ, an operator of the Marsden Point Oil Refinery in Northland, was fined for using firefighting foam containing PFAS.
</p><p>"In addition to the known PFAS-contaminated locations, there are likely many more unknown sites yet to be identified through active investigation from both governmental and private entities," according to Gobindlal. "We're likely just at the tip of the iceberg."
</p><p>In the US, the chemical and manufacturing company 3M negotiated a $10 billion settlement with cities and towns over PFAS pollution in water. In Europe, a group of news organisations including Le Monde say at least 17,000 sites across Europe and the UK are contaminated with PFAS.
</p><p>New Zealand's EPA has proposed a ban on PFAS in cosmetics. The chemicals are in our drinking water, but at lower concentrations than in other countries.
</p><p>Levels in water "are still concerning because PFAS bioaccumulate and biomagnify; they build up in our bodies, environment, and food web," wrote Dr. Lokesh Padhye, Dr. Erin Leitao, and Dr. Melanie Kah, of the University's faculties of science and engineering, in <i>Newsroom</i> in March
										 																				
																				</p><p><strong>More information:</strong>
												Kapish Gobindlal et al, Mechanochemical destruction of per- and polyfluoroalkyl substances in aqueous film-forming foams and contaminated soil, <i>Environmental Science: Advances</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1039/D3VA00099K" target="_blank">DOI: 10.1039/D3VA00099K</a>
																						
																					</p>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												'Forever chemicals'? Maybe not (2023, August 14)
												retrieved 15 August 2023
												from https://phys.org/news/2023-08-chemicals.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Code Is Not Literature (2014) (181 pts)]]></title>
            <link>https://gigamonkeys.com/code-reading/</link>
            <guid>37134520</guid>
            <pubDate>Tue, 15 Aug 2023 14:36:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gigamonkeys.com/code-reading/">https://gigamonkeys.com/code-reading/</a>, See on <a href="https://news.ycombinator.com/item?id=37134520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                
                <p>I have started code reading groups at the last two companies I‚Äôve worked at, Etsy and Twitter, and some folks have asked for my advice about code reading and running code reading groups. Tl;dr: don‚Äôt start a code reading group. What you should start instead I‚Äôll get to in a moment but first I need to explain how I arrived at my current opinion.</p>
                <p>As a former English major and a sometimes writer, I had always been drawn to the idea that code is like literature and that we ought to learn to write code the way we learn to write English: by reading good examples. And I‚Äôm certainly not the only one to have taken this point of view‚ÄîDonald Knuth, in addition to his work on <i>The Art of Computer Programming</i> and TeX, has long been a proponent of what he calls Literate Programming and has published several of his large programs as books.</p>
                <p>On the other hand, long before I got to Etsy and started my first code reading group I had in hand several pieces of evidence that should have suggested to me that this was the wrong way to look at things.</p>
                <p>First, when I did my book of interviews with programmers, <a href="http://www.codersatwork.com/">Coders at Work</a>, I asked pretty much everyone about code reading. And while most of them said it was important and that programmers should do it more, when I asked them about what code they had read recently, very few of them had any great answers. Some of them had done some serious code reading as young hackers but almost no one seemed to have a regular habit of reading code. Knuth, the great synthesizer of computer science, does seem to read a lot of code and Brad Fitzpatrick was able to talk about several pieces of open source code that he had read just for the heck of it. But they were the exceptions.</p>
                <p>If that wasn‚Äôt enough, after I finished <i>Coders</i> I had a chance to <a href="http://www.gigamonkeys.com/code-quarterly/2011/hal-abelson/">interview Hal Abelson</a>, the famous MIT professor and co-author of the <i>Structure and Interpretation of Computer Programs</i>. The first time I talked to him I asked my usual question about reading code and he gave the standard answer‚Äîthat it was important and we should do it more. But he too failed to name any code he had read recently other than code he was obliged to: reviewing co-workers‚Äô code at Google where he was on sabbatical and grading student code at MIT. Later I asked him about this disconnect:</p>
                <blockquote>
                <div>
                    <p><i>Seibel:</i> I‚Äôm still curious about this split between what people say and what they actually do. Everyone says, ‚ÄúPeople should read code‚Äù but few people seem to actually do it. I‚Äôd be surprised if I interviewed a novelist and asked them what the last novel they had read was, and they said, ‚ÄúOh, I haven‚Äôt really read a novel since I was in grad school.‚Äù Writers actually read other writers but it doesn‚Äôt seem that programmers really do, even though we say we should.</p>
                    <p><i>Abelson:</i> Yeah. You‚Äôre right. But remember, a lot of times you crud up a program to make it finally work and do all of the things that you need it to do, so there‚Äôs a lot of extraneous stuff around there that isn‚Äôt the core idea.</p>
                    <p><i>Seibel:</i> So basically you‚Äôre saying that in the end, most code isn‚Äôt worth reading?</p>
                    <p><i>Abelson:</i> Or it‚Äôs built from an initial plan or some kind of pseudocode. A lot of the code in books, they have some very cleaned-up version that doesn‚Äôt do all the stuff it needs to make it work.</p>
                    <p><i>Seibel:</i> I‚Äôm thinking of the preface to SICP, where it says, ‚Äúprograms must be written for people to read and only incidentally for machines to execute.‚Äù But it seems the reality you just described is that in fact, most programs are written for machines to execute and only incidentally, if at all, for people to read.</p>
                    <p><i>Abelson:</i> Well, I think they start out for people to read, because there‚Äôs some idea there. You explain stuff. That‚Äôs a little bit of what we have in the book. There are some fairly significant programs in the book, like the compiler. And that‚Äôs partly because we think the easiest way to explain what it‚Äôs doing is to express that in code.</p>
                </div>
                </blockquote>
                <p>Yet somehow even this explicit acknowledgement that most real code isn‚Äôt actually in a form that can be simply read wasn‚Äôt enough to lead me to abandon the literature seminar model when I got to Etsy. For our first meeting I picked Jeremy Ashkenas‚Äôs backbone.js because many of the Etsy developers would be familiar with Javascript and because I know Jeremy is particularly interested in writing readable code. I still envisioned something like a literature seminar but I figured that a lot of people wouldn‚Äôt actually have done the reading in advance (well, maybe not so different from a literature seminar) so I decided to start things off by presenting the code myself before the group discussion.</p>
                <p>As I prepared my presentation, I found myself falling into my usual pattern when trying to really understand a piece of code‚Äîin order to grok it I have to essentially rewrite it. I‚Äôll start by renaming a few things so they make more sense to me and then I‚Äôll move things around to suit my ideas about how to organize code. Pretty soon I‚Äôll have gotten deep into the abstractions (or lack thereof) of the code and will start making bigger changes to the structure of the code. Once I‚Äôve completely rewritten the thing I usually understand it pretty well and can even go back to the original and understand it too. I have always felt kind of bad about this approach to code reading but it's the only thing that's ever worked for me.</p>
                <p>My presentation to the code reading group started with stock backbone.js and then walked through the changes I would make to it to make it, by my lights, more understandable. At one point I asked if people thought we should move on to the group discussion but nobody seemed very interested. Hopefully seeing my refactoring gave people some of the same insights into the underlying structure of the original that I had obtained by doing the refactoring.</p>
                <p>The second meeting of the Etsy code reading group featured Avi Bryant demonstrating how to use the code browsing capabilities of Smalltalk to navigate through some code. In that case, because few of the Etsy engineers had any experience with Smalltalk, we had no expectation that folks would read the code in advance. But the presentation was an awesome chance for folks to get exposed to the power of the Smalltalk development environment and for me to heckle Avi about Smalltalk vs Lisp differences.</p>
                <hr>
                <p>When I got to Twitter I inexplicably still had the literature seminar model in mind even though neither of the two meetings of the Etsy reading group‚Äîwhich folks seemed to like pretty well‚Äîfollowed that model at all. When I sent out the email inviting Twitter engineers to join a code reading group the response was pretty enthusiastic. The first meeting was, yet again, a presentation of some code, in this case the internals of the Scala implementation of Future that is used throughout Twitter‚Äôs many services, presented by Marius Eriksen, who wrote most of it.</p>
                <p>It was sometime after that presentation that I finally realized the obvious: code is not literature. We don‚Äôt read code, we <i>decode</i> it. We examine it. A piece of code is not literature; it is a specimen. Knuth said something that should have pointed me down this track when I asked him about his own code reading:</p>
                <blockquote>
                <p><i>Knuth:</i> But it‚Äôs really worth it for what it builds in your brain. So how do I do it? There was a machine called the Bunker Ramo 300 and somebody told me that the Fortran compiler for this machine was really amazingly fast, but nobody had any idea why it worked. I got a copy of the source-code listing for it. I didn‚Äôt have a manual for the machine, so I wasn‚Äôt even sure what the machine language was.</p>
                <p>But I took it as an interesting challenge. I could figure out <code>BEGIN</code> and then I would start to decode. The operation codes had some two-letter mnemonics and so I could start to figure out ‚ÄúThis probably was a load instruction, this probably was a branch.‚Äù And I knew it was a Fortran compiler, so at some point it looked at column seven of a card, and that was where it would tell if it was a comment or not.</p>
                <p>After three hours I had figured out a little bit about the machine. Then I found these big, branching tables. So it was a puzzle and I kept just making little charts like I‚Äôm working at a security agency trying to decode a secret code. But I knew it worked and I knew it was a Fortran compiler‚Äîit wasn‚Äôt encrypted in the sense that it was intentionally obscure; it was only in code because I hadn‚Äôt gotten the manual for the machine.</p>
                <p>Eventually I was able to figure out why this compiler was so fast. Unfortunately it wasn‚Äôt because the algorithms were brilliant; it was just because they had used unstructured programming and hand optimized the code to the hilt.</p>
                <p>It was just basically the way you solve some kind of an unknown puzzle‚Äîmake tables and charts and get a little more information here and make a hypothesis. In general when I‚Äôm reading a technical paper, it‚Äôs the same challenge. I‚Äôm trying to get into the author‚Äôs mind, trying to figure out what the concept is. The more you learn to read other people‚Äôs stuff, the more able you are to invent your own in the future, it seems to me.</p>
                </blockquote>
                <p>He‚Äôs not describing reading literature; he‚Äôs describing a scientific investigation. So now I have a new mode for how people should get together to gain insights from code which I explained to the Twitter code reading group like this:</p>
                <blockquote>
                <p>Preparing for the talk I‚Äôm going to give to the Girls who Code cohort, I started thinking about what to tell them about code reading and code they should read. And once again it struck me that for all the lip service we pay to the idea of reading code, most programmers really don‚Äôt read that much code, at least not just for the sake of reading it. As a simple proof: name me one piece of code that you‚Äôve read and that you can be reasonably sure that most other good programmers will have read or will at least have heard of. Not many, right? Probably none.</p>
                <p>But then it hit me. Code is not literature and we are not readers. Rather, interesting pieces of code are specimens and we are naturalists. So instead of trying to pick out a piece of code and reading it and then discussing it like a bunch of Comp Lit. grad students, I think a better model is for one of us to play the role of a 19th century naturalist returning from a trip to some exotic island to present to the local scientific society a discussion of the crazy beetles they found: ‚ÄúLook at the antenna on this monster! They look incredibly ungainly but the male of the species can use these to kill small frogs in whose carcass the females lay their eggs.‚Äù</p>
                <p>The point of such a presentation is to take a piece of code that the presenter has understood deeply and for them to help the audience understand the core ideas by pointing them out amidst the layers of evolutionary detritus (a.k.a. kluges) that are also part of almost all code. One reasonable approach might be to show the real code and then to show a stripped down reimplementation of just the key bits, kind of like a biologist staining a specimen to make various features easier to discern.</p>
                <p>The ideal presentation should be aimed at an audience of gentleman and lady programmers‚Äîsmart, and generally capable but without, necessarily, any specific knowledge of the domain from which the code comes. Presentations should provide enough context for the audience to to understand what the code is and should explain any details of the implementation language that may be obscure to the average programmer.</p>
                </blockquote>
                <p>Since I had my epiphany we‚Äôve had several meetings of the code reading group, now known as the Royal Society of Twitter for Improving Coding Knowledge, along the new lines. We‚Äôre still learning about the best ways to present code but the model feels very right. Also, I no longer feel bad about my dissection-based approach to reading code.</p>
                <p>The biggest lesson so far is that code is very dense. A half hour presentation is just enough time to present maybe a dozen meaty lines of code and one main idea. It is also almost certainly the case that the presenters, who have to actually really dig down into a piece of code, get more out of it than anybody. But it does seem that a good presentation can at least expose people to the main ideas and maybe give them a head start if they do decide to read the code themselves.</p>
                <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Schizophrenia drugs may have been off target for decades, study finds (134 pts)]]></title>
            <link>https://www.msn.com/en-us/health/medical/schizophrenia-drugs-may-have-been-off-target-for-decades-study-finds/ar-AA1fh9Nt</link>
            <guid>37134373</guid>
            <pubDate>Tue, 15 Aug 2023 14:24:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/health/medical/schizophrenia-drugs-may-have-been-off-target-for-decades-study-finds/ar-AA1fh9Nt">https://www.msn.com/en-us/health/medical/schizophrenia-drugs-may-have-been-off-target-for-decades-study-finds/ar-AA1fh9Nt</a>, See on <a href="https://news.ycombinator.com/item?id=37134373">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox finally outperforming Google Chrome in SunSpider (767 pts)]]></title>
            <link>https://www.phoronix.com/news/Firefox-Faster-SunSpider</link>
            <guid>37134092</guid>
            <pubDate>Tue, 15 Aug 2023 13:58:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Firefox-Faster-SunSpider">https://www.phoronix.com/news/Firefox-Faster-SunSpider</a>, See on <a href="https://news.ycombinator.com/item?id=37134092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="MOZILLA" src="https://www.phoronix.com/assets/categories/mozilla.webp" width="100" height="100"></p><p>
Mozilla developers are celebrating that they are now faster than Google Chrome with the SunSpider JavaScript benchmark, although that test has been superseded by the JetStream benchmark.
</p><p>
Last week a new Firefox Nightly News was published that outlines that "We‚Äôre now apparently beating Chrome on the SunSpider JavaScript benchmark!" The provided numbers now show Firefox easily beating Chrome in this decade-old JavaScript benchmark.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=firefox_faster_sunspider" alt="SunSpider browser benchmark results"></p>
<p>The benchmarks come from <a href="https://arewefastyet.com/win10/benchmarks/overview?numDays=60">AreWeFastYet.com</a>. Meanwhile for the newer and more demanding JetStream 2.0 benchmark, Google Chrome continues to win easily over Firefox:
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=chrome_jetstream_win" alt="Chrome much faster in JetStream 2"></p>
<p>Besides Firefox running the JavaScript SunSpider benchmark much faster over the roughly past month, there's been work on the HTTP/2 upload speed improvements, and various other enhancements.
</p><p>
Learn about the latest Firefox Nightly build advancements via the <a href="https://blog.nightly.mozilla.org/2023/08/10/a-view-to-a-better-faster-web-these-weeks-in-firefox-issue-143/">Firefox Nightly News</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Entrepreneurship for Engineers: Selling Open Source Software (160 pts)]]></title>
            <link>https://thenewstack.io/entrepreneurship-for-engineers-selling-open-source-software/</link>
            <guid>37134089</guid>
            <pubDate>Tue, 15 Aug 2023 13:58:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/entrepreneurship-for-engineers-selling-open-source-software/">https://thenewstack.io/entrepreneurship-for-engineers-selling-open-source-software/</a>, See on <a href="https://news.ycombinator.com/item?id=37134089">Hacker News</a></p>
Couldn't get https://thenewstack.io/entrepreneurship-for-engineers-selling-open-source-software/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Stellar Developers (602 pts)]]></title>
            <link>https://stellarsamurai.com/</link>
            <guid>37133872</guid>
            <pubDate>Tue, 15 Aug 2023 13:38:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stellarsamurai.com/">https://stellarsamurai.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37133872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MainContent" role="main" tabindex="-1">
      <div id="shopify-section-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24"><p>
        <h2 id="SectionHeading-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24">
          Favorite Categories
        </h2></p><slider-component>
      <ul id="Slider-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24" role="list"><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-1" data-cascade="">


          </li><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-2" data-cascade="">


          </li><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-3" data-cascade="">


          </li></ul></slider-component></div><div id="shopify-section-template--19422400577876__256b0e40-e01d-4353-8ed0-5b40603a99f1">
    <div>
      <p><img src="https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=1500" alt="" srcset="https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=198 198w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=432 432w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=642 642w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=900 900w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=1284 1284w" width="1500" height="1500" loading="lazy" sizes="(min-width: 1200px) 659.9868002639947px,
              (min-width: 750px) calc((100vw - 130px) / 1.667), calc((100vw - 50px) / 1.667)">
</p>
    </div>
    <div id="ImageWithText--template--19422400577876__256b0e40-e01d-4353-8ed0-5b40603a99f1"><h2>
                Our Vision
              </h2><div>
                <p>We aim to portray the essence of ancient warriors know as Samurai into everyday clothing and accessories.</p><p>Each  design holds a story, wisdom and energy that we want to share with the world.</p><p>Unique clothing, accessories and home d√©cor that gives a new feeling and an elegant touch. </p>
              </div><p><a href="https://stellarsamurai.com/pages/about">
                  Learn more about us
                </a></p></div>
  </div><div id="shopify-section-template--19422400577876__c5052349-f3ac-4407-aee0-d90ebc946611"><p>As a part of our launch, all customers get:</p><h2 data-cascade="">
                20% Discount for orders over 150$!
              </h2></div><div id="shopify-section-template--19422400577876__9207cebc-a5dc-4b77-8753-369fdc80294f">
    <h2>
      Find your style!
    </h2>
    
  </div><div data-cascade="" id="shopify-section-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3"><slider-component>
      <ul id="Slider-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3" role="list"><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-1" data-cascade="">
            <div><h3>Quality</h3><p>Each product is tested and reviewed before it reaches your home. We do our best to ensure the quality of each item meets our standards.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-2" data-cascade="">
            <div><h3>Durability</h3><p>We optimize for better and not cheaper materials. This type of approach makes sure that each design will last for as long as possible.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-3" data-cascade="">
            <div><h3>Style</h3><p>Whether it's a backpack, T-Shirt or phone case each product holds it's own unique look that catches the eye and makes you stand out.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-4" data-cascade="">
            <div><h3>Uniqueness</h3><p>No two designs are the same. Explore our various palettes and be pleasantly surprised with the volume of variety that we have prepared!</p></div>
          </li></ul></slider-component>
    
  </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why host your own LLM? (236 pts)]]></title>
            <link>http://marble.onl/posts/why_host_your_own_llm.html</link>
            <guid>37133504</guid>
            <pubDate>Tue, 15 Aug 2023 13:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://marble.onl/posts/why_host_your_own_llm.html">http://marble.onl/posts/why_host_your_own_llm.html</a>, See on <a href="https://news.ycombinator.com/item?id=37133504">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Andrew Marble<br><a href="http://marble.onl/">marble.onl</a><br>andrew@willows.ai<br>August 13, 2023</p>
<p>In the Terminator movies, good relationships beat technological superiority. Kyle Reese and Sarah Connor outwit the advanced T-800 who in turn helps Sarah and John beat the ultra-advanced T-1000. OpenAI‚Äôs GPT-4 is currently the most advanced publicly available language model. There are also analyses showing it‚Äôs generally cheaper to run than self-hosting comparable models. I want to argue that despite everything OpenAI‚Äôs models have going for them, it‚Äôs worth considering self-hosting anyway, especially if you‚Äôre building a product or an internal capability.</p>
<p>If you‚Äôre using language models for custom applications, you can use an API from companies like OpenAI or Anthropic, where you submit your prompt, get a response, and pay usage based fees. Or you can configure your own model and host it locally or in the cloud. There are many models available for self-hosting. A few recent analyses have made a case for using OpenAI‚Äôs API based on cost and performance<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a><sup>,</sup><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Very detailed cost calculations are possible, but the obvious cost advantage of a usage based API is that you only pay for the hardware when you use it. Most self-hosted applications will be challenged to get good utilization from dedicated GPUs and so are paying a lot for idle time.</p>
<p>There‚Äôs a lot of subtlety in gauging performance ‚Äì personally I think there‚Äôs not a 1:1 relationship between rankings in the various benchmarks and ‚Äúleaderboards‚Äù<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and performance on specific commercially relevant tasks. But GPT-4 is unequivocally better than the rest across a wide range of skills and only the best publicly available models compete with Claude (Anthropic‚Äôs model) and GPT-3.5.</p>
<p>Despite the advantages, there is still a compelling case for working with publicly available models. (Note I don‚Äôt say open source, many have other limitations that disqualify them from being labeled as such<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, but I won‚Äôt dwell on that here.) To me it boils down to a the ‚Äúrelationship‚Äù. Using APIs means you‚Äôre a taker of whatever OpenAI et al. are offering. Model features, customizations, values (censorship and world view) etc. are all dictated by those companies. You can only build a front-end. This also means you don‚Äôt have access to internal states and so are limited, for example in applying advanced accountability techniques or guardrails on top. All of this could be good, it means you don‚Äôt have to worry about it. But it also makes whatever you build utterly dependent on a start-up company.</p>
<p>For ‚Äúrelationship-based‚Äù development, there are good reasons to use self-hosted models. Having control over the model architecture and weights removes uncertainty about future changes, and means you don‚Äôt have to take what OpenAI decides to give you. There is a rich ecosystem of different models to experiment with, as well as the ability to customize ‚Äì for example by fine-tuning on your own terms. The construct ultimately lets you build a long-term relationship with your AI model and adapt your product around it, having clarity that what you build is going to keep working with the model that you‚Äôve chosen and giving you control over when and if you decide to make changes. It lets you build something that isn‚Äôt just a front-end on somebody else‚Äôs language model but is deeply integrated.</p>
<p>Also, for many applications, the well-rounded superiority of a GPT-like model is not what‚Äôs driving value. Running a model as big as GPT-4 is potentially $10,000‚Äôs per month. But it‚Äôs possible to run 7B and 13B models (models with 7 and 13 billion parameters, common sizes for LLaMA and other public models) on a laptop. These models are big enough to perform many tasks competently and can be cost-effective as part of local systems.</p>
<p>‚ÄúResponsible‚Äù use of AI has many meanings. Tech companies have often focused on political correctness and superficial notions of bias, largely to avoid controversy in broadly capable public models like chat-GPT. For many applications, particularly specialized knowledge work<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>, those concerns are mostly irrelevant and give way to real issues about factual accuracy, completeness, or simply staying on-topic. Many techniques for ‚Äúkeeping models in line‚Äù require access to internal states, gradients, and intermediate outputs<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Using an API-based model limits the kind of experimentation and augmentation that is possible.</p>
<p>The same holds true for various optimizations such as caching internal model states, as well as model fine-tuning. APIs offer options, but they are limited compared to what is available. The technology is evolving so quickly still that new models and techniques are becoming available every day. For those that are using the LLM as a tightly integrated part of a product or tool, the only way to have the flexibility to evolve with the technology is to have a self-hosted model.</p>
<p>An additional aspect of the fast pace of change in language models right now is that the skills and knowledge required to work with the technology are evolving quickly. Working with self-hosted models gives institutional and individual experience in this evolving landscape, in a way that APIs don‚Äôt. For professional development of employees as well as adaptability to change, keeping ‚ÄúAI‚Äù at a deeper technical level is important for many companies, particularly those that are building applications. It‚Äôs not a mature technology, and part of the ‚Äúmoat‚Äù that we practitioners have is simply knowing what‚Äôs going on. I‚Äôll actually go further and say that any organization making nontrivial use of AI should internally or through advisers have access to some deep knowledge of the technology, not just the API reference, to be able to understand what it fundamentally does best. As AI gets commodified and hyped-up, there often ends up being a big disconnect between what it can do and what it‚Äôs used or proposed for.</p>
<p>In a few years, I expect the landscape will look very different ‚Äì there will be agreed upon things that are critical to be able to do with a model, and APIs will support this. For a new, still experimental, and rapidly evolving technology, real participation requires deep access to the models and code. This doesn‚Äôt mean that all companies or products require such access ‚Äì there are many valuable things that can be built on top of an API and would probably be a waste of time to self-host. But these are different kinds of products.</p>
<p>Back to the Terminator, Reese and the T-800 both built strong relationships that led to their successful completion of their missions. The Skynet-tasked Terminators just went around flexing their superior technological prowess, and it wasn‚Äôt enough to win the day. Part of building the relationships is access. I know it‚Äôs a silly analogy, but I believe the same is true with these models, it‚Äôs about being able to deeply understand the strengths of the tool and built something tightly integrated, and you can‚Äôt do that with an API.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526">https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526</a><a href="#fnref1" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://www.cursor.so/blog/llama-inference">https://www.cursor.so/blog/llama-inference</a><a href="#fnref2" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a><a href="#fnref3" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="http://marble.onl/posts/software-licenses-masquerading-as-open-source.html">http://marble.onl/posts/software-licenses-masquerading-as-open-source.html</a><a href="#fnref4" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5" role="doc-endnote"><p>https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/<a href="#fnref5" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn6" role="doc-endnote"><p>There is a wide range of literature and research into model accountability. For a narrow example, see ‚ÄúThe Internal State of an LLM Knows When its Lying‚Äù <a href="https://arxiv.org/pdf/2304.13734.pdf">https://arxiv.org/pdf/2304.13734.pdf</a> but also <a href="https://arxiv.org/pdf/2307.00175.pdf">https://arxiv.org/pdf/2307.00175.pdf</a><a href="#fnref6" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Llama2 Embeddings FastAPI Server (154 pts)]]></title>
            <link>https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service</link>
            <guid>37133163</guid>
            <pubDate>Tue, 15 Aug 2023 12:31:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service">https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service</a>, See on <a href="https://news.ycombinator.com/item?id=37133163">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Llama2 Embeddings FastAPI Service</h2>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto">The Llama2 Embedding Server is designed to facilitate and optimize the process of obtaining text embeddings using different LLMs via llama_cpp and langchain. To avoid wasting computation, these embeddings are cached in SQlite and retrieved if they have already been computed before. To speed up the process of loading multiple LLMs, optional RAM Disks can be used, and the process for creating and managing them is handled automatically for you.</p>
<p dir="auto">Some additional useful endpoints are provided, such as computing semantic similarity between submitted text strings (using various measures of similarity, such as cosine similarity, but also more esoteric measures like <a href="https://blogs.sas.com/content/iml/2021/05/03/examples-hoeffding-d.html" rel="nofollow">Hoeffding's D</a> and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705121008297" rel="nofollow">HSIC</a>, and semantic search across all your cached embeddings using FAISS vector searching.</p>
<p dir="auto">You can also submit a plaintext file or PDF file (not requiring OCR) and get back a zip file containing all of the embeddings for each sentence as JSON, organized in various ways such <code>records</code>, <code>table</code>, etc. (i.e., all the export options from the Pandas <code>to_json()</code> function). The results of getting the embeddings for all sentences in a document can be returned either as a zip file containing a JSON file (so it won't crash Swagger among other things), or as a direct JSON response if you're using curl or similar.</p>
<p dir="auto">In addition to fixed-sized embedding vectors, we also expose functionality that allows you to get back token-level embeddings, where each token in the input stream is embedded with its context in the string as a full sized vector, thus producing a matrix that has a number of rows equal to the number of tokens in the input string. This includes far more nuanced information about the contents of the string at the expense of much greater compute and storage requirements. The other drawback is that, instead of having the same sized output for every string, regardless of length (which makes it very easy to compare unequal length strings using cosine similarity and other measures), the token-level embedding matrix obviously differs in dimensions for two different strings if the strings have different numbers of tokens. To deal with this, we introduce combined feature vectors, which compute the column-wise mean, min, max, and std. deviation of the token-level emeddding matrix, and concatenate these together in to a single huge matrix; this allows you to compare strings of different lengths while still capturing more nuance. The combined results, including the embedding matrix and associated combined feature vector, can similarly be returned as either a zip file or direct JSON response.</p>
<h2 tabindex="-1" dir="auto">Screenshot</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service/raw/main/Llama2-FastAPI-Service-%20Swagger%20Screenshot.png"><img src="https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service/raw/main/Llama2-FastAPI-Service-%20Swagger%20Screenshot.png" alt="Llama2 FastAPI Service Swagger UI"></a></p>
<p dir="auto"><em>TLDR:</em> If you just want to try it very quickly on a fresh Ubuntu 22+ machine (warning, this will install docker using apt):</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service
cd llama_embeddings_fastapi_service
chmod +x setup_dockerized_app_on_fresh_machine.sh
sudo ./setup_dockerized_app_on_fresh_machine.sh"><pre>git clone https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service
<span>cd</span> llama_embeddings_fastapi_service
chmod +x setup_dockerized_app_on_fresh_machine.sh
sudo ./setup_dockerized_app_on_fresh_machine.sh</pre></div>
<p dir="auto">Then open a browser to <code>&lt;your_static_ip_address&gt;:8089</code> if you're using a VPS.</p>
<p dir="auto">Or to <code>localhost:8089</code> if you're using your own machine-- but, really, you should never run untrusted code with sudo on your own machine! Just get a cheap VPS to experiment with for $30/month.</p>
<p dir="auto">Watch the the automated setup process in action <a href="https://asciinema.org/a/601603" rel="nofollow">here</a>.</p>
<hr>
<h2 tabindex="-1" dir="auto">Features</h2>
<ol dir="auto">
<li><strong>Text Embedding Computation</strong>: Utilizes pre-trained LLama2 and other LLMs via llama_cpp and langchain to generate embeddings for any provided text, including token-level embeddings that capture more nuanced information about the content.</li>
<li><strong>Embedding Caching</strong>: Efficiently stores and retrieves computed embeddings in SQLite, minimizing redundant computations. It supports caching both fixed-sized embedding vectors and token-level embeddings.</li>
<li><strong>Advanced Similarity Measurements and Retrieval</strong>: Offers various measures of similarity like cosine similarity, Hoeffding's D, HSIC, and semantic search across cached embeddings using FAISS vector searching.</li>
<li><strong>File Processing for Documents</strong>: Submit plaintext files or PDFs (not requiring OCR) to get back a ZIP file or JSON response containing embeddings for each sentence, organized in various ways like <code>records</code>, <code>table</code>, etc., using Pandas <code>to_json()</code> function.</li>
<li><strong>Token-Level Embeddings and Combined Feature Vectors</strong>: Provides token-level embeddings to capture the context of each token in the input string. Introduces combined feature vectors by computing the column-wise mean, min, max, and std. deviation of the token-level embedding matrix, allowing comparison of unequal length strings.</li>
<li><strong>RAM Disk Usage</strong>: Optionally uses RAM Disk to store models for faster access and execution. Automatically handles the creation and management of RAM Disks.</li>
<li><strong>Robust Exception Handling</strong>: Features comprehensive exception management to ensure system resilience.</li>
<li><strong>Interactive API Documentation</strong>: Integrates with Swagger UI for an interactive and user-friendly experience, accommodating large result sets without crashing.</li>
<li><strong>Scalability and Concurrency</strong>: Built on the FastAPI framework, handles concurrent requests and supports parallel inference with configurable concurrency levels.</li>
<li><strong>Flexible Configurations</strong>: Offers configurable settings through environment variables and input parameters, including response formats like JSON or ZIP files.</li>
<li><strong>Comprehensive Logging</strong>: Captures essential information with detailed logs, without overwhelming storage or readability.</li>
<li><strong>Support for Multiple Models and Measures</strong>: Accommodates multiple embedding models and similarity measures, allowing flexibility and customization based on user needs.</li>
</ol>
<h2 tabindex="-1" dir="auto">Demo Screen Recording in Action:</h2>
<p dir="auto"><a href="https://asciinema.org/a/39dZ8vv9nkcNygasUl35wnBPq" rel="nofollow">Here</a> is the live console output while I interact with it from the Swagger page to make requests.</p>
<hr>
<h2 tabindex="-1" dir="auto">Requirements:</h2>
<div data-snippet-clipboard-copy-content="fastapi
pydantic
uvicorn
sqlalchemy
python-decouple
psutil
aiosqlite
faiss-cpu
pandas
PyPDF2
python-multipart
python-magic
langchain
scikit-learn
llama-cpp-python
httpx
numba
scipy
hyppo"><pre><code>fastapi
pydantic
uvicorn
sqlalchemy
python-decouple
psutil
aiosqlite
faiss-cpu
pandas
PyPDF2
python-multipart
python-magic
langchain
scikit-learn
llama-cpp-python
httpx
numba
scipy
hyppo
</code></pre></div>
<h2 tabindex="-1" dir="auto">Running the Application</h2>
<p dir="auto">You can run the application using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python llama_2_embeddings_fastapi_server.py"><pre>python llama_2_embeddings_fastapi_server.py</pre></div>
<p dir="auto">The server will start on <code>0.0.0.0</code> at the port defined by the <code>LLAMA_EMBEDDING_SERVER_LISTEN_PORT</code> variable.</p>
<p dir="auto">Access the Swagger UI:</p>
<div data-snippet-clipboard-copy-content="http://localhost:<LLAMA_EMBEDDING_SERVER_LISTEN_PORT>"><pre><code>http://localhost:&lt;LLAMA_EMBEDDING_SERVER_LISTEN_PORT&gt;
</code></pre></div>
<h2 tabindex="-1" dir="auto">Configuration</h2>
<p dir="auto">You can configure the service easily by editing the included <code>.env</code> file. Here's a list of available configuration options:</p>
<ul dir="auto">
<li><code>USE_SECURITY_TOKEN</code>: Whether to use a hardcoded security token. (e.g., <code>True</code>)</li>
<li><code>USE_PARALLEL_INFERENCE_QUEUE</code>: Use parallel processing. (e.g., <code>True</code>)</li>
<li><code>MAX_CONCURRENT_PARALLEL_INFERENCE_TASKS</code>: Maximum number of parallel inference tasks. (e.g., <code>30</code>)</li>
<li><code>DEFAULT_MODEL_NAME</code>: Default model name to use. (e.g., <code>llama2_7b_chat_uncensored</code>)</li>
<li><code>LLM_CONTEXT_SIZE_IN_TOKENS</code>: Context size in tokens for LLM. (e.g., <code>512</code>)</li>
<li><code>LLAMA_EMBEDDING_SERVER_LISTEN_PORT</code>: Port number for the service. (e.g., <code>8089</code>)</li>
<li><code>MINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING</code>: Minimum string length for document embedding. (e.g., <code>15</code>)</li>
<li><code>MAX_RETRIES</code>: Maximum retries for locked database. (e.g., <code>10</code>)</li>
<li><code>DB_WRITE_BATCH_SIZE</code>: Database write batch size. (e.g., <code>25</code>)</li>
<li><code>RETRY_DELAY_BASE_SECONDS</code>: Retry delay base in seconds. (e.g., <code>1</code>)</li>
<li><code>JITTER_FACTOR</code>: Jitter factor for retries. (e.g., <code>0.1</code>)</li>
<li><code>USE_RAMDISK</code>: Use RAM disk. (e.g., <code>True</code>)</li>
<li><code>RAMDISK_PATH</code>: Path to the RAM disk. (e.g., <code>"/mnt/ramdisk"</code>)</li>
<li><code>RAMDISK_SIZE_IN_GB</code>: RAM disk size in GB. (e.g., <code>40</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">If you'd like to contribute to the project, please submit a pull request.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This project is licensed under the MIT License.</p>
<hr>
<h2 tabindex="-1" dir="auto">Setup and Configuration</h2>
<h3 tabindex="-1" dir="auto">RAM Disk Configuration</h3>
<p dir="auto">To enable password-less sudo for RAM Disk setup and teardown, edit the <code>sudoers</code> file with <code>sudo visudo</code>. Add the following lines, replacing <code>username</code> with your actual username:</p>
<div data-snippet-clipboard-copy-content="username ALL=(ALL) NOPASSWD: /bin/mount -t tmpfs -o size=*G tmpfs /mnt/ramdisk
username ALL=(ALL) NOPASSWD: /bin/umount /mnt/ramdisk"><pre lang="plaintext"><code>username ALL=(ALL) NOPASSWD: /bin/mount -t tmpfs -o size=*G tmpfs /mnt/ramdisk
username ALL=(ALL) NOPASSWD: /bin/umount /mnt/ramdisk
</code></pre></div>
<p dir="auto">The application provides functionalities to set up, clear, and manage RAM Disk. RAM Disk is used to store models in memory for faster access. It calculates the available RAM and sets up the RAM Disk accordingly. The functions <code>setup_ramdisk</code>, <code>copy_models_to_ramdisk</code>, and <code>clear_ramdisk</code> manage these tasks.</p>
<h2 tabindex="-1" dir="auto">API Endpoints</h2>
<p dir="auto">The following endpoints are available:</p>
<ul dir="auto">
<li><strong>GET <code>/get_list_of_available_model_names/</code></strong>: Retrieve Available Model Names. Retrieves the list of available model names for generating embeddings.</li>
<li><strong>GET <code>/get_all_stored_strings/</code></strong>: Retrieve All Strings. Retrieves a list of all stored strings from the database for which embeddings have been computed.</li>
<li><strong>GET <code>/get_all_stored_documents/</code></strong>: Retrieve All Stored Documents. Retrieves a list of all stored documents from the database for which embeddings have been computed.</li>
<li><strong>POST <code>/get_embedding_vector_for_string/</code></strong>: Retrieve Embedding Vector for a Given Text String. Retrieves the embedding vector for a given input text string using the specified model.</li>
<li><strong>POST <code>/get_token_level_embeddings_matrix_and_combined_feature_vector_for_string/</code></strong>: Retrieve Token-Level Embeddings and Combined Feature Vector for a Given Input String. Retrieve the token-level embeddings and combined feature vector for a given input text using the specified model.</li>
<li><strong>POST <code>/compute_similarity_between_strings/</code></strong>: Compute Similarity Between Two Strings. Compute the similarity between two given input strings using specified model embeddings and a selected similarity measure.</li>
<li><strong>POST <code>/search_stored_embeddings_with_query_string_for_semantic_similarity/</code></strong>: Get Most Similar Strings from Stored Embeddings in Database. Find the most similar strings in the database to the given input "query" text.</li>
<li><strong>POST <code>/get_all_embedding_vectors_for_document/</code></strong>: Get Embeddings for a Document. Extract text embeddings for a document, supporting both plain text and PDF files (PDFs requiring OCR are not supported).</li>
<li><strong>POST <code>/clear_ramdisk/</code></strong>: Clear Ramdisk Endpoint. Clears the RAM Disk if it is enabled.</li>
</ul>
<p dir="auto">For detailed request and response schemas, please refer to the Swagger UI available at the root URL or the section at the end of this <code>README</code>.</p>
<h2 tabindex="-1" dir="auto">Exception Handling</h2>
<p dir="auto">The application has robust exception handling to deal with various types of errors, including database errors and general exceptions. Custom exception handlers are defined for <code>SQLAlchemyError</code> and general <code>Exception</code>.</p>
<h2 tabindex="-1" dir="auto">Logging</h2>
<p dir="auto">Logging is configured at the INFO level to provide detailed logs for debugging and monitoring. The logger provides information about the state of the application, errors, and activities.</p>
<p dir="auto">The logs are stored in a file named <code>llama2_embeddings_fastapi_service.log</code>, and a log rotation mechanism is implemented to handle log file backups. The rotating file handler is configured with a maximum file size of 10 MB, and it keeps up to 5 backup files.</p>
<p dir="auto">When a log file reaches its maximum size, it is moved to the <code>old_logs</code> directory, and a new log file is created. The log entries are also printed to the standard output stream.</p>
<p dir="auto">Here are some details of the logging configuration:</p>
<ul dir="auto">
<li>Log Level: INFO</li>
<li>Log Format: <code>%(asctime)s - %(levelname)s - %(message)s</code></li>
<li>Max Log File Size: 10 MB</li>
<li>Backup Count: 5</li>
<li>Old Logs Directory: <code>old_logs</code></li>
</ul>
<p dir="auto">Additionally, the log level for SQLAlchemy's engine is set to WARNING to suppress verbose database logs.</p>
<h2 tabindex="-1" dir="auto">Database Structure</h2>
<p dir="auto">The application uses a SQLite database via SQLAlchemy ORM. Here are the data models used, which can be found in the <code>embeddings_data_models.py</code> file:</p>
<h3 tabindex="-1" dir="auto">TextEmbedding Table</h3>
<p dir="auto">This table stores individual text embeddings.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>text</code>: Text for which the embedding was computed</li>
<li><code>text_hash</code>: Hash of the text, computed using SHA3-256</li>
<li><code>model_name</code>: Model used to compute the embedding</li>
<li><code>embedding_json</code>: The computed embedding in JSON format</li>
<li><code>ip_address</code>: Client IP address</li>
<li><code>request_time</code>: Timestamp of the request</li>
<li><code>response_time</code>: Timestamp of the response</li>
<li><code>total_time</code>: Total time taken to process the request</li>
<li><code>document_id</code>: Foreign Key referencing the DocumentEmbedding table</li>
<li>Unique Constraint on <code>text_hash</code> and <code>model_name</code></li>
</ul>
<h3 tabindex="-1" dir="auto">DocumentEmbedding Table</h3>
<p dir="auto">This table stores embeddings for entire documents.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>document_id</code>: Foreign Key referencing the Documents table</li>
<li><code>filename</code>: Name of the document file</li>
<li><code>mimetype</code>: MIME type of the document file</li>
<li><code>file_hash</code>: Hash of the file</li>
<li><code>model_name</code>: Model used to compute the embedding</li>
<li><code>file_data</code>: Binary data of the original file</li>
<li><code>document_embedding_results_json</code>: The computed embedding results in JSON format</li>
<li><code>ip_address</code>: Client IP address</li>
<li><code>request_time</code>: Timestamp of the request</li>
<li><code>response_time</code>: Timestamp of the response</li>
<li><code>total_time</code>: Total time taken to process the request</li>
<li>Unique Constraint on <code>file_hash</code> and <code>model_name</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Document Table</h3>
<p dir="auto">This table represents a document.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>model_name</code>: Model name associated with the document</li>
<li><code>document_hash</code>: Hash of the document (concatenation of specific attributes from the <code>document_embeddings</code> relationship)</li>
</ul>
<h3 tabindex="-1" dir="auto">TokenLevelEmbedding Table</h3>
<p dir="auto">This table stores token-level embeddings.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>token</code>: Token for which the embedding was computed</li>
<li><code>token_hash</code>: Hash of the token, computed using SHA3-256</li>
<li><code>model_name</code>: Model used to compute the embedding</li>
<li><code>token_level_embedding_json</code>: The computed token-level embedding in JSON format</li>
<li><code>ip_address</code>: Client IP address</li>
<li><code>request_time</code>: Timestamp of the request</li>
<li><code>response_time</code>: Timestamp of the response</li>
<li><code>total_time</code>: Total time taken to process the request</li>
<li><code>token_level_embedding_bundle_id</code>: Foreign Key referencing the TokenLevelEmbeddingBundle table</li>
<li>Unique Constraint on <code>token_hash</code> and <code>model_name</code></li>
</ul>
<h3 tabindex="-1" dir="auto">TokenLevelEmbeddingBundle Table</h3>
<p dir="auto">This table stores token-level embedding bundles.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>input_text</code>: Input text associated with the token-level embeddings</li>
<li><code>input_text_hash</code>: Hash of the input text</li>
<li><code>model_name</code>: Model used to compute the embeddings</li>
<li><code>token_level_embeddings_bundle_json</code>: JSON containing the token-level embeddings</li>
<li><code>ip_address</code>: Client IP address</li>
<li><code>request_time</code>: Timestamp of the request</li>
<li><code>response_time</code>: Timestamp of the response</li>
<li><code>total_time</code>: Total time taken to process the request</li>
<li>Unique Constraint on <code>input_text_hash</code> and <code>model_name</code></li>
</ul>
<h3 tabindex="-1" dir="auto">TokenLevelEmbeddingBundleCombinedFeatureVector Table</h3>
<p dir="auto">This table stores combined feature vectors for token-level embedding bundles.</p>
<ul dir="auto">
<li><code>id</code>: Primary Key</li>
<li><code>token_level_embedding_bundle_id</code>: Foreign Key referencing the TokenLevelEmbeddingBundle table</li>
<li><code>model_name</code>: Model name associated with the combined feature vector</li>
<li><code>combined_feature_vector_json</code>: JSON containing the combined feature vector</li>
<li><code>combined_feature_vector_hash</code>: Hash of the combined feature vector</li>
<li>Unique Constraint on <code>combined_feature_vector_hash</code> and <code>model_name</code></li>
</ul>
<h2 tabindex="-1" dir="auto">Performance Optimizations</h2>
<p dir="auto">This section highlights the major performance enhancements integrated into the provided code to ensure swift responses and optimal resource management.</p>
<h3 tabindex="-1" dir="auto">1. <strong>Asynchronous Programming</strong>:</h3>
<ul dir="auto">
<li><strong>Benefit</strong>: Handles multiple tasks concurrently, enhancing efficiency for I/O-bound operations like database transactions and network requests.</li>
<li><strong>Implementation</strong>: Utilizes Python's <code>asyncio</code> library for asynchronous database operations.</li>
</ul>
<h3 tabindex="-1" dir="auto">2. <strong>Database Optimizations</strong>:</h3>
<ul dir="auto">
<li><strong>Write-Ahead Logging (WAL) Mode</strong>: Enables concurrent reads and writes, optimizing for applications with frequent write demands.</li>
<li><strong>Retry Logic with Exponential Backoff</strong>: Manages locked databases by retrying operations with progressive waiting times.</li>
<li><strong>Batch Writes</strong>: Aggregates write operations for more efficient database interactions.</li>
<li><strong>DB Write Queue</strong>: Uses an asynchronous queue to serialize write operations, ensuring consistent and non-conflicting database writes.</li>
</ul>
<h3 tabindex="-1" dir="auto">3. <strong>RAM Disk Utilization</strong>:</h3>
<ul dir="auto">
<li><strong>Benefit</strong>: Speeds up I/O-bound tasks by prioritizing operations in RAM over disk.</li>
<li><strong>Implementation</strong>: Detects and prioritizes a RAM disk (<code>/mnt/ramdisk</code>) if available, otherwise defaults to the standard file system.</li>
</ul>
<h3 tabindex="-1" dir="auto">4. <strong>Model Caching</strong>:</h3>
<ul dir="auto">
<li><strong>Benefit</strong>: Reduces overhead by keeping loaded models in memory for subsequent requests.</li>
<li><strong>Implementation</strong>: Uses a global <code>model_cache</code> dictionary to store and retrieve models.</li>
</ul>
<h3 tabindex="-1" dir="auto">5. <strong>Parallel Inference</strong>:</h3>
<ul dir="auto">
<li><strong>Benefit</strong>: Enhances processing speed for multiple data units, like document sentences.</li>
<li><strong>Implementation</strong>: Employs <code>asyncio.gather</code> for concurrent inferences, regulated by a semaphore (<code>MAX_CONCURRENT_PARALLEL_INFERENCE_TASKS</code>).</li>
</ul>
<h3 tabindex="-1" dir="auto">6. <strong>Embedding Caching</strong>:</h3>
<ul dir="auto">
<li><strong>Benefit</strong>: Once embeddings are computed for a particular text, they are stored in the database, eliminating the need for re-computation during subsequent requests.</li>
<li><strong>Implementation</strong>: When a request is made to compute an embedding, the system first checks the database. If the embedding for the given text is found, it is returned immediately, ensuring faster response times.</li>
</ul>
<hr>
<h3 tabindex="-1" dir="auto">Dockerized Llama2 Embeddings API Service App</h3>
<p dir="auto">A bash script is included in this repo, <code>setup_dockerized_app_on_fresh_machine.sh</code>, that will automatically do everything for you, including installing docker with apt install.</p>
<p dir="auto">To use it, first make the script executable and then run it like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x setup_dockerized_app_on_fresh_machine.sh
sudo ./setup_dockerized_app_on_fresh_machine.sh"><pre>chmod +x setup_dockerized_app_on_fresh_machine.sh
sudo ./setup_dockerized_app_on_fresh_machine.sh</pre></div>
<p dir="auto">If you prefer a manual setup, then read the following instructions:</p>
<h4 tabindex="-1" dir="auto">Prerequisites</h4>
<p dir="auto">Ensure that you have Docker installed on your system. If not, follow these steps to install Docker on Ubuntu:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get update
sudo apt-get install docker.io
sudo systemctl start docker
sudo docker --version
sudo usermod -aG docker $USER"><pre>sudo apt-get update
sudo apt-get install docker.io
sudo systemctl start docker
sudo docker --version
sudo usermod -aG docker <span>$USER</span></pre></div>
<p dir="auto">You may need to log out and log back in or restart your system to apply the new group permissions, or use sudo in the following steps to build and run the container.</p>
<h4 tabindex="-1" dir="auto">Setup and Running the Application</h4>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the Repository:</strong></p>
<p dir="auto">Clone the Llama2 Embeddings API Service repository to your local machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service
cd llama_embeddings_fastapi_service"><pre>git clone https://github.com/Dicklesworthstone/llama_embeddings_fastapi_service
<span>cd</span> llama_embeddings_fastapi_service</pre></div>
</li>
<li>
<p dir="auto"><strong>Build the Docker Image:</strong></p>
<p dir="auto">Build the Docker image using the provided Dockerfile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo docker build -t llama-embeddings ."><pre>sudo docker build -t llama-embeddings <span>.</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Run the Docker Container:</strong></p>
<p dir="auto">Run the Docker container, mapping the container's port 8089 to the host's port 8089:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo docker run -p 8089:8089 llama-embeddings"><pre>sudo docker run -p 8089:8089 llama-embeddings</pre></div>
</li>
<li>
<p dir="auto"><strong>Accessing the Application:</strong></p>
<p dir="auto">The FastAPI application will now be accessible at <code>http://localhost:8089</code> or at the static IP address of your VPS instance if you're running on one (You can get a 10-core, 30gb RAM, 1tb SSD with a static IP running Ubuntu 22.04 at Contabo for around $30/month, which is the cheapest I've found so far).</p>
<p dir="auto">You can interact then with the API using tools like <code>curl</code> or by accessing the FastAPI documentation at <code>http://localhost:8089/docs</code>.</p>
</li>
<li>
<p dir="auto"><strong>Viewing Logs:</strong></p>
<p dir="auto">Logs from the application can be viewed directly in the terminal where you ran the <code>docker run</code> command.</p>
</li>
</ol>
<h4 tabindex="-1" dir="auto">Stopping and Managing the Container</h4>
<ul dir="auto">
<li>To stop the running container, press <code>Ctrl+C</code> in the terminal or find the container ID using <code>docker ps</code> and run <code>sudo docker stop &lt;container_id&gt;</code>.</li>
<li>To remove the built image, use <code>sudo docker rmi llama-embeddings</code>.</li>
</ul>
<hr>
<p dir="auto">Based on the provided code, I'll help you update the <code>Startup Procedures</code> section of your <code>readme.md</code> file. Here's the updated version:</p>
<hr>
<h2 tabindex="-1" dir="auto">Startup Procedures</h2>
<p dir="auto">During startup, the application performs the following tasks:</p>
<ol dir="auto">
<li><strong>Database Initialization</strong>:
<ul dir="auto">
<li>The application initializes the SQLite database, setting up tables and executing important PRAGMAs to optimize performance.</li>
<li>Some of the important SQLite PRAGMAs include setting the database to use Write-Ahead Logging (WAL) mode, setting synchronous mode to NORMAL, increasing cache size to 1GB, setting the busy timeout to 2 seconds, and setting the WAL autocheckpoint to 100.</li>
</ul>
</li>
<li><strong>Initialize Database Writer</strong>:
<ul dir="auto">
<li>A dedicated database writer (<code>DatabaseWriter</code>) is initialized with a dedicated asynchronous queue to handle the write operations.</li>
<li>A set of hashes is created which represents the operations that are currently being processed or have already been processed. This avoids any duplicate operations in the queue.</li>
</ul>
</li>
<li><strong>RAM Disk Setup</strong>:
<ul dir="auto">
<li>If the <code>USE_RAMDISK</code> variable is enabled and the user has the required permissions, the application sets up a RAM Disk.</li>
<li>The application checks if there's already a RAM Disk set up at the specified path, if not, it calculates the optimal size for the RAM Disk and sets it up.</li>
<li>If the RAM Disk is enabled but the user lacks the required permissions, the RAM Disk feature is disabled and the application proceeds without it.</li>
</ul>
</li>
<li><strong>Model Downloads</strong>:
<ul dir="auto">
<li>The application downloads the required models.</li>
</ul>
</li>
<li><strong>Model Loading</strong>:
<ul dir="auto">
<li>Each downloaded model is loaded into memory. If any model file is not found, an error log is recorded.</li>
</ul>
</li>
<li><strong>Build FAISS Indexes</strong>:
<ul dir="auto">
<li>The application creates FAISS indexes for efficient similarity search using the embeddings from the database.</li>
<li>Separate FAISS indexes are built for token-level embeddings.</li>
<li>Associated texts are stored by model name for further use.</li>
</ul>
</li>
</ol>
<p dir="auto">Note:</p>
<ul dir="auto">
<li>If the RAM Disk feature is enabled but the user lacks the required permissions, the application will disable the RAM Disk feature and proceed without it.</li>
<li>For any database operations, if the database is locked, the application will attempt to retry the operation a few times with an exponential backoff and a jitter.</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Endpoint Functionality and Workflow Overview</h2>
<p dir="auto">Here's a detailed breakdown of the main endpoints provided by the FastAPI server, explaining their functionality, input parameters, and how they interact with underlying models and systems:</p>
<h3 tabindex="-1" dir="auto">1. <code>/get_embedding_vector_for_string/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Retrieve the embedding vector for a given input text string using the specified model.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>text</code>: The input text for which the embedding vector is to be retrieved.</li>
<li><code>model_name</code>: The model used to calculate the embedding (optional, will use the default model if not provided).</li>
<li><code>token</code>: Security token (optional).</li>
<li><code>client_ip</code>: Client IP address (optional).</li>
</ul>
<h4 tabindex="-1" dir="auto">Workflow</h4>
<ol dir="auto">
<li><strong>Retrieve Embedding</strong>: The function retrieves or computes the embedding vector for the provided text using the specified or default model.</li>
<li><strong>Return Result</strong>: The embedding vector for the input text string is returned in the response.</li>
</ol>
<h3 tabindex="-1" dir="auto">2. <code>/compute_similarity_between_strings/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Compute the similarity between two given input strings using specified model embeddings and a selected similarity measure.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>text1</code>: The first input text.</li>
<li><code>text2</code>: The second input text.</li>
<li><code>model_name</code>: The model used to calculate embeddings (optional).</li>
<li><code>similarity_measure</code>: The similarity measure to be used. It can be <code>cosine_similarity</code>, <code>hoeffdings_d</code>, or <code>hsic</code> (optional, default is <code>cosine_similarity</code>).</li>
<li><code>token</code>: Security token (optional).</li>
</ul>
<h4 tabindex="-1" dir="auto">Workflow</h4>
<ol dir="auto">
<li><strong>Retrieve Embeddings</strong>: The embeddings for <code>text1</code> and <code>text2</code> are retrieved or computed using the specified or default model.</li>
<li><strong>Compute Similarity</strong>: The similarity between the two embeddings is calculated using the specified similarity measure.</li>
<li><strong>Return Result</strong>: The similarity score, along with the embeddings and input texts, is returned in the response.</li>
</ol>
<h3 tabindex="-1" dir="auto">3. <code>/search_stored_embeddings_with_query_string_for_semantic_similarity/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Find the most similar strings in the database to the given input "query" text. This endpoint uses a pre-computed FAISS index to quickly search for the closest matching strings.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>query_text</code>: The input text for which to find the most similar string.</li>
<li><code>model_name</code>: The model used to calculate embeddings.</li>
<li><code>number_of_most_similar_strings_to_return</code>: (Optional) The number of most similar strings to return, defaults to 10.</li>
<li><code>token</code>: Security token (optional).</li>
</ul>
<h4 tabindex="-1" dir="auto">Workflow</h4>
<ol dir="auto">
<li><strong>Search FAISS Index</strong>: The FAISS index, built on stored embeddings, is searched to find the most similar embeddings to the <code>query_text</code>.</li>
<li><strong>Return Result</strong>: The most similar strings found in the database, along with the similarity scores, are returned in the response.</li>
</ol>
<h3 tabindex="-1" dir="auto">4. <code>/get_all_embedding_vectors_for_document/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Extract text embeddings for a document. This endpoint supports both plain text and PDF files. OCR is not supported.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>file</code>: The uploaded document file (either plain text or PDF).</li>
<li><code>model_name</code>: (Optional) The model used to calculate embeddings.</li>
<li><code>json_format</code>: (Optional) The format of the JSON response.</li>
<li><code>send_back_json_or_zip_file</code>: Whether to return a JSON file or a ZIP file containing the embeddings file (optional, defaults to <code>zip</code>).</li>
<li><code>token</code>: Security token (optional).</li>
</ul>
<h3 tabindex="-1" dir="auto">5. <code>/get_list_of_available_model_names/</code> (GET)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Retrieve the list of available model names for generating embeddings.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>token</code>: Security token (optional).</li>
</ul>
<h3 tabindex="-1" dir="auto">6. <code>/get_all_stored_strings/</code> (GET)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Retrieve a list of all stored strings from the database for which embeddings have been computed.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>token</code>: Security token (optional).</li>
</ul>
<h3 tabindex="-1" dir="auto">7. <code>/get_all_stored_documents/</code> (GET)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Retrieve a list of all stored documents from the database for which embeddings have been computed.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>token</code>: Security token (optional).</li>
</ul>
<h3 tabindex="-1" dir="auto">8. <code>/clear_ramdisk/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Clear the RAM Disk to free up memory.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>token</code>: Security token (optional).</li>
</ul>
<h3 tabindex="-1" dir="auto">9. <code>/get_token_level_embeddings_matrix_and_combined_feature_vector_for_string/</code> (POST)</h3>
<h4 tabindex="-1" dir="auto">Purpose</h4>
<p dir="auto">Retrieve the token-level embeddings and combined feature vector for a given input text using the specified model.</p>
<h4 tabindex="-1" dir="auto">Parameters</h4>
<ul dir="auto">
<li><code>text</code>: The input text for which the embeddings are to be retrieved.</li>
<li><code>model_name</code>: The model used to calculate the embeddings (optional).</li>
<li><code>db_writer</code>: Database writer instance for managing write operations (internal use).</li>
<li><code>req</code>: HTTP request object (optional).</li>
<li><code>token</code>: Security token (optional).</li>
<li><code>client_ip</code>: Client IP address (optional).</li>
<li><code>json_format</code>: Format for JSON response of token-level embeddings (optional).</li>
<li><code>send_back_json_or_zip_file</code>: Whether to return a JSON response or a ZIP file containing the JSON file (optional, defaults to <code>zip</code>).</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The OpenTF Manifesto (496 pts)]]></title>
            <link>https://opentf.org/</link>
            <guid>37133054</guid>
            <pubDate>Tue, 15 Aug 2023 12:19:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opentf.org/">https://opentf.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37133054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
          Terraform was open-sourced in 2014 under the Mozilla Public License (v 2.0) (the ‚ÄúMPL‚Äù).
          Over the next ~9 years, it built up a community that included thousands of users, contributors, customers,
          certified practitioners, vendors, and an ecosystem of open-source modules, plugins,
          libraries, and extensions.

          Then, on August 10th, 2023, with little or no advance notice or chance for much, if not all,
          of the community to have any input, HashiCorp switched the license for Terraform from the
          MPL to the Business Source License (v1.1) (the ‚ÄúBSL‚Äù), a non-open source license. In our
          opinion, this change threatens the entire community and ecosystem that‚Äôs built up around
          Terraform over the last 9 years.
        </p>

        <p>
          Our concern: the BSL license is a poison pill for Terraform.
        </p>

        <p>
          Overnight, tens of thousands of businesses, ranging from one-person shops to the
          Fortune 500, woke up to a new reality where the underpinnings of their infrastructure
          suddenly became a potential legal risk. The BSL and the additional use grant written by
          the HashiCorp team are vague, and now every company, vendor, and developer using Terraform
          has to wonder whether what they are doing could be construed as competitive with HashiCorp‚Äôs
          offerings. The FAQ provides some solace for end-customers and systems integrators today,
          but even if you might be in the clear now, how can you build confidence that your usage
          won't violate the license terms in the future? What if your products or HashiCorp's products
          change? What if HashiCorp changes how they interpret competitive? What if they change the
          license again? As a result, everything that uses Terraform is on shaky ground.
        </p>

        <p>
          It is clear to us that under the new license, the thriving ecosystem built up around the
          open source Terraform will dwindle and wither. As developers consider what tools to learn
          and what ecosystems to contribute to, and as companies consider what tools to use to manage
          their infrastructure, more and more, they'll pick alternatives that are genuinely open-source.
          Existing Terraform codebases will turn into outdated liabilities, independent tooling will
          all but disappear, and the community will fracture and disappear.
        </p>

        <p>
          This sort of change also harms all similar open-source projects. Every company and every
          developer now needs to think twice before adopting and investing in an open-source project
          in case the creator suddenly decides to change the license. Imagine if the creators of Linux
          or Kubernetes suddenly switched to a non-open-source license that only permitted
          non-competitive usage.
        </p>

        <p>
          We believe that the essential building blocks of the modern Internet, such as Linux, Kubernetes, 
          and Terraform need to be truly open source: that is the only way to ensure
          that we are building our industry on top of solid and predictable underpinnings.
        </p>

        <p>
          Our goal: ensure Terraform remains truly open source‚Äîalways.
        </p>

        <p>
          Our aim with this manifesto is to return Terraform to a fully open source license. BSL 
          is <em>not</em> open source, so this would mean moving Terraform back to the MPL license, 
          or some other well-known, widely accepted open source license (e.g., Apache License 2.0). 
          Moreover, we want to be confident that Terraform will always remain open source, so you 
          don't have to worry about another sudden license change putting everything at risk. 
        </p>

        <p>
          Our request to HashiCorp: switch Terraform back to an open source license.
        </p>

        <p>
          We ask HashiCorp to do the right thing by the community: instead of going forward with the
          BSL license change, switch Terraform back to a truly open source license, and commit to keeping
          it that way forever going forward. That way, instead of fracturing the community, we end up with 
          a single, impartial, reliable home for Terraform where the whole community can unite to keep 
          building this amazing ecosystem.
        </p>

        <p>
          Our fallback plan: fork Terraform into a foundation.
        </p>

        <p>
          If HashiCorp is unwilling to switch Terraform back to an open source license, we propose to fork
          the legacy MPL-licensed Terraform and maintain the fork in the foundation. This is similar to how 
          Linux and Kubernetes are managed by foundations (the Linux Foundation and the Cloud Native 
          Computing Foundation, respectively), which are run by multiple companies, ensuring the tool stays 
          truly open source and neutral, and not at the whim of any one company.
        </p>

        <p>
          In particular, we want to create a foundation for Terraform that is:
        </p>

        <ul>
          <li>
            <span>Truly open source</span> - under a well-known and widely-accepted license that companies can trust,
            that won't suddenly change in the future, and isn't subject to the whims of a single vendor
          </li>
          <li>
            <span>Community-driven</span> - so that the community governs the project for the community, where pull
            requests are regularly reviewed and accepted on their merit
          </li>
          <li>
            <span>Impartial</span> - so that valuable features and fixes are accepted based on their value to the community,
            regardless of their impact on any particular vendor
          </li>
          <li>
            <span>Layered and modular</span> - with a programmer-friendly project structure
            to encourage building on top, enabling a new vibrant ecosystem of
            tools and integrations
          </li>
          <li>
            <span>Backwards-compatible</span> - so that the existing code can drive value for years to come
          </li>
        </ul>

        <h2>LIST OF PLEDGING COMPANIES AND PLEDGED RESOURCES:</h2>

        <p>
          We acknowledge that maintaining an open source project such as Terraform takes a considerable investment 
          in terms of time, skill, effort, and coordination. We are grateful to HashiCorp for creating Terraform
          and their leadership in getting it to this point, and to the thousands of community members for their 
          contributions so far. The next step for Terraform must be to remain open source, either by HashiCorp 
          switching it back to a truly open source license or by us forking it into a foundation. Whichever way 
          it turns out, to ensure that there is sufficient investment to grow and evolve Terraform, the 
          signatories below pledge to pool our resources to build a more open, inclusive future 
          for an open source Terraform.
        </p>

        <p>
          If you‚Äôre willing to join our cause, please sign the manifesto by
          <a href="https://github.com/opentffoundation/manifesto">creating a
            PR</a> and adding yourself at the bottom of this page and optionally
          let us know how you‚Äôd like to help, either as an individual or as an
          organization.
        </p>

        <h2>Pledged Companies</h2>

        <ul>
          <li><a href="https://gruntwork.io/">Gruntwork</a></li>
          <li><a href="https://spacelift.io/">Spacelift</a></li>
          <li><a href="https://env0.com/">env0</a></li>
          <li><a href="https://scalr.com/">Scalr</a></li>
          <li><a href="https://digger.dev/">Digger</a></li>
          <li><a href="https://doppler.com/">Doppler</a></li>
          <li><a href="https://massdriver.cloud/">Massdriver</a></li>
          <li><a href="https://www.qovery.com/">Qovery</a></li>
          <li><a href="https://rivet.gg/">Rivet</a></li>
          <li><a href="https://terramate.io/">Terramate</a></li>
          <li><a href="https://terrateam.io/">Terrateam</a></li>    
          <li><a href="https://verifa.io/">Verifa</a></li>    
          <li><a href="https://finisterra.io/">Finisterra</a></li>
        </ul>

        <h2>Contact us</h2>

        <p>
          If you are a member of the community, a member of the press, an employee of HashiCorp, or anyone else
          with questions or feedback to share, you can reach the team behind this manifesto by emailing us at
          <a href="mailto:pledge@opentf.org">pledge@opentf.org</a>.
        </p>

        <h2>Share</h2>

        
        <p>
          August 14th, 2023
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo DS cameras are the best lo-fi photo trend (134 pts)]]></title>
            <link>https://www.polygon.com/23827844/nintendo-ds-dsi-3ds-camera-lofi-photo-trend</link>
            <guid>37133033</guid>
            <pubDate>Tue, 15 Aug 2023 12:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polygon.com/23827844/nintendo-ds-dsi-3ds-camera-lofi-photo-trend">https://www.polygon.com/23827844/nintendo-ds-dsi-3ds-camera-lofi-photo-trend</a>, See on <a href="https://news.ycombinator.com/item?id=37133033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p id="5yq28G">Sometimes I miss the unique charms of the <a href="https://www.polygon.com/23818670/nintendo-3ds-eshop-closure-game-sales-2023">Nintendo DS and 3DS handhelds</a>. They were <em>weird</em>. I liked the wonky touch controls, and some games featured weird gimmicks with the built-in mic and cameras. Now the <a href="https://www.polygon.com/2016/1/6/10718652/best-nintendo-3ds-games-2016">family of handhelds</a> has newfound relevance; people are using the cameras on the Nintendo DSi, Nintendo 3DS, and other DS consoles to take photos and videos. As it turns out, the cameras are the perfect way to nostalgically capture the low-fidelity look of the 2010s. </p>
<p id="7cpJk8">My first real brush with the resurgence of the Nintendo DS camera was when a hip-hop duo called Joey Valence &amp; Brae recorded an entire music video on a Nintendo DS. The two musicians seem to really lean into a classic ‚Äô90s vibe sound-wise, and they recorded the entire music video for a song called ‚ÄúPunk Tactics‚Äù on a Nintendo DS. The crunchy, pixelated, lo-fi look seemed to connect with audiences; a teaser for the video got over 2.1 million views on <a href="https://www.polygon.com/tiktok">TikTok</a> and over 7.2 million views on YouTube, and the song went on to inspire its own viral trend. </p>
<div id="6r0qR6"><p><iframe src="https://www.youtube.com/embed/OklSZmIx9-o?rel=0" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;"></iframe></p></div>
<p id="eRWjDK">Since the release of this video, it‚Äôs become trendy to use the Nintendo DSi and 3DS handhelds as cameras. The first DS with a built-in camera was the Nintendo DSi, which Nintendo released globally in 2009. It contained two 0.3-megapixel cameras, one facing outwards and the other in. The cameras were pretty rudimentary even for the time, but for many who had the device (especially children), it could have been their first time having their own camera. Fast-forward to 2023, almost a decade since these consoles came out, and now the aging gen Z population can look back at that time period with <a href="https://www.tiktok.com/@joeystechtime/video/7227258242089471274">an air of romance</a>. </p>

<p id="kDj04I">Modern phone cameras are getting more and more advanced. Yet, some people still want something that looks stylized or isn‚Äôt picture-perfect. This inspires some to use old film cameras, but others might still have a Nintendo DS sitting around somewhere. These lo-fi cameras take extra-crunchy and pixelated photos that easily evoke a nostalgic aesthetic. </p>
<p id="ZrrlUY">In one viral video, a person shows off <a href="https://www.tiktok.com/@theaaroncain/video/7265386902071790894">photos they took during a trip to Yosemite</a>. The photos have a dreamy, painting-like quality. ‚ÄúThis is the Gen Z version of Polaroids and I love it,‚Äù one user commented. </p>

<p id="VWRzuy">So if you happen to have a Nintendo DSi or 3Ds laying around, maybe try taking it out to take photos. I‚Äôve seen people take <a href="https://www.tiktok.com/@geniewishes25/video/7237950772879691054">them to concerts</a> or <a href="https://www.tiktok.com/@heybentai/video/7162559435406134571">car shows</a>, but these devices are small enough to bring anywhere. I personally took mine on my last walk, and although I felt a little silly holding it up, I got some lovely shots of flowers. </p>
<div data-cid="apps/image_gallery-1692134788_4062_43293" data-cdata="{&quot;routing&quot;:false,&quot;keyboard&quot;:false,&quot;two_col&quot;:false,&quot;display_headline&quot;:false,&quot;expandable&quot;:false,&quot;svg_logo_data&quot;:null,&quot;anthem_component_id&quot;:2149989,&quot;entry_id&quot;:23591885}" id="z9GHn7">
    
    
    <div data-ui="viewer">
      <p><img src="" alt="">
      </p>
      </div>
    
    <div data-ui="scroller">
        <ul>
          
          <li data-ui="thumb" data-asset="24843462" data-slug="0" data-index="0">
            <a href="https://cdn.vox-cdn.com/uploads/chorus_asset/file/24843462/HNI_0021.JPG" role="button">
              
            </a>
            <span>
               
            </span>
            <span>
              Photo: Ana Diaz/Polygon
            </span>
          </li>
          
          <li data-ui="thumb" data-asset="24843464" data-slug="1" data-index="1">
            <a href="https://cdn.vox-cdn.com/uploads/chorus_asset/file/24843464/HNI_0026.JPG" role="button">
              
            </a>
            <span>
               
            </span>
            <span>
              Photo: Ana Diaz/Polygon
            </span>
          </li>
          
          <li data-ui="thumb" data-asset="24843467" data-slug="2" data-index="2">
            <a href="https://cdn.vox-cdn.com/uploads/chorus_asset/file/24843467/HNI_0025.JPG" role="button">
              
            </a>
            <span>
               
            </span>
            <span>
              Photo: Ana Diaz/Polygon
            </span>
          </li>
          
        </ul>
      </div>
  </div>



  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Selectors: A Visual Guide (267 pts)]]></title>
            <link>https://fffuel.co/css-selectors/</link>
            <guid>37132754</guid>
            <pubDate>Tue, 15 Aug 2023 11:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fffuel.co/css-selectors/">https://fffuel.co/css-selectors/</a>, See on <a href="https://news.ycombinator.com/item?id=37132754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section id="universal"><h2><span>*</span> universal selector</h2><p>Select all elements:</p><div><pre><code>div * {
  background: coral;
}</code></pre></div></section><section id="element"><h2><span>element</span> element selector</h2><p>Select element(s):</p><div><pre><code>p {
  background: deeppink;
}</code></pre></div></section><section id="class"><h2><span>.class</span> class selector</h2><p>Select all elements with the specified class name:</p><div><pre><code>.my-class {
  background: royalblue;
}</code></pre></div></section><section id="id"><h2><span>#id</span> ID selector</h2><p>Select the element with the specified ID:</p><div><pre><code>#my-id {
  background: aquamarine;
}</code></pre></div></section><section id="multiple"><h2><span>.class.class-2</span> multiple selectors</h2><p>Chain two or more classes or IDs to select the element(s) that have all the specified classes/IDs:</p><div><pre><code>.my-class.special {
  background: royalblue;
}</code></pre></div></section><section id="comma"><h2><span>.class, .class-2</span> comma combinator</h2><p>Separate multiple selector declarations using a <strong>comma</strong>. This makes it easy to apply the same styles to multiple selector declarations:</p><div><pre><code>.item-1, .item-2 {
  background: sandybrown;
}</code></pre></div></section><section id="descendant"><h2><span>.class .class-2</span> descendant selector</h2><p>Leave a <strong>space</strong> (<em>descendant combinator</em>) between selectors to select element(s) that are descendant of another element:</p><div><pre><code>.wrapper .card {
  background: lightblue;
}</code></pre></div></section><section id="adjacent"><h2><span>.class + .class-2</span> adjacent selector</h2><p>Use a <strong>plus sign</strong> (<em>adjacent combinator</em>) to select an element that is a direct sibling to a first element:</p><div><pre><code>.item-1 + div {
  background: yellowgreen;
}</code></pre></div></section><section id="child"><h2><span>.class &gt; .class-2</span> child selector</h2><p>Use a <strong>&gt;</strong> sign (<em>child combinator</em>) to select element(s) that are direct children to another element:</p><div><pre><code>.wrapper &gt; div {
  background: olive;
}</code></pre></div></section><section id="subsequent"><h2><span>.class ~ .class-2</span> subsequent selector</h2><p>Use a <strong>tilde sign</strong> (<em>subsequent combinator</em>) to select every element that is preceded by the first element, without having to be a direct sibling to the first element:</p><div><pre><code>.item-1 ~ div {
  background: lightcoral;
}</code></pre></div></section><section id="lobotomized"><h2><span>* + *</span> lobotomized owl</h2><p>A selector pattern where all elements that have a preceding sibling are selected. Use it for example to add spacing to elements within a container except for the first element, which has no preceding sibling:</p><div><pre><code>* + * {
  background: khaki;
}</code></pre></div></section><section id="attribute"><h2><span>[attr]</span> attribute selector</h2><p>Select element(s) that have a specified attribute:</p><div><pre><code>[data-text] {
  background: deepskyblue;
}</code></pre></div></section><section id="attribute-value"><h2><span>[attr=val]</span> attribute &amp; attribute value</h2><p>Select element(s) that have the specified attribute and attribute value:</p><div><pre><code>[data-text="hello"] {
  background: lemonchiffon;
}</code></pre></div></section><section id="attribute-tilde-value"><h2><span>[attr~=val]</span> attribute &amp; one of the attribute's values</h2><p>Select element(s) that have the specified attribute with one of it's space-separated values matching the value:</p><div><pre><code>[title~="old"] {
  background: crimson;
}</code></pre></div></section><section id="attribute-star-value"><h2><span>[attr*=val]</span> attribute &amp; partial value</h2><p>Select element(s) that have the specified attribute with <em>val</em> being included in the attribute value:</p><div><pre><code>[title*="saur"] {
  background: darkgoldenrod;
}</code></pre></div></section><section id="link"><h2><span>:link :visited :hover &amp; :active</span> link pseudo-class selectors</h2><p>These 4 pseudo-classes are useful to select elements such as links in various states. These 4 are most often used with links, but <strong>:active</strong> is also useful for buttons and <strong>:hover</strong> can be used on all kinds of elements:</p><ul><li><strong>:link</strong> - Targets unvisited links. It allows you to style hyperlinks that the user hasn't clicked on yet.</li><li><strong>:visited</strong> - Targets links that have already been visited by the user. This pseudo-class lets you apply styles to previously clicked hyperlinks.</li><li><strong>:hover</strong> - Targets elements (commonly links) when they are being hovered over by the user's pointer, such as a mouse cursor.</li><li><strong>:active</strong> - Targets elements (typically links or buttons) during the moment they are being activated, like when a user clicks on them.</li></ul><div><pre><code>a:link {
  background: aliceblue;
}
a:visited {
  background: blanchedalmond;
}
a:hover {
  background: honeydew;
}
a:active {
  background: lavenderblush;
}</code></pre></div></section><section id="focus"><h2><span>:focus</span> focused input element(s)</h2><p>The <strong>:focus</strong> pseudo-class targets an element when it receives focus, such as when a user clicks on an input field or navigates to it using the keyboard:</p><div><pre><code>input:focus {
  border: 2px solid deepskyblue;
  background: lightcyan;
  outline: none;
  box-shadow: 0 0 8px rgba(0, 191, 255, 0.5);
}</code></pre><p><label for="my-input">Your name: </label></p></div></section><section id="checked"><h2><span>:checked</span> checked input element(s)</h2><p>The <strong>:checked</strong> pseudo-class targets <em>radio buttons, checkboxes, or options</em> in a select element that are currently selected/checked.</p><p>In the following example I make use of <em>appearance: none</em> to remove the default styling of the checkbox input, then use the <strong>:checked</strong> pseudo-class to style the sibling label, while also adding styling on the label when the checkbox is focused:</p><div><pre><code>input[type='checkbox'] {
  /* remove default
     checkbox styles */
  all: unset;
  -webkit-appearance: none;
  appearance: none;
  margin: 0;
}
input[type='checkbox']:checked + label {
  background: mediumseagreen;
}
input[type='checkbox']:focus + label {
  box-shadow: 0 0 0 2px yellow;
}</code></pre><p> <label for="checkbox1">I am a label</label></p></div></section><section id="disabled"><h2><span>:disabled</span> disabled input element(s)</h2><p>The <strong>:disabled</strong> pseudo-class matches form elements like buttons or text inputs that are disabled:</p><div><pre><code>input[type="text"] {
  padding: 10px;
  border: 2px solid mediumslateblue;
  margin-right: 10px;
}

input[type="text"]:disabled {
  background: lightgray;
  border: 2px solid darkgray;
  color: darkgray;
}</code></pre></div></section><section id="enabled"><h2><span>:enabled</span> enabled input element(s)</h2><p>The <strong>:enabled</strong> pseudo-class matches form elements that are interactive and can receive input:</p><div><pre><code>input[type="text"] {
  padding: 10px;
  border: 1px solid gray;
  margin-right: 10px;
}

input[type="text"]:enabled {
  background: lightgreen;
  border: 1px solid green;
}</code></pre></div></section><section id="valid"><h2><span>:valid</span> valid input element(s)</h2><p>The <strong>:valid</strong> pseudo-class is used to target an input element that has content that matches the requirements as specified by its attributes (like <em>pattern</em>, <em>type</em>, etc.):</p><div><pre><code>input[type="email"]:valid {
  border: 2px solid limegreen;
  background: honeydew;
}</code></pre><p><label>Email: </label></p></div></section><section id="invalid"><h2><span>:invalid</span> invalid input element(s)</h2><p>The <strong>:invalid</strong> pseudo-class is used to target input elements that have content that doesn't match the requirements:</p><div><pre><code>input[type="email"]:invalid {
  border: 2px solid tomato;
  background: mistyrose;
}</code></pre><p><label>Email: </label></p></div></section><section id="required"><h2><span>:required</span> required input element(s)</h2><p>The <strong>:required</strong> pseudo-class targets input elements that have the <em>required</em> attribute, indicating that they must be filled out before the form can be submitted:</p><div><pre><code>input:required {
  border: 2px dotted orangered;
  background: floralwhite;
  box-shadow: 0 0 10px rgba(255, 69, 0, 0.2);
}</code></pre><p><label>Name (required): </label> <label>Optional Field: </label></p></div></section><section id="optional"><h2><span>:optional</span> optional input element(s)</h2><p>The <strong>:optional</strong> pseudo-class targets input elements that do not have the <em>required</em> attribute, implying that they're not mandatory to fill out:</p><div><pre><code>input:optional {
  border: 2px dotted darkgray;
  background: whitesmoke;
  box-shadow: 0 0 10px rgba(105, 105, 105, 0.2);
}</code></pre><p><label>Name (required): </label> <label>Optional Field: </label></p></div></section><section id="first-child"><h2><span>:first-child</span> first child element</h2><p>The <strong>:first-child</strong> pseudo-class targets the first child element within its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:first-child {
  background: lightblue;
  border-color: deepskyblue;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Third Child</p></div></div></section><section id="last-child"><h2><span>:last-child</span> last child element</h2><p>The <strong>:last-child</strong> pseudo-class targets the last child element within its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:last-child {
  background: lightblue;
  border-color: deepskyblue;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Last Child</p></div></div></section><section id="nth-child"><h2><span>:nth-child</span> nth child element</h2><p>The <strong>:nth-child</strong> pseudo-class targets elements based on their position within their parent, allowing for a wide variety of selections:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:nth-child(2) {
  background: lightcoral;
  border-color: darkred;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Last Child</p></div></div></section><section id="nth-last-child"><h2><span>:nth-last-child</span> nth child element, counting backwards from last</h2><p>The <strong>:nth-last-child</strong> pseudo-class is similar to :nth-child, but it counts from the last child backwards:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:nth-last-child(2) {
  background: darkorchid;
  border-color: indigo;
  color: white;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Third Child</p><p>Last Child</p></div></div></section><section id="only-child"><h2><span>:only-child</span> only child of an element</h2><p>The <strong>:only-child</strong> pseudo-class targets an element if it's the only child element of its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:only-child {
  background: lightsalmon;
  border-color: darksalmon;
  border-style: solid;
}</code></pre><div><div><p>1st Child</p><p>Inner child 1</p><p>Inner child 2</p></div><div><p>2nd Child</p><p>Only child of '2nd Child'</p></div></div></div></section><section id="first-of-type"><h2><span>:first-of-type</span> first element of a type</h2><p>The <strong>:first-of-type</strong> pseudo-class targets the first element of its type within its parent:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

div:first-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>First div</p><p>Second div</p></div></div></section><section id="last-of-type"><h2><span>:last-of-type</span> last element of a type</h2><p>The <strong>:last-of-type</strong> pseudo-class targets the last element of its type within a parent:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

p:last-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>First div</p><p>Second div</p></div></div></section><section id="nth-of-type"><h2><span>:nth-of-type</span> nth element of a type</h2><p>The <strong>:nth-of-type</strong> pseudo-class matches elements based on their type and position among siblings:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

p:nth-of-type(3) {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>Third paragraph</p><p>First div</p><p>Second div</p><p>Third div</p></div></div></section><section id="nth-last-of-type"><h2><span>:nth-last-of-type</span> nth element of type, counting backwards</h2><p>The <strong>:nth-last-of-type</strong> pseudo-class matches elements based on their type and position among siblings, but counting from the end:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

div:nth-last-of-type(3) {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>Third paragraph</p><p>First div</p><p>Second div</p><p>Third div</p></div></div></section><section id="only-of-type"><h2><span>:only-of-type</span> only element of its type</h2><p>The <strong>:only-of-type</strong> pseudo-class targets an element that is the only element of its type among its siblings:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

:only-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p></div></div></section><section id="target"><h2><span>:target</span> target element selector</h2><p>The <strong>:target</strong> pseudo-class selects an element with an ID attribute matching the URL fragment (eg: <em>https://example.com/#fragment</em>).</p><p><strong>:target</strong> is often used to style sections of a page that are directly linked to, typically used with in-page links:</p><div><pre><code>div:target {
  border: 3px solid deepskyblue;
  background-color: lightcyan;
  transform: scale(1.05);
}</code></pre><div><p><a href="#section1">Go to Section 1</a> <a href="#section2">Go to Section 2</a></p><p>Section 1 content</p><p>Section 2 content</p></div></div></section><section id="not"><h2><span>:not()</span> negation pseudo-class</h2><p>The <strong>:not()</strong> functional pseudo-class allows you to target elements that do not match a specified selector or condition. It's essentially an exclusion filter:</p><div><pre><code>div:not(.exclude) {
  border: 2px solid royalblue;
  background: aliceblue;
}</code></pre><div><p>This div is targeted</p><p>This div has the '.exclude' class</p><p>Another targeted div</p></div></div></section><section id="has"><h2><span>:has()</span> parent selector</h2><p>The <strong>:has()</strong> functional pseudo-class allows to style an element if it contains a certain element or another selector:</p><div><pre><code>div:has(p.special) {
  border: 2px solid darkkhaki;
  background-color: peachpuff;
}</code></pre><div><p>Regular paragraph.</p><p>This paragraph has a the '.special' class, so its parent div is styled!</p><p>Another regular paragraph.</p></div></div></section><section id="before"><h2><span>::before</span> first child pseudo-element</h2><p>The <strong>::before</strong> pseudo-element is used to insert content before the content of an element. It can be used to add decorative content, icons, or other elements that don't need to be in the actual DOM:</p><div><pre><code>.alert::before {
  content: '‚ö†Ô∏è ';
  margin-right: 0.25rem;
}</code></pre></div></section><section id="after"><h2><span>::after</span> last child pseudo-element</h2><p>The <strong>::after</strong> pseudo-element is similar to <em>::before</em> and is used to insert content after the content of an element:</p><div><pre><code>.info::after {
  content: '';
  display: inline-block;
  width: 0.75rem;
  height: 0.75rem;
  border-radius: 35%;
  background: darkseagreen;
  margin-left: 0.35rem;
  rotate: 45deg;
  vertical-align: middle;
}</code></pre></div></section><section id="first-letter"><h2><span>::first-letter</span> first letter pseudo-element</h2><p>The <strong>::first-letter</strong> pseudo-element is used to style the first letter of a block-level element, allowing for design elements like drop caps:</p><div><pre><code>p::first-letter {
  font-size: 2em;
  font-weight: bold;
  float: left;
  color: crimson;
}</code></pre><p>Once upon a time, in a land far, far away, there lived a curious coder on a quest for knowledge.</p></div></section><section id="first-line"><h2><span>::first-line</span> first line pseudo-element</h2><p>The <strong>::first-line</strong> pseudo-element is used to style the first line of a block-level element. This allows for typographic effects that can adapt dynamically based on the size of the containing element and the font size:</p><div><pre><code>p::first-line {
  font-weight: bold;
  color: darkslategray;
  text-transform: uppercase;
}</code></pre><p>It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...</p></div></section><section id="placeholder"><h2><span>::placeholder</span> text input placeholder</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style the placeholder text of form fields like <em>&lt;input&gt;</em> and <em>&lt;textarea&gt;</em>:</p><div><pre><code>input::placeholder {
  font-style: italic;
  color: thistle;
  opacity: 0.7;
}</code></pre></div></section><section id="selection"><h2><span>::selection</span> style highlighted box</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style the portion of an element that has been highlighted or selected by the user. For instance, when a user clicks and drags to select text, the <em>::selection</em> pseudo-element can be used to modify the background color, text color, and other styles of that selection:</p><div><pre><code>::selection {
  background-color: gold;
  color: darkblue;
}</code></pre><p>Click and drag to select this text to see the custom selection styling in action.</p></div></section><section id="marker"><h2><span>::marker</span> list marker pseudo-element</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style marker boxes in list items, which typically contain bullets (for unordered lists) or numbers/letters (for ordered lists).</p><p>Before the introduction of the <em>::marker</em> pseudo-element, customizing these markers often required workarounds, but this pseudo-element gives us a bit more control:</p><div><pre><code>li::marker {
  color: LightSeaGreen;
  font-size: 1.5em;
  font-weight: bold;
}</code></pre><div><ul><li>Apples üçé</li><li>Bananas üçå</li><li>Cherries üçí</li></ul></div></div></section><svg width="300" viewBox="0 0 687 155" xmlns="http://www.w3.org/2000/svg"><g stroke="currentColor" stroke-width="7" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"><path d="M20 58c27-13.33333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.66666667 80.5 20" opacity=".1"></path><path d="M20 78c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20" opacity=".2"></path><path d="M20 98c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20" opacity=".6"></path><path d="M20 118c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20"></path></g></svg><section id="more"><h2>More pseudo-classes</h2><p>Here's some additional pseudo-classes that are available in CSS:</p><ul><li><strong>:root:</strong> Targets the highest-level parent element in a document, typically the <em>&lt;html&gt;</em> element in HTML documents. Useful to define CSS variables that will be available to all elements within the page.</li><li><strong>:is():</strong> Matches elements that can be one of several selectors, making long selector lists shorter and easier to read. For example, <em>:is(h1, h2, h3)</em> would match any of those three heading elements.</li><li><strong>:where():</strong> Similar to <em>:is()</em>, but allows for selecting elements based on conditions without affecting the specificity of the selector.</li><li><strong>:default:</strong> Matches UI elements (like radio buttons or checkboxes) that are set to their default selection state.</li><li><strong>:empty:</strong> Selects elements that have no children (including text nodes).</li><li><strong>:fullscreen:</strong> Targets elements that are currently displayed in fullscreen mode.</li><li><strong>:in-range:</strong> Matches form elements with a value that is within the specified range (using attributes like <em>min</em> and <em>max</em>).</li><li><strong>:out-of-range:</strong> Matches form elements with a value that is outside the specified range.</li><li><strong>:indeterminate:</strong> Targets form elements whose state is uncertain, such as a checkbox that's neither checked nor unchecked (often seen in tree-view structures).</li><li><strong>:read-only:</strong> Matches form elements that are not editable by the user, due to the <em>readonly</em> attribute.</li><li><strong>:read-write:</strong> Targets form elements that are editable by the user, implying they are not <em>readonly</em>.</li><li><strong>:lang()</strong>: Matches elements based on their language attribute. E.g., <em>:lang(en)</em> selects elements defined in English.</li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Servicer, pm2 alternative built on Rust and systemd (137 pts)]]></title>
            <link>https://servicer.dev</link>
            <guid>37132651</guid>
            <pubDate>Tue, 15 Aug 2023 11:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://servicer.dev">https://servicer.dev</a>, See on <a href="https://news.ycombinator.com/item?id=37132651">Hacker News</a></p>
<div id="readability-page-1" class="page"><nav><hr></nav><p><a href="https://github.com/servicer-labs/servicer"><img src="https://simpleicons.org/icons/github.svg" alt="Github repo"></a></p><p><a href="https://crates.io/crates/servicer"><img src="https://img.shields.io/crates/v/servicer?style=flat-square" alt="Crates.io"></a>
<a href="https://crates.io/crates/servicer"><img src="https://img.shields.io/crates/d/servicer?style=flat-square" alt="Crates.io"></a>
<a href="https://servicer.dev/LICENSE-MIT"><img src="https://img.shields.io/badge/license-MIT-blue?style=flat-square" alt="License"></a></p><h2 id="simplify-service-management-on-systemd">Simplify Service Management on systemd</h2><p><code>servicer</code> is a user-friendly CLI tool designed to simplify service management on <code>systemd</code>, abstracting away the complexities of the systemd ecosystem. With an easy-to-use API comparable to popular tools like <code>pm2</code>, servicer empowers users to create, control, and manage services effortlessly.</p><p><code>servicer</code> is lightweight, written in Rust and doesn‚Äôt run in the background. It does not fork services nor run a custom logging solution. It is a thin layer on <code>systemd</code> that creates <code>.ser.service</code> files. Logging is handled by journald.</p><h3 id="install">Install</h3><p>Download the binary from the <a href="https://github.com/servicer-labs/servicer/releases/download/v0.1.2/servicer">release page</a> or setup as-</p><pre><code>wget https://github.com/servicer-labs/servicer/releases/download/v0.1.2/servicer

# grant permissions
chmod +rwx ./servicer

# Rename to ser and make it accessable from path
sudo mv ./servicer /usr/bin/ser

# This should work now
ser --help
</code></pre><p>Or build from source</p><pre><code>cargo install servicer
sudo ln -s ~/.cargo/bin/servicer /usr/bin/ser
</code></pre><h3 id="create-service">Create service</h3><pre><code>sudo ser create index.js --start --enable

# Custom interpreter
sudo ser create index.js --start --enable --interpreter deno
</code></pre><p>Or write your own custom <code>.service</code> file. <code>servicer</code> provides a starter template to get you started quickly.</p><pre><code>sudo ser edit index.js
</code></pre><p>Got an existing service? No problem. Rename your <code>.service</code> file to <code>.ser.service</code> and servicer will pick it up.</p><h3 id="view-services">View services</h3><pre><code>ser status
</code></pre><pre><code>+-------+-------------+--------+----------------+-------+--------+
| pid   | name        | active | enable on boot | cpu % | memory |
+-------+-------------+--------+----------------+-------+--------+
| 24294 |    index.js | active | true           | 0     | 9.5 KB |
+-------+-------------+--------+----------------+-------+--------+
</code></pre><h3 id="view-logs">View logs</h3><pre><code>ser logs index.js
</code></pre><h3 id="stop-disable-or-delete">Stop, disable or delete</h3><pre><code># Stop a running service
sudo ser stop index.js

# Disable load on boot
sudo ser disable index.js

# Delete the .service file
sudo ser rm index.js
</code></pre><h3 id="view-file-and-unit-path">View file and unit path</h3><pre><code>ser which index.js
</code></pre><pre><code>Paths for index.js.ser.service:
+--------------+-----------------------------------------------------------+
| name         | path                                                      |
+--------------+-----------------------------------------------------------+
| Service file | /etc/systemd/system/index.js.ser.service                  |
+--------------+-----------------------------------------------------------+
| Unit file    | /org/freedesktop/systemd1/unit/index_2ejs_2eser_2eservice |
+--------------+-----------------------------------------------------------+
</code></pre><ul></ul></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPU-Accelerated LLM on an Orange Pi (193 pts)]]></title>
            <link>https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi</link>
            <guid>37132209</guid>
            <pubDate>Tue, 15 Aug 2023 10:30:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi">https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi</a>, See on <a href="https://news.ycombinator.com/item?id=37132209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>
        <time datetime="2023-08-09T13:30:00+00:00" itemprop="datePublished">
          Aug 9, 2023
        </time>
        
        ‚Ä¢ 
        
      </p>
      
    <br>
    <h2 id="tldr">TL;DR</h2>

<p>This post shows GPU-accelerated LLM running smoothly on an embedded device at a reasonable speed. More specifically, on a $100 Orange Pi 5 with Mali GPU, we achieve 2.5 tok/sec for Llama2-7b and 5 tok/sec for RedPajama-3b through Machine Learning Compilation (MLC) techniques. Additionally, we are able to run a Llama-2 13b model at 1.5 tok/sec on a 16GB version of the Orange Pi 5+ under $150.</p>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/orange-pi.jpg" width="90%">
</p>

<h2 id="background">Background</h2>

<p>Progress in open language models has been catalyzing innovation across question-answering, translation, and creative tasks. While current solutions demand high-end desktop GPUs to achieve satisfactory performance, to unleash LLMs for everyday use, we wanted to understand how usable we could deploy them on the affordable embedded devices.</p>

<p>Many embedded devices come with mobile GPUs that can serve as a source of acceleration. In this post, we pick Orange Pi 5, a RK35888-based board that is similar to Raspberry Pi but also features a more powerful Mali-G610 GPU. This post summarizes our first attempt at leveraging Machine Learning Compilation and provides out-of-box GPU acceleration for this device.</p>

<h2 id="machine-learning-compilation-for-mali">Machine Learning Compilation for Mali</h2>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/WebXYZ%20Images.svg" width="90%">
</p>

<p>Machine learning compilation (MLC) is an emerging technology that automatically compiles and optimizes machine learning workloads, and deploys the compiled workload to a broad set of backends. At the time of writing, based on Apache TVM Unity, MLC supports platforms including browsers (WebGPU, WASM), NVIDIA GPUs (CUDA), AMD GPUs (ROCm, Vulkan), Intel GPUs (Vulkan), iOS and MacBooks (Metal), Android (OpenCL), and Mali GPUs (this post).</p>

<h3 id="generalizable-ml-compilation-for-mali-codegen">Generalizable ML Compilation for Mali Codegen</h3>

<p>MLC is built on top of  Apache TVM Unity, a generalizable stack for compiling machine learning models across different hardwares and backends. To compile LLMs onto Mali GPUs, we reuse all the existing compilation pipeline without any code optimizations. More specifically, we successfully deployed Llama-2 and RedPajama models with the following steps:</p>

<ul>
  <li>Reuse model optimization passes, including quantization, fusion, layout optimization, etc;</li>
  <li>Reuse a generic GPU kernel optimization space written in TVM TensorIR and re-target it to Mali GPUs;</li>
  <li>Reuse OpenCL codegen backend from TVM, and re-target it to Mali GPUs;</li>
  <li>Reuse the existing user interface, including Python APIs, CLI, and REST APIs.</li>
</ul>

<h2 id="try-it-out">Try it out</h2>

<p>This section provides a step-by-step guide so that you can try it out on your own orange pi device. Here we use <code>RedPajama-INCITE-Chat-3B-v1-q4f16_1</code> as the running example. You can replace that by <code>‚Äã‚ÄãLlama-2-7b-chat-hf-q4f16_1</code> or <code>‚Äã‚ÄãLlama-2-13b-chat-hf-q4f16_1</code> (requires a 16GB board).</p>

<h3 id="prepare">Prepare</h3>

<p>Please first follow the instruction <a href="https://mlc.ai/mlc-llm/docs/install/gpu.html#orange-pi-5-rk3588-based-sbc">here</a>, to setup the RK3588 board with OpenCL driver. Then clone the MLC-LLM from the source, and download weights and prebuilt libs.</p>

<div><pre><code><span># clone mlc-llm from GitHub</span>
git clone <span>--recursive</span> https://github.com/mlc-ai/mlc-llm.git <span>&amp;&amp;</span> <span>cd </span>mlc-llm
<span># Download prebuilt weights and libs</span>
git lfs <span>install
mkdir</span> <span>-p</span> dist/prebuilt <span>&amp;&amp;</span> <span>cd </span>dist/prebuilt
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git lib
git clone https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span>cd</span> ../../..
</code></pre></div>

<h3 id="try-out-the-cli">Try out the CLI</h3>

<p>Build mlc_llm_cli from the source code</p>

<div><pre><code><span>cd </span>mlc-llm/
<span># create build directory</span>
<span>mkdir</span> <span>-p</span> build <span>&amp;&amp;</span> <span>cd </span>build
<span># generate build configuration</span>
python3 ../cmake/gen_cmake_config.py
<span># build `mlc_chat_cli`</span>
cmake .. <span>&amp;&amp;</span> cmake <span>--build</span> <span>.</span> <span>--parallel</span> <span>$(</span><span>nproc</span><span>)</span> <span>&amp;&amp;</span> <span>cd</span> ..
</code></pre></div>

<p>Verify installation</p>

<div><pre><code><span># expected to see `mlc_chat_cli`, `libmlc_llm.so` and `libtvm_runtime.so`</span>
<span>ls</span> <span>-l</span> ./build/
<span># expected to see help message</span>
./build/mlc_chat_cli <span>--help</span>
</code></pre></div>

<p>Run LLMs through mlc_chat_cli</p>

<div><pre><code>./build/mlc_chat_cli <span>--local-id</span> RedPajama-INCITE-Chat-3B-v1-q4f16_1 ‚Äìdevice mali
</code></pre></div>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/cli.png" width="90%">
</p>

<h3 id="try-out-the-python-api">Try out the Python API</h3>

<p>Build TVM runtime</p>

<div><pre><code><span># clone from GitHub</span>
git clone <span>--recursive</span> https://github.com/mlc-ai/relax.git tvm_unity <span>&amp;&amp;</span> <span>cd </span>tvm_unity/
<span># create build directory</span>
<span>mkdir</span> <span>-p</span> build <span>&amp;&amp;</span> <span>cd </span>build
<span># generate build configuration</span>
<span>cp</span> ../cmake/config.cmake <span>.</span> <span>&amp;&amp;</span> <span>echo</span> <span>"set(CMAKE_BUILD_TYPE RelWithDebInfo)</span><span>\n</span><span>set(USE_OPENCL ON)"</span> <span>&gt;&gt;</span> config.cmake
<span># build `mlc_chat_cli`</span>
cmake .. <span>&amp;&amp;</span> cmake <span>--build</span> <span>.</span> <span>--target</span> runtime <span>--parallel</span> <span>$(</span><span>nproc</span><span>)</span> <span>&amp;&amp;</span> <span>cd</span> ../..
</code></pre></div>

<p>Setup python path (please set it to the <code>bashrc</code> or <code>zshrc</code> for persistent settings)</p>

<div><pre><code><span>export </span><span>TVM_HOME</span><span>=</span><span>$(</span><span>pwd</span><span>)</span>/tvm_unity
<span>export </span><span>MLC_LLM_HOME</span><span>=</span><span>$(</span><span>pwd</span><span>)</span>/mlc-llm
<span>export </span><span>PYTHONPATH</span><span>=</span><span>$TVM_HOME</span>/python:<span>$MLC_LLM_HOME</span>/python:<span>${</span><span>PYTHONPATH</span><span>}</span>
</code></pre></div>

<p>Run the following python script.</p>

<div><pre><code><span>from</span> <span>mlc_chat</span> <span>import</span> <span>ChatModule</span>
<span>from</span> <span>mlc_chat.callback</span> <span>import</span> <span>StreamToStdout</span>
<span>cm</span> <span>=</span> <span>ChatModule</span><span>(</span><span>model</span><span>=</span><span>"RedPajama-INCITE-Chat-3B-v1-q4f16_1"</span><span>)</span>

<span># Generate a response for a given prompt
</span><span>output</span> <span>=</span> <span>cm</span><span>.</span><span>generate</span><span>(</span>
   <span>prompt</span><span>=</span><span>"What is the meaning of life?"</span><span>,</span>
   <span>progress_callback</span><span>=</span><span>StreamToStdout</span><span>(</span><span>callback_interval</span><span>=</span><span>2</span><span>),</span>
<span>)</span>

<span># Print prefill and decode performance statistics
</span><span>print</span><span>(</span><span>f</span><span>"Statistics: </span><span>{</span><span>cm</span><span>.</span><span>stats</span><span>()</span><span>}</span><span>\n</span><span>"</span><span>)</span>
</code></pre></div>

<h2 id="discussion-and-future-work">Discussion and Future Work</h2>

<p>Our current experiments show that 3B models might be a sweet spot. The RedPajama-3B model can provide up to 5 tok/sec and a decent chat experience. There is also room for improvements, specifically around the integer-to-float conversions. Moving forward, we will address the related issues and improve Mali GPUs‚Äô performance.</p>

<p>This post contributes to our quest to integrate LLMs into affordable devices and bring AI to everyone. Our future endeavors will focus on harnessing advancements in single-board computers, refining software frameworks like OpenCL and MLC-LLM, and exploring broader applications such as smart home devices. Collaborative efforts in the open-source community and a commitment to continuous learning and adaptation will be pivotal in navigating the evolving landscape of LLM deployment on emerging hardware.</p>

<h2 id="contributions">Contributions</h2>

<p>LLM on Orange Pi is primarily completed by <a href="https://www.linkedin.com/in/haolin-zhang-534530231/">Haolin Zhang</a>. The support of mali optimizations comes from Siyuan Feng, with foundation support from Junru Shao and Bohan Hou and other community members.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy friendly ESP32 smart doorbell with Home Assistant local integration (285 pts)]]></title>
            <link>https://tristam.ie/2023/758/</link>
            <guid>37131957</guid>
            <pubDate>Tue, 15 Aug 2023 09:46:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tristam.ie/2023/758/">https://tristam.ie/2023/758/</a>, See on <a href="https://news.ycombinator.com/item?id=37131957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p id="bkmrk-privacy-focused-%22sma">Privacy focused ‚Äúsmart‚Äù doorbells seem to be few and far between so I decided to build one that integrates with Home Assistant via ESPHome and is easy to build.</p>



<p id="bkmrk-this-project-is-aime">This project is aimed at being simple while allowing a ton of customisation and flexibility. To get started, you‚Äôll need an instance of <a rel="noreferrer noopener" href="https://www.home-assistant.io/" target="_blank">Home Assistant</a> running with the <a rel="noreferrer noopener" href="https://esphome.io/index.html" target="_blank">ESPHome</a> add-on as well as the Home Assistant companion app on your mobile phone to receive notifications when someone presses the doorbell button.</p>



<p>I have used an 8 RGB LED ring light in my version but if you want to simplify things, you can skip this and use the ESP32-CAM‚Äôs built in LED as a flash ‚Äì it‚Äôs surprisingly bright.</p>



<h4 id="bkmrk-parts"><strong>Parts list:</strong></h4>



<ul id="bkmrk-esp32-cam-%28amazon---">
<li>ESP32-CAM (Amazon ‚Äì <a rel="noreferrer noopener" href="https://amzn.to/3KoP1x1" target="_blank">US</a>, <a rel="noreferrer noopener" href="https://amzn.to/442cUBH" target="_blank">UK</a>, <a href="https://amzn.to/3DPKhNn" target="_blank" rel="noreferrer noopener">DE</a>) Make sure to get one with a ‚Äúflash/download/io0‚Äù button to make your life easier when you flash ESPHome onto it for the first time. If you make the same mistake as me and buy the one without that button, follow <a rel="noreferrer noopener" href="https://hagensieker.com/2021/10/03/esp32-cam-tiny-live-stream-camera/" target="_blank">this guide</a> to flash the ESP32-CAM using an FTDI adapter.</li>



<li>Momentary push button (Amazon ‚Äì&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/3Yfktna" target="_blank">US</a>, <a href="https://amzn.to/3DO0EtP" data-type="link" data-id="https://amzn.to/3DO0EtP" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/44RJ3gs" target="_blank">DE</a>)</li>



<li>10k resistor</li>



<li>8&nbsp; RGB LED ring light (Amazon ‚Äì <a href="https://amzn.to/3KizvTx" target="_blank" rel="noreferrer noopener">US</a>, <a href="https://amzn.to/3OBUinp" target="_blank" rel="noreferrer noopener">DE</a>) Note: these aren‚Äôt the exact ones that I used but they are the closest ones that I could find. I used the Pi Supply PIS-1270 from <a href="https://ie.rs-online.com/web/p/led-development-tools/2011639" target="_blank" rel="noreferrer noopener">RS Components</a>.</li>



<li>10m Micro USB cable (Amazon ‚Äì&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/3rIiUlt" target="_blank">US</a>, <a href="https://amzn.to/3OORUtF" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/457hjnD" target="_blank">DE</a>)</li>



<li>M2.5 brass inserts (Amazon ‚Äì <a rel="noreferrer noopener" href="https://amzn.to/3VYSZ4i" target="_blank">US</a>, <a rel="noreferrer noopener" href="https://amzn.to/455KYOz" target="_blank">UK</a>, <a href="https://amzn.to/3sazoD6" target="_blank" rel="noreferrer noopener">DE</a>) </li>



<li>M2.5 screws (Amazon ‚Äì <a href="https://amzn.to/3IIuAuG">US</a>, <a href="https://amzn.to/45iVkdM" target="_blank" rel="noreferrer noopener">UK</a>, <a href="https://amzn.to/3lUMHVs">DE</a>)</li>



<li>eSUN white PETG filament (Amazon ‚Äì&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/45aTyes" target="_blank">US</a>, <a href="https://amzn.to/3OtHTAN" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/45aTyes" target="_blank">DE</a>)&nbsp;</li>
</ul>



<p>You can find the .stl‚Äôs on Printables <a rel="noreferrer noopener" href="https://www.printables.com/model/542900-privacy-friendly-esp32-smart-doorbell-with-home-as" target="_blank">here</a> and the home assistant config in my github repo: <a rel="noreferrer noopener" href="https://github.com/thatguy-za/esp32-cam-doorbell" target="_blank">thatguy-za/esp32-cam-doorbell</a>.</p>



<h4 id="bkmrk-build-guide"><strong>Build guide</strong></h4>



<h5 id="bkmrk-step-1---printing-th"><strong>Step 1 ‚Äì Printing the enclosure</strong></h5>



<p id="bkmrk-this-step-takes-the-">This step takes the longest so lets send the .stl‚Äôs to the printer while we crack on with the rest of the build. There are three pieces that you‚Äôll need to print:<br>1. The main body<br>2. The ESP32-CAM retention plate<br>3. The back plate/wall mount</p>



<p id="bkmrk-you%27ll-need-to-print">You‚Äôll need to print the front and the back of the enclosure with supports. I printed it using PLA but you‚Äôll want to use PETG or ABS filament so it is waterproof and use 20-30% infill.&nbsp;</p>



<p id="bkmrk-once-the-enclosure-h">Once everything has printed, you‚Äôll need to add two M2.5 threaded inserts:<br>1. Into the front cover so you can screw the ESP32-CAM retention bracket into it.<br>1. Into the bottom of the backplate so you can screw on the face plate with a 10mm M2.5 screw</p>







<h5 id="bkmrk-step-1---adding-the-"><strong>Step 2 ‚Äì Configuring the ESP32-CAM in ESPHome</strong></h5>



<p id="bkmrk-it%27s-easiest-to-do-t">Hold down the&nbsp;<strong>‚Äúflash/download/io0‚Äù button</strong> and connect your ESP32-CAM to your computer using a micro USB cable.&nbsp; This will boot it into flashing mode.</p>



<p id="bkmrk-head-over-to-your-in">Launch Google Chrome, go to your instance of Home Assistant and launch the ESPHome Add-on by clicking&nbsp;<strong>Settings -&gt; Add-ons -&gt; ESPHome -&gt; Open Web UI</strong>. Chrome is important because it seems to be the most reliable browser for flashing firmware onto the ESP32-CAM.</p>



<p id="bkmrk-click-%2B-new-device-t">Click <strong>+ New Device</strong> to add a new device.Give it a name (‚ÄúDoorbell‚Äù is probably a good starting point).</p>



<p id="bkmrk-when-asked-to-select">When asked to select the device type, select <strong>ESP32 </strong>and check the box <strong>‚Äúuse recommended settings‚Äô</strong>.</p>



<p id="bkmrk-once-the-configurati">Once the configuration has been created, you can skip installing it onto the device ‚Äì we‚Äôll do that later.</p>



<p id="bkmrk-from-your-list-of-es">From your list of ESPHome devices, click <strong>Edit</strong> on the device that you have just created.</p>



<p id="bkmrk-at-the-bottom-of-the">At the bottom of the yaml file (below <code>captive_portal:</code>), paste the configuration code from my github repository that is linked above.&nbsp;</p>



<p id="bkmrk-click-save-and-insta">Click <strong>Save</strong> and <strong>Install</strong>.</p>



<p id="bkmrk-select-%22plug-into-th">Select&nbsp;<strong>Plug into this computer</strong>.</p>



<p id="bkmrk-click-open-esphome-w">Click <strong>Open ESPHome Web</strong>, this will allow you to flash the firmware onto the device from the web browser. This is where it is important that you are using Google Chrome.</p>



<p id="bkmrk-once-the-firmware-ha">Once the firmware has compiled, you should be able to click <strong>Download Project</strong> ‚Äì this could take a few minutes.</p>



<p id="bkmrk-head-over-to-esphome">Head over to <strong>ESPHome Web </strong>and follow the prompts to flash the firmware onto your ESP32-CAM.</p>



<p id="bkmrk-once-flashing-is-com">Home Assistant should discover the new device once the new firmware has been flashed onto it ‚Äì yay! Now you can add whatever entities you want to your dashboard.</p>



<figure><img decoding="async" width="495" height="241" src="https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724.png 495w, https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724-300x146.png 300w" sizes="(max-width: 495px) 100vw, 495px"><figcaption>Screenshot: New device found in Home Assistant.</figcaption></figure>



<h5 id="bkmrk-">Step 3 ‚Äì&nbsp; Time for some automation &amp; notifications</h5>



<p id="bkmrk--1">We want to create an Automation to take a snapshot from the doorbell‚Äôs camera and send it to your mobile phone when someone presses the doorbell button.</p>



<p>Click <strong>Settings -&gt; Automations -&gt; + Create Automation</strong> and then create a new automation from scratch. </p>



<p>Click on the three vertical dots in the top right hand corner of the screen and then click <strong>Edit in YAML</strong></p>



<p>Paste the automation from my github repo (linked above) into the editor and  update entity names for devices such as your mobile phone. </p>



<p>Save the automation and restart Home Assistant so the new automation becomes active.</p>



<p>Here is a summary of how the automation should behave.</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1.png" data-slb-active="1" data-slb-asset="1931303768" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="417" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-1024x417.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-1024x417.png 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-300x122.png 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-768x313.png 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1.png 1155w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<h5 id="bkmrk-wiring">Step 4 ‚Äì Time to wire it up</h5>



<p id="bkmrk-once-the-enclosure-i">Once the enclosure is printed, we can start the final assembly.</p>



<p id="bkmrk-i-tried-to-keep-this">There are a few variants of the ESP32-CAM board, each with slightly different pinouts so double check the pinout on the board you get.</p>



<p id="bkmrk-follow-the-wiring-gu">Follow the wiring guide below. I soldered everything onto the back of the lower PCB (the one with the micro USB port).&nbsp; It‚Äôs&nbsp;important to add the 10k ohm pull down resistor between GPIO14 and ground because without it, I noticed GPIO14 was floating high on quite often.&nbsp;</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring.png" data-slb-active="1" data-slb-asset="848354910" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="497" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-1024x497.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-1024x497.png 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-300x145.png 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-768x372.png 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring.png 1060w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>This is what it should look like when you‚Äôre done. Bonus points for covering the resistor in heatshrink tube.</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside.jpg" data-slb-active="1" data-slb-asset="164896066" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Here are some pics of mine before it goes up next to the front door!</p>



<figure>
<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall.jpg" data-slb-active="1" data-slb-asset="2010640426" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" data-id="772" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside.jpg" data-slb-active="1" data-slb-asset="2025733673" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" data-id="773" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l.jpg" data-slb-active="1" data-slb-asset="1634067867" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="576" height="1024" data-id="774" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-576x1024.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-576x1024.jpg 576w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-169x300.jpg 169w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-768x1365.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-864x1536.jpg 864w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l.jpg 1080w" sizes="(max-width: 576px) 100vw, 576px"></a></figure>
</figure>



<div>
<p><em>*The product links in this post may contain affiliate links. Any commission earned is used to keep the servers running and the gin cool.</em></p>



<p><strong>Thanks for making it to the end of the post! Did this article help you or do you like my work? </strong><br>‚òï<strong><a rel="noreferrer noopener" href="https://www.buymeacoffee.com/tristam" target="_blank">Buy Me a Coffee</a></strong>‚òï</p>
</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things you forgot because of React (456 pts)]]></title>
            <link>https://joshcollinsworth.com/blog/antiquated-react</link>
            <guid>37131802</guid>
            <pubDate>Tue, 15 Aug 2023 09:18:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshcollinsworth.com/blog/antiquated-react">https://joshcollinsworth.com/blog/antiquated-react</a>, See on <a href="https://news.ycombinator.com/item?id=37131802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
	


<main id="main" tabindex="-1"><article><img src="https://d33wubrfki0l68.cloudfront.net/3a018a423858b084f337e9d9b7c167dc2b4849bd/74eac/images/post_images/because-of-react.png" alt="" width="320" height="180">

		

		
		<p><b>Published:</b> August 4, 2023
			<br>
				<b>Updated:</b> August 9, 2023</p>
		
		
		

<h2 id="part-1-an-intro-about-music-defaults-and-bubbles">Part 1: an intro about music, defaults, and bubbles</h2>
<p>Like a lot of people, there was a time when the only music I listened to was whatever was played on my local radio station. (<em>A lot of people over 30 or so, anyway. If this doesn‚Äôt sound familiar to you yet, just stick with me for a minute here</em>.) At the time, I was happy with that. It seemed like all I needed.</p>


<p>Looking back, I realize: I naively trusted that anything good inevitably became popular‚Äîand therefore, anything worth knowing would eventually come my way on its own.</p>

<p>Eventually, though, <em>other</em> music began to take root in my life. Through new friends and the internet, I became acquainted with new artists, further and further from the things I liked before‚Äîor, at least, <em>thought</em> I liked.</p>
<p><em>This</em> music was different. I wasn‚Äôt in love with it one week and sick of it the next. Listening to it wasn‚Äôt part of an endless cycle.</p>
<p>If anything, it was the <em>opposite</em>; it was music I actually liked and appreciated <em>more</em> the more I listened to it. There was depth to it. Sure, it didn‚Äôt have the loud distorted guitars, punch-line lyrics, or sugar-coated melodies I‚Äôd enjoyed up until that point. But to my surprise, that actually somehow made it <em>better</em>, not worse.</p>
<p>That‚Äôs when I began to realize: maybe I was never really as satisfied as I thought I was.</p>
<p>Maybe my bliss was, in fact, predicated on ignorance.</p>
<h3 id="finding-richness-beyond-the-defaults">Finding richness beyond the defaults</h3>
<p>I suspect you can probably relate to that story, even if it‚Äôs not with music specifically.</p>
<p>Most likely, you now count a food or drink you didn‚Äôt once like among your favorites. Or, maybe you were surprised to find a movie, book, game, podcast, influencer, or hobby you didn‚Äôt expect to like resonated with you.</p>
<p>The details aren‚Äôt important; all I‚Äôm getting at is:</p>


<p>You‚Äôve probably experienced finding something great beyond the periphery of popular defaults.</p>

<p>Not to sound like a frontend version of a snobby hipster. That‚Äôs not my intention. If your idea of a good time is Bud Lites at Olive Garden: cool, pass the breadsticks.</p>
<p>But what I <em>am</em> trying to do is: gently share the idea that <em>maybe</em> you‚Äôre shutting yourself off to something great, without even realizing it.</p>
<p>Maybe this whole concept‚Äîfinding better things beyond familiar boundaries‚Äîapplies to our tools and workflows just as much as it does any other area of life.</p>
<p>And maybe‚Äîjust <em>maybe</em>‚Äî your current satisfaction comes, at least a little bit, from simply <em>not knowing what you‚Äôre missing</em>.</p>
<h3 id="completing-the-analogy-and-acknowledging-its-shortcomings">Completing the analogy, and acknowledging its shortcomings</h3>
<p>I‚Äôve written before about how <a href="https://reactjs.org/" rel="nofollow">React</a> is <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react">the new default</a> frontend framework, and how I don‚Äôt think most people using React on a regular basis realize quite how much it‚Äôs fallen behind.</p>
<p>And on that note, this is where our analogy begins to fall short.</p>
<p>Assuming we were only talking about personal preferences, I‚Äôd never write a blog post arguing about what you like, or trying to change your mind. (Not at this age, anyway.) Who cares? If you enjoy it, have fun.</p>


<p>But unlike music or other subjective things meant for our own enjoyment, our choice of frontend tools has empirical, measurable effects on others.</p>

<p>That decision carries a real responsibility. It‚Äôs not just about what we like. When it comes to development‚Äîunless we‚Äôre building things purely for ourselves, anyway‚Äîour enjoyment is secondary; the user‚Äôs experience is what matters most.</p>
<p>If you love your tools, that‚Äôs wonderful. I hope you do. But that‚Äôs a side quest at best, and a potentially harmful distraction at worst. Developer experience (DX) shouldn‚Äôt ever supersede user experience (UX).</p>
<p>So forgive me for choosing a flimsy metaphor. You can keep listening to the same music for the rest of your life, if you want to. I support that. But we have very valid and important reasons to push beyond the comfort of our existing preferences when it comes to the tools we use.</p>
<h3 id="the-react-bubble">The React bubble</h3>
<p>The idea that React lags behind its peers might be new to you. Like many, you might still consider React the modern standard in frontend. So let‚Äôs quickly poke at that bubble, in this one last section before we get into the titular list.</p>
<p>This, from <a href="https://toot.cafe/@slightlyoff" rel="nofollow">Alex Russell, via Mastodon</a>, is what started me writing this post:</p>
<blockquote><p>Someone asked me today if there was a case for using React in a new app that doesn‚Äôt need to support IE.</p>
<p>I could not come up with a single reason‚Ä¶</p>
<p>It‚Äôs astonishing how antiquated React is.</p></blockquote>
<p>Alex mentions React‚Äôs lack of support for web components in that thread. That feature has been glaringly missing from React for years. And yes, it‚Äôs ‚Äúon the roadmap.‚Äù As of this writing, though, there‚Äôs no firm commitment to either an implementation or an expected ship date.</p>


<p>Meanwhile, pretty much all of React‚Äôs contemporaries‚Äîany framework or technology you might choose instead of React‚Äîalready have that story shipped and in production.</p>

<p>Web components are one thing. But they‚Äôre far from the only item on the list of ‚Äústuff everything else does already and/or better.‚Äù (We‚Äôll cover several others below.)</p>
<p>React benefitted mightily from being early to the framework game; it set the standard. But that‚Äôs come with severe drawbacks in agility and adaptability. Every decision React‚Äôs made since its inception circa 2013 is another layer of tech debt‚Äîone that its newer contemporaries aren‚Äôt constrained by.</p>
<p>To <a href="https://toot.cafe/@slightlyoff/110512849934452558" rel="nofollow">quote Alex once more</a>:</p>
<blockquote><p>React is ‚Äò13 tech designed to ‚Äò08 constraints. Nothing about it is innovative in 2023; in fact, it‚Äôs the slowest way to get functional-reactive frontend programming in the modern era‚Ä¶</p></blockquote>
<p><a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react">React has aged, and how I don‚Äôt think most people realize how much or how poorly</a>. So to put the quote above another way (and tie it back to our intro about music):</p>


<p>React was designed seven Taylor Swift albums ago, for a world where John Mayer and Jennifer Aniston were still dating.</p>

<p>(<em>Seven</em> new <em>Taylor Swift albums ago, that is. That doesn‚Äôt even count the</em> Taylor‚Äôs Version <em>releases</em>.)</p>
<p>So if you‚Äôre one of the many developers whose whole world has been React for the past few years, there might be things you‚Äôve forgotten‚Äîor never knew at all‚Äîbecause you‚Äôve been using React for so long.</p>
<p>As fast as modern frontend moves, we seem to be very slow in realizing the world which crowned React king, in many ways, no longer exists. (<em>If it ever did; not many organizations had anything resembling Facebook‚Äôs specific set of problems to begin with</em>.)</p>
<p>Browsers have seen <em>wild</em> growth in new feature adoption in the last ten years, in both JavaScript and CSS. Technology and user expectations have evolved, and the current ecosystem of tools has done a <em>lot</em> more than you might think to iterate and adapt past React, in ways such legacy software can‚Äôt.</p>
<p>I realize calling React ‚Äúlegacy software‚Äù is controversial, but I think it‚Äôs fair; it‚Äôs comparatively complicated, relatively old, contains a lot of rules and gotchas, beginners are often afraid of it, and the architectural decisions it‚Äôs built on top of have long since become an impediment to its ability to iterate.</p>
<hr>
<p>If I haven‚Äôt completely alienated you yet by this point (<em>with some combination of quasi-elitism, rambling preamble, and overuse of parenthetical interjections</em>), I‚Äôd like to share some things you might have missed if your head‚Äôs been entirely in the React world for a while, in the hopes of introducing you to some tunes you might be surprised to find are better than what‚Äôs on your current playlist.</p>
<h2 id="part-2-things-you-forgot-or-never-knew-because-of-react">Part 2: things you forgot (or never knew) because of React</h2>

<p>I‚Äôve touched on this in <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react#community-and-support">other posts</a>, but any time an ‚Äúunproven‚Äù framework‚Äôs name comes up as a potential tool for a dev project, the first question anybody seems to care about is: <em>how big is the ecosystem</em>?</p>
<p>You might have even had that thought as soon as you read the premise of this post. <em>Move from React to another framework? Are any of them big enough yet?</em></p>
<p>Why do we have this obsession with ecosystem size?</p>
<p>Sure, we want to be certain this framework won‚Äôt just vanish on us, or stop being maintained in a few years. That‚Äôs perfectly valid. And yes, we wouldn‚Äôt bet the farm on something <em>too</em> new or unproven. But <a href="https://vuejs.org/" rel="nofollow">Vue</a>, <a href="https://svelte.dev/" rel="nofollow">Svelte</a>, <a href="https://preactjs.com/" rel="nofollow">Preact</a>, <a href="https://www.solidjs.com/" rel="nofollow">Solid</a>, <a href="https://astro.build/" rel="nofollow">Astro</a>, and others are all <em>far</em> past that point, well-supported and well-maintained. So it clearly isn‚Äôt just that.</p>
<p>So what <em>is</em> the sticking point? I have a theory:</p>
<p>We‚Äôve been trained that packages need to be built <em>specifically for our framework</em>.</p>
<p>You could reasonably argue that mindset started with jQuery, but I think React accelerated it.</p>
<p>With React, any time we needed a module or a widget or a library to do something specific (a carousel, a map, an accordion, or whatever else), <em>it had to be a React thing</em>; a plain web thing or a vanilla JavaScript thing just wouldn‚Äôt do. All of React‚Äôs rules and handling of state and quirks of component lifecycles meant that any available package or library which <em>wasn‚Äôt</em> explicitly written for React probably wasn‚Äôt going to work.</p>


<p>React trained us that things need to be built <em>specifically for a certain framework</em>. But that‚Äôs not very true anymore, and it arguably never should have been.</p>

<p>We shouldn‚Äôt <em>need</em> to do that‚Äîespecially for a framework that so often claims it‚Äôs ‚Äújust JavaScript.‚Äù If it‚Äôs <em>just JavaScript</em>, then it should <em>just work</em> with anything that‚Äôs <em>actually just JavaScript</em>.</p>
<p>Sure, other frontend frameworks have their own rules and conventions about state and architecture. You can step on figurative rakes in their yards, too. And yes, there will always be things that are, and need to be, built specifically to work with Svelte or Vue or whatever else.</p>
<p>But crucially‚Äîand I want to emphasize this as strongly as possible:</p>


<p>No other modern frontend framework is as stubbornly incompatible with the platform as React is.</p>

<p>If you‚Äôre building using other modern tools and frameworks, it‚Äôs <em>far</em> more likely that the vanilla JavaScript packages available will work just fine for you‚Äîand there are <em>thousands</em> of them. They‚Äôre far less likely to cause issues with render cycles, or other framework-specific issues. Not to mention: they all have the option of using web components, too.</p>
<p>You often don‚Äôt <em>need</em> a specialized package or library tailor-built for your thing, because your thing probably already works with the platform, and therefore, everything else that‚Äôs already out there.</p>
<p><a href="https://preactjs.com/guide/v10/signals/" rel="nofollow">Preact Signals</a> is a phenomenal example: although built for use with Preact, it can be imported and used in <em>any</em> framework, or even in vanilla JavaScript. Web components, too, are compatible with just about any modern non-React framework.</p>
<p>Where the frameworks fall short, it‚Äôs likely the platform already has the thing you need. (Form submission, for example; always a pain point in React, now made infinitely easier by two-way data binding and just using the conventions browsers give to us.)</p>
<p>And worst-case, it‚Äôs probably a lot <em>easier</em> to build whatever thing you need than it was in React. (It shouldn‚Äôt take very much comparing of <code>useState</code> with other frameworks‚Äô versions to see that.)</p>
<p>Being newer is often considered a disadvantage by conservative-minded developers who are wary to test the waters with something that hasn‚Äôt been thoroughly vetted in every which way possible. But it‚Äôs important to remember that being new is <em>also</em> an advantage, because there‚Äôs less tech debt and old browser support to worry about‚Äî<em>and</em> new things are free to iterate further on existing good ideas and more modern browser features.</p>
<h3 id="react-hooks-are-actually-kind-of-outdated">React hooks are actually kind of outdated</h3>
<p>Hooks are the newest evolution of React, replacing class components.</p>
<p>Credit where it‚Äôs due: hooks <em>were</em> a massive shift in the frontend space. They revolutionized how we composed logic and state in our applications. Hooks are undeniably great, and pretty much every framework has coalesced around a hooks-like model for managing state.</p>
<p>But React hooks aren‚Äôt new anymore. (In fact, stable React with hooks is almost exactly the same age as my kid, and he‚Äôs starting pre-k in a couple of weeks.)</p>
<p>Hooks are no longer a competitive advantage, or even a notable feature; they‚Äôre the baseline. They‚Äôre just the way we do things.</p>


<p>Every other framework not only has its own implementation of hooks, but notably: every one of them is faster, smarter, easier to write, or a combination of all three.</p>

<p>Preact‚Äôs Signals warrant mention here; so do Svelte‚Äôs dead-simple stores. Solid, too, has Signals. Even Vue 3‚Äôs composition API, which is pretty directly inspired by hooks, has some key advantages over the React implementation.</p>
<p>Hooks are an excellent pattern, and React deserves credit for popularizing it. But pretty much every other framework does hooks better, with fewer rules, and with less boilerplate.</p>
<p>If you‚Äôre unfamiliar with the concept of Signals: it‚Äôs a crude oversimplification, but you could think of them as the next, better evolution of reactive state; an update to hooks, with better defaults around what causes re-renders, to only re-render the nodes that need to be re-rendered, instead of entire components.</p>
<h3 id="you-dont-need-to-micro-manage-rendering-anymore">You don‚Äôt need to micro-manage rendering anymore</h3>
<p>I have a confession to make: I‚Äôm still not exactly sure what the difference between <code>useMemo</code> and <code>useCallback</code> is‚Äîor when you should and shouldn‚Äôt use them‚Äîeven though I <em>literally read multiple articles on that exact topic earlier today</em>. (No joke.)</p>
<p>I have a second confession: it‚Äôs still not intuitive to me what should and shouldn‚Äôt go into the <code>useEffect</code> dependency array, or why. I feel like every time I write a <code>useEffect</code> call, I spend like 15 minutes refactoring my code to be in a shape the linter likes, even when <em>I‚Äôm 99% certain it‚Äôs actually fine</em> and it‚Äôs not going to suck my app into an infinite abyss.</p>
<p>I‚Äôm betting if you use React, you can probably relate to those confessions. And maybe you‚Äôve even just accepted that confusion and ambiguity as normal. But if so, you should know:</p>
<p>We haven‚Äôt had to do this kind of rendering cycle micromanagement in other frameworks for <em>years</em>.</p>


<p>These days, frameworks are smart enough to handle this kind of thing without you needing to hold their hand and explain what they should do.</p>

<p>They already know not to waste precious resources re-rendering when there‚Äôs no real need. They‚Äôre intelligent enough to only update values, and not constantly reevaluate things that don‚Äôt need it.</p>
<p>‚Ä¶Most of the time, anyway. They‚Äôre not perfect. But they <em>are</em> much better than React at knowing what to do, and doing it in a performant way by default.</p>
<p>You <em>might</em> need to optimize some things in other frameworks, too. They‚Äôre not perfect. But by the time you do, you‚Äôre way, <em>way</em> past the point where you would‚Äôve needed to in React.</p>
<h3 id="nobody-else-is-afraid-of-their-frameworks-version-of-useeffect">Nobody else is afraid of their framework‚Äôs version of <code>useEffect</code></h3>
<p>When you want a component to just do something when it enters the DOM‚Äîand/or when you want it to recalculate something dynamically, based on some other data or variable(s)‚Äîjust about every other framework has a better way than <code>useEffect</code>.</p>
<p>I don‚Äôt think I need to harp too much on this here, because even within React communities, <code>useEffect</code> is considered notoriously hazardous, and often even avoided altogether. But trust me: no other non-React-based frontend framework has people so afraid to use such a normal, useful feature, and nowhere else are there such obtuse rules around it.</p>
<p>Nobody else is looking at third-party packages just to do something when a component is mounted without shooting themselves in the foot.</p>
<h3 id="scaling-isnt-really-a-frontend-concern-anymore">Scaling isn‚Äôt really a frontend concern anymore</h3>
<p>This is the <em>other</em> question people immediately ask when a new(er than React) framework comes up: <em>does it scale</em>? But I believe that question might be a bit outdated.</p>
<p>It‚Äôs worth remembering: the world that gave us React had a different set of problems.</p>
<p>In that world, most frontend UIs were built either with vanilla JavaScript, or with jQuery (or similar alternatives). And that method of building apps, as we now know, didn‚Äôt scale well beyond a certain limit.</p>
<p>That‚Äôs because you had to write your own selectors for each and every element and DOM node you might want to interact with, and you had to come up with your own manual way of tracking and syncing state. That usually involved writing to and retrieving from the DOM, which was messy, error-prone, and most importantly, slow. (That‚Äôs where the virtual DOM came in, but even <em>that</em> has been <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead" rel="nofollow">pretty thoroughly outdated for years</a>.)</p>
<p>Writing modular code back then was difficult to impossible, and JS files often ballooned to hundreds of lines, if not thousands. If multiple authors were working on the same project, they‚Äôd often reinvent, repeat, or even override each other‚Äôs code (partly because code often went into a shared global namespace, which made collisions even more likely). And the bigger or more complex your app (<em>Facebook</em>), the worse the problem was.</p>
<p>It‚Äôs important to remember: that‚Äôs our baseline for ‚Äúdoes it scale?‚Äù as it relates to frontend. Does it stay reasonably maintainable even if my app grows exponentially?</p>


<p>The worry that a frontend framework might not scale is as old as jQuery, and should be considered just as antiquated in relation to modern web development.</p>

<p>React solved many of these problems, yes. But it didn‚Äôt do so by being a marvel of modern engineering, so much as simply coming up with a good way to manage and share state, make data reactive, abstract complexity, and enable developers to share the same programming patterns without conflicts, namespace collisions, or overrides.</p>
<p>React wasn‚Äôt the best, only, or even <em>first</em> solution to frontend scalability; it‚Äôs just one of many possible versions of the same paradigm.</p>
<p>(It also happens to be among the oldest.)</p>
<p>How do I know this? Because a plethora of benchmark tests have been run, with publicly available results, comparing the performance of React to every other frontend framework at scale. (I‚Äôm not linking to any here, because they‚Äôre readily available online.) They all confirm that just about every other option in the frontend space does as well or better than React‚Äîand in many cases, <em>dramatically</em> better.</p>
<p>Here I‚Äôm referring to scaling in the general sense; making sure complexity stays minimal, and doesn‚Äôt grow linearly as the app increases in size. Certainly, some frameworks will scale much better or worse than others in terms of, say, building static HTML from Markdown files, or other more specialized tasks.</p>
<h3 id="server-side-rendering-isnt-special-anymore">Server-side rendering isn‚Äôt special anymore</h3>
<p>An earlier version of this section erroneously conflated server-side rendering with React Server Components (for reasons that I hope are at least understandable, given the confusing naming conventions).</p>
<p>There was a time, several years ago, when React was pretty much the only game in town when it came to server-rendered content (mainly via Next JS). People were rightly excited for the idea that React could be rendered on a server as HTML, instead of on the client as a Single-Page App (SPA). The speed and SEO gains were impossible to ignore, and initially, it took other frameworks a bit to catch up.</p>
<p>However, as is a theme with these things in general, and with this post in particular: the first to iterate is rarely the best.</p>
<p><a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a> is server-rendered by default, without you needing to do anything, and offers fine-grained control over its rendering patterns. <a href="https://nuxt.com/" rel="nofollow">Nuxt</a>, Vue‚Äôs meta-framework, was earlier to the game (being obviously inspired by Next).</p>
<p><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a> (Deno‚Äôs frontend framework) is entirely server-rendered, except for what you designate as an ‚Äúisland‚Äù (client-rendered); anything else just ships as static HTML. Fresh also uses Preact (which, again, is even faster than React, and which has <a href="https://preactjs.com/guide/v10/signals/" rel="nofollow">Signals</a>, a much more performant and ergonomic version of <code>useState</code> and the reactivity model).</p>
<p>Astro has server-rendering, and just lets you server-render whatever flavor of components you want. It can render other frameworks‚Äô components just fine, and has even been noted as a major performance upgrade from Next, in some cases.</p>
<p><a href="https://start.solidjs.com/getting-started/what-is-solidstart" rel="nofollow">SolidStart</a> (Solid‚Äôs meta-framework) has server rendering. Qwik is entirely built around it. Even some older frameworks like <a href="https://emberjs.com/" rel="nofollow">Ember</a> and <a href="https://angularjs.org/" rel="nofollow">Angular</a> have a story here; I‚Äôm sure I‚Äôm leaving out others, too.</p>
<p>Point is: way back when, React was one of few frameworks that had the concept of rendering client view framework components on a server. But now, server rendering is table stakes. A lot of newer frameworks don‚Äôt just have the <em>option</em> to render on the server; they do it <em>by default</em>.</p>
<p>PHP is back, baby.</p>
<h3 id="two-way-data-binding-isnt-hard-and-it-isnt-a-bad-idea">Two-way data binding isn‚Äôt hard and it isn‚Äôt a bad idea</h3>
<p>I think it‚Äôs important to remember that React was created by Facebook, in order to solve Facebook‚Äôs unique set of problems.</p>
<p>One of React‚Äôs strongest opinions‚Äîthat data should flow only one way (top down)‚Äîis a good example of how the engineering challenges of Facebook in the early 2010s indelibly shaped React‚Äôs architecture.</p>
<p>For some time, it seemed like one-way data flow was considered a best practice. These days, though, we‚Äôve mostly figured out solutions to the pitfalls of two-way data binding, and found that in many cases, it‚Äôs actually much more convenient.</p>
<p>Working with forms in React is notoriously cumbersome because every user keystroke is a two-step process: get the value from the input; then set the state to match it (which in turn needlessly re-renders the input, to contain the exact value it already did, but synced up with React state). Sure, it‚Äôs usually too fast to notice, but it‚Äôs a lot of extra work.</p>
<p>Svelte, Vue, and many others don‚Äôt have this issue. You can just bind state in such a way that it updates automatically from both ends. If the state changes, the DOM updates; if the DOM changes, the state updates.</p>
<p>This way, you don‚Äôt have to do the multi-step dance. If you just want to capture, say, the value of a text box, you do two-way data binding. Then, when the user types into the field, the data updates automatically, and you can get it whenever the time is right with no further steps. If in the meantime you need to do something like set a value or clear the field, that‚Äôs also a simple one-liner.</p>
<p>Two-way data binding lets you keep data and the DOM in sync without the need to constantly make sure one is keeping up with the other.</p>
<p>Could you get in trouble using these? For sure. But I find that dogmatic ideals of best practices get in the way as much or more than they help. One-way data flow is a prime example.</p>
<h3 id="styling-is-easy-actually">Styling is easy, actually</h3>
<p>If you work mostly in React, it‚Äôs quite possible you‚Äôve gone through two, three, or more iterations of handling styles in your frontend components.</p>
<p>You might have imported .css files straight into JSX components, or used CSS Modules, Styled Components, and/or Tailwind (probably with either the <code>classnames</code> or <code>tailwind-merge</code> packages‚Äîor maybe even both, plus some extra Tailwind add-ons). And those are just the most popular options.</p>
<p>Tailwind is its own rabbit hole (and its own frontend framework I‚Äôm not particularly a fan of; I consider it cutting against the grain of the platform in exchange for short-term gains that eventually compound into long-term losses). But in any case, these styling solutions exist and see significant adoption at least partly because React‚Äôs had a vacuum in place of first-party styling options for as long as it‚Äôs been around.</p>


<p>You might not realize styling is a solved problem in several other frameworks.</p>

<p>In particular, Vue and Svelte both have their own component styling story. They both have component-level scoping (Vue‚Äôs is opt-in; Svelte‚Äôs is opt-out). They both work wonderfully with vanilla CSS, if that‚Äôs the way you want to go. But both of them‚Äîalong with every other frontend framework‚Äîare still compatible with CSS modules, Tailwind, Sass, or whatever else you like to use.</p>
<p>But most importantly: all the supposed problems with CSS‚Äîwhether you actually consider them problems or not‚Äîare fully addressed by the built-in style handling. You don‚Äôt need a mess of packages and configs nearly as much anywhere else, because scoped CSS solves just about every issue you could possibly imagine.</p>
<p>Seriously; read through any list of reasons CSS is supposedly bad (it‚Äôs not, but people who are bad at it like to say that). Just about any critique you could possibly have of CSS is solved by scoped styling, and multiple non-React frameworks just come with it already built in.</p>
<h3 id="frameworks-arent-as-hard-to-learn-anymore">Frameworks aren‚Äôt as hard to learn anymore</h3>
<p>I theorize developers mainly trained on React think back to how difficult it was to learn, and assess the learning curve of other frameworks similarly. And that‚Äôs probably part of what keeps us from trying new things; it seems really hard, because it sure was the first time.</p>
<p>All the ins and outs of state management, props, nesting, component lifecycles, hooks, and of course, how to write JSX‚Ä¶it‚Äôs a lot. Even the most ardent React fans would likely concede it‚Äôs not the easiest thing for beginners to pick up quickly. (Anyone who says otherwise has probably forgotten what it was like to be a beginner.)</p>
<p>If you can relate, I have good news:</p>


<p>There‚Äôs no comparable tool as difficult to learn as React is. But once you know one framework, you have a huge head start on all the others.</p>

<p>I compare this to learning your <em>second</em> musical instrument (not just to tie this back to music again). The <em>first</em> time you learn to play, you‚Äôre learning <em>everything about music</em>, on top of learning your specific instrument, and how to get it to make the sounds you want. But when you learn your <em>second</em> instrument, you get to skip so much. All the concepts are familiar. You understand music. All you need to do is transfer your existing knowledge and muscle memory into a slightly different shape.</p>
<p>Frontend is similar: every frontend framework has components; they‚Äôre all compatible with TypeScript; they all have the concept of props, children, and reactive state. These are things we‚Äôve generally agreed we like and are good. They just have different takes on implementation.</p>
<p>And speaking of which: while React undoubtedly helped to proliferate these ideas, it would be silly to consider React the ideal implementation of them.</p>
<p>Great things are created through iteration, and for the most part, other choices in the frontend space that came later have the distinct advantage of iterating on top of the core ideas of React.</p>
<p>This means React is a bit like a git branch that‚Äôs fallen well behind <code>main</code>. You might not realize it, if React is the star your galaxy orbits around, but‚Ä¶well, frontend has moved on. The ecosystem has taken those ideas and run with them to make things that are even better.</p>
<p>We have no shortage of more performant, less complex, less difficult-to-learn options available to us now. And if you know React already, none of them will be very hard to learn as well.</p>
<h2 id="part-3-the-other-stuff-you-should-try">Part 3: the other stuff you should try</h2>
<p>You probably started wondering a few dozen paragraphs ago: if React is so antiquated, what‚Äôs the alternative?</p>
<p>I‚Äôm going to cover several here, and mention their use cases as well. One of the issues with React is that it‚Äôs long tried to be everything for everyone, and useful though a React-shaped tool might be, I think maybe two or three different tools could be better than one Swiss army knife.</p>
<p>Two quick notes before we dive in, though:</p>
<ol><li><p>I list several options here, for the sake of covering all the other modern frameworks I mentioned above. <strong>I don‚Äôt expect anyone to learn about‚Äîlet alone <em>use</em>‚Äîall of them</strong>. If you have to pick one, go with Svelte, or maybe Vue. But in any case, know that I‚Äôm only listing them all for the sake of thoroughness.</p></li>
<li><p>I didn‚Äôt list <em>all</em> the options here. There are others.</p>
<p>I omitted Ember and Angular, for example, because they‚Äôre both older than React, and don‚Äôt generally tend to outperform React significantly, if at all, in benchmark tests (sorry, Mel).</p>
<p>I also omitted the lightweight options like <a href="https://alpinejs.dev/" rel="nofollow">Alpine</a> and <a href="https://github.com/vuejs/petite-vue" rel="nofollow">Petite Vue</a>, since those are more replacements for jQuery than React, and shine where you might not need something as heavy-handed as a framework.</p>
<p>Finally, I also omitted exceptionally good tools in and around this category, like <a href="https://www.11ty.dev/" rel="nofollow">Eleventy</a>, since it‚Äôs more of a pure static site generator than a framework. (Still worth a look if you‚Äôre using Gatsby, however.)</p></li></ol>
<p>All that said: here‚Äôs your Discover Weekly.</p>
<h3 id="svelte-my-personal-pick"><a href="https://svelte.dev/" rel="nofollow">Svelte</a> (my personal pick)</h3>
<blockquote><p><em>Ladies and gentlemen of the class of 2023: use Svelte.</em></p>
<p><em>If I could offer you only one tip for the future, Svelte would be it.</em></p></blockquote>
<p>Joking aside: if I were to pick one thing from this list to recommend over React, it would be <a href="https://svelte.dev/" rel="nofollow">Svelte</a>. I‚Äôve long maintained that ‚ÄúSvelte is React, but without the bullshit,‚Äù as I originally quipped on Twitter back in 2019 (RIP), and if anything, that‚Äôs only grown truer over time.</p>
<p>Svelte is delightfully simple to use, comparatively easy to learn (especially if you‚Äôre coming from the React world already; even the syntax is often similar), much, much more performant in just about all cases, and capable of anything React is and more. This site, and all my own side projects these days, are written in <a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a>.</p>
<p>Svelte is fast; it‚Äôs comparable to the fastest options available. Its DX is phenomenal; it regularly appears at or near the top of most-loved frameworks in developer surveys.</p>
<p>Svelte hews as closely to the web platform as possible, so even though it‚Äôs incredibly powerful, its concepts will be largely familiar. Svelte also includes transitions, easings, CSS handling, component-scoped styles, and more niceties out of the box.</p>
<p>That might make you wonder about framework size, but where Svelte differs is: instead of being a JavaScript runtime, it‚Äôs a compiler. Anything you don‚Äôt use is stripped away at build time, and your code is transpiled into tiny bits of vanilla JavaScript. That means Svelte‚Äôs bundles are generally a fraction the size of React‚Äôs.</p>


<p>Although it feels and works like a framework, Svelte is, essentially, a small, elegant superset of HTML, with a delightfully simple syntax, which compiles to fast, minimal bundles.</p>

<p>Svelte‚Äôs own meta-framework, <a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a>, is highly versatile and powerful, capable of static, server-rendered, deployment to the edge, and even mixing per-route. It hit version 1.0 at the end of 2022 and is very ready for production. (It‚Äôs also supported by Vercel, who make Next.js as well.)</p>
<h4 id="svelte-is-recommended-if">Svelte is recommended if:</h4>
<p>You want to rediscover the joy of frontend with (what I consider to be) the best all-around option, for the reasons above.</p>
<h4 id="svelte-replaces">Svelte replaces:</h4>
<p>Anything you‚Äôre doing with React. Svelte can replace React itself, or SvelteKit is versatile enough to sub in for Next, Gatsby, and/or Remix (or even all at once).</p>
<h3 id="vue"><a href="https://vuejs.org/" rel="nofollow">Vue</a></h3>
<p><a href="https://vuejs.org/" rel="nofollow">Vue</a> is possibly the closest option to React, and likely has the next-biggest ecosystem. It‚Äôs significantly more performant than React, however, and a bit more UI-focused.</p>
<p>In some ways, Vue is the smallest leap from React, especially now that it has a similar hooks-based approach in Vue 3. But Vue uses a templating language closer to default HTML than to JSX, which makes it much easier to write conditionals and loops in template files, without having to reach for workarounds like <code>map</code> and ternaries.</p>
<p>Vue has a similar meta-framework to Next in <a href="https://nuxtjs.org/" rel="nofollow">Nuxt</a>, which is well maintained and adding powerful new features all the time. Vue is also a bit more batteries-included than React, coming with things like scoped CSS handling and easy transitions/animations out of the box.</p>
<h4 id="vue-is-recommended-if">Vue is recommended if:</h4>
<p>Community size/overall framework popularity is an important factor for you; you want something like React, but more batteries-included or HTML-like; you prefer your framework to be independent and <em>not</em> be owned by a large corporation.</p>
<h4 id="vue-replaces">Vue replaces:</h4>
<p>React itself, or <a href="https://nuxt.com/" rel="nofollow">Nuxt</a> can replace anything you might be using Next for.</p>
<h3 id="solid"><a href="https://www.solidjs.com/" rel="nofollow">Solid</a></h3>
<p><a href="https://www.solidjs.com/" rel="nofollow">Solid</a> is what I would call React, but better. It looks almost (if not entirely) identical to React in many cases, but Solid is far, far more performant. It‚Äôs one of the fastest options available, in fact.</p>
<p>Solid essentially starts with React, and then rethinks it to eliminate complexity, performance issues, and a lot of boilerplate. Signals appear as a concept in Solid, which eliminate a great deal of the confusion and footguns around component rendering and lifecycles. It might even be fair to say Solid is React, if React was built in the modern era, on top of all the lessons we‚Äôve learned since 2013.</p>
<p>Solid also offers its own meta-framework in <a href="https://start.solidjs.com/getting-started/what-is-solidstart" rel="nofollow">SolidStart</a>, though that is currently in beta. Solid itself is plenty mature enough to use, though, and boasts an impressive gallery of sponsors.</p>
<h4 id="solid-is-recommended-if">Solid is recommended if:</h4>
<p>You generally like React (and JSX), but you just wish it was more modern, faster and/or easier; performance is an absolute top priority.</p>
<h4 id="solid-replaces">Solid replaces:</h4>
<p>React and React DOM. SolidStart will likely be capable of replacing Next one day, but it‚Äôs still in beta as of this writing.</p>
<h3 id="fresh"><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a></h3>
<p><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a> is a server-rendered frontend framework with islands architecture, built on Deno. It‚Äôs a bit younger than most of the rest of the items on this list, but it‚Äôs full of promise as a minimal-JS, island-based framework that can run on the edge‚Äîpowered by Deno, no less, which means your server code is faster, more secure, TypeScript by default, and all the other benefits Deno brings over traditional Node (such as easier, first-party linting, testing, and code formatting settings).</p>
<p>Every Fresh component is either static-rendered and served at response time as HTML, with no JavaScript, or an ‚Äúisland,‚Äù which means it renders only on the client. You can mix and match as needed. Because it runs on Deno, this opens the gate for extremely fast, dynamic content that loads as quickly as possible on any device anywhere in the world.</p>
<p>Fresh uses Preact, so you know it‚Äôs fast, and won‚Äôt be difficult to pick up if you‚Äôre coming from React, either. And again: building on Deno feels great.</p>
<h4 id="fresh-is-recommended-if">Fresh is recommended if:</h4>
<p>You like the idea of a server-side app globally available in the cloud, shipping absolutely minimal JavaScript, and/or building on the latest technology.</p>
<h4 id="fresh-replaces">Fresh replaces:</h4>
<p>Remix is probably the closest thing to Fresh in React-land.</p>
<h3 id="astro"><a href="https://astro.build/" rel="nofollow">Astro</a></h3>
<p><a href="https://astro.build/" rel="nofollow">Astro</a> is a next-gen, highly performant static site generator that does more than static. Astro is one of the newest options on this list, but it‚Äôs already at a very stable 1.0 release and has garnered widespread praise and adoption.</p>
<p>Built mainly to be a new generation of SSG (hey, React fans: it supports JSX and MDX), Astro now also features dynamic, server-side capabilities as well. I‚Äôd definitely recommend it over, say, Gatsby, or for any content-heavy or static sites.</p>
<p>The real killer feature is: Astro ships zero JavaScript by default. You opt in to only what you want to use.</p>
<p>Astro is also compatible with whatever frontend framework you want to use, so if you prefer to template in React, Vue, Svelte, or others, you can!</p>
<h4 id="astro-is-recommended-if">Astro is recommended if:</h4>
<p>You‚Äôre building a largely static, or content/Markdown-based site (even if you may need some server-side rendering or logic); you want to ship minimal JavaScript; you want to bring your own frontend framework.</p>
<h4 id="astro-replaces">Astro replaces:</h4>
<p>Gatsby, or similar React-based content tools.</p>
<h3 id="preact"><a href="https://preactjs.com/" rel="nofollow">Preact</a></h3>
<p>You probably already know about <a href="https://preactjs.com/" rel="nofollow">Preact</a> if you live in React land, but it warrants mention here. It‚Äôs a much slimmer, much faster version of React. Although it began more-or-less as a drop-in replacement for React, it‚Äôs beginning to gain some superior features React doesn‚Äôt have (like <em>Signals</em>, which we‚Äôve already mentioned).</p>
<h4 id="preact-is-recommended-if">Preact is recommended if:</h4>
<p>You want to stick with React, essentially, but you just want it to be faster.</p>
<h4 id="preact-replaces">Preact replaces:</h4>
<p>React. (Actually, it just adds a P to the beginning. The P stands for performance. I made all that up; don‚Äôt blame the Preact team for that.)</p>
<h3 id="qwik"><a href="https://qwik.builder.io/" rel="nofollow">Qwik</a></h3>
<p><a href="https://qwik.builder.io/" rel="nofollow">Qwik</a> server-renders React-like code (JSX) with a new approach to hydration and performance. In fact, what it does can‚Äôt really be called ‚Äúhydration‚Äù at all; instead, it serializes JavaScript into the DOM, and loads it in tiny bits only when it‚Äôs needed. Qwik is one of the deeper cuts on this list, but if you have a <em>lot</em> of interactivity that you need to run as fast as possible, it‚Äôs well worth a look.</p>
<h4 id="qwik-is-recommended-if">Qwik is recommended if:</h4>
<p>You‚Äôre shipping <em>lots</em> of JavaScript to the browser, and you want a way to make that more performant.</p>
<h4 id="qwik-replaces">Qwik replaces:</h4>
<p>React itself, allowing it to run very efficiently on the edge.</p>
<h3 id="web-component-libraries">Web component libraries</h3>
<p>I won‚Äôt go very deep on this one, because frankly, I‚Äôm not the guy for that. I don‚Äôt have the experience with either <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components" rel="nofollow">web components</a> on their own, or web component frameworks, to speak well on the topic.</p>
<p>That said, there <em>is</em> a certain class of projects that could benefit from a <a href="https://www.webcomponents.org/libraries" rel="nofollow">web component framework/library</a> like <a href="https://lit.dev/" rel="nofollow">Lit</a>, <a href="https://stenciljs.com/" rel="nofollow">Stencil</a>, <a href="https://www.polymer-project.org/" rel="nofollow">Polymer</a>, or others. Rather than generating ‚Äúproprietary‚Äù components in a specific frontend framework, these libraries help you write actual web components, which are then portable to any web project.</p>
<p>In my opinion, most projects still benefit from using a frontend framework over pure web components‚Äîor, at the very least, both together. Maybe that will change in the future, but for now, I think the tradeoffs still tilt away from a pure web component approach in most cases.</p>
<p>Still, there are certainly use cases for which a purely web component-based approach ought to be considered. And for <em>those</em> projects, React is definitely overkill. The web component libraries mentioned above would be a much better fit.</p>
<h4 id="web-component-libraries-are-recommended-if">Web component libraries are recommended if:</h4>
<p>You need to reuse the same components in multiple environments; want to future-proof yourself against framework changes; or just prefer using the platform, and are prepared to deal with the tradeoffs of web component authoring.</p>
<h4 id="web-components-replace">Web components replace:</h4>
<p>React, but maybe only partially, depending on your use case</p>
<h2 id="epilogue">Epilogue</h2>
<p>This post is, admittedly, a lot like my post from last year, <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react"><em>The self-fulfilling prophecy of React</em></a>. It treads some of the same territory, and makes some of the same arguments (albeit hopefully in new ways or from new perspectives).</p>
<p>I didn‚Äôt set out to repeat myself, but clearly, I think about this stuff a lot‚Äîspurred no doubt by my professional shift to working with React full time around the time that post was published, by coincidence.</p>
<p>I‚Äôve come to believe React‚Äôs popularity is, in no small part, because folks don‚Äôt look beyond it. It‚Äôs not the greatest, but most people aren‚Äôt looking for the greatest; they‚Äôre just looking for good enough. (We‚Äôre humans. There are a lot of personal, emotional, irrational reasons for our decisions, all of us, and that‚Äôs fine. We‚Äôre busy.)</p>
<p>It seems like we adopt technologies in leaps, rather than in a linear motion, at least in the world of frontend. Part of what caused everyone to jump on the React bandwagon was that <em>everyone</em> at the time was stuck on antiquated technology, and was looking for something better. We didn‚Äôt gradually advance to the new thing, in small steps (maybe because that wasn‚Äôt really an option to begin with); we took a giant <em>leap</em> from where we were to the next thing.</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/d1e525bee1dc5534a4f1c1466ce1a429e2b2d286/4f0d3/images/post_images/tech-adoption.png" alt="A linear line with an arrow pointing forward, labeled 'progress.' There are a few arced leaps of progress on top of the line, jumping from left to right, labeled 'adoption.' The final leap, however, lands well short of the furthest edge of the straight 'progress' line."></p>
<p>But the thing is: we‚Äôve been sitting there, in mostly that same spot, since we took that leap all those years ago.</p>
<p>My sense is: we‚Äôre beginning to near another leap.</p>
<p>I don‚Äôt know what it will be, or why. But I think we‚Äôre starting to feel all the problems React actually <em>doesn‚Äôt</em> solve for us, like we felt with jQuery back in those days. And I think eventually, it will be clear that it‚Äôs time to advance.</p>
<p>What will that new thing be? I don‚Äôt know. Maybe it‚Äôll just be the web platform. Maybe we won‚Äôt even need frameworks. Maybe it‚Äôll be a framework above; maybe it‚Äôll be something we haven‚Äôt even seen yet. Maybe it won‚Äôt even be <em>a thing</em>; maybe there will be more diversity of tooling and less coalescing around one single accepted standard (though of all the above options, I‚Äôd say that seems the least likely, because again: humans. We‚Äôre busy little monkeys and so we like defaults.)</p>
<p>I think, though, that the delta between React and that thing, whatever it is, will continue to grow larger and larger over time.</p>
<p>So every new day is an even better day than the one before it to explore what you‚Äôve been missing.</p>
<p>Happy listening.</p>

		

		
</article></main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Bankman-Fried is going to jail (150 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/</link>
            <guid>37131626</guid>
            <pubDate>Tue, 15 Aug 2023 08:49:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/">https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/</a>, See on <a href="https://news.ycombinator.com/item?id=37131626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      From Bahamas penthouse to Manhattan‚Äôs big house    ‚Äî
</h4>
            
            <h2 itemprop="description">Judge also denied SBF's request to delay jail time.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258714385-800x534.jpg" alt="Sam Bankman-Fried.">
      <figcaption><div><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258714385.jpg" data-height="683" data-width="1024">Enlarge</a> <span>/</span> Sam Bankman-Fried.</p></div></figcaption>  </figure>

  




<!-- cache hit 57:single/related:14f233083c90a25e64cd5632e9f5f425 --><!-- empty -->
<p>A federal judge in New York today ordered disgraced FTX founder Sam Bankman-Fried's to jail after revoking his bail, <a href="https://www.nytimes.com/2023/08/11/technology/sam-bankman-fried-to-be-sent-to-jail-after-judge-revokes-bail.html">The New York Times reported</a>.</p>
<p>Bankman-Fried had been under house arrest, but prosecutors convinced Judge Lewis A. Kaplan of the Federal District Court in Manhattan that Bankman-Fried had fed documents to the media in order to intimidate a witness in the case. Now Bankman-Fried has to prepare his defense to seven criminal charges from jail.</p>
<p>In June, Bankman-Fried filed a motion to dismiss, hoping that some of those charges would be dropped. But Kaplan decided that his arguments in the motion were "either moot or without merit,‚Äù <a href="https://www.cnn.com/2023/06/27/business/sbf-motion-to-dismiss-denied/index.html">CNN reported</a>.</p>
<p>A New York Times <a href="https://www.nytimes.com/2023/07/20/technology/ftx-caroline-ellison-bankman-fried.html">report</a> was among media stories that the prosecution shared to convince the court to give Bankman-Fried jail time. In that report, Bankman-Fried shared private writings of Caroline Ellison, a former FTX executive and former girlfriend to Bankman-Fried who has pled guilty and is currently cooperating with law enforcement in their investigation of the cryptocurrency exchange, the Times reported.</p>
<p>Prosecutors claimed that Bankman-Fried shared Ellison's communications to intimidate her. The court found that Bankman-Fried tampered with witnesses at least twice, <a href="https://www.reuters.com/legal/ftxs-bankman-fried-seeking-avoid-jail-due-back-court-2023-08-11/">Reuters reported</a>.</p>
<p>It wasn't just the New York Times report that alarmed the court, however. Bankman-Fried's other communications with the media led the prosecution to request a gag order to block all talks with the media.</p>
<p>According to The New York Times, "The Times, the Reporters Committee for the Freedom of the Press, and a documentarian making a film" about Bankman-Fried "each submitted court filings raising First Amendment concerns about the gag order."</p>
<p>Bankman-Fried requested his detention be delayed, pending an appeal of the order revoking his bail, Reuters reported, but the judge denied that request.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous batching to increase LLM inference throughput and reduce p50 latency (106 pts)]]></title>
            <link>https://www.anyscale.com/blog/continuous-batching-llm-inference</link>
            <guid>37131477</guid>
            <pubDate>Tue, 15 Aug 2023 08:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">https://www.anyscale.com/blog/continuous-batching-llm-inference</a>, See on <a href="https://news.ycombinator.com/item?id=37131477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Due to the large GPU memory footprint and <a href="https://www.anyscale.com/large-language-models">compute cost of LLMs</a>, serving dominates the compute cost for most real world applications. ML engineers often treat LLMs like "black boxes" that can only be optimized with internal changes such as quantization and custom CUDA kernels. However, this is not entirely the case. Because LLMs iteratively generate their output, and because LLM inference is often memory and not compute bound, there are surprising <i>system-level</i> batching optimizations that make 10x or more differences in real-world workloads.</p><p>One recent such proposed optimization is <b>continuous batching</b>, also known as <b>dynamic batching</b>, or batching with <b>iteration-level scheduling</b>. We wanted to see how this optimization performs. We will get into details below, including how we simulate a production workload, but to summarize our findings:</p><ul><li><p>Up to 23x throughput improvement using continuous batching and continuous batching-specific memory optimizations (using <a href="https://twitter.com/zhuohan123/status/1671234707206590464?s=20"><u>vLLM</u></a>).</p></li><li><p>8x throughput over naive batching by using continuous batching (both on <a href="https://docs.ray.io/en/latest/serve/index.html"><u>Ray Serve</u></a> and <a href="https://github.com/huggingface/text-generation-inference"><u>Hugging Face‚Äôs text-generation-inference</u></a>).</p></li><li><p>4x throughput over naive batching by using an optimized model implementation (<a href="https://github.com/NVIDIA/FasterTransformer"><u>NVIDIA‚Äôs FasterTransformer</u></a>).</p></li></ul><p>You can try out continuous batching today: see <a href="https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py"><u>this example to run vLLM on Ray Serve</u></a>.</p><p>The remainder of this blog is structured as follows:</p><ul><li><p>We‚Äôll cover the basics of how LLM inference works and highlight inefficiencies in traditional request-based dynamic batching policies.</p></li><li><p>We‚Äôll introduce continuous batching and how it answers many of the inefficiencies of request-based dynamic batching.&nbsp;</p></li><li><p>We then discuss our benchmarks and the implications this has on how to serve LLM models cost-effectively.</p></li></ul><hr><h2>The basics of LLM inference</h2><p>There is a lot to know about LLM inference, and we refer users to <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one"><i><u>Efficient Inference on a Single GPU</u></i></a><i> </i>and <a href="https://huggingface.co/blog/bloom-inference-optimization"><i><u>Optimization story: Bloom inference</u></i></a> for more detail. However, at a high level, LLM inference is pretty straightforward.</p><p>For each request:</p><ol><li><p>You start with a sequence of tokens (called the "prefix" or "prompt").</p></li><li><p>The LLM produces a sequence of completion tokens, stopping only after producing a stop token or reaching a maximum sequence length. </p></li></ol><p>This is an iterative process. You get one additional completion token for each new forward pass of the model. For example, suppose you prompt with a sentence "What is the capital of California: ", it would take ten forward pass iterations to get back the full response of ["S", "a", "c", "r", ‚Äúa‚Äù, "m", "e", "n", "t", "o"]. This example simplifies things a little bit because in actuality tokens do not map 1:1 to ASCII characters (a popular token encoding technique is <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding"><u>Byte-Pair Encoding</u></a> which is beyond the scope of this blog post), but the iterative nature of generation is the same regardless of how you tokenize your sequences.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/4Htl7q5sOaX47ViD1EaMdT/039ba19b73bcf58e7be4130d53b147d4/01_diagram-llm-basics_aspect_ratio.png" alt="cb 01 diagram-llm-basics"></p></div><p><span>Simplified LLM inference. This toy example shows a hypothetical model which supports a maximum sequence length of 8 tokens (T1, T2, ‚Ä¶, T8). Starting from the prompt tokens (yellow), the iterative process generates a single token at a time (blue). Once the model generates an end-of-sequence token (red), the generation loop stops. This example shows a batch of only one input sequence, so the batch size is 1.</span></p></div><p>Now that we understand the simplicity of the iterative process, let‚Äôs dive deeper with some things you may not know about LLM inference:</p><ol><li><p>The initial ingestion (‚Äúprefill‚Äù) of the prompt "What is the capital of California: " takes about as much time as the generation of each subsequent token. This is because the <a href="https://github.com/huggingface/text-generation-inference/tree/f59fb8b630844c2ad2cd80e689202de89d45c37e/router#prefill-decode-and-past-key-values"><u>prefill phase</u></a> pre-computes <a href="https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache"><u>some inputs</u></a> of the attention mechanism that remain constant over the lifetime of the generation. This prefill phase efficiently uses the GPU‚Äôs parallel compute because these inputs can be computed independently of each other.</p></li><li><p>LLM inference is <a href="https://en.wikipedia.org/wiki/Memory_bandwidth"><u>memory-IO bound</u></a>, not compute bound. In other words, it currently takes more time to load 1MB of data to the GPU‚Äôs compute cores than it does for those compute cores to perform LLM computations on 1MB of data. This means that LLM inference throughput <i>is largely determined by how large a batch you can fit into high-bandwidth GPU memory</i>. See <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf"><u>this page</u></a> in the NVIDIA docs for more details.</p></li><li><p>The amount of GPU memory consumed scales with the base model size + the length of the token sequence. In <a href="https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model"><i><u>Numbers every LLM developer should know</u></i></a>, it‚Äôs estimated that a 13B parameter model consumes nearly 1MB of state for each token in a sequence. On a higher-end A100 GPU with 40GB RAM, back-of-the-envelope math suggests that since 14 GB are left after storing the 26GB of model parameters, ~14k tokens can be held in memory at once. This may seem high but is actually quite limiting; if we limit our sequence lengths to 512, we can process at most ~28 sequences in a batch. The problem is worse for higher sequence lengths; a sequence length of 2048 means our batch size is limited to 7 sequences. Note that this is an upper bound since it doesn‚Äôt leave room for storing intermediate computations.</p></li></ol><p>What this all means is that there is substantial ‚Äúroom on the table‚Äù so to speak if you can optimize memory usage. This is why approaches such as model quantization strategies such as <a href="https://github.com/PanQiWei/AutoGPTQ"><u>AutoGPTQ</u></a> are potentially so powerful; if you could halve the memory usage by moving from 16-bit to 8-bit representations, you could double the space available for larger batch sizes. However, not all strategies require modifications to the model weights. For example, <a href="https://github.com/HazyResearch/flash-attention"><u>FlashAttention</u></a> found significant throughput improvements by reorganizing the attention computation to require less memory-IO.</p><p>Continuous batching is another memory optimization technique which does not require modification of the model. We next explain how naive batching works (and is inefficient), and how continuous batching increases the memory-efficiency of LLM generation.</p><hr><h2>LLM batching explained</h2><p>GPUs are massively-parallel compute architectures, with compute rates (measured in floating-point operations per second, or flops) in the teraflop (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf"><u>A100</u></a>) or even petaflop (<a href="https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet"><u>H100</u></a>) range. Despite these staggering amounts of compute, LLMs struggle to achieve saturation because so much of the chip‚Äôs memory bandwidth is spent loading model parameters.</p><p>Batching is one way to improve the situation; instead of loading new model parameters each time you have an input sequence, you can load the model parameters once and then use them to process many input sequences. This more efficiently uses the chip‚Äôs memory bandwidth, leading to higher compute utilization, higher throughput, and cheaper LLM inference.</p><h3>Naive batching / static batching</h3><p>We call this traditional approach to batching <i>static batching</i>, because the size of the batch remains constant until the inference is complete. Here‚Äôs an illustration of static batching in context of LLM inference:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png" alt="cb 02 diagram-static-batching"></p></div><p><span>Completing four sequences using static batching. On the first iteration (left), each sequence generates one token (blue) from the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes because each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two iterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation (in this example, sequence 2 after six iterations).</span></p></div><p>Unlike traditional deep learning models, batching for LLMs can be tricky due to the iterative nature of their inference. Intuitively, this is because requests can "finish" earlier in a batch, but it is tricky to release their resources and add new requests to the batch that may be at different completion states. This means that as the GPU is underutilized as generation lengths of different sequences in a batch differ from the largest generation length of the batch. In the figure on the right above, this is illustrated by the white squares after end-of-sequence tokens for sequences 1, 3, and 4.</p><p>How often does static batching under-utilize the GPU? It depends on the generation lengths of sequences in a batch. For example, one could use LLM inference to emit a single token as a classification task (there are better ways to do this but let‚Äôs use this as an example). In this case, every output sequence is the same size (1 token). If the input sequences are also the same size (say, 512 tokens), then each static batch will achieve the best possible GPU utilization.</p><p>On the other hand, a LLM-powered chatbot service cannot assume fixed-length input sequences, nor assume fixed-length output sequences. Proprietary models offer maximum context lengths in excess of 8K tokens at the time of writing. With static batching, variance in generation output could cause massive underutilization of GPUs. It‚Äôs no wonder OpenAI CEO Sam Altman described the compute costs as <a href="https://twitter.com/sama/status/1599669571795185665?lang=en">eye-watering</a>.</p><p>Without restrictive assumptions on user input and model output, unoptimized production-grade LLM systems simply can‚Äôt serve traffic without underutilizing GPUs and incurring unnecessarily high costs. We need to optimize how we serve LLMs for their power to be broadly accessible.</p><h3>Continuous batching</h3><p>The industry recognized the inefficiency and came up with a better approach. <a href="https://www.usenix.org/conference/osdi22/presentation/yu"><i><u>Orca: A Distributed Serving System for Transformer-Based Generative Models</u></i></a> is a paper presented in OSDI ‚Äò22 which is the first to our knowledge to tackle this problem. Instead of waiting until every sequence in a batch has completed generation, Orca implements <i>iteration-level</i> scheduling where the batch size is determined per iteration. The result is that once a sequence in a batch has completed generation, a new sequence can be inserted in its place, yielding higher GPU utilization than static batching.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png" alt="cb 03 diagram-continuous-batching"></p></div><p><span>Completing seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the batch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e. sequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete before starting a new one.</span></p></div><p>Reality is a bit more complicated than this simplified model: since the prefill phase takes compute and has a different computational pattern than generation, it cannot be easily batched with the generation of tokens. Continuous batching frameworks currently manage this via hyperparameter: <a href="https://github.com/huggingface/text-generation-inference/blob/f59fb8b630844c2ad2cd80e689202de89d45c37e/launcher/src/main.rs#L124-L135"><u>waiting_served_ratio</u></a>, or the ratio of requests waiting for prefill to those waiting end-of-sequence tokens.</p><p>Speaking of frameworks, Hugging Face has productionized continuous batching in their Rust- and Python-based <a href="https://github.com/huggingface/text-generation-inference/tree/main"><u>text-generation-inference LLM inference server</u></a>. We use their implementation to understand the performance characteristics of continuous batching in our benchmarks below.</p><p><b><i>Note</i></b><i>: Continuous batching, dynamic batching, and iteration-level scheduling are all close enough in meaning that any one of them can be used to describe the batching algorithm. We chose to use continuous batching. Dynamic batching is fitting but can be confused with request-level batching, where an LLM inference server uses a static batch whose size is chosen when the current batch has completely finished generation. We feel that iteration-level scheduling is descriptive of the scheduling mechanism but not the process as a whole.</i></p><hr><h2>PagedAttention and vLLM</h2><p>For this blog post, we want to showcase the differences between static batching and continuous batching. It turns out that continuous batching can unlock memory optimizations that are not possible with static batching by improving upon Orca‚Äôs design.</p><p>PagedAttention is a new attention mechanism implemented in <a href="https://vllm.ai/"><u>vLLM</u></a> (<a href="https://github.com/vllm-project/vllm/tree/main#easy-fast-and-cheap-llm-serving-for-everyone"><u>GitHub</u></a>). It takes inspiration from traditional OS concepts such as <a href="https://en.wikipedia.org/wiki/Memory_paging"><u>paging</u></a> and <a href="https://en.wikipedia.org/wiki/Virtual_memory"><u>virtual memory</u></a>. They allow the KV cache (what is computed in the ‚Äúprefill‚Äù phase, discussed above) to be non-contiguous by allocating memory in fixed-size ‚Äúpages‚Äù, or blocks. The attention mechanism can then be rewritten to operate on block-aligned inputs, allowing attention to be performed on non-contiguous memory ranges.</p><p>This means that buffer allocation can happen just-in-time instead of ahead-of-time: when starting a new generation, the framework does not need to allocate a contiguous buffer of size maximum_context_length. Each iteration, the scheduler can decide if it needs more room for a particular generation, and allocate on the fly without any degradation to PagedAttention‚Äôs performance. This doesn‚Äôt guarantee perfect utilization of memory (<a href="https://vllm.ai/"><u>their blog</u></a> says the wastage is now limited to under 4%, only in the last block), but it significantly improves upon wastage from ahead-of-time allocation schemes used widely by the industry today.</p><p>Altogether, PagedAttention + vLLM enable massive memory savings as most sequences will not consume the entire context window. These memory savings translate directly into a higher batch size, which means higher throughput and cheaper serving. We include vLLM in our benchmarks below.</p><hr><h2>Benchmarking setup</h2><p>We‚Äôll discuss our experimental setup then dive into the results of our benchmarks.</p><h3>Experiments</h3><p>Our goal is to see how continuous batching performs versus static batching on a simulated real-world live-inference workload. Fundamentally, we care about cost. We break this down into throughput and latency since cost is directly downstream of how efficiently you can serve at a given latency.</p><table><tbody><tr><td><p><b>Benchmark goal</b></p></td><td><p><b>Measurement</b></p></td></tr><tr><td><p>Measure throughput</p></td><td><p>Time-to-process a queue of 1000 requests, each with 512 input tokens and generation length sampled from an exponential distribution.</p></td></tr><tr><td><p>Measure latency</p></td><td><p>Request latencies for 100 requests, with varying input lengths, output lengths, and arrival times at a fixed average rate.</p></td></tr></tbody></table><p>We‚Äôll discuss the datasets and other details of the experiments in their respective results section.</p><h3>Hardware/model</h3><p><br>We benchmark throughput and latency on a single <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf"><u>NVIDIA A100 GPU</u></a> provided by <a href="https://www.anyscale.com/"><u>Anyscale</u></a>. Our A100 has 40GB of GPU RAM. We selected <a href="https://huggingface.co/facebook/opt-13b"><u>Meta‚Äôs OPT-13B</u></a> model because each framework under test had a readily-available integration with this model. We selected the 13B variant because it fits into our GPU without requiring tensor parallelism, yet is still large enough to present memory efficiency challenges. We opt not to use tensor parallelism, where each transformer block is split over multiple GPUs, to keep our experiments simple, although both static batching and continuous batching work with tensor parallelism.</p><h3>Frameworks</h3><div><p><img src="https://images.ctfassets.net/xjan103pcp94/3K202bzJfK6ZlhmJpgwZ9q/7ccf0aacaf298e24bf824ee0ac429c47/06_frameworks_aspect_ratio.png" alt="cb 06 frameworks"></p></div><p>We test two static batching frameworks and three continuous batching frameworks. Our static batching frameworks are:</p><ul><li><p><a href="https://huggingface.co/docs/transformers/pipeline_tutorial"><b><u>Hugging Face‚Äôs Pipelines</u></b></a><b>.</b> This is the simplest inference solution. It provides static batching with an easy-to-use API that works with any model and supports more tasks than simple text-generation. We use this as our baseline.&nbsp;</p></li><li><p><a href="https://github.com/NVIDIA/FasterTransformer"><b><u>NVIDIA‚Äôs FasterTransformer</u></b></a><b>.</b> This is a library which provides optimized implementations of various transformer models. It currently only provides static batching (the <a href="https://github.com/triton-inference-server/server"><u>Triton inference server</u></a> provides request-level dynamic batching, but not continuous batching yet). This provides us with an idea of how far an extremely optimized implementation of our model can get us with static batching ‚Äì it provides a more competitive baseline than the relatively unoptimized OPT-13B implementation <a href="https://huggingface.co/facebook/opt-13b"><u>available on Hugging Face Hub</u></a>.</p></li></ul><p>Our continuous batching frameworks are:</p><ul><li><p><a href="https://github.com/huggingface/text-generation-inference"><b><u>Hugging Face‚Äôs text-generation-inference</u></b></a><b>.</b> This is the inference server Hugging Face uses to power their LLM live-inference APIs. It <a href="https://github.com/huggingface/text-generation-inference/tree/main/router#continuous-batching"><u>implements</u></a> continuous batching.</p></li><li><p><u><b>Continuous batching on Ray Serve</b></u><b>.</b> <a href="https://docs.ray.io/en/latest/serve/index.html"><u>Ray Serve</u></a> leverages Ray‚Äôs serverless capabilities to provide seamless autoscaling, high-availability, and support for complex DAGs. We wanted to understand how continuous batching works, so we re-implemented text-generation-inference‚Äôs core continuous batching logic in pure-Python on Ray Serve. As you will see in our results, our implementation achieves the same performance as text-generation-inference, which validates our understanding.</p></li><li><p><a href="https://vllm.ai/"><b><u>vLLM</u></b></a><b>.</b> This is an open-source project recently released by folks at UC Berkeley (<a href="https://github.com/vllm-project/vllm"><u>GitHub</u></a>). It builds upon Orca‚Äôs continuous batching design by taking full control of dynamic memory allocations, allowing it to significantly reduce different forms of GPU memory fragmentation. We test this framework because it shows the impact of further optimizations made possible by iteration-level scheduling and continuous batching.</p></li></ul><h2>Benchmarking results: Throughput</h2><p>Based on our understanding of static batching, we expect continuous batching to perform significantly better when there is higher <i>variance</i> in sequence lengths in each batch. To show this, we run our throughput benchmark four times for each framework, each time on a dataset with higher variance in sequence lengths.</p><p>To do this, we create a dataset containing 1000 sequences each with 512 input tokens. We configure our model to always emit a per-sequence generation length by ignoring the end-of-sequence token and configuring max_tokens. We then generate 1000 generation lengths, one for each request, sampled from an<a href="https://en.wikipedia.org/wiki/Exponential_distribution"><u> exponential distribution</u></a> with mean=128 tokens. We use an exponential distribution as it is a good approximation of the generation lengths that one may encounter while serving an application like ChatGPT. To vary the variance of each run, we select only samples from the exponential distribution that are less than or equal to 32, 128, 512, and 1536. The total output sequence length is then, at most, 512+32=544, 512+128=640, 512+512=1024, and 512+1536=2048 (the maximum sequence length of our model).</p><p>We then use a simple asyncio Python benchmarking script to submit HTTP requests to our model server. The benchmarking script submits all requests in burst fashion, so that the compute is saturated.</p><p>The results are as follows:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/1Os82uuLDUkqP90Nlhp3vh/1e783aff1edb97cd25b5139d26083c1c/cb_07_throughput_table.png" alt="cb 07 throughput table"></p></div><p><span>Throughput in tokens per second of each framework as variance in sequence length increases.</span></p></div><p>As expected, the static batchers and naive continuous batchers perform approximately identically for lower-variance generation lengths. However as the variance increases, naive static batching‚Äôs performance plummets to 81 token/s. FasterTransformers improves upon naive static batching significantly, nearly keeping up with the naive continuous batchers until generation length limit of 1536. Continuous batching on Ray Serve and text-generation-inference achieves about the same performance, which is what we expect since they use the same batching algorithm.</p><p>What is most impressive here is vLLM. For each dataset, vLLM more than doubles performance compared to naive continuous batching. We have not analyzed what optimization contributes the most to vLLM performance the most, but we suspect vLLM‚Äôs ability to reserve space dynamically instead of ahead-of-time allows vLLM to dramatically increase the batch size.</p><p>We plot these performance results relative to naive static batching:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/46OIG2WmA2j0SBfcG5fdq7/3bcdebf8014730a1a592a18f023cfdcc/cb_08_throughput_graph.png" alt="cb 08 throughput graph"></p></div><p><span>Our throughput benchmark results presented as improvement multiples over naive static batching, log scale.</span></p></div><p>It‚Äôs important to note how impressive even FasterTransformer‚Äôs 4x improvement is; we‚Äôre very interested in benchmarking FasterTransformers plus continuous batching when NVIDIA implements it. However, continuous batching is clearly a significant improvement over static batching even with an optimized model. The performance gap becomes gigantic when you include further memory optimization enabled by continuous batching and iteration-level scheduling as vLLM does.</p><h2>Benchmarking results: Latency</h2><p>Live-inference endpoints often face latency-throughput tradeoffs that must be optimized based on user needs. We benchmark latency on a realistic workload and measure how the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"><u>cumulative distribution function</u></a> of latencies changes with each framework.</p><p>Similar to the throughput benchmark, we configure the model to always emit a specified amount of tokens specified per-request. We prepare 100 randomly-generated prompts by sampling lengths from a <a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution"><u>uniform distribution</u></a> between 1 token and 512 tokens. We sample 100 output lengths from a capped exponential distribution with mean=128 and maximum size of 1536. These numbers were chosen because they are reasonably realistic and allow the generation to use up the full context-length of our model (512+1536=2048).</p><p>Instead of submitting all requests at the same time as done in the throughput benchmark, we delay each request by a predetermined number of seconds. We sample a <a href="https://en.wikipedia.org/wiki/Poisson_distribution"><u>Poisson distribution</u></a> to determine how long each request waits after the previously submitted request. The Poisson distribution is parameterized by Œª, the expected rate, which in our case is how many queries per second (QPS) hit our model endpoint. We measure latencies at both QPS=1 and QPS=4 to see how the latency distribution changes as load changes.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/4ElanYNZRv3sUBL0459zWV/ce78b3daf7e05f1bb84dad61906f1663/cb_09_latency_table.png" alt="cb 09 latency table"></p></div><p><span>Median generation request latency for each framework, under average load of 1 QPS and 4 QPS. Continuous batching systems improve median latency.</span></p></div><p>We see that while improving throughput, continuous batching systems also <i>improve</i> median latency. This is because continuous batching systems allow for new requests to be added to an existing batch if there is room, each iteration. But how about other percentiles? In fact, we find that they improve latency across all percentiles:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/6zynLiX4AJVO23tRfQ1rnV/763589eb4a6418157f21a51e6e36abaf/cb_10_latency_cdf_qps_1.png" alt="cb 10 latency cdf qps=1"></p></div><p><span>Cumulative distribution function of generation request latencies for each framework with QPS=1. Static batchers and continuous batchers have distinct curve shapes caused by the presence of iteration-level batch scheduling in continuous batchers. All continuous batchers perform approximately equally under this load; FasterTransformers performs noticeably better than static batching on a naive model implementation.</span></p></div><p>The reason why continuous batching improves latency at all percentiles is the same as why it improves latency at p50: new requests can be added regardless of how far into generation other sequences in the batch are. However, like static batching, continuous batching is still limited by how much space is available on the GPU. As your serving system becomes saturated with requests, meaning a higher on-average batch size, there are less opportunities to inject new requests immediately when they are received. We can see this as we increase the average QPS to 4:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/2az2DSpj3IujUOOu2i5WPp/5f7457205acae98fcd7fb3170e93b773/cb_11_latency_cdf_qps_4.png" alt="cb 11 latency cdf qps=4"></p></div><p><span>Cumulative distribution function of generation request latencies for each framework with QPS=4. Compared to QPS=1, FasterTransformer‚Äôs distribution of latencies becomes more similar to static batching on a naive model. Both Ray Serve and text-generation-inference‚Äôs continuous batching implementations perform similarly, but noticeably worse than vLLM.</span></p></div><p>We observe that FasterTransformer becomes more similar to naive static batching, and that both text-generation-inference and Ray Serve‚Äôs implementation of continuous batching are on their way to look like FasterTransformer‚Äôs curve with QPS=1. That is, as the systems become saturated there are less opportunities to inject new requests immediately, so request latency goes up. This lines up with the vLLM curve ‚Äì it remains mostly unchanged between QPS=1 and QPS=4. This is because due to its advanced memory optimizations, it has a higher maximum batch size.</p><p>Anecdotally, we observe that vLLM becomes saturated around QPS=8 with a throughput near 1900 token/s. To compare these numbers apples-to-apples to the other serving systems requires more experimentation; however we have shown that continuous batching significantly improves over static batching by 1) reducing latency by injecting new requests immediately when possible, and 2) enable advanced memory optimizations (in vLLM‚Äôs case) that increase the QPS that the serving system can handle before becoming saturated.</p><h2>Conclusion</h2><p>LLMs present some amazing capabilities, and we believe their impact is still mostly undiscovered. We have shared how a new serving technique, continuous batching, works and how it outperforms static batching. It improves throughput by wasting fewer opportunities to schedule new requests, and improves latency by being capable of immediately injecting new requests into the compute stream. We are excited to see what people can do with continuous batching, and where the industry goes from here.</p><h2>Try out continuous batching for yourself</h2><p>We have a <a href="https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py"><u>vLLM + Ray Serve example</u></a> that allows you to try out continuous batching. We are integrating continuous batching systems into <a href="https://aviary.anyscale.com/"><u>Aviary</u></a>, a webapp <a href="https://www.anyscale.com/blog/announcing-aviary-open-source-multi-llm-serving-solution"><u>that allows you to compare the outputs of different LLMs in parallel</u></a>, and will release it within the week.</p><p><i>Acknowledgements. We‚Äôd like to thank the following people for assisting in benchmarking and/or reviewing our results. </i>Anyscale<i>: Stephanie Wang, Antoni Baum, Edward Oakes, and Amog Kamsetty; </i>UC Berkeley<i>: Zhuohan Li and Woosuk Kwon.</i></p><h2>Get involved with Ray</h2><p>The <a href="https://github.com/anyscale/llm-continuous-batching-benchmarks"><u>code used for the experiments in the blog post is here</u></a>. To connect with the Ray community, join <a href="https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"><u>the Ray Slack</u></a> or ask questions <a href="https://discuss.ray.io/"><u>on the Discuss forum</u></a>. If you are interested in hosting LLMs, check out <a href="https://www.anyscale.com/platform"><u>our managed Ray offering</u></a>. If you are interested in learning more about Ray, see <a href="http://ray.io/">ray.io</a> and <a href="http://docs.ray.io/">docs.ray.io</a>.</p><p>See our earlier <a href="https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"><u>blog series on solving Generative AI infrastructure</u></a> and using <a href="https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"><u>LangChain with Ray</u></a>.</p><p><b>Ray Summit 2023</b>: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, <a href="https://raysummit.anyscale.com/"><u>join Ray Summit on September 18-20th</u></a>! We have a set of great keynote speakers including <a href="http://joschu.net/"><u>John Schulman</u></a> from OpenAI and <a href="https://aidangomez.ca/"><u>Aidan Gomez</u></a> from <a href="https://cohere.com/"><u>Cohere</u></a>, community and tech talks about Ray <a href="https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"><u>as well as practical training focused on LLMs</u></a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New responsibilities (128 pts)]]></title>
            <link>https://www.hadess.net/2023/08/new-responsibilities.html</link>
            <guid>37131263</guid>
            <pubDate>Tue, 15 Aug 2023 07:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hadess.net/2023/08/new-responsibilities.html">https://www.hadess.net/2023/08/new-responsibilities.html</a>, See on <a href="https://news.ycombinator.com/item?id=37131263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-727042356482467106" itemprop="description articleBody">
<p><span>, my management chain has made the decision to stop all 
upstream and downstream work on desktop Bluetooth, multimedia 
applications (namely totem, rhythmbox and sound-juicer) and 
libfprint/fprintd. The rest of my upstream and downstream work will be 
reassigned depending on Red Hat's own priorities (see below), as I am 
transferred to another team that deals with one of a list of Red Hat‚Äôs 
priority projects.</span></p><p><span>I'm
 very disappointed, because those particular projects were already 
starved for resources: I spent less than 10% of my work time on them in 
the past year, with other projects and responsibilities taking most of 
my time.</span></p><p><span>This means that, in the medium-term at least, all those GNOME projects will go without a maintainer, reviewer, or triager:</span></p><p><span>- gnome-bluetooth (including Settings panel and gnome-shell integration)</span></p><p><span>- totem, totem-pl-parser, gom</span></p><p><span>- libgnome-volume-control</span></p><p><span>- libgudev</span></p><p><span>- geocode-glib</span></p><p><span>- gvfs AFC backend</span></p><p><span>Those freedesktop projects will be archived until further notice:</span></p><p><span>- power-profiles-daemon</span></p><p><span>- switcheroo-control</span></p><p><span>- iio-sensor-proxy</span></p><p><span>- low-memory-monitor</span></p><p><span>I will not be available for reviewing libfprint/fprintd, upower, grilo/grilo-plugins, gnome-desktop thumbnailer sandboxing patches, or any work related to XDG specifications.</span></p><p><span>Kernel
 work, reviews and maintenance, including recent work on SteelSeries 
headset and Logitech devices kernel drivers, USB revoke for Flatpak 
Portal support, or core USB is suspended until further notice.</span></p><p><span>All my <a href="https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/WRHVGQBKKFU74CBO3CHIJC3Q5VEKH2AV/">Fedora </a></span><span><a href="https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/WRHVGQBKKFU74CBO3CHIJC3Q5VEKH2AV/">packages 
were orphaned</a> about a month and a half ago, it's likely that there are 
still some that are orphaned, if there are takers. RHEL packages were 
unassigned about 3 weeks ago, they've been reassigned 
since then, so I cannot point to the new maintainer(s).</span></p><p><span>If
 you are a partner, or a customer, I would recommend that you get in 
touch with your Red Hat contacts to figure out what the plan is going 
forward for the projects you might be involved with.</span></p><p><span>If
 you are a colleague that will take on all or part of the 90% of the 
work that's not being stopped, or a community member that was relying on
 my work to further advance your own projects, get in touch, I'll do my 
best to accommodate your queries, time permitting.</span></p><p><span>I'll try to make sure to update this post, or create a new one if and when any of the above changes.<br></span></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Features of Project Loom incorporated in JDK 21 (211 pts)]]></title>
            <link>https://jdk.java.net/loom/</link>
            <guid>37130138</guid>
            <pubDate>Tue, 15 Aug 2023 04:21:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jdk.java.net/loom/">https://jdk.java.net/loom/</a>, See on <a href="https://news.ycombinator.com/item?id=37130138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="footer"><p><a href="https://oracle.com/"><img alt="Oracle logo" src="https://jdk.java.net/images/oracle.png"></a></p><p> ¬© 2023 Oracle Corporation and/or its affiliates </p><div><p><a href="https://jdk.java.net/tou">Terms of Use</a>
          ¬∑ <a href="https://www.oracle.com/legal/privacy/">Privacy</a>
          ¬∑ <a href="https://openjdk.org/legal/openjdk-trademark-notice.html">Trademarks</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: t.co is adding a five-second delay to some domains (495 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37130060</link>
            <guid>37130060</guid>
            <pubDate>Tue, 15 Aug 2023 04:09:03 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37130060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37130129"><td></td></tr>
                <tr id="37130240"><td></td></tr>
            <tr id="37130260"><td></td></tr>
            <tr id="37130194"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130194" href="https://news.ycombinator.com/vote?id=37130194&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Agree/confirmed - just recorded a number of different nytimes urls that pass through t.co, all 4.7s+. various cnbc and google articles through t.co were ~130-200ms response time from t.co specifically (not total redirect-&gt;page load).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130246"><td></td></tr>
            <tr id="37130186"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130186" href="https://news.ycombinator.com/vote?id=37130186&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I almost didn't believe OP, because it's so comically inept and petty. But, I can also confirm in some private testing there is a deliberate delay.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130327"><td></td></tr>
                        <tr id="37130143"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130143" href="https://news.ycombinator.com/vote?id=37130143&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I think that HN itself also shadow flags submissions from a list of domains it doesn't like.<p>Try submitting a URL from the following domains, and it will be automatically flagged (but you can't see its flagged unless you log out):</p><pre><code>  - archive.is
  - watcher.guru
  - stacker.news
  - zerohedge.com
  - freebeacon.com
  - thefederalist.com
  - breitbart.com</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130147"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130147" href="https://news.ycombinator.com/vote?id=37130147&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Well, yes, many sites are banned on HN. Others are penalized (see e.g. <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=by%3Adang%20%22major%20media%22&amp;sort=byDate&amp;type=comment" rel="nofollow noreferrer">https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;que...</a>). None of this is secret, though we don't publish the lists themselves.<p>Edit: about 67k sites are banned on HN. Here's a random selection of 10 of them:</p><pre><code>  vodlockertv.com
  biggboss.org
  infoocode.com
  newyorkpersonalinjuryattorneyblog.com
  moringajuice.wordpress.com
  surrogacymumbai.com
  maximizedlivingdrlabrecque.com
  radio.com
  gossipcare.com
  tecteem.com</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130185"><td></td></tr>
                <tr id="37130234"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130234" href="https://news.ycombinator.com/vote?id=37130234&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>The problem is that if you publish the lists it leads to more abuses. For example if spammers find out which sites are banned then they just post other ones.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130270"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130270" href="https://news.ycombinator.com/vote?id=37130270&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>&gt; For example if spammers find out which sites are banned then they just post other ones.<p>I don't think that makes sense. The supposed spammers can just try looking up whether their submissions show up or not when not logged in.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130279"><td></td></tr>
                              <tr id="37130261"><td></td></tr>
                <tr id="37130315"><td></td></tr>
                        <tr id="37130155"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130155" href="https://news.ycombinator.com/vote?id=37130155&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>The difference is that HN is explicitly heavily moderated while Twitter pretends to be an equitable free speech platform.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130144"><td></td></tr>
                <tr id="37130157"><td></td></tr>
                <tr id="37130177"><td></td></tr>
                <tr id="37130220"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130220" href="https://news.ycombinator.com/vote?id=37130220&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>And how was  the decision made to ban Federalist, but not say Guardian or The Daily Beast? Do you have any process in place to ensure that your political biases don't influence the list, or you don't care about that?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130299"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130299" href="https://news.ycombinator.com/vote?id=37130299&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Hey, man, if you want to go read those sites go for it. It's a free country.<p>This is a moderated site targeted at a specific community. It's under no obligation to be politically balanced. It's certainly under no obligation to promote right-wing propaganda and hate.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130325"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37130325" href="https://news.ycombinator.com/vote?id=37130325&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>&gt; It's under no obligation to be politically balanced.<p>And obviously, I'm under no obligation to not voice my concern about that.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130255"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130255" href="https://news.ycombinator.com/vote?id=37130255&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I'm guessing it's reactive, and Federalist links tended to be garbage often enough to convince someone they should hit the ban button, whereas the others didn't rise up with trash often enough to matter?</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37130170"><td></td></tr>
                        <tr id="37130217"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130217" href="https://news.ycombinator.com/vote?id=37130217&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>the wise man bowed his head solemnly and spoke: "theres actually zero difference between good &amp; bad things." -- @dril</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130078" href="https://news.ycombinator.com/vote?id=37130078&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Remember when people were excoriating Google AMP for encouraging walled gardens? If true, this seems in so much worse faith than that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130131"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130131" href="https://news.ycombinator.com/vote?id=37130131&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Not worse. They are both as evil as it gets. Typical: take public resource and use it for an exclusive  profit.<p>What happened to net neutrality? Could it applied for this case?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130099"><td></td></tr>
                <tr id="37130191"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130191" href="https://news.ycombinator.com/vote?id=37130191&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Enshitification is different. It‚Äôs when companies destroy a product with hundreds of changes that prioritise
internal politics above what end users want.<p>This is something else - just the ego of one rich guy petulantly satisfying his inner demons.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                            <tr id="37130102"><td></td></tr>
                <tr id="37130163"><td></td></tr>
            <tr id="37130151"><td></td></tr>
                  <tr id="37130179"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130179" href="https://news.ycombinator.com/vote?id=37130179&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>If true and intentional, then this is a strong move by Musk against his ideological opponents. Hard to believe he has the cognizance to recognize them as such but maybe he purged more of the 3-letter agency folks from X than it seemed.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130111"><td></td></tr>
            <tr id="37130073"><td></td></tr>
                <tr id="37130101"><td></td></tr>
                <tr id="37130117"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130117" href="https://news.ycombinator.com/vote?id=37130117&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>A good test might include a bunch of domains. And checking the timing on each. Could we demonstrate the delay is on t.co and not on NYT?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130180"><td></td></tr>
                  <tr id="37130208"><td></td></tr>
                  <tr id="37130109"><td></td></tr>
            <tr id="37130226"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130226" href="https://news.ycombinator.com/vote?id=37130226&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>they already told you they did tested it and you don't believe them.<p>what else could they say that would make you believe them?</p><p>you might as well just test it yourself like i did with time wget. it's not like you're going to believe anything anyone writes.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130137"><td></td></tr>
            <tr id="37130119"><td></td></tr>
                <tr id="37130192"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130192" href="https://news.ycombinator.com/vote?id=37130192&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>It is probably not illegal in America. Would it be illegal in Europe? Because (at least w/r/t Threads) it is an anti-competitive practice?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130161"><td></td></tr>
                  <tr id="37130097"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130097" href="https://news.ycombinator.com/vote?id=37130097&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>i mean, you can stop visiting the site, no? just leave, bro. it's not that hard. there are other means to connect to people.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130107"><td></td></tr>
            <tr id="37130127"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130127" href="https://news.ycombinator.com/vote?id=37130127&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>You're right! Which is why making Twitter's product <i>worse</i> when there are active competitors taking big bites out of their business seems... dumb?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130245"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130245" href="https://news.ycombinator.com/vote?id=37130245&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>yep, it also seems to me that the helmsman of that site is dumb. good thing i left that site many years ago.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130165"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130165" href="https://news.ycombinator.com/vote?id=37130165&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Unfortunately not. All of my local government agencies - Police, Fire, DOT, Weather Service, Emergency Management updates, etc. are exclusively on twitter - they frequently post things there they don't even post to their own websites.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130268"><td></td></tr>
                        </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unpublished preface to Orwell‚Äôs Animal Farm (264 pts)]]></title>
            <link>https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/</link>
            <guid>37129768</guid>
            <pubDate>Tue, 15 Aug 2023 03:18:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/">https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/</a>, See on <a href="https://news.ycombinator.com/item?id=37129768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-swiftype-name="body">
			
<p><a href="http://www.bbc.co.uk/history/historic_figures/orwell_george.shtml">George Orwell</a>‚Äòs novella <em><a href="http://www.george-orwell.org/Animal_Farm/0.html">Animal Farm</a></em> (1945) was a political fable. The cleverly portrayed animals who chase off the farmer and try to run the farm as a utopia slowly begin to replicate all the attitudes and practices against which they had rebelled. The story, summarized <a href="https://interestingliterature.com/2020/05/a-summary-and-analysis-of-george-orwells-animal-farm/">here,</a> satirizes the Soviet Union‚Äôs transition from revolution to totalitarianism under Joseph Stalin (1878‚Äì1953). In fact, the animal characters and incidents are often allusions to <a href="https://www.enotes.com/homework-help/what-some-examples-allusions-book-animal-farm-349433">historical Soviet figures and events.</a></p>







<figure><div>
<p><iframe title="Animal Farm Video Summary" width="500" height="281" src="https://www.youtube.com/embed/BFP1IMyKyy4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div></figure>







<p>His Preface, <a href="https://orwell.ru/library/novels/Animal_Farm/english/efp_go">‚ÄúThe Freedom of the Press,‚Äù</a> was omitted from the first edition of the book, then disappeared, and <a href="https://www.bl.uk/collection-items/orwells-proposed-introduction-to-animal-farm">was not rediscovered until 1971.</a> From it, we learn that Orwell had considerable difficulty getting his fable published. That wasn‚Äôt principally because of wartime issues. There was a shortage of books and his was highly readable. Rather, British intellectuals of the day did not wish to hear any criticism of Stalin or allusions to his atrocities:</p>



<p>Obviously it is not desirable that a government department should have any power of censorship (except security censorship, which no one objects to in war time) over books which are not officially sponsored. But the chief danger to freedom of thought and speech at this moment is not the direct interference of the MOI or any official body. If publishers and editors exert themselves to keep certain topics out of print, it is not because they are frightened of prosecution but because they are frightened of public opinion. In this country intellectual cowardice is the worst enemy a writer or journalist has to face, and that fact does not seem to me to have had the discussion it deserves‚Ä¶</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-892x1597.jpg" alt="" width="276" height="494" srcset="https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-892x1597.jpg 892w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-551x987.jpg 551w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-768x1375.jpg 768w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-858x1536.jpg 858w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-1144x2048.jpg 1144w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1.jpg 1430w" sizes="(max-width: 276px) 100vw, 276px"></figure></div>


<p>At this moment what is demanded by the prevailing orthodoxy is an uncritical admiration of Soviet Russia. Everyone knows this, nearly everyone acts on it. Any serious criticism of the Soviet r√©gime, any disclosure of facts which the Soviet government would prefer to keep hidden, is next door to unprintable. And this nation-wide conspiracy to flatter our ally takes place, curiously enough, against a background of genuine intellectual tolerance. For though you arc not allowed to criticise the Soviet government, at least you are reasonably free to criticise our own. Hardly anyone will print an attack on Stalin, but it is quite safe to attack Churchill, at any rate in books and periodicals. And throughout five years of war, during two or three of which we were fighting for national survival, countless books, pamphlets and articles advocating a compromise peace have been published without interference. More, they have been published without exciting much disapproval. So long as the prestige of the USSR is not involved, the principle of free speech has been reasonably well upheld. There are other forbidden topics, and I shall mention some of them presently, but the prevailing attitude towards the USSR is much the most serious symptom. It is, as it were, spontaneous, and is not due to the action of any pressure group. </p>



<p>Orwell, it should be said, was very much a man of the Left. But he was not a totalitarian. That combination perhaps enabled him to publish some of the most broadly appealing  popular-level dissections of the evils of totalitarian rule in English.</p>



<p>For example, he offers us a significant insight in the passage above. The censorship he had to address was not a conspiracy or even a campaign; it was spontaneous. Every right-thinking intellectual somehow <em>knew</em> that a candid assessment of Soviet rule was, well, just <em>not the done thing!‚Ä¶</em></p>



<p>Why not? Well, gentle reader, if you have ever encountered such an environment, you will know ‚Äî or suspect anyway ‚Äî that most of the people who know for sure which political views need censoring could not ably defend their opinion. Their defense is, precisely, groupthink. They don‚Äôt need to think much about it individually. And they don‚Äôt. In fact, if you challenge them on their censorship, they may act aggrieved, as if they were the victims of a calculated personal injury. It‚Äôs doubtless all the more tiresome if, as Orwell found, the groupthinkers are held up as the leading intellectuals of the day:</p>



<p>But now to come back to this book of mine. The reaction towards it of most English intellectuals will be quite simple: ‚ÄòIt oughtn‚Äôt to have been published.‚Äô Naturally, those reviewers who understand the art of denigration will not attack it on political grounds but on literary ones. They will say that it is a dull, silly book and a disgraceful waste of paper. This may well be true, but it is obviously not [th]e whole of the story. One does not say that a book ‚Äòought not to have been published‚Äô merely because it is a bad book. After all, acres of rubbish are printed daily and no one bothers. The English intelligentsia, or most of them, will object to this book because it traduces their Leader and (as they see it) does harm to the cause of progress. If it did [th]e opposite they would have nothing to say against it, even if its literary faults were ten times as glaring as they are. The success of, for instance, the Left Book Club over a period of four or five years shows how willing they are to tolerate both scurrility and slipshod writing, provided that it tells them what they want to hear.</p>



<p>And he ends his Preface on a high note,</p>



<p>I know that the English intelligentsia have plenty of reason for their timidity and dishonesty, indeed I know by heart the arguments by which they justify themselves. But at least let us have no more nonsense about defending liberty against Fascism. If liberty means anything at all it means the right to tell people what they do not want to hear.</p>



<p>Orwell would doubtless be pleased that millions of people worldwide have offered a much more positive assessment of <em>Animal Farm.</em> Many of us might also find key points of comparison between his situation and the shrill calls for censorship that we hear so often today.</p>







<figure><div>
<p><iframe loading="lazy" title="Animal Farm trailer" width="500" height="375" src="https://www.youtube.com/embed/LAeKX5n-5IE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div></figure>







<p><em>Note:</em> The better-known <em><a href="https://www.amazon.com/1984-George-Orwell/dp/0451516753">1984</a></em> was not published until 1949, not long before Orwell‚Äôs death from tuberculosis. Also, ‚ÄúGeorge Orwell‚Äù was a pen name; he was known in life as <a href="https://www.cliffsnotes.com/literature/n/1984/george-orwell-biography">Eric Blair.</a></p>



<p><em>You may also wish to read:</em> In Big Tech World: the journalist as <a href="https://mindmatters.ai/2021/02/in-big-tech-world-the-journalist-as-censor-hit-man-and-snitch/">censor, hit man, and snitch.</a> Glenn Greenwald looks at a disturbing trend in media toward misrepresentation as well as censorship.</p>
					</div></div>]]></description>
        </item>
    </channel>
</rss>