<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Jun 2025 20:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Spark, An advanced 3D Gaussian Splatting renderer for Three.js (139 pts)]]></title>
            <link>https://sparkjs.dev/</link>
            <guid>44249565</guid>
            <pubDate>Wed, 11 Jun 2025 17:02:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparkjs.dev/">https://sparkjs.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=44249565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  
  




<div>
  <h2><img src="https://sparkjs.dev/assets/images/logo-hero.png"></h2>
  <h2>An advanced 3D Gaussian Splatting renderer for THREE.js</h2>
  <p>
  Integrate in your scene with other meshes and splats, fast rendering on all devices, programmable dynamic splat effects, wide format support (ply, spz, splat, ksplat)
  </p>
  <p><a href="https://sparkjs.dev/docs/">Get started →</a></p>
</div>







  
  




  



                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Beach Boys' Brian Wilson Dies at 82 (300 pts)]]></title>
            <link>https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/</link>
            <guid>44249467</guid>
            <pubDate>Wed, 11 Jun 2025 16:52:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/">https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/</a>, See on <a href="https://news.ycombinator.com/item?id=44249467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><a href="https://pitchfork.com/artists/4651-brian-wilson/">Brian Wilson</a>, the co-founder and primary songwriter of <a href="https://pitchfork.com/artists/310-the-beach-boys/">the Beach Boys</a>,&nbsp;has died, his family <a data-offer-url="https://www.instagram.com/p/DKxHMogupsf/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.instagram.com/p/DKxHMogupsf/&quot;}" href="https://www.instagram.com/p/DKxHMogupsf/" rel="nofollow noopener" target="_blank">announced</a>. While an official cause of death was not disclosed, the beloved musical auteur, who helped pioneer the studio-as-instrument, influencing generations of musicians in pop and beyond, was <a href="https://pitchfork.com/news/the-beach-boys-brian-wilson-living-with-neurocognitive-disorder-family-proposes-conservatorship/" target="_blank">revealed</a> in early 2024 to be living with a neurocognitive disorder akin to dementia. Wilson was 82 years old.</p><p>“We are heartbroken to announce that our beloved father Brian Wilson has passed away,” his family wrote in a statement shared on Wilson’s official <a data-offer-url="https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl&quot;}" href="https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl" rel="nofollow noopener" target="_blank">Facebook</a>. “We are at a loss for words right now. Please respect our privacy at this time as our family is grieving. We realize that we are sharing our grief with the world. Love &amp; Mercy.”</p><p>Born in Inglewood, California, Wilson formed the band—then called the Pendletones—as a teenager with his brothers Dennis and Carl, their cousin Mike Love, and high school friend Al Jardine. Their first song, “Surfin’,” was released by Candix Records, who changed the band’s name to the Beach Boys without the members’ permission. A year later, the band&nbsp;signed with Capitol&nbsp;to&nbsp;release its debut,&nbsp;<em>Surfin’ Safari</em>; the following year, “Surfin’ U.S.A.” became the Beach Boys’ first U.S. Top 10 single. In&nbsp;1963, the band released three albums:&nbsp;<em>Surfin’ U.S.A.</em>, <em>Surfer Girl</em>, and <em>Little Deuce Coupe.</em> By then, Wilson had started his career as a producer for other musicians. He worked with Jan and Dean, the Castellas, Donna Loren, Sharon Marie, and others.</p><p>In 1964,&nbsp;Brian Wilson decided to stop&nbsp;touring with the Beach Boys after&nbsp;experiencing a panic attack due to the band’s heavy schedule. He focused on his production, and in 1965&nbsp;started work on the landmark experimental pop album&nbsp;<a href="https://pitchfork.com/features/article/9870-the-beach-boys-pet-sounds-celebrates-its-50th-anniversary-artists-pay-tribute-to-the-eternal-teenage-symphony/"><em>Pet Sounds</em></a>.&nbsp;Wilson was the mastermind behind the <em>Pet Sounds</em> sessions, working alongside famed studio musicians the&nbsp;Wrecking Crew. At the time of its release, <em>Pet Sounds</em> was considered a relative commercial flop and critical failure. In 2004, it was inducted&nbsp;into the Library of Congress’ National Recording Registry due to its cultural, historical, and aesthetic significance.</p><p>“I wanted to grow musically, so I experimented,” Wilson told the <a data-offer-url="https://hbr.org/2016/12/brian-wilson" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://hbr.org/2016/12/brian-wilson&quot;}" href="https://hbr.org/2016/12/brian-wilson" rel="nofollow noopener" target="_blank"><em>Harvard Business Review</em></a> in 2016. “I wasn’t the type to sit around and be satisfied with an accomplishment, especially not in the studio. And I had ideas coming into my head all the time. Many had to do with using instruments as voices and voices as instruments. I would put sounds together to create something new. Some ideas didn’t work, because they were too difficult to achieve at the time. But most did. And then I immediately moved to the next thing.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Wilson planned a&nbsp;follow-up album called <em><a href="https://pitchfork.com/reviews/albums/16000-the-smile-sessions/">Smile</a>,</em> which he described as a&nbsp;“teenage symphony to God,” but it was scrapped after continual delays. After the album’s cancellation in 1967, Wilson’s role in the band receded. In 1968, he entered a psychiatric hospital for treatment. In the years after the release of&nbsp;<em>Pet Sounds</em>, Wilson briefly owned and operated a health food store called the Radiant Radish. He&nbsp;continued to work with the band while struggling with drug and alcohol addiction in the ’70s.</p><p>After a family&nbsp;intervention into Wilson’s deteriorating mental and physical&nbsp;health, he became embroiled with the controversial psychologist Eugene Landy. Their decades-long treatment later became the subject of a biographical film called&nbsp;<a href="https://pitchfork.com/news/58627-paul-dano-john-cusack-star-as-brian-wilson-in-love-mercy-trailer/"><em>Love &amp; Mercy</em></a>.&nbsp;The Beach Boys&nbsp;enjoyed a brief resurgence after&nbsp;their 1977 album&nbsp;<em>Love You</em>, but&nbsp;Wilson’s health issues persisted.&nbsp;In 1982, Landy&nbsp;removed him from the band&nbsp;for intensive treatment and exercised increasing&nbsp;control over Wilson’s financial and creative endeavors, prompting Wilson to release his debut solo album in 1988.&nbsp;In 1992, following legal action from Carl Wilson and other members of the Wilson family, Landy’s psychology license was revoked and he received a restraining order from Wilson.</p><p>Wilson eventually revisited&nbsp;<em>Smile,</em> reworking the archived studio sessions with Darian Sahanaja to bring the once-scrapped project to a live concert. He released&nbsp;<em>Brian Wilson: Presents Smile</em> in 2004 to critical acclaim. He continued to release solo work in the 2000s. His last solo album was 2015’s <a href="https://pitchfork.com/news/68850-read-the-introduction-and-first-chapter-of-brian-wilsons-memoir/"><em>No Pier Pressure</em></a>, which featured contributions from Kacey Musgraves and Zooey Deschanel. He released a <a href="https://pitchfork.com/news/68850-read-the-introduction-and-first-chapter-of-brian-wilsons-memoir/">memoir</a> in 2016, and the following year shared “<a href="https://pitchfork.com/news/listen-to-brian-wilsons-previously-unreleased-song-some-sweet-day/">Some Sweet Day</a>,” a previously unreleased song recorded in the 1990s, as well as a new single called “<a href="https://pitchfork.com/news/listen-to-brian-wilsons-new-song-run-james-run/">Run James Run</a>.”</p><p>Wilson and the Beach Boys were inducted into the Rock &amp; Roll Hall of Fame in 1988. Wilson <a data-offer-url="https://www.grammy.com/grammys/artists/brian-wilson" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grammy.com/grammys/artists/brian-wilson&quot;}" href="https://www.grammy.com/grammys/artists/brian-wilson" rel="nofollow noopener" target="_blank">won</a> two Grammy awards (out of nine nominations), and the Beach Boys were honored in 2001 with a Grammy Lifetime Achievement Award.</p><figure data-testid="IframeEmbed"><div data-testid="IframeEmbedContainer"><p><iframe height="315" width="560" sandbox="allow-scripts allow-popups allow-same-origin allow-presentation allow-popups-to-escape-sandbox" title="Embedded Frame" data-src="https://www.youtube-nocookie.com/embed/DVUBpzlELOg" allow="autoplay *; encrypted-media *; clipboard-write; autoplay; fullscreen; picture-in-picture"></iframe></p></div></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dolly Parton's Dollywood Express (112 pts)]]></title>
            <link>https://thetransitguy.substack.com/p/dolly-parton-runs-a-train-busier</link>
            <guid>44249417</guid>
            <pubDate>Wed, 11 Jun 2025 16:46:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thetransitguy.substack.com/p/dolly-parton-runs-a-train-busier">https://thetransitguy.substack.com/p/dolly-parton-runs-a-train-busier</a>, See on <a href="https://news.ycombinator.com/item?id=44249417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>As I’ve written before, there are few institutions, or people, w</span><strong>ith a higher approval rating than Dolly Parton</strong><span>. While I love her for her music and the feeling of joy she gives us, I also love that Dollywood Express, the heritage steam train that operates within Dollywood, </span><strong>has a higher rail ridership than 27 states</strong><span>. </span></p><p><span>Dolly grew up in the back hills of the Great Smoky Mountains, not far from a Civil War–themed tourist park called </span><em>Rebel Railroad</em><span>. It featured a steam train, a general store, a blacksmith shop, and a saloon. In 1986, Dolly bought a stake in the park and helped relaunch it as </span><strong>Dollywood</strong><span>, which has since doubled in size and become the area’s largest employer.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg" width="660" height="372" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:372,&quot;width&quot;:660,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Dolly Parton invites visitors to Dollywood in a 1980s photo.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Dolly Parton invites visitors to Dollywood in a 1980s photo." title="Dolly Parton invites visitors to Dollywood in a 1980s photo." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9031b7ac-54df-462f-b3ed-9fa30862525a_660x372.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Dolly Parton at Dollywood Welcome Sign </figcaption></figure></div><p><span>I’ve never been to Dollywood, but it’s high on my list. Not just because I love Dolly, but I’d really like to ride the </span><strong>Dollywood Express</strong><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png" width="1456" height="935" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:935,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5678789,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thetransitguy.substack.com/i/159437284?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84beb67b-80e6-4778-ac68-bbb2ba4e2873_2136x1372.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>The 2.5-mile heritage rail loop has been part of the park since the beginning and features three coal-fired steam engines:</p><ul><li><p><em>The Cinderella</em></p></li><li><p><em>The Beatrice</em><span> (currently under restoration)</span></p></li><li><p><em>Klondike Katie</em></p></li></ul><p>The trains run all day during park hours and can burn through up to four tons of coal per day and 4000 gallons of water.</p><p><span>The Dollywood Express locomotives have an unexpected origin: the </span><strong>White Pass &amp; Yukon Route</strong><span>, one of the most remote railways in North America. Built in 1898 during the Klondike Gold Rush, this narrow-gauge line connected Whitehorse, Yukon, to Skagway, Alaska.</span></p><p>At the height of its postwar success, the railway purchased several steam locomotives to meet demand during World War II. But by the mid-1950s, the company transitioned to diesel, and the steam engines were retired.</p><p>Meanwhile, the White Pass &amp; Yukon line now operates as a heritage railway, primarily serving cruise passengers. It’s co-owned by Carnival Cruise Lines and still runs through the rugged terrain of the far north.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg" width="700" height="550" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:550,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02731e28-86c4-420d-b606-6a0401a4aaa6_700x550.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Eventually, these engines made their way to Tennessee, where they were given a second life at Dollywood.</p><p>The Dollywood Express departs every hour and takes 30 minutes round-trip, rain or shine. That makes it the most frequent passenger train in all of Tennessee, by far. Technically, it’s not transit since it drops you off where you started, but if it were used to move people around the park. For the purposes of this piece, we’ll consider it.</p><p><span>Outside of Amtrak’s once-daily </span><strong>City of New Orleans</strong><span>, Tennessee has only three rail transit systems (two of which are currently suspended). That leaves just the </span><strong>WeGo Star</strong><span> in Nashville, which is among the lowest-ridership commuter rail lines in the country. </span><em><span>(I wrote about it </span><a href="https://thetransitguy.substack.com/p/how-disney-became-one-of-the-largest" rel="">here</a><span>.)</span></em></p><p>Meanwhile, the Dollywood Express, with seven cars and a capacity of about 550 riders per trip, likely carries more people than the entire state’s rail transit combined.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg" width="1280" height="720" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:720,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:329380,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d003374-22b0-4996-a3f1-8dfb1e6aef3f_1280x720.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Dollywood Express Depot (</span><a href="https://www.google.com/search?q=dollywood+express+depot+park+z&amp;oq=dollywood+express+depot+park+z&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigATIHCAQQIRigATIHCAUQIRiPAjIHCAYQIRiPAtIBCDYyODNqMGo0qAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8" rel="">Park Z</a><span>)</span></figcaption></figure></div><p><span>As I covered in </span><em><a href="https://thetransitguy.substack.com/p/how-disney-became-one-of-the-largest" rel="">How Disney Became One of the Largest Transit Agencies in North America</a></em><span>, I’ll leave this one question open-ended:</span></p><p>“What does it say about the state of our country’s infrastructure when the most ridden train in a $432.2B economy is DollyWood’s heritage steam train?” </p><p><span>To put that in perspective, </span><strong>Tennessee’s economy is equal to Denmark’s</strong><span>. (Yes, I know Denmark is half the size of the state)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png" width="1456" height="959" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:959,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1269799,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thetransitguy.substack.com/i/159437284?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe95bf575-6b77-4efe-a857-003fd03576f9_2246x1480.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Denmark’s Rail Network (</span><a href="https://larstransportmaps.com/2021/04/28/trains-in-denmark-2021/" rel="">Source</a><span>)</span></figcaption></figure></div><p><span>Zooming out: Dollywood moves about </span><strong><a href="https://themeparksbydon.com/dollywood-express-the-pinnacle-of-theme-park-train-rides/" rel="">5,000 people per day</a></strong><span>. That’s </span><strong>92% of all rail riders in Tennessee</strong><span>, if you exclude Amtrak’s single daily train, the </span><em>City of New Orleans</em><span>.</span></p><p>Maybe it’s just a fun stat. Maybe it’s an entirely reasonable take. But to me, it’s absolutely insane.</p><p>I love Disney and I love Dolly Parton, but it says something deeply broken about our national priorities when their theme park trains outclass public infrastructure in billion-dollar economies. Be serious. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png" width="800" height="518" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:518,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97e4f48-69f6-430b-a68b-3210e562aca0_800x518.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Map of Dollywood</figcaption></figure></div><p><span>(</span><em><strong>Author’s Note</strong></em><span>: Chattanooga’s Lookout Mountain Incline Railway was omitted as it is currently “suspended indefinitely” from a rockslide) </span></p><p data-attrs="{&quot;url&quot;:&quot;https://thetransitguy.substack.com/p/dolly-parton-runs-a-train-busier?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://thetransitguy.substack.com/p/dolly-parton-runs-a-train-busier?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Medical aid in dying, my health, and so on (110 pts)]]></title>
            <link>https://blog.the-brannons.com/post/Medical-Aid-in-Dying-My-Health-and-so-on</link>
            <guid>44249303</guid>
            <pubDate>Wed, 11 Jun 2025 16:33:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.the-brannons.com/post/Medical-Aid-in-Dying-My-Health-and-so-on">https://blog.the-brannons.com/post/Medical-Aid-in-Dying-My-Health-and-so-on</a>, See on <a href="https://news.ycombinator.com/item?id=44249303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I’ll start at the end, because that’s the most important part.  Later this month, I’m obtaining medical aid in dying AKA death with dignity.  Barring unforeseen circumstances or unexpected changes, my last day on earth will be June 13th, 2025.  Realize that I’m just over 46 years old.  So how the hell did we get here?  I’ve written part of the story in dribs and drabs over the years, so I may as well write up the whole thing.</p>
<p>Late on the night of November 14th, 2021, I went to the local emergency room.  I had a blood pressure of 55/37 and a pulse of 220.  I had a heart attack at some point.  I’m not quite sure when that happened.  They admitted me for a hospital stay.</p>
<p>One of the first things they did was an arteriogram, and they found blockage in one artery.  They thought they would be able to treat it with stent placement during the procedure, but it was too severe for that.  They had a surgeon consult with me about doing an open heart procedure.  It would be a single bypass.  If I refused the procedure, my life expectancy would be about 4 and a half years.  So it seemed reasonable to go ahead with it.</p>
<p>On Thursday, I went in for surgery.  It was miserable, but I survived it.  By Monday afternoon, I had recovered sufficiently to be released from the hospital, so I went home and finished the recovery process there.</p>
<p>During my hospital stay, I was also diagnosed with heart failure with reduced ejection fraction.  My ejection fraction was between 25 and 30 percent.  They told me that they would probably want to place an ICD (implantable cardioverter defibrillator) if my ejection fraction didn’t improve.  It never really did.  The best it got was 30 to 35 percent.  So in February of 2023 I went in for an out patient stay to have a subcutaneous implantable cardioverter defibrillator (SICD) placed.</p>
<p>Fastforward a few months.  I’m sitting in my recliner, eating some plantain chips, when all of a sudden, I got shocked.  I jumped and screamed.  Still, it only happened once, and I’m thinking: this is no big deal.  A few weeks later, in August, I received another shock.  It only happened once, so again, I’m like “no big deal.”</p>
<p>On September 11th, the situation changed.  I felt “off” all evening.  I was shocked once, and I kept feeling like another one was coming.  So I went to the ER.  My ICD device fired while I was on the ambulance and when I was being wheeled into the ER.  They kept me for a couple days, ran some tests, told me to follow up with cardiology, and sent me home.</p>
<p>A week later, I was back in the ER with the same complaint.  I don’t remember whether there were multiple shocks.  The same thing happened: they kept me for a couple days to make sure I was stable, ran some tests, sent me home, and told me to follow up with cardiology.</p>
<p>On the first Friday of October, I kept feeling “off”, as though I was going to have a heart episode.  The device didn’t fire, but I went to the ER anyway.  When I was in the ambulance, they told me that I was in VTac (ventricular tachycardia), and they shocked me.  I was shocked several times by hospital staff while they tried to get me stable.  This time was a little different.  The cardiac electrophysiologist decided that it was time to do a cardiac ablation.  They performed that procedure on the following Monday or Tuesday, and I went home in the middle of the week.</p>
<p>Things seemed ok for the next couple of months.  Then, on the night of February 1, 2024, I started receiving more shocks from the ICD.  I went back to the ER.  That night, my ICD shocked me at least seventeen times, and I was also shocked several additional times by paramedics and ER staff.  If you’ve ever taken a physical beating that has left you sore for days, you know exactly what I went through that night.  I took a beating or two of that sort when I was younger, and that’s exactly how being shocked more than twenty times felt.  Every movement of my upper body hurt for the next several days, including just shifting around in bed.  The day after I landed in the emergency room, they sent me to a much larger university hospital in Portland.  That hospital stabilized me, switched out my ICD for another device, and sent me home a couple days later.</p>
<p>I spent the next few months in a state of total mental paralysis that I suspect was some kind of PTSD.  I kept feeling like I was going to have heart episodes and waiting for the next shock.  The new device they placed when I was in Portland had some pacing functionality, so I’m pretty sure I avoided some shocks.  When I saw my cardiologist in the summer of 2024, she told me that the pacing functionality barely succeeded keeping me out of a round of shocks.  She and her boss wanted to do another cardiac ablation.</p>
<p>At this point, I started wondering just how much life I had left.  I bluntly asked for an estimation of my life expectancy and was told maybe ten years with the ablation and the defibrillator.  The point of the ablation was to act as a “band-aid”, to calm the heart and hopefully keep me out of the emergency room and from being shocked repeatedly.  I agreed to yet another procedure, and it was done in late August.</p>
<p>Less than a month later, I received multiple shocks from the ICD.  I went back to the ER.  I was shocked at least nine times that night.  They stabilized me, kept me for a couple days, and told me to follow up with cardiology.</p>
<p>So I followed up with cardiology.  And I bluntly asked the life expectancy question again.  “Obviously, the cardiac ablation did not work.  How much life would you estimate that I have left?”  “Maybe two years, but I honestly don’t know.”  At that point, I asked them to disable the defibrillator.  I was tired of getting multiple shocks and not knowing when they would come, tired of landing in the ER.  Realize that in one year, I had ended up in the ER and hospital 5 times for the exact same problem.  My prognosis without the defibrillator was a few months.</p>
<p>But at first, I wasn’t going to let it stop me.  In December, I started a cardiac exercise program.  Basically, they monitored my heart while I exercised.  That went well for a few days.  Then one day, my blood pressure bottomed out and my heart rate spiked during a session.  During the next session, I felt “off” afterward.  A couple days before Christmas, I was riding my stationary bike at home for a while.  At some point, I started feeling like I was about to have a VTac episode, so I stopped.  I spent the next few hours in agony, until I was finally able to sleep.</p>
<p>It gets worse.  At this point, even doing trivial tasks became a burden.  I’d start having heart palpitations and become winded while bringing in a couple bags of groceries or doing the dishes.  This part of the story is told more completely in an older post: <a href="https://blog.the-brannons.com/post/Reason-666-Why-US-Health-Care-Is-So-Fucked-The-Testing-Obsession">Reason 666 Why US Health Care Is So Fucked: The Testing Obsession!</a>.</p>
<p>I’m basically terminally ill.  I got a referral to hospice, and I’ve been on that program for several months.  I still have heart episodes.  I can back them off with morphein, which I use very judiciously.  But I expect that at any time, I could have an episode strong enough to land me back in the ER.  At that point, I’d most likely just die naturally.</p>
<p>I am afraid to so much as go for a walk, because the last thing I want is to die in the middle of the sidewalk, or worse, be discovered by paramedics and accidentally receive medical intervention.</p>
<p>A while ago, I started investigating Oregon’s Death with Dignity program.  Not because I want to die.  I don’t.  But I’d rather die in a controlled manner than die in pain and be found unresponsive in the middle of my floor by my girlfriend.  I want to live, but I’m strongly opposed to my few remaining possibilities for medical treatment (I’ll get to that in a moment).  No, I am not a burden to my family.  They’re glad to care for me.  In fact, it is going to be bad for them when I’m gone.  Especially in this hellworld that the US has become under a second Trump Presidency with a stacked Congress and Supreme Court, as well as a crazed billionaire and his wrecking crew actively destroying the government.  I’m sad to leave them, and I worry about how they’ll make it without me.  I’m angry that I have severe heart failure that started when I was in my early forties.  I’m too young for this, but here we are, and here is what I’ve chosen.</p>
<p>So let’s talk about my scant options for treatment.  I could have my defibrillator reactivated, and maybe have a couple more years.  If I do that, I expect many more ER visits and more painful and unpredictable shocks.  I already said no to that.  Another slim possibility is a heart transplant.  I am not interested.  I’ve explained my reasons more fully to people who are close to me, but they basically boil down to the fact that an organ transplant requires a full commitment to the process, and I’m not willing to make that commitment.</p>
<p>A few months ago, a friend sent me a link to a university doing clinical trials using stem cells to repair the heart.  It seemed really promising.  Unfortunately, they are in Germany, and they only accept German residents.  So that wasn’t an option.</p>
<p>So anyway here I am, planning for my upcoming death.  And I don’t have enough nice things to say about this process.  At every point, the choice is entirely mine.  From the beginning of the process all the way until I consume the cocktail of meds that will end my life, I can decide not to proceed, or I can decide to wait to fill the prescription, or whatever.  This is my choice all the way.</p>
<p>As it was put to me: “Chris, you’re dying of heart disease.  You can choose to just let it naturally run its course.  Or you can choose a controlled exit.  Either way, if you choose medical aid or you choose to let it run its course, you’re dying.”</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[V-JEPA 2 world model and new benchmarks for physical reasoning (182 pts)]]></title>
            <link>https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/</link>
            <guid>44248165</guid>
            <pubDate>Wed, 11 Jun 2025 14:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/">https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/</a>, See on <a href="https://news.ycombinator.com/item?id=44248165">Hacker News</a></p>
Couldn't get https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/: Error: Request failed with status code 500]]></description>
        </item>
        <item>
            <title><![CDATA[AI at Amazon: A case study of brittleness (107 pts)]]></title>
            <link>https://surfingcomplexity.blog/2025/06/08/ai-at-amazon-a-case-study-of-brittleness/</link>
            <guid>44247998</guid>
            <pubDate>Wed, 11 Jun 2025 14:28:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://surfingcomplexity.blog/2025/06/08/ai-at-amazon-a-case-study-of-brittleness/">https://surfingcomplexity.blog/2025/06/08/ai-at-amazon-a-case-study-of-brittleness/</a>, See on <a href="https://news.ycombinator.com/item?id=44247998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>A year ago, Mihail Eric wrote a blog post detailing his experiences working on AI inside Amazon: <a href="https://www.mihaileric.com/posts/how-alexa-dropped-the-ball-conversational-ai/">How Alexa Dropped the Ball on Being the Top Conversational System on the Planet</a>. It’s a great first-person account, with lots of detail of the issues that kept Amazon from keeping up with its peers in the LLM space. From my perspective, Eric’s post makes a great case study in what resilience engineering researchers refer to as <em>brittleness</em>, which is a term that the researchers use to refer to as a kind of opposite of resilience.</p>



<p>In the paper <a href="https://www.researchgate.net/publication/333952644_Basic_Patterns_in_How_Adaptive_Systems_Fail">Basic Patterns in How Adaptive Systems Fail</a>, the researchers David Woods and Matthieu Branlat note that brittle systems tend to suffer from the following three patterns:</p>



<ol>
<li><em>Decompensation</em>: exhausting capacity to adapt as challenges cascade</li>



<li><em>Working at cross-purposes</em>: behavior that is locally adaptive but globally maladaptive</li>



<li><em>Getting stuck in outdated behaviors</em>: the world changes but the system remains stuck in what were previously adaptive strategies (over-relying on past successes)</li>
</ol>



<p>Eric’s post demonstrates how all three of these patterns were evident within Amazon. </p>



<h2>Decompensation</h2>



<blockquote>
<p><em>It would take weeks to get access to any internal data for analysis or experiments</em>… <br>Experiments had to be run in resource-limited compute environments. Imagine trying to train a transformer model when all you can get a hold of is CPUs. Unacceptable for a company sitting on one of the largest collections of accelerated hardware in the world.</p>
</blockquote>



<p>If you’ve ever seen a service fall over after receiving a spike in external requests, you’ve seen a <em>decompensation</em> system failure. This happens when a system isn’t able to keep up with the demands that are placed upon on it. </p>



<p>In organizations, you can see the decompensation failure pattern emerge when decision-making is very hierarchical: you end up having to wait for the decision request to make its way up to someone who has the authority to make the decision, and then make its way down again. In the meantime, the world isn’t standing still waiting for that decision to be made. </p>



<p>As described in the <strong>Bad Technical Process</strong> section of Eric’s post, Amazon was not able to keep up with the rate at which its competitors were making progress on developing AI technology, even though Amazon had both the talent and the compute resources necessary in order to make progress. The people inside the organization who needed the resources weren’t able to get them in a timely fashion. That slowed down AI development and, consequently, they got lapped by their competitors.</p>



<h2>Working at cross-purposes</h2>



<blockquote>
<div><p>Alexa’s org structure was decentralized by design meaning there were multiple small teams working on sometimes identical problems across geographic locales. </p><p>This introduced an almost Darwinian flavor to org dynamics where teams scrambled to get their work done to avoid getting reorged and subsumed into a competing team.</p></div>



<p>The consequence was an organization plagued by antagonistic mid-managers that had little interest in collaborating for the greater good of Alexa and only wanted to preserve their own fiefdoms.</p>



<p>My group by design was intended to span projects, whereby we found teams that aligned with our research/product interests and urged them to collaborate on ambitious efforts. The resistance and lack of action we encountered was soul-crushing.</p>
</blockquote>



<p>Where <em>decompensation</em> is a consequence of poor centralization, <em>working at cross-purposes</em> is a consequence of poor decentralization. In a decentralized organization, the individual units are able to work more quickly, but there’s a risk of <em>alignment</em>: enabling everyone to row faster isn’t going to help if they’re rowing in different directions. </p>



<p>In the <strong>Fragmented Org Structures</strong> section of Eric’s writeup, he goes into vivid, almost painful detail about how Amazon’s decentralized org structure worked against them.</p>



<h2>Getting stuck in outdated behaviors</h2>



<blockquote>
<p>Alexa was viciously customer-focused which I believe is admirable and a principle every company should practice. Within Alexa, this meant that every engineering and science effort had to be aligned to some downstream product.</p>



<p>That did introduce tension for our team because we were supposed to be taking experimental bets for the platform’s future. These bets couldn’t be baked into product without hacks or shortcuts in the typical quarter as was the expectation.</p>



<p>So we had to constantly justify our existence to senior leadership and massage our projects with metrics that could be seen as more customer-facing.</p>



<p>…</p>



<p>This introduced product/science conflict in every weekly meeting to track the project’s progress leading to manager churn every few months and an eventual sunsetting of the effort.</p>
</blockquote>



<p>I’m generally not a fan of management books, but <a href="https://marshallgoldsmith.com/book-page-what-got-you-here/">What got you here won’t get you there</a> is a pretty good summary of the third failure pattern: when organizations continue to apply approaches that were well-suited to problems in the past but are ill-suited to problems in the present. </p>



<p>In the <strong>Product-Science Misalignment</strong> section of his post, Eric describes how Amazon’s traditional <em>viciously</em> <em>customer-focused</em> approach to development was a poor match for the research-style work that was required for developing AI. Rather than Amazon changing the way they worked in order to facilitate the activities of AI researchers, the researchers had to try to fit themselves into Amazon’s pre-existing product model. Ultimately, that effort failed. </p>



<hr>



<p>I write mostly about software incidents on this blog, which are <em>high-tempo</em> affairs. But the failure of Amazon to compete effectively in the AI space, despite its head start with Alexa, its internal talent, and its massive set of compute resources, can also be viewed as a kind of incident. As demonstrated in this post, we can observe the same sorts of patterns in failures that occur in the span of months as we can in failures that occur in the span of minutes. How well Amazon is able to learn from this incident remains to be seen. </p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: RomM – An open-source, self-hosted ROM manager and player (141 pts)]]></title>
            <link>https://github.com/rommapp/romm</link>
            <guid>44247964</guid>
            <pubDate>Wed, 11 Jun 2025 14:25:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rommapp/romm">https://github.com/rommapp/romm</a>, See on <a href="https://news.ycombinator.com/item?id=44247964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/isotipo.png"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/isotipo.png" height="180px" width="auto" alt="romm logo"></a>
  <br>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/logotipo.png"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/logotipo.png" height="45px" width="auto" alt="romm logotype"></a></p><p dir="auto"><h3 tabindex="-1" dir="auto">
    A beautiful, powerful, self-hosted rom manager.
  </h3><a id="user-content-----a-beautiful-powerful-self-hosted-rom-manager--" aria-label="Permalink: 
    A beautiful, powerful, self-hosted rom manager.
  " href="#----a-beautiful-powerful-self-hosted-rom-manager--"></a></p>
  
<p dir="auto"><a href="https://github.com/rommapp/romm/blob/release/LICENSE"><img src="https://camo.githubusercontent.com/7ce8206775bef5a1f8540ffbe7e28eef7e4601bbf48c009d8a38650c333e5703/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d626164676526636f6c6f723d613332643261" alt="license-badge-img" data-canonical-src="https://img.shields.io/github/license/rommapp/romm?style=for-the-badge&amp;color=a32d2a"></a>
<a href="https://github.com/rommapp/romm/releases"><img src="https://camo.githubusercontent.com/238cf16a9a6e9d8f4220f66435e0431a6d04c14beb7a44c7609d87271a35538c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d6261646765" alt="release-badge-img" data-canonical-src="https://img.shields.io/github/v/release/rommapp/romm?style=for-the-badge"></a>
<a href="https://hub.docker.com/r/rommapp/romm" rel="nofollow"><img src="https://camo.githubusercontent.com/626dc557351dfc46009d10fc29b5c2ed9a946f2d4b746da44ce563ee7c478f46/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d6261646765266c6162656c3d70756c6c73" alt="docker-pulls-badge-img" data-canonical-src="https://img.shields.io/docker/pulls/rommapp/romm?style=for-the-badge&amp;label=pulls"></a></p>
<p dir="auto"><a href="https://discord.gg/P5HtHnhUDH" rel="nofollow"><img src="https://camo.githubusercontent.com/ab7559905653044cd7aab1e2c5082f3b0c390a0a17632752bcd41916fe56fe76/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d3732383964613f7374796c653d666f722d7468652d6261646765" alt="discord-badge-img" data-canonical-src="https://img.shields.io/badge/discord-7289da?style=for-the-badge"></a>
<a href="https://docs.romm.app/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/370bc4357fb7df07e3ef934562c5434f60da5d2f16dae7d032d13620719cc49f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d3733366539623f7374796c653d666f722d7468652d6261646765" alt="docs-badge-img" data-canonical-src="https://img.shields.io/badge/docs-736e9b?style=for-the-badge"></a></p>
  </div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#overview">Overview</a>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#preview">Preview</a></li>
</ul>
</li>
<li><a href="#installation">Installation</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#community">Community</a></li>
<li><a href="#technical-support">Technical Support</a></li>
<li><a href="#project-support">Project Support</a></li>
<li><a href="#our-friends">Our Friends</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">RomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Scans and enhance your game library with metadata from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#igdb" rel="nofollow">IGDB</a>, <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#screenscraper" rel="nofollow">Screenscraper</a> and <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#mobygames" rel="nofollow">MobyGames</a></li>
<li>Fetch custom arwork from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#steamgriddb" rel="nofollow">SteamGridDB</a></li>
<li>Display your achievements from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#retroachievements" rel="nofollow">Retroachievements</a></li>
<li>Metadata available for <a href="https://docs.romm.app/latest/Platforms-and-Players/Supported-Platforms/" rel="nofollow">400+ platforms</a></li>
<li>Play games directly from the browser using <a href="https://docs.romm.app/latest/Platforms-and-Players/EmulatorJS-Player/" rel="nofollow">EmulatorJS</a> and <a href="https://docs.romm.app/latest/Platforms-and-Players/RuffleRS-Player/" rel="nofollow">RuffleRS</a></li>
<li>Share your library with friends with limited access and permissions</li>
<li>Official apps for <a href="https://github.com/rommapp/playnite-plugin">Playnite</a> and <a href="https://github.com/rommapp/muos-app">muOS</a></li>
<li>Supports multi-disk games, DLCs, mods, hacks, patches, and manuals</li>
<li>Parse and filter by <a href="https://docs.romm.app/latest/Getting-Started/Folder-Structure/#tag-support" rel="nofollow">tags</a> in filenames</li>
<li>View, upload, update, and delete games from any modern web browser</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preview</h2><a id="user-content-preview" aria-label="Permalink: Preview" href="#preview"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>🖥 Desktop</th>
<th>📱 Mobile</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/screenshots/preview-desktop.webp"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/screenshots/preview-desktop.webp" alt="desktop preview"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/screenshots/preview-mobile.webp"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/screenshots/preview-mobile.webp" alt="mobile preview"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To start using RomM, check out the <a href="https://docs.romm.app/latest/Getting-Started/Quick-Start-Guide/" rel="nofollow">Quick Start Guide</a> in the docs. If you are having issues with RomM, please review the page for <a href="https://docs.romm.app/latest/Troubleshooting/Scanning-Issues/" rel="nofollow">troubleshooting steps</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">To contribute to RomM, please check <a href="https://github.com/rommapp/romm/blob/release/CONTRIBUTING.md">Contribution Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">Here are a few projects maintained by members of our community. Please note that the RomM team does not regularly review their source code.</p>
<ul dir="auto">
<li><a href="https://github.com/idio-sync/romm-comm">romm-comm</a>: Discord Bot by @idio-sync</li>
<li><a href="https://github.com/PeriBluGaming/DeckRommSync-Standalone">DeckRommSync</a>: SteamOS downloader and sync by @PeriBluGaming</li>
<li>CasaOS app via the <a href="https://github.com/bigbeartechworld/big-bear-casaos">BigBear App Store</a></li>
</ul>
<p dir="auto">Join us on Discord, where you can ask questions, submit ideas, get help, showcase your collection, and discuss RomM with other users.</p>
<p dir="auto"><a href="https://discord.gg/P5HtHnhUDH" rel="nofollow"><img src="https://camo.githubusercontent.com/2d55f891c5f4e62ae883d68c0c1952600da5d11d5e7968b3aeb8e8169620e21b/68747470733a2f2f696e7669646765742e737769746368626c6164652e78797a2f50354874486e68554448" alt="discord-invite-img" data-canonical-src="https://invidget.switchblade.xyz/P5HtHnhUDH"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Support</h2><a id="user-content-technical-support" aria-label="Permalink: Technical Support" href="#technical-support"></a></p>
<p dir="auto">If you have any issues with RomM, please <a href="https://github.com/rommapp/romm/issues/new">open an issue</a> in this repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Support</h2><a id="user-content-project-support" aria-label="Permalink: Project Support" href="#project-support"></a></p>
<p dir="auto">Consider supporting the development of this project on Open Collective.</p>
<p dir="auto"><a href="https://opencollective.com/romm" rel="nofollow"><img src="https://camo.githubusercontent.com/06822411e9fb2f082e781925defacffc39ef14fa852b74e7c05b69bfb1f128e1/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f726f6d6d2f646f6e6174652f627574746f6e2e706e673f636f6c6f723d626c7565" alt="oc-donate-img" data-canonical-src="https://opencollective.com/romm/donate/button.png?color=blue"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Our Friends</h2><a id="user-content-our-friends" aria-label="Permalink: Our Friends" href="#our-friends"></a></p>
<p dir="auto">Here are a few projects that we think you might like:</p>
<ul dir="auto">
<li><a href="https://emulatorjs.org/" rel="nofollow">EmulatorJS</a>: An embeddable, browser-based emulator</li>
<li><a href="https://retrodeck.net/" rel="nofollow">RetroDECK</a>: Retro gaming on SteamOS and Linux</li>
<li><a href="https://es-de.org/" rel="nofollow">ES-DE Frontend</a>: Emulator frontend for Linux, macOS and Windows</li>
<li><a href="https://github.com/gaseous-project/gaseous-server">Gaseous</a>: Another ROM manager with web-based emulator</li>
<li><a href="https://github.com/JMBeresford/retrom">Retrom</a>: A centralized game library/collection management service</li>
<li><a href="https://steamgriddb.github.io/steam-rom-manager/" rel="nofollow">Steam ROM Manager</a>: An app for managing ROMs in Steam</li>
</ul>




</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bypassing GitHub Actions policies in the dumbest way possible (115 pts)]]></title>
            <link>https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass</link>
            <guid>44247881</guid>
            <pubDate>Wed, 11 Jun 2025 14:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass">https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass</a>, See on <a href="https://news.ycombinator.com/item?id=44247881">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>ENOSUCHBLOG</h2>
<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/series">Series</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
    <li><a href="https://yossarian.net/">Main Site</a></li>
    <li><a href="https://yossarian.net/til">TILs</a></li>
    
</ul>

<hr>



<h2>
  <p>
    <span><em>Jun 11, 2025</em></span>

    &nbsp; &nbsp;

    
      <span>
        Tags:
        
        
          <a href="https://blog.yossarian.net/tags#security">security</a>
        
      </span>
    

    &nbsp; &nbsp;

    
  </p>
</h2>






<hr>


<p><strong>TL;DR</strong>: GitHub Actions provides a policy mechanism for limiting the kinds of
actions and reusable workflows that can be used within a repository,
organization, or entire enterprise. Unfortunately, this mechanism is <strong>trivial
to bypass</strong>. GitHub has told me that they don’t consider
this a security issue (I disagree), so I’m publishing this post as-is.</p>

<p><img src="https://blog.yossarian.net/assets/oops-all-uses.jpg" alt="" height="400px"></p>

<h2 id="background">Background</h2>

<p>GitHub Actions is GitHub’s CI/CD offering. I’m a big fan of it, despite its
<a href="https://www.wiz.io/blog/github-actions-security-guide">spotty</a>
<a href="https://grafana.com/blog/2025/04/27/grafana-security-update-no-customer-impact-from-github-workflow-vulnerability/">security</a>
<a href="https://blog.pypi.org/posts/2024-12-11-ultralytics-attack-analysis/">track</a>
<a href="https://adnanthekhan.com/2024/12/21/cacheract-the-monster-in-your-build-cache/">record</a>.</p>

<p>Because a CI/CD offering is essentially <strong>arbitrary code execution as
a service</strong>, users are <em>expected</em> to be careful about what they allow
to run in their workflows, <em>especially</em> privileged workflows that have access
to secrets and/or can modify the repository itself. That, in effect, means
that users need to be careful about what <a href="https://docs.github.com/en/actions/sharing-automations/avoiding-duplication">actions</a> and <a href="https://docs.github.com/en/actions/using-workflows/reusing-workflows">reusable workflows</a>
they trust.</p>

<p>Like with other open source ecosystems, downstream consumers (i.e., users
of GitHub Actions) retrieve their components (i.e., action definitions)
from an essentially open index (the <a href="https://github.com/marketplace?type=actions">“Actions Marketplace”</a>​<sup id="fnref:open"><a href="#fn:open" rel="footnote" role="doc-noteref">1</a></sup>).</p>

<p>To establish trust in those components, downstream users perform all of the
normal fuzzy heuristics: they look at the number of stars, the number
of other user, recency of activity, whether the user/organization
is a “good” one, and so forth.</p>

<p>Unfortunately, this isn’t good enough along two dimensions:</p>

<ol>
  <li>
    <p>Even actions that satisfy these heuristics can be compromised.
They’re heuristics after all, not verifiable assertions of quality
or trustworthiness.</p>

    <p>The recent <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-attack/">tj-actions attack</a> typifies this: even popular, widely-used
actions are themselves software components, with their own supply chains
(and CI/CD setups).</p>
  </li>
  <li>
    <p>This kind of acceptance scheme <em>just doesn’t scale</em>, both in terms of
human effort and system complexity: complex CI/CD setups can have dozens
(or hundreds) of workflows, each of which can contain dozens (or hundreds)
of jobs that in turn employ actions and reusable workflows.</p>

    <p>These sorts of large setups don’t necessarily have a single owner
(or even a single team) responsible for gating admission and preventing a
the introduction of unvetted actions and reusable workflows.</p>
  </li>
</ol>

<h2 id="github-actions-policies">GitHub Actions policies</h2>

<p>The problem (as stated above) is best solved by <strong>eliminating the failure
mode</strong> itself: rather than giving the system’s committers the ability to
introduce new actions and reusable workflows without sufficient review,
the system should <strong>prevent them from doing so in the first place</strong>.</p>

<p>To their credit, GitHub understands this! They have a feature called
“Actions policies<sup id="fnref:name"><a href="#fn:name" rel="footnote" role="doc-noteref">2</a></sup>” that does exactly this. From the
<a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/enabling-features-for-your-repository/managing-github-actions-settings-for-a-repository#managing-github-actions-permissions-for-your-repository">Manage GitHub Actions settings</a> documentation:</p>

<blockquote>
  <p>You can restrict workflows to use actions and reusable workflows in specific
organizations and repositories. Specified actions cannot be set to more than
1000. (sic)</p>

  <p>To restrict access to specific tags or commit SHAs of an action or reusable
workflow, use the same syntax used in the workflow to select the action or
reusable workflow.</p>

  <ul>
    <li>
      <p>For an action, the syntax is <code>OWNER/REPOSITORY@TAG-OR-SHA</code>. For example, use
<code>actions/javascript-action@v1.0.1</code> to select a tag or
<code>actions/javascript-action@a824008085750b8e136effc585c3cd6082bd575f</code> to
select a SHA. For more information, see Using pre-written building blocks
in your workflow.</p>
    </li>
    <li>
      <p>For a reusable workflow, the syntax is
<code>OWNER/REPOSITORY/PATH/FILENAME@TAG-OR-SHA</code>. For example,
<code>octo-org/another-repo/.github/workflows/workflow.yml@v1</code>. For more
information, see Reusing workflows.</p>
    </li>
  </ul>

  <p>You can use the <code>*</code> wildcard character to match patterns. For example, to allow
all actions and reusable workflows in organizations that start with <code>space-org</code>,
you can specify <code>space-org*/*</code>. To allow all actions and reusable workflows in
repositories that start with <code>octocat</code>, you can use <code>*/octocat**@*</code>. For more
information about using the <code>*</code> wildcard, see Workflow syntax for GitHub
Actions.</p>

  <p>Use <code>,</code> to separate patterns. For example, to allow <code>octocat</code> and <code>octokit</code>, you can
specify <code>octocat/*, octokit/*</code>.</p>
</blockquote>

<p>GitHub also provides special “preset” cases for this functionality, such as
allowing only actions and reusable workflows that belong to the same
organization namespace as the repository itself. Here’s what that looks
like on a dummy organization and repository of mine:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-1.png" alt="A screen capture of the &quot;Actions permissions&quot; section"></p>

<p>…and here’s what happens when I try to violate that policy, e.g. by using
<code>actions/checkout@v4</code> in a workflow:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-2.png" alt="The error message from the resulting policy violation"></p>

<p>This is fantastic, except that it’s trivial to bypass. Let’s see how.</p>

<h2 id="bypassing-policies">Bypassing policies</h2>

<p>To understand how we’re going to bypass this, we need to understand a few
of the building blocks underneath actions and reusable workflows.</p>

<p>In particular:</p>

<ul>
  <li>Actions and reusable workflows share the same namespace as the rest of
GitHub, i.e. <code>owner/repo</code><sup id="fnref:slug"><a href="#fn:slug" rel="footnote" role="doc-noteref">3</a></sup>;</li>
  <li>When a user writes something like <code>uses: actions/checkout@v4</code> in a workflow,
GitHub <em>resolves</em> that reference to mean “the <code>action.yml</code> file defined at
tag <code>v4</code> in the <code>actions/checkout</code> repository”;</li>
  <li><code>uses:</code> keywords can <strong>also</strong> refer to relative paths on the runner itself.
For example, <code>uses: ./</code> runs the step with the <code>action.yml</code> in the current
directory.</li>
  <li>Relative paths from the runner are <strong>not</strong> inherently part of the repository
state itself: the runner is can contain any state introduced by previous
steps within the same job.</li>
</ul>

<p>These four aspects of GitHub Actions compose together into the <strong>world’s
dumbest policy bypass</strong>: instead of doing <code>uses: actions/checkout@v4</code>,
the user can <code>git clone</code> (or otherwise fetch) the <code>actions/checkout</code>
repository into the runner’s filesystem, and then use <code>uses: ./path/to/checkout</code>
to run the very same action.</p>

<p>Here’s what that looks like in practice:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td><pre><span>on</span><span>:</span> <span>[</span><span>push</span><span>,</span> <span>pull_request</span><span>]</span>

<span>jobs</span><span>:</span>
  <span>test</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>|</span>
          <span>mkdir -p ./tmp</span>
          <span>git clone https://github.com/actions/checkout.git ./tmp/checkout</span>

      <span>-</span> <span>uses</span><span>:</span> <span>./tmp/checkout</span>
        <span>with</span><span>:</span>
          <span>repository</span><span>:</span> <span>woodruffw/gha-hazmat</span>
          <span>path</span><span>:</span> <span>gha-hazmat</span>

      <span>-</span> <span>run</span><span>:</span> <span>ls &amp;&amp; pwd</span>

      <span>-</span> <span>run</span><span>:</span> <span>ls tmp/checkout</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>(The actual <code>with:</code> block of the <code>uses: ./tmp/checkout</code> step is inconsequential
— I just used that repository for the demo, but anything would work.)</p>

<p>And naturally, it works just fine:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-3.png" alt="A screenshot of the bypass running successfully "></p>

<h2 id="fixing-it">Fixing it?</h2>

<p>The fix for this bypass is simple, if potentially somewhat painful:
GitHub Actions could consider “local” <code>uses:</code> references to be another
category for the purpose of policies, and reject them whenever the policy
doesn’t permit them.</p>

<p>This would seal off the entire problem, since <code>uses: ./foo</code> would just stop
working. The downside is that it would potentially break existing users
of policies who also use local actions and reusable workflows, assuming
there are significant numbers of them<sup id="fnref:honest"><a href="#fn:honest" rel="footnote" role="doc-noteref">4</a></sup>.</p>

<p>The other option would be to leave it the way it is, but <strong>explicitly document</strong>
local <code>uses:</code> references as a limitation of this policy mechanism. I honestly
think this would be perfectly fine; what matters is that users<sup id="fnref:users"><a href="#fn:users" rel="footnote" role="doc-noteref">5</a></sup>
are <em>informed</em> of a feature’s limitations, not necessarily that the feature
<em>lacks</em> limitations.</p>

<h2 id="does-this-matter">Does this matter?</h2>

<p>First, I’ll couch this again: this is not exactly fancy stuff. It’s a very
dumb bypass, and I don’t think it’s critical by any means.</p>

<p>At the same time, I think this <strong>matters a great deal</strong>: ineffective
policy mechanisms are <strong>worse</strong> than missing policy mechanisms, because they provide
all of the <em>feeling</em> of security through compliance while actually
<strong>incentivizing malicious forms of compliance</strong>.</p>

<p>In this case, the maliciously complying party is almost certainly a developer
just trying to get their job done: like most other developers who encounter
an inscrutable policy restriction, they will try to hack around it such that
the policy is satisfied in name only.</p>

<p>For that reason alone I think GitHub should fix this bypass, either by
<em>actually</em> fixing it or at least documenting its limitations. Without either
of those, projects and organizations are likely to <strong>mistakenly believe</strong> that
these sorts of policies provide a <em>security boundary</em> where <strong>none in fact
exists</strong>.</p>

<hr>




<hr>


<span>
  Discussions:
  
  <a href="https://infosec.exchange/@yossarian/114665097805372166">Mastodon</a>
  
  <a href="https://bsky.app/profile/yossarian.net/post/3lrdl3j3ez52s">Bluesky</a>
  
  <a href="https://www.reddit.com/r/enosuchblog/comments/1l8th6c/bypassing_github_actions_policies_in_the_dumbest/">Reddit</a>
  
</span>
<hr>



  






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ikuyo a Travel Planning Web Application (194 pts)]]></title>
            <link>https://ikuyo.kenrick95.org/</link>
            <guid>44247029</guid>
            <pubDate>Wed, 11 Jun 2025 12:44:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ikuyo.kenrick95.org/">https://ikuyo.kenrick95.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44247029">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Menstrual tracking app data is gold mine for advertisers that risks women safety (233 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report</link>
            <guid>44246920</guid>
            <pubDate>Wed, 11 Jun 2025 12:31:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report">https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report</a>, See on <a href="https://news.ycombinator.com/item?id=44246920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Cambridge researchers urge public health bodies like the NHS to provide trustworthy, research-driven alternatives to platforms driven by profit.</p>
</div><div><p>Smartphone apps that track menstrual cycles are a “gold mine” for consumer profiling, collecting information on everything from exercise, diet and medication to sexual preferences, hormone levels and contraception use.</p>

<p><a href="https://www.mctd.ac.uk/femtech-high-stakes-tracking-menstruation/">This is according to a new report</a> from the University of Cambridge’s <a href="https://www.mctd.ac.uk/">Minderoo Centre for Technology and Democracy</a>, which argues that the financial worth of this data is “vastly underestimated” by users who supply profit-driven companies with highly intimate details in a market lacking in regulation.</p>

<p>The report’s authors caution that cycle tracking app (CTA) data in the wrong hands could result in risks to job prospects, workplace monitoring, health insurance discrimination and cyberstalking – and limit access to abortion.</p>

<p>They call for better governance of the booming ‘femtech’ industry to protect users when their data is sold at scale, arguing that apps must provide clear consent options rather than all-or-nothing data collection, and urge public health bodies to launch alternatives to commercial CTAs.</p>

<p>“Menstrual cycle tracking apps are presented as empowering women and addressing the gender health gap,” said Dr Stefanie Felsberger, lead author of the report from Cambridge’s Minderoo Centre. “Yet the business model behind their services rests on commercial use, selling user data and insights to third parties for profit.”</p>

<p>“There are real and frightening privacy and safety risks to women as a result of the commodification of the data collected by cycle tracking app companies.”</p>

<p>As most cycle tracking apps are targeted at women aiming to get pregnant, the download data alone is of huge commercial value, say researchers, as – other than home buying – no life event is linked to such dramatic shifts in consumer behaviour.</p>

<p>In fact, data on pregnancy is believed to be over two hundred times more valuable than data on age, gender or location for targeted advertising. The report points out that period tracking could also be used to target women at different points in their cycle. For example, the oestrogen or ‘mating’ phase could see an increase in cosmetics adverts.</p>

<p>Just the three most popular apps had estimated global download figures of a quarter of a billion in 2024. So-called femtech – digital products focused on women’s health and wellbeing – is estimated to reach over US$60 billion by 2027, with cycle tracking apps making up half of this market.</p>

<p>With such intense demand for period tracking, the report argues that the UK’s National Health Service (NHS) should develop its own transparent and trustworthy app to rival those from private companies, with apps allowing permission for data to be used in valid medical research.</p>

<p>“The UK is ideally positioned to solve the question of access to menstrual data for researchers, as well as privacy and data commodification concerns, by developing an NHS app to track menstrual cycles,” said Felsberger, who points out that Planned Parenthood in the US already has its own app, but the UK lacks an equivalent.</p>

<p>“Apps that are situated within public healthcare systems, and not driven primarily by profit, will mitigate privacy violations, provide much-needed data on reproductive health, and give people more agency over how their menstrual data is used.”</p>

<p>“The use of cycle tracking apps is at an all-time high,” said Prof Gina Neff, Executive Director of Cambridge’s Minderoo Centre. “Women deserve better than to have their menstrual tracking data treated as consumer data, but there is a different possible future.”</p>

<p>“Researchers could use this data to help answer questions about women’s health. Care providers could use this data for important information about their patients’ health. Women could get meaningful insights that they are searching for,” Neff said.</p>

<p>In the UK and EU, period tracking data is considered “special category”, as with that on genetics or ethnicity, and has more legal safeguarding. However, the report highlights how in the UK, apps designed for women's health have been used to charge women for illegally accessing abortion services</p>

<p>In the US, data about menstrual cycles has been collected by officials in an attempt to undermine abortion access. Despite this, data from CTAs are regulated simply as “general wellness” and granted no special protections.</p>

<p>“Menstrual tracking data is being used to control people’s reproductive lives,” said Felsberger. “It should not be left in the hands of private companies.”</p>

<p>Investigations by media, non-profit, and consumer groups have revealed CTAs sharing data with third parties ranging from advertisers and data brokers to tech giants such as Facebook and Google.</p>

<p>The report cites work <a href="http://chrome-extension//efaidnbmnnnibpcajpcglclefindmkaj/https:/www.privacyinternational.org/sites/default/files/2025-05/period%20tracking%20with%20recs.pdf">published last month</a> from Privacy International showing that, while the major CTA companies have updated their approach to data sharing, device information is still collected in the UK and US with “no meaningful consent”.</p>

<p>Despite data protection improvements, the report suggests that user information is still shared with third parties such as cloud-based delivery networks that move the data around, and outside developers contracted to handle app functionalities.</p>

<p>At the very least, commercial apps could include delete buttons, says Felsberger, allowing users to erase data in the app as well as the company servers, helping protect against situations – from legal to medical – where data could be used against them.&nbsp;</p>

<p>“Menstrual tracking in the US should be classed as medical data,” said Felsberger. “In the UK and EU, where this data is already afforded special category status, more focus needs to be placed on enforcing existing regulation.”</p>

<p>The report stresses the need to improve public awareness and digital literacy around period tracking. The researchers argue that schools should educate students on medical data apps and privacy, so young people are less vulnerable to health hoaxes.</p>

<p>The report ‘<a href="https://www.mctd.ac.uk/femtech-high-stakes-tracking-menstruation/">The High Stakes of Tracking Menstruation</a>’ is authored by Dr Stefanie Felsberger with a foreword by Professor Gina Neff and published by the Minderoo Centre for Technology and Democracy (MCTD).</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox OS's story from a Mozilla insider not working on the project (2024) (119 pts)]]></title>
            <link>https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html</link>
            <guid>44246518</guid>
            <pubDate>Wed, 11 Jun 2025 11:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html">https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html</a>, See on <a href="https://news.ycombinator.com/item?id=44246518">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>
Firefox OS's story from a mozilla insider not working on the project
</h3>
</div><div id="post-body-8603683558975605482">
<p><a href="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/f8WqF3f8DYaw/3tH3eMdS4oPNr889fLN3ymcy2eybKJK1XXoQiTWh.jpg"><img data-original-height="800" data-original-width="600" height="800" src="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/f8WqF3f8DYaw/3tH3eMdS4oPNr889fLN3ymcy2eybKJK1XXoQiTWh.jpg" width="600"></a></p><p>I clearly remember, but can't date it. I was working for Mozilla messaging at the time (<a href="https://en.wikipedia.org/wiki/Mozilla_Messaging">momo</a>), being the QA lead for Thunderbird. It was at the end of one of the Mozilla All-hands, maybe in 2011 or 2012. At one of the ending keynotes, we were introduced to Boot 2 Gecko. A hack that would let US - Mozilla own the platform to run a mobile browser on. At the time, the iPhone was going strong and Google was trying to catch up with Android. <a href="https://en.wikipedia.org/wiki/MeeGo">MeeGo</a> had been in development at Nokia for a while but was going nowhere even when Intel tried to help. <a href="https://en.wikipedia.org/wiki/BlackBerry">Blackberry</a> was slowly starting to die.<br></p><p>In the Silicon Valley everything was about mobile, mobile, mobile and the emerging South Easter Asian market, where people would skip computers and use smartphones to join the internet revolution. We were struggling with Chrome and the massive investment by Google to take market share. Our Firefox port on Android was having loads of issues. We were denied by Apple's policies to be present on iPhones. I was running <a href="https://play.google.com/store/search?q=firefox%20nightly&amp;c=apps">Nightly</a> on my then <a href="https://en.wikipedia.org/wiki/Samsung_Galaxy_S_series">Galaxy S Samsung</a> android powered phone. As android was open source, the idea to use it as a base for a complete phone OS that would make the web a platform emerged has an idea. At that time, Mozilla consisted of around 600 employees, all working on Firefox on the desktop. Most of our <a href="https://moztw.org/~petercpg/asknot/#!/progornoprog/softtest">huge community</a> was helping the project, making it available in many languages (Like <a href="https://en.wikipedia.org/wiki/West_Frisian_language">https://en.wikipedia.org/wiki/West_Frisian_language</a>), helping with some marketing efforts too.</p><p>B2G, or Android's version, were not Mozilla first effort to be present on Mobile. The first effort I'm aware of is <a href="https://www-archive.mozilla.org/projects/minimo/">Minimo</a>, who was targeting Palm like handheld devices.</p><p>As I said above, Mozilla's management was really afraid to miss the mobile revolution. They hired someone from the mobile industry to run the company, this led to some culture changes : no more a flat org, but a pyramidal one with middle managers. Culture became way less engineering centric, and started being a bit more top -&gt; down. Focus was now solely on B2G. This impacted my work, because it was decided that Thunderbird had no future (and no business model to support its development). That meant I changed roles in Mozilla and joined the IT organization, as I wanted to see the server side of Email (this was long before Mozilla switched to Google workplace for email). I always felt that B2G to be renamed Firefox OS, killed the team I was part of, that was working on TB. I have no insight on who made the decision and why, but that how I felt. This made me not liking B2G.</p><p>Besides becoming more like a normal company, the new CEO grew the size of our teams, added project managers, Sales people, to make sure B2G would reach a huge audience. We started making deals with phone carriers, and each of these had different requirements. We also made deals with phone makers, our Taiwan office was set up to be as close a possible as the Chinese phone makers - so we'd be at the edge of the mobile phone market. Having different deals owner made the life of the project complicated, as each of our partners had different sets of requirements for their go-to market plans. The teams were busy implementing X for partner X and Y for partner Y. Sometimes X and Y would conflict :-( With the rapid development pace, quality was omitted to reach launch deadlines. As B2G was the priority, this also meant that Firefox desktop was neglected and was slowly loosing ground to chrome. Not that we could compare the size of the devs teams, but as nobody in upper management cared about desktop, it was there, that's it. Remember that Firefox desktop was the cash machine that paid for all the rest. Without Desktop, no revenues. Then all of a sudden, by the end of 2015, Mozilla pulled the plug on B2G and got back and focused on its source of revenue, desktop. By then Mozilla had doubled in size, reaching almost 1200 employees.<br></p><p>I first got to play with Firefox OS back in 2012 when I switched to IT.</p><p><a href="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/v5lJOtoNXe9y/FzVj2Wx79IoiHYoyDrevGSbMf5OElb2geO1cnYd1.jpg"><img data-original-height="600" data-original-width="800" height="589" src="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/v5lJOtoNXe9y/FzVj2Wx79IoiHYoyDrevGSbMf5OElb2geO1cnYd1.jpg" width="611"></a></p><p>This was a <a href="https://www.devicesfaq.com/en/characteristics/turkcell-maxiplus-5">TURKCELL MaxiPLus5</a>. This was slow and unusable. I had work needs, so I never used it. The phone was available upon request for Mozilla employees willing to try B2G out. I'm not sure many were sent or used. After that, testing B2G as an employee was complicated if you were not working on the Firefox OS team. By early 2015, someone at the great idea to <a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">dogfood</a> the product and four hundred phones were made available to employees. I requested one. This was a <a href="https://www.gsmarena.com/sony_xperia_z3-6539.php">Sony Xperia</a> phone, it came with a protective cover, a mandatory mailing list to share your experience with it and file bugs if you could. I was finally getting interested with the product. Took it with me as a secondary phone. That summer I went to <a href="https://www.flickr.com/photos/hirlimann/albums/72157669622246115/with/28756027225">Mongolia</a> and the carrier/OS didn't work together, so had to use it over Wi-Fi. I managed to find a few bugs with the email client (don't know why, but that's where I found bugs, as well as in picture/metadata handling). I was not alone reporting bugs, they were getting fixed too. But nonetheless, Management decided the experiment was over. Well that's not completely true, it lives at <a href="https://www.kaiostech.com/">https://www.kaiostech.com/</a></p><p>With retrospect, I think B2G was a good idea - challenging Apple would also have been a good idea, as we had an internal demo of <a href="https://en.wikipedia.org/wiki/Gecko_(software)">gecko</a> powered Firefox for iOS. Owning the complete stack was the right approach. It gives you the power to have something that work nicely on the devices you support. I think the development approach we took was the wrong one. We were too in a hurry and that ended up neglecting Desktop. I believe we should have engaged potential partners way later, with a better, more finished and more QAed product. We should have grown to work on B2G, but not at the expense of our source of revenue. We should have dogfooded the product a lot more and once ready reached out to partners. And then start using our community to market the product and gain market share and all. The death of B2G, also meant the death of most of our engagement with ordinary people, known as the Mozilla community.<br></p><p>&nbsp;I probably forgot some details - I'll gladly edit If I feel that the things I have forgotten are important.</p><p>Vision from a developer who worked on the graphics stack, in French <a href="https://linuxfr.org/nodes/107897/comments/1643786">https://linuxfr.org/nodes/107897/comments/1643786</a><br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: S3mini – Tiny and fast S3-compatible client, no-deps, edge-ready (186 pts)]]></title>
            <link>https://github.com/good-lly/s3mini</link>
            <guid>44245577</guid>
            <pubDate>Wed, 11 Jun 2025 08:55:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/good-lly/s3mini">https://github.com/good-lly/s3mini</a>, See on <a href="https://news.ycombinator.com/item?id=44245577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">s3mini | Tiny &amp; fast S3 client for node and edge platforms.</h2><a id="user-content-s3mini--tiny--fast-s3-client-for-node-and-edge-platforms" aria-label="Permalink: s3mini | Tiny &amp; fast S3 client for node and edge platforms." href="#s3mini--tiny--fast-s3-client-for-node-and-edge-platforms"></a></p>
<p dir="auto"><code>s3mini</code> is an ultra-lightweight Typescript client (~14 KB minified, ≈15 % more ops/s) for S3-compatible object storage. It runs on Node, Bun, Cloudflare Workers, and other edge platforms. It has been tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, and MinIO. (No Browser support!)</p>
<p dir="auto">[<a href="https://github.com/good-lly/s3mini">github</a>]
[<a href="https://github.com/good-lly/s3mini/issues">issues</a>]
[<a href="https://www.npmjs.com/package/s3mini" rel="nofollow">npm</a>]</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🚀 Light and fast: averages ≈15 % more ops/s and only ~14 KB (minified, not gzipped).</li>
<li>🔧 Zero dependencies; supports AWS SigV4 (no pre-signed requests).</li>
<li>🟠 Works on Cloudflare Workers; ideal for edge computing, Node, and Bun (no browser support).</li>
<li>🔑 Only the essential S3 APIs—improved list, put, get, delete, and a few more.</li>
<li>📦 <strong>BYOS3</strong> — <em>Bring your own S3-compatible bucket</em> (tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, MinIO and Garage! Ceph and AWS are in the queue).</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tested On</h4><a id="user-content-tested-on" aria-label="Permalink: Tested On" href="#tested-on"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/good-lly/s3mini/blob/dev/testedon.png"><img src="https://github.com/good-lly/s3mini/raw/dev/testedon.png" alt="Tested On"></a></p>
<p dir="auto">Dev:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e"><img src="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e" alt="GitHub commit activity (branch)" data-canonical-src="https://img.shields.io/github/commit-activity/m/good-lly/s3mini/dev?color=greeen"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69" alt="GitHub Issues or Pull Requests" data-canonical-src="https://img.shields.io/github/issues/good-lly/s3mini"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml/badge.svg?branch=dev" alt="CodeQL Advanced"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml/badge.svg?branch=dev" alt="Test:e2e(all)"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c"><img src="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/good-lly/s3mini?style=social"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69"><img src="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69" alt="NPM Downloads" data-canonical-src="https://img.shields.io/npm/dm/s3mini"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e" alt="npm package minimized gzipped size" data-canonical-src="https://img.shields.io/bundlejs/size/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/good-lly/s3mini"></a></p>
<p dir="auto"><a href="https://github.com/good-lly/s3mini/issues/"> <img src="https://camo.githubusercontent.com/b20f68f697b27055937e1f8238eec47e2cb37f5fc74960da5dfbfe40e04af19a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e737667" alt="Contributions welcome" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg"></a></p>
<p dir="auto">Performance tests was done on local Minio instance. Your results may vary depending on environment and network conditions, so take it with a grain of salt.
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png"><img src="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png" alt="performance-image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#supported-ops">Supported Ops</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#security-notes">Security Notes</a></li>
<li><a href="#contributions-welcomed">💙 Contributions welcomed!</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Ops</h2><a id="user-content-supported-ops" aria-label="Permalink: Supported Ops" href="#supported-ops"></a></p>
<p dir="auto">The library supports a subset of S3 operations, focusing on essential features, making it suitable for environments with limited resources.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Bucket ops</h4><a id="user-content-bucket-ops" aria-label="Permalink: Bucket ops" href="#bucket-ops"></a></p>
<ul dir="auto">
<li>✅ HeadBucket (bucketExists)</li>
<li>✅ createBucket (createBucket)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Objects ops</h4><a id="user-content-objects-ops" aria-label="Permalink: Objects ops" href="#objects-ops"></a></p>
<ul dir="auto">
<li>✅ ListObjectsV2 (listObjects)</li>
<li>✅ GetObject (getObject, getObjectResponse, getObjectWithETag, getObjectRaw, getObjectArrayBuffer, getObjectJSON)</li>
<li>✅ PutObject (putObject)</li>
<li>✅ DeleteObject (deleteObject)</li>
<li>✅ HeadObject (objectExists, getEtag, getContentLength)</li>
<li>✅ listMultipartUploads</li>
<li>✅ CreateMultipartUpload (getMultipartUploadId)</li>
<li>✅ completeMultipartUpload</li>
<li>✅ abortMultipartUpload</li>
<li>✅ uploadPart</li>
<li>❌ CopyObject: Not implemented (tbd)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>



<blockquote>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Environment Support Notice</strong></p>
<p dir="auto">This library is designed to run in environments like <strong>Node.js</strong>, <strong>Bun</strong>, and <strong>Cloudflare Workers</strong>. It does <strong>not support browser environments</strong> due to the use of Node.js APIs and polyfills.</p>
<p dir="auto"><strong>Cloudflare Workers:</strong> To enable built-in Node.js Crypto API, add the <code>nodejs_compat</code> compatibility flag to your Wrangler configuration file. This also enables <code>nodejs_compat_v2</code> as long as your compatibility date is <code>2024-09-23</code> or later. <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#nodejs-compatibility-flag" rel="nofollow">Learn more about the Node.js compatibility flag and v2</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { s3mini, sanitizeETag } from 's3mini';

const s3client = new s3mini({
  accessKeyId: config.accessKeyId,
  secretAccessKey: config.secretAccessKey,
  endpoint: config.endpoint,
  region: config.region,
});

// Basic bucket ops
let exists: boolean = false;
try {
  // Check if the bucket exists
  exists = await s3client.bucketExists();
} catch (err) {
  throw new Error(`Failed bucketExists() call, wrong credentials maybe: ${err.message}`);
}
if (!exists) {
  // Create the bucket based on the endpoint bucket name
  await s3client.createBucket();
}

// Basic object ops
// key is the name of the object in the bucket
const smallObjectKey: string = 'small-object.txt';
// content is the data you want to store in the object
// it can be a string or Buffer (recommended for large objects)
const smallObjectContent: string = 'Hello, world!';

// check if the object exists
const objectExists: boolean = await s3client.objectExists(smallObjectKey);
let etag: string | null = null;
if (!objectExists) {
  // put/upload the object, content can be a string or Buffer
  // to add object into &quot;folder&quot;, use &quot;folder/filename.txt&quot; as key
  const resp: Response = await s3client.putObject(smallObjectKey, smallObjectContent);
  // you can also get etag via getEtag method
  // const etag: string = await s3client.getEtag(smallObjectKey);
  etag = sanitizeETag(resp.headers.get('etag'));
}

// get the object, null if not found
const objectData: string | null = await s3client.getObject(smallObjectKey);
console.log('Object data:', objectData);

// get the object with ETag, null if not found
const response2: Response = await s3mini.getObject(smallObjectKey, { 'if-none-match': etag });
if (response2) {
  // ETag changed so we can get the object data and new ETag
  // Note: ETag is not guaranteed to be the same as the MD5 hash of the object
  // ETag is sanitized to remove quotes
  const etag2: string = sanitizeETag(response2.headers.get('etag'));
  console.log('Object data with ETag:', response2.body, 'ETag:', etag2);
} else {
  console.log('Object not found or ETag does match.');
}

// list objects in the bucket, null if bucket is empty
// Note: listObjects uses listObjectsV2 API and iterate over all pages
// so it will return all objects in the bucket which can take a while
// If you want to limit the number of objects returned, use the maxKeys option
// If you want to list objects in a specific &quot;folder&quot;, use &quot;folder/&quot; as prefix
// Example s3client.listObjects({&quot;/&quot; &quot;myfolder/&quot;})
const list: object[] | null = await s3client.listObjects();
if (list) {
  console.log('List of objects:', list);
} else {
  console.log('No objects found in the bucket.');
}

// delete the object
const wasDeleted: boolean = await s3client.deleteObject(smallObjectKey);

// Multipart upload
const multipartKey = 'multipart-object.txt';
const large_buffer = new Uint8Array(1024 * 1024 * 15); // 15 MB buffer
const partSize = 8 * 1024 * 1024; // 8 MB
const totalParts = Math.ceil(large_buffer.length / partSize);
// Beware! This will return always a new uploadId
// if you want to use the same uploadId, you need to store it somewhere
const uploadId = await s3client.getMultipartUploadId(multipartKey);
const uploadPromises = [];
for (let i = 0; i < totalParts; i++) {
  const partBuffer = large_buffer.subarray(i * partSize, (i + 1) * partSize);
  // upload each part
  // Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel
  // but be careful with the number of parallel uploads, it can cause throttling
  // or errors if you upload too many parts at once
  // You can also use generator functions to upload parts in batches
  uploadPromises.push(s3client.uploadPart(multipartKey, uploadId, partBuffer, i + 1));
}
const uploadResponses = await Promise.all(uploadPromises);
const parts = uploadResponses.map((response, index) => ({
  partNumber: index + 1,
  etag: response.etag,
}));
// Complete the multipart upload
const completeResponse = await s3client.completeMultipartUpload(multipartKey, uploadId, parts);
const completeEtag = completeResponse.etag;

// List multipart uploads
// returns object with uploadId and key
const multipartUploads: object = await s3client.listMultipartUploads();
// Abort the multipart upload
const abortResponse = await s3client.abortMultipartUpload(multipartUploads.key, multipartUploads.uploadId);

// Multipart download
// lets test getObjectRaw with range
const rangeStart = 2048 * 1024; // 2 MB
const rangeEnd = 8 * 1024 * 1024 * 2; // 16 MB
const rangeResponse = await s3client.getObjectRaw(multipartKey, false, rangeStart, rangeEnd);
const rangeData = await rangeResponse.arrayBuffer();"><pre><span>import</span> <span>{</span> <span>s3mini</span><span>,</span> <span>sanitizeETag</span> <span>}</span> <span>from</span> <span>'s3mini'</span><span>;</span>

<span>const</span> <span>s3client</span> <span>=</span> <span>new</span> <span>s3mini</span><span>(</span><span>{</span>
  <span>accessKeyId</span>: <span>config</span><span>.</span><span>accessKeyId</span><span>,</span>
  <span>secretAccessKey</span>: <span>config</span><span>.</span><span>secretAccessKey</span><span>,</span>
  <span>endpoint</span>: <span>config</span><span>.</span><span>endpoint</span><span>,</span>
  <span>region</span>: <span>config</span><span>.</span><span>region</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// Basic bucket ops</span>
<span>let</span> <span>exists</span>: <span>boolean</span> <span>=</span> <span>false</span><span>;</span>
<span>try</span> <span>{</span>
  <span>// Check if the bucket exists</span>
  <span>exists</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>bucketExists</span><span>(</span><span>)</span><span>;</span>
<span>}</span> <span>catch</span> <span>(</span><span>err</span><span>)</span> <span>{</span>
  <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>`Failed bucketExists() call, wrong credentials maybe: <span><span>${</span><span>err</span><span>.</span><span>message</span><span>}</span></span>`</span><span>)</span><span>;</span>
<span>}</span>
<span>if</span> <span>(</span><span>!</span><span>exists</span><span>)</span> <span>{</span>
  <span>// Create the bucket based on the endpoint bucket name</span>
  <span>await</span> <span>s3client</span><span>.</span><span>createBucket</span><span>(</span><span>)</span><span>;</span>
<span>}</span>

<span>// Basic object ops</span>
<span>// key is the name of the object in the bucket</span>
<span>const</span> <span>smallObjectKey</span>: <span>string</span> <span>=</span> <span>'small-object.txt'</span><span>;</span>
<span>// content is the data you want to store in the object</span>
<span>// it can be a string or Buffer (recommended for large objects)</span>
<span>const</span> <span>smallObjectContent</span>: <span>string</span> <span>=</span> <span>'Hello, world!'</span><span>;</span>

<span>// check if the object exists</span>
<span>const</span> <span>objectExists</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>objectExists</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>let</span> <span>etag</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>null</span><span>;</span>
<span>if</span> <span>(</span><span>!</span><span>objectExists</span><span>)</span> <span>{</span>
  <span>// put/upload the object, content can be a string or Buffer</span>
  <span>// to add object into "folder", use "folder/filename.txt" as key</span>
  <span>const</span> <span>resp</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>putObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>smallObjectContent</span><span>)</span><span>;</span>
  <span>// you can also get etag via getEtag method</span>
  <span>// const etag: string = await s3client.getEtag(smallObjectKey);</span>
  <span>etag</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>resp</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
<span>}</span>

<span>// get the object, null if not found</span>
<span>const</span> <span>objectData</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data:'</span><span>,</span> <span>objectData</span><span>)</span><span>;</span>

<span>// get the object with ETag, null if not found</span>
<span>const</span> <span>response2</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3mini</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>{</span> <span>'if-none-match'</span>: <span>etag</span> <span>}</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>response2</span><span>)</span> <span>{</span>
  <span>// ETag changed so we can get the object data and new ETag</span>
  <span>// Note: ETag is not guaranteed to be the same as the MD5 hash of the object</span>
  <span>// ETag is sanitized to remove quotes</span>
  <span>const</span> <span>etag2</span>: <span>string</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>response2</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data with ETag:'</span><span>,</span> <span>response2</span><span>.</span><span>body</span><span>,</span> <span>'ETag:'</span><span>,</span> <span>etag2</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object not found or ETag does match.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// list objects in the bucket, null if bucket is empty</span>
<span>// Note: listObjects uses listObjectsV2 API and iterate over all pages</span>
<span>// so it will return all objects in the bucket which can take a while</span>
<span>// If you want to limit the number of objects returned, use the maxKeys option</span>
<span>// If you want to list objects in a specific "folder", use "folder/" as prefix</span>
<span>// Example s3client.listObjects({"/" "myfolder/"})</span>
<span>const</span> <span>list</span>: <span>object</span><span>[</span><span>]</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listObjects</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>list</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'List of objects:'</span><span>,</span> <span>list</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'No objects found in the bucket.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// delete the object</span>
<span>const</span> <span>wasDeleted</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>deleteObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>

<span>// Multipart upload</span>
<span>const</span> <span>multipartKey</span> <span>=</span> <span>'multipart-object.txt'</span><span>;</span>
<span>const</span> <span>large_buffer</span> <span>=</span> <span>new</span> <span>Uint8Array</span><span>(</span><span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>15</span><span>)</span><span>;</span> <span>// 15 MB buffer</span>
<span>const</span> <span>partSize</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span><span>;</span> <span>// 8 MB</span>
<span>const</span> <span>totalParts</span> <span>=</span> <span>Math</span><span>.</span><span>ceil</span><span>(</span><span>large_buffer</span><span>.</span><span>length</span> <span>/</span> <span>partSize</span><span>)</span><span>;</span>
<span>// Beware! This will return always a new uploadId</span>
<span>// if you want to use the same uploadId, you need to store it somewhere</span>
<span>const</span> <span>uploadId</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getMultipartUploadId</span><span>(</span><span>multipartKey</span><span>)</span><span>;</span>
<span>const</span> <span>uploadPromises</span> <span>=</span> <span>[</span><span>]</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>totalParts</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>const</span> <span>partBuffer</span> <span>=</span> <span>large_buffer</span><span>.</span><span>subarray</span><span>(</span><span>i</span> <span>*</span> <span>partSize</span><span>,</span> <span>(</span><span>i</span> <span>+</span> <span>1</span><span>)</span> <span>*</span> <span>partSize</span><span>)</span><span>;</span>
  <span>// upload each part</span>
  <span>// Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel</span>
  <span>// but be careful with the number of parallel uploads, it can cause throttling</span>
  <span>// or errors if you upload too many parts at once</span>
  <span>// You can also use generator functions to upload parts in batches</span>
  <span>uploadPromises</span><span>.</span><span>push</span><span>(</span><span>s3client</span><span>.</span><span>uploadPart</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>partBuffer</span><span>,</span> <span>i</span> <span>+</span> <span>1</span><span>)</span><span>)</span><span>;</span>
<span>}</span>
<span>const</span> <span>uploadResponses</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>uploadPromises</span><span>)</span><span>;</span>
<span>const</span> <span>parts</span> <span>=</span> <span>uploadResponses</span><span>.</span><span>map</span><span>(</span><span>(</span><span>response</span><span>,</span> <span>index</span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
  <span>partNumber</span>: <span>index</span> <span>+</span> <span>1</span><span>,</span>
  <span>etag</span>: <span>response</span><span>.</span><span>etag</span><span>,</span>
<span>}</span><span>)</span><span>)</span><span>;</span>
<span>// Complete the multipart upload</span>
<span>const</span> <span>completeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>completeMultipartUpload</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>parts</span><span>)</span><span>;</span>
<span>const</span> <span>completeEtag</span> <span>=</span> <span>completeResponse</span><span>.</span><span>etag</span><span>;</span>

<span>// List multipart uploads</span>
<span>// returns object with uploadId and key</span>
<span>const</span> <span>multipartUploads</span>: <span>object</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listMultipartUploads</span><span>(</span><span>)</span><span>;</span>
<span>// Abort the multipart upload</span>
<span>const</span> <span>abortResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>abortMultipartUpload</span><span>(</span><span>multipartUploads</span><span>.</span><span>key</span><span>,</span> <span>multipartUploads</span><span>.</span><span>uploadId</span><span>)</span><span>;</span>

<span>// Multipart download</span>
<span>// lets test getObjectRaw with range</span>
<span>const</span> <span>rangeStart</span> <span>=</span> <span>2048</span> <span>*</span> <span>1024</span><span>;</span> <span>// 2 MB</span>
<span>const</span> <span>rangeEnd</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>2</span><span>;</span> <span>// 16 MB</span>
<span>const</span> <span>rangeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObjectRaw</span><span>(</span><span>multipartKey</span><span>,</span> <span>false</span><span>,</span> <span>rangeStart</span><span>,</span> <span>rangeEnd</span><span>)</span><span>;</span>
<span>const</span> <span>rangeData</span> <span>=</span> <span>await</span> <span>rangeResponse</span><span>.</span><span>arrayBuffer</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">For more check <a href="https://github.com/good-lly/s3mini/blob/dev/USAGE.md">USAGE.md</a> file, examples and tests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security Notes</h2><a id="user-content-security-notes" aria-label="Permalink: Security Notes" href="#security-notes"></a></p>
<ul dir="auto">
<li>The library masks sensitive information (access keys, session tokens, etc.) when logging.</li>
<li>Always protect your AWS credentials and avoid hard-coding them in your application (!!!). Use environment variables. Use environment variables or a secure vault for storing credentials.</li>
<li>Ensure you have the necessary permissions to access the S3 bucket and perform operations.</li>
<li>Be cautious when using multipart uploads, as they can incur additional costs if not managed properly.</li>
<li>Authors are not responsible for any data loss or security breaches resulting from improper usage of the library.</li>
<li>If you find a security vulnerability, please report it to us directly via email. For more details, please refer to the <a href="https://github.com/good-lly/s3mini/blob/dev/SECURITY.md">SECURITY.md</a> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions welcomed!</h2><a id="user-content-contributions-welcomed" aria-label="Permalink: Contributions welcomed!" href="#contributions-welcomed"></a></p>
<p dir="auto">Contributions are greatly appreciated! If you have an idea for a new feature or have found a bug, we encourage you to get involved:</p>
<ul dir="auto">
<li>
<p dir="auto"><em>Report Issues</em>: If you encounter a problem or have a feature request, please open an issue on GitHub. Include as much detail as possible (environment, error messages, logs, steps to reproduce, etc.) so we can understand and address the issue.</p>
</li>
<li>
<p dir="auto"><em>Pull Requests</em>: We welcome PRs! If you want to implement a new feature or fix a bug, feel free to submit a pull request to the latest <code>dev branch</code>. For major changes, it's a good idea to discuss your plans in an issue first.</p>
</li>
<li>
<p dir="auto"><em>Lightweight Philosophy</em>: When contributing, keep in mind that s3mini aims to remain lightweight and dependency-free. Please avoid adding heavy dependencies. New features should provide significant value to justify any increase in size.</p>
</li>
<li>
<p dir="auto"><em>Community Conduct</em>: Be respectful and constructive in communications. We want a welcoming environment for all contributors. For more details, please refer to our <a href="https://github.com/good-lly/s3mini/blob/dev/CODE_OF_CONDUCT.md">CODE_OF_CONDUCT.md</a>. No one reads it, but it's there for a reason.</p>
</li>
</ul>
<p dir="auto">If you figure out a solution to your question or problem on your own, please consider posting the answer or closing the issue with an explanation. It could help the next person who runs into the same thing!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/good-lly/s3mini/blob/dev/LICENSE.md">LICENSE.md</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsor This Project</h2><a id="user-content-sponsor-this-project" aria-label="Permalink: Sponsor This Project" href="#sponsor-this-project"></a></p>
<p dir="auto">Developing and maintaining s3mini (and other open-source projects) requires time and effort. If you find this library useful, please consider sponsoring its development. Your support helps ensure I can continue improving s3mini and other projects. Thank you!</p>
<p dir="auto"><a href="https://github.com/sponsors/good-lly"><img src="https://camo.githubusercontent.com/a12b8f68983edf77bb6d60e2b29a80bcfafd07082c88b0a5decd3c0f3b1dd672/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462539322542385f4769744875622d53706f6e736f722d6666363962343f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" alt="Become a Sponsor" data-canonical-src="https://img.shields.io/badge/%F0%9F%92%B8_GitHub-Sponsor-ff69b4?logo=github&amp;logoColor=white"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Left-Pad (2024) (277 pts)]]></title>
            <link>https://azerkoculu.com/posts/left-pad</link>
            <guid>44245166</guid>
            <pubDate>Wed, 11 Jun 2025 07:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://azerkoculu.com/posts/left-pad">https://azerkoculu.com/posts/left-pad</a>, See on <a href="https://news.ycombinator.com/item?id=44245166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>8 years have passed since the <a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident">the left-pad incident</a> happened. It's been good for me to avoid this subject for focusing on actual work. Also, silence is gold. On the other hand, left-pad is seen as a notable event mentioned in books, so I'll share more info and thoughts about what exactly happened.</p>
<p>In most of 2016, I spent almost every weekend camping in remote areas without any signal. If you're curious how I felt when making the decision to unpublish; it was a choice made during self-reflection in nature.</p>
<p>Not driven by logic, anger, or greed.<br>
It was a decision guided by my heart.</p>
<p>And it came from a simple principle: if NPM breaks its own rules to remove one of my packages, they should remove all of them.</p>
<p>Not that I'm an inflexible "rules" person — quite the opposite. The spirit behind the rules matter more than the rules themselves to me. In a different context, we could be asking NPM to take down a package without owner's permission for a "good" reason. However, in this situation, a company like <a href="https://en.wikipedia.org/wiki/Kik_Messenger">Kik Messenger</a> was just posturing and exercising power over the open source community NPM was built on, sending threats such as "we'll bang on your door" and "take down your accounts". This is the context in which NPM went against the rules they wrote themselves, to serve something they deemed "higher" than the soul of their company and the community.</p>
<p>I was not afraid of Kik's threats, <br>
but NPM was afraid of losing Kik.</p>
<p>Many people oversimplify this event, framing it as "an angry man protested corporate interests". This narrative shows us three things; first, they didn't look at the dates of the emails. They don't understand the timeline. Second, they can't relate to standing your ground in a high pressure situation involving threats. And third, they haven't read Al-Ghazali yet, don't quite understand how (free) people make decisions.</p>
<p>There was nothing sudden or unexpected for NPM. I asked NPM to remove my modules, waited their response. I didn't set any deadline; NPM had the opportunity to adjust their APIs and tooling to make this transition smooth. Interestingly, they chose to provide me a script that removes all my packages at once. On the NPM side, I observed general condescending attitude towards developers, which led them make series of unreasonable decisions and ultimately blame me for all the cost.</p>
<p>Most of my open source work followed Unix philosophy, so the packages did one thing at a time. There was 350+ of them. In the surface, it looked like nobody used them. NPM didn't show usage stats, and there was almost no activity on Github. As a user, it was impossible to know the impact of unpublishing packages; but I still don't understand why NPM didn't take the time to find out if any of my modules were widely used and consider ways to handle the unpublishing without breaking anything.</p>
<h2>8 Years Later</h2>
<p>Several months after the left-pad incident, I quit my job and left US permanently, spent a year in Morocco, Jordan, Türkiye and Indonesia. I walked trails like Lycian Way, found new camping spots nobody knows about.</p>
<p>Left-pad was like a "death" and "re-birth" moment for me. The part of me passionate about open-source was dead, and something new took over. Now, I'm passionate about business, marketing, running companies / teams in different ways, as much as I'm about programming.</p>
<p>Life goes on.</p>
<p>Thanks for reading.</p>
<p><img src="https://live.staticflickr.com/65535/51992852851_0ba272a470_b.jpg" alt=""></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's the end of observability as we know it (and I feel fine) (270 pts)]]></title>
            <link>https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</link>
            <guid>44243050</guid>
            <pubDate>Wed, 11 Jun 2025 00:34:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine">https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</a>, See on <a href="https://news.ycombinator.com/item?id=44243050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>In a really broad sense, the history of observability tools over the past couple of decades have been about a pretty simple concept: how do we make terabytes of heterogeneous telemetry data comprehensible to human beings? New Relic did this for the Rails revolution, Datadog did it for the rise of AWS, and Honeycomb led the way for <a href="https://opentelemetry.io/" target="_blank" rel="noreferrer noopener">OpenTelemetry</a>.</p>
<p>The loop has been the same in each case. New abstractions and techniques for software development and deployment gain traction, those abstractions make software more accessible by hiding complexity, and that complexity requires new ways to monitor and measure what’s happening. We build tools like dashboards, adaptive alerting, and dynamic sampling. All of these help us compress the sheer amount of <em>stuff</em> happening into something that’s comprehensible to our human intelligence.</p>
<p>In AI, I see the death of this paradigm. It’s already real, it’s already here, and it’s going to fundamentally change the way we approach systems design and operation in the future.</p>
<div>
<hr>
<div>
<p>New to Honeycomb? Get your <strong>free</strong> account today.</p>

</div>

<hr>
</div>
<h2 id="useful">LLMs are just universal function approximators, but it turns out that those are really useful</h2>
<p>I’m going to tell you&nbsp;a story. It’s about this picture:</p>
<figure><img decoding="async" width="1024" height="808" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png" alt="LLMs are just universal function approximators, but it turns out that those are really useful" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-768x606.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-640x505.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-348x275.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image.png 1460w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>If you’ve ever seen a Honeycomb demo, you’ve probably seen this image. We love it, because it’s not only a great way to show a real-world problem—it’s something that plays well to our core strengths of enabling investigatory loops. Those little peaks you see in the heatmap represent slow requests in a <a href="https://www.honeycomb.io/frontend-observability">frontend</a> service that rise over time before suddenly resetting. They represent a small percentage of your users experiencing poor performance—and we all know what this means in the real world: lost sales, poor experience, and general malaise at the continued enshittification of software.</p>
<p>In a Honeycomb demo, we show you how easy it is to use our UI to understand what those spikes actually mean. You draw a box around them, and we run <a href="https://www.honeycomb.io/bubbleup">BubbleUp</a> to detect anomalies by analyzing the trace data that’s backing this visualization, showing you what’s similar and what’s different between the spikes and the baseline. Eventually, you can drill down to the specific service and even method call that’s causing the problem. It’s a great demo, and it really shows the power of our platform.</p>
<p>Last Friday, I showed a demo at our weekly internal Demo Day. It started with what I just showed you, and then I ran a single prompt through an AI agent that read as follows:</p>
<p><strong>Please investigate the odd latency spikes in the frontend service that happen every four hours or so, and tell me why they’re happening.</strong></p>
<figure><img decoding="async" width="1024" height="366" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png" alt="" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-768x275.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1536x549.png 1536w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-640x229.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-348x124.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>The screenshot here elides the remainder of the response from the LLM (please find the entire text at the end of this post), but there’s a few things I want to call out. First, this wasn’t anything too special. The agent was something I wrote myself in a couple of days; it’s just an LLM calling tools in a loop. The model itself is off-the-shelf <a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noreferrer noopener">Claude Sonnet 4</a>. The integration with Honeycomb is our new <a href="https://www.honeycomb.io/blog/mcp-easy-as-1-2-3">Model Context Protocol (MCP)</a> server. It took 80 seconds, made eight tool calls, and not only did it tell me why those spikes happened, it figured it out in a pretty similar manner to how we’d tell you to do it with BubbleUp.</p>
<p>This isn’t a contrived example. I basically asked the agent the same question we’d ask you in a demo, and the agent figured it out with no additional prompts, training, or guidance. It effectively zero-shot a real-world scenario.</p>
<p>And it did it for <strong>sixty cents.</strong></p>
<h2 id="youcandoit">If I can do this, so can you. So can anyone.</h2>
<p>I want to be clear, this was perhaps the <em>least </em>optimized version of this workflow. Inference costs are only going down, and we can certainly make our MCP server more efficient. There are ways to reduce the amount of input tokens even more. We can play around with more tailored aggregations and function calls that return LLM-optimized query results. It’s an exciting new era!</p>
<p>It also should serve as a wakeup call to the entire industry. This is a seismic shift in how we should conceptualize observability tooling. <strong>If your product’s value proposition is nice graphs and easy instrumentation, you are </strong><strong><em>le cooked</em></strong><strong>. </strong>An LLM commoditizes the analysis piece, OpenTelemetry commoditizes the instrumentation piece. <strong>The moats are emptying.</strong></p>
<p>I’m not gonna sit here and say this destroys the idea of <a href="https://www.honeycomb.io/blog/ai-where-in-the-loop-should-humans-go">humans being involved in the process</a>, though. I don’t think that’s true. The rise of the cloud didn’t destroy the idea of IT. The existence of Rails doesn’t mean we don’t need server programmers. Productivity increases <em>expand the map</em>. There’ll be more software, of all shapes and sizes. We’re going to need <em>more</em> of everything.</p>
<p>The question, then, is: what does this require from us? Where does observability sit in a world where code is cheap, refactors are cheap, and analysis is a constant factor?</p>
<h2 id="feedback">Fast feedback is the only feedback</h2>
<p>I’m gonna put a marker out there: the only thing that really matters is fast, tight feedback loops at every stage of development and operations. AI thrives on speed—it’ll outrun you every time. To succeed, you need tools that move at the speed of AI as well. The slower your analysis engine, the worse your results. LLMs will generate, test, and discard hypotheses faster than you ever can. They might be wrong a dozen times before they get it right, but again, <em>we’re talking about fractional pennies here</em>.&nbsp;</p>
<p>I don’t see this as a reason to despair, but as something to celebrate. The things that AI-assisted development and operations can lead us to are the things <em>we’ve always been about</em>. Honeycomb’s entire modus operandi is predicated on fast feedback loops, collaborative knowledge sharing, and treating everything as an experiment. We’re built for the future that’s here today, on a platform that allows us to be the best tool for tomorrow.</p>
<p>The exact contours of what this industry looks like in two, five, or ten years are out of my view. I don’t think there’s anyone that can credibly tell you what things will look like. I’m also not really in the business of making predictions. What I can say, though, is that I think we’re going to see a spectrum of AI assistance in development and operational tasks across the entire <a href="https://www.honeycomb.io/blog/honeycomb-fit-software-development-lifecycle">software development lifecycle</a>:</p>
<ul>
<li>AI agents can run passively as you write and deploy code, suggesting fixes to improve code quality based on real-world observations from production.</li>
<li>Emergent system behavior can be detected and addressed by AI, with unattended agents identifying impacts to users, investigating them, compiling reports, and presenting those to human operators for approval and tweaks before making changes.</li>
<li>Bleeding-edge organizations can adopt entirely unattended SWE/SRE roles, powered by tools like Honeycomb and friends, optimizing and writing software to meet business needs and goals.</li>
</ul>
<p>In all of these cases, what matters most is <em>speed</em>. Traditional monitoring and observability tools don’t have what it takes to compete in this world. Pretty dashboards, pre-built alerts, and magic integrations aren’t sufficient for the future we’re building today. Sub-second query performance, unified data storage, and collaborative workflows between humans and AI are the keys to success in the future of observability.</p>
<p>Let’s go do something cool.</p>
<p><em>Addendum:</em> Here’s the full output from the investigation screenshot earlier.</p>
<p><strong>Perfect! Now I have a clear picture of what’s happening. Based on my investigation, I can explain the latency spikes you’re seeing in your frontend service.</strong></p>
<hr>
<p><strong>🔍 Investigation Results: Frontend Latency Spikes</strong></p>
<hr>
<p><strong>Root Cause Identified: Checkout Service Performance Issues</strong></p>
<p>The latency spikes you’re experiencing every ~4 hours are <strong>NOT actually periodic</strong>—this appears to be a <strong>load testing or synthetic traffic pattern</strong> rather than a scheduled system issue. Here’s what I found:</p>
<hr>
<p><strong>📊 Key Findings:</strong></p>
<ol>
<li><strong>Primary Culprit: Checkout Operations</strong><strong><br></strong>
<ul>
<li>/cart/checkout endpoint: <strong>P95 = 3.7 seconds</strong> (vs normal ~500ms)</li>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li><strong>Maximum latency observed: 7.6 seconds</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Spike Pattern Analysis:</strong><strong><br></strong>
<ul>
<li>Spikes are concentrated in <strong>checkout operations only</strong><strong><br></strong></li>
<li>Other endpoints remain relatively stable</li>
<li><strong>485 slow checkout operations</strong> in the last 24 hours</li>
<li>Pattern suggests <strong>load testing or batch checkout processing</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Downstream Service Impact:</strong><strong><br></strong>
<ul>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.6 seconds</strong><strong><br></strong></li>
<li>getDiscounts operation: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li>ShippingService/ShipOrder: <strong>P95 = 4.0 seconds</strong><strong><br></strong></li>
</ul>
</li>
</ol>
<hr>
<p><strong>🕵️ Evidence from Trace Analysis:</strong></p>
<p>Looking at the sample traces, I can see:</p>
<ul>
<li><strong>User Agent Pattern:</strong> python-requests/2.28.2 and python-requests/2.31.0</li>
<li><strong>Consistent Client IPs:</strong> 52.45.250.5 and internal IPs</li>
<li><strong>High Cart Values:</strong> $6,861, $7,036, $259 (suggesting test scenarios)</li>
<li><strong>Same User IDs:</strong> Multiple requests from user 20109 with identical cart totals</li>
</ul>
<hr>
<p><strong>💡 Why This is Happening:</strong></p>
<p><strong>Most Likely Cause: Load Testing or Synthetic Monitoring</strong></p>
<ul>
<li>The pattern suggests <strong>automated testing</strong> rather than real user traffic</li>
<li>Python requests library indicates <strong>scripted traffic</strong><strong><br></strong></li>
<li>High cart values and repeated scenarios point to <strong>test data</strong><strong><br></strong></li>
<li>The “every 4 hours” pattern is likely your <strong>load testing schedule</strong><strong><br></strong></li>
</ul>
<p><strong>Technical Root Cause:</strong></p>
<ul>
<li><strong>Checkout service bottleneck</strong> during high-volume operations</li>
<li><strong>Discount calculation delays</strong> (4.2s P95)</li>
<li><strong>Shipping service delays</strong> during order processing</li>
<li>Possible <strong>database contention</strong> or <strong>external API timeouts</strong><strong><br></strong></li>
</ul>
<hr>
<p><strong>🎯 Recommendations:</strong></p>
<ol>
<li><strong>Immediate Actions:</strong><strong><br></strong>
<ul>
<li><strong>Verify if this is load testing</strong>—check with your testing team</li>
<li><strong>Monitor checkout service resources</strong> during spike periods</li>
<li><strong>Check discount service performance</strong> and external dependencies</li>
</ul>
</li>
<li><strong>Performance Optimization:</strong><strong><br></strong>
<ul>
<li><strong>Optimize discount calculation logic</strong><strong><br></strong></li>
<li><strong>Add caching</strong> for shipping calculations</li>
<li><strong>Implement circuit breakers</strong> for external payment/shipping APIs</li>
<li><strong>Scale checkout service</strong> if this represents real user load</li>
</ul>
</li>
<li><strong>Monitoring Improvements:</strong><strong><br></strong>
<ul>
<li><strong>Separate alerts</strong> for synthetic vs. real user traffic</li>
<li><strong>Add SLI tracking</strong> specifically for checkout operations</li>
<li><strong>Monitor downstream dependencies</strong> (payment, shipping, discount services)</li>
</ul>
</li>
</ol>
<hr>
<p><strong>🚨 Action Items:</strong></p>
<ol>
<li>Confirm if the 4-hour pattern matches your load testing schedule</li>
<li>If it’s real traffic, investigate checkout service capacity and dependencies</li>
<li>Consider implementing checkout-specific performance monitoring and alerting</li>
</ol>
<hr>
<p><strong>The good news is that your other services are performing well—this appears to be an isolated issue with the checkout flow, likely exacerbated by load testing scenarios.</strong></p>
<hr>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Gentle Singularity (234 pts)]]></title>
            <link>https://blog.samaltman.com/the-gentle-singularity</link>
            <guid>44241549</guid>
            <pubDate>Tue, 10 Jun 2025 21:17:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.samaltman.com/the-gentle-singularity">https://blog.samaltman.com/the-gentle-singularity</a>, See on <a href="https://news.ycombinator.com/item?id=44241549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_body_2202875"><p>We are past the event horizon; the takeoff has started. Humanity is close to building digital superintelligence, and at least so far it’s much less weird than it seems like it should be.</p><p>Robots are not yet walking the streets, nor are most of us talking to AI all day. People still die of disease, we still can’t easily go to space, and there is a lot about the universe we don’t understand.</p><p>And yet, we have recently built systems that are smarter than people in many ways, and are able to significantly amplify the output of people using them. The least-likely part of the work is behind us; the scientific insights that got us to systems like GPT-4 and o3 were hard-won, but will take us very far.</p><p>AI will contribute to the world in many ways, but the gains to quality of life from AI driving faster scientific progress and increased productivity will be enormous; the future can be vastly better than the present. Scientific progress is the biggest driver of overall progress; it’s hugely exciting to think about how much more we could have.</p><p>In some big sense, ChatGPT is already more powerful than any human who has ever lived. Hundreds of millions of people rely on it every day and for increasingly important tasks; a small new capability can create a hugely positive impact; a small misalignment multiplied by hundreds of millions of people can cause a great deal of negative impact.</p><p>2025 has seen the arrival of agents that can do real cognitive work; writing computer code will never be the same. 2026 will likely see the arrival of systems that can figure out novel insights. 2027 may see the arrival of robots that can do tasks in the real world.</p><p>A lot more people will be able to create software, and art. But the world wants a lot more of both, and experts will probably still be much better than novices, as long as they embrace the new tools. Generally speaking, the ability for one person to get much more done in 2030 than they could in 2020 will be a striking change, and one many people will figure out how to benefit from.</p><p>In the most important ways, the 2030s may not be wildly different. People will still love their families, express their creativity, play games, and swim in lakes.<br></p><p>But in still-very-important-ways, the 2030s are likely going to be wildly different from any time that has come before. We do not know how far beyond human-level intelligence we can go, but we are about to find out.</p><p>In the 2030s, intelligence and energy—ideas, and the ability to make ideas happen—are going to become wildly abundant. These two have been the fundamental limiters on human progress for a long time; with abundant intelligence and energy (and good governance), we can theoretically have anything else.</p><p>Already we live with incredible digital intelligence, and after some initial shock, most of us are pretty used to it. Very quickly we go from being amazed that AI can generate a beautifully-written paragraph to wondering when it can generate a beautifully-written novel; or from being amazed that it can make live-saving medical diagnoses to wondering when it can develop the cures; or from being amazed it can create a small computer program to wondering when it can create an entire new company. This is how the singularity goes: wonders become routine, and then table stakes.</p><p>We already hear from scientists that they are two or three times more productive than they were before AI. Advanced AI is interesting for many reasons, but perhaps nothing is quite as significant as the fact that we can use it to do faster AI research. We may be able to discover new computing substrates, better algorithms, and who knows what else. If we can do a decade’s worth of research in a year, or a month, then the rate of progress will obviously be quite different.</p><p>From here on, the tools we have already built will help us find further scientific insights and aid us in creating better AI systems. Of course this isn’t the same thing as an AI system completely autonomously updating its own code, but nevertheless this is a larval version of recursive self-improvement.</p><p>There are other self-reinforcing loops at play. The economic value creation has started a flywheel of compounding infrastructure buildout to run these increasingly-powerful AI systems. And robots that can build other robots (and in some sense, datacenters that can build other datacenters) aren’t that far off.&nbsp;</p><p>If we have to make the first million humanoid robots the old-fashioned way, but then they can operate the entire supply chain—digging and refining minerals, driving trucks, running factories, etc.—to build more robots, which can build more chip fabrication facilities, data centers, etc, then the rate of progress will obviously be quite different.</p><p>As datacenter production gets automated, the cost of intelligence should eventually converge to near the cost of electricity. (People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.)</p><p>The rate of technological progress will keep accelerating, and it will continue to be the case that people are capable of adapting to almost anything. There will be very hard parts like whole classes of jobs going away, but on the other hand the world will be getting so much richer so quickly that we’ll be able to seriously entertain new policy ideas we never could before. We probably won’t adopt a new social contract all at once, but when we look back in a few decades, the gradual changes will have amounted to something big.</p><p>If history is any guide, we will figure out new things to do and new things to want, and assimilate new tools quickly (job change after the industrial revolution is a good recent example). Expectations will go up, but capabilities will go up equally quickly, and we’ll all get better stuff. We will build ever-more-wonderful things for each other. People have a long-term important and curious advantage over AI: we are hard-wired to care about other people and what they think and do, and we don’t care very much about machines.</p><p>A subsistence farmer from a thousand years ago would look at what many of us do and say we have fake jobs, and think that we are just playing games to entertain ourselves since we have plenty of food and unimaginable luxuries. I hope we will look at the jobs a thousand years in the future and think they are very fake jobs, and I have no doubt they will feel incredibly important and satisfying to the people doing them.</p><p>The rate of new wonders being achieved will be immense. It’s hard to even imagine today what we will have discovered by 2035; maybe we will go from solving high-energy physics one year to beginning space colonization the next year; or from a major materials science breakthrough one year to true high-bandwidth brain-computer interfaces the next year. Many people will choose to live their lives in much the same way, but at least some people will probably decide to “plug in”.</p><p>Looking forward, this sounds hard to wrap our heads around. But probably living through it will feel impressive but manageable. From a relativistic perspective, the singularity happens bit by bit, and the merge happens slowly. We are climbing the long arc of exponential technological progress; it always looks vertical looking forward and flat going backwards, but it’s one smooth curve. (Think back to 2020, and what it would have sounded like to have something close to AGI by 2025, versus what the last 5 years have actually been like.)</p><p>There are serious challenges to confront along with the huge upsides. We do need to solve the safety issues, technically and societally, but then it’s critically important to widely distribute access to superintelligence given the economic implications. The best path forward might be something like:</p><ol>
<li><p>Solve the alignment problem, meaning that we can robustly guarantee that we get AI systems to learn and act towards what we collectively really want over the long-term (social media feeds are an example of misaligned AI; the algorithms that power those are incredible at getting you to keep scrolling and clearly understand your short-term preferences, but they do so by exploiting something in your brain that overrides your long-term preference).</p></li>
<li><p>Then focus on making superintelligence cheap, widely available, and not too concentrated with any person, company, or country. Society is resilient, creative, and adapts quickly. If we can harness the collective will and wisdom of people, then although we’ll make plenty of mistakes and some things will go really wrong, we will learn and adapt quickly and be able to use this technology to get maximum upside and minimal downside. Giving users a lot of freedom, within broad bounds society has to decide on, seems very important. The sooner the world can start a conversation about what these broad bounds are and how we define collective alignment, the better.</p></li>
</ol><p>We (the whole industry, not just OpenAI) are building a brain for the world. It will be extremely personalized and easy for everyone to use; we will be limited by good ideas. For a long time, technical people in the startup industry have made fun of “the idea guys”; people who had an idea and were looking for a team to build it. It now looks to me like they are about to have their day in the sun.</p><p>OpenAI is a lot of things now, but before anything else, we are a superintelligence research company. We have a lot of work in front of us, but most of the path in front of us is now lit, and the dark areas are receding fast. We feel extraordinarily grateful to get to do what we do.</p><p>Intelligence too cheap to meter is well within grasp. This may sound crazy to say, but if we told you back in 2020 we were going to be where we are today, it probably sounded more crazy than our current predictions about 2030.</p><p>May we scale smoothly, exponentially and uneventfully through superintelligence.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chatbots are replacing Google's search, devastating traffic for some publishers (193 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</link>
            <guid>44241407</guid>
            <pubDate>Tue, 10 Jun 2025 21:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141">https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</a>, See on <a href="https://news.ycombinator.com/item?id=44241407">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a 3D printed VTOL drone (378 pts)]]></title>
            <link>https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</link>
            <guid>44241278</guid>
            <pubDate>Tue, 10 Jun 2025 20:47:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can">https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</a>, See on <a href="https://news.ycombinator.com/item?id=44241278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div id="youtube2-e7AIKGDrlgs" data-attrs="{&quot;videoId&quot;:&quot;e7AIKGDrlgs&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/e7AIKGDrlgs?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><br><span>I made this 130 mile capable VTOL drone in only 90 days. It can fly for 3 hours on a single charge. That would make it one of the longest range and endurance 3D printed VTOLs in the world.</span></p><p>This is the thing I'm most proud of building to date!</p><p><span>Before this project, I was a total CAD, 3D printing and aerodynamic modeling beginner. I had only </span><a href="https://www.tsungxu.com/p/first-vtol-design-build-and-flight" rel="">built and flown one VTOL before</a><span>. My CAD skills were restricted to extrusions of basic sketches. My Bambu A1 3D printer was only one month old, and I had never tried printing foaming PLA. I had never used any aerodynamic modeling tools to simulate lift-to-drag ratios or stability curves like Cm-alpha.</span></p><p>This video edit ended up shorter than I planned. Being my first Youtube video with significant post production effort, I underestimated the work required to make a longer in-depth video with voiceover, edited footage, etc.</p><p>Because of that, I did not discuss dozens of project challenges like how I chose the design parameters, learnt airframe CAD design, sourced components, improved foaming PLA print quality, in-depth power loss troubleshooting, and many more challenges this project faced.</p><p>If you want more details on my design and build process, let me know!</p><p><em><span>P.S. Reid Hoffman just </span><a href="https://x.com/reidhoffman/status/1932511491506262413" rel="">quote tweeted</a><span> my viral </span><a href="https://x.com/tsungxu/status/1932224925450043845" rel="">video post on X</a><span>:</span></em></p><blockquote><p><em>A century ago, you needed at least a brother and a bicycle shop to pioneer flight. Today, you just need the right toolchain…</em></p></blockquote><p>Incredibly humbling! </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A “Course” as an MCP Server (186 pts)]]></title>
            <link>https://mastra.ai/course</link>
            <guid>44241202</guid>
            <pubDate>Tue, 10 Jun 2025 20:36:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastra.ai/course">https://mastra.ai/course</a>, See on <a href="https://news.ycombinator.com/item?id=44241202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page"><nav><div><p><a href="https://mastra.ai/"><img alt="mastra" loading="lazy" width="24" height="23" decoding="async" data-nimg="1" src="https://mastra.ai/logo.svg"></a></p><ul data-open="false"><li><a href="https://mastra.ai/blog">Blog</a></li><li><a href="https://mastra.ai/university">University</a></li><li><a target="_blank" href="https://mastra.ai/docs">Docs</a></li></ul></div></nav><main><div><div><ul><li>Explained by Shane Thomas CPO of Mastra</li><li>Led by an MCP Agent</li><li>3 Lessons and more soon</li><li>12 Hours to compete</li><li>Interactive progress</li></ul><div><p>In this hands-on course, you'll learn to develop and deploy AI agents using Mastra, including how to equip them with tools, memory, and MCP. But here's the twist: you're not just learning about agents, you're learning from one.</p><p>This course is delivered entirely inside your agentic code editor, where your code agent guides you step-by-step, writes code with you, and helps you build real, working agents from the ground up.</p></div></div><div><div><p><span>Begin Mastra 101</span></p><div><p><span>1</span></p><p>Choose your editor and paste the command to install the mcp server automatically</p></div></div><div><p><span></span>Course Outline<span></span></p><div><div><p><span>Build your first agent</span><span><a href="https://youtu.be/buqexw-Xkvk" target="_blank">Watch lesson <!-- -->1<!-- --> intro</a></span></p><div><p>Build and deploy your first AI agent that can read data from external sources, use custom tools, and remember previous interactions. Set up your development environment, test your agent in a live playground, and ship your agent to production. This section covers the basics needed to create a working, memory and tool-enabled agent.</p><div><p>introduction to mastra</p><p>what is mastra</p><p>verifying installation</p></div></div></div><div><p><span>Adding Tools and MCP</span><span><a href="https://youtu.be/J5WVCPKTRxA" target="_blank">Watch lesson <!-- -->2<!-- --> intro</a></span></p><div><p>Add new abilities to your agent by connecting it to external services using MCP servers. Search across MCP registries to easily integrate tools for email, social media, GitHub, news, and local files, without writing any custom code.</p><div><p>introduction to mcp</p><p>installing mcp</p><p>setting up mcp configuration</p></div></div></div><div><p><span>Adding Memory</span><span><a href="https://youtu.be/Io2xtouOsMk" target="_blank">Watch lesson <!-- -->3<!-- --> intro</a></span></p><div><p>Add memory to your agent so it can remember previous conversations, user preferences, and important details. Learn how to configure conversation history, semantic recall, and working memory to create agents that provide more relevant and personalized responses.</p><div><p>understanding memory</p><p>why memory matters</p><p>installing memory</p></div></div></div></div><div><h3 id="faqs"><a href="#faqs">#</a>Faqs</h3></div></div></div></div></main></div></div>]]></description>
        </item>
    </channel>
</rss>