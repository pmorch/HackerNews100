<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 27 Dec 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Janet Jackson had the power to crash laptop computers (2022) (112 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</link>
            <guid>46403291</guid>
            <pubDate>Sat, 27 Dec 2025 17:16:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994">https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</a>, See on <a href="https://news.ycombinator.com/item?id=46403291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-106994">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <p>A colleague of mine shared a story from Windows XP product support. A major computer manufacturer discovered that playing the music video for Janet Jackson’s “<a href="https://en.wikipedia.org/wiki/Rhythm_Nation">Rhythm Nation</a>” would crash certain models of laptops. I would not have wanted to be in the laboratory that they must have set up to investigate this problem. Not an artistic judgement.</p>
<p>One discovery during the investigation is that playing the music video also crashed some of their competitors’ laptops.</p>
<p>And then they discovered something extremely weird: Playing the music video on one laptop caused a laptop sitting nearby to crash, even though that other laptop wasn’t playing the video!</p>
<p>What’s going on?</p>
<p>It turns out that the song contained one of the natural resonant frequencies for the model of 5400 rpm laptop hard drives that they and other manufacturers used.</p>
<p>The manufacturer worked around the problem by adding a custom filter in the audio pipeline that detected and removed the offending frequencies during audio playback.</p>
<p>And I’m sure they put a digital version of a “Do not remove” sticker on that audio filter. (Though I’m worried that in the many years since the workaround was added, nobody remembers why it’s there. Hopefully, their laptops are not still carrying this audio filter to protect against damage to a model of hard drive they are no longer using.)</p>
<p>And of course, no story about natural resonant frequencies can pass without a reference to <a href="https://www.history.com/this-day-in-history/tacoma-narrows-bridge-collapses"> the collapse of the Tacoma Narrows Bridge in 1940</a>.¹</p>
<p><b>Related</b>: <a href="https://www.youtube.com/watch?v=tDacjrSCeq4"> Shouting in the Datacenter</a>.</p>
<p><b>Bonus chatter</b>: <a href="https://twitter.com/WindowsDocs/status/1558114944738103297"> Video version of this story</a> and a <a href="https://twitter.com/WindowsDocs/status/1558115433449852929"> Twitter poll</a>.</p>
<p>Also, <a href="https://twitter.com/osterman">Larry Osterman</a> had a similar experience with <a href="https://twitter.com/osterman/status/1558676353196494850"> a specific game that crashed a prototype PC</a>.</p>
<p><b>Follow-up</b>: <a href="https://devblogs.microsoft.com/oldnewthing/20220920-00/?p=107201"> Janet Jackson had the power to crash laptop computers, follow-up</a>.</p>
<p>¹ <b>Follow-up 2</b>: Yes, I know that the Tacoma Narrows Bridge collapse was not the result of resonance, but I felt I had to drop the reference to forestall the “You forgot to mention the Tacoma Narrows Bridge!” comments.</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><div><p><img src="https://devblogs.microsoft.com/oldnewthing/wp-content/uploads/sites/38/2019/02/RaymondChen_5in-150x150.jpg" alt="Raymond Chen"></p></div><p>Raymond has been involved in the evolution of Windows for more than 30 years. In 2003, he began a Web site known as The Old New Thing which has grown in popularity far beyond his wildest imagination, a development which still gives him the heebie-jeebies. The Web site spawned a book, coincidentally also titled The Old New Thing (Addison Wesley 2007). He occasionally appears on the Windows Dev Docs Twitter account to tell stories which convey no useful information.</p></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia just paid $20B for a company that missed its revenue target by 75% (160 pts)]]></title>
            <link>https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a</link>
            <guid>46403041</guid>
            <pubDate>Sat, 27 Dec 2025 16:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a">https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a</a>, See on <a href="https://news.ycombinator.com/item?id=46403041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>If you’ve heard “Grok” thrown around lately, you’re probably thinking of Elon’s chatbot from xAI. We’re not talking about that one. That model isn’t particularly good, but its whole value prop is being politically incorrect so you can get it to say edgy things.</p><p>The company Nvidia bought is Groq (with a Q). Totally different beast.</p><p><span>If you’ve used any high quality LLM, you’ve noticed it takes </span><em>a while to generate a response.</em><span> Especially for something rapid fire like a conversation, you want high quality AND speed. But speed is often what gets sacrificed. There’s always that “thinking... gathering my notes... taking some time to form the best response” delay.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7ppM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7ppM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 424w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 848w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1272w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png" width="1456" height="777" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:777,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3954485,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.drjoshcsimmons.com/i/182706966?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7ppM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 424w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 848w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1272w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>My default Zellij layout so I can parallelize Claude Code tasks.</figcaption></figure></div><p><span>Groq specialized in hardware and software that makes this way faster. They created a new type of chip called an LPU (Language Processing Unit). It’s based on an ASIC, an application specific integrated circuit. If that’s confusing, don’t worry about it. It’s just </span><strong>a processor that does a specific type of task really well.</strong></p><p>So imagine you’re talking to Gemini and it takes a couple seconds to respond. Now imagine it responded instantly, like 10 or 100 times faster. That’s the problem Groq was solving.</p><p>To go one level deeper on LPUs versus GPUs (the processors most LLMs run on, typically Nvidia cards): those GPU calculations have to access a lot of things in memory. Nvidia’s chips depend on HBM, high bandwidth memory. But LPUs use something called SRAM that’s much faster to reference.</p><p>Think about it like this. Your wife has a grocery list for you. You go to the store but forget the list. Every time you’re in an aisle, you have to call her on the phone: “Hey, do I need anything from the bread aisle?” Get the bread. Put the phone down. Go to the next aisle. “Hey, do I need anything from canned goods?” And so on through produce, meat, pick up the beer, check out, get home. Very inefficient.</p><p>Groq’s approach is like you just took the list to the store. Get to a new aisle, look at the list. Next aisle, look at the list. Much faster than a phone call.</p><p>That’s the key difference. Nvidia GPUs are phoning out every time they hit a new aisle. Groq’s LPUs mean the shopper has the list in their pocket.</p><p>Groq’s main offering is GroqCloud. An engineer like me isn’t going to go out and buy an LPU (I don’t even know if they’re commercially available). What I’m going to do is, if I need lightning fast response in an application I’m building, I’ll use GroqCloud. That inference happens at an extremely fast rate, running on LPUs in a data center somewhere.</p><p>Their value prop is: fast, cheap, low energy.</p><p>Where they’ve been falling short is they mostly use open source models. Llama, Mistral, GPT-OSS (OpenAI’s open source offering). These are decent models, but nowhere near the quality of something like Anthropic’s Opus 4.5 or Gemini 3 Pro.</p><p>Groq positioned themselves for hardcore use cases where milliseconds matter. One of their big industry fits is real time data analysis for Formula 1. When your driver’s out there on the track, you don’t have time to send a query to Gemini and wait 20 to 30 seconds to figure out if you should pit this guy for new tires this lap. You want something like Groq which does pretty good analysis, really really really fast.</p><p>This is insider baseball you’re going to miss from mainstream news headlines. The media is paying attention to Grok (Elon’s chatbot), not Groq (the chip company). This isn’t a conspiracy, it’s just that the only people aware of Groq are software developers and tech nuts like me.</p><p>This is a canary in the coal mine for worse things to come in 2026.</p><p>About a year ago, Groq announced a $1.5 billion infrastructure investment deal with Saudi Arabia. They also secured a $750 million Series D funding round. These are crazy multiples even for a software company that’s somewhat leveraged in hardware. Bubble level projections.</p><p>To visualize $1.5 billion: if you cashed that check out in $100 bills and stacked them one on top of another, it would reach a five story building. For ordinary plebeians like us, at the average US salary of around $75K, you’d need to work 20,000 years to earn that.</p><p>At that time, the company was valued at $2 billion. Hello bubble.</p><p><span>Then in maybe one of the best rug pulls of all time, in July they quietly changed their revenue projections to $500 million.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-182706966" href="https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a#footnote-1-182706966" target="_self" rel="">1</a></span><span> A 75% cut in four months. I’ve never seen anything like that since the 2008 financial crisis.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.youtube.com/watch?v=2po-s2yOCcg&quot;,&quot;text&quot;:&quot;Prefer to Watch the Video&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.youtube.com/watch?v=2po-s2yOCcg" rel=""><span>Prefer to Watch the Video</span></a></p><p>This was a company valued at $2 billion, enough that the government of Saudi Arabia was investing at that valuation. Then they took a 75% haircut four months later without anything major happening.</p><p>If it can happen to Groq, who else can it happen to? Nvidia? Intel?</p><p>The rumors started flying on Christmas Eve. Confirmed the 26th: Nvidia will be buying Groq, their key product line, and key personnel including the CEO, for $20 billion.</p><p><strong>Let’s walk through that again:</strong></p><ul><li><p><strong>February: $2 billion valuation</strong></p></li><li><p><strong>July: Down to $500 million</strong></p></li><li><p><strong>December: Nvidia buys them for $20 billion after they took a 75% haircut</strong></p></li></ul><p>What is going on? This is a bubble.</p><p>The only explanation is this is a fear purchase. Groq was promising faster, cheaper, more efficient, less electricity use for their chips. But they couldn’t scale fast enough to compete with the vendor lock in and buddy system Nvidia has going.</p><p>Nvidia’s buying them with their insanely inflated war chest. They don’t want a chunk taken out of their market share. They can’t afford to take that chance. So it’s like they’re just saying: “Shut up, take the $20 billion, walk away from this project.”</p><p>This is a sign that in order to succeed, Nvidia needs a monopoly on the market. Otherwise they would not pay ten times the company’s valuation that was then decreased by 75%. This is a desperate move to consolidate the market into “you have to go with us.”</p><p>Saudi Arabia didn’t keep that $1.5 billion sitting around. They redirected it to Nvidia and AMD instead. Nvidia still gets paid ofc.</p><p><span>Smaller competitors like Cerebras and Inflection, doing things in Groq’s space or exploring different architectures for AI inference, are canceling IPOs, dropping like flies, seeking emergency funding. The chatter I’m hearing from VCs and friends in that world? </span><strong>Ain’t nobody buying it right now.</strong></p><p>Google made their own chip. Microsoft and Amazon are racing to make competition chips that run on less electricity, are more efficient, faster. But no matter what anybody does, the market is consolidating around the Nvidia monopoly for AI hardware. Engineers like me and architects at large enterprises are trying to escape this. Once they consolidate enough of the market, they can set their price for chips and usage. If you don’t own them, you go through Oracle or some cloud computing service, and they can charge whatever they want because there will be no competitors. Even a competitor having a rough time but getting some traction? They just buy them out for $20 billion because with this monopoly going, that’s pocket change.</p><p>$20 billion is a rounding error to Nvidia.</p><p><strong>The AI infrastructure boom ran on one large errant assumption: that power is cheap and plentiful. </strong><span>That assumption was very, very wrong.</span></p><p><span>We’ve seen parts of the power grid fail, go unmaintained. </span><a href="https://www.palladiummag.com/2023/06/01/complex-systems-wont-survive-the-competence-crisis/" rel="">There’s a great article on Palladium called “The Competency Crisis” that explains some of what’s going on in the US right now.</a><span> Electricity is now the bottleneck. It’s the constraint. Too expensive or you can’t get enough of it. Who’s paying these costs? The tech companies aren’t. Trump is meeting with Jensen Huang, with Sam Altman. He hasn’t been over my place lately. He hasn’t invited you to the White House to talk about how you can’t afford groceries and eggs cost three times what they did a few years ago.</span></p><p>No.</p><p><strong>You and I are going to pay to subsidize the electricity constraints.</strong></p><p>US data centers are using about 4% of all electricity right now. If growth continues at the same rate, in ten years they’ll use 9%. Almost one tenth of all electricity generated in the US.</p><p>I’m not much of an environmentalist. But even someone like me, pretty jaded to some amount of waste because of industrialization, something like this actually makes my stomach turn. In places with lots of data centers, AI heavy regions, they’re experiencing about a 250% increase in energy costs compared to just five years ago. That’s huge. Even compared to grocery costs, which are out of control. At least with food you have alternatives. Red meat too expensive? Buy chicken. Chicken too expensive? Buy eggs. But electricity? You have to run electricity. It’s a public utility. You can’t just “use a lot less” when your bill goes up 2.5x.</p><p>Let me walk you through what happens. Some business development guy from Oracle wants to build a new data center in Rural America. A year before it’s even built, they meet with city officials, maybe the governor. They grease the right people. Get legislation passed with the utility companies that says they’ll pay a preferential rate for electricity because they’re going to use a shit ton.</p><p>They’re Oracle or Nvidia, they’re good for it. They pay five years upfront. Then the utility decides what to do with the rest of the electricity. The grid is strained. They want everyone else to use less. They can’t just tell them to use less, so they keep raising the price until naturally you just can’t afford to use more.</p><p>You turn the lights off. Turn the TV off. Get those LED bulbs that disrupt your sleep. Do everything to keep electricity costs down. But you and I are left holding the bag. The data center folks aren’t paying for that, they paid upfront at a preferential rate.</p><p>Senate Democrats are allegedly investigating this. I’ll believe it when I see it. These tech companies have their hooks so far into influential politicians. It’s a vote grab. I’d be happy to be wrong about that, but I know I’m not going to be.</p><p>I talked about this in my last piece on the AI bubble. This computer communism that’s going on, pricing you out of personal computing, keeping it all in the family of these tech companies. But with the Groq deal confirmed, it’s gone one step further. Nvidia is not just selling chips anymore. They are lending money to customers who buy the chips. They are artificially inflating demand.</p><p>It’s like if I run a small grocery store and I need to show good signal to investors that I’m bringing cash in the door. So I go downtown during farmer’s market and give everybody a $20 voucher to use at my store. I take a wholesale hit when they come in and buy stuff. But what I can show is: “Hey, people are coming in and spending money. Look at the revenue. Give me more venture capital money.”</p><p>This can’t work infinitely, for the same reason any perpetual motion machine can’t work.</p><p><span>Back in September, Nvidia announced a $100 billion investment in OpenAI. Coming through in $10 billion tranches over time. And it’s not equity, it’s lease agreements for Nvidia’s chips. Literally paying them to pay themselves.There’s probably some tax shenanigans going on there since they’re typically offering $xxx in </span><em>leases of their chips</em><span> to the company they’re “lending” to. Assuming they do this in part because the depreciation on the physical asset (the chips) can be written off on their taxes somehow. They’re essentially incentivizing another person to use them with money. Even OpenAI’s CFO has admitted, quote: “Most of the money will go back to Nvidia.”</span></p><p>They’re playing both sides. In the OpenAI case, they’re financing software that uses their chips. But they’re also getting their hooks into data centers.</p><p>CoreWeave: they have something like 7% share in the company. Worth $3 billion. That’s funded by GPU revenue from CoreWeave.</p><p>Lambda: another data center operator. That’s a $1.3 billion spending deal. Renting Nvidia’s own chips back from them.</p><p>They’ve pledged to invest £2 billion across British startups, which of course are going to go back to Nvidia chips one way or another.</p><p>In 2024, they invested about $1 billion across startups and saw a $24 billion return in chip spend. They 24x’d their billion dollar investment in one year. Nvidia has more power than the Fed right now. More power than the president over the economy. They have their hand on the knob of the economy. They can choose how fast or slow they want it to go. If the Nvidia cash machine stops printing, if they stop funding startups, data centers, hardware companies, software companies, that whole part of the economy slows way down and maybe crashes if investors get spooked.</p><p>I’m waiting for somebody to blow the whistle on this. I’m not a finance guy, so it’s strange I’m even talking about it. But their entire success story for the next couple years hinges on their $100 billion investment in OpenAI, which they’re expecting to bring back about $300 billion in chip purchases.</p><p><span>It’s vendor financing. It’s sweetening the pot on your own deals. </span><strong>I cannot believe more people are not talking about this.</strong></p><p>OpenAI, the leader of this space, the company whose CEO Sam Altman is invited to the White House numerous times, probably has a direct line to Trump, a lot of the economy hinges on this guy’s strategy, opinions, and public statements.</p><p><strong>And he runs a company that is not profitable. </strong><span>Actually insane if you think about it. All he’s done with that company, from an economics point of view, is rack up debt. Spent more than he’s earned.</span></p><p>By that metric, I’m richer than Sam Altman. Not in net worth. But if I consider myself a business and the fact that I bring in any salary at all, even a dollar a year, that would make me earn more than Sam Altman, who has only lost money. In the next few years, they’re expected to burn something like $75 billion a year. Just set that money on fire. I have a credit card with no preset spending limit, but I assume if I run up a $75 billion charge it’s going to get denied. By their projections, they think they’ll become profitable around 2029/2030, and they need $200 billion in annual revenue to offset their debts and losses.</p><p>To visualize $200 billion: if you cashed that out in $100 bills and stacked them, it would reach halfway to the International Space Station. That’s how much they’d have to make every single year to just be profitable. Not be a massively successful company. Just to not spend more than they earn.</p><ul><li><p>2024: Spent $5 billion, earned $3.7 billion. Spending $1.35 for every dollar earned.</p></li><li><p>2025: Set $8 billion on fire (after profits).</p></li><li><p>2028 (projected): Will lose $74 billion that year. Just lose it.</p></li><li><p>Cumulative losses through 2029: $143 billion.</p></li></ul><p>They’d need to make $200 billion in a year to offset that. Are they including interest? I have no idea how these things work, but in simple terms: they’re spending a lot more money than they make.</p><p>Groq was kind of a one off because Nvidia panic bought the competition. But they also need to figure out how to get prices down or they can’t keep this money machine moving.</p><p>Groq is probably one of the last companies that caught a good lifeboat off a sinking ship.</p><p>What we’re going to see this year: it’ll start small, but get major. A company first, then multiple larger ones, that had a 2025 valuation, will go to raise. They’ll do a 409A evaluation. A bunch of smart analysts will say what it’s worth to investors. And you’re going to see the valuation drop. They won’t be able to raise money.</p><p>Then the shit is really going to start to hit the fan.</p><p>The dominoes will start falling. That’s probably what kicks off the actual pop, and it’s imminent. Any day now.</p><p>Part of the big gamble they’ve sold investors is: we’ve got to replace you. The worker. That’s why we need to spend so much money, go into debt. It’ll all be worth it because then we won’t have to pay people to do these jobs anymore.</p><p>We’re going to continue to see massive labor displacement. To a degree this is a shell game, an investor illusion. What these larger enterprise companies are hoping to do: cut a lot of folks, have big layoffs, say it’s because AI is replacing the jobs.</p><p>In some cases they’re right. Data entry, for example. I don’t mean to be mean if you do data entry for a living, but there are models very good at that now. You need someone to manage the work, spot check it. But it’s kind of a job AI can do.</p><p>Like how grocery stores have automatic checkout lines now with one person monitoring six or eight of them. So some of that’s real. A lot of it isn’t though.</p><p>They cut a bunch of American workers under the guise that AI’s replacing workers. A lot of these megacorp execs are actually convinced of it. Americans are expensive, especially tech workers.</p><p>Then they see: damn, maybe we could have cut some, but not as many. We got greedy. Now our services are failing in production. AWS in Northern Virginia is flaky, going down again. That just happened, by the way, direct result of these layoffs.</p><p>So instead they think, “We’ll look globally! Spin up a campus in Hyderabad!” Pay them way less. The cost of living is less there, they expect less. Bring them over on H-1B when needed. I’ve written about the H-1B program. This is nothing against the individuals on that program. I’ve worked with very talented H-1Bs, and some very inferior ones, just like American citizens.</p><p>But the corporate sleight of hand is something like this, we can get those H-1B visas, and they’re not going to ask for pesky stuff like Sunday off for church. We can put them on call 24/7 and they can’t say no because if they do, we kick them back to their country. Same thing that’s happened with migrant labor in farming over the past century. Even if it doesn’t look like it on the surface, corporations know that they can pay H-1B employees less than American citizens. If a US citizen and H-1B recipient both make $120k but the H-1B works double the hours because they have no room to push back and are under threat of being sent home, they are making 50% less than the American per-hour.</p><p>Amazon/AWS: 14,000 laid off.</p><p>Microsoft: 15,000 total just in 2025.</p><p>Salesforce: My favorite one. 4,000 customer support roles cut. And the CEO is gloating about it in interviews. So great that he can replace workers!</p><p><a href="https://timesofindia.indiatimes.com/technology/tech-news/after-laying-off-4000-employees-and-automating-with-ai-agents-salesforce-executives-admit-we-were-more-confident-about-/articleshow/126121875.cms" rel="">Now Salesforce is admitting that they fucked up. They cut too many people. </a></p><p><span>I’ve been on the other side of this when I was an engineering director at a Fortune 500 company. They were </span><em>neurotic</em><span> about tracking AI use. Spending an exorbitant amount of money on shitty AI tools. Like tools from a year ago. GPT-4o in the GPT-5 era, in the Anthropic dominance era. More or less useless.</span></p><p>Not only would they monitor all usage across employees, specifically who’s using it how much, they could see every single message being sent to the AI. So theoretically you could check somebody’s queries and do a performance evaluation based on where you perceive them to be.</p><p>Pretty creepy.</p><p>They’re using yesterday’s tools because of regulation and compliance, blowing an absurd amount of money. The CEO just sees a lot going out the door: “I thought these were supposed to save money, what’s going on here?” So his lieutenants have to get a grip on it, monitor everything. Even at Amazon this is being included in performance reviews, how much they’re using AI.</p><p>That’s why if you’re a software developer wondering why your boss is on you about using AI tools. They’re probably getting pressure from their bosses. I want my engineers to be as productive as possible. I think AI is probably part of that tool belt for everyone at this point. But is tracking it really the best way? I’m going to gauge performance on metrics, how you interacted with the team, what you shipped. It puts the cart before the horse to say you’re paid by how much you use these AI tools. If I’m a developer, I’m just spinning up a script to send lorem ipsum text 24 hours a day, get maximum ratings because I used GPT-3.5 the most.</p><p>MIT did a study in summer 2025. They’re saying 95% of companies report zero measurable ROI on AI products.</p><p>Actually not that crazy if you consider that a lot of them did layoffs and subbed in AI. That’s just going according to plan in my estimation.</p><p>They estimate about $30 to $40 billion has been spent on enterprise AI. That’s the money your JPMorgan Chase is spending for their engineers to use Claude Code or whatever dated tool they have access to.</p><p>The market heard that signal. When that study came out:</p><ul><li><p>Nvidia crashed 3.5%</p></li><li><p>Palantir down 10%</p></li><li><p>Nasdaq in general down 1.2%</p></li></ul><p>That last one isn’t encouraging for an AI bubble pop because it indicates this is a big part of the economy.</p><p>January through February: Maybe down valuations, just a very flat market without much growth.</p><p>Q1 to Q2: We’ll start to see a couple businesses, or maybe one major one at first like a domino starting to fall, not able to raise capital at their 2025 valuation. We’ll see valuations go down. VCs will be like: “Not touching it, not giving them more money, cutting our losses.”</p><p>Then timing a little more indeterminate but these things will happen quickly in succession:</p><ul><li><p>Credit markets start to tighten</p></li><li><p>Debt refinancing pressure builds up in the system</p></li><li><p>Nvidia revises its revenue guidance to something at least vaguely linked to reality</p></li></ul><p>And that’s when the big reckoning begins.</p><p>I want to be clear. AI is not going anywhere. It’s going to continue being a mainstream part of the world. I like a lot of these tools, I think they’re very helpful.</p><p>But the talking heads have really promised us the world. It’s clear the technology cannot deliver above and beyond what it’s doing now.</p><p>We’ve seen progress of these models slow at an exponential rate of slowing over the past few releases. Each release is better, but the gap between the current release and the last release is much smaller than it used to be.</p><p>Because of that, a lot of these AI companies are going to survive, but their valuations are going to get giga slashed.</p><p>Valuations of $500 billion for OpenAI, $42 billion for Anthropic are unsustainable. We’re going to actually see them become unsustainable in 2026 as they’re eventually cut. Smaller companies will face those slashes first. But it’s coming for the major AI labs as well.</p><p>This is good news, honestly. This AI hype has really turned tech into something different these days, different than it used to be. While I don’t feel AI is going anywhere, I do feel we’ll get a little more back to normal once this bubble pops and resolves.</p><p><em>What do you think? Drop a comment below. Subscribe if you found this interesting.</em></p></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Floor796 (257 pts)]]></title>
            <link>https://floor796.com/</link>
            <guid>46401612</guid>
            <pubDate>Sat, 27 Dec 2025 13:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://floor796.com/">https://floor796.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46401612">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases open-source model that instantly turns 2D photos into 3D views (329 pts)]]></title>
            <link>https://github.com/apple/ml-sharp</link>
            <guid>46401539</guid>
            <pubDate>Sat, 27 Dec 2025 12:58:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/ml-sharp">https://github.com/apple/ml-sharp</a>, See on <a href="https://news.ycombinator.com/item?id=46401539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sharp Monocular View Synthesis in Less Than a Second</h2><a id="user-content-sharp-monocular-view-synthesis-in-less-than-a-second" aria-label="Permalink: Sharp Monocular View Synthesis in Less Than a Second" href="#sharp-monocular-view-synthesis-in-less-than-a-second"></a></p>
<p dir="auto"><a href="https://apple.github.io/ml-sharp/" rel="nofollow"><img src="https://camo.githubusercontent.com/cafdcb5612b1ab28528d47af9245604f8f7b0792562c7c5151fff90340a3d6cc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d506167652d677265656e" alt="Project Page" data-canonical-src="https://img.shields.io/badge/Project-Page-green"></a>
<a href="https://arxiv.org/abs/2512.10685" rel="nofollow"><img src="https://camo.githubusercontent.com/dba21084a03f7471ad5ab1cbe4b2eeb9c6c4333dde6dae06cd437bc9b9163cf7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323531322e31303638352d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg"></a></p>
<p dir="auto">This software project accompanies the research paper: <em>Sharp Monocular View Synthesis in Less Than a Second</em>
by <em>Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, Amaël Delaunoy,
Tian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun</em>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-sharp/blob/main/data/teaser.jpg"><img src="https://github.com/apple/ml-sharp/raw/main/data/teaser.jpg" alt=""></a></p>
<p dir="auto">We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">We recommend to first create a python environment:</p>
<div data-snippet-clipboard-copy-content="conda create -n sharp python=3.13"><pre><code>conda create -n sharp python=3.13
</code></pre></div>
<p dir="auto">Afterwards, you can install the project using</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
<p dir="auto">To test the installation, run</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Using the CLI</h2><a id="user-content-using-the-cli" aria-label="Permalink: Using the CLI" href="#using-the-cli"></a></p>
<p dir="auto">To run prediction:</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians
</code></pre></div>
<p dir="auto">The model checkpoint will be downloaded automatically on first run and cached locally at <code>~/.cache/torch/hub/checkpoints/</code>.</p>
<p dir="auto">Alternatively, you can download the model directly:</p>
<div data-snippet-clipboard-copy-content="wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt"><pre><code>wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt
</code></pre></div>
<p dir="auto">To use a manually downloaded checkpoint, specify it with the <code>-c</code> flag:</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt
</code></pre></div>
<p dir="auto">The results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS <code>.ply</code> files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering trajectories (CUDA GPU only)</h3><a id="user-content-rendering-trajectories-cuda-gpu-only" aria-label="Permalink: Rendering trajectories (CUDA GPU only)" href="#rendering-trajectories-cuda-gpu-only"></a></p>
<p dir="auto">Additionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the <code>--render</code> option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto">Please refer to the paper for both quantitative and qualitative evaluations.
Additionally, please check out this <a href="https://apple.github.io/ml-sharp/" rel="nofollow">qualitative examples page</a> containing several video comparisons against related work.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work useful, please cite the following paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\&quot;{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}"><pre><span>@inproceedings</span>{<span>Sharp2025:arxiv</span>,
  <span>title</span>      = <span><span>{</span>Sharp Monocular View Synthesis in Less Than a Second<span>}</span></span>,
  <span>author</span>     = <span><span>{</span>Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun<span>}</span></span>,
  <span>journal</span>    = <span><span>{</span>arXiv preprint arXiv:2512.10685<span>}</span></span>,
  <span>year</span>       = <span><span>{</span>2025<span>}</span></span>,
  <span>url</span>        = <span><span>{</span>https://arxiv.org/abs/2512.10685<span>}</span></span>,
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Our codebase is built using multiple opensource contributions, please see <a href="https://github.com/apple/ml-sharp/blob/main/ACKNOWLEDGEMENTS">ACKNOWLEDGEMENTS</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Please check out the repository <a href="https://github.com/apple/ml-sharp/blob/main/LICENSE">LICENSE</a> before using the provided code and
<a href="https://github.com/apple/ml-sharp/blob/main/LICENSE_MODEL">LICENSE_MODEL</a> for the released models.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ez FFmpeg – Video editing in plain English (284 pts)]]></title>
            <link>http://npmjs.com/package/ezff</link>
            <guid>46400251</guid>
            <pubDate>Sat, 27 Dec 2025 08:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://npmjs.com/package/ezff">http://npmjs.com/package/ezff</a>, See on <a href="https://news.ycombinator.com/item?id=46400251">Hacker News</a></p>
Couldn't get http://npmjs.com/package/ezff: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Health Care Software Company Sentenced for $1B Fraud Conspiracy (121 pts)]]></title>
            <link>https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy</link>
            <guid>46398752</guid>
            <pubDate>Sat, 27 Dec 2025 03:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy">https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy</a>, See on <a href="https://news.ycombinator.com/item?id=46398752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>An Arizona man was sentenced Friday to 15 years in prison and ordered to pay more than $452 million in restitution for conspiring to defraud Medicare and other federal health care benefit programs of more than $1 billion by operating a platform that generated false doctors’ orders used to support fraudulent claims for various medical items.</p><p>“This just sentence is the result of one of the largest telemarketing Medicare fraud cases ever tried to verdict,” said Acting Assistant Attorney General Matthew R. Galeotti of the Justice Department’s Criminal Division.&nbsp;“Telemedicine scammers who use junk mailers, spam calls and the internet to target senior citizens steal taxpayer money and harm vulnerable populations. The Criminal Division will continue dedicating substantial resources to the fight against telemedicine and medical equipment frauds that drain our health care benefit programs.”</p><p>“Together with our partners, the FBI will aggressively pursue those who defraud taxpayer funded health care programs,” said Acting Assistant Director Rebecca Day of the FBI’s Criminal Investigative Division.&nbsp;“Programs like Medicare are intended to help the most vulnerable among us, and fraud schemes like the one orchestrated by the defendant can jeopardize the delivery of critical care to those who need it the most.”</p><p>“This sentence sends a clear message: those who exploit telemedicine to prey on seniors and steal from taxpayer-funded health care programs will be held accountable,” said Deputy Inspector General for Investigations Christian J. Schrank of the U.S. Department of Health and Human Services, Office of Inspector General (HHS-OIG). “This scheme was a massive betrayal of trust, built on deception and greed. Our investigators, working with law enforcement partners, dismantled this billion-dollar fraud operation that targeted vulnerable patients and undermined the integrity of Medicare. We will not relent in our mission to protect the public and safeguard Medicare and other federal health care programs from fraud, waste, and abuse.”</p><p>“This sentencing underscores the Veterans Affairs Office of Inspector General’s (VA OIG) commitment to vigorously investigate those who would seek to defraud VA healthcare programs,” said Special Agent in Charge David Spilker with the VA OIG Southeast Field Office. “The VA OIG thanks the Department of Justice and our law enforcement partners for their efforts in this investigation.”</p><p>“This investigation underscores the Defense Criminal Investigative Service’s (DCIS) commitment to protecting the integrity of the TRICARE program and ensuring that taxpayer-funded military health benefits are not exploited for personal gain,” said Special Agent in Charge Jason Sargenski of DCIS’s Southeast Field Office. &nbsp;“Fraud schemes that siphon resources from TRICARE directly undermine the care promised to service members and their families.&nbsp;As the criminal investigative arm of DoD’s Office of Inspector General, DCIS remains focused on disrupting these schemes and holding responsible parties accountable.”</p><p>According to court documents and evidence presented at trial, Gary Cox, 79, of Maricopa County, was the CEO of Power Mobility Doctor Rx, LLC (DMERx). Cox and his co-conspirators targeted hundreds of thousands of Medicare beneficiaries who provided their personally identifiable information and agreed to accept medically unnecessary orthotic braces, pain creams and other items through misleading mailers, television advertisements and calls from offshore call centers. Cox and his co-conspirators owned, controlled and operated DMERx, an internet-based platform that generated false and fraudulent doctors’ orders for these items. As part of the scheme, Cox connected pharmacies, durable medical equipment (DME) suppliers and marketers with telemedicine companies that would accept illegal kickbacks and bribes in exchange for signed doctors’ orders transmitted using the DMERx platform. Cox and his co-conspirators received payments for coordinating these illegal kickback transactions and referring the completed doctors’ orders to the DME suppliers, pharmacies and telemarketers that paid kickbacks and bribes for the orders.</p><p>The fraudulent doctors’ orders generated by DMERx falsely represented that a doctor had examined and treated the Medicare beneficiaries when, in fact, purported telemedicine companies paid doctors to sign the orders without regard to medical necessity, based only on a brief telephone call with the beneficiary or no interaction with the beneficiary at all. The DME suppliers and pharmacies that paid illegal kickbacks in exchange for these doctors’ orders billed Medicare and other insurers more than $1&nbsp;billion, and Medicare and the insurers paid more than $360 million based on these claims. According to evidence presented at trial, Cox and his co-conspirators concealed the scheme through sham contracts and by eliminating from doctors’ orders what one co-conspirator described as “dangerous words” that might cause Medicare to audit the scheme’s DME suppliers.</p><p>In June 2025, Cox was&nbsp;<a href="https://www.justice.gov/usao-sdfl/pr/ceo-health-care-software-company-convicted-1b-fraud-conspiracy">convicted</a> of conspiracy to commit health care fraud and wire fraud, three counts of health care fraud, conspiracy to pay and receive health care kickbacks and conspiracy to defraud the United States and make false statements in connection with health care matters.</p><p>The FBI, HHS-OIG, VA-OIG and DCIS investigated the case.</p><p>Trial Attorneys Darren C. Halverson and Jennifer E. Burns of the Criminal Division’s Fraud Section prosecuted the case. Fraud Section Trial Attorney Shane Butland assisted in the prosecution. Trial Attorney Evan N. Schlom with the Fraud Section’s Special Matters Unit provided valuable assistance.</p><p>The Fraud Section leads the Criminal Division’s efforts to combat health care fraud through the Health Care Fraud Strike Force Program. Since March 2007, this program, currently comprised of nine strike forces operating in 27 federal districts, has charged more than 5,800 defendants who collectively have billed federal health care programs and private insurers more than $30 billion. In addition, the Centers for Medicare &amp; Medicaid Services, working in conjunction with HHS-OIG, are taking steps to hold providers accountable for their involvement in health care fraud schemes. More information can be found at&nbsp;<a href="http://www.justice.gov/criminal-fraud/health-care-fraud-unit">www.justice.gov/criminal-fraud/health-care-fraud-unit</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside the proton, the ‘most complicated thing you could possibly imagine’ (2022) (106 pts)]]></title>
            <link>https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/</link>
            <guid>46398666</guid>
            <pubDate>Sat, 27 Dec 2025 03:00:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/">https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/</a>, See on <a href="https://news.ycombinator.com/item?id=46398666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        
    <h2>Inside the Proton, the ‘Most Complicated Thing You Could Possibly Imagine’</h2>
    
    <div>
        <p>
            The positively charged particle at the heart of the atom is an object of unspeakable complexity, one that changes its appearance depending on how it is probed. We’ve attempted to connect the proton’s many faces to form the most complete picture yet.          </p>
        
    </div>
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-scaled.webp" alt="" decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Researchers recently discovered that the proton sometimes includes a charm quark and charm antiquark, colossal particles that are each heavier than the proton itself.</p>
            <p data-pm-slice="1 1 []">Samuel Velasco/Quanta Magazine</p>
        </div>
</figcaption>
    </figure>
<div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>More than a century after Ernest Rutherford discovered the positively charged particle at the heart of every atom, physicists are still struggling to fully understand the proton.</p>
<p>High school physics teachers describe them as featureless balls with one unit each of positive electric charge — the perfect foils for the negatively charged electrons that buzz around them. College students learn that the ball is actually a bundle of three elementary particles called quarks. But decades of research have revealed a deeper truth, one that’s too bizarre to fully capture with words or images.</p>
<p>“This is the most complicated thing that you could possibly imagine,” said <a href="https://physics.mit.edu/faculty/michael-williams/">Mike Williams</a>, a physicist at the Massachusetts Institute of Technology. “In fact, you can’t even imagine how complicated it is.”</p>
<p>The proton is a quantum mechanical object that exists as a haze of probabilities until an experiment forces it to take a concrete form. And its forms differ drastically depending on how researchers set up their experiment. Connecting the particle’s many faces has been the work of generations. “We’re kind of just starting to understand this system in a complete way,” said <a href="https://physics.mit.edu/faculty/richard-milner/">Richard Milner</a>, a nuclear physicist at MIT.</p>
        
        
<p>As the pursuit continues, the proton’s secrets keep tumbling out. Most recently, a <a href="https://www.nature.com/articles/s41586-022-04998-2">monumental data analysis</a> published in August found that the proton contains traces of particles called charm quarks that are heavier than the proton itself.</p>
<p>The proton “has been humbling to humans,” Williams said. “Every time you think you kind of have a handle on it, it throws you some curveballs.”</p>
<p>Recently, Milner, together with Rolf Ent at Jefferson Lab, MIT filmmakers Chris Boebel and Joe McMaster, and animator James LaPlante, set out to transform a set of arcane plots that compile the results of hundreds of experiments into a series of animations of the shape-shifting proton. We’ve incorporated their animations into our own attempt to unveil its secrets.</p>
<h2><strong>Cracking Open the Proton</strong></h2>
<p>Proof that the proton contains multitudes came from the Stanford Linear Accelerator Center (SLAC) in 1967. In earlier experiments, researchers had pelted it with electrons and watched them ricochet off like billiard balls. But SLAC could hurl electrons more forcefully, and researchers saw that they bounced back differently. The electrons were hitting the proton hard enough to shatter it — a process called deep inelastic scattering — and were rebounding from point-like shards of the proton called quarks. “That was the first evidence that quarks actually exist,” said <a href="https://www.phys.virginia.edu/People/personal.asp?UID=xz5y">Xiaochao Zheng</a>, a physicist at the University of Virginia.</p>
<p>After SLAC’s discovery, which won the Nobel Prize in Physics in 1990, scrutiny of the proton intensified. Physicists have carried out hundreds of scattering experiments to date. They infer various aspects of the object’s interior by adjusting how forcefully they bombard it and by choosing which scattered particles they collect in the aftermath.</p>
</div>
    </div>
    <figure>
        <div>
                            <p><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Proton_gun_fix_560-Mobile.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Proton_gun_fix_920-Desktop.svg" alt="" decoding="async">                </p>
                        </div>
            </figure>
<figure>
    
</figure>
<div data-role="selectable">
    <p>By using higher-energy electrons, physicists can ferret out finer features of the target proton. In this way, the electron energy sets the maximum resolving power of a deep inelastic scattering experiment. More powerful particle colliders offer a sharper view of the proton.</p>
<p>Higher-energy colliders also produce a wider array of collision outcomes, letting researchers choose different subsets of the outgoing electrons to analyze. This flexibility has proved key to understanding quarks, which careen about inside the proton with different amounts of momentum.</p>
<p>By measuring the energy and trajectory of each scattered electron, researchers can tell if it has glanced off a quark carrying a large chunk of the proton’s total momentum or just a smidgen. Through repeated collisions, they can take something like a census — determining whether the proton’s momentum is mostly bound up in a few quarks, or distributed over many.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_STATES_FIX_3-Desktop.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_STATES_FIX_3-Mobile.svg" alt="" decoding="async">    </p>
    </figure>

<p>Even SLAC’s proton-splitting collisions were gentle by today’s standards. In those scattering events, electrons often shot out in ways suggesting that they had crashed into quarks carrying a third of the proton’s total momentum. The finding matched a theory from Murray Gell-Mann and George Zweig, who in 1964 posited that a proton consists of three quarks.</p>
<p>Gell-Mann and Zweig’s “quark model” remains an elegant way to imagine the proton. It has two “up” quarks with electric charges of +2/3 each and one “down” quark with a charge of −1/3, for a total proton charge of +1.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>But the quark model is an oversimplification that has serious shortcomings.</p>
<p>It fails, for instance, when it comes to a proton’s spin, a quantum property analogous to angular momentum. The proton has half a unit of spin, as do each of its up and down quarks. Physicists initially supposed that — in a calculation echoing the simple charge arithmetic — the half-units of the two up quarks minus that of the down quark must equal half a unit for the proton as a whole. But in 1988, the European Muon Collaboration <a href="https://www.sciencedirect.com/science/article/abs/pii/0370269388915237?via%3Dihub">reported</a> that the quark spins add up to far less than one-half. Similarly, the masses of two up quarks and one down quark only comprise about 1% of the proton’s total mass. These deficits drove home a point physicists were already coming to appreciate: The proton is much more than three quarks.</p>
<h2><strong>Much More Than Three Quarks</strong></h2>
<p>The Hadron-Electron&nbsp;Ring&nbsp;Accelerator (HERA), which operated in Hamburg, Germany, from 1992 to 2007, slammed electrons into protons roughly a thousand times more forcefully than SLAC had. In HERA experiments, physicists could select electrons that had bounced off of extremely low-momentum quarks, including ones carrying as little as 0.005% of the proton’s total momentum. And detect them they did: HERA’s electrons rebounded from a maelstrom of low-momentum quarks and their antimatter counterparts, antiquarks.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>The results confirmed a sophisticated and outlandish theory that had by then replaced Gell-Mann and Zweig’s quark model. Developed in the 1970s, it was a quantum theory of the “strong force” that acts between quarks. The theory describes quarks as being roped together by force-carrying particles called gluons. Each quark and each gluon has one of three types of “color” charge, labeled red, green and blue; these color-charged particles naturally tug on each other and form a group — such as a proton — whose colors add up to a neutral white. The colorful theory became known as quantum chromodynamics, or QCD.</p>
<p>According to QCD, gluons can pick up momentary spikes of energy. With this energy, a gluon splits into a quark and an antiquark — each carrying just a tiny bit of momentum — before the pair annihilates and disappears. It’s this “sea” of transient gluons, quarks and antiquarks that HERA, with its greater sensitivity to lower-momentum particles, detected firsthand.</p>
<p>HERA also picked up hints of what the proton would look like in more powerful colliders. As physicists adjusted HERA to look for lower-momentum quarks, these quarks — which come from gluons — showed up in greater and greater numbers. The results suggested that in even higher-energy collisions, the proton would appear as a cloud made up almost entirely of gluons.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>The gluon dandelion is exactly what QCD predicts. “The HERA data are direct experimental proof that QCD describes nature,” Milner said.</p>
<p>But the young theory’s victory came with a bitter pill: While QCD beautifully described the dance of short-lived quarks and gluons revealed by HERA’s extreme collisions, the theory is useless for understanding the three long-lasting quarks seen in SLAC’s gentle bombardment.</p>
<p>QCD’s predictions are easy to understand only when the strong force is relatively weak. And the strong force weakens only when quarks are extremely close together, as they are in short-lived quark-antiquark pairs. Frank Wilczek, David Gross and David Politzer identified this defining feature of QCD in 1973, winning the Nobel Prize for it 31 years later.</p>
<p>But for gentler collisions like SLAC’s, where the proton acts like three quarks that mutually keep their distance, these quarks pull on each other strongly enough that QCD calculations become impossible. Thus, the task of further demystifying the three-quark view of the proton has fallen largely to experimentalists. (Researchers who run “digital experiments,” in which QCD predictions are simulated on supercomputers, have also made <a href="https://www.quantamagazine.org/impossible-particle-discovery-adds-key-piece-to-the-strong-force-puzzle-20210927/">key contributions</a>.) And it’s in this low-resolution picture that physicists keep finding surprises.</p>
<h2><strong>A Charming New View</strong></h2>
<p>Recently, a team led by <a href="https://research.vu.nl/en/persons/juan-rojo">Juan Rojo</a> of the National Institute for Subatomic Physics in the Netherlands and VU University Amsterdam analyzed more than 5,000 proton snapshots taken over the last 50 years, using machine learning to infer the motions of quarks and gluons inside the proton in a way that sidesteps theoretical guesswork.</p>
<p><strong><em>&nbsp;</em></strong>The new scrutiny picked up a background blur in the images that had escaped past researchers. In relatively soft collisions just barely breaking the proton open, most of the momentum was locked up in the usual three quarks: two ups and a down. But a small amount of momentum appeared to come from a “charm” quark and charm antiquark — colossal elementary particles that each outweigh the entire proton by more than one-third.</p>
</div>
<figure>
    <div>
        
                    
            </div>
</figure>
<div data-role="selectable">
    <p>Short-lived charms frequently show up in the “quark sea” view of the proton (gluons can split into any of six different quark types if they have enough energy). But the results from Rojo and colleagues suggest that the charms have a more permanent presence, making them detectable in gentler collisions. In these collisions, the proton appears as a quantum mixture, or superposition, of multiple states: An electron usually encounters the three lightweight quarks. But it will occasionally encounter a rarer “molecule” of five quarks, such as an up, down and charm quark grouped on one side and an up quark and charm antiquark on the other.</p>
<p>Such subtle details about the proton’s makeup could prove consequential. At the Large Hadron Collider, physicists search for new elementary particles by bashing high-speed protons together and seeing what pops out; to understand the results, researchers need to know what’s in a proton to begin with. The occasional apparition of giant charm quarks would <a href="https://arxiv.org/abs/1512.06666">throw off the odds</a> of making more exotic particles.</p>
<p>And when protons called cosmic rays hurtle here from outer space and slam into protons in Earth’s atmosphere, charm quarks popping up at the right moments would shower Earth with <a href="https://arxiv.org/abs/2107.13852">extra-energetic neutrinos</a>, researchers calculated in 2021. These could confound observers <a href="https://www.quantamagazine.org/cosmic-map-of-ultrahigh-energy-particles-points-to-long-hidden-treasures-20210427/">searching</a> for high-energy neutrinos coming from across the cosmos.</p>
<p>Rojo’s collaboration plans to continue exploring the proton by searching for an imbalance between charm quarks and antiquarks. And heavier constituents, such as the top quark, could make even rarer and harder-to-detect appearances.</p>
<p>Next-generation experiments will seek still more unknown features. Physicists at Brookhaven National Laboratory hope to fire up the Electron-Ion Collider in the 2030s and pick up where HERA left off, taking higher-resolution snapshots that will enable the first 3D reconstructions of the proton. The EIC will also use spinning electrons to create detailed maps of the spins of the internal quarks and gluons, just as SLAC and HERA mapped out their momentums. This should help researchers to finally pin down the origin of the proton’s spin, and to address other fundamental questions about the baffling particle that makes up most of our everyday world.</p>

<p><em><strong>Correction:</strong> October 20, 2022</em><br>
<em>A previous version of the article erroneously implied that lower-momentum quarks live shorter lives than higher-momentum quarks in the quark sea. The text has been updated to clarify that all these quarks are lower-momentum and shorter-lived than those in the three quark-picture.</em></p>
</div>
                
                
            </div><div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="729" src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1720x729.webp" alt="Harry Halpin in a T-shirt and sports coat outdoors" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1720x729.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-520x220.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-768x325.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1536x651.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-2048x868.webp 2048w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>The Computer Scientist Who’s Boosting Privacy on the Internet</p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QNX Self-Hosted Developer Desktop (243 pts)]]></title>
            <link>https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/</link>
            <guid>46398201</guid>
            <pubDate>Sat, 27 Dec 2025 01:16:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/">https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/</a>, See on <a href="https://news.ycombinator.com/item?id=46398201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://devblog.qnx.com/tag/news/">News</a>
            
                <p>Try out the initial release of the QNX Developer Desktop -- a self-hosted development environment for QNX. No more cross-compilation!</p>

            <div>
                <p><a href="https://devblog.qnx.com/author/johnatqnx/">
                                <img src="https://devblog.qnx.com/content/images/size/w160/2025/01/me_avatar_good_cropped_more.png" alt="JohnAtQNX">
                            </a>
                </p>
                
            </div>

                <figure>
        <img srcset="https://devblog.qnx.com/content/images/size/w320/2025/12/desktop-preview1-1.png 320w,
                    https://devblog.qnx.com/content/images/size/w600/2025/12/desktop-preview1-1.png 600w,
                    https://devblog.qnx.com/content/images/size/w960/2025/12/desktop-preview1-1.png 960w,
                    https://devblog.qnx.com/content/images/size/w1200/2025/12/desktop-preview1-1.png 1200w,
                    https://devblog.qnx.com/content/images/size/w2000/2025/12/desktop-preview1-1.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://devblog.qnx.com/content/images/size/w1200/2025/12/desktop-preview1-1.png" alt="QNX Self-Hosted Developer Desktop -- Initial Release">
    </figure>

        </header>

        <section>
            <p>The team and I are beyond excited to share what we've been cooking up over the last little while: <strong>a full desktop environment running on QNX 8.0, with support for self-hosted compilation</strong>! This environment both makes it easier for newly-minted QNX developers to get started with building for QNX, but it also vastly simplifies the process of porting Linux applications and libraries to QNX 8.0.</p><p>This self-hosted target environment is pre-loaded with many of the ports you'll find on <a href="https://oss.qnx.com/?ref=devblog.qnx.com" rel="noreferrer">the QNX Open-source Dashboard</a>. (The portal currently includes over 1,400 ports across various targets, QNX versions, and architectures, of which more than 600 are unique ports!)</p><p>In this initial release, you can grab a copy of the QEMU image and give it a try for yourself. There's still so much more to add, but it's in a great place today for this first release. The team is really passionate about this one, and we're eagerly looking forward to your feedback!</p><h2 id="whats-included">What's Included</h2><p>For the initial release of Desktop, we tried to cover all the basics: windowing, terminal, IDEs, browser, file management, and samples. To that end, here's what makes up the QNX Developer Desktop:</p><ul><li>A customizable XFCE desktop environment running on Wayland</li><li>The tools you need to compile and/or run your code (<code>clang</code>, gcc, <code>clang++</code>, Python, <code>make</code>, <code>cmake</code>, <code>git</code>, etc)</li><li>A web browser (can you join <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">the QNX Discord</a> from the QNX Desktop? 🏅👀)</li><li>Ports of popular IDEs/editors, like Geany, Emacs, Neovim, and vim</li><li>Thunar, for file management</li><li>Preloaded samples, like Hello World in C, C++, and Python, and GTK demos OpenGL ES demos</li><li>... and of course, a terminal.</li></ul><h2 id="system-requirements">System Requirements</h2><p>This environment runs as a virtual machine, using QEMU on Ubuntu. To try the image, you'll need:</p><ul><li>Ubuntu 22.04 or 24.04</li></ul><h2 id="try-it-yourself">Try It Yourself</h2><p>(Keep in mind this is the first release, so it takes a minute to get started and it's a bit rough around the edges.)</p><p>With <a href="https://qnx.com/getqnx?ref=devblog.qnx.com" rel="noreferrer">a free QNX license</a>, you can find this release in QNX Software Center. On the <strong>Available</strong> tab of the <strong>Manage Installation</strong> pane, search for "quick start" and install the "QNX SDP 8.0 Quick Start Target Image for QEMU".</p><p>You'll find the image in your QNX installation directory, usually <code>~/qnx800/images</code> by default. Follow the <code>README.md</code> file in the <code>qemu</code> directory to extract &amp; combine the multiple QNX packages downloaded under the hood.</p><p>Next, follow the PDF instructions found in the new <code>./qemu_qsti/docs/</code> directory to install the required dependencies and boot up.</p><div><p>💡</p><p>If you experience any trouble starting the environment, check the PDF's <b><strong>Troubleshooting</strong></b> chapter, or come <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">ask us on Discord</a>.</p></div><h2 id="whats-next">What's Next</h2><p>This is just the very first release! Over the next few months and beyond, we'll drop more updates of Desktop. You can look forward to:</p><ul><li>QEMU images for Windows &amp; macOS, and native images for x86</li><li>A native Desktop image on Raspberry Pi</li><li>Enhanced documentation</li><li>Features to help use this self-hosted environment in CI jobs</li><li>More samples &amp; stability</li><li>... and more! Have suggestions? Let us know.</li></ul><p>Lastly, if you want some help with your QNX journey, you can find the QNX team and community:</p><ul><li>in Discord here: <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">discord.gg/Jj4EkkrFTT</a></li><li>on Reddit at: <a href="https://reddit.com/r/qnx?ref=devblog.qnx.com" rel="noreferrer">reddit.com/r/qnx</a></li></ul>
        </section>

    </article>

        

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Publishing your work increases your luck (238 pts)]]></title>
            <link>https://github.com/readme/guides/publishing-your-work</link>
            <guid>46397991</guid>
            <pubDate>Sat, 27 Dec 2025 00:43:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/readme/guides/publishing-your-work">https://github.com/readme/guides/publishing-your-work</a>, See on <a href="https://news.ycombinator.com/item?id=46397991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="">



        <p>No matter how hard you work, it still takes a little bit of luck for something to hit. That can be discouraging, since luck feels like a force outside our control. But the good news is that we can increase our chances of encountering good luck. That may sound like magic, but it’s not supernatural. The trick is to increase the number of opportunities we have for good fortune to find us. The simple act of publishing your work is one of the best ways to invite a little more luck into your life.</p>
<p>Before we get into the “how,” it’s important to get on the same page about the “what.” What are we talking about when we say “luck?” There are a lot of definitions that could apply, but let’s stick with a simple one: Luck is when something unexpected and good happens to you. Unexpected and good. Who doesn’t want to increase the odds of something unexpected and good?</p>
<p>In our world, luck can include:</p>
<ul><li><p>Having your OSS library take off</p></li><li><p>Being invited to speak at a conference</p></li><li><p>Landing a new job</p></li><li><p>Getting a new consulting client</p></li><li><p>Being invited onto a podcast</p></li><li><p>Making new friends in your community</p></li></ul>
<p>None of these things are totally in your control, which can at times feel frustrating.&nbsp;</p>
<p>How can we increase the odds of finding luck? By being a person who works in public. By doing work and being public about it, you build a reputation for yourself. You build a track record. You build a public body of work that speaks on your behalf better than any resume ever could.</p>
<p>The goal is not to become famous, the goal is to increase the chances of luck finding us. For me, one of the most helpful ways to think about this has always been the concept of the “Luck Surface Area,” described in an <a href="https://www.codusoperandi.com/posts/increasing-your-luck-surface-area"><u>old post by Jason Roberts</u></a>. He wrote (and note, the emphasis is mine):&nbsp;</p>
<p><i>"The amount of serendipity that will occur in your life, your Luck Surface Area, is directly proportional to the degree to which you </i><i><b>do something</b></i><i> you’re passionate about combined with the total number of people to whom this is </i><i><b>effectively communicated</b></i><i>."</i></p>
<p>Going further, he codifies it into a formula where:</p>
<div tabindex="0">
        <pre><span><span>Luck</span> <span>=</span> <span>[</span><span>Doing</span> <span>Things</span><span>]</span> <span>*</span> <span>[</span><span>Telling</span> <span>People</span><span>]</span></span></pre>

  </div>


<p>The more things you do multiplied by the more people you tell, the larger your Luck Surface Area becomes. The larger your Luck Surface Area, the more likely you are to catch luck as it flows by.</p>
<picture>
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=avif 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=avif 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=avif 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=avif 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=avif 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=avif 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/avif">
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=webp 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=webp 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=webp 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=webp 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=webp 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=webp 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/webp">
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=jpg 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=jpg 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=jpg 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=jpg 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=jpg 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=jpg 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/jpeg">
  <img width="468" height="342" loading="lazy" decoding="async" alt="A graph with &quot;Doing&quot; on the Y axis, and &quot;Telling&quot; on the X axis. The more you do and tell, the better." src="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=jpg">
</picture>
<h3>Source:<a href="https://www.codusoperandi.com/posts/increasing-your-luck-surface-area"><u> Jason Roberts</u></a></h3>
<hr>

<h2>Doing the work</h2>
<p>Before you can publish your work, you have to actually <i>do</i> the work. The good news for you is that by even reading this Guide on The ReadME Project, you’ve probably already self-selected into a group of people for whom “doing things” comes somewhat naturally. You’re a developer, a designer, a creator, an author, or something else entirely.&nbsp; Whatever moniker you want to give yourself, you’re built to <i>do</i> things, and that’s the important part.&nbsp;</p>
<p>If that doesn’t ring true for you, you may fall into one of two groups:</p>
<ol><li><p>You actually <i>are</i> doing things, you’ve just trained yourself to think that anything you do isn’t worth sharing.</p></li><li><p>You <i>want</i> to be doing things, but you can’t bring yourself to get started.</p></li></ol>
<p>If you’re in the first group, you may need to step back and reframe the work you’re already doing. This is a common blind spot for people who are executing at a high level! They’ve forgotten just how much they know. They think that they’re not doing anything interesting because they assume that everyone knows as much as they do. This effect is only exacerbated when everyone in your immediate vicinity is at a similar—or higher—skill level. As you become more of an expert, your quality bar gets higher and higher and you forget that everything you know is not known by everyone.</p>
<p>If you’re in this group I want to give you a challenge: Watch the communities where you hang out and see what people are sharing and what gets noticed. Is it something you could have done? Is it something you’ve <i>already</i> done? At its worst this could lead you in the direction of becoming bitter, critical, and thinking that you’re smarter than everyone. To that I say “resist!” There is no life there. My encouragement to you is to view that as objective evidence that people want to know all of the things that you already know! There is a huge opportunity for you, should you decide to start sharing your work.&nbsp;</p>
<p>If you’re in the second group, you just need to start. Start anywhere, start on anything, start something. You’ll never come up with the perfect idea for an OSS library, a business, a podcast, or an article by just thinking about it. Start on something, today. It won’t be the perfect version of the thing you have in your head, but you’ll be in motion. Motion begets motion, progress begets progress. Pick the smallest thing you can do and get started.</p>
<p>Doing the work is the most important part. It’s the nucleus around which everything else revolves. What that “work” looks like, though, is entirely up to you! That’s the fun part. It can take any form and be in any domain. Wherever your curiosity or expertise draw you, dive into that.&nbsp;</p>
<p>Projects outside of work are a good place to dive into your curiosity.&nbsp;</p>
<ul><li><p>If you want to make<a href="https://twitter.com/aschmelyun/status/1506960015063625733"><u> a thermal receipt printer that prints GitHub issues</u></a>, you should.&nbsp;</p></li><li><p>If you want to<a href="https://twitter.com/aarondfrancis/status/1333866090573811723"><u> turn a prefabricated shed into an office</u></a>, go for it.&nbsp;</p></li><li><p>If you want to go all in on an<a href="https://twitter.com/steveruizok/status/1419048412431933441"><u> SVG drawing tool</u></a>, do it.&nbsp;</p></li><li><p>If you want to write tens of thousands of words about <a href="https://bam.kalzumeus.com/archive/"><u>the infrastructure of modern money</u></a>, that’s a newsletter.&nbsp;</p></li></ul>
<p>Your curiosity will naturally pull you in certain directions, so don’t be afraid to go super deep into a topic that you’re interested in. When a person is truly interested in the thing they’re writing or talking about, their excitement is contagious. Whatever you’re excited about, be excited about it publicly. Whatever you’re curious about, be curious about it publicly. People will want to follow along and you’ll inspire people along the way.</p>
<p>Projects at work can be a good place to dive into your expertise.&nbsp;</p>
<p>It's likely you're constantly solving problems and learning interesting things at your job. This is a great opportunity to take what you’re already doing and repurpose it for the benefit of others. You can turn those learnings into blog posts, conference talks, meetups, podcasts, or open source projects.&nbsp;</p>
<p>Of course not everything you do at work is shareable. If the specifics aren’t shareable, the concepts, lessons, and takeaways likely are. While you’re working, keep a scratch pad open and jot down any problems you come across, interesting patterns you see, or things you found confusing. Do this for a month and you’ll have more things to share than you know what to do with!</p>
<p>You’ve done the work, now it's time to tell people.</p>


<h2>Hitting the publish button</h2>
<p>This part of the formula can be harder for most of us. Most of us really enjoy the building aspect but start to get a little shy when it comes to telling people about the stuff we’ve built. That could be for any number of reasons: fear, embarrassment, self-preservation, or an aversion to being perceived as hawking your wares.</p>
<p>It’s a valuable exercise to investigate whether or not you resonate with any of those reasons. Are you afraid people are going to make fun of what you built? Are you embarrassed that it isn’t up to your own (admittedly high) standards? Are you waiting for some elusive perfect moment? Do you have an aversion to “marketing” and don’t want to become the thing you hate? Whatever it is for you, I encourage you to really dig into it and see if that fear is worth keeping around.</p>
<p>Sharing things you’re learning or making is not prideful. People are drawn to other people in motion. People <i>want</i> to follow along, people <i>want</i> to learn things, people <i>want</i> to be a part of your journey. It’s not bragging to say, “I’ve made a thing and I think it’s cool!” Bringing people along is a good thing for everyone. By publishing your work you’re helping people learn. You’re inspiring others to create.</p>
<p>You can “publish” anywhere. For me that’s mostly Twitter because that’s where most of my peers hang out. It doesn’t have to be Twitter for you. It could be GitHub, a newsletter, a podcast, forums, your blog, YouTube, or something completely different that’s not even on my radar. Anywhere that’s not your hard drive counts!&nbsp;</p>
<p>Publishing is a skill, it’s something you can learn. You’ll need to build your publishing skill just like you built every other skill you have.&nbsp;</p>
<p>Don’t be afraid to publish along the way. You don’t have to wait until you’re done to drop a perfect, finished artifact from the sky (in fact, you may use that as an excuse to <i>never</i> publish). People like stories, so use that to your benefit. Share the wins, the losses, and the thought processes. Bring us along! If you haven’t been in the habit of sharing your work, it’s going to feel weird when you start. That’s normal! Keep going, you get used to it.&nbsp;</p>
<p>You’ve done the work. You’ve hit the publish button. You’ve done your part!&nbsp;</p>
<h2>Capturing the luck</h2>
<p>You’ve <i>increased the odds</i> that good, unexpected things will come your way. The exact form is hard to predict, but here are a few potential outcomes:&nbsp;</p>
<ul><li><p>People start to know you as the person that talks about X, Y, and Z.&nbsp;</p></li><li><p>You start to get emails from people saying that they read your stuff and liked it.&nbsp;</p></li><li><p>You get a DM about a job you might be interested in.</p></li><li><p>People ask you if you’re taking on new clients.</p></li><li><p>Someone you’ve never met or interacted with will mention you as being an expert in your area.&nbsp;</p></li><li><p>A meetup asks you to come talk about the things you’ve been sharing.</p></li><li><p>You become friends with other people in your industry.</p></li><li><p>Your OSS library starts gaining mindshare.</p></li></ul>
<p>This is not a random list of made-up examples, it’s a list of things that have literally happened to me once I got over my fears and started sharing my work. I had been doing the work all along, but was too afraid to publish. Once I overcame that fear, my Luck Surface Area expanded and good, unexpected things started happening.&nbsp;</p>
<p>The formula is simple.</p>
<p>Do the work. Don’t be afraid to dive deep into your curiosity and your expertise. We need more people that are intensely curious. We need more people with deep expertise.</p>
<p>Tell people. Press publish, bring us along, share the journey. Tell us what you’ve learned, what you’ve built, or what you’re excited about.</p>
<p>The formula may be simple, but I’ll admit it’s not always easy. It’s scary to put yourself out there. It’s hard to open yourself up to criticism. People online can be mean. But for every snarky comment, there are ten times as many people quietly following along and admiring not only your work, but your bravery to put it out publicly. And at some point, one of those people quietly following along will reach out with a life-changing opportunity and you’ll think, “Wow, that was lucky.”</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exe.dev (374 pts)]]></title>
            <link>https://exe.dev/</link>
            <guid>46397609</guid>
            <pubDate>Fri, 26 Dec 2025 23:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exe.dev/">https://exe.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46397609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>ssh exe.dev <span>_</span></span>
        </p>
        <p>
            The disk persists. You have sudo.
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Always Bet on Text (305 pts)]]></title>
            <link>https://graydon2.dreamwidth.org/193447.html</link>
            <guid>46397379</guid>
            <pubDate>Fri, 26 Dec 2025 23:09:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graydon2.dreamwidth.org/193447.html">https://graydon2.dreamwidth.org/193447.html</a>, See on <a href="https://news.ycombinator.com/item?id=46397379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I figured I should just post this somewhere so I can make future reference to how I feel about the matter, anytime someone asks me about such-and-such video, 3D, game or "dynamic" multimedia system. Don't get me wrong, I like me some illustrations, photos, movies and music.</p><p>But text wins by a mile. Text is everything. My thoughts on this are quite absolute: <em>text is the most powerful, useful, effective communication technology ever</em>, period.</p><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/The_oldest_writing_in_the_world_-_The_Sumerian_Stone_Tablet.jpg/220px-The_oldest_writing_in_the_world_-_The_Sumerian_Stone_Tablet.jpg">Text is the <em>oldest and most stable</em> communication technology (assuming we treat speech/signing as natural phenomenon -- there are no human societies without it -- whereas textual capability has to be transmitted, taught, acquired) and it's incredibly <em>durable</em>. We can read texts from <em>five thousand years ago</em>, almost the moment they started being produced. It's (literally) "rock solid" -- you can readily inscribe it in granite that will likely outlast the human species.<br></p></div><br><div><p><img src="https://upload.wikimedia.org/math/1/3/e/13e30a48d35ec57948e8a577db7eb787.png">Text is the <em>most flexible</em> communication technology. Pictures may be worth a thousand words, when there's a picture to match what you're trying to say. But let's hit the random button on wikipedia and pick a sentence, see if you can draw a picture to convey it, mm? Here:</p><blockquote>"Human rights are moral principles or norms that describe certain standards of human behaviour, and are regularly protected as legal rights in national and international law."<br></blockquote><p>Not a <em>chance</em>. Text can convey <em>ideas</em> with a precisely controlled level of ambiguity and precision, implied context and elaborated content, unmatched by anything else. It is not a coincidence that all of literature and poetry, history and philosophy, mathematics, logic, programming and engineering rely on textual encodings for their ideas.<br></p></div><br><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Rees%27s_Cyclopaedia_Chappe_telegraph.png/220px-Rees%27s_Cyclopaedia_Chappe_telegraph.png"> Text is the <em>most efficient</em> communication technology. By <em>orders of magnitude</em>. This blog post is likely to take perhaps 5000 bytes of storage, and could compress down to maybe 2000; by comparison the following 20-pixel-square image of the silhouette of a tweeting bird takes 4000 bytes: <img src="https://abs.twimg.com/errors/logo23x19.png">. At every step of communication technology, textual encoding comes first, everything else after. Because it's <em>vastly cheaper</em> on a symbol-by-symbol basis. You have a working <a href="https://en.wikipedia.org/wiki/Optical_telegraph">optical telegraph</a> network running in <em>1790</em> in France. You the better part of a century of <a href="https://en.wikipedia.org/wiki/Electrical_telegraph">electrical telegraphy</a>, trans-oceanic cables and everything, before anyone bothers with trying to carry voice. You have decades of teleprinter and text-only computer networking, mail and news, chat and publishing, editing and diagnostics, before bandwidth gets cheap enough for images, voice and video. You have pagers, SMS, WAP, USSD and blackberries before iPhones. You have <a href="http://en.wikipedia.org/wiki/Teletext">Teletext</a> and BBSs, netnews and gopher before the web. And today many of the best, and certainly the most efficient parts of the web remain text-centric. I can download <em>all of wikipedia</em> and carry it around on the average smartphone.</p></div><br><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Gnus-reading-news.png/220px-Gnus-reading-news.png"><br>Text is the most <em>socially useful</em> communication technology. It works <em>well</em> in 1:1, 1:N, and M:N modes. It can be <em>indexed</em> and <em>searched</em> efficiently, even by hand. It can be <em>translated</em>. It can be produced and consumed at variable speeds. It is asynchronous. It can be compared, diffed, clustered, corrected, summarized and filtered algorithmically. It permits multiparty editing. It permits branching conversations, lurking, annotation, quoting, reviewing, summarizing, structured responses, exegesis, even fan fic. The breadth, scale and depth of ways people use text is unmatched by anything. There is no equivalent in <em>any other communication technology</em> for the social, communicative, cognitive and reflective complexity of a library full of books or an internet full of postings. Nothing else comes close.<br></p></div><p>So this is my stance on text: always pick text first. As my old boss might have said: always bet on text. If you can use text for something, use it. It will very seldom let you down.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Toys with the highest play-time and lowest clean-up-time (445 pts)]]></title>
            <link>https://joannabregan.substack.com/p/toys-with-the-highest-play-time-and</link>
            <guid>46395885</guid>
            <pubDate>Fri, 26 Dec 2025 20:28:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joannabregan.substack.com/p/toys-with-the-highest-play-time-and">https://joannabregan.substack.com/p/toys-with-the-highest-play-time-and</a>, See on <a href="https://news.ycombinator.com/item?id=46395885">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The worst toy is one with many pieces that my kids dump on the ground and then play with for only 2 minutes. This makes a cleaning to playtime ratio: 2 minute play vs 10 minute clean up</p><p>Sucking away my life as a parent.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!x5G6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!x5G6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 424w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 848w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!x5G6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg" width="413" height="310.576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1128,&quot;width&quot;:1500,&quot;resizeWidth&quot;:413,&quot;bytes&quot;:178259,&quot;alt&quot;:&quot;Hape Chunky Alphabet Puzzle&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Hape Chunky Alphabet Puzzle" title="Hape Chunky Alphabet Puzzle" srcset="https://substackcdn.com/image/fetch/$s_!x5G6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 424w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 848w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!x5G6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ac5d4e9-dea2-4715-be08-56ff5635ddb0_1500x1128.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>This is not fun to clean</figcaption></figure></div><p>A beautiful toy is one that the kids play with a lot, over a long time, and that isn’t hard to clean up.</p><p><strong>1. Repeatability</strong><span> </span></p><p>Play once 1 —|—|—|— 5 Play daily for years </p><p><strong>2. Length of play session</strong></p><p>One minute 1 —|—|—|—5 30+ minutes</p><p><strong>3. Clean up ease</strong></p><p>Annoying 1 —|—|—|— 5 Easy</p><p>Score: 13</p><p><span>Repeatability: 5</span><br><span>Length of play session: 4</span><br><span>Clean up ease: 4</span></p><p>Score: 13</p><p><span>Repeatability: 5</span><br><span>Length of play session: 5</span><br><span>Clean up ease: 3</span></p><p>Score: 12</p><p><span>Repeatability: 4</span><br><span>Length of play session: 4</span><br><span>Clean up ease: 4</span></p><p>Score: 6</p><p><span>Repeatability: 2</span><br><span>Length of play session: 2</span><br><span>Clean up ease: 2</span></p><p>Comparing the toys I score high and the toy I scored low, here are the principles that I think give a toy a high score.</p><p>The high-scoring toys can become many different kinds of objects. At our house, they are robots, they are rocket ships. They are a fishing hook that then we go fishing with.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_-lD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_-lD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 424w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 848w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 1272w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_-lD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png" width="374" height="374" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:670,&quot;width&quot;:670,&quot;resizeWidth&quot;:374,&quot;bytes&quot;:728854,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://joannabregan.substack.com/i/181462642?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_-lD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 424w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 848w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 1272w, https://substackcdn.com/image/fetch/$s_!_-lD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4773ae5e-9838-475d-ab75-94e0a3b82dc4_670x670.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>My son showing off his creations</figcaption></figure></div><p>The giant magnet tiles and small magnet tiles also become containers for a narrative to play out. The magnet tiles are often present boxes to deliver birthday presents to each other, or houses that other toys live inside of. The giant tiles are houses, rocket ships, nap pods, or shops. I would rather have a pile of giant magnet tiles than one “play store”. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lYUb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lYUb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 424w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 848w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 1272w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lYUb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png" width="372" height="331.0082644628099" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:646,&quot;width&quot;:726,&quot;resizeWidth&quot;:372,&quot;bytes&quot;:1016102,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://joannabregan.substack.com/i/181462642?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lYUb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 424w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 848w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 1272w, https://substackcdn.com/image/fetch/$s_!lYUb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300c674b-3b0e-4f0f-898b-a032cc2be6b2_726x646.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xwVG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xwVG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xwVG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg" width="374" height="374" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:512,&quot;resizeWidth&quot;:374,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Melissa &amp; Doug - Fresh Mart Grocery Store&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Melissa &amp; Doug - Fresh Mart Grocery Store" title="Melissa &amp; Doug - Fresh Mart Grocery Store" srcset="https://substackcdn.com/image/fetch/$s_!xwVG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xwVG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e77359-3b78-498f-90bb-04a4cd89a009_512x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>By contrast, the pieces in the Minecraft toy are each a specific thing: a tree, water, or lava. There are fewer world building possibilities, with everything fitting into the strong frame that the toy offers. It makes sense that they grow bored of playing with them if it has fewer “games” to offer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZiYV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZiYV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZiYV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg" width="342" height="193" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:193,&quot;width&quot;:342,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Amazon.com: 100PCS Magnetic Blocks-Build Mine Magnet World Building Craft  Set for Boys &amp; Girls Age 3-5 6-8, STEM Sensory Toys for 3+ Years Old Girls  Boys, 1\&quot; Magnet Cubes Classroom Must Haves&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Amazon.com: 100PCS Magnetic Blocks-Build Mine Magnet World Building Craft  Set for Boys &amp; Girls Age 3-5 6-8, STEM Sensory Toys for 3+ Years Old Girls  Boys, 1&quot; Magnet Cubes Classroom Must Haves" title="Amazon.com: 100PCS Magnetic Blocks-Build Mine Magnet World Building Craft  Set for Boys &amp; Girls Age 3-5 6-8, STEM Sensory Toys for 3+ Years Old Girls  Boys, 1&quot; Magnet Cubes Classroom Must Haves" srcset="https://substackcdn.com/image/fetch/$s_!ZiYV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ZiYV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c30ecf7-94f1-45d0-8884-dafa21f8a5f6_342x193.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each piece of the high scoring toys has a fun relationship with the others. If the pieces are different, they are different enough that it doesn’t take a lot of thought to choose between them. </p><p>The toy below looks less fun to me because it looks more fiddly. If I was playing, I would need to figure out if I want a slightly bent angle or a straight one, and it seems like not a fun choice.</p><p>Apparently every toy that I find easiest to clean up has magnets on it. Maybe I feel the satisfaction of clicking them together as I clean them up. Cleaning becomes a little like playing.</p><p>With the high scoring toys, the magnets are strong, and the connection between parts feels satisfying when you make it.</p><p>On the other side, the Minecraft toy magnets are less strong and feel less satisfying to put together. It doesn’t have a satisfying sense of being complete when you stick them together; it’s more like a temporary paste.</p><p>The toy Clixo seems cool: flexible play, elegant shapes, and magnetic. I predict it would be a top scoring toy.</p></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[T-Ruby is Ruby with syntax for types (160 pts)]]></title>
            <link>https://type-ruby.github.io/</link>
            <guid>46395871</guid>
            <pubDate>Fri, 26 Dec 2025 20:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://type-ruby.github.io/">https://type-ruby.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=46395871">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><div><h2>What is T-Ruby?</h2><div><div><h3>Ruby and More</h3><p>T-Ruby extends Ruby with additional syntax. <strong>This means static types and compile time.</strong></p></div><div><h3>A Result You Can Trust</h3><p>T-Ruby code converts to Ruby and RBS, <strong>guaranteeing compatibility with the entire Ruby and RBS ecosystem.</strong></p></div><div><h3>Safety at Scale</h3><p>Ensure type safety for your Ruby products with T-Ruby. <strong>Stay safe even at large scale.</strong></p></div></div></div><main><div><h2>Example</h2><p>Write typed Ruby, compile to standard Ruby with generated type signatures.</p><div><div><pre tabindex="0"><code><span><span># hello.trb</span><span></span><br></span><span><span></span><span>def</span><span> </span><span>greet</span><span>(</span><span>name</span><span>:</span><span> </span><span>String</span><span>)</span><span>:</span><span> </span><span>String</span><span></span><br></span><span><span>  </span><span>"Hello, </span><span>#{</span><span>name</span><span>}</span><span>!"</span><span></span><br></span><span><span></span><span>end</span><span></span><br></span><span><span></span><br></span><span><span></span><span>def</span><span> </span><span>add</span><span>(</span><span>a</span><span>:</span><span> </span><span>Integer</span><span>,</span><span> </span><span>b</span><span>:</span><span> </span><span>Integer</span><span>)</span><span>:</span><span> </span><span>Integer</span><span></span><br></span><span><span>  a </span><span>+</span><span> b</span><br></span><span><span></span><span>end</span><br></span></code></pre></div><p>→</p><div><div><pre tabindex="0"><code><span><span># hello.rb</span><span></span><br></span><span><span></span><span>def</span><span> </span><span>greet</span><span>(</span><span>name</span><span>)</span><span></span><br></span><span><span>  </span><span>"Hello, </span><span>#{</span><span>name</span><span>}</span><span>!"</span><span></span><br></span><span><span></span><span>end</span><span></span><br></span><span><span></span><br></span><span><span></span><span>def</span><span> </span><span>add</span><span>(</span><span>a</span><span>,</span><span> b</span><span>)</span><span></span><br></span><span><span>  a </span><span>+</span><span> b</span><br></span><span><span></span><span>end</span><br></span></code></pre></div><p>+</p><div><pre tabindex="0"><code><span><span># hello.rbs</span><span></span><br></span><span><span></span><span>def</span><span> </span><span>greet</span><span>:</span><span> </span><span>(</span><span>name</span><span>:</span><span> </span><span>String</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> </span><span>String</span><span></span><br></span><span><span></span><br></span><span><span></span><span>def</span><span> </span><span>add</span><span>:</span><span> </span><span>(</span><span>a</span><span>:</span><span> </span><span>Integer</span><span>,</span><span> </span><span>b</span><span>:</span><span> </span><span>Integer</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> </span><span>Integer</span><br></span></code></pre></div></div></div></div><div><h2>Existing Methods</h2><p>Compare with existing Ruby typing solutions and see why T-Ruby is different.</p><div><div><h3>How it works</h3><p>A static type checker for Ruby developed by Stripe. Uses sig blocks to declare types on methods.</p><div><pre tabindex="0"><code><span><span># typed: strict</span><span></span><br></span><span><span></span><span>require</span><span> </span><span>'sorbet-runtime'</span><span></span><br></span><span><span></span><br></span><span><span></span><span>class</span><span> </span><span>Greeter</span><span></span><br></span><span><span>  </span><span>extend</span><span> </span><span>T</span><span>::</span><span>Sig</span><br></span><span><span></span><br></span><span><span>  sig </span><span>{</span><span> params</span><span>(</span><span>name</span><span>:</span><span> </span><span>String</span><span>)</span><span>.</span><span>returns</span><span>(</span><span>String</span><span>)</span><span> </span><span>}</span><span></span><br></span><span><span>  </span><span>def</span><span> </span><span>greet</span><span>(</span><span>name</span><span>)</span><span></span><br></span><span><span>    </span><span>"Hello, </span><span>#{</span><span>name</span><span>}</span><span>!"</span><span></span><br></span><span><span>  </span><span>end</span><span></span><br></span><span><span></span><span>end</span><br></span></code></pre></div></div><div><h3>Limitations</h3><ul><li>Requires runtime dependency (sorbet-runtime gem)</li><li>Types must be written separately in sig blocks, like comments above function code.</li><li>Requires learning sig block's unique DSL syntax.</li></ul></div><div><h3>T-Ruby Approach</h3><p>T-Ruby uses inline types like TypeScript without runtime dependencies, and generates standard RBS files.</p></div></div></div><div><h2>Quick Start</h2><div><div><p>1</p><div><h3>Initialize project</h3><div><pre tabindex="0"><code><span><span>gem </span><span>install</span><span> t-ruby</span><br></span><span><span>trc </span><span>--init</span><br></span></code></pre></div></div></div><div><p>2</p><div><h3>Start watch mode</h3></div></div><div><p>3</p><div><h3>Write typed Ruby</h3><div><p>src/hello.trb</p><div><pre tabindex="0"><code><span><span>def</span><span> </span><span>greet</span><span>(</span><span>name</span><span>:</span><span> </span><span>String</span><span>)</span><span>:</span><span> </span><span>String</span><span></span><br></span><span><span>  </span><span>"Hello, </span><span>#{</span><span>name</span><span>}</span><span>!"</span><span></span><br></span><span><span></span><span>end</span><br></span></code></pre></div></div></div></div></div></div><div><h2>Works with your tools</h2><p>T-Ruby integrates seamlessly with the Ruby ecosystem.</p></div><div><h2>Join the Journey</h2><p>T-Ruby is an open source project. Your contribution makes a difference.<br>It's still experimental. The core compiler works, but there's much to improve.<br>Feedback and suggestions are always welcome!</p></div></main></div></div>]]></description>
        </item>
    </channel>
</rss>