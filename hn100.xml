<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 15 Jun 2025 12:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Q-learning is not yet scalable (151 pts)]]></title>
            <link>https://seohong.me/blog/q-learning-is-not-yet-scalable/</link>
            <guid>44279850</guid>
            <pubDate>Sun, 15 Jun 2025 00:56:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seohong.me/blog/q-learning-is-not-yet-scalable/">https://seohong.me/blog/q-learning-is-not-yet-scalable/</a>, See on <a href="https://news.ycombinator.com/item?id=44279850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
            <h2>Does RL scale?</h2>
            <p>
                Over the past few years,
                we've seen that next-token prediction scales, denoising diffusion scales, contrastive learning scales,
                and so on, all the way to the point where we can train models with billions of parameters
                with a <i>scalable</i> objective that can eat up as much data as we can throw at it.
                Then, what about reinforcement learning (RL)?
                <b>Does RL also <i>scale</i> like all the other objectives?</b>
            </p>
            <p>
                Apparently, it does.
                In 2016, RL achieved superhuman-level performance in games like Go and Chess.
                Now, RL is solving complex reasoning tasks in math and coding with large language models (LLMs).
                This is great. However, there is one important caveat:
                most of the current real-world successes of RL have been achieved with <b>on-policy RL</b> algorithms
                (<i>e.g.</i>, REINFORCE, PPO, GRPO, etc.),
                which <i>always</i> require fresh, newly sampled rollouts from the current policy,
                and cannot reuse previous data
                (<i>note: while PPO-like methods can technically reuse data to some (limited) degree, I'll classify them as on-policy RL,
                as in <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">OpenAI's documentation</a></i>).
                This is not a problem in <i>some</i> settings like board games and LLMs,
                where we can cheaply generate as many rollouts as we want.
                However, it is a significant limitation in <i>most</i> real-world problems.
                For example, in robotics, it takes <a href="https://x.com/KyleStachowicz/status/1885359401546162638">more than several months</a> in the real world to generate
                the amount of samples used to post-train a language model with RL,
                not to mention that a human must be present 24/7 next to the robot to reset it during the entire training time!
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/on_off.png">
                </p>
                <p><span>
                    On-policy RL can only use fresh data collected by the current policy \(\pi\).
                    Off-policy RL can use <i>any</i> data \(\mathcal{D}\).
                </span>
            </p></div>
        <div>
            <p>
                This is where <b>off-policy RL</b> comes to the rescue.
                In principle, off-policy RL algorithms can use <i>any</i> data, regardless of when and how it was collected.
                Hence, they generally lead to much better sample efficiency, by reusing data many times.
                For example, off-policy RL can train <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">a dog robot to walk in 20 minutes from scratch in the real world</a>.
                <b>Q-learning</b> is the most widely used off-policy RL algorithm.
                It minimizes the following temporal difference (TD) loss:
                <span>
                    $$\begin{aligned}
                    \mathbb{E}_{(s, a, r, s') \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \big(r + \gamma \max_{a'} Q_{\bar \theta}(s', a') \big) \Big)^2 \bigg],
                    \end{aligned}$$
                </span>
                where \(\bar \theta\) is the parameter of the target network.
                Most practical (model-free) off-policy RL algorithms are based on some variants of the TD loss above.
                So, to apply RL to many real-world problems,
                the question becomes: <b>does Q-learning (TD learning) scale?</b>
                If the answer is yes, this would lead to at least an equivalent level of impact as the successes of AlphaGo and LLMs,
                enabling RL to solve far more diverse and complex real-world tasks very efficiently,
                in robotics, computer-using agents, and so on.
            </p>
            <h2>Q-learning is not yet scalable</h2>
            <p>
                Unfortunately, my current belief is that the answer is <b>not yet</b>.
                I believe current Q-learning algorithms are not readily scalable, at least to <i>long-horizon</i> problems that require more than (say) 100 semantic decision steps.
            </p>
            <p>
                Let me clarify. My definition of scalability here is the ability to solve <i>more challenging, longer-horizon</i> problems
                with more data (of sufficient coverage), compute, and time.
                This notion is different from the ability to solve merely a <i>larger number</i> of (but not necessarily harder) tasks with a single model,
                which many excellent <a href="https://sites.google.com/view/scaling-offlinerl/home">prior</a> <a href="https://sites.google.com/view/perceiver-actor-critic">scaling</a> <a href="https://arxiv.org/abs/2505.23150">studies</a> have shown to be possible.
                You can think of the former as the "depth" axis and the latter as the "width" axis.
                The depth axis is more important and harder to push, because it requires developing more advanced decision-making capabilities.
            </p>
            <p>
                I claim that Q-learning, in its current form, is <i>not</i> highly scalable along the depth axis.
                In other words, I believe we still need <i>algorithmic breakthroughs</i> to scale up Q-learning (and off-policy RL) to complex, long-horizon problems.
                Below, I'll explain two main reasons why I think so:
                one is anecdotal, and the other is based on our <a href="https://arxiv.org/abs/2506.04168">recent scaling study</a>.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/logos.png">
                </p>
                <p><span>
                    Both AlphaGo and DeepSeek are based on <i>on-policy</i> RL and do not use TD learning.
                </span>
            </p></div>
        <div>
            <p>
                Anecdotal evidence first.
                As mentioned earlier, most real-world successes of RL are based on on-policy RL algorithms.
                AlphaGo, AlphaZero, and MuZero are based on model-based RL and Monte Carlo tree search, and do not use TD learning on board games
                (see 15p of the <a href="https://arxiv.org/abs/1911.08265">MuZero</a> paper).
                OpenAI Five achieves superhuman performance in Dota 2 with PPO
                (see footnote 6 of the <a href="https://arxiv.org/abs/1912.06680">OpenAI Five</a> paper).
                RL for LLMs is currently dominated by variants of on-policy policy gradient methods, such as PPO and GRPO.
                Let me ask: do we know of any <i>real-world</i> successes of off-policy RL (1-step TD learning, in particular) on a similar scale to AlphaGo or LLMs?
                If you do, please let me know and I'll happily update this post.
            </p>
            <p>
                Of course, I'm not making this claim based only on anecdotal evidence.
                As said before, I'll show concrete experiments to empirically prove this point later in this post.
                Also, please don't get me wrong, I'm still highly optimistic about off-policy RL and Q-learning (as an RL researcher who mainly works in off-policy RL!).
                I just think that we are not there yet, and <b>the purpose of this post is to call for research in RL algorithms, rather than to discourage it!</b>
            </p>
            <h2>What's the problem?</h2>
            <p>
                Then, what fundamentally makes Q-learning not readily scalable to complex, long-horizon problems, unlike other objectives?
                Here is my answer:
                <span>
                    $$\begin{aligned}
                    \definecolor{myblue}{RGB}{89, 139, 231}
                    \mathbb{E}_{(s, a, r, s') \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \underbrace{\big(r + \gamma \max_{a'} Q_{\bar \theta}(s', a') \big)}_{{\color{myblue}\texttt{Biased }} (\textit{i.e., }\neq Q^*(s, a))} \Big)^2 \bigg]
                    \end{aligned}$$
                </span>
                Q-learning struggles to scale because <b>the prediction targets are biased, and these biases <i>accumulate</i> over the horizon.</b>
                The presence of <b>bias accumulation</b> is a fundamental limitation that is <i>unique</i> to Q-learning (TD learning).
                For example, there are no biases in prediction targets in other scalable objectives
                (<i>e.g.</i>, next-token prediction, denoising diffusion, contrastive learning, etc.)
                or at least these biases do not accumulate over the horizon (<i>e.g.</i>, BYOL, DINO, etc.).
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/toy_accumulation_simple.png">
                </p>
                <p><span>
                    Biases accumulate over the horizon.
                </span>
            </p></div>
        <div>
            <p>
                As the problem becomes more complex and the horizon gets longer, the biases in bootstrapped targets accumulate more and more severely,
                to the point where we cannot easily mitigate them with more data and larger models.
                I believe this is the main reason why we almost never use larger discount factors (\(\gamma &gt; 0.999\)) in practice,
                and why it is challenging to scale up Q-learning.
                Note that policy gradient methods suffer much less from this issue.
                This is because <a href="https://arxiv.org/abs/1506.02438">GAE</a> or similar on-policy value estimation techniques
                can deal with longer horizons relatively more easily (though at the expense of higher variance), without strict 1-step recursions.
            </p>
            <h2>Empirical scaling study</h2>
            <p>
                In <a href="https://arxiv.org/abs/2506.04168">our recent paper</a>, we empirically verified the above claim via diverse, controlled scaling studies.
            </p>
            <p>
                We wanted to see whether current off-policy RL methods can solve highly challenging tasks by just scaling up data and compute.
                To do this, we first prepared highly complex, previously unsolved tasks in <a href="https://seohong.me/projects/ogbench/">OGBench</a>.
                Here are some videos:
            </p>
        </div>
        <div>
                
                <p><span>
                    <span>humanoidmaze</span><br>
                </span>
                <span>
                    <span>humanoidmaze</span><br>
                </span>
            </p></div>
        <div>
            <p>
                These tasks are really difficult.
                To solve them, the agent must learn complex goal-reaching behaviors from unstructured, random (play-style) demonstrations.
                At test time, the agent must perform precise manipulation, combinatorial puzzle-solving, or long-horizon navigation,
                over 1,000 environment steps.
            </p>
            <p>
                We then collected <i>near-infinite</i> data on these environments, to the degree that overfitting is virtually impossible.
                We also removed as many confounding factors as possible.
                For example, we focused on offline RL to abstract away exploration.
                We ensured that the datasets had sufficient coverage, and that all the tasks were solvable from the given datasets.
                We directly provided the agent with the ground-truth state observations to reduce the burden of representation learning.
            </p>
            <p>
                Hence, a "scalable" RL algorithm must really be able to solve these tasks, given sufficient data and compute.
                If Q-learning does not scale <b>even in this controlled setting with near-infinite data</b>,
                there is little hope that it will scale in more realistic settings,
                where we have limited data, noisy observations, and so on.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser1.png">
                </p>
                <p><span>
                    Standard offline RL methods struggle to scale on complex tasks, even with \(1000\times\) more data.
                </span>
            </p></div>
        <div>
            <p>
                So, how did the existing algorithms work?
                The results were a bit disappointing.
                None of the standard, widely used offline RL algorithms (flow BC, IQL, CRL, and SAC+BC) were able to solve all of these tasks,
                even with 1B-sized datasets, which are \(1000 \times\) larger than typical datasets used in offline RL.
                More importantly, their performance often plateaued far below the optimal performance.
                In other words, they didn't scale well on these complex, long-horizon tasks.
            </p>
            <p>
                You might ask:
                Are you really sure these tasks are solvable? Did you try larger models?
                Did you train them for longer? Did you try different hyperparameters? And so on.
                In the paper, we tried our best to address as many questions as possible with a number of ablations and controlled experiments,
                showing that <b>none</b> of these fixes worked...
                <b>except for one:</b>
            </p>
            <h2>Horizon reduction makes RL scalable</h2>
            <p>
                Recall that my claim earlier was that the <b>horizon</b> (and bias accumulation thereof) is the main obstacle to scaling up off-policy RL.
                To verify this,
                we tried diverse <i>horizon reduction</i> techniques (<i>e.g.</i>, n-step returns, hierarchical RL, etc.) that reduce the number of biased TD backups.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser2.png">
                </p>
                <p><span>
                    Horizon reduction was the only technique we found that substantially improved scaling.
                </span>
            </p></div>
        <div>
            <p>
                The results were promising!
                Even simple tricks like n-step returns
                significantly improved scalability and even <i>asymptotic performance</i>
                (so it is <i>not</i> just a "trick" that merely makes training faster!).
                Full-fledged hierarchical methods worked even better.
                More importantly, horizon reduction is the <b>only</b> technique that worked across the board in our experiments.
                This suggests that simply scaling up data and compute is <i>not</i> enough to address the curse of horizon.
                In other words, we need <i>better algorithms</i> that directly address this fundamental horizon problem.
            </p>
            <h2>Call for research: find a <i>scalable</i> off-policy RL objective</h2>
            <p>
                We saw that horizon reduction unlocks the scalability of Q-learning.
                So are we done? Can we now just scale up Q-learning?
                I'd say this is only the beginning.
                While it is great to know the cause and have some solutions,
                most of the current horizon reduction techniques (n-step returns, hierarchical RL, etc.) only <i>mitigate</i> the issue by a constant factor,
                and do not fundamentally solve the problem.
                I think <b>we're currently missing an off-policy RL algorithm that scales to arbitrarily complex, long-horizon problems</b>
                (or perhaps we may already have a solution, but just haven't stress-tested it enough yet!).
                I believe finding such a scalable off-policy RL algorithm is <b>the most important missing piece in machine learning today</b>.
                This will enable solving <i>much</i> more diverse real-world problems,
                including robotics, language models, agents, and basically any data-driven decision-making tasks.
            </p>
            <p>
                I'll conclude this post with my thoughts about potential solutions to scalable off-policy RL.
            </p>
            <ul>
                <li>
                    Can we find a simple, scalable way to extend beyond two-level hierarchies to deal with horizons of arbitrary lengths?
                    Such a solution should be able to naturally form a recursive hierarchical structure,
                    while being <i>simple enough</i> to be scalable.
                    One great example of this (though in a different field) is chain-of-thought in LLMs.
                </li>
                <li>
                    Another completely different approach (which I intentionally didn't mention so far for simplicity)
                    is <i>model-based RL</i>.
                    We know that model learning is scalable, because it's just supervised learning.
                    We also know that on-policy RL is scalable.
                    So why don't we combine the two, where we first learn a model and run on-policy RL within the model?
                    Would model-based RL indeed <i>scale</i> better than TD-based Q-learning?
                </li>
                <li>
                    Or is there a way to just completely avoid TD learning?
                    Among the methods that I know of,
                    one such example is <a href="https://www.tongzhouwang.info/quasimetric_rl/">quasimetric RL</a>,
                    which is essentially based on the LP formulation of RL.
                    Perhaps this sort of "exotic" RL methods, or MC-based methods like <a href="https://ben-eysenbach.github.io/contrastive_rl/">contrastive RL</a>, might eventually scale better than TD-based approaches?
                </li>
            </ul>
            <p>
                Our setup above can be a great starting point for testing these ideas.
                We have already designed a set of highly challenging robotic tasks, made the datasets, and verified that they are solvable.
                One can even make the tasks arbitrarily difficult (<i>e.g.</i>, by adding more cubes)
                and further stress-test the scalability of algorithms in a controlled way.
                We also put our efforts into making the code as clean as possible.
                Check out <a href="https://github.com/seohongpark/horizon-reduction">our code</a>!
            </p>
            <p>
                Feel free to let me know via email/Twitter/X or reach out to me at conferences if you have any questions, comments, or feedback.
                I hope that at some point, I can write another post about off-policy RL with a more positive title in the near future!
            </p>
            <h3>Acknowledgments</h3>
            <p>
                I would like to thank
                <a href="https://kvfrans.com/">Kevin Frans</a>,
                <a href="https://hongsukchoi.github.io/">Hongsuk Choi</a>,
                <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>,
                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a>,
                and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                for their helpful feedback on this post.
                This post is partly based on our recent work, <a href="https://arxiv.org/abs/2506.04168">Horizon Reduction Makes RL Scalable</a>.
                The views in this post are my own, and do not necessarily reflect those of my coauthors.
            </p>
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Grid of Resistors (177 pts)]]></title>
            <link>https://www.mathpages.com/home/kmath668/kmath668.htm</link>
            <guid>44279181</guid>
            <pubDate>Sat, 14 Jun 2025 22:12:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mathpages.com/home/kmath668/kmath668.htm">https://www.mathpages.com/home/kmath668/kmath668.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44279181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
 <tbody><tr>
  <td>
  <p><b><span>Infinite Grid of
  Resistors</span></b></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>�����������������
  Remain, remain thou here,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>While
  sense can keep it on. And, sweetest, fairest,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>As
  I my poor self did exchange for you,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>To
  your so infinite loss, so in our trifles</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>I
  still win of you: for my sake wear this...</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>������������������������� ������������������������������� Shakespeare</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>There is a well-known puzzle based on the premise of an
  �infinite� grid of resistors connecting adjacent nodes of a square lattice. A
  small portion of such a grid is illustrated below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><img width="232" height="229" src="https://www.mathpages.com/home/kmath668/kmath668_files/image001.png"></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Between every pair of adjacent nodes is a resistance R,
  and we�re told that this grid of resistors extends �to infinity� in all
  direction, and we�re asked to determine the effective resistance between two
  adjacent nodes, or, more generally, between any two specified nodes of the
  lattice.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For adjacent nodes, the usual solution of this puzzle is
  to consider the current flow field as the sum of two components, one being
  the flow field of a grid with current injected into a single node, and the
  other being the flow field of a grid with current extracted from a single
  (adjacent) node. The symmetry of the two individual cases then enables us to
  infer the flow rates through the immediately adjacent resistors, and hence we
  can conclude (as explained in more detail below) that the effective
  resistance between two adjacent nodes is R/2. This solution has a certain
  intuitive plausibility, since it�s similar to how the potential field of an
  electric dipole can be expressed as the sum of the fields of a positive and a
  negative charge, each of which is spherically symmetrical about its
  respective charge. Just as the electric potential satisfies the Laplace equation, the voltages of the grid nodes satisfy the discrete from of the Laplace equation, which is to say, the voltage at each node is the average of the voltages
  of the four surrounding nodes. It�s also easy to see that solutions are
  additive, in the sense that the sum of any two solutions for given boundary
  conditions is a solution for the sum of the boundary conditions.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>If we accept the premise of an infinite grid of resistors,
  along with some tacit assumption about the behavior of the voltages and
  currents �at infinity�, and if we accept the idea that we can treat the
  current fields for the positive and negative nodes separately, and that
  applying a voltage to a single node of the infinite grid will result in some
  current flow into the grid, the puzzle is easily solved by simple symmetry
  considerations. We assert (somewhat naively) that if we inject (say) four
  Amperes of current into a given node, with no removal of current at any
  finite point of the grid, the current will flow equally out through the four
  resistors, so one Ampere will flow toward each of the four adjacent nodes.
  This one Ampere must flow out through the three other lines emanating from
  that adjacent node, as indicated in the left hand figure below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><img width="375" height="155" src="https://www.mathpages.com/home/kmath668/kmath668_files/image002.png"></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The figure on the right shows the four nodes surrounding
  the �negative� node, assuming we are extracting four Amperes from that node
  (with no current injected at any finite node of the grid). Again, simple
  symmetry dictates the distribution of currents indicated in the figure.
  Adding the two current fields together, we see that the link between the
  positive and negative nodes carries a total of 2 Amperes away from the
  positive node, and the other three links emanating from the positive node
  carry away a combined total of 1 + 1 + 1 <span>-</span>
  (2α <span>+</span> β<span>)</span> = 2 Amperes. Thus the direct link carries
  the same current as all the other paths, so the resistance of the direct link
  equals the effective resistance of the entire grid excluding that link. The
  direct link is in parallel with the remainder of the grid, so the combined
  resistance is simply R/2.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This is an appealing argument, and it certainly gives the
  �right� answer (as can be verified by other methods), but the premises are
  somewhat questionable, and the reasoning involves some subtle issues that
  need to be addressed before it can qualify as a rigorous proof. The
  fundamental problem with the simple argument, as stated above, is that it
  relies on the notion of forcing current into a node of an infinite grid,
  without satisfactorily explaining where this current goes. One �hand-waving�
  explanation is that we may regard the grid as being grounded �at infinity�,
  but this isn�t strictly valid, because the resistance from any given node to
  �infinity� is infinite. This is easily seen from the fact that a given node
  is surrounded by a sequence of concentric squares, and the number of
  resistive links beginning from the central node and expanding outward to
  successive concentric squares are 4, 12, 20, 28, �, etc., which implies that
  the total resistance to infinity is (at least roughly)</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="381" height="45" src="https://www.mathpages.com/home/kmath668/kmath668_files/image003.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The odd harmonic series diverges, so this resistance is
  infinite. Therefore, in order for current to enter the grid at a node and
  exit at infinity, we would require infinite potential (i.e., voltage) between
  the node and the grid points �at infinity�. To make the argument rigorous, we
  could consider a large but finite grid, and convince ourselves that the
  behavior approaches the expected result in the limit as the grid size
  increases. This is not entirely trivial, because we must be sure the two
  sequences of expanding grids, one concentric about the positive node and one
  concentric about the negative node, approach mutually compatible boundary
  conditions in the limit, giving zero net flow �to infinity�. To evaluate this
  limit, the voltage at the central node (relative to the voltage at infinity)
  must approach infinity to provide a fixed amount of current. This is
  discussed further in another <span><a href="https://www.mathpages.com/home/kmath669/kmath669.htm">note</a></span>, where we also discuss the
  arbitrariness of the solution for a truly �infinite� grid. Strictly speaking,
  for a truly infinite grid, the solution is indeterminate unless some
  asymptotic boundary conditions are imposed (which are not specified in the
  usual statements of the problem). </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The unphysical aspect of the problem can also be seen in
  the fact that the flow field is assumed to be fully developed, to infinity, a
  situation that could not have been established by any realistic physical
  process in any finite amount of time. Of course, the postulated grid consists
  purely of ideal resistances, with no capacitances or inductances, so there
  are no dynamics to consider, and hence one could argue that the entire
  current field is established instantaneously to infinity - but this merely
  illustrates that the postulated grid is idealized to the point of violating
  the laws of physics. All real circuits have capacitance and inductance, which
  is why the propagation speed cannot be infinite. One might think that such
  idealizations are harmless for this problem, but they actually render the
  problem totally indeterminate if we apply them rigorously. Our intuitive
  sense that there is a unique answer comes precisely from our unconscious
  imposition of �physically reasonable� asymptotic behavior emanating from a
  localized source, based on the asymptotic behavior of a finite grid as the
  size increases � a conception that arises from our physical notions of
  locality and finite propagation of effects, notions which are not justified
  in the idealized setting.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Setting aside these issues, and just naively adopting the
  usual tacit assumptions about the asymptotic conditions of the grid, we can
  consider the more general problem of determining the resistance between any
  two nodes. The most common method is based on superimposing solutions of the
  basic difference equation. Again this method tacitly imposes plausible
  boundary conditions to force a unique answer, essentially by requiring that
  the grid behaves like the limit of a large finite grid. Consider first the
  trivial example of a one-dimensional �grid� of unit resistors, in which the
  net current emanating from the nth node is given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="152" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image004.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we stipulate that I<sub>0</sub> = <span>-</span>1 Amp and I<sub>n</sub> = 0 for all n
  ≠ 0. Notice that for all n ≠ 0 would could negate the signs of
  the indices on the right hand side without affecting the equation, because
  the net current is zero at those nodes, and the equations above and below the
  origin are symmetrical. However, the case n = 0 is different if we stipulate
  that V<sub>1</sub> = V<sub><span>-</span>1</sub>,
  and if we stipulate that I<sub>0</sub> = �1, which implies</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="173" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image005.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>It follows that, for unit resistors, if we stipulate V<sub>0</sub>
  = 0, we must have V<sub>1</sub> = 1/2. Therefore, if we imagine current being
  extracted from just the node at the origin, while all the other nodes have
  zero net current flow, then for all n ≠ 0 we have the relation</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="149" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image006.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The characteristic polynomial is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="95" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image007.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For any value of μ that satisfies this equation, it�s
  clear that one solution of the preceding difference equation is V<sub>n</sub>
  = Aμ<sup>n</sup> for any constant A. However, the characteristic
  equation has the repeated roots μ<sub>1</sub> = μ<sub>2</sub> = 1,
  so we have a resonance term, and the general form of the solution of the
  discrete difference equation is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="267" height="31" src="https://www.mathpages.com/home/kmath668/kmath668_files/image008.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>for some constants A and B, chosen to make the solution
  consistent with the specified boundary conditions. We want V<sub>0</sub> = 0,
  so we must put A = 0. Also, since the recurrence relation doesn�t apply at n
  = 0, we can chose B equal to +1/2 for positive n, and �1/2 for negative n,
  which amounts to taking the absolute value of n, giving the result V<sub>n</sub>
  = |n|/2. This implies that the effective resistance between nodes separated
  by k resistors is (as expected) simply kR, where R is the resistance of an
  individual resistor.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We can similarly consider the difference equation for
  grids of higher dimensions. For a two-dimensional grid, the current emanating
  from node (m,n) for all m,n &gt; 0 is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="516" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image009.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We stipulate that I<sub>m,n</sub> = 0 for all m,n &gt; 0,
  so the characteristic equation for this two-dimensional difference equation
  is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="440" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image010.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This shows that there are infinitely many �eigenvalues�.
  Indeed, for any value of μ we can solve for the corresponding value
  ν, and vice versa. For any such pair of values μ,ν satisfying
  this equation it�s clear that a solution of the preceding difference equation
  is given by V<sub>m,n</sub> = A(μ,ν) μ<sup>m</sup>ν<sup>n</sup>
  for any constant A(μ,ν). If we define parameters α,β such
  that iα = ln(μ) and iβ = ln(ν), then these solutions can
  be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="475" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image011.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>In terms of α and β the characteristic equation
  can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="460" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image012.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now, any combination of these solutions will satisfy the
  original difference equation for nodes with zero net current, but we want I<sub>0,0</sub>
  = <span>-</span>1 Amp, and to achieve this we
  (again) take the absolute values of the indices, i.e., we define</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="524" height="28" src="https://www.mathpages.com/home/kmath668/kmath668_files/image013.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For values of m and n different from zero, taking the absolute
  values has no effect on the difference equation, i.e., it still gives zero
  net current. However, for m = n = 0 we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="424" height="24" src="https://www.mathpages.com/home/kmath668/kmath668_files/image014.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This shows that if we put V<sub>0,0</sub> = 0 then for �1
  Amp of current we must have V<sub>1,0</sub> = 1/4 volts, which is consistent
  with the fact that the resistance between two adjacent nodes of 1/2 ohms,
  because we superimpose this solution with an equal and opposite solution
  centered on the adjacent node with +1 Amp current.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We don�t yet have a definite solution satisfying all the
  conditions, because with the absolute values of the indices we generally get
  non-zero currents for I<sub>m,0</sub> and I<sub>0,n</sub>. To solve this
  problem, we will simply superimpose several of these solutions, and impose
  the requirement that the net currents of the form I<sub>m,0</sub> and I<sub>0,n</sub>
  all vanish. To do this, we integrate the solutions of the form (4) over
  α ranging from �π to π. Thus we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="517" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image015.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>with the understanding that β is given as a function
  of α by equation (3). We will find that this determines the function
  A(α,β). Consider first the requirement I<sub>m,0</sub> = 0.
  Inserting the expression for V<sub>m,0</sub> from equation (4) into equation
  (1), we have, for all m &gt; 0, </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="305" height="72" src="https://www.mathpages.com/home/kmath668/kmath668_files/image016.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making use of the characteristic equation (3), this can be
  written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="281" height="72" src="https://www.mathpages.com/home/kmath668/kmath668_files/image017.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now we seek to superimpose many of these solutions such
  that the net current for these expressions is zero. To do this, we will
  integrate this expression over α ranging from �π to π. Thus we
  have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="531" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image018.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>At this point we recall that the exponential Fourier
  series for an arbitrary function f(x) is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="397" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image019.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Therefore we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="308" height="48" src="https://www.mathpages.com/home/kmath668/kmath668_files/image020.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we�ve made use of the fact that I<sub>0,0</sub> = <span>-</span>1 and I<sub>m,0</sub> = 0 for all m
  ≠ 0. From this we infer that</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="168" height="47" src="https://www.mathpages.com/home/kmath668/kmath668_files/image021.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Inserting this into equation (5), we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image022.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Again, it�s understood that β is given as a function
  of α by equation (3). It might seem as if we cannot now force the
  currents I<sub>0,n</sub> to equal zero. If we had imposed that requirement
  first, instead of I<sub>m,0</sub> = 0, by symmetry we would have found</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image023.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>which seems superficially different from (7). However,
  notice that the limits of integration are reversed, because α and β
  progress in opposite directions. Furthermore, if we take the differential of
  the characteristic equation (3) we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="132" height="47" src="https://www.mathpages.com/home/kmath668/kmath668_files/image024.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>which proves that (7) and (8) are equivalent. As discussed
  previously, the resistance between the origin and the node (m,n) is twice the
  value of V<sub>m,n</sub>� <span>-</span> V<sub>0,0</sub>,
  so we have the formula</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="273" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image025.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This can be split into two integrals, one ranging from
  �π to 0, and the other ranging from 0 to +π, as follows:</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="485" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image026.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Reversing the sign of α in the first integral, and
  noting that cos(�α) = cos(α),� this can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="441" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image027.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>and hence we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="516" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image028.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For example, the resistance between two diagonal corners
  of a lattice square is given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="267" height="59" src="https://www.mathpages.com/home/kmath668/kmath668_files/image029.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>If we define the variable s = cos(α) we have α =
  acos(s) and </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="87" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image030.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>As α ranges from 0 to π, the parameter s ranges
  from 1 to �1, so the preceding integral can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="272" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image031.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We now make use of the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="213" height="29" src="https://www.mathpages.com/home/kmath668/kmath668_files/image032.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>to re-write the integral as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="239" height="52" src="https://www.mathpages.com/home/kmath668/kmath668_files/image033.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Also, from the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="301" height="43" src="https://www.mathpages.com/home/kmath668/kmath668_files/image034.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we have</p>
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="380" height="48" src="https://www.mathpages.com/home/kmath668/kmath668_files/image035.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>so the integral can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="387" height="203" src="https://www.mathpages.com/home/kmath668/kmath668_files/image036.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>(This same result can be derived purely algebraically,
  without the use of Fourier series, as described in another <span><a href="https://www.mathpages.com/home/kmath673/kmath673.htm">note</a></span>.)
  Knowing this resistance value for diagonal nodes, and the resistance value
  1/2 for adjacent nodes, we can immediately compute the resistances to several
  other nodes by simple application of the basic difference equation. Thus we
  have the resistances relative to the origin shown below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="372" height="307" src="https://www.mathpages.com/home/kmath668/kmath668_files/image037.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Returning to equation (9), we see that the same
  substitutions and identities that we used to simplify R<sub>1,1</sub> enable
  us to write the general expression as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="324" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image038.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where h<sub>m</sub>(s) denote the trigonometric
  polynomials giving cos(mα) as a function of s = cos(α). The first
  several of these polynomials are</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="155" height="187" src="https://www.mathpages.com/home/kmath668/kmath668_files/image039.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The coefficients of these polynomials are given by a
  simple recurrence relation, and they also have a simple trigonometric
  expression. However, we don�t actually need to deal with these polynomials,
  because it is sufficient to determine the values of R<sub>0,n</sub> by the
  above integral, and then all the remaining resistances are easily determined
  by simple algebra. Hence we can focus on just the integrals</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="279" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image040.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>To simplify this still further, we can define the
  parameter</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="169" height="28" src="https://www.mathpages.com/home/kmath668/kmath668_files/image041.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>in terms of which s and ds are given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="276" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image042.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Re-writing the expression for R<sub>0,n</sub> in terms of
  the parameter σ, we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="259" height="55" src="https://www.mathpages.com/home/kmath668/kmath668_files/image043.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making use of the indefinite integrals</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="385" height="192" src="https://www.mathpages.com/home/kmath668/kmath668_files/image044.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where</p>
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="113" height="24" src="https://www.mathpages.com/home/kmath668/kmath668_files/image045.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we can determine the values</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="468" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image046.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>and so on.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Another way of simplifying the integrals involved in the
  infinite grid solution is to return to equation (7), focusing on the case of
  positive m and n with m &gt; n, and recalling that the resistance from the
  origin to the node (m,n) is <i>twice</i> the voltage given by (7), so we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="501" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image047.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where, for convenience, we�ve shifted the limits of
  integration from the range [�π,+π] to the range [0,2π]. Now
  suppose we define new variables r,s such that α = r + s and β = r �
  s. Substituting for α and β in equation (3) gives the relation
  cos(r)cos(s) = 1. In terms of these parameters equation (10) can be written
  as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image048.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Obviously we have dα = dr + ds, and differentiating
  the relation cos(r)cos(s) = 1 gives</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="147" height="49" src="https://www.mathpages.com/home/kmath668/kmath668_files/image049.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>so we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="509" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image050.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making this substitution into (11) gives</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="544" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image051.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now, since cos(r)cos(s)=1 where cos(z)=(e<sup>iz</sup>+e<sup>−iz</sup>)/2,
  and in view of the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="261" height="67" src="https://www.mathpages.com/home/kmath668/kmath668_files/image052.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we see that r and s can be expressed parametrically in
  terms of a single parameter t such that</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="201" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image053.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>From this we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="271" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image054.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>With these substitutions, equation (12) becomes</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="529" height="59" src="https://www.mathpages.com/home/kmath668/kmath668_files/image055.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>To this point we have continued to specify the limits of
  integration in terms of α, but now we note as α ranges over the
  real values from 0 to 2π we have t = (1�i)τ where τ is a
  real-valued parameter ranging from infinity to 0. Hence we make this change
  of variable to express the result as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="551" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image056.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we�ve made use of the fact that the variable τ
  can be multiplied by an arbitrary factor inside the curved parentheses
  without affecting the integral. For the first diagonal node we have m = n = 1
  and the integral is simply</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="417" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image057.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For the resistances to the other nodes along the diagonal
  of the lattice, notice that for any m we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="453" height="243" src="https://www.mathpages.com/home/kmath668/kmath668_files/image058.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Consequently we have the well-known result</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="247" height="45" src="https://www.mathpages.com/home/kmath668/kmath668_files/image059.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>from which all the other resistances are easily computed
  using the basic recurrence relation (1). In another <span><a href="https://www.mathpages.com/home/kmath669/kmath669.htm">note</a></span> we consider the same problem
  from a more algebraic standpoint.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><a href="https://www.mathpages.com/home/index.htm">Return to
  MathPages Main Menu</a></span></p>
  </td>
 </tr>
</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seven replies to the viral Apple reasoning paper and why they fall short (299 pts)]]></title>
            <link>https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</link>
            <guid>44278403</guid>
            <pubDate>Sat, 14 Jun 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple">https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</a>, See on <a href="https://news.ycombinator.com/item?id=44278403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The Apple paper on </span><a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf" rel="">limitations in the “reasoning” of Large Reasoning Models</a><span>, which raised challenges for the latest scaling hypothesis, has clearly touched a nerve. Tons of media outlets covered it; huge numbers of people on social media are discussing. </span></p><p><span>M</span><a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">y own post here laying out the Apple paper in historical and scientific context </a><span>was so popular that well over 150,000 people read it, biggest in this newsletter’s history. </span><em>The Guardian</em><span> published an adaptation of my post (“</span><a href="https://www.theguardian.com/commentisfree/2025/jun/10/billion-dollar-ai-puzzle-break-down" rel="">When billion-dollar AIs break down over puzzles a child can do, it’s time to rethink the hype</a><span>”) The editor tells me readers spent a long time reading it, notably longer than usual, as if people really wanted to understand the arguments in detail.  (The ACM computer science society is reposting the essay, too, and </span><a href="https://legrandcontinent.eu/fr/2025/06/10/ia-llm-marcus/" rel="">there is now a French version as well</a><span>).</span></p><p>Tons of GenAI optimists took cracks at the Apple paper (see below), and it is worth considering their arguments. Overall I have seen roughly seven different efforts at rebuttal, ranging from nitpicking and ad hominem to the genuinely clever. Most (not all) are based on grains of truth, but are any of them actually compelling?</p><p> Let’s consider.</p><ol><li><p><strong>Humans have trouble with complex problems and memory demands</strong><span>. True! But incomplete.  We have every right to expect machines to do things we can’t. Cars have more stamina, calculators don’t make arithmetical errors. That’s why we invented computers: to do repetitive calculation without errors. And in many cases (including the Tower of Hanoi, which featured prominently in the paper)) we have existing systems that work perfectly without errors. AGI should be a step forward; in many cases LLMs are a step backwards. And note the bait and switch from “we’re going to build AGI that can revolutionize the world” to “give us some credit, our systems make errors and humans do, too”. The real takeaway from the Apple paper is that LLMs can’t be trusted to run algorithms as complexity and distance from the training distribution grows (just as humans shouldn’t serve as calculators). If we want to get to AGI, we will have to better.</span></p></li><li><p><strong>The Large Reasoning Models (LRMs) couldn’t possibly solve the problem, because the outputs would require too many output tokens</strong><span> (which is to say the correct answer would be too long for the LRMs to produce). Partial truth, and a clever observation: LRMs (which are enhanced LLMs) have a shortcoming, which is a limit on how long their outputs can be. The correct answer to Tower of Hanoi with 12 moves would be too long for some LRMs to spit out, and the authors should have addressed that.  But crucially (i) this objection, clever as it is, doesn’t actually explain the overall pattern of results. The LRMs failed on  Tower of Hanoi with 8 discs, where the optimal solution is 255 moves, well within so-called token limits; (ii) well-written symbolic AI systems generally don’t suffer from this problem, and AGI should not either. The length limit on LLM is a bug, and most certainly not a feature. And look, if an LLM can’t reliably execute something as basic as Hanoi, what makes you think it is going to compute military strategy (especially with the fog of war) or molecular biology (with many unknowns) correctly? What the Apple team asked for was way easier than what the real world often demands.</span></p></li><li><p><strong>The paper was written by an intern</strong><span>. This one, which infuriated me because is a form of ad hominem argument rather than substance, is misguided and only barely true – and utterly lacking in context. It is </span><em>true</em><span> that the first author was an intern at Apple, Parshin Shojaee, but (i) she (not </span><em>he</em><span> as some fool I won’t name presumed, without spending two seconds of research) also happens to be </span><a href="https://parshinsh.github.io/" rel="">a very promising third year Ph.D. student with many paper presentations at leading conferences</a><span>, (ii) the article, if you actually read it, makes it clear she shared lead responsibility with </span><a href="https://imirzadeh.me/" rel="">Iman Mirzadeh</a><span>, who has a Ph.D.;  (iii) the paper actually has six authors, not one, and four have Ph.D.s; for good measure one is Yoshua Bengio’s brother </span><a href="https://bengio.abracadoudou.com/" rel="">Samy Bengio</a><span>, well-known and very distinguished in his own right within the machine learning community; (iv) it is a common practice in many scientific fields to put the junior author first, senior author last, as this paper did; thousands of major articles have done the same (and never been criticized for doing so — it’s a true desperation measure); (v) what really matters is the quality of the work. Alfred Sturtevant was an </span><em>undergraduate</em><span> when he invented gene maps. </span></p></li><li><p><strong>Bigger models might to do better</strong><span>. True, and this is always the case (I have seen one report that o3-pro can do at least one of the problems, at least some of the time; I have not seen a thorough study yet).  Bigger models sometimes do better because of genuine improvements in the model, sometimes because of problem-specific training. (From the outside we can never know which). But here’s the thing, we can’t know in advance what model is big enough (if any) for any given problem. And Apple’s result that some pretty large models could succeed at 6 discs, giving the illusion of mastery, but collapse by 8, is ominous.  One is left simply having to test everything, all the time, with little guarantees of anything. Some model might be big enough for task T of size S and fail on the next size, or on Task T’ that is slightly different, etc. It all becomes a crapshoot. Not good.</span></p></li><li><p><strong>The systems can solve the puzzles with code.</strong><span> True in some cases, and a huge win for neurosymbolic AI, given that they can’t reliably solve the puzzles without code, and given that code is symbolic.  </span><em>Huge</em><span> vindication for what I have been saying all along: we need AI that integrates both neural networks and symbolic algorithms and representations (such as logic, code, knowledge graphs, etc). But also, we need to do so reliably, and in a general way and we haven’t yet crossed that threshold. (Importantly, the point of the Apple paper goal was to see how LRM’s unaided explore a space of solutions via reasoning and backtracking, not see how well it could use preexisting code retrieved from the web.) An analogy: A student might complain about a math exam requiring integration or differentiation by hand, even though math software can produce the correct answer instantly. The teacher’s goal in assigning the problem, though, isn’t finding the answer to that question (presumably the teacher already know the answer),  but to assess the student’s conceptual understanding. Do LLM’s </span><em>conceptually</em><span> understand Hanoi?  That’s what the Apple team was getting at. (Can LLMs download the right code? Sure. But downloading code without conceptual understanding  is of less help in the case of new problems, dynamically changing environments, and so on.)</span></p></li><li><p><strong>The paper has only four examples, and at least one of them (Hanoi) isn’t perfect</strong><span>. I doubt any of them are perfect, but the four together provide converging evidence with dozens of other prior papers (including some of my own), and I am confident many more examples will be uncovered.  I already found a couple of analogous failures in algorithm application myself, which I will write about in a few days. Tal Linzen at NYU </span><a href="https://x.com/tallinzen/status/1933184078821360084?s=61" rel="">just published yet another example</a><span>, with “</span><em>models .. able to do the right thing for simple versions of [a language] problem (small grammars, short strings), but [with] accuracy drop[ping] very quickly as the problem becomes more complex</em><span>.” Mark my words: in time, we will see scores of papers reinforcing the Apple results. </span></p></li><li><p><strong>The paper is not news; we already knew these models generalize poorly</strong><span>. True! (I personally have been trying to tell people this for almost thirty years; Subbarao Rao Kambhampati has been trying his best, too). But then why do we think these models are the royal road to AGI? The real news here, aside from the fact that this was a clever study nailing down an important point, is that </span><em>people are finally starting to pay attention,</em><span> to (one of the) two biggest Achilles’ Heels of generative AI, and to appreciate its significance. (Tune in to this newsletter on the weekend to hear about the other.)  Hilarious, by the way, is hearing both “it’s wrong” and “we knew it all along”, simultaneously. In at least one case I saw a single person say both, minutes apart!</span></p></li></ol><p>Bottom line? None of the rejoinders are compelling. If people like Sam Altman are sweating, it’s because they should. The Apple paper is yet another clear sign that scaling is not the answer; for once, people are paying attention. </p><p>§</p><p>The kicker? A Salesforce paper also just posted, that many people missed:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" width="1251" height="1791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1791,&quot;width&quot;:1251,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:523217,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/165791446?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In the “multi-turn” condition, which presumably would require reasoning and algorithmic precision, performance was only 35%. </p><p>Talk about convergence evidence. Taking the SalesForce report together with the Apple paper, it’s clear the current tech is not to be trusted.</p><p><em><strong>Gary Marcus</strong><span>, professor emeritus at NYU, and author of The Algebraic Mind and “Deep learning is hitting a wall”, both of which anticipated the correct results, is thrilled to see people finally realize that scaling is not enough to get us to AGI. Now perhaps we can begin to build better AI.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo's market share in San Francisco exceeds Lyft's (137 pts)]]></title>
            <link>https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</link>
            <guid>44277355</guid>
            <pubDate>Sat, 14 Jun 2025 16:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/">https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</a>, See on <a href="https://news.ycombinator.com/item?id=44277355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><article id="post-141765"><div><p><img width="810" height="608" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1" alt="" decoding="async" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=5356&amp;ssl=1 5356w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1500%2C1125&amp;ssl=1 1500w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=2048%2C1536&amp;ssl=1 2048w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=1620&amp;ssl=1 1620w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=2430&amp;ssl=1 2430w" sizes="(max-width: 810px) 100vw, 810px" data-attachment-id="141769" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/san-francisco-ca-usa-california-street-autonomes-fahrzeug/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=5356%2C4017&amp;ssl=1" data-orig-size="5356,4017" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;8&quot;,&quot;credit&quot;:&quot;Dietmar Rabich&quot;,&quot;camera&quot;:&quot;Canon EOS 5D Mark IV&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1656635759&quot;,&quot;copyright&quot;:&quot;Dietmar Rabich, D\u00fclmen&quot;,&quot;focal_length&quot;:&quot;105&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.005&quot;,&quot;title&quot;:&quot;San Francisco (CA, USA), California Street, autonomes Fahrzeug (&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="San Francisco (CA, USA), California Street, autonomes Fahrzeug (" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1"></p><h2><span>If a data-backed trend plays out, Waymo could become San Francisco’s biggest ride-hailing service before the year ends.</span></h2><p><a href="https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?gaa_at=eafs&amp;gaa_n=ASWzDAhnWF-iL0nB_VD1P9sE3oEvTzxfPRciScEXu2atNMFFerpPTdeL9vKlknl-ljg%3D&amp;gaa_ts=684048ca&amp;gaa_sig=T-9NFT-D_HBZgQzoyvZnU-0sp1-gS4FSVJMIo57FFlhxVVVMK8UFbvp6Eu_ndwTO23KoDxeZ9sZoC09rHkiDCg%3D%3D"><span>Waymo’s celestial ascent into the cultural zeitgeist</span></a><span> — a rise that has been propelled by dystopian memes and sheer, futuristic novelty — has only been matched by how it continues swallowing its competition. The Alphabet-owned autonomous driving company saw explosive exponential growth in ridership in 2024, with driverless rides increasing from </span><a href="https://underscoresf.com/self-driving-world-record-was-quietly-broken-in-sf-bay-area-last-week/"><span>77,000 to more than 312,000 lifts</span></a><span> by August of last year alone, according to the California DMV; as of publishing, Waymo asserts that 30% of their rides are to local small businesses.</span></p><figure id="attachment_141767" aria-describedby="caption-attachment-141767"><img data-recalc-dims="1" decoding="async" data-attachment-id="141767" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/502321336_17879766099336220_8766268074646908687_n/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=1236%2C919&amp;ssl=1" data-orig-size="1236,919" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="502321336_17879766099336220_8766268074646908687_n" data-image-description="<p>Screenshot: Courtesy of YipitData</p>
" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=300%2C223&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=810%2C602&amp;ssl=1" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=810%2C602&amp;ssl=1" alt="" width="810" height="602" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?w=1236&amp;ssl=1 1236w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=300%2C223&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=768%2C571&amp;ssl=1 768w" sizes="(max-width: 810px) 100vw, 810px"><figcaption id="caption-attachment-141767">Screenshot: Courtesy of YipitData</figcaption></figure><p><span>Independent contractors for Lyft and Uber have been </span><a href="https://www.theregister.com/2024/09/07/uber_driver_waymo/"><span>saying they’re “cooked”</span></a><span> for a while, citing massive declines in available requests as a result of Waymo’s success. (This, however, is a tandem issue: Waymo’s ride-hailing operations in San Francisco coincided with the increased number of regional rideshare drivers that began working during and after the pandemic.) But now factual data is showing that the aforementioned broiling is, indeed, happening … and at a rate quicker than once thought.</span></p><p><span>According to YipitData, a data and analytics firm based out of New York City, Waymo’s gross bookings from August of 2023 to April of this year have surpassed Lyft’s in market share. The twenty-month data analysis highlighted Uber’s dominance in San Francisco ridership — well over 50% of all trips booked via a ridesharing application were done on Uber throughout the analysis — but showed, perhaps more surprisingly, how quickly Waymo clambered into the commonplace. Waymo is also currently beating Lyft, a company that has operated rides in San Francisco since 2012, in total gross bookings.&nbsp;</span></p><p><span>In a staggeringly short amount of time, Waymo, which is about to celebrate its first anniversary of city-wide ride-hailing operations, has gone from effectively 0% market share of bookings in San Francisco to over 25%. Lyft has continuously gathered fewer bookings than Uber, but still managed to hold onto a roughly 30% market share since 2023. </span><span><br> </span><span><br> </span><span>Waymo has now flipped that stake, surpassing Lyft to become San Francisco’s second most-popular ride-hailing service. If the research published by YipitData is extrapolated outwardly, Waymo could easily beat Uber to become SF’s foremost taxi-like service by early next year. Or sooner.</span></p><p><span>What does this mean for San Francisco, the city that launched ridesharing services as we know them today? On the roads, not much; self-driving Jaguar iPaces would become even more prominent. But on an economic level, a subset of blue-collar workers (which numbers in the tens of thousands in San Francisco) would find themselves either regionally displaced or outright vocationally exterminated by a branch of artificial intelligence.&nbsp;</span></p><p><span>We don’t need a graph to tell you how that window into a </span><a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic"><span>looming dystopian landscape plays out.</span></a><span> But hey, at least you won’t have to surrender to mind-numbing small talk on </span><a href="https://missionlocal.org/2025/03/sf-waymo-sfo-airport-robotaxis-autonomous-vehicles-teamsters/"><span>your way to SFO.</span></a></p><hr><p><em>Photo: Courtesy of Wikimedia Commons</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SSHTron: A multiplayer lightcycle game that runs through SSH (108 pts)]]></title>
            <link>https://github.com/zachlatta/sshtron</link>
            <guid>44277245</guid>
            <pubDate>Sat, 14 Jun 2025 16:22:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zachlatta/sshtron">https://github.com/zachlatta/sshtron</a>, See on <a href="https://news.ycombinator.com/item?id=44277245">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bc9d7c30610ff3856269304c178036353443e404143412dc895b3300732b1d1c/68747470733a2f2f63646e2e7261776769742e636f6d2f7a6163686c617474612f73736874726f6e2f6d61737465722f6c6f676f2e737667"><img src="https://camo.githubusercontent.com/bc9d7c30610ff3856269304c178036353443e404143412dc895b3300732b1d1c/68747470733a2f2f63646e2e7261776769742e636f6d2f7a6163686c617474612f73736874726f6e2f6d61737465722f6c6f676f2e737667" alt="SSHTron" data-canonical-src="https://cdn.rawgit.com/zachlatta/sshtron/master/logo.svg"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto">SSHTron is a multiplayer lightcycle game that runs through SSH. Just run the command below and you'll be playing in seconds:</p>
<div data-snippet-clipboard-copy-content="$ ssh sshtron.zachlatta.com"><pre><code>$ ssh sshtron.zachlatta.com
</code></pre></div>
<p dir="auto"><em>Controls: WASD or vim keybindings to move (<strong>do not use your arrow keys</strong>). Escape or Ctrl+C to exit.</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/zachlatta/sshtron/blob/master/static/img/gameplay.gif"><img src="https://github.com/zachlatta/sshtron/raw/master/static/img/gameplay.gif" alt="Demo" data-animated-image=""></a></p>
<p dir="auto"><strong>Code quality disclaimer:</strong> <em>SSHTron was built in ~20 hours at <a href="https://brickhack.io/" rel="nofollow">BrickHack 2</a>. Here be dragons.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Want to choose color yourself?</h2><a id="user-content-want-to-choose-color-yourself" aria-label="Permalink: Want to choose color yourself?" href="#want-to-choose-color-yourself"></a></p>
<p dir="auto">There are total 7 colors to choose from: Red, Green, Yellow, Blue, Magenta, Cyan and White</p>
<div data-snippet-clipboard-copy-content="$ ssh red@sshtron.zachlatta.com"><pre><code>$ ssh red@sshtron.zachlatta.com
</code></pre></div>
<p dir="auto">If the color you picked is already taken in all open games, you'll randomly be assigned a color.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Your Own Copy</h2><a id="user-content-running-your-own-copy" aria-label="Permalink: Running Your Own Copy" href="#running-your-own-copy"></a></p>
<p dir="auto">Clone the project and <code>cd</code> into its directory. These instructions assume that you have your <code>GOPATH</code> setup correctly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create an RSA public/private keypair in the current directory for the server
# to use. Don't give it a passphrase.
$ ssh-keygen -t rsa -f id_rsa

# Download dependencies and compile the project
$ go get &amp;&amp; go build

# Run it! You can set PORT to customize the HTTP port it serves on and SSH_PORT
# to customize the SSH port it serves on.
$ ./sshtron"><pre><span><span>#</span> Create an RSA public/private keypair in the current directory for the server</span>
<span><span>#</span> to use. Don't give it a passphrase.</span>
$ ssh-keygen -t rsa -f id_rsa

<span><span>#</span> Download dependencies and compile the project</span>
$ go get <span>&amp;&amp;</span> go build

<span><span>#</span> Run it! You can set PORT to customize the HTTP port it serves on and SSH_PORT</span>
<span><span>#</span> to customize the SSH port it serves on.</span>
$ ./sshtron</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running under a Docker container</h2><a id="user-content-running-under-a-docker-container" aria-label="Permalink: Running under a Docker container" href="#running-under-a-docker-container"></a></p>
<p dir="auto">Clone the project and <code>cd</code> into its directory.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the SSHTron Docker image
$ docker build -t sshtron .

# Spin up the container with always-restart policy
$ docker run -t -d -p 2022:2022 --restart always --name sshtron sshtron"><pre><span><span>#</span> Build the SSHTron Docker image</span>
$ docker build -t sshtron <span>.</span>

<span><span>#</span> Spin up the container with always-restart policy</span>
$ docker run -t -d -p 2022:2022 --restart always --name sshtron sshtron</pre></div>
<p dir="auto">For Raspberry Pi, use the following to build the Docker image:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker build -t sshtron --build-arg BASE_IMAGE=resin/raspberry-pi-golang:latest ."><pre>$ docker build -t sshtron --build-arg BASE_IMAGE=resin/raspberry-pi-golang:latest <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">CVE-2016-0777</h2><a id="user-content-cve-2016-0777" aria-label="Permalink: CVE-2016-0777" href="#cve-2016-0777"></a></p>
<p dir="auto"><a href="https://www.qualys.com/2016/01/14/cve-2016-0777-cve-2016-0778/openssh-cve-2016-0777-cve-2016-0778.txt" rel="nofollow">CVE-2016-0777</a>
revealed two SSH client vulnerabilities that can be exploited by a malicious SSH server. While SSHTron does not exploit
these vulnerabilities, you should still patch your client before you play. SSHTron is open source, but the server
could always be running a modified version of SSHTron that does exploit the vulnerabilities described
in <a href="https://www.qualys.com/2016/01/14/cve-2016-0777-cve-2016-0778/openssh-cve-2016-0777-cve-2016-0778.txt" rel="nofollow">CVE-2016-0777</a>.</p>
<p dir="auto">If you haven't yet patched your SSH client, you can follow
<a href="https://www.jacobtomlinson.co.uk/quick%20tip/2016/01/15/fixing-ssh-vulnerability-CVE-2016-0777/" rel="nofollow">these instructions</a> to do so now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">SSHTron is licensed under the MIT License. See the full license text in <a href="https://github.com/zachlatta/sshtron/blob/master/LICENSE"><code>LICENSE</code></a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside the Apollo "8-Ball" FDAI (Flight Director / Attitude Indicator) (148 pts)]]></title>
            <link>https://www.righto.com/2025/06/inside-apollo-fdai.html</link>
            <guid>44277051</guid>
            <pubDate>Sat, 14 Jun 2025 15:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2025/06/inside-apollo-fdai.html">https://www.righto.com/2025/06/inside-apollo-fdai.html</a>, See on <a href="https://news.ycombinator.com/item?id=44277051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2250215887377196568" itemprop="description articleBody">
<p>During the Apollo flights to the Moon, the astronauts observed the spacecraft's orientation on a special instrument
called the FDAI (Flight Director / Attitude Indicator).
This instrument showed the spacecraft's attitude—its orientation—by rotating a ball.
This ball was nicknamed the "8-ball" because it was black (albeit only on one side).
The instrument also acted as a flight director, using three yellow needles to indicate how the astronauts should maneuver
the spacecraft. Three more pointers showed how fast the spacecraft was rotating.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-opened.jpg"><img alt="An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles." height="511" src="https://static.righto.com/images/fdai/fdai-opened-w500.jpg" title="An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles." width="500"></a></p><p>An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles.</p>
<p>Since the spacecraft rotates along three axes (roll, pitch, and yaw), the ball also rotates along three axes.
It's not obvious how the ball can rotate to an arbitrary orientation while remaining attached.
In this article, I look inside an FDAI from Apollo that was repurposed for a Space Shuttle simulator<span id="fnref:simulator"><a href="#fn:simulator">1</a></span> and explain how it operates. (Spoiler: the ball mechanism is firmly attached
at the "equator" and rotates in two axes. What you see is two hollow shells around the ball mechanism that spin around the third axis.)</p>

<p>For the missions to the Moon, the Lunar Module had two FDAIs, as shown below: one on the left for the Commander (Neil Armstrong in Apollo 11) and
one on the right for the Lunar Module Pilot (Buzz Aldrin in Apollo 11).
With their size and central positions, the FDAIs dominate the instrument panel, a sign of their importance.
(The Command Module for Apollo also had two FDAIs, but with a different design; I won't discuss them here.<span id="fnref:honeywell"><a href="#fn:honeywell">2</a></span>)</p>
<p><a href="https://static.righto.com/images/fdai/lm-panel.jpg"><img alt="The instrument panel in the Lunar Module. From Apollo 15 Lunar Module, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture." height="500" src="https://static.righto.com/images/fdai/lm-panel-w600.jpg" title="The instrument panel in the Lunar Module. From Apollo 15 Lunar Module, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture." width="600"></a></p><p>The instrument panel in the Lunar Module. From <a href="https://archive.org/details/S71-40761">Apollo 15 Lunar Module</a>, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture.</p>
<p>Each Lunar Module FDAI could display inputs from multiple sources, selected by switches on the panel.<span id="fnref:lm-fdai"><a href="#fn:lm-fdai">3</a></span> The ball could display attitude from either the
Inertial Measurement Unit
or from the backup Abort Guidance System, selected by the "ATTITUDE MON" toggle switch next to either FDAI.
The pitch attitude could also be supplied by an electromechanical unit called ORDEAL (Orbital Rate Display Earth And Lunar)
that simulates a circular orbit.
The error indications came from the Apollo Guidance Computer, the Abort Guidance System, the landing radar,
or the rendezvous radar (controlled by the "RATE/ERROR MON" switches).
The pitch, roll, and yaw rate displays were driven by the Rate Gyro Assembly (RGA).
The rate indications were scaled by a switch below the FDAI, selecting 25°/sec or 5°/sec.</p>
<h2>The FDAI mechanism</h2>
<p>The ball inside the indicator shows rotation around three axes.
I'll first explain these axes in the context of an aircraft, since the axes of a spacecraft are more arbitrary.<span id="fnref:axes"><a href="#fn:axes">4</a></span>
The roll axis indicates the aircraft's angle if it rolls side-to-side along its axis of flight, raising one wing
and lowering the other.
Thus, the indicator shows the tilt of the horizon as the aircraft rolls.
The pitch axis indicates the aircraft's angle if it pitches up or down, with the indicator showing the horizon
moving down or up in response.
Finally, the yaw axis indicates the compass direction that the aircraft is heading,
changing as the aircraft turns left or right.
(A typical aircraft attitude indicator omits yaw.)</p>
<p>I'll illustrate how the FDAI rotates the ball in three axes, using an orange as an example.
Imagine pinching the horizontal axis between two fingers with your arm extended.
Rotating your arm will roll the ball counter-clockwise or clockwise (red arrow).
In the FDAI, this rotation is accomplished by a motor turning the frame that holds the ball.
For pitch, the ball rotates forward or backward around the horizontal axis (yellow arrow).
The FDAI has a motor inside the ball to produce this rotation.
Yaw is a bit more difficult to envision: imagine hemisphere-shaped shells attached to the top and bottom shafts.
When a motor rotates these shells (green arrow), the hemispheres will rotate, even though
the ball mechanism (the orange) remains stationary.</p>
<p><a href="https://static.righto.com/images/fdai/orange.jpg"><img alt="A sphere, showing the three axes." height="334" src="https://static.righto.com/images/fdai/orange-w400.jpg" title="A sphere, showing the three axes." width="400"></a></p><p>A sphere, showing the three axes.</p>
<p>The diagram below shows the mechanism inside the FDAI.
The indicator uses three motors to move the ball.
The roll motor is attached to the FDAI's frame, while the pitch and yaw motors are inside the ball.
The roll motor rotates the roll gimbal through gears, causing the ball to rotate clockwise or counterclockwise.
The roll gimbal is attached to the ball mechanism at two points along the "equator";
these two points define the pitch axis.
Numerous wires on the roll gimbal enter the ball along the pitch axis.
The roll control transformer provides position feedback, as will be explained below.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-internals-labeled.jpg"><img alt="The main components inside the FDAI." height="399" src="https://static.righto.com/images/fdai/fdai-internals-labeled-w700.jpg" title="The main components inside the FDAI." width="700"></a></p><p>The main components inside the FDAI.</p>
<p>Removing the hemispherical shells reveals the 
mechanism inside the ball.
When the roll gimbal is rotated, this mechanism rotates with it.
The pitch motor causes the ball mechanism to rotate around the pitch axis.
The yaw motor and control transformer are not visible in this photo; they are behind the pitch components, oriented
perpendicularly.
The yaw motor turns the vertical shaft, with
the two hemisphere shells attached to the top and bottom of the shaft.
Thus, the yaw motor rotates the ball shells around the yaw axis, while the mechanism itself
remains stationary.
The control transformers for pitch and yaw provide position feedback.</p>
<p><a href="https://static.righto.com/images/fdai/ball-labeled.jpg"><img alt="The components inside the ball of the FDAI." height="479" src="https://static.righto.com/images/fdai/ball-labeled-w550.jpg" title="The components inside the ball of the FDAI." width="550"></a></p><p>The components inside the ball of the FDAI.</p>
<p>Why doesn't the wiring get tangled up as the ball rotates?
The solution is two sets of slip rings to implement the electrical connections.
The photo below shows the first slip ring assembly, which handles rotation around the roll axis.
These slip rings connect the stationary part of the FDAI to the
rotating roll gimbal.
The vertical metal brushes are stationary; there are 23 pairs of brushes, one for each connection to the ball mechanism.
Each pair of brushes contacts one metal ring on the striped shaft, maintaining contact as the shaft rotates.
Inside the shaft, 23 wires connect the circular metal contacts to the roll gimbal.</p>
<p><a href="https://static.righto.com/images/fdai/sliprings.jpg"><img alt="The slip ring assembly in the FDAI." height="447" src="https://static.righto.com/images/fdai/sliprings-w450.jpg" title="The slip ring assembly in the FDAI." width="450"></a></p><p>The slip ring assembly in the FDAI.</p>
<p>A second set of slip rings inside the ball handles rotation around the pitch axis.
These rings provide the electrical connection between the
wiring on the roll gimbal and the ball mechanism.
The yaw axis does not use slip rings since only the hemisphere shells rotate around the yaw axis;
no wires are involved.</p>
<h2>Synchros and the servo loop</h2>
<p>In this section, I'll explain how the FDAI is controlled by synchros and servo loops.
In the 1950s and 1960s, the standard technique for transmitting a rotational signal electrically was through a synchro.
Synchros were used for everything from rotating an instrument indicator in avionics to rotating the gun on a navy battleship.
A synchro produces an output that depends on the shaft's rotational position, and transmits this output signal
on three wires.
If you connect these wires to a second synchro, you can use the first synchro to control the second one:
the shaft of the second synchro will rotate to the same angle as
the first shaft.
Thus, synchros are a convenient way to send a control signal electrically.</p>
<p>The photo below shows a typical synchro, with the input shaft on the top and five wires
at the bottom: two for power and three for the output.</p>
<p><a href="https://static.righto.com/images/fdai/synchro.jpg"><img alt="A synchro transmitter." height="324" src="https://static.righto.com/images/fdai/synchro-w200.jpg" title="A synchro transmitter." width="200"></a></p><p>A synchro transmitter.</p>
<p>Internally, the synchro has a rotating winding called the rotor that is driven with 400 Hz AC.
Three fixed stator windings provide the three AC output signals. As the shaft rotates, the voltages of the
output signals change, indicating the angle.
(A synchro resembles a transformer with three variable secondary windings.)
If two connected synchros have different angles, the magnetic fields create a torque that rotates the shafts into alignment.</p>
<p><a href="https://static.righto.com/images/fdai/synchro-schematic.png"><img alt="The schematic symbol for a synchro transmitter or receiver." height="192" src="https://static.righto.com/images/fdai/synchro-schematic-w200.png" title="The schematic symbol for a synchro transmitter or receiver." width="200"></a></p><p>The schematic symbol for a synchro transmitter or receiver.</p>
<p>The downside of synchros is that they don't produce a lot of torque.
The solution is to use a more powerful motor, controlled by the synchro and a feedback loop called a servo loop.
The servo loop drives the motor in the appropriate direction to eliminate the error between the desired position and the
current position.</p>
<p>The diagram below shows how the servo loop is constructed from a combination of electronics and mechanical components.
The goal is to rotate the output shaft to an angle that exactly matches the input angle,
specified by the three synchro wires.
The control transformer compares the input angle and the output shaft position, producing an error signal.
The amplifier uses this error signal to drive the motor in the appropriate direction until the error signal drops to zero.
To improve the dynamic response of the servo loop, the tachometer signal is used as a negative feedback voltage.
The feedback slows the motor as the system gets closer to the right position, so the motor doesn't overshoot the position and oscillate.
(This is sort of like a PID controller.)</p>
<p><a href="https://static.righto.com/images/fdai/servo-diagram.jpg"><img alt="This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle." height="228" src="https://static.righto.com/images/fdai/servo-diagram-w600.jpg" title="This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle." width="600"></a></p><p>This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle.</p>
<p>A control transformer
is similar to a synchro in appearance and construction, but the rotating shaft operates as an input, not the output.
In a control transformer, the three stator windings receive the inputs and the rotor winding provides the error output.
If the rotor angle of the synchro transmitter and control transformer are the same, the signals cancel out and there is
no error voltage.
But as the difference between the two shaft angles increases, the rotor winding produces an error signal. The phase of the
error signal indicates the direction of the error.</p>
<p>In the FDAI, the motor is a special <a href="https://www.righto.com/2024/02/bendix-cadc-servomotor-tachometer.html">motor/tachometer</a>, a device that was often used in avionics servo loops.
This motor is more complicated than a regular electric motor.
The motor is powered by 115 volts AC at 400 hertz, but this won't spin the motor on its own.
The motor also has two low-voltage control windings. Energizing the control windings with the proper phase causes the
motor to spin in one direction or the other.
The motor/tachometer unit also contains a tachometer to measure its speed for the feedback loop.
The tachometer is driven by another 115-volt AC winding and generates a low-voltage AC signal that is proportional
to the motor's rotational speed.</p>
<p><a href="https://static.righto.com/images/fdai/motor-disassembled.jpg"><img alt="A motor/tachometer similar (but not identical) to the one in the FDAI." height="262" src="https://static.righto.com/images/fdai/motor-disassembled-w500.jpg" title="A motor/tachometer similar (but not identical) to the one in the FDAI." width="500"></a></p><p>A motor/tachometer similar (but not identical) to the one in the FDAI.</p>
<p>The photo above shows a motor/tachometer with the rotor removed.
The unit has many wires because of its multiple windings.
The rotor has two drums. The drum on the left, with the spiral stripes, is for the motor. This drum is a "squirrel-cage rotor",
which spins due to induced currents.
(There are no electrical connections to the rotor; the drums interact with the windings through magnetic fields.)
The drum on the right is the tachometer rotor; it induces a signal in the output winding proportional to the speed due to eddy currents.
The tachometer signal is at 400 Hz like the driving signal, either in phase or 180º out of phase, depending on the direction
of rotation.
For more information on how a motor/tachometer works, see my <a href="https://www.righto.com/2024/02/bendix-cadc-servomotor-tachometer.html">teardown</a>.</p>
<h2>The amplifiers</h2>
<p>The FDAI has three servo loops—one for each axis—and each servo loop has a separate control transformer, motor, and amplifier.
The photo below shows one of the three amplifier boards. The construction is unusual and somewhat chaotic,
with some components stacked on top of others to save space.
Some of the component leads are long and protected with clear plastic sleeves.<span id="fnref:pcb"><a href="#fn:pcb">5</a></span>
The cylindrical pulse transformer in the middle has five colorful wires coming out of it.
At the left are the two transistors that drive the motor's control windings, with two capacitors between them.
The transistors are mounted on a heat sink that is screwed down to the case of the amplifier assembly for cooling.
Each amplifier is connected to the FDAI through seven wires with pins that
plug into the sockets on the right of the board.<span id="fnref:jumpers"><a href="#fn:jumpers">6</a></span></p>
<p><a href="https://static.righto.com/images/fdai/amplifier-board.jpg"><img alt="One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating." height="351" src="https://static.righto.com/images/fdai/amplifier-board-w600.jpg" title="One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating." width="600"></a></p><p>One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating.</p>
<p>The function of the board is to amplify the error signal so the motor rotates in the appropriate direction.
The amplifier also uses the tachometer output from the motor unit to slow the motor as the error signal decreases, preventing
overshoot.
The inputs to the amplifier are 400 hertz AC signals, with the magnitude indicating the amount of error or speed and the
phase indicating the direction.
The two outputs from the amplifier drive the two control windings of the motor, determining which direction the motor rotates.</p>
<p>The schematic for the amplifier board is below. <span id="fnref:zener"><a href="#fn:zener">7</a></span>
The two transistors on the left amplify the error and tachometer signals, driving the pulse transformer.
The outputs of the pulse transformer will have opposite phases, driving the output transistors for opposite halves of
the 400 Hz cycle.
This activates the motor control winding, causing the motor to spin in the desired direction.<span id="fnref:control"><a href="#fn:control">8</a></span></p>
<p><a href="https://static.righto.com/images/fdai/amplifier-schematic.jpg"><img alt="The schematic of an amplifier board." height="262" src="https://static.righto.com/images/fdai/amplifier-schematic-w500.jpg" title="The schematic of an amplifier board." width="500"></a></p><p>The schematic of an amplifier board.</p>
<h2>History of the FDAI</h2>
<p>Bill Lear, born in 1902, was a prolific inventor with over 150 patents,
creating everything from the 8-track tape to the Learjet, the iconic
private plane of the 1960s.
He created multiple companies in the 1920s as well as inventing one of the first car radios for Motorola before starting Lear Avionics,
a company that specialized in aerospace instruments.<span id="fnref:lear"><a href="#fn:lear">9</a></span>
Lear produced innovative aircraft instruments and flight control systems such as
the <a href="https://archive.org/details/sim_flight-operations_1951-01_35_1/page/38/">F-5 automatic pilot</a>, which received a trophy as the "greatest aviation achievement in America" for 1950.</p>
<p>Bill Lear went on to solve an indicator problem for the Air Force:
the supersonic F-102 Delta Dagger interceptor (1953) could climb at steep angles, but existing
attitude indicators could not handle nearly vertical flight. 
Lear developed a remote two-gyro platform that drove the cockpit indicator while avoiding "gimbal lock" during vertical
flight.
For the experimental X-15 rocket-powered aircraft (1959), Lear improved this indicator to handle three axes:
roll, pitch, and yaw.</p>
<p>Meanwhile, the Siegler Corporation started in 1950 to manufacture space heaters for homes. A few years later, Siegler was acquired
by John Brooks, an entrepreneur who was enthusiastic about acquisitions. In 1961, Lear Avionics became his latest acquisition, and
the merged company was called Lear Siegler Incorporated, often known as LSI.
(Older programmers may know Lear Siegler through the <a href="https://en.wikipedia.org/wiki/ADM-3A">ADM-3A</a>, an inexpensive video display terminal from 1976 that 
housed the display and keyboard in a stylish white case.)</p>
<p>The X-15's attitude indicator became the basis of the indicator for the F-4 fighter plane
(the <a href="https://www.righto.com/2024/09/f4-attitude-indicator.html">ARU/11-A</a>).
Then, after "<a href="https://ntrs.nasa.gov/api/citations/19680016105/downloads/19680016105.pdf#page=120">a minimum of modification</a>",
the attitude-director indicator was used in the Gemini space program.
In total, Lear Siegler provided 11 instruments in the Gemini instrument panel, with the attitude director the most important.
Next, Gemini's indicator was modified to become the FDAI (flight director-attitude indicator) in the Lunar Module for Apollo.<span id="fnref:parts"><a href="#fn:parts">10</a></span>
Lear Siegler provided numerous components for the Apollo program, from a directional gyro for the Lunar Rover
to the electroluminescent display for the Apollo Guidance Computer's Display/Keyboard (DSKY).</p>
<p><a href="https://static.righto.com/images/fdai/lsi-instruments.jpg"><img alt="An article titled &quot;LSI Instruments Aid in Moon Landing&quot; from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)" height="385" src="https://static.righto.com/images/fdai/lsi-instruments-w600.jpg" title="An article titled &quot;LSI Instruments Aid in Moon Landing&quot; from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)" width="600"></a></p><p>An article titled "LSI Instruments Aid in Moon Landing" from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)</p>
<p>In 1974, Lear Siegler obtained a contract to develop the Attitude-Director Indicator (ADI) for the Space Shuttle, producing
a dozen ADI units for the Space Shuttle.
However, by this time, Lear Siegler was losing enthusiasm for low-volume space avionics.
The Instrument Division president said that "the business that we were in was an engineering business and engineers
love a challenge."
However, manufacturing refused to deal with the special procedures required for space manufacturing,
so the Shuttle units were built by the engineering department.
Lear Siegler didn't bid on later Space Shuttle avionics and the Shuttle ADI became its last space product.
In the early 2000s, the Space Shuttle's instruments were upgraded to a "glass cockpit" with 11 flat-panel displays known
as the Multi-function Electronic Display System (MEDS).
The MEDS was produced by Lear Siegler's long-term competitor, Honeywell.</p>
<p>Getting back to Bill Lear, he wanted to manufacture aircraft, not just aircraft instruments, so
he created the Learjet, the first mass-produced business jet.
The first Learjet flew in 1963, with over 3000 eventually delivered.
In the early 1970s, Lear designed a steam turbine automobile engine. Rather than water, the turbine
used a secret fluorinated hydrocarbon called "Learium". Lear had visions of thousands of low-pollution "<a href="https://www.nytimes.com/1971/04/04/archives/bill-lear-thinks-hell-have-the-last-laugh.html">Learmobiles</a>", but the engine failed
to catch on. 
Lear had been on the verge of bankruptcy in the 1960s; one of his VPs explained that
"the great creative minds can't be bothered with withholding taxes and investment credits and all this crap".
But by the time of his death in 1978, Lear had a fortune estimated at $75 million.</p>
<h2>Comparing the ARU/11-A and the FDAI</h2>
<p>Looking inside our FDAI sheds more details on the evolution of Lear Siegler's attitude directors.
The photo below compares the Apollo FDAI (top) to the earlier ARU/11-A used in the F-4 aircraft (bottom).
While the basic mechanism and the electronic amplifiers are the same between the two indicators, there are
also substantial changes.</p>
<p><a href="https://static.righto.com/images/fdai/aru-vs-fdai.jpg"><img alt="Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI." height="482" src="https://static.righto.com/images/fdai/aru-vs-fdai-w600.jpg" title="Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI." width="600"></a></p><p>Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI.</p>
<p>The biggest difference between the ARU-11/A indicator and the FDAI is that the electronics for the ARU-11/A 
are in a separate module that was plugged into the back of the indicator, while the FDAI includes the electronics
internally, with boards mounted on the instrument frame.
Specifically, the ARU-11/A has a separate unit containing a multi-winding transformer, a power supply board,
and three amplifier boards (one for each axis), while the FDAI contains these components internally.
The amplifier boards in the ARU-11/A and the FDAI are identical, constructed from germanium transistors rather than
silicon.<span id="fnref:amplifiers"><a href="#fn:amplifiers">11</a></span>
The unusual 11-pin transformers are also the same.
However, the power supply boards are different, probably because the boards also contain scaling resistors
that vary between the units.<span id="fnref:resistors"><a href="#fn:resistors">12</a></span>
The power supply boards are also different shapes to fit the available space.</p>
<p>The ball assemblies of the ARU/11-A and the FDAI are almost the same, with the same motor assemblies and slip ring
mechanism. 
The gearing has minor changes. In particular, the FDAI has two plastic gears, while the ARU/11-A uses
exclusively metal gears.</p>
<p>The ARU/11-A has a <a href="https://patents.google.com/patent/US2941305A">patented</a> pitch trim feature that was
mostly—but not entirely—removed from the Apollo FDAI.
The motivation for this feature is that an aircraft in level flight will be pitched up a few degrees, the "angle of attack".
It is desirable for the attitude indicator to show the aircraft as horizontal, so a pitch trim knob allows the
angle of attack to be canceled out on the display.
The problem is that if you fly your fighter plane vertically, you want the indicator to show precisely
vertical flight, rather than applying the pitch trim adjustment.
The solution in the ARU-11/A is a special 8-zone potentiometer on the pitch axis that will apply the pitch trim
adjustment in level flight but not in vertical flight, while providing a smooth transition between the regions.
This special potentiometer is mounted inside the ball of the ARU-11/A.
However, this pitch trim adjustment is meaningless for a spacecraft, so it is not implemented in the Apollo
or Space Shuttle instruments.
Surprisingly, the shell of the potentiometer still exists in our FDAI, but without the potentiometer itself or the
wiring.
Perhaps it remained to preserve the balance of the ball.
In the photo below, the cylindrical potentiometer shell is indicated by an arrow. Note the holes in the front
of the shell; in the ARU-11/A, the potentiometer's wiring terminals protrude through these holes, but in the 
FDAI, the holes are covered with tape.</p>
<p><a href="https://static.righto.com/images/fdai/potentiometer.jpg"><img alt="Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow." height="354" src="https://static.righto.com/images/fdai/potentiometer-w400.jpg" title="Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow." width="400"></a></p><p>Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow.</p>
<p>Finally, the mounting of the ball hemispheres is slightly different.
The ARU/11-A uses four screws at the pole of each hemisphere.
Our FDAI, however, uses a single screw at each pole; the screw is tightened with a Bristol Key, causing the
shaft to expand and hold the hemisphere in place.</p>
<p>To summarize, the Apollo FDAI occupies a middle ground: while it isn't simply a repurposed ARU-11/A, neither is
it a complete redesign.
Instead, it preserves the old design where possible, while stripping out undesired features such as pitch trim.
The separate amplifier and mechanical units of the ARU/11-A were combined to form the larger FDAI.</p>
<h2>Differences from Apollo</h2>
<p>The FDAI that we examined is a special unit:
it was originally built for Apollo but was repurposed for a Space Shuttle simulator.
Our FDAI is labeled Model 4068F, which is a Lunar Module part number.
Moreover, the FDAI is internally stamped with the date "Apr. 22 1968", over a year before the first Moon landing.</p>
<p>However, a closer look shows that several key components were modified to make the Apollo FDAI work in the Shuttle Simulator.<span id="fnref:systems-handbook"><a href="#fn:systems-handbook">14</a></span>
The Apollo FDAI (and the Shuttle ADI) used resolvers as inputs to control the ball, while our FDAI uses synchros.
(Resolvers and synchros are similar, except resolvers use sine and cosine inputs, 90° apart, on two wire pairs, while
synchros use three inputs, 120° apart, on three wires.)
NASA must have replaced the three resolver control transformers in the FDAI with synchro control transformers for use
in the simulator.</p>
<p>The Apollo FDAI used electroluminescent lighting for the display, while ours uses eight small incandescent bulbs.
The metal case of our FDAI has a Dymo <a href="https://en.wikipedia.org/wiki/Embossing_tape">embossed tape</a> label "INCANDESCENT
LIGHTING", alerting users to the change from Apollo's illumination.
Our FDAI also contains a step-down transformer to convert the 115 VAC input into 5 VAC to power the bulbs,
while the Shuttle powered its ADI illumination directly from <a href="https://airandspace.si.edu/collection-media/NASM-49BE0A798E632_006">5 volts</a>.</p>
<p>The dial of our FDAI was repainted to match the dial of the Shuttle FDAI.
The Apollo FDAI had red bands on the left and right of the dial.
A close examination of our dial shows that black paint was carefully applied over the red paint, but a few specks of
red paint are still visible (below).
Moreover, the edges of the lines and the lozenge show slight unevenness from the repainting.
Second, the Apollo FDAI had the text "ROLL RATE", "PITCH RATE", and "YAW RATE" in white next to the needle scales.
In our FDAI, this text has been hidden by black paint to match the Shuttle display.<span id="fnref:panel"><a href="#fn:panel">13</a></span>
Third, the Apollo LM FDAI had a crosshair in the center of the instrument, while our FDAI has a white U-shaped 
indicator, the same as the Shuttle (and the Command Module's FDAI).
Finally, the ball of the Apollo FDAI has red circular regions at the poles to warn of orientations that can cause
gimbal lock. Our FDAI (like the Shuttle) does not have these circles. We couldn't see any evidence that these
regions were repainted, so we suspect that our FDAI has Shuttle hemispheres on the ball.</p>
<p><a href="https://static.righto.com/images/fdai/repaint.jpg"><img alt="A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red." height="279" src="https://static.righto.com/images/fdai/repaint-w400.jpg" title="A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red." width="400"></a></p><p>A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red.</p>
<p>Our FDAI has also been modified electrically. 
Small green connectors (Micro-D MDB1) have been added between the slip rings and the motors, as well as on the gimbal arm.
We think these connectors were added post-Apollo, since they are attached somewhat sloppily with glue and don't
look flight-worthy.
Perhaps these connectors were added to make disassembly and modification easier.
Moreover, our FDAI has an elapsed time indicator, also mounted with glue.</p>
<p>The back of our FDAI is completely different from Apollo.
First, the connector's pinout is completely different.
Second, each of the six indicator needles has a mechanical adjustment as well as a trimpot (<a href="https://space1.com/Artifacts/Artifacts_FOR_SALE/FS__Shuttle_Sim_Avionics/FS__Shuttle_Sim_ADI/fs__shuttle_sim_adi.html">details</a>).
Finally, each of the three axes has an adjustment potentiometer.</p>
<h2>The Shuttle's ADI (Attitude Director Indicator)</h2>
<p>Each Space Shuttle had three ADIs (Attitude Director Indicators), which were very similar to the Apollo FDAI, despite
the name change.
The photo below shows the two octagonal ADIs in the forward flight deck, one on the left in front of the Commander,
and one on the right in front of the Pilot.
The <a href="https://catalog.archives.gov/id/22919771">aft flight deck station</a> had a third ADI.<span id="fnref:MEDS"><a href="#fn:MEDS">15</a></span></p>
<p><a href="https://static.righto.com/images/fdai/shuttle-flight-deck.jpg"><img alt="This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the National Archives." height="385" src="https://static.righto.com/images/fdai/shuttle-flight-deck-w600.jpg" title="This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the National Archives." width="600"></a></p><p>This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the <a href="https://catalog.archives.gov/id/23894173">National Archives</a>.</p>
<p>Our FDAI appears to have been significantly modified for use in the Shuttle simulator, as described above.
However, it is much closer to the Apollo FDAI than the ADI used in the Shuttle, as I'll show in this section.
My hypothesis is that the simulator was built before the Shuttle's ADI was created, so the Apollo FDAI was pressed
into service.</p>
<p>The Shuttle's ADI was much more complicated electrically than the Apollo FDAI and our FDAI, providing improved
functionality.<span id="fnref:shuttle-adi"><a href="#fn:shuttle-adi">16</a></span>
For instance, while the Apollo FDAI had a simple "OFF" indicator flag to show that the indicator had lost power,
the Shuttle's ADI had extensive error detection. 
It contained voltage level monitors to check its five power supplies. (The Shuttle ADI used three DC power sources
and two AC power sources, compared to the single AC supply for Apollo.)
The Shuttle's ADI also monitored the ball servos to detect position errors. Finally, it received an external "Data OK" signal.
If a fault was detected by any of these monitors, the "OFF" flag was deployed to indicate that the ADI could not
be trusted.</p>
<p>The Shuttle's ADI had six needles, the same as Apollo, but the Shuttle used feedback to make the positions more accurate.
Specifically, each Shuttle needle had a feedback sensor, a Linear Variable Differential Transformer (LVDT) that generates
a voltage based on the needle position.
The LVDT output drove a servo feedback loop to ensure that the needle was in the exact desired position.
In the Apollo FDAI, on the other hand, the needle input voltage drove a galvanometer, swinging the needle proportionally,
but there was no closed loop to ensure accuracy.</p>
<p>I assume that the Shuttle's ADI had integrated circuit electronics to implement this new functionality, considerably
more modern than the germanium transistors in the Apollo FDAI.
The Shuttle probably used the same mechanical structures to rotate the ball, but I can't confirm that.</p>
<h2>Conclusions</h2>
<p>The FDAI was a critical instrument in Apollo, indicating the orientation of the spacecraft in three axes.
It wasn't obvious to me how the "8-ball" can rotate in three axes while still being securely connected to the
instrument.
The trick is that most of the mechanism rotates in two axes, while hollow hemispherical shells provide the
third rotational axis.</p>
<p>The FDAI has an interesting evolutionary history, from the experimental X-15 rocket plane and the F-4 fighter to
the Gemini, Apollo, and Space Shuttle flights.
Our FDAI has an unusual position in this history: since it was modified from Apollo to function in a Space Shuttle
simulator, it shows aspects of both Apollo and the Space Shuttle indicators.
It would be interesting to compare the design of a Shuttle ADI to the Apollo FDAI, but I haven't been able to find
interior photos of a Shuttle ADI (or of an unmodified Apollo FDAI).<span id="fnref:photos"><a href="#fn:photos">17</a></span></p>
<p>You can see a brief video of the FDAI in motion <a href="https://bsky.app/profile/did:plc:svh6dgjnpkdl4dhxahj4xvkv/post/3lrlbsnh5z22j">here</a>. For more, follow me on
 Bluesky (<a href="https://bsky.app/profile/righto.com">@righto.com</a>),
Mastodon (<a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="6d0608031e05041f1f040b0b2d0201090f1419081e431e1d0c0e08">[email&nbsp;protected]</span></a>),
or <a href="https://www.righto.com/feeds/posts/default">RSS</a>. (I've given up on Twitter.)
I worked on this project with CuriousMarc, Mike Stewart, and Eric Schlapfer, so expect a
video at some point. Thanks to Richard for providing the FDAI.
I wrote about the F-4 fighter plane's attitude indicator <a href="https://www.righto.com/2024/09/f4-attitude-indicator.html">here</a>.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-opened2.jpg"><img alt="Inside the FDAI. The amplifier boards have been removed for this photo." height="505" src="https://static.righto.com/images/fdai/fdai-opened2-w700.jpg" title="Inside the FDAI. The amplifier boards have been removed for this photo." width="700"></a></p><p>Inside the FDAI. The amplifier boards have been removed for this photo.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch (421 pts)]]></title>
            <link>https://github.com/yousef-rafat/miniDiffusion</link>
            <guid>44276476</guid>
            <pubDate>Sat, 14 Jun 2025 13:56:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yousef-rafat/miniDiffusion">https://github.com/yousef-rafat/miniDiffusion</a>, See on <a href="https://news.ycombinator.com/item?id=44276476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">miniDiffusion</h2><a id="user-content-minidiffusion" aria-label="Permalink: miniDiffusion" href="#minidiffusion"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/yousef-rafat/miniDiffusion/blob/main/assets/display.png"><img src="https://github.com/yousef-rafat/miniDiffusion/raw/main/assets/display.png" alt="SD3 Diagram"></a></p>
<p dir="auto">miniDiffusion is a reimplementation of the Stable Diffusion 3.5 model in pure PyTorch with minimal dependencies. It's designed for educational, experimenting, and hacking purposes.
It's made with the mindset of having the least amount of code necessary to recreate Stable Diffusion 3.5 from scratch, with only ~2800 spanning from VAE to DiT to the Train and Dataset scripts.</p>
<p dir="auto"><strong>-Files:</strong> The main Stable Diffusion model code is located in dit.py, dit_components.py, and attention.py. The dit.py file contains the main model, dit_components.py contains the embedding, normalization, patch embedding, and help functions for the DiT code, and attention.py contains the Joint Attention implementation.
The noise.py is where the Euler Scheduler is located for solving the ODE of Rectified Flow.</p>
<p dir="auto">The text encoders are in t5_encoder.py and clip.py, and their tokenizers are both in tokenizer.py. The metrics.py implements the Fréchet inception distance (FID).</p>
<p dir="auto">The common.py is a place for helper functions for training, the common_ds.py is an implementation of an iterable dataset that converts image data to trainable data for the DiT model.</p>
<p dir="auto"><strong>-Folders:</strong> The model folder saves the model's checkpoint and logs after training. The encoders folder saves other modules' checkpoints (e.g., VAE, CLIP).</p>
<blockquote>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>Warning</strong>:
This repository still has experimental features and requires more testing.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Image Generation Modules</h3><a id="user-content-core-image-generation-modules" aria-label="Permalink: Core Image Generation Modules" href="#core-image-generation-modules"></a></p>
<ul dir="auto">
<li>Implementations of VAE, CLIP, and T5 Text Encoders</li>
<li>Implementation of Byte-Pair &amp; Unigram tokenizers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">SD3 Components</h3><a id="user-content-sd3-components" aria-label="Permalink: SD3 Components" href="#sd3-components"></a></p>
<ul dir="auto">
<li>Multi-Modal Diffusion Transformer Model</li>
<li>Flow-Matching Euler Scheduler</li>
<li>Logit-Normal Sampling</li>
<li>Joint Attention</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Train and Inference Scripts For SD3</h3><a id="user-content-train-and-inference-scripts-for-sd3" aria-label="Permalink: Train and Inference Scripts For SD3" href="#train-and-inference-scripts-for-sd3"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Get the repo</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone &quot;https://github.com/yousef-rafat/miniDiffusion&quot;"><pre>git clone <span><span>"</span>https://github.com/yousef-rafat/miniDiffusion<span>"</span></span></pre></div>
<p dir="auto">Install Dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Install Checkpoints for Models</p>
<ul dir="auto">
<li><em>Add a Hugging Face Token in get_checkpoints.py before running the script.</em></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 encoders/get_checkpoints.py"><pre>python3 encoders/get_checkpoints.py</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is under the MIT License and is made for educational and experimental purposes.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unsupervised Elicitation of Language Models (128 pts)]]></title>
            <link>https://arxiv.org/abs/2506.10139</link>
            <guid>44276041</guid>
            <pubDate>Sat, 14 Jun 2025 12:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2506.10139">https://arxiv.org/abs/2506.10139</a>, See on <a href="https://news.ycombinator.com/item?id=44276041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+J" rel="nofollow">Jiaxin Wen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ankner,+Z" rel="nofollow">Zachary Ankner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Somani,+A" rel="nofollow">Arushi Somani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hase,+P" rel="nofollow">Peter Hase</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Marks,+S" rel="nofollow">Samuel Marks</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goldman-Wetzler,+J" rel="nofollow">Jacob Goldman-Wetzler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Petrini,+L" rel="nofollow">Linda Petrini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sleight,+H" rel="nofollow">Henry Sleight</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Burns,+C" rel="nofollow">Collin Burns</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+H" rel="nofollow">He He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+S" rel="nofollow">Shi Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perez,+E" rel="nofollow">Ethan Perez</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leike,+J" rel="nofollow">Jan Leike</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2506.10139">View PDF</a>
    <a href="https://arxiv.org/html/2506.10139v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Jiaxin Wen [<a href="https://arxiv.org/show-email/5affddae/2506.10139" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 11 Jun 2025 19:40:08 UTC (1,235 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>