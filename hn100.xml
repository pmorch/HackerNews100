<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Sep 2023 21:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CFPB kicks off rulemaking to remove medical bills from credit reports (120 pts)]]></title>
            <link>https://www.consumerfinance.gov/about-us/newsroom/cfpb-kicks-off-rulemaking-to-remove-medical-bills-from-credit-reports/</link>
            <guid>37616171</guid>
            <pubDate>Fri, 22 Sep 2023 18:56:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-kicks-off-rulemaking-to-remove-medical-bills-from-credit-reports/">https://www.consumerfinance.gov/about-us/newsroom/cfpb-kicks-off-rulemaking-to-remove-medical-bills-from-credit-reports/</a>, See on <a href="https://news.ycombinator.com/item?id=37616171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p data-block-key="ie0t5"><b>WASHINGTON, D.C.</b> – The Consumer Financial Protection Bureau (CFPB) today announced it is beginning a rulemaking process to remove medical bills from Americans’ credit reports. The CFPB outlined proposals under consideration that would help families financially recover from medical crises, stop debt collectors from coercing people into paying bills they may not even owe, and ensure that creditors are not relying on data that is often plagued with inaccuracies and mistakes.</p><p data-block-key="8cm31">“Research shows that medical bills have little predictive value in credit decisions, yet tens of millions of American households are dealing with medical debt on their credit reports,” said CFPB Director Rohit Chopra. “When someone gets sick, they should be able to focus on getting better, rather than fighting debt collectors trying to extort them into paying bills they may not even owe.”</p><p data-block-key="7iks1">A <a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-estimates-88-billion-in-medical-bills-on-credit-reports/">2022 report</a> found that roughly 20% of Americans report having medical debt, but <a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-study-finds-medical-debt-overly-penalizes-consumer-credit-scores/">previous research</a> by the CFPB has shown that medical billing data on a credit report is less predictive of future repayment than reporting on traditional credit obligations. Mistakes and inaccuracies in medical billing are common and can be compounded by problems such as disputes over insurance payments or complex billing practices.</p><p data-block-key="83c6a">The Fair Credit Reporting Act restricts creditors’ ability to use medical information in making credit decisions and places limits on the inclusion of medical information on credit reports. The FCRA also granted five financial regulators authority to create regulatory exemptions to the restriction on creditors’ use of medical information, and in 2005, those regulators created an exception to allow creditors to rely on medical data if it could be characterized as “financial information.”</p><p data-block-key="btipa">The document released today is an outline of proposals and alternatives under consideration for the CFPB’s medical debt rulemaking. If finalized, they would:</p><ul><li data-block-key="b8ha"><b>Remove medical bills from consumers’ credit reports:</b> Consumer reporting companies would be prohibited from including medical debts and collection information on consumer reports that creditors use in making underwriting decisions.</li><li data-block-key="7rv83"><b>Stop creditors from relying on medical bills for underwriting decisions:</b> The proposal would narrow the 2005 exception and prohibit creditors from using medical collections information when evaluating borrowers’ credit applications.</li><li data-block-key="51gc"><b>Stop coercive collection practices:</b> As unpaid medical bills would no longer appear on consumers’ credit reports used by creditors in making underwriting decisions, debt collectors would no longer be able to use the credit reporting system as leverage to pressure consumers into paying questionable debts.</li></ul><p data-block-key="9ht47">The proposal would not stop creditors from obtaining medical bill information for other purposes, such as verifying the need for medical forbearances, or evaluating loan applications to pay for medical services.</p><p data-block-key="539ph">In advance of beginning the rulemaking process, the CFPB has engaged with the public on this issue. In a <a href="https://www.consumerfinance.gov/about-us/newsroom/prepared-remarks-of-director-rohit-chopra-for-the-cfpb-hearing-on-medical-billing-and-collections/">public hearing</a> in July 2023, the CFPB met with and listened to people from across the country on the impact poor medical billing practices and coercive credit reporting have on patients and families. The CFPB, <a href="https://www.consumerfinance.gov/about-us/newsroom/inquiry-into-costly-credit-cards-and-loans-pushed-on-patients-for-health-care-costs/">in partnership with other agencies</a>, is currently reviewing information submitted by the public on medical billing practices, including high-cost specialty financial products such as medical credit cards and installment loans. The CFPB continues to <a href="https://files.consumerfinance.gov/f/documents/cfpb_complaint-bulletin-medical-billing_report_2022-04.pdf"><span>receive complaints</span> <svg viewBox="0 0 12 19" xmlns="http://www.w3.org/2000/svg"><path d="M11.16 16.153a.477.477 0 0 1-.476.475H1.316a.476.476 0 0 1-.475-.475V3.046a.476.476 0 0 1 .475-.475h6.95l2.893 2.893zm-1.11-9.925H8.059a.575.575 0 0 1-.574-.573V3.679H1.95v11.84h8.102zm-1.234 5.604L6.388 14.26a.554.554 0 0 1-.784 0l-2.428-2.428a.554.554 0 1 1 .783-.784l1.483 1.482V7.41a.554.554 0 1 1 1.108 0v5.12l1.482-1.482a.554.554 0 0 1 .784.783z"></path></svg></a> from the public about illegal debt collection and credit reporting practices related to medical billing.</p><p data-block-key="5ghmd">The CFPB is taking steps to empower consumers by having them take more control over their personal financial data and how it is being used by companies. In addition to today’s announcement, the CFPB <a href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-launches-inquiry-into-the-business-practices-of-data-brokers/">previously launched</a> an inquiry into the practices of data brokers, and how companies that track and collect information on people’s personal lives impact consumers. As <a href="https://www.consumerfinance.gov/about-us/newsroom/remarks-of-cfpb-director-rohit-chopra-at-white-house-roundtable-on-protecting-americans-from-harmful-data-broker-practices/">announced</a> on August 15, 2023, the CFPB is considering proposals relating to data brokers.</p><p data-block-key="aanqt">The document released today is an outline of proposals and alternatives under consideration for the CFPB’s Fair Credit Reporting Act Rulemaking. The medical debt announcement, and the August announcement on data brokers, are part of that rulemaking.</p><p data-block-key="fv2u8"><a href="https://files.consumerfinance.gov/f/documents/cfpb_consumer-reporting-rule-sbrefa_outline-of-proposals.pdf"><span>Read today’s Outline of Proposals and Alternatives Under Consideration.</span> <svg viewBox="0 0 12 19" xmlns="http://www.w3.org/2000/svg"><path d="M11.16 16.153a.477.477 0 0 1-.476.475H1.316a.476.476 0 0 1-.475-.475V3.046a.476.476 0 0 1 .475-.475h6.95l2.893 2.893zm-1.11-9.925H8.059a.575.575 0 0 1-.574-.573V3.679H1.95v11.84h8.102zm-1.234 5.604L6.388 14.26a.554.554 0 0 1-.784 0l-2.428-2.428a.554.554 0 1 1 .783-.784l1.483 1.482V7.41a.554.554 0 1 1 1.108 0v5.12l1.482-1.482a.554.554 0 0 1 .784.783z"></path></svg></a></p><p data-block-key="kqi0"><a href="https://www.consumerfinance.gov/about-us/newsroom/prepared-remarks-of-cfpb-director-rohit-chopra-on-medical-debt-at-a-press-call-hosted-by-vice-president-kamala-harris/">Read Director Chopra’s remarks at a call hosted by Vice President Kamala Harris</a></p><p data-block-key="2k3ph"><a href="https://www.consumerfinance.gov/rules-policy/small-business-review-panels/small-business-review-panel-for-consumer-reporting-rulemaking/">Additional related materials are available on our rulemaking page.</a></p><p data-block-key="dk2nl">Consumers can submit complaints about financial products or services by visiting the <a href="https://www.consumerfinance.gov/complaint/">CFPB’s website</a> or by calling (855) 411-CFPB (2372).</p><p data-block-key="edtpa">Employees who believe their companies have violated federal consumer financial protection laws are encouraged to send information about what they know to <a href="mailto:whistleblower@cfpb.gov">whistleblower@cfpb.gov</a>.</p>
            </div><div>



    <center><i>###</i></center><p><i>The Consumer Financial Protection Bureau is a 21st century agency that implements and enforces Federal consumer financial law and ensures that markets for consumer financial products are fair, transparent, and competitive. For more information, visit</i> <a href="https://www.consumerfinance.gov/"><i>consumerfinance.gov</i></a><i>.</i></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[0-days exploited by commercial surveillance vendor in Egypt (196 pts)]]></title>
            <link>https://blog.google/threat-analysis-group/0-days-exploited-by-commercial-surveillance-vendor-in-egypt/</link>
            <guid>37614816</guid>
            <pubDate>Fri, 22 Sep 2023 17:21:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/threat-analysis-group/0-days-exploited-by-commercial-surveillance-vendor-in-egypt/">https://blog.google/threat-analysis-group/0-days-exploited-by-commercial-surveillance-vendor-in-egypt/</a>, See on <a href="https://news.ycombinator.com/item?id=37614816">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;0\u002Ddays exploited by commercial surveillance vendor in Egypt&quot;
         }"><p data-block-key="z0lph">Last week Google’s Threat Analysis Group (TAG), in partnership with <a href="https://citizenlab.ca/2023/09/predator-in-the-wires-ahmed-eltantawy-targeted-with-predator-spyware-after-announcing-presidential-ambitions">The Citizen Lab</a>, discovered an in-the-wild 0-day exploit chain for iPhones. Developed by the commercial surveillance vendor, Intellexa, this exploit chain is used to install its Predator spyware surreptitiously onto a device.</p><p data-block-key="db5qn">In response, yesterday, Apple patched the bugs in <a href="https://support.apple.com/en-us/HT213927">iOS 16.7</a> and <a href="https://support.apple.com/en-us/HT213926">iOS 17.0.1</a> as CVE-2023-41991, CVE-2023-41992, CVE-2023-41993. This quick patching from Apple helps to better protect users and we encourage all iOS users to install them as soon as possible.</p><h3 data-block-key="9mihu">Exploit delivery via man-in-the-middle (MITM)</h3><p data-block-key="3tkrk">The Intellexa exploit chain was delivered via a “man-in-the-middle” (MITM) attack, where an attacker is in between the target and the website they’re trying to reach. If the target is going to a website using ‘http’, then the attacker can intercept the traffic and send fake data back to the target to force them to a different website. Visiting a website using ‘https’ means that the traffic is encrypted, and it is easily verifiable that the received data came from the intended website using their certificate. That is not the case when using ‘http’.</p><p data-block-key="43ulj">In the case of this campaign, if the target went to any ‘http’ site, the attackers injected traffic to silently redirect them to an Intellexa site, c.betly[.]me. If the user was the expected targeted user, the site would then redirect the target to the exploit server, sec-flare[.]com. While there’s a spotlight on “0-click” vulnerabilities (bugs that don’t require user interaction) this MITM delivery also didn’t require the user to open any documents, click a specific link, or answer any phone calls.</p><h3 data-block-key="7bvmk">iOS Exploit Chain</h3><p data-block-key="3mmcg">As soon as the attacker redirected the target to their exploit server, the exploit chain began to execute. For iOS, this chain included three vulnerabilities:</p><ul><li data-block-key="9h3i5">CVE-2023-41993: Initial remote code execution (RCE) in Safari</li><li data-block-key="b9i2o">CVE-2023-41991: PAC bypass</li><li data-block-key="bchsp">CVE-2023-41992: Local privilege escalation (LPE) in the XNU Kernel</li></ul><p data-block-key="d9tj1">The chain then ran a small binary to decide whether or not to install the full Predator implant. However, TAG was unable to capture the full Predator implant.</p><p data-block-key="9beab">We plan to publish a technical deep dive on these exploits in line with the <a href="https://about.google/appsecurity/">Google vulnerability disclosure policy</a>.</p><h3 data-block-key="7qhpg">Android Exploit Chain</h3><p data-block-key="8ub7c">The attacker also had an exploit chain to install Predator on Android devices in Egypt. TAG observed these exploits delivered in two different ways: the MITM injection and via one-time links sent directly to the target. We were only able to obtain the initial renderer remote code execution vulnerability for Chrome, which was exploiting <a href="https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop.html">CVE-2023-4762</a>.</p><p data-block-key="1me89">This bug had already been separately reported to the Chrome Vulnerability Rewards Program by a security researcher and was patched on September 5th. We assess that Intellexa was also previously using this vulnerability as a 0-day.</p><h3 data-block-key="fp05">Chrome's work to protect against MITM</h3><p data-block-key="6d3d2">For years, Chrome has worked toward universal HTTPS adoption across the web. Additionally <a href="https://blog.chromium.org/2021/07/increasing-https-adoption.html">Chrome has an “HTTPS-First Mode”</a> that can reduce the likelihood of exploits being delivered via MITM network injection. "HTTPS-First Mode" will attempt to load all pages over HTTPS, and show a large warning before falling back to sending an HTTP request. This setting is currently on by default for users enrolled in the <a href="https://landing.google.com/advancedprotection/">Advanced Protection Program</a> who are also signed into Chrome. We encourage all users <a href="https://support.google.com/chrome/answer/10468685?hl=en&amp;co=GENIE.Platform%3DAndroid">to enable “HTTPS-First Mode”</a> to better protect themselves from MITM attacks.</p><h3 data-block-key="1gn1m">Conclusion</h3><p data-block-key="5b2rm">This campaign is yet another example of the abuses caused by the proliferation of commercial surveillance vendors and their serious risk to the safety of online users. TAG will continue to take action against, and publish research about, the commercial spyware industry, as well as work across the public and private sectors to push this work forward.</p><p data-block-key="1q972">We would like to acknowledge and thank The Citizen Lab for their collaboration and partnership in the capturing and analysis of these exploits, and Apple for deploying a timely patch for the safety of online users.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An open letter to our community (191 pts)]]></title>
            <link>https://blog.unity.com/news/open-letter-on-runtime-fee</link>
            <guid>37614793</guid>
            <pubDate>Fri, 22 Sep 2023 17:20:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.unity.com/news/open-letter-on-runtime-fee">https://blog.unity.com/news/open-letter-on-runtime-fee</a>, See on <a href="https://news.ycombinator.com/item?id=37614793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To our community:</p>
<p>I’m Marc Whitten, and I lead Unity Create which includes the Unity engine and editor teams.</p>
<p>I want to start with this: I am sorry.&nbsp;</p>
<p>We should have spoken with more of you and we should have incorporated more of your feedback before announcing our new Runtime Fee policy. Our goal with this policy is to ensure we can continue to support you today and tomorrow, and keep deeply investing in our game engine.</p>
<p>You are what makes Unity great, and we know we need to listen, and work hard to earn your trust. We have heard your concerns, and we are making changes in the policy we announced to address them.</p>
<p>Our Unity Personal plan will remain free and there will be no Runtime Fee for games built on <strong>Unity Personal</strong>. We will be increasing the cap from $100,000 to $200,000 and we will remove the requirement to use the <em>Made with Unity</em> splash screen.</p>
<p>No game with less than $1 million in trailing 12-month revenue will be subject to the fee.</p>
<p>For those creators on <strong>Unity Pro and Unity Enterprise</strong>, we are also making changes based on your feedback.</p>
<p>The Runtime Fee policy will only apply <strong>beginning with the next LTS version of Unity</strong> shipping in 2024 and beyond. Your games that are currently shipped and the projects you are currently working on will not be included – unless you choose to upgrade them to this new version of Unity.</p>
<p><strong>We will make sure that you can stay on the terms applicable for the version of Unity editor you are using </strong>– as long as you keep using that version.</p>
<p>For games that are subject to the runtime fee, we are giving you a choice of either a <strong>2.5% revenue share</strong> or the calculated amount based on the number of new people engaging with your game each month. Both of these numbers are <strong>self-reported</strong> from data you already have available. You will always be billed the lesser amount.</p>
<p>We want to continue to build the best engine for creators. We truly love this industry and you are the reason why.</p>
<p>I’d like to invite you to join me for a <a href="https://youtube.com/live/qyLcI5O9iUY" target="_blank">live fireside chat</a> hosted by Jason Weimann today at 4:00 pm ET/1:00 pm PT, where I will do my best to answer your questions. In the meantime, <a href="https://unity.com/pricing-updates" target="_blank">here</a> are some more details.<sup>*</sup></p>
<p>Thank you for caring as deeply as you do, and thank you for giving us hard feedback.</p>
<p>Marc Whitten</p>
<p><sup><em>*We are working to localize translations of this web page.</em></sup></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple fucked us on right to repair (again) (138 pts)]]></title>
            <link>https://pluralistic.net/2023/09/22/vin-locking/#thought-differently</link>
            <guid>37614279</guid>
            <pubDate>Fri, 22 Sep 2023 16:43:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pluralistic.net/2023/09/22/vin-locking/#thought-differently">https://pluralistic.net/2023/09/22/vin-locking/#thought-differently</a>, See on <a href="https://news.ycombinator.com/item?id=37614279">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6729">
	<!-- .entry-header -->

	
	
	<div>
		<p><!--
Tags:
vin locking, apple, right to repair, california, ifixit, iphones, sb244, parts pairing, serialization, dmca 1201, felony contempt of business model, ewaste, repairwashing, fuckery

Summary:
Apple fucked us on right to repair (again); Hey look at this

URL:
https://pluralistic.net/2023/09/22/vin-locking/

Title:
Pluralistic: Apple fucked us on right to repair (again) (22 Sept 2023) vin-locking

Bullet:
🎒

Separator:
_,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,_

Top Sources:
Today's top sources: Naked Capitalism (https://www.nakedcapitalism.com/).

--><br>
<a href="https://pluralistic.net/2023/09/22/vin-locking/"><img data-lazy-fallback="1" decoding="async" src="https://i0.wp.com/craphound.com/images/22Sep2023.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/22Sep2023.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></p>
<h2>Today's links</h2>
<ul>
<li><a href="https://pluralistic.net/2023/09/22/vin-locking/#thought-differently">Apple fucked us on right to repair (again)</a>: "Parts-pairing" is a scam.
</li>
<li><a href="https://pluralistic.net/2023/09/22/vin-locking/#linkdump">Hey look at this</a>: Delights to delectate.
</li>
<li><a href="https://pluralistic.net/2023/09/22/vin-locking/#retro">This day in history</a>: 2003, 2008, 2013, 2018, 2022
</li>
<li><a href="https://pluralistic.net/2023/09/22/vin-locking/#bragsheet">Colophon</a>: Recent publications, upcoming/recent appearances, current writing projects, current reading
</li>
</ul>

<hr>
<p><a name="thought-differently"></a><br>
<img data-lazy-fallback="1" decoding="async" alt="An ornate fireplace and mantlepiece. There is a roaring fire in the grate. Over the mantlepiece is an AR-15 assault rifle, surmounted by Apple's 'Think Different' wordmark. The scene is pockmarked with bullet-holes." src="https://i0.wp.com/craphound.com/images/apple-gun-on-mantlepiece.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/apple-gun-on-mantlepiece.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<h2>Apple fucked us on right to repair (again) (<a href="https://pluralistic.net/2023/09/22/vin-locking/#thought-differently">permalink</a>)</h2>
<p>Right to repair has no cannier, more dedicated adversary than Apple, a company whose most innovative work is dreaming up new ways to sneakily sabotage electronics repair while claiming to be a caring environmental steward, a lie that covers up the mountains of e-waste that Apple dooms our descendants to wade through.</p>
<p>Why does Apple hate repair so much? It's not that they want to poison our water and bodies with microplastics; it's not that they want to hasten the day our coastal cities drown; it's not that they relish the human misery that accompanies every gram of conflict mineral. They aren't sadists. They're merely sociopathically greedy.</p>
<p>Tim Cook laid it out for his investors: when people can repair their devices, they don't buy new ones. When people don't buy new devices, Apple doesn't sell them new devices. It's that's simple:</p>
<p><a href="https://www.inverse.com/article/52189-tim-cook-says-apple-faces-2-key-problems-in-surprising-shareholder-letter">https://www.inverse.com/article/52189-tim-cook-says-apple-faces-2-key-problems-in-surprising-shareholder-letter</a></p>
<p>So Apple does everything it can to monopolize repair. Not just because this lets the company gouge you on routine service, but because it lets them decide when your phone is beyond repair, so they can offer you a trade-in, ensuring both that you buy a new device and that the device you buy is another Apple.</p>
<p>There are so many tactics Apple gets to use to sabotage repair. For example, Apple engraves microscopic Apple logos on the subassemblies in its devices. This allows the company to enlist US Customs to seize and destroy refurbished parts that are harvested from dead phones by workers in the Pacific Rim:</p>
<p><a href="https://repair.eu/news/apple-uses-trademark-law-to-strengthen-its-monopoly-on-repair/">https://repair.eu/news/apple-uses-trademark-law-to-strengthen-its-monopoly-on-repair/</a></p>
<p>Of course, the easiest way to prevent harvested components from entering the parts stream is to destroy as many old devices as possible. That's why Apple's so-called "recycling" program <em>shreds</em> any devices you turn over to them. When you trade in your old iPhone at an Apple Store, it is converted into immortal e-waste (no other major recycling program does this). The logic is straightforward: no parts, no repairs:</p>
<p><a href="https://www.vice.com/en/article/yp73jw/apple-recycling-iphones-macbooks">https://www.vice.com/en/article/yp73jw/apple-recycling-iphones-macbooks</a></p>
<p>Shredding parts and cooking up bogus trademark claims is just for starters, though. For Apple, the true anti-repair innovation comes from the most pernicious US tech law: Section 1201 of the Digital Millennium Copyright Act (DMCA).</p>
<p>DMCA 1201 is an "anti-circumvention" law. It bans the distribution of any tool that bypasses "an effective means of access control." That's all very abstract, but here's what it means: if a manufacturer sticks some Digital Rights Management (DRM) in its device, then anything you want to do that involves removing that DRM is now illegal – even if the thing itself is perfectly legal.</p>
<p>When Congress passed this stupid law in 1998, it had a very limited blast radius. Computers were still pretty expensive and DRM use was limited to a few narrow categories. In 1998, DMCA 1201 was mostly used to prevent you from de-regionalizing your DVD player to watch discs that had been released overseas but not in your own country.</p>
<p>But as we warned back then, computers were only going to get smaller and cheaper, and eventually, it would only cost manufacturers pennies to wrap their products – or even subassemblies in their products – in DRM. Congress was putting a gun on the mantelpiece in Act I, and it was bound to go off in Act III.</p>
<p>Welcome to Act III.</p>
<p>Today, it costs about a quarter to add a system-on-a-chip to even the tiniest parts. These SOCs can run DRM. Here's how that DRM works: when you put a new part in a device, the SOC and the device's main controller communicate with one another. They perform a cryptographic protocol: the part says, "Here's my serial number," and then the main controller prompts the user to enter a manufacturer-supplied secret code, and the master controller sends a signed version of this to the part, and the part and the system then recognize each other.</p>
<p>This process has many names, but because it was first used in the automotive sector, it's widely known as VIN-Locking (VIN stands for "vehicle identification number," the unique number given to every car by its manufacturer). VIN-locking is used by automakers to block independent mechanics from repairing your car; even if they use the manufacturer's own parts, the parts and the engine will refuse to work together until the manufacturer's rep keys in the unlock code:</p>
<p><a href="https://pluralistic.net/2023/07/24/rent-to-pwn/#kitt-is-a-demon">https://pluralistic.net/2023/07/24/rent-to-pwn/#kitt-is-a-demon</a></p>
<p>VIN locking is everywhere. It's how John Deere stops farmers from fixing their own tractors – something farmers have done literally since tractors were invented:</p>
<p><a href="https://pluralistic.net/2022/05/08/about-those-kill-switched-ukrainian-tractors/">https://pluralistic.net/2022/05/08/about-those-kill-switched-ukrainian-tractors/</a></p>
<p>It's in <em>ventilators</em>. Like mobile phones, ventilators are a grotesquely monopolized sector, controlled by a single company Medtronic, whose biggest claim to fame is effecting the world's largest tax inversion in order to manufacture the appearance that it is an Irish company and therefore largely untaxable. Medtronic used the resulting windfall to gobble up most of its competitors.</p>
<p>During lockdown, as hospitals scrambled to keep their desperately needed supply of ventilators running, Medtronic's VIN-locking became a lethal impediment. Med-techs who used donor parts from one ventilator to keep another running – say, transplanting a screen – couldn't get the device to recognize the part because all the world's civilian aircraft were grounded, meaning Medtronic's technicians couldn't swan into their hospitals to type in the unlock code and charge them hundreds of dollars.</p>
<p>The saving grace was an anonymous, former Medtronic repair tech, who built pirate boxes to generate unlock codes, using any housing they could lay hands on to use as a case: guitar pedals, clock radios, etc. This tech shipped these gadgets around the world, observing strict anonymity, because Article 6 of the EUCD <em>also</em> bans circumvention:</p>
<p><a href="https://pluralistic.net/2020/07/10/flintstone-delano-roosevelt/#medtronic-again">https://pluralistic.net/2020/07/10/flintstone-delano-roosevelt/#medtronic-again</a></p>
<p>Of course, <em>Apple</em> is a <em>huge</em> fan of VIN-locking. In phones, VIN-locking is usually called "serializing" or "parts-pairing," but it's the same thing: a tiny subassembly gets its own microcontroller whose sole purpose is to prevent independent repair technicians from fixing your gadget. Parts-pairing lets Apple block repairs even when the technician uses new, Apple parts – but it also lets Apple block refurb parts and third party parts.</p>
<p>For many years, Apple was the senior partner and leading voice in blocking state Right to Repair bills, which it killed by the <em>dozen</em>, leading a coalition of monopolists, from Wahl (who boobytrap their hair-clippers with springs that cause their heads irreversibly decompose if you try to sharpen them at home) to John Deere (who reinvented tenant farming by making farmers tenants of their tractors, rather than their land).</p>
<p>But Apple's opposition to repair eventually became a problem for the company. It's bad optics, and both Apple customers and Apple employees are volubly displeased with the company's ecocidal conduct. But of course, Apple's <em>management</em> and <em>shareholders</em> hate repair and want to block it as much as possible.</p>
<p>But Apple knows how to Think Differently. It came up with a way to eat its cake and have it, too. The company embarked on a program of visibly support right to repair, while working behind the scenes to sabotage it.</p>
<p>Last year, Apple announced a repair program. It was <em>hilarious</em>. If you wanted to swap your phone's battery, all you had to do was let Apple put a $1200 hold on your credit card, and then wait while the company shipped you 80 pounds' worth of specialized tools, packed in two special Pelican cases:</p>
<p><a href="https://pluralistic.net/2022/05/22/apples-cement-overshoes/">https://pluralistic.net/2022/05/22/apples-cement-overshoes/</a></p>
<p>Then, you swapped your battery, but you weren't done! After your battery was installed, you had to conference in an authorized Apple tech who would tell you what code to type into a laptop you tethered to the phone in order to pair it with your phone. Then all you had to do was lug those two 40-pound Pelican cases to a shipping depot and wait for Apple to take the hold off your card (less the $120 in parts and fees).</p>
<p>By contrast, independent repair outfits like iFixit will sell you all the tools you need to do your own battery swap – including the battery! for $32. The whole kit fits in a padded envelope:</p>
<p><a href="https://www.ifixit.com/products/iphone-x-replacement-battery">https://www.ifixit.com/products/iphone-x-replacement-battery</a></p>
<p>But while Apple was able to make a showy announcement of its repair program and then hide the malicious compliance inside those giant Pelican cases, sabotaging right to repair legislation is a lot harder.</p>
<p>Not that they didn't try. When New York State passed the first general electronics right-to-repair bill in the country, <em>someone</em> convinced New York Governor Kathy Hochul to neuter it with last-minute modifications:</p>
<p><a href="https://arstechnica.com/gadgets/2022/12/weakened-right-to-repair-bill-is-signed-into-law-by-new-yorks-governor/">https://arstechnica.com/gadgets/2022/12/weakened-right-to-repair-bill-is-signed-into-law-by-new-yorks-governor/</a></p>
<p>But that kind of trick only works once. When California's right to repair bill was introduced, it was clear that it was gonna pass. Rather than get run over by that train, Apple got on board, supporting the legislation, which passed unanimously:</p>
<p><a href="https://www.ifixit.com/News/79902/apples-u-turn-tech-giant-finally-backs-repair-in-california">https://www.ifixit.com/News/79902/apples-u-turn-tech-giant-finally-backs-repair-in-california</a></p>
<p>But Apple got the last laugh. Because while California's bill contains many useful clauses for the independent repair shops that keep your gadgets out of a landfill, it's a state law, and DMCA 1201 is federal. A state law can't simply legalize the conduct federal law prohibits. California's right to repair bill is a banger, but it has a weak spot: parts-pairing, the scourge of repair techs:</p>
<p><a href="https://www.ifixit.com/News/69320/how-parts-pairing-kills-independent-repair">https://www.ifixit.com/News/69320/how-parts-pairing-kills-independent-repair</a></p>
<p><img data-lazy-fallback="1" decoding="async" alt="An iFixit bar-chart showing the rising trend to parts-pairing in iPhones." src="https://i0.wp.com/craphound.com/images/Parts-Pairing-Bar-Chart_1922wide.png?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/Parts-Pairing-Bar-Chart_1922wide.png?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<p>Every generation of Apple devices does more parts-pairing than the previous one, and the current models are so infested with paired parts as to be effectively unrepairable, except by Apple. It's so bad that iFixit has dropped its repairability score for the iPhone 14 from a 7 ("recommend") to a <em>4</em> (do not recommend):</p>
<p><a href="https://www.ifixit.com/News/82493/we-are-retroactively-dropping-the-iphones-repairability-score-en">https://www.ifixit.com/News/82493/we-are-retroactively-dropping-the-iphones-repairability-score-en</a></p>
<p>Parts-pairing is bullshit, and Apple are scum for using it, but they're hardly unique. Parts-pairing is at the core of the fuckery of inkjet printer companies, who use it to fence out third-party ink, so they can charge $9,600/gallon for ink that pennies to make:</p>
<p><a href="https://www.eff.org/deeplinks/2020/11/ink-stained-wretches-battle-soul-digital-freedom-taking-place-inside-your-printer">https://www.eff.org/deeplinks/2020/11/ink-stained-wretches-battle-soul-digital-freedom-taking-place-inside-your-printer</a></p>
<p>Parts-pairing is also rampant in powered wheelchairs, a heavily monopolized sector whose predatory conduct is jaw-droppingly depraved:</p>
<p><a href="https://uspirgedfund.org/reports/usp/stranded">https://uspirgedfund.org/reports/usp/stranded</a></p>
<p>But if turning phones into e-waste to eke out another billion-dollar stock buyback is indefensible, stranding people with disabilities for months at a time while they await repairs is so obviously wicked that the conscience recoils. That's why it was so great when Colorado passed the nation's first wheelchair right to repair bill last year:</p>
<p><a href="https://www.eff.org/deeplinks/2022/06/when-drm-comes-your-wheelchair">https://www.eff.org/deeplinks/2022/06/when-drm-comes-your-wheelchair</a></p>
<p>California actually just passed <em>two</em> right to repair bills; the other one was SB-271, which mirrors Colorado's HB22-1031:</p>
<p><a href="https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB271">https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB271</a></p>
<p>This is big! It's momentum! It's a start!</p>
<p>But it can't be the end. When Bill Clinton signed DMCA 1201 into law 25 years ago, he loaded a gun and put it on the nation's mantlepiece and now it's Act III and we're all getting sprayed with bullets. Everything from ovens to insulin pumps, thermostats to lightbulbs, has used DMCA 1201 to limit repair, modification and improvement.</p>
<p>Congress needs to rid us of this scourge, to let us bring back all the benefits of interoperability. I explain how this all came to be – and what we should do about it – in my new Verso Books title, <em>The Internet Con: How to Seize the Means of Computation.</em></p>
<p><a href="https://www.versobooks.com/products/3035-the-internet-con">https://www.versobooks.com/products/3035-the-internet-con</a></p>
<p>(<i>Image: <a href="https://commons.wikimedia.org/wiki/File:Daytona_Skeleton_AR-15_completed_rifle_%2817551907724%29.jpg">Mitch Barrie</a>, <a href="https://creativecommons.org/licenses/by-sa/2.0/deed.en">CC BY-SA 2.0</a>; <a href="https://www.flickr.com/photos/kambanji/4135216486/">Kambanji</a>, <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a>; <a href="https://www.rawpixel.com/image/12438797/png-white-background">Rawpixel</a>; modified</i>)</p>
<hr>

<h2>Hey look at this (<a href="https://pluralistic.net/2023/09/22/vin-locking/#linkdump">permalink</a>)</h2>
<p><img data-lazy-fallback="1" decoding="async" src="https://i0.wp.com/craphound.com/images/heylookatthis.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/heylookatthis.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<ul>
<li>Book giveaway for The Bezzle (Martin Hench #2) <a href="https://www.goodreads.com/giveaway/show/371380-the-bezzle-a-martin-hench-novel">https://www.goodreads.com/giveaway/show/371380-the-bezzle-a-martin-hench-novel</a>
</li>
<li>
<p>West of House <a href="https://brokenneedle.gumroad.com/l/westofhouse">https://brokenneedle.gumroad.com/l/westofhouse</a> (h/t Wil Wheaton)</p>
</li>
<li>
<p>T-Shirts Now Available! <a href="http://www.imagineeringdisney.com/blog/2023/9/19/t-shirts-now-available.html">http://www.imagineeringdisney.com/blog/2023/9/19/t-shirts-now-available.html</a></p>
</li>
</ul>
<hr>
<p><a name="retro"></a><br>
<img data-lazy-fallback="1" decoding="async" alt="A Wayback Machine banner." src="https://i0.wp.com/craphound.com/images/wayback-machine-hed-796x416.png?resize=796%2C416&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/wayback-machine-hed-796x416.png?resize=796%2C416&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<h2>This day in history (<a href="https://pluralistic.net/2023/09/22/vin-locking/#retro">permalink</a>)</h2>
<p>#20yrsago New voting machines are criminally bad <a href="https://www.salon.com/2003/09/23/bev_harris/">https://www.salon.com/2003/09/23/bev_harris/</a></p>
<p>#15yrsago Your chance to mark up the Wall Street bailout bill <a href="https://web.archive.org/web/20080929041702/http://publicmarkup.org/">https://web.archive.org/web/20080929041702/http://publicmarkup.org/</a></p>
<p>#15yrsago Hank Paulson’s bailout 419 letter <a href="https://web.archive.org/web/20080923194140/https://www.thenation.com/blogs/jstreet/363133/bailout_satire">https://web.archive.org/web/20080923194140/https://www.thenation.com/blogs/jstreet/363133/bailout_satire</a></p>
<p>#15yrsago Stanford and Harvard b-school profs vs. free/open source software <a href="https://news.slashdot.org/story/08/09/22/2254228/stanford-teaching-mbas-how-to-fight-open-source">https://news.slashdot.org/story/08/09/22/2254228/stanford-teaching-mbas-how-to-fight-open-source</a></p>
<p>#15yrsago Sexist pigs earn more than normal men <a href="https://www.science20.com/news_releases/old_fashioned_men_make_more_money_study">https://www.science20.com/news_releases/old_fashioned_men_make_more_money_study</a></p>
<p>#15yrsago Corrupted Science: the history, cause, effect and state of bad science <a href="https://memex.craphound.com/2008/09/22/corrupted-science-the-history-cause-effect-and-state-of-bad-science/">https://memex.craphound.com/2008/09/22/corrupted-science-the-history-cause-effect-and-state-of-bad-science/</a></p>
<p>#10yrsago Chaos Computer Club claims it can unlock Iphones with fake fingers/cloned fingerprints <a href="https://www.ccc.de/en/updates/2013/ccc-breaks-apple-touchid">https://www.ccc.de/en/updates/2013/ccc-breaks-apple-touchid</a></p>
<p>#5yrsago Anonymous stock-market manipulators behind $20B+ of “mispricing” can be tracked by their writing styles <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3198384">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3198384</a></p>
<p>#1yrago Twitch does a chokepoint capitalism: "Amazon is charging Amazon so much money to run the business via Amazon that it has no choice but to take more money from streamers." <a href="https://pluralistic.net/2022/09/22/amazon-vs-amazon/#pray-i-dont-alter-it-further">https://pluralistic.net/2022/09/22/amazon-vs-amazon/#pray-i-dont-alter-it-further</a></p>
<hr>
<p><a name="bragsheet"></a><br>
<img data-lazy-fallback="1" decoding="async" src="https://i0.wp.com/craphound.com/images/colophonimages.jpeg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/colophonimages.jpeg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<h2>Colophon (<a href="https://pluralistic.net/2023/09/22/vin-locking/#bragsheet">permalink</a>)</h2>
<p>Today's top sources: Naked Capitalism (<a href="https://www.nakedcapitalism.com/">https://www.nakedcapitalism.com/</a>).</p>
<p>Currently writing:</p>
<ul>
<li>A Little Brother short story about DIY insulin PLANNING
</li>
<li>
<p>Picks and Shovels, a Martin Hench noir thriller about the heroic era of the PC. FORTHCOMING TOR BOOKS JAN 2025</p>
</li>
<li>
<p>The Bezzle, a Martin Hench noir thriller novel about the prison-tech industry. FORTHCOMING TOR BOOKS FEB 2024</p>
</li>
<li>
<p>Vigilant, Little Brother short story about remote invigilation. FORTHCOMING ON TOR.COM</p>
</li>
<li>
<p>Moral Hazard, a short story for MIT Tech Review's 12 Tomorrows. FIRST DRAFT COMPLETE, ACCEPTED FOR PUBLICATION</p>
</li>
<li>
<p>Spill, a Little Brother short story about pipeline protests. FORTHCOMING ON TOR.COM</p>
</li>
</ul>
<p>Latest podcast: Plausible Sentence Generators <a href="https://craphound.com/news/2023/09/17/plausible-sentence-generators/">https://craphound.com/news/2023/09/17/plausible-sentence-generators/</a><br>
Upcoming appearances:</p>
<ul>
<li>DIG Festival (Modena, Italy), Sept 22<br>
<a href="https://dig-awards.org/en/dig-festival-2023-first-speakers-announced/">https://dig-awards.org/en/dig-festival-2023-first-speakers-announced/</a>
</li>
<li>
<p>Launch for Justin C Key's "The World Wasn’t Ready for You," Book Soup (LA), Sept 22<br>
<a href="https://www.booksoup.com/event/justin-c-key">https://www.booksoup.com/event/justin-c-key</a></p>
</li>
<li>
<p>Launch for "The Internet Con" and Brian Merchant's "Blood in the Machine," Chevalier's Books (LA), Sept 27<br>
<a href="https://www.eventbrite.com/e/the-internet-con-by-cory-doctorow-blood-in-the-machine-by-brian-merchant-tickets-696349940417">https://www.eventbrite.com/e/the-internet-con-by-cory-doctorow-blood-in-the-machine-by-brian-merchant-tickets-696349940417</a></p>
</li>
<li>
<p>An Evening with VE Schwab (Boise), Oct 2<br>
<a href="https://www.thecabinidaho.org/all-events/ve-schwab">https://www.thecabinidaho.org/all-events/ve-schwab</a></p>
</li>
<li>
<p>Wired Nextfest (Milano), Oct 7-8<br>
<a href="https://eventi.wired.it/nextfest23-milano">https://eventi.wired.it/nextfest23-milano</a></p>
</li>
<li>
<p>The Internet Con at Moon Palace Books (Minneapolis), Oct 15<br>
<a href="https://moonpalacebooks.com/events/30127">https://moonpalacebooks.com/events/30127</a></p>
</li>
<li>
<p>26th ACM Conference On Computer-Supported Cooperative Work and Social Computing keynote (Minneapolis), Oct 16<br>
<a href="https://cscw.acm.org/2023/index.php/keynotes/">https://cscw.acm.org/2023/index.php/keynotes/</a></p>
</li>
<li>
<p>41st annual McCreight Lecture in the Humanities (Charleston, WV), Oct 19<br>
<a href="https://festivallcharleston.com/venue/university-of-charleston/">https://festivallcharleston.com/venue/university-of-charleston/</a></p>
</li>
<li>
<p>Seizing the Means of Computation (Edinburgh Futures Institute), Oct 25<br>
<a href="https://efi.ed.ac.uk/event/seizing-the-means-of-computation-with-cory-doctorow/">https://efi.ed.ac.uk/event/seizing-the-means-of-computation-with-cory-doctorow/</a></p>
</li>
</ul>
<p>Recent appearances:</p>
<ul>
<li>Against Enshittification | Medium Day 2023<br>
<a href="https://www.youtube.com/watch?v=mSeBelDVrgE">https://www.youtube.com/watch?v=mSeBelDVrgE</a>
</li>
<li>
<p>The Jim Rutt Show<br>
<a href="https://www.jimruttshow.com/cory-doctorow-2/">https://www.jimruttshow.com/cory-doctorow-2/</a></p>
</li>
<li>
<p>How to Take Back the Internet (Wired Have a Nice Future)<br>
<a href="https://www.wired.com/story/have-a-nice-future-podcast-21/">https://www.wired.com/story/have-a-nice-future-podcast-21/</a></p>
</li>
</ul>
<p>Latest books:</p>
<ul>
<li>"The Internet Con": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (<a href="http://seizethemeansofcomputation.org/">http://seizethemeansofcomputation.org</a>). Signed copies at Book Soup (<a href="https://www.booksoup.com/book/9781804291245">https://www.booksoup.com/book/9781804291245</a>).
</li>
<li>
<p>"Red Team Blues": "A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before." Tor Books <a href="http://redteamblues.com/">http://redteamblues.com</a>. Signed copies at Dark Delicacies (US): <a href="https://www.darkdel.com/store/p2873/Wed%2C_Apr_26th_6pm%3A_Red_Team_Blues%3A_A_Martin_Hench_Novel_HB.html#/"> and Forbidden Planet (UK): </a><a href="https://forbiddenplanet.com/385004-red-team-blues-signed-edition-hardcover/">https://forbiddenplanet.com/385004-red-team-blues-signed-edition-hardcover/</a>.</p>
</li>
<li>
<p>"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 <a href="https://chokepointcapitalism.com/">https://chokepointcapitalism.com</a></p>
</li>
<li>
<p>"Attack Surface": The third Little Brother novel, a standalone technothriller for adults. The <em>Washington Post</em> called it "a political cyberthriller, vigorous, bold and savvy about the limits of revolution and resistance." Order signed, personalized copies from Dark Delicacies <a href="https://www.darkdel.com/store/p1840/Available_Now%3A_Attack_Surface.html">https://www.darkdel.com/store/p1840/Available_Now%3A_Attack_Surface.html</a></p>
</li>
<li>
<p>"How to Destroy Surveillance Capitalism": an anti-monopoly pamphlet analyzing the true harms of surveillance capitalism and proposing a solution. <a href="https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59">https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59</a> (print edition: <a href="https://bookshop.org/books/how-to-destroy-surveillance-capitalism/9781736205907">https://bookshop.org/books/how-to-destroy-surveillance-capitalism/9781736205907</a>) (signed copies: <a href="https://www.darkdel.com/store/p2024/Available_Now%3A__How_to_Destroy_Surveillance_Capitalism.html">https://www.darkdel.com/store/p2024/Available_Now%3A__How_to_Destroy_Surveillance_Capitalism.html</a>)</p>
</li>
<li>
<p>"Little Brother/Homeland": A reissue omnibus edition with a new introduction by Edward Snowden: <a href="https://us.macmillan.com/books/9781250774583">https://us.macmillan.com/books/9781250774583</a>; personalized/signed copies here: <a href="https://www.darkdel.com/store/p1750/July%3A__Little_Brother_%26_Homeland.html">https://www.darkdel.com/store/p1750/July%3A__Little_Brother_%26_Homeland.html</a></p>
</li>
<li>
<p>"Poesy the Monster Slayer" a picture book about monsters, bedtime, gender, and kicking ass. Order here: <a href="https://us.macmillan.com/books/9781626723627">https://us.macmillan.com/books/9781626723627</a>. Get a personalized, signed copy here: <a href="https://www.darkdel.com/store/p2682/Corey_Doctorow%3A_Poesy_the_Monster_Slayer_HB.html#/">https://www.darkdel.com/store/p2682/Corey_Doctorow%3A_Poesy_the_Monster_Slayer_HB.html#/</a>.</p>
</li>
</ul>
<p>Upcoming books:</p>
<ul>
<li>The Lost Cause: a post-Green New Deal eco-topian novel about truth and reconciliation with white nationalist militias, Tor Books, November 2023
</li>
<li>
<p>The Bezzle: a sequel to "Red Team Blues," about prison-tech and other grifts, Tor Books, February 2024</p>
</li>
<li>
<p>Picks and Shovels: a sequel to "Red Team Blues," about the heroic era of the PC, Tor Books, February 2025</p>
</li>
<li>
<p>Unauthorized Bread: a graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2025</p>
</li>
</ul>
<hr>
<p><img data-lazy-fallback="1" decoding="async" src="https://i0.wp.com/craphound.com/images/by.svg.png?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/by.svg.png?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<p>This work – excluding any serialized fiction – is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net.</p>
<p><a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></p>
<p>Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution.</p>
<hr>
<h2>How to get Pluralistic:</h2>
<p>Blog (no ads, tracking, or data-collection):</p>
<p><a href="http://pluralistic.net/">Pluralistic.net</a></p>
<p>Newsletter (no ads, tracking, or data-collection):</p>
<p><a href="https://pluralistic.net/plura-list">https://pluralistic.net/plura-list</a></p>
<p>Mastodon (no ads, tracking, or data-collection):</p>
<p><a href="https://mamot.fr/@pluralistic">https://mamot.fr/@pluralistic</a></p>
<p>Medium (no ads, paywalled):</p>
<p><a href="https://doctorow.medium.com/">https://doctorow.medium.com/</a></p>
<p>(Latest Medium column: "How To Think About&nbsp;Scraping: In privacy and labor fights, copyright is a clumsy tool at&nbsp;best <a href="https://doctorow.medium.com/how-to-think-about-scraping-2db6f69a7e3d">https://doctorow.medium.com/how-to-think-about-scraping-2db6f69a7e3d</a>)</p>
<p>Twitter (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://twitter.com/doctorow">https://twitter.com/doctorow</a></p>
<p>Tumblr (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://mostlysignssomeportents.tumblr.com/tagged/pluralistic">https://mostlysignssomeportents.tumblr.com/tagged/pluralistic</a></p>
<p>"<em>When life gives you SARS, you make sarsaparilla</em>" -Joey "Accordion Guy" DeVilla</p>


	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Roman Republic, Part IV: The Senate (143 pts)]]></title>
            <link>https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/</link>
            <guid>37614216</guid>
            <pubDate>Fri, 22 Sep 2023 16:38:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/">https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/</a>, See on <a href="https://news.ycombinator.com/item?id=37614216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>This is the third section of the third part of our our planned five part series (<a href="https://acoup.blog/2023/07/21/collections-how-to-roman-republic-101-part-i-spqr/">I</a>, <a href="https://acoup.blog/2023/07/27/collections-how-to-roman-republic-101-part-ii-romans-assemble/">II</a>, <a href="https://acoup.blog/2023/08/11/collections-how-to-roman-republic-101-part-iiia-starting-down-the-path-of-honors/">IIIa</a>, <a href="https://acoup.blog/2023/08/18/collections-how-to-roman-republic-101-part-iiib-imperium/">IIIb</a>, <a href="https://acoup.blog/2023/08/25/collections-how-to-roman-republic-part-iiic-ten-tribunes-two-censors-and-twenty-six-guys/" data-type="link" data-id="https://acoup.blog/2023/08/25/collections-how-to-roman-republic-part-iiic-ten-tribunes-two-censors-and-twenty-six-guys/">IIIc</a>) on the structure of the Roman Republic during the third and second centuries, the ‘Middle’ Republic.’  Over the last few posts we looked at the role of Roman magistrates who carried out a range of executive functions for the republic.  This week, we’re looking at the Roman Senate, an institution so important that it is included alongside the people of Rome in the SPQR formulation that the Romans used to represent the republic, and yet also paradoxically it is an institution that lacks any kind of formal legal powers.</p>



<p>Despite that lack of formal powers, the Senate of the Roman Republic largely directed the overall actions of the republic, coordinating its strategic policy (both military and diplomatic), setting priorities for legislation, handling Rome’s finances and assigning and directing the actions of the various magistrates.  The Senate – <em>not the Pontifex Maximus</em><span id="easy-footnote-1-20956"></span><span><a href="#easy-footnote-bottom-1-20956" title="I stress this point because this is a common mistake: assuming that the <em>Pontifex Maximus</em> as Rome&amp;#8217;s highest priest was in some way the &amp;#8216;boss&amp;#8217; of all of Rome&amp;#8217;s other priests.  He was not; he was the presiding officer of the college of Pontiffs and the manager of the calendar (this was a very significant role), but the <em>Pontifex Maximus</em> was not the head of some priestly heirarchy and his power over the other <em>pontifices</em> was limited.  Moreover his power over other religious officials (the <em>augures</em>, <em>haruspices</em>, the <em>quindecimviri sacris faciundis</em> and so on) was very limited.  Instead, these figures report to the Senate, though the Senate will generally defer to the judgement of the <em>pontifices</em>."><sup>1</sup></a></span>  – was also the final authority for questions of religion.  The paradox exists because the Senate’s power is almost entirely based in its <em>auctoritas</em> and the strong set of political norms and cultural assumptions which push Romans to defer to that <em>auctoritas</em>.</p>



<p>And if you want to wield the <em>auctoritas</em> of the <em>ACOUP</em> Senate, you can support this project on <a href="https://www.patreon.com/user?u=20122096">Patreon at the <em>patres et matres conscripti</em></a> tier, to be able to submit questions and weigh in on what topics we should cover next (I promise the long-awaited Greek-and-Phoenician colonization treatment is coming!). As always, if you like this, please share it! If you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings, assuming there is still a Twitter by the time this post goes live. I am also on Bluesky (@bretdevereaux.bsky.social) and (less frequently) Mastodon (@bretdevereaux@historians.social).</p>






<h2>Membership</h2>



<p>We should start with who is in the Senate.  Now what you will generally hear in survey courses is this neat summary: the Senate had 300 members (600 after Sulla) and included all Romans who had obtained the office of the quaestorship or higher <em>and</em> its members were selected by the censors.  And for a basic summary, that actually serves pretty well, but thinking about it for a few minutes one quickly realizes that there must be quite a bit of uncertainty and complexity underneath those neat easy rules.  And indeed, there is!</p>



<p>First we can start with eligibility by holding office.  We know that in the Sullan constitution, holding the quaestorship entitled one into entrance into the Senate.  Lintott notes that the lex repetundarum of 123/4 lumped every office aedile-and-above together in a phrasing ‘anyone who has or shall have been in the Senate’ when setting eligibility for the juries for the repetundae courts (the aim being to exclude the magistrate class from judging itself on corruption charges), and so assumes that prior to Sulla, it was aediles and up (but not quaestors) who were entitled to be in the Senate.<span id="easy-footnote-2-20956"></span><span><a href="#easy-footnote-bottom-2-20956" title="With sitting tribunes able to attend meetings of the Senate, but not being granted lifelong membership."><sup>2</sup></a></span>  The problem immediately occurs: these higher offices don’t provide enough members to reach the frequently attested 300-Senator size of the Senate with any reasonable set of life expectancies.</p>



<p>By contrast, if we assume that the quaestors were enrolled in the Senate, as we know them to have been post-Sulla (Cicero is a senator for sure in 73, having been quaestor in 75), we have 8 quaestors a year elected around age 30 each with roughly 30 years of life expectancy<span id="easy-footnote-3-20956"></span><span><a href="#easy-footnote-bottom-3-20956" title="A touch higher than the 24 years a L3 Model West life table (what we generally use to simulate Roman populations) leads us to expect, but then these are elites who are likely to be well nourished and not in hazardous occupations, so they might live a bit longer."><sup>3</sup></a></span> we get a much more reasonable 240, to which we might add some holders of senior priesthoods who didn’t go into politics and the ten sitting tribunes and perhaps a few reputable scions of important families selected by the censors to reach 300 without too much difficulty.  The alternative is to assume the core membership of the Senate was aediles and up, which would provide only around 150 members, in which case the censors would have to supplement that number with important, reputable Romans.</p>



<p>To which we may then ask: who might they choose?  The obvious candidates would be…current and former quaestors and plebeian tribunes.  And so we end up with a six-of-one, half-dozen of the other situation, where it is possible that quaestors were not <em>automatically</em> enrolled before Sulla, but were customarily chosen by the censors to ‘fill out’ the Senate.  Notably, when Sulla wants to expand the Senate, he radically expands (to twenty) the number of quaestors, which in turn provides roughly enough Senators for his reported 600-person Senate.</p>



<p>That leads us to the roll of the censors: if holding a sufficiently high office (be it the quaestorship or aedileship) <em>entitles</em> one to membership for life in the Senate, what on earth is the role of the censors in selecting the Senate’s membership?  Here the answer is in the sources for us: we repeatedly see the formula that the meetings of the Senate were attended by two groups: the Senators themselves and “those who are permitted to state their opinion in the Senate.”  Presumably the distinction here is between men designated as senators by the censors and men not yet so designated who nevertheless, by virtue of office-holding, have a right to speak in the Senate.  It’s also plausible that men who were still <em>iuniores</em> might not yet be Senators (whose very name, after all, implies old age; <em>Senator</em> has at its root <em>senex</em>, “old man”) or perhaps men still under the <em>potestas </em>of a living father (who thus could hardly be one of the <em>patres conscripti</em>, a standard term for Senators) might be included in the latter group.</p>



<p>In any case, the censors seem to have three rolls here.  First, they confirm the membership in the Senate of individuals entitled to it by having held high office.  Second, they can fill out an incomplete Senate with additional Roman aristocrats so that it reaches the appropriate size.  Finally, they can <em>remove</em> a Senator for moral turpitude, though this is rare and it is clear that the conduct generally needed to be egregious.</p>



<p>In this way, we get a Senate that is as our sources describe: <em>roughly</em> 300 members at any given time (brought to the right number every five years by the censors), consisting mostly of former office holders (with some add-ons) who have held offices at or above the quaestorship and whose membership has been approved by the censors, though office holders might enter the Senate – provisionally, as it were – immediately pending censorial confirmation at a later date.  If it seems like I am giving short shrift to the ‘filling the rank’ add-ons the censors might provide, it is because – as we’ll see in a moment – Senate procedure combined with Roman cultural norms was likely to render them quite unimportant.  The role of <em>senior</em> ex-magistrates in the Senate was to speak, the role of junior ex-magistrates (and certainly of any senator who had not held high office!) was to <em>listen </em>and indicate concurrence with a previously expressed opinion, as we’re going to see when we get to procedure.</p>



<h2>Meetings</h2>



<p>Meetings of the Senate were formal affairs, but unlike modern legislatures the Senate did not stay in session over long periods.  Instead, it met in specific venues – they had to be inaugurated – when called by a magistrate with the power to do so.</p>



<p>We may begin with place: the Senate had no single fixed meeting spot, though the <em>curia</em> in the Forum was the most common location, however the place the Senate met had to be religiously prepared via inauguration (the taking of the auspices by the augurs) and by sacrifices in order to make sure the gods approved of the proceedings and its results.  Consequently, the Senate always met in a <em>templum</em> in the sense of a consecrated space, but also it tended to meet literally <em>in temples</em>, with meetings in the temples of Jupiter Optimus Maximus, the temple of Fides, the temple of Concord, and so on.  Notably, two locations, the temple of Bellona and the temple of Apollo were also used and these sat <em>outside</em> the <em>pomerium</em>, enabling the Senate to meet with magistrates who, because of their active command of an army, could not cross the <em>pomerium</em>; they were also sometimes used to meet with foreign dignitaries the Senate did not wish to let into the city.  Later added to this number of sites outside the pomerium was Pompey’s theater, which included a temple of Venus Victrix and a <em>curia</em> as part of the overall complex.</p>



<figure><img data-attachment-id="21040" data-permalink="https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/image-9-13/" data-orig-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?fit=1024%2C768&amp;ssl=1" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-9" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?fit=1024%2C768&amp;ssl=1" decoding="async" loading="lazy" width="1024" height="768" src="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=1024%2C768&amp;ssl=1" alt="" srcset="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?w=1024&amp;ssl=1 1024w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=800%2C600&amp;ssl=1 800w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=600%2C450&amp;ssl=1 600w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/image-9.webp?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Via <a href="https://en.wikipedia.org/wiki/Curia_Julia" data-type="link" data-id="https://en.wikipedia.org/wiki/Curia_Julia">Wikipedia</a>, the interior of the <em>curia Julia</em>, the Senate’s primary meeting place.  The <em>curia Julia</em> itself comes late, finished in 29 BC to replace the destroyed <em>Curia Cornelia</em> which in turn had replaced the older <em>Curia Hostilia</em>.  Still, this gives us a sense of what a meeting place for the Senate might be like, rather less spacious than most movies would have it, with 300 senators it would have been quite crowded (and the <em>curia Hostilia</em> was no larger in terms of footprint).</figcaption></figure>



<p>In order to meet, the Senate had to be called or more correctly ‘driven together’ (<em>cogere</em>, often translated adequately as ‘summoned,’ but as Lintott notes, it has an element of compulsion to it) by a magistrate.  There were a few standard dates on which this would effectively always happen, particularly the first day of the consular year, but beyond that it was expected that magistrates in Rome could call the Senate at any time to discuss any issue on relatively short notice.  There was initially no <em>requirement</em> that Senators live in the city of Rome, but it was clearly assumed.  Early on in the second century, we get regulations requiring Senators to stay close to Rome unless they had an official reason to be elsewhere, though Senators might be permitted to leave if they needed to fulfill a vow.  In the Late Republic it seems to have been common also for Senators to leave the city during the spring <em>res prolatae</em>, a sort of recess from public business (literally “the deferring of business”), but these informal breaks did not mean the Senate was truly ‘out of session’ and it could still be summoned by a magistrate.</p>



<figure><img data-attachment-id="21042" data-permalink="https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/sestertius_showing_temple_of_concord/" data-orig-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?fit=1024%2C1024&amp;ssl=1" data-orig-size="1024,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sestertius_showing_Temple_of_Concord" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?fit=1024%2C1024&amp;ssl=1" decoding="async" loading="lazy" width="1024" height="1024" src="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=1024%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=600%2C600&amp;ssl=1 600w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Sestertius_showing_Temple_of_Concord.jpg?resize=200%2C200&amp;ssl=1 200w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Via <a href="https://en.wikipedia.org/wiki/Temple_of_Concord" data-type="link" data-id="https://en.wikipedia.org/wiki/Temple_of_Concord">Wikipedia</a>, two sestertii from the reign of Tiberius minted in 36/7 in Rome commemorating his restoration of the Temple of Concord, another traditional meeting place of the Senate.  The large ‘SC’ on the reverse does in fact, stand for <em>senatus consulto</em>, because the weights and values of Roman bronze coinage was decreed by the Senate.</figcaption></figure>



<p>Generally, meetings of the Senate began at dawn, though they could begin later, and they proceeded either until the business was concluded or to dusk.  Because of the ritual preparations required, no meeting of the Senate could last more than a day, much like the assemblies, so if the business was not finished, a new meeting would need to be called and the process begun from scratch.  While it seems that magistrates generally tried to avoid calling the Senate during festival days, <em>dies nefandi</em> (days unsuited for public business) and meetings of the popular assemblies, there was no requirement to do so and the Senate might be called for any day for most of the Republic, with laws restricting the Senate’s meeting days only coming midway through the first century.</p>



<p>Beyond this, Senators were expected to show up and we hear of threats of fines or other censure for failure to show up, but it also seems like no meeting of the senate was ever very close to the full body and quorums for the Senate were fairly low, 100 or 150.  For the Sullan Senate, notionally of 600 members, the highest attendances we know of, as noted by Lintott, are 415, 417 and 392.  Of course some significant number of Senators will, at any time, have been active magistrates overseas, or serving as military tribunes, or as senatorial <em>legati</em>, but it seems clear that even beyond this attendance was not universal even if it was in theory supposed to be.</p>



<h2>Procedure</h2>



<p>Once the Senate met, it had a standard procedure which was followed.  <strong>The Senate was fundamentally a deliberative body</strong>, unlike the assemblies which merely voted, and so most of the procedure focused on that deliberation and debate.  And this makes sense, as the notional purpose of the Senate was as a large advisory council to the magistrates, in the same way that it had once been the advisory council to the kings.</p>



<p>Generally, each meeting of the Senate had a set agenda item, presented by the convening magistrate, although for that traditional meeting at the start of the consular year it was common to convene the Senate <em>de re publica infinite</em>, “on the whole Republic.”  By far the most common convening magistrate was the consul, but other magistrates could convene the Senate (notably the tribunes).  The convening magistrate spoke first with an introductory speech, a <em>relatio</em>, with introduced the topic and often presented a proposed course of action on that topic.  In some cases, that introduction might be a short one, but it could also be a long and involved speech or the introduction of an ambassador or the reading of dispatches.  The Senate seems also at sometimes to have formed subcommittees and a magistrate might use an introduction to put the recommendations of a subcommittee, such as pre-drafted legislation, before the body.  But it was just as common to open a debate on a broad topic.</p>



<p>Once the convening magistrate was done introducing the issue, the opinions of each senator, in turn, were sought.  The order was set by the censors, but it was based on offices held and seniority, so while the censors could shift (<em>‘movere’</em>) a senator down in the order, they were expected to have a good reason (typically conspicuous moral turpitude).  The order began with the <em>princeps senatus</em>, traditionally the most senior ex-consul, though ‘most senior’ here often meant both in age and in influence, so while the <em>princeps senatus</em> was never young, it might not strictly be the oldest senator.  After that, the former consuls (in Latin <em>consulares</em>, which enters English as ‘consulars’ to mean ‘men who have held the consulship’) spoke, with the most former (and thus likely oldest) going first.  And then the Senate proceeded down in rank order to ex-praetors and so on all the way down.  As you may well imagine, the figures who spoke first tended to set the terms of the debate and indeed the whole reason they spoke first is because they were understood to be preeminent in <em>auctoritas</em>.</p>



<p>Each senator, as the order came to them was expected to present their opinion (<em>sententia</em>).  The expression of their view could be as long or as short as they wished (filibusters were possible), but they had to answer.  Senators were given broad latitude in their response, able to interrogate the magistrate or visiting ambassadors, respond to their fellows or wander entirely off of the origional <em>relatio</em> to another topic entirely  They could also launch into long speeches which, again, might or might not actually pertain primarily to the topic at hand.  Cato the Elder was famous for ending speeches on topics entirely unrelated to the matter with an exhortation that “Carthage must be destroyed,” while Cicero took the opportunity of a debate on some relatively minor matters of roads and mints to launch into his Seventh Philippic, a speech exhorting the Senate to move against Marcus Antonius.  Still it was expected that at some point the response ought to speak to the matter at hand and so Cicero ends the Seventh Philippic with <em>quibus de rebus refers, P. Servilio adsentior</em>, “on the matters you refer [to the Senate], I agree with Publius Servilius.”</p>



<figure><img data-attachment-id="21044" data-permalink="https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/cicero_denounces_catiline_in_the_roman_senate_by_cesare_maccari/" data-orig-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?fit=1280%2C833&amp;ssl=1" data-orig-size="1280,833" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?fit=300%2C195&amp;ssl=1" data-large-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?fit=1024%2C666&amp;ssl=1" decoding="async" loading="lazy" width="1024" height="666" src="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=1024%2C666&amp;ssl=1" alt="" srcset="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=1024%2C666&amp;ssl=1 1024w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=300%2C195&amp;ssl=1 300w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=768%2C500&amp;ssl=1 768w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=1200%2C781&amp;ssl=1 1200w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?resize=1100%2C716&amp;ssl=1 1100w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Cicero_Denounces_Catiline_in_the_Roman_Senate_by_Cesare_Maccari.png?w=1280&amp;ssl=1 1280w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Via <a href="https://en.wikipedia.org/wiki/Cesare_Maccari" data-type="link" data-id="https://en.wikipedia.org/wiki/Cesare_Maccari">Wikipedia</a>, Cicero Denounces Catiline (1888), painting by Cesare Maccari.  As noted, the Senate’s meeting place would not have been so spacious or rounded like this, but the painting does capture the movement of the pedarii, in this case abandoning Catiline as Cicero denounces him with the <a href="https://en.wikipedia.org/wiki/Catilinarian_orations" data-type="link" data-id="https://en.wikipedia.org/wiki/Catilinarian_orations">First Catilinaria</a>n.</figcaption></figure>



<p>On the other hand, the minimum possible response a senator could give, and this must have been a very frequent response, was to agree with what some other senator earlier in the speaking order had said and we certainly get the sense that this was the most common thing for more junior senators to do.  Indeed, we’re told that, while the more senior senators were speaking, more junior senators, termed <em>pedarii</em> (lit: ‘footmen’ but really ‘walkers’) by Cicero for reasons that will soon be obvious, would move in the hall to sit by the speaker with whom they agreed to show their support.  Given the elimination-contest nature of the Roman office holding, the Senate must always have had far more of these junior senators than of senior consulars whose opinion would carry real weight, but their movement <em>en masse</em> to agree with one or another position would have carried its own <em>auctoritas</em>.</p>



<p>Now, junior senators absolutely could hold forth on their own basis and we are told of occasions where they did so.  Sallust presents the debate on the fate of Catiline’s conspirators in 63 as turning on the opinions of Marcus Porcius Cato (the younger) and Gaius Julius Caesar; Cato was almost certainly one of the most junior members of the Senate, being the <em>quaestor urbanus</em> that very year, while Caesar was more senior as a praetor-elect and Pontifex Maximus, but still hardly the most senior fellow in the room.  But in practice, moments like these were probably relatively rare (Caesar and Cato both may have felt emboldened because of their current positions, though Cato especially, a powerful speaker, maintained outsized influence in the Senate despite never rising to the consulship).  Roman culture strongly encouraged deference both to elders and social superiors and the senior consulars were both, while in turn the mechanics of Roman politics will have meant that junior senators would have been wise to attach themselves to powerful consulars in any event.</p>



<p>At the end of this process of speaking, the presiding magistrate could put the issue to a vote.  The magistrate in question set the terms of the vote, laying out a proposed <em>senatus consultum</em> (opinion of the Senate) and asking all senators who agreed to go to one side while everyone who disagreed go to the other.  Multiple such motions could be voted in succession and the Senate might approve or disapprove of any set of them, but the order was up to the magistrate who might thus frame proposals tactically to achieve a given outcome.  If the vote passed then the proposal – drafted into written form by the presiding magistrate, usually with the assistance of a few more junior senators – and issued as a formal decree of the Senate, called a <em>senatus consultum</em>.</p>



<p>A few things could interrupt this stately order.  The simplest was that senators could change their mind, based presumably on the arguments of others later in the speaking order; they seem to have been able to interrupt the order to announce that change, that they now agreed with so-and-so.  Alternately, there might be calls for various kinds of floor motions, akin to points of order.  The most common was <em>consule!</em> (‘consult!’), a call to signal that the magistrate needed to consult the senators (run through the speaking order) before proceeding to a vote on a given measure; you couldn’t just propose a vote and ram it through without debate.  Alternately, senators might call out <em>divide</em>! (‘divide!’) to signal that certain motions being presented together needed to be split up, discussed and voted on separately.  Also <em>numera!</em> (‘numbers!’ meaning ‘count!’) was the call to demand a quorum check.  The Senate might even call for discussion to be opened on an issue, demanding a new <em>relatio</em> to discuss issues that had been raised – most often in the reading of dispatches or the reports of ambassadors, but sometimes from debates that broke out between senators.</p>



<p>Consequently, while the presiding magistrate convened the Senate and in theory set the agenda, in practice the Senate was much less under their control than the voting assembly.  The presiding magistrate, it must be noted, could not cut off debate, though he could simply refuse to put a motion to a vote at the end of it.  In addition, just to gum up the process further, a <em>senatus consultum</em> could be veto’d by the consuls or the tribunes, though the opinion of the Senate was still registered, just merely as the <em>senatus auctoritas</em> rather than <em>consultum</em>, though this carried a lot less weight.  A veto’d proposal could simply be brought another day, in the hope that it might escape veto subsequently and at least until the 130s, it seems to have been an accepted part of the <em>mos maiorum</em> that one did not maintain a veto indefinitely against either the popular will or the will of the Senate (much less both), so in the third and most of the second century, a veto was a delaying tactic rather than a decisive killing of a motion.</p>



<p>As an aside before we move on to the Senate’s (lack of) formal powers and (abundant) traditional prerogatives, it seems worth noting how much emphasis this sort of system placed both on the speaking ability of senators and their personal <em>auctoritas</em>.  Unlike as in many modern legislatures, the speeches here were not cosmetic and senators were not bound to a ‘party line’ in their votes.  Consequently, powerful speakers or senators with formidable reputations, <em>if they were present</em> (and not, say, deployed in the provinces) could exert a lot of sway among the junior members of the senate who would make up the majority of votes when it came time to approve a <em>senatus consultum</em>.</p>



<h2>Powers and Prerogatives</h2>



<p>Now we have a <em>senatus consultum</em>, so we may ask what power it has and the answer is ‘none.’  But it is also ‘very much.’  And that brings us to the central paradox of the Senate.  On the one hand, as I have my students chant in class, <strong>the Senate has no formal powers</strong> (in the Republic, before Sulla).  It cannot legislate, it cannot adjudicate court cases, it cannot create or appoint magistrates, it cannot order armies or convene assemblies.  <em><strong>All</strong></em> the Senate can do is<em><strong> issue opinions</strong></em>, which are not <em>technically</em> binding on anyone.  This is particularly true when a <em>senatus consultum</em> comes into conflict with a decision of a popular assembly (a <em>lex</em>); at all points, the <em>lex</em> wins.</p>



<p><strong>But the advice of the Senate is almost <em>always</em> followed, especially in certain key areas of the Republic</strong>.</p>



<p>Consequently while the Senate’s <em>de iure</em> powers basically don’t exist, its <em>de facto</em> powers under the <em>mos maiorum </em>are vast to the point that outside observers like Polybius conclude that the Senate is the most powerful part of the Roman state.<span id="easy-footnote-4-20956"></span><span><a href="#easy-footnote-bottom-4-20956" title="Though it must be noted that Polybius is mostly looking at Roman foreign policy, where the Senate is strongest."><sup>4</sup></a></span>  As a result, in the period we can see most clearly from the fourth to the second century, the Senate almost always gets its way and exercises a customary control over key aspects of Roman governance, especially foreign policy, that is nearly complete.  And while the Senate might not have any clearly defined legal authority to do <em>any of this</em>, <strong>governments are how they function</strong> and this is how the <em>res publica</em> functioned during the third and second centuries.</p>



<p>Domestically, the <em>auctoritas</em> of the Senate was such that it was clearly hard to pass laws without obtaining the <em>consilium</em> (consultation, counsel) of the Senate and receiving a favorable <em>senatus consultum</em>.  You can easily see the mix of pressures here: on the one hand, a politician pushing a law would be profoundly unwise to trample the Senate to do so, because at the end of his short one-year term he would go back to being just a senator and in any case reliant on the influence of other senators to win future high office or other honors.  At the same time, Roman voters seem to have taken the <em>consultum</em> of the Senate pretty seriously, so if the Senate was strongly against a motion, they were likely to vote it down <em>anyway</em>.  And that makes sense: the Senate is a body of the most successful, most prestigious, most respected Romans in a society where you are expected to the defer to the judgements of such men.  The People were not always supine before the Senate by any means, but the <em>auctoritas</em> <em>senatus</em> meant a lot.</p>



<p>That said, the Senate’s authority was unevenly spread.  It seems fairly clear, especially by the fourth century, that the Senate felt it necessary to yield – if only slowly – on proposals where it faced widespread popular opposition.  One may, of course, immediately contrast the Senate of the late-second and early-first century, which squanders its <em>auctoritas</em> trying to stop proposals of this sort.  So when it came to domestic affairs, the power of the Senate was perhaps at a low ebb; it could offer its approval to laws or its disapproval, but carrying out the laws was up to the magistrates and the assemblies of the people had the final say in passing or not passing proposals.</p>



<p>In foreign policy, the Senate was far more influential, to the point of dominance.  One crucial task that is clearly regularized well before 218, when the survival of more of Livy lets us see it clearly, is the assignment of magistrates to specific jobs or <em>provinciae</em>.  It is frankly unclear how strong the legal basis for these assignments was and in any event it doesn’t matter: what matters is that the Senate advised the magistrates and the magistrates always acted accordingly.  Consequently, the Senate was the senior strategic directing organ of the Republic, coordinating strategy and indeed <em>grand</em> strategy over the whole of the Mediterranean through a system of provincial assignments, directives to the magistrates assigned and transfers of resources between Italy and those provinces (including province-to-province transfers) accomplished by assigned senatorial commissioners (called <em>legati</em>).<span id="easy-footnote-5-20956"></span><span><a href="#easy-footnote-bottom-5-20956" title=" I should note much of this picture is owed to Michael Taylor, with whom I am collaborating on work on this very topic; I&amp;#8217;ll be sure to hollar if/when that work appears in print, of course."><sup>5</sup></a></span>  What we see in Livy is a regular cycle where these assignments are decided on at the start of each consular year, along with the Senate ‘advising’ the magistrates and pro-magistrates on how many <a href="https://acoup.blog/2023/06/16/collections-how-to-raise-a-roman-army-the-dilectus/" data-type="post" data-id="19439">soldiers they should levy</a> and where those troops ought to go (along with supplies, as mentioned); the Senates prerogative in this regard becomes so ironclad that it is by the career of Scipio Aemilianus treated as effectively a legal restriction, which can only be finessed by taking volunteers, but not by simply passing a law or ignoring the Senate.</p>



<p>Connected to this was the Senate’s oversight over state finance.  The finances of the <em>res publica</em>, you will recall, were overseen by quite junior officials, the quaestors.  In practice what that seems to have really meant was that the larger financial picture was handled by the Senate, which ,set the annual <em>tributum</em> (Rome’s direct tax on citizens) and approved major expenditures from the treasury, the largest of which will have been to the armies, thus neatly tying up with the above paragraph.  It is not hard to see why the Senate ends up running this aspect, both because it was one which required long-term vision, but also because the quaestors, as junior magistrates looking to move further up the <em>cursus honorum</em>, wouldn’t have have had any incentive or sufficient personal influence to defy the Senate’s <em>auctoritas</em> in any event.  The Senate thus ‘advised’ the quaestors on what to do with Rome’s finances, and they did it.  Once again, the question of if the Senate had the <em>legal</em> power to do this was rather beside the point: in any event the quaestors complied.</p>



<p>Likewise, the Senate had a powerful role in foreign policy, though in the end only the <em>comitia centuriata</em> could make peace or declare wars.  Foreign ambassadors had to report to the consuls, but the consuls role, in addition to hosting them, was primarily to introduce the ambassadors to speak to the Senate, and then it was the Senate which considered their positions.  That makes a lot of sense, as the Senate is a continuing body: an agreement with a consul would only hold potentially for a year, but an agreement with the Senate has some permanence to it and it certainly seems like if the Senate suggested treaty ratification to the assembly, it passed.  As noted previously, assemblies bucking the Senate on questions of war and peace was extremely rare; when it happened, concessions were made and eventually the People followed the advice of the Senate.</p>



<p>Finally, the Senate was also the effective final authority on Roman religion.  Quite a few students tend to assume this authority must rest with Rome’s most senior priest, the Pontifex Maximus, but it does not.  Instead, Rome’s senior priests, like the College of Pontiffs advised the Senate on religious matters, but it was the Senate which decided.  Thus it was the Senate which instituted the <em>Ludi Apollinares</em> (Livy  25.12), it was the Senate that directed that the worship of Cybele be brought to Italy as the Magna Mater (Livy 29.10), the Senate which ordered the suppression and later regulations (Livy 39.14 and <a href="https://en.wikipedia.org/wiki/Senatus_consultum_de_Bacchanalibus" data-type="link" data-id="https://en.wikipedia.org/wiki/Senatus_consultum_de_Bacchanalibus">we have the text of the <em>senatus consultum</em></a>) of the Bacchanalia.  </p>



<figure><img data-attachment-id="21047" data-permalink="https://acoup.blog/2023/09/22/collections-how-to-roman-republic-part-iv-the-senate/senatus_consultum_de_bacchanalibus/" data-orig-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?fit=559%2C524&amp;ssl=1" data-orig-size="559,524" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Senatus_consultum_de_bacchanalibus" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?fit=300%2C281&amp;ssl=1" data-large-file="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?fit=559%2C524&amp;ssl=1" decoding="async" loading="lazy" width="559" height="524" src="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?resize=559%2C524&amp;ssl=1" alt="" srcset="https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?w=559&amp;ssl=1 559w, https://i0.wp.com/acoup.blog/wp-content/uploads/2023/09/Senatus_consultum_de_bacchanalibus.jpg?resize=300%2C281&amp;ssl=1 300w" sizes="(max-width: 559px) 100vw, 559px" data-recalc-dims="1"><figcaption>Via <a href="https://en.wikipedia.org/wiki/Senatus_consultum_de_Bacchanalibus" data-type="link" data-id="https://en.wikipedia.org/wiki/Senatus_consultum_de_Bacchanalibus">Wikipedia</a>, a reproduction of the <em>senatus consultum</em> <em>de Bacchanalibus</em>, which regulated the the Bacchanalia in Italy, firmly asserting senatorial authority over it.</figcaption></figure>



<h2>Auctoritas Senatus</h2>



<p>All of these prerogatives are based on the Senate’s informal authority, its <em>auctoritas</em>, rather than any formal powers of compulsion (which it lacks).  The extent of that <em>auctoritas</em> and the degree to which it was clearly felt by most Romans to be necessary to respect, can come as a bit of a surprise.  But it is worth remembering the sort of society the Romans have and also the kind of organization the Senate is.</p>



<p>On the one hand, Roman culture is one where deference to age and authority is expected and normalized, with the Romans being quite comfortable with open hierarchies among citizens.  These sorts of cultural values were mot than just a dry sense that one needed to be polite to the elderly; the Romans <em>felt</em> the demands of honor and deference quite keenly.<span id="easy-footnote-6-20956"></span><span><a href="#easy-footnote-bottom-6-20956" title="For a sense of how keenly, see C. Barton, <em>Roman Honor: the fire in the bones</em> (2001)."><sup>6</sup></a></span>  The Senate was in turn a collection of the 300 very highest status individuals, the men with the <em>most</em> <em>auctoritas</em> in a society that did not have many alternative structures of power or influence.  Modern politicians have to compete with journalists, pundits, business moguls and media stars for influence; but to a substantial degree the nature of Roman society meant that nearly all of the most influential men in this period were in the Senate.  These were the wealthiest men in Rome and also the most politically accomplished men in Rome and also the most visible and notable men in Rome.  When they spoke collectively through a <em>senatus consultum</em>, the <em>auctoritas</em> of the thing was immense.</p>



<p>At the same time, the Senate’s <em>auctoritas</em> also came from the accomplishments and in a sense the expertise of its members.  Because it was a body of ex-magistrates, pretty much every Roman with senior leadership experience in either civil or military affairs was a member.  It was at once a council of Rome’s most experienced political leaders and its most experienced generals, which also included its most experienced priests.  And as the only <em>deliberative</em> part of the Roman political process, it provided those figures the opportunity to debate and discuss; it is thus perhaps unsurprising that it usually took quite a lot for the Roman People to come to the decision that the Senate was making a poor choice.</p>



<p>Finally, the <em>auctoritas</em> of the Senate clearly rested substantially on its history, something that it seems to me the senators themselves did not fully understand.  In the early third century, the Senate – in as much as we can see with our often less-than-ideal sources – both managed the closing stages of the Struggle of the Orders and the successful completion of the last two wars to dominate Italy (the Third Samnite War, 298-290 and the Pyrrhic War, 280-275) against formidable opponents.  Then under the leadership of the Senate, Rome emerged victorious from a long and grueling first war against Carthage (264-241).  It seems little surprise then that, when deciding how to manage the governance of their new overseas provinces (Sicily, then Corsica et Sardinia) the Romans leaned on the Senate.  When the Second Punic War (218-201) came around, the Senate assumed a decisive role in strategic direction, especially after the initial setbacks.</p>



<p>In short, the Senate’s <em>auctoritas</em> was also substantially rooted in the perception that its advice was generally wise, that it had been generally successful, particularly through the third century.  That in turn explains substantially just how <em>dominant</em> the Senate is through much of the second century – its <em>auctoritas</em>, when we come to see it clearly – was at a high ebb.</p>



<p>But of course that <em>auctoritas</em> was to a substantial degree dependent on future performance.  We see pretty clearly that faith in the Senate wanes over the back half of the second century and its <em>auctoritas</em> with it.  Setbacks in the Third Punic War (149-146) and the Numantine War (143-133) lead the assemblies to demand unconventional generals – in both cases, electing the legally-ineligable-at-the-time Scipio Aemilianus to the task, over the objections of the Senate.  And the Senate’s stubborn failure to address perceived economic problems and the boiling crisis over the status of the <em>socii</em> also seems to have degraded its <em>auctoritas</em>.</p>



<p>The great advantage of government by <em>auctoritas</em> rather than laws was that the Senate’s authority could flow wherever it was needed, enabling the Roman system to adapt to the shift from being one city in Italy to an Italian empire to a Mediterranean Empire smoothly.  But such a government was also necessarily fragile in ways I am not sure the Romans ever quite realized, because if that <em>auctoritas</em> were squandered, there was no legal, compulsive authority for the Senate, which had become the central organ of Roman governance, to fall back on.  Sulla’s effort to restore the power of the Senate by radically expanding it seems, here, particularly misguided, as if adding 300 more mediocrities could restore the respect of an institution after Marius and Sulla himself had gotten done killing nearly all of the men of experience, capability and consequence in the body.</p>



<p>But all of this is getting rather too close to a narrative of the collapse of the Republic, which is a formidable topic in its own right.  Next time, we’ll close out the main trunk of this series with a look at Rome’s courts (but expect <em>addenda</em> also covering provincial government and Rome’s Italian alliance system).</p>
<ol><li><span id="easy-footnote-bottom-1-20956"></span>I stress this point because this is a common mistake: assuming that the <em>Pontifex Maximus</em> as Rome’s highest priest was in some way the ‘boss’ of all of Rome’s other priests.  He was not; he was the presiding officer of the college of Pontiffs and the manager of the calendar (this was a very significant role), but the <em>Pontifex Maximus</em> was not the head of some priestly heirarchy and his power over the other <em>pontifices</em> was limited.  Moreover his power over other religious officials (the <em>augures</em>, <em>haruspices</em>, the <em>quindecimviri sacris faciundis</em> and so on) was very limited.  Instead, these figures report to the Senate, though the Senate will generally defer to the judgement of the <em>pontifices</em>.<a href="#easy-footnote-1-20956"></a></li><li><span id="easy-footnote-bottom-2-20956"></span>With sitting tribunes able to attend meetings of the Senate, but not being granted lifelong membership.<a href="#easy-footnote-2-20956"></a></li><li><span id="easy-footnote-bottom-3-20956"></span>A touch higher than the 24 years a L3 Model West life table (what we generally use to simulate Roman populations) leads us to expect, but then these are elites who are likely to be well nourished and not in hazardous occupations, so they might live a bit longer.<a href="#easy-footnote-3-20956"></a></li><li><span id="easy-footnote-bottom-4-20956"></span>Though it must be noted that Polybius is mostly looking at Roman foreign policy, where the Senate is strongest.<a href="#easy-footnote-4-20956"></a></li><li><span id="easy-footnote-bottom-5-20956"></span> I should note much of this picture is owed to Michael Taylor, with whom I am collaborating on work on this very topic; I’ll be sure to hollar if/when that work appears in print, of course.<a href="#easy-footnote-5-20956"></a></li><li><span id="easy-footnote-bottom-6-20956"></span>For a sense of how keenly, see C. Barton, <em>Roman Honor: the fire in the bones</em> (2001).<a href="#easy-footnote-6-20956"></a></li></ol>	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm all-in on server-side SQLite (2022) (182 pts)]]></title>
            <link>https://fly.io/blog/all-in-on-sqlite-litestream/</link>
            <guid>37613747</guid>
            <pubDate>Fri, 22 Sep 2023 16:02:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/all-in-on-sqlite-litestream/">https://fly.io/blog/all-in-on-sqlite-litestream/</a>, See on <a href="https://news.ycombinator.com/item?id=37613747">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
             <img src="https://fly.io/blog/all-in-on-sqlite-litestream/assets/litestream-cover.webp" alt="">
           <p>I’m Ben Johnson. I wrote BoltDB, an embedded database that is the backend for systems like etcd. Now I work at <a href="https://fly.io/">Fly.io</a>, on <a href="https://litestream.io/">Litestream</a>. Litestream is an open-source project that makes SQLite tenable for full-stack applications through the power of ✨replication✨. If you can set up a SQLite database, <a href="https://fly.io/docs/speedrun/">you can get Litestream working in less than 10 minutes</a>.</p>
<p>The conventional wisdom of full-stack applications is the n-tier architecture, which is now so common that it’s easy to forget it even has a name. It’s what you’re doing when you run an “application server” like Rails, Django, or Remix alongside a “database server” like Postgres. According to the conventional wisdom, SQLite has a place in this architecture: as a place to run unit tests.</p>

<p>The conventional wisdom could use some updating. I think that for many applications –&nbsp;production applications, with large numbers of users and high availability requirements –&nbsp;SQLite has a better place, in the center of the stack, as the core of your data and persistence layer.</p>

<p>It’s a big claim. It may not hold for your application. But you should consider it, and I’m here to tell you why.</p>
<h2 id="a-brief-history-of-application-databases"><a href="#a-brief-history-of-application-databases" aria-label="Anchor"></a>A Brief History Of Application Databases</h2>
<p>50 years is not a long time. In that time, we’ve seen a staggering amount of change in how our software manages data.</p>

<p>In the beginning of our story, back in the ‘70s, there were <a href="https://www.oreilly.com/library/view/sql-in-a/9780596155322/ch01s01s01.html">Codd’s rules,</a> defining what we now call “<a href="https://en.wikipedia.org/wiki/Relational_database">relational databases</a>”, also known today as “databases”. You know them, even if you don’t: all data lives in tables; tables have columns, and rows are addressable with keys; C.R.U.D.; schemas; a textual language to convey these concepts. The language, of course, is SQL, which prompted a Cambrian explosion of SQL databases, from Oracle to DB2 to Postgres to MySQL, throughout the '80s and '90s.</p>

<p>It hasn’t all been good. The 2000s got us XML databases. But our industry atoned  by building some <a href="https://www.vertica.com/secrets-behind-verticas-performance/">great columnar databases</a> during the same time. By the 2010s, we saw dozens of large-scale, open-source distributed database projects come to market.  Now anyone can spin up a cluster and query terabytes of data.</p>

<p>As databases evolved, so too did the strategies we use to plug them in to our applications. Almost since Codd, we’ve divided those apps into tiers. First came the database tier. Later, with <a href="https://memcached.org/">memcached</a> and <a href="https://redis.io/">Redis</a>, we got the caching tier. We’ve got <a href="https://sidekiq.org/">background job tiers</a> and we’ve got <a href="https://www.pgbouncer.org/">routing tiers</a> and distribution tiers. The tutorials pretend that there are 3 tiers, but we all know it’s called “n-tier” because nobody can predict how many tiers we’re going to end up with.</p>

<p>You know where we’re going with this. Our scientists were so preoccupied with whether or not they could, and so on.</p>

<p>See, over these same five decades, we’ve also seen CPUs, memory, &amp; disks become hundreds of times faster and cheaper. A term that practically defines database innovation in the 2010s is “big data”. But hardware improvements have made that concept slippery in the 2020s. Managing a 1 GB database in 1996? A big deal. In 2022? Run it on your laptop, or a t3.micro.</p>

<p>When we think about new database architectures, we’re hypnotized by scaling limits. If it can’t handle petabytes, or at least terabytes, it’s not in the conversation. But most applications will never see a terabyte of data, even if they’re successful. We’re using jackhammers to drive finish nails.</p>
<h2 id="the-sweet-release-of-sqlite"><a href="#the-sweet-release-of-sqlite" aria-label="Anchor"></a>The Sweet Release of SQLite</h2>
<p>There’s a database that bucks a lot of these trends. It’s one of the most popular SQL databases in the world, so standardized it’s an <a href="https://www.sqlite.org/locrsf.html">official archival format of the Library of Congress</a>, it’s renowned for its reliability and its <a href="https://www.sqlite.org/testing.html">unfathomably encompassing test suite</a>, and its performance is so good that citing its metrics on a message board invariably starts an argument about whether it should be disqualified. I probably don’t have to name it for you, but, for the one person in the back with their hand raised, I’m talking about <a href="https://www.sqlite.org/">SQLite</a>.</p>

<p>SQLite is an embedded database. It doesn’t live in a conventional architectural tier; it’s just a library, linked into your application server’s process. It’s the standard bearer of the “<a href="https://crawshaw.io/blog/one-process-programming-notes">single process application</a>”: the server that runs on its own, without relying on nine other sidecar servers to function.</p>

<p>I got interested in these kinds of applications because I build databases. I wrote <a href="https://github.com/boltdb/bolt">BoltDB</a>, which is a popular embedded K/V store in the Go ecosystem. BoltDB is reliable and, as you’d expect from an in-process database, it performs like a nitro-burning funny car. But BoltDB has limitations: its schema is defined in Go code, and so it’s hard to migrate databases. You have to build your own tooling for it; there isn’t even a REPL.</p>

<p>If you’re careful, using this kind of database can get you a lot of performance. But for general-purpose use, you don’t want to run your database off the open headers like a funny car. I thought about the kind of work I’d have to do to make BoltDB viable for more applications, and the conclusion I quickly reached was: that’s what SQLite is for.</p>

<p>SQLite, as you are no doubt already typing into the message board comment, is not without its own limitations. The biggest of them is that a single-process application has a single point of failure: if you lose the server, you’ve lost the database. That’s not a flaw in SQLite; it’s just inherent to the design.</p>
<h2 id="enter-litestream"><a href="#enter-litestream" aria-label="Anchor"></a>Enter Litestream</h2>
<p>There are two big reasons everyone doesn’t default to SQLite. The first is resilience to storage failures, and the second is concurrency at scale. Litestream has something to say about both concerns.</p>

<p>How Litestream works is that it takes control of SQLite’s <a href="https://sqlite.org/wal.html">WAL-mode journaling</a>. In WAL mode, write operations append to a log file stored alongside SQLite’s main database file. Readers check both the WAL file and the main database to satisfy queries. Normally, SQLite automatically checkpoints pages from the WAL back to the main database. Litestream steps in the middle of this: we open an indefinite read transaction that prevents automatic checkpoints. We then capture WAL updates ourselves, replicate them, and trigger the checkpointing ourselves.</p>
<div><p>The most important thing you should understand about Litestream is that it’s just SQLite. Your application uses standard SQLite, with whatever your standard SQLite libraries are. We’re not parsing your queries or proxying your transactions, or even adding a new library dependency. We’re just taking advantage of the journaling and concurrency features SQLite already has, in a tool that runs alongside your application. For the most part, your code can be oblivious to Litestream’s existence.</p>

<p>Or, think of it this way: you can build a Remix application backed by Litestream-replicated SQLite, and, while it’s running, crack open the database using the standard <code>sqlite3</code> REPL and make some changes. It’ll just work.</p>

<p>You can read more about <a href="https://litestream.io/how-it-works/">how this works here</a>.</p>
</div>
<p>It sounds complicated, but it’s incredibly simple in practice, and <a href="https://litestream.io/getting-started/">if you play with it</a> you’ll see that it “just works”. You run the Litestream binary on the server your database lives on in “replicate” mode:</p>
<div>
  <pre><code>litestream replicate fruits.db s3://my-bukkit:9000/fruits.db
</code></pre>
</div>

<p>And then you can “restore” it to another location:</p>
<div>
  <pre><code>litestream restore -o fruits-replica.db s3://my-bukkit:9000/fruits.db
</code></pre>
</div>

<p>Now commit a change to your database; if you restore again then you’ll see the change on your new copy.</p>
<p>We’ll replicate almost anywhere: to S3, or Minio; to Azure, or Backblaze B2, or Digital Ocean or Google Cloud, or an SFTP server.</p>
<p>The ordinary way people use Litestream today is to replicate their SQLite database to S3 (it’s remarkably cheap for most SQLite databases to live-replicate to S3). That, by itself, is a huge operational win: your database is as resilient as you ask it to be, and easily moved, migrated, or mucked with.</p>

<p>But you can do more than that with Litestream. The upcoming release of Litestream will let you live-replicate SQLite directly between databases, which means you can set up a write-leader database with distributed read replicas. Read replicas can <a href="https://fly.io/blog/globally-distributed-postgres/">catch writes and redirect them to the leader</a>; most applications are read-heavy, and this setup gives those applications a globally scalable database.</p>
<figure>
  <figcaption>
    <h2>Litestream SQLite, Postgres, CockroachDB, or any other database</h2>
    <p>They all work on Fly.io; we do built-in persistent storage and private networking for painless clustering, so it’s easy to try new stuff out.</p>
    <a href="https://fly.io/docs/speedrun/">
      Try Fly  <span>→</span>
    </a>
  </figcaption>
  <p><img src="https://fly.io/static/images/cta-rabbit.webp" srcset="https://fly.io/static/images/cta-rabbit@2x.webp 2x" alt="">
  </p>
</figure>

<h2 id="you-should-take-this-option-more-seriously"><a href="#you-should-take-this-option-more-seriously" aria-label="Anchor"></a>You Should Take This Option More Seriously</h2>
<p>One of my first jobs in tech in the early 2000s was as an Oracle Database Administrator (DBA) for an Oracle9i database. I remember spending hours poring over books and documentation to learn the ins  and  outs of the Oracle database. And there were a lot. The <a href="https://docs.oracle.com/cd/A91034_01/DOC/server.901/a90117.pdf">administration guide</a> was almost a thousand pages—and that was just one of over <a href="https://docs.oracle.com/cd/A91034_01/DOC/nav/docindex.htm">a hundred documentation guides</a>.</p>

<p>Learning what knobs to turn to optimize queries or to improve writes could make a big difference back then. We had disk drives that could only read tens of megabytes per second so utilizing a better index could change a 5-minute query into a 30 second query.</p>

<p>But database optimization has become less important for typical applications. If you have a 1 GB database, an NVMe disk can slurp the whole thing into memory in under a second. As much as I love tuning SQL queries, it’s becoming a dying art for most application developers. Even poorly tuned queries can execute in under a second for ordinary databases.</p>

<p>Modern Postgres is a miracle. I’ve learned a ton by reading its code over the years. It includes a slew of features like a genetic query optimizer, row-level security policies, and a half dozen different types of indexes. If you need those features, you need them. But most of you probably don’t.</p>

<p>And if you don’t need the Postgres features, they’re a liability. For example, even if you don’t use multiple user accounts, you’ll still need to configure and debug host-based authentication. You have to firewall off your Postgres server. And more features mean more documentation, which makes it difficult to understand the software you’re running. The documentation for Postgres 14 is nearly <a href="https://www.postgresql.org/files/documentation/pdf/14/postgresql-14-US.pdf">3,000 pages</a>.</p>

<p>SQLite has a subset of the Postgres feature set. But that subset is 99.9% of what I typically need. Great SQL support, <a href="https://www.sqlite.org/windowfunctions.html">windowing</a>, <a href="https://www.sqlite.org/lang_with.html">CTEs</a>, <a href="https://www.sqlite.org/fts5.html">full-text search</a>, <a href="https://www.sqlite.org/json1.html">JSON</a>. And when it lacks a feature, the data is already next to my application. So there’s little overhead to pull it in and process it in my code.</p>

<p>Meanwhile, the complicated problems I really need to solve aren’t really addressed by core database functions. Instead, I want to optimize for just two things: latency &amp; developer experience.</p>

<p>So one reason to take SQLite seriously is that it’s operationally much simpler. You spend your time writing application code, not designing intricate database tiers. But then there’s the other problem.</p>
<h2 id="the-light-is-too-damn-slow"><a href="#the-light-is-too-damn-slow" aria-label="Anchor"></a>The light is too damn slow</h2>
<p>We’re beginning to hit theoretical limits. In a vacuum, light travels about 186 miles in 1 millisecond. That’s the distance from Philadelphia to New York City and back. Add in layers of network switches, firewalls, and application protocols and the latency increases further.</p>

<p>The per-query latency overhead for a Postgres query within a single AWS region can be up to a millisecond. That’s not Postgres being slow—it’s you hitting the limits of how fast data can travel. Now, handle an HTTP request in a modern application. A dozen database queries and you’ve burned over 10ms before business logic or rendering.</p>

<p>There’s a magic number for application latency: <strong>responses in 100ms or less feel instantaneous</strong>. Snappy applications make happy users. 100ms seems like a lot, but it’s easy to carelessly chew it up. The 100ms threshold is so important that people  <a href="https://jamstack.org/">pre-render their pages and post them on CDNs</a> just to reduce latency.</p>

<p>We’d rather just move our data close to our application.  How much closer? Really close.</p>

<p>SQLite isn’t just on the same machine as your application, but actually built into your application process. When you put your data right next to your application, you can see per-query latency drop to 10-20 microseconds. That’s micro, with a μ. A 50-100x improvement over an intra-region Postgres query.</p>

<p>But wait, there’s more. We’ve effectively eliminated per-query latency. Our application is fast, but it’s also simpler. We can break up larger queries into many smaller, more manageable queries, and spend the time we’ve been using to hunt down corner-casey N+1 patterns building new features.</p>

<p>Minimizing latency isn’t just for production either. Running integration tests with a traditional client/server database easily grows to take minutes locally and the pain continues once you push to CI. Reducing the feedback loop from code change to test completion doesn’t just save time but also preserves our focus while developing. A one-line change to SQLite will let you run it in-memory so you can run integration tests in seconds or less.</p>
<h2 id="small-fast-reliable-globally-distributed-choose-any-four"><a href="#small-fast-reliable-globally-distributed-choose-any-four" aria-label="Anchor"></a>Small, Fast, Reliable, Globally Distributed: Choose Any Four</h2>
<p>Litestream is distributed and replicated and, most importantly, still easy to get your head around. Seriously, <a href="https://litestream.io/getting-started/">go try it</a>. There’s just not much to know.</p>

<p>My claim is this: by building reliable, easy-to-use replication for SQLite, we make it attractive for all kinds of full-stack applications to run entirely on SQLite. It was reasonable to overlook this option 170 years ago, when <a href="https://guides.rubyonrails.org/getting_started.html">the Rails Blog Tutorial</a> was first written. But SQLite today can keep up with the write load of most applications, and replicas can scale reads out to as many instances as you choose to load-balance across.</p>

<p>Litestream has limitations. I built it for single-node applications, so it won’t work well on ephemeral, serverless platforms or when using rolling deployments. It needs to restore all changes sequentially which can make database restores take minutes to complete. We’re <a href="https://github.com/benbjohnson/litestream/issues/8">rolling out live replication</a>, but the separate-process model restricts us to course-grained control over replication guarantees.</p>

<p>We can do better. For the past year, what I’ve been doing is nailing down the core of Litestream and keeping a focus on correctness. I’m happy with where we’ve landed. It started as a simple, streaming back up tool but it’s slowly evolving into a reliable, distributed database. Now it’s time to make it faster and more seamless, which is my whole job at Fly.io. There are improvements coming to Litestream — improvements that aren’t at all tied to Fly.io! — that I’m psyched to share.</p>

<p>Litestream has a new home at Fly.io, but it is and always will be an open-source project. My plan for the next several years is to keep making it more useful, no matter where your application runs, and see just how far we can take the SQLite model of how databases can work.</p>

           
         </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Paisa – Open-Source Personal Finance Manager (254 pts)]]></title>
            <link>https://paisa.fyi/</link>
            <guid>37613054</guid>
            <pubDate>Fri, 22 Sep 2023 15:08:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paisa.fyi/">https://paisa.fyi/</a>, See on <a href="https://news.ycombinator.com/item?id=37613054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>2022/01/01</span> <span>Salary</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span>    Income:Salary:Acme</span><span>     </span><span>-100,000</span><span> </span><span>INR</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span>    Assets:Checking</span><span>         </span><span>100,000</span><span> </span><span>INR</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span>2022/01/03</span> <span>Rent</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span>    Assets:Checking</span><span>         </span><span>-20,000</span><span> </span><span>INR</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span>    Expenses:Rent</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span>2022/01/07</span> <span>Investment</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span>    Assets:Checking</span><span>         </span><span>-20,000</span><span> </span><span>INR</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span>    Assets:Equity:NIFTY</span><span>   </span><span>168.690</span><span> </span><span>NIFTY</span><span> </span><span>@</span><span> </span><span>118.56</span><span> </span><span>INR</span>
</span></code></pre></div>
<div><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span>2022/01/01</span> <span>Salary</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span>    Income:Salary:Acme</span><span>      </span><span>$</span><span>-5,000</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span>    Assets:Checking</span><span>          </span><span>$</span><span>5,000</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span>2022/01/03</span> <span>Rent</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span>    Assets:Checking</span><span>         </span><span>$</span><span>-2,000</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span>    Expenses:Rent</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span>2022/01/07</span> <span>Investment</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span>    Assets:Checking</span><span>         </span><span>$</span><span>-1,000</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span>    Assets:Equity:AAPL</span><span>   </span><span>6.452</span><span> </span><span>AAPL</span><span> </span><span>@</span><span> </span><span>$</span><span>154.97</span>
</span></code></pre></div>
<div><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span>2022/01/01</span> <span>Salary</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span>    Income:Salary:Acme</span><span>      </span><span>-5,000</span><span>€</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span>    Assets:Checking</span><span>          </span><span>5,000</span><span>€</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span>2022/01/03</span> <span>Rent</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span>    Assets:Checking</span><span>         </span><span>-2,000</span><span>€</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span>    Expenses:Rent</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span>2022/01/07</span> <span>Investment</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span>    Assets:Checking</span><span>         </span><span>-1,000</span><span>€</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span>    Assets:Equity:AAPL</span><span>   </span><span>6.452</span><span> </span><span>AAPL</span><span> </span><span>@</span><span> </span><span>154.97</span><span>€</span>
</span></code></pre></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java 21: The Nice, the Meh, and the Momentous (178 pts)]]></title>
            <link>https://horstmann.com/unblog/2023-09-19/index.html</link>
            <guid>37612975</guid>
            <pubDate>Fri, 22 Sep 2023 15:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://horstmann.com/unblog/2023-09-19/index.html">https://horstmann.com/unblog/2023-09-19/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37612975">Hacker News</a></p>
<div id="readability-page-1" class="page"><h2>Java 21: The Nice, The Meh, and the ... Momentous</h2>
    <blockquote><p>When Java 17 was released in 2021 as a “long term support” version, I wrote <a href="https://horstmann.com/unblog/2021-09-14/index.html">an article</a> dissecting its features and came to the conclusion that it had a few nice features, but none that were compelling reasons to upgrade. Except one: tens of thousands of bug fixes. </p>
      <p>Java 21 was released today, as another “long term support” release. How does it rate on the momentousness scale? Read on for an unbiased opinion.</p>
    </blockquote>
    <p><img src="https://horstmann.com/unblog/2023-09-19/gbu.jpeg" alt=".jpeg" loading="lazy"></p>

    <h2>The Momentousness Ratings</h2>

    <p>Every six months, there is a new Java release. Ever so often (currently, every two years), Oracle labels a release as “long term support”, and Java users wonder whether they should upgrade. In theory, <a href="https://foojay.io/almanac/jdk-21/">other JDK distributors</a> could offer “long term support” for other releases, but it seems everyone is following Oracle's lead.</p>
    <p>Should you upgrade?</p>
    <p>Here are the major features of Java 21. I omit preview and incubator features (which you are surely not going to use in production), JVM internals, highly specialized features such as <a href="https://openjdk.org/jeps/452">this one</a>, and deprecations.</p>
    
    <table><tbody><tr><th>Feature</th>
        <th>Example</th>
        <th>Momentousness rating</th>
        <th>Why care?</th>
      </tr>
      <tr><td>Pattern matching for <code>switch</code></td>
        <td>
        <pre>Employee e = . . .;
String description = switch (e) {
   case Executive exec when exec.getTitle().length() &gt;= 20 -&gt;
      "An executive with an impressive title";
   case Executive __ -&gt; "An executive";
   case Manager m -&gt; {
      m.setBonus(10000);
      yield "A manager who just got a bonus";
   }
   default -&gt; "A lowly employee with a salary of " + e.getSalary();
};
</pre></td>
        <td>Nice</td>
        <td>It's better than chains of <code>if/else/else</code> with <code>instanceof</code>. Do you do that often? The JDK source has over 5 million LOC with about a thousand <code>instanceof</code> preceded by <code>else</code>.</td>
      </tr>
      <tr><td>Record Patterns</td>
        <td>
        <pre>String description = switch (p)
   {
      case Point(var x, var y) when x == 0 &amp;&amp; y == 0 -&gt; "origin";
      case Point(var x, var __) when x == 0 -&gt; "on x-axis";
      case Point(var __, var y) when y == 0 -&gt; "on y-axis";
      default -&gt; "not on either axis";
   };
</pre></td>
        <td>Nice</td>
        <td>How many records are in your codebase? (The Java 21 API has <a href="https://docs.oracle.com/en/java/javase/21/docs/api/allclasses-index.html">two</a>.)</td>
      </tr>
      <tr><td>Sequenced Collections</td>
        <td>
        <pre>List&lt;String&gt; words = ...;
String lastWord = words.getLast();
for (String word : words.reversed()) System.out.println(word);
</pre></td>
        <td>Nice</td>
        <td>Good to have, but you wouldn't upgrade for that.</td>
      </tr>
      <tr><td>Virtual threads</td>
        <td>
        <pre>try {
   var response = client.send(request, HttpResponse.BodyHandlers.ofString()); 
   for (URL url : getImageURLs(response.body())) {
      saveImage(getImage(url));
   }
}
catch (...) { ... }
</pre></td>
        <td>Momentous</td>
        <td>No more async gobbledygook!
        <pre>client.sendAsync(request, HttpResponse.BodyHandlers.ofString())
   .thenApply(HttpResponse::body)
   .thenApply(this::getImageURLs)
   .thenCompose(this::getImages)
   .thenAccept(this::saveImages)
   .exceptionally(this::ohNoes);
</pre></td>
      </tr>
      <tr><td><a href="https://javaalmanac.io/jdk/21/apidiff/17/">Miscellaneous new methods</a></td>
        <td><pre>"Hello, World!".<a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html#splitWithDelimiters(java.lang.String,int)">splitWithDelimiters</a>
      ("\\pP\\s*", -1)
  // ["Hello", ", ", "World", "!", ""]
</pre></td>
        <td>Meh</td>
        <td>Good that the API keeps evolving in small ways, but the changes are pretty minor.</td>
      </tr>
      <tr><td><a href="https://bugs.openjdk.org/browse/JDK-8309268?jql=project%20%3D%20JDK%20AND%20statusCategory%20%3D%20Done%20%20AND%20fixVersion%20%3E%3D%2018%20AND%20fixVersion%20%3C%3D%2021%20ORDER%20BY%20updated%20DESC">Over 10,000 bug fixes</a></td>
          <td><a href="https://bugs.openjdk.org/browse/JDK-8054022">Bug JDK-8054022</a> HttpURLConnection timeouts with Expect: 100-Continue and no chunking</td>
          <td>Count me in!</td>
          <td>Unless you are sure that none of them might impact you, shouldn't you upgrade?</td>
      </tr>
    </tbody></table>

    <p>Let's look at these features in more detail.</p>

    <h2>Virtual Threads</h2>
    <p><a href="https://horstmann.com/unblog/2023-06-27/index.html">Virtual threads</a> are a big deal. Similar to generics, lambda expressions, and modules, they solve a major problem for which the language has otherwise no good alternative. If you have the problem that they are designed to solve, you will have a powerful motivation to upgrade.</p>
    <p>Here is the problem. If you write applications that process many more concurrent requests than available platform threads, you currently have two unappealing choices:</p>
    <ul><li>Use a synchronous programming style and accept that throughput is limited by the number of platform threads</li>
      <li>Use an asynchronous or “reactive” programming style</li>
    </ul>
    <p>What is wrong with an asynchronous programming style? You have to structure your program as chunks of callbacks. You need library support for sequencing, branches, loops, and exception handling, instead of using the features that are built into Java. Debugging is more challenging since the debugger cannot show you a complete execution history when it stops at a breakpoint. Not convinced? Make one of your junior programmers read through the documentation of <a href="https://projectreactor.io/docs/core/release/reference">Project Reactor</a> and then assign a simple task, such as loading a web page and then loading all images in it. </p>
    <p>Of course, virtual threads are not appropriate for all concurrent programming. They only work for tasks that spend most of their time waiting for network I/O. This is the situation in many business applications where much of the request processing consists of calls to the database and external services.</p>
    <p>Interestingly, there is <a href="https://horstmann.com/unblog/2023-06-27/index.html">very little to learn</a> in order to use virtual threads. You just use them like regular threads. In most scenarios, you simply configure your application framework to invoke your business logic on virtual threads, and watch throughput increase.</p>
    <p>One idiom is worth learning. To run multiple tasks in parallel, use a <em>local</em> instance of <code>ExecutorService</code>:</p>
    <pre>try (var service = Executors.newVirtualThreadPerTaskExecutor()) {
   Future&lt;T1&gt; f1 = service.submit(callable1);
   Future&lt;T2&gt; f2 = service.submit(callable2);
   result = combine(f1.get(), f2.get());
}
</pre>
    <p>Obtaining the result with <code>get</code> is a blocking call, but so what, blocking is cheap with virtual threads.</p>
    <p><a href="https://openjdk.org/jeps/453">Structured Concurrency</a>, a preview feature in Java 21, simplifies error handling and makes it easier to harvest the results of multiple concurrent requests.</p>
    <p>There are a few caveats:</p>
    <ul><li>In the past, a thread pool didn't just throttle the incoming requests but also the concurrent resources that your app consumed. If you now accept many more incoming requests, you may need other ways to manage resource consumption.</li>
      <li>One resource that deserves particular attention is thread locals. With many more threads than before, do you really want many more thread locals? Or are there more appropriate mechanisms to achieve whatever you wanted to achieve with thread locals? Your framework provider needs to think this through, and if you actively use thread locals, so should you. A <a href="https://openjdk.org/jeps/446">lighter-weight alternative</a> is in preview.</li>
      <li>Virtual threads do not yet work well with blocking calls inside <code>synchronized</code> methods or blocks. The remedy is to rewrite the offending code with <code>java.util.concurrent</code> locks. Be sure that the providers of your framework, database driver, and so on, update their code to work well with virtual threads. Quite a few already did.</li>
    </ul>
 

    <h2>Pattern Matching</h2>
    <p>Many functional languages have some form of pattern matching that makes it convenient to work with “algebraic data types”, which in Java are implemented with sealed hierarchies and record classes.</p>
    <p>Java has chosen to extend the syntax for <code>instanceof</code> and <code>switch</code> for pattern matching, in order to leverage existing programmer knowledge. These extensions have been in preview until Java 20 and are now in their final form.</p>
    <p>Are you using sealed hierarchies and records in your code base? Then pattern matching is appealing. Here is an example, a simple JSON hierarchy:</p>
    <pre>sealed interface JSONValue permits JSONArray, JSONObject, JSONPrimitive {}

final class JSONArray extends ArrayList&lt;JSONValue&gt; implements JSONValue {}

final class JSONObject extends HashMap&lt;String, JSONValue&gt; implements JSONValue {}

sealed interface JSONPrimitive extends JSONValue
   permits JSONNumber, JSONString, JSONBoolean, JSONNull {}

final record JSONNumber(double value) implements JSONPrimitive {}

final record JSONString(String value) implements JSONPrimitive {}

enum JSONBoolean implements JSONPrimitive {
   FALSE, TRUE;
}

enum JSONNull implements JSONPrimitive {
   INSTANCE;
}

</pre>
    <p><img src="https://horstmann.com/unblog/2023-09-19/json.png" alt=".png" loading="lazy"></p>
    <p>Now you can process JSON values like this:</p>

<pre>JSONPrimitive p = . . .;
double value = switch (p) {
   case JSONString(var v) when v.matches("-?(0|[1-9]\\d*)(\\.\\d+)?([eE][+-]?\\d+)?") -&gt;
      Double.parseDouble(v);
   case JSONString __ -&gt; Double.NaN;
   case JSONNumber(var v) -&gt; v;
   case JSONBoolean.TRUE -&gt; 1;
   case JSONBoolean.FALSE, JSONNull.INSTANCE -&gt; 0;
}
</pre>
    <p>Note the following:</p>
    <ul><li>This is a <code>switch</code> expression that yields a value</li>
      <li>The compiler checks that the <code>switch</code> is exhaustive</li>
      <li>The pattern <code>JSONString(var v)</code> binds the variable <code>v</code> to the component of the record</li>
      <li>The <code>when</code> clause restricts a match to a Boolean condition</li>
      <li>With <a href="https://openjdk.org/jeps/445">JEP 445</a>, you will be able to use <code>case JSONString _</code>, with a single underscore, to indicate that you do not need the variable binding. But that is still a preview feature.</li>
      <li>Since Java 14, you can have multiple constants in a single <code>case</code></li>
    </ul>
    <p>All this is certainly nicer than the <code>instanceof</code> and casting that one might do right now with Jackson. But you might want to hold off switching to a new JSON hierarchy until Java gives us <a href="https://openjdk.org/jeps/8277163">value classes</a>.</p>
    <p>In general, pattern matching is more useful in contexts that are designed for pattern matching. Today's use cases are perhaps not all that compelling, but it is an investment in the future.</p>
    <h2>Sequenced Collections</h2>
    <p>When you have a <code>Collection</code>, how do you get the first element? With a <code>List</code>, it's <code>list.get(0)</code>, but in general, you'd call <code>collection.iterator().next()</code>. Except with a stack or queue it is <code>peek</code>, with a deque <code>getFirst</code>, and the <code>SortedSet</code> interface has <code>first</code>. And what about the last element?</p>
    <p>And how do you visit the elements in reverse order? <code>Deque</code> and <code>NavigableSet</code> have a handy <code>descendingIterator</code>. For lists, you iterate backwards, starting from the last element.</p>
    <p><a href="https://openjdk.org/jeps/431">JEP 431</a> cleans up this situation with a <code>SequencedCollection</code> interface. It has these methods:</p>
    <pre>E getFirst();
E getLast();
void addFirst(E);
void addLast(E);
E removeFirst();
E removeLast();
SequencedCollection&lt;E&gt; reversed();
</pre>
    <p>The first six methods are the same as in the <code>Deque</code> interface, which is now a subinterface.</p>
    <p>There is also a <code>SequencedSet</code>, where <code>reversed</code> yiels a set, and a <code>SequencedMap</code>, with methods to get and put the first and last entry, and with sequenced views for the keys, values, and entries.</p>
    <p>This figure, by Stuart Marks, shows the change in the collections hierarchy.</p>
    <p><img src="https://cr.openjdk.org/~smarks/collections/SequencedCollectionDiagram20220216.png" alt=".png" loading="lazy"></p>
    <p>TL;DR Reverse iteration over a list, deque, tree set, or tree map is now more uniform. Getting the first and laste element too. That's nice. Obviously not momentous.</p>

    <h2>Should You Upgrade?</h2>
    <p>When Java 17 was released, I opined that none of its features were momentous enough to warrant upgrading, and one was downright ugly. Still, upgrading was a no-brainer: tens of thousands of bug fixes. </p>
    <p>Of course you should upgrade again to Java 21. Because, lots of bug fixes.</p>
    <p>And this time there is a truly momentous feature: virtual threads. If you are contemplating the use of reactive programming, or you are already unhappily doing so, you definitely want to check them out.</p>

    <h2>Also Nice</h2>
    <p>Oracle now has an <a href="https://dev.java/playground/">online “playground”</a> for testing Java snippets. Check it out!</p>
    

    
    
    <!-- You can specify a per page discussion id on the next line, if your URLs might change. -->
    
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working Remotely Can More Than Halve an Office Employee’s Carbon Footprint (240 pts)]]></title>
            <link>https://www.scientificamerican.com/article/working-remotely-can-more-than-halve-an-office-employees-carbon-footprint/</link>
            <guid>37612968</guid>
            <pubDate>Fri, 22 Sep 2023 15:02:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/working-remotely-can-more-than-halve-an-office-employees-carbon-footprint/">https://www.scientificamerican.com/article/working-remotely-can-more-than-halve-an-office-employees-carbon-footprint/</a>, See on <a href="https://news.ycombinator.com/item?id=37612968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>By not going into the office, an at-home worker&nbsp;can cut greenhouse emissions in excess of 50 percent if they take energy-conservation steps</p></div><section data-behavior="article_progress"><figure aria-label="media" itemscope="" itemid="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=590&amp;h=800&amp;8528EB6F-5BF3-4267-BA75B863731DF855" itemprop="image" itemtype="http://schema.org/ImageObject" id="image-1"><div><picture><source media="(min-width: 768px) and (max-width: 1023px)" srcset="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=690&amp;h=930&amp;8528EB6F-5BF3-4267-BA75B863731DF855"><source media="(max-width: 767px)" srcset="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=390&amp;h=520&amp;8528EB6F-5BF3-4267-BA75B863731DF855"><source media="(min-width: 768px) and (max-width: 1023px)" srcset="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=690&amp;h=930&amp;8528EB6F-5BF3-4267-BA75B863731DF855"><source media="(max-width: 767px)" srcset="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=390&amp;h=520&amp;8528EB6F-5BF3-4267-BA75B863731DF855"><img importance="high" src="https://static.scientificamerican.com/sciam/cache/file/D6A3B954-3B65-4F57-B24B52167B111D97_source.jpg?w=590&amp;h=800&amp;8528EB6F-5BF3-4267-BA75B863731DF855" width="590" height="800" alt="Working Remotely Can More Than Halve an Office Employee's Carbon Footprint" itemprop="url"></picture></div><figcaption itemprop="caption description"> Credit: <a href="https://www.gettyimages.com/detail/photo/businesswoman-using-mobile-phone-in-front-of-laptop-royalty-free-image/1325244318?adppopup=true" target="_blank">10,000 Hours/Getty Images</a></figcaption></figure><div data-behavior="newsletter_promo dfp_article_rendering" data-dfp-adword="Advertisement" data-newsletterpromo_article-text="<p>Sign up for <em>Scientific American</em>&amp;rsquo;s free newsletters.</p>" data-newsletterpromo_article-image="https://static.scientificamerican.com/sciam/cache/file/4641809D-B8F1-41A3-9E5A87C21ADB2FD8_source.png" data-newsletterpromo_article-button-text="Sign Up" data-newsletterpromo_article-button-link="https://www.scientificamerican.com/page/newsletter-sign-up/?origincode=2018_sciam_ArticlePromo_NewsletterSignUp" name="articleBody" itemprop="articleBody"><p>At the height of the COVID pandemic, an estimated <a href="https://www.bls.gov/opub/mlr/2022/article/telework-during-the-covid-19-pandemic.htm">50 percent of all Americans</a> began working remotely. Since then many workers have returned to the office—but around 20 percent have <a href="https://www.scientificamerican.com/article/were-fumbling-the-return-to-physical-offices/">continued to work from home</a> at least part-time.</p>

<p>The benefits of remote work have become a hotly debated topic. Proponents argue that working from home is better for both workers’ health and the planet. And intuitively, it makes sense that cutting out a daily office commute would save a substantial amount of greenhouse gas emissions. Yet there have been few in-depth studies into how sustainable remote work actually is.</p>

<p>A new analysis examines the sustainability question and provides a comprehensive insight into the climate mitigation potential of remote work in the U.S. By looking at five factors, including commuting, noncommute travel, information technology devices, office energy efficiency and residential energy use, researchers were able to calculate how much carbon the average American office employee saves by working from home. The team found that remote work has the potential to reduce an individual’s carbon emissions by more than half—but only if they take the necessary measures at home. The <a href="https://www.pnas.org/doi/full/10.1073/pnas.2304099120">results were published</a> on September 18 in the <em>Proceedings of the National Academy of Sciences USA</em>.</p>

<p>Calculating the carbon cost of remote versus in-office work was a challenge. “This problem is fairly complicated,” says Fengqi You, a systems engineer at Cornell University and co-author of the paper. “The systems involved are complex.”</p>

<p>You and his team were able to obtain a massive anonymized dataset from Microsoft that gave them an unprecedented window into remote workers’ day-to-day energy use and lifestyle. By comparing these data with recorded greenhouse gas emissions from in-person office work, they calculated the actual carbon reduction potential of working from home. Some of the results, You says, were “surprising.”</p>

<p>For example, many previous analyses of remote work assumed that cutting out an office commute meant that workers wouldn’t drive during the day. But You and his team found that this isn’t the case. In fact, remote workers often drive more often than their in-office counterparts by taking several short car trips throughout the day.</p>

<p>The researchers also found that working from home can prompt people to use more energy over the course of a workday on things such as air-conditioning and a dishwasher. And remote workers are more likely to move out of big, centralized cities, where lifestyles are generally less carbon-intensive than in suburban or rural areas.</p>

<p>All of these observations have big policy implications. Recently some politicians have championed working from home as a major climate solution and have credited it with a 95 percent reduction in emissions. Unfortunately, “that’s not true,” You says.</p>

<p>Brian Caulfield, a civil and structural engineer at Trinity College Dublin, agrees. “It doesn’t stand up to scrutiny,” says Caulfield, who was not involved in the study.</p>

<p>This doesn’t mean that working from home cannot lower emissions substantially, however. Biking to a nearby coffee shop with your laptop, for example, is an extremely carbon-efficient way to work. The study found that people who work remotely four or more days a week can reduce their carbon footprint by up to 54 percent, and those who do so up to four days a week can reduce it by up to 29. But these reductions only hold if workers implement strategies such as turning off unnecessary lights and appliances, driving an electric vehicle or sourcing their home electricity from solar panels or wind turbines.</p>

<p>“It’s not all about how many days you work from home,” says Yanqiu Tao, a sustainability engineer at Cornell and first author of the paper. “It’s about how well you live sustainably.”</p>

<p>The study’s authors also point out that office buildings can be made greener. If older buildings were revamped with more energy-efficient appliances and put on a decarbonized grid, then in-office work could match the greenhouse gas emissions of working from home. Taking public transit can also contribute significantly to reducing an individual’s carbon footprint, even if they are working from an office.</p>

<p>Although the paper’s results were specific to the U.S., Caulfield believes that the same basic principles should hold for other industrialized countries. “The kind of patterns we see across the world are very similar,” he says, including in his home city of Dublin.</p>

<p>The biggest takeaway, the authors say, is that <a href="https://www.scientificamerican.com/article/covid-changed-the-world-of-work-forever/">remote work is here to stay</a>, and it can absolutely be part of a greener, more sustainable future—but it shouldn’t be seen as a panacea.</p>

<p>“The pandemic has really motivated us to think about [remote work] in a broader and more complex way, as a society,” You says. “So we really need to understand what we are putting into practice.”</p></div><section><h3>ABOUT THE AUTHOR(S)</h3><div><ul></ul><p><strong>Joanna Thompson</strong> is an insect enthusiast and former <em>Scientific American</em> intern. She is based in New York City. Follow Thompson on Twitter <a href="https://twitter.com/jojofoshosho0">@jojofoshosho0</a></p></div></section></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Actions could be so much better (241 pts)]]></title>
            <link>https://blog.yossarian.net/2023/09/22/GitHub-Actions-could-be-so-much-better</link>
            <guid>37612420</guid>
            <pubDate>Fri, 22 Sep 2023 14:21:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yossarian.net/2023/09/22/GitHub-Actions-could-be-so-much-better">https://blog.yossarian.net/2023/09/22/GitHub-Actions-could-be-so-much-better</a>, See on <a href="https://news.ycombinator.com/item?id=37612420">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>ENOSUCHBLOG</h2>
<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/series">Series</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
      <li><a href="https://yossarian.net/">Main Site</a></li>
    
</ul>

<hr>



<h2>
  <p>
    <span><em>Sep 22, 2023</em></span>

    &nbsp; &nbsp;

    
      <span>
        Tags:
        
        
          <a href="https://blog.yossarian.net/tags#programming">programming</a>,
        
          <a href="https://blog.yossarian.net/tags#rant">rant</a>,
        
          <a href="https://blog.yossarian.net/tags#workflow">workflow</a>
        
      </span>
    

    &nbsp; &nbsp;

    
  </p>
</h2>






<hr>


<p>I <em>love</em> GitHub Actions: I’ve been a daily user of it since 2019 for both professional
and hobbyist projects, and have found it invaluable to both my overall productivity
and peace of mind. I’m <em>just</em> old enough to have used <a href="https://www.travis-ci.com/">Travis CI</a>
et al. professionally before moving to GitHub Actions, and I do not look back with joy<sup id="fnref:back" role="doc-noteref"><a href="#fn:back" rel="footnote">1</a></sup>.</p>

<p>By and large, GitHub Actions continues to delight me and grow new features that I
appreciate: <a href="https://github.blog/2021-11-29-github-actions-reusable-workflows-is-generally-available/">reusable workflows</a>, <a href="https://github.blog/changelog/2021-10-27-github-actions-secure-cloud-deployments-with-openid-connect/">OpenID connect</a>, <a href="https://github.blog/changelog/2022-05-09-github-actions-enhance-your-actions-with-job-summaries/">job summaries</a>, <a href="https://github.blog/changelog/2023-05-09-introducing-actions-on-the-repository-view-on-github-mobile/">integrations into GitHub Mobile</a>,
and so forth.</p>

<p>At the same time, GitHub Actions is a regular source of <em>profound</em> frustration and time loss<sup id="fnref:loss" role="doc-noteref"><a href="#fn:loss" rel="footnote">2</a></sup>
in my development processes. This post lists some of those frustrations, and how I think GitHub
could selfishly<sup id="fnref:selfishly" role="doc-noteref"><a href="#fn:selfishly" rel="footnote">3</a></sup> improve on them (or even fix them outright)<sup id="fnref:roadmap" role="doc-noteref"><a href="#fn:roadmap" rel="footnote">4</a></sup>.</p>

<hr>

<h2 id="debugging-like-im-15-again">Debugging like I’m 15 again</h2>

<p>Here’s a pretty typical session of me trying to set up a release workflow on GitHub Actions:</p>

<p><img src="https://blog.yossarian.net/assets/github-actions-fails.png" alt=""></p>

<p>In this particular case, it took me 4 separate commits (and 4 failed releases) to debug
the various small errors I made: not using <code>${{ ... }}</code><sup id="fnref:jekyll" role="doc-noteref"><a href="#fn:jekyll" rel="footnote">5</a></sup> where I needed to, forgetting
a <code>needs:</code> relationship, &amp;c.</p>

<p>Here’s another (this time of a PR-creating workflow), from a few weeks later:</p>

<p><img src="https://blog.yossarian.net/assets/github-actions-fails-2.png" alt=""></p>

<p>I am not the world’s most incredible programmer; like many (most?), I program intuitively
and follow the error messages until they stop happening.</p>

<p>GitHub Actions is <strong>not</strong> responsible for catching every possible error I could make,
and ensuring that every workflow I write will run successfully on the first try.</p>

<p>At the same time, the current debugging cycle in GitHub Actions is <em>ridiculous</em>:
even the smallest change on the most trivial workflow is a 30+ second process
of tabbing out of my development environment (context switch #1), digging through
my browser for the right tab (context switch #2), clicking through the infernal
nest of actions summaries, statuses, &amp;c. (context switch #3), and impatiently
refreshing a buffered console log to figure out which error I need to fix next
(context switch #4). Rinse and repeat.</p>

<h3 id="fixing-this">Fixing this</h3>

<ul>
  <li>
    <p>Give us an interactive debugging shell, or (at least) let us re-run workflows
with small changes <em>without</em> having to go through a <code>git add; git commit; git push</code> cycle<sup id="fnref:breaks" role="doc-noteref"><a href="#fn:breaks" rel="footnote">6</a></sup>.</p>
  </li>
  <li>
    <p>Give us a repository setting to reject commits with obviously invalid workflows (things
like syntax that can’t possibly work, or references to jobs/steps that don’t exist).
It’s <em>infuriating</em> when I <code>git push</code> a workflow that silently fails because of invalid YAML;
<em>especially</em> when I then merge that workflow’s branch under the mistaken impression
that the workflow is <em>passing</em>, rather than not running at all.</p>
  </li>
</ul>

<h2 id="security-woes">Security woes</h2>

<p>Speaking from experience: it’s <em>shockingly</em> easy to wreck yourself with GitHub Actions. <em>Way</em> easier
than it should be.</p>

<p>Here is just a small handful of the ways in which I have <em>personally</em> written potentially vulnerable
workflows over the past few years:</p>

<ol>
  <li>
    <p>Using the <code>${{ ... }}</code> expansion syntax in a shell or other context where a
(potentially malicious) user controls the expansion’s contents. The following, for example, would
allow a user to inject code that could then exfiltrate <code>$MY_IMPORTANT_SECRET</code>:</p>

    <div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
</pre></td><td><pre><span>-</span> <span>name</span><span>:</span> <span>do something serious</span>
  <span>run</span><span>:</span> <span>|</span>
   <span>something-serious "${{ inputs.frob }}"</span>
  <span>env</span><span>:</span>
    <span>MY_IMPORTANT_SECRET</span><span>:</span> <span>${{ secrets.MY_IMPORTANT_SECRET }}</span>
</pre></td></tr></tbody></table></code></pre></div>

    <p>Some among you will observe that a ✨good✨ programmer would simply know
not to do this, and that a bad programmer would eventually learn their
(painful) lesson. This might be an acceptable position for a niche
piece of software to hold; it is <strong>not</strong> an acceptable position
for the CI/CD platform that, to a first approximation, hosts the entire
open source ecosystem.</p>
  </li>
  <li>
    <p>Using <code>pull_request_target</code>. As far as I can tell, it’s <em>practically</em>
impossible to use this event safely in a non-trivial workflow<sup id="fnref:trivial" role="doc-noteref"><a href="#fn:trivial" rel="footnote">7</a></sup>.</p>

    <p>This event appears to exist for an <em>extremely</em> narrow intended use case, i.e.
labeling or commenting on PRs that come from forks. I don’t understand
why GitHub Actions chooses to expose such a (relatively) simple operation
through as massive of a foot-gun as <code>pull_request_target</code>.</p>
  </li>
  <li>
    <p>Over-scoping my workflow and job-level permissions.</p>

    <p>The default access set for Actions’ ordinary <code>GITHUB_TOKEN</code> is
<a href="https://docs.github.com/en/actions/security-guides/automatic-token-authentication#permissions-for-the-github_token"><em>very</em> permissive</a>:
the only thing it <em>doesn’t</em> provide access to are the workflow’s OpenID Connect token.</p>

    <p>This consistently bites me in two different ways:</p>

    <ol>
      <li>I consistently forget to down-scope the default token, especially when
working with repositories under my personal account (rather than under an org,
where the default scope can be reduced across all repositories).</li>
      <li>
        <p>I consistently <em>over-scope</em> my tokens because I don’t know exactly
how much access my workflow will need.</p>

        <p>This is further complicated by the messy ways in which GitHub’s permission
model gets shoehorned into a single permissions dimension of <code>read/write/none</code>:
why does <code>id-token: write</code> grant me the ability to <strong>read</strong> the workflow’s OpenID Connect
token? Why do
<a href="https://docs.github.com/en/rest/overview/permissions-required-for-github-apps?apiVersion=2022-11-28#repository-permissions-for-repository-security-advisories">some <code>GET</code> operations</a>
on security advisories require <code>write</code>, while others only require <code>read</code>?</p>
      </li>
    </ol>
  </li>
</ol>

<p>There are also a few things that I <em>haven’t</em> done<sup id="fnref:donetome" role="doc-noteref"><a href="#fn:donetome" rel="footnote">8</a></sup>, but are scary enough that I think they’re worth
mentioning.</p>

<p>For example, can you see what’s wrong with this workflow step?</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
</pre></td><td><pre><span>steps</span><span>:</span>
  <span>-</span> <span>uses</span><span>:</span> <span>actions/checkout@c7d749a2d57b4b375d1ebcd17cfbfb60c676f18e</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>Despite all appearances, SHA ref
<a href="https://github.com/actions/checkout/commit/c7d749a2d57b4b375d1ebcd17cfbfb60c676f18e"><code>c7d749a2d57b4b375d1ebcd17cfbfb60c676f18e</code></a>
is <strong>not</strong> a commit on the <code>actions/checkout</code> repository! It’s actually a commit on a fork in
<code>actions/checkout</code>’s network which, thanks to GitHub’s use of
<a href="https://github.blog/2015-09-22-counting-objects/#your-very-own-fork-of-rails">alternates</a>,
<em>appears</em> to belong to the parent repository.</p>

<p><a href="https://www.chainguard.dev/unchained/what-the-fork-imposter-commits-in-github-actions-and-ci-cd">Chainguard has an excellent post on this</a><sup id="fnref:stole" role="doc-noteref"><a href="#fn:stole" rel="footnote">9</a></sup>,
but to summarize:</p>

<ol>
  <li>SHA references from forks are visually indistinguishable from SHA references
in the intended target repository. The only way to tell the two apart is
to manually inspect each reference and confirm that it appears on the expected
repository, and not one of its forks.</li>
  <li>GitHub’s own REST API makes no distinction between SHA references in a repository
graph — <code>/repos/{user}/{repo}/commits/{ref}</code> returns a JSON response that <em>only</em> references
<code>{user}/{repo}</code>, even if <code>{ref}</code> is only on a fork.</li>
  <li>Because GitHub fails to distinguish between fork and non-fork SHA references, forks
can bypass security settings on GitHub Actions that would otherwise restrict
actions to only “trusted” sources (such as GitHub themselves or the repository’s
own organization).</li>
</ol>

<p>GitHub’s response to this (so far) has been to add
<a href="https://docs.github.com/en/actions/learn-github-actions/finding-and-customizing-actions#using-shas">a little bit of additional language</a>
to their documentation, rather than to forbid misleading SHA references outright.</p>

<h3 id="fixing-this-1">Fixing this</h3>

<ul>
  <li>
    <p>Give us push-time rejection of obviously insecure workflows. In other words:
let us toggle<sup id="fnref:default" role="doc-noteref"><a href="#fn:default" rel="footnote">10</a></sup> a “paranoid workflow security” mode that, when enabled,
causes <code>git push</code> to fail with an explanation of what I’m doing wrong. Essentially
the same thing as the debugging request above, but for security!</p>
  </li>
  <li>
    <p>Give us runtime checks on our workflows, analogous to runtime instrumentation like
<a href="https://clang.llvm.org/docs/AddressSanitizer.html">AddressSanitizer</a>
in the world of compiled languages. There are <em>so many</em> things that could
be turned into hard failures for security wins without breaking 99.9% of legitimate
users, like failing any attempt to use <code>actions/checkout</code> on a <code>pull_request_target</code>
with a ref that isn’t from the targeted repository.</p>
  </li>
  <li>
    <p>Maybe just deprecate and remove <code>pull_request_target</code> entirely.
<a href="https://securitylab.github.com/research/github-actions-preventing-pwn-requests/">GitHub’s own Security Lab</a>
has been aware of how dangerous this event is for years; maybe it’s time to get rid of it
entirely.</p>
  </li>
  <li>
    <p>Allow us to set a more restrictive default token scope on our personal repositories,
similar to how organizations and enterprises can restrict their default
<code>GITHUB_TOKEN</code> scopes across all repositories at once.</p>
  </li>
  <li>
    <p>By default, reject any SHA-pinned action for which the SHA only appears
on a fork and not the referenced repository. It’s hard to imagine a
<em>legitimate</em> reason to ever need to do this!</p>
  </li>
</ul>

<h2 id="real-types-would-be-nice">Real types would be nice</h2>

<p>When writing a custom GitHub Action, you can specify the actions inputs
using a mapping under the <code>inputs:</code> key. For example, the following
defines a <code>frobulation-level</code> input with a description (used for tooltips
in many IDEs) and a default value:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
</pre></td><td><pre><span>inputs</span><span>:</span>
  <span>frobulation-level</span><span>:</span>
    <span>description</span><span>:</span> <span>"</span><span>the</span><span> </span><span>level</span><span> </span><span>to</span><span> </span><span>frobulate</span><span> </span><span>at"</span>
    <span>default</span><span>:</span> <span>"</span><span>1"</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>Notably, this syntax does <strong>not</strong> allow for type enforcement; the following
<strong>does not work</strong>:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
</pre></td><td><pre><span>inputs</span><span>:</span>
  <span>frobulation-level</span><span>:</span>
    <span>description</span><span>:</span> <span>"</span><span>the</span><span> </span><span>level</span><span> </span><span>to</span><span> </span><span>frobulate</span><span> </span><span>to"</span>
    <span>default</span><span>:</span> <span>1</span>
    <span># NOTE: this SHOULD cause a workflow failure if the input</span>
    <span># isn't a valid number, but doesn't</span>
    <span>type</span><span>:</span> <span>number</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>This absence is strange, but what makes it <em>bizarre</em> is that GitHub is <strong>inconsistent</strong>
about where types can appear in actions and workflows:</p>

<ul>
  <li><code>workflow_call</code> supports <code>type</code> with <code>boolean</code>, <code>number</code>, or <code>string</code></li>
  <li><code>workflow_dispatch</code> supports <code>type</code> with <code>boolean</code>, <code>choice</code>, <code>number</code>, or <code>string</code></li>
  <li>Action inputs: no types at all</li>
</ul>

<p>Unfortunately, this is only the first level: even inputs that <em>do</em> support
typing doesn’t support compounded data structures, like lists or objects.
For example, neither of the following works:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
</pre></td><td><pre><span>-</span> <span>uses</span><span>:</span> <span>example/example</span>
  <span>with</span><span>:</span>
    <span># INVALID: can't use arrays as inputs</span>
    <span>paths</span><span>:</span> <span>[</span><span>foo</span><span>,</span> <span>bar</span><span>,</span> <span>baz</span><span>]</span>
    <span># INVALID: can't use objects as inputs</span>
    <span>headers</span><span>:</span>
      <span>foo</span><span>:</span> <span>bar</span>
      <span>baz</span><span>:</span> <span>quux</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>…which means that action writers end up requiring users to do silly things like these:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
</pre></td><td><pre><span>-</span> <span>uses</span><span>:</span> <span>example/example</span>
  <span>with</span><span>:</span>
    <span># SILLY: action does ad-hoc CSV-ish parsing</span>
    <span>paths</span><span>:</span> <span>foo,bar,baz</span>
    <span># SILLY: action forcefully flattens a natural hierarchy</span>
    <span>header-foo</span><span>:</span> <span>bar</span>
    <span>header-baz</span><span>:</span> <span>quux</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>This is bad for maintainability, and bad for security: maintainability
because actions must carefully manage a single flat namespace of inputs
(with no types!), and security because both action writer and workflow writer
are forced into <a href="https://langsec.org/occupy/">ad-hoc, unspecified languages</a>
for complex inputs.</p>

<h3 id="fixing-this-2">Fixing this</h3>

<ul>
  <li>
    <p>Let action and workflow writers use <code>type:</code> everywhere, and let
us use <code>choice</code> everywhere — not just in <code>workflow_dispatch</code>!</p>
  </li>
  <li>
    <p>Give us stricter type-checking. Where action and workflow types
can be inferred statically, detect errors and reject incorrectly typed
workflow changes at <code>push</code> time, rather than waiting for the workflow
to inevitably fail.</p>
  </li>
  <li>
    <p>Give us <code>type: object</code> and <code>type: array</code> types. These won’t be perfect
to start with (thanks to potentially heterogeneous interior types),
but they’ll be a significant improvement over the status quo. Implementation-wise,
forward these as JSON-serialized strings or something similar<sup id="fnref:json" role="doc-noteref"><a href="#fn:json" rel="footnote">11</a></sup> where
appropriate (such as in auto-created <code>INPUT_{WHATEVER}</code> environment variables).</p>
  </li>
</ul>

<h2 id="more-official-actions-would-be-nice">(More) official actions would be nice</h2>

<p>The third-party ecosystem on GitHub Actions is great: there are a <em>lot</em>
of high-quality, easy-to-use actions being maintained by open source contributors.
I maintain a <a href="https://github.com/pypa/gh-action-pip-audit">handful</a>
<a href="https://github.com/sigstore/gh-action-sigstore-python">of them</a>!</p>

<p>Beneath the surface of these excellent third-party actions is a substrate
of <em>official</em>, GitHub-maintained actions. These actions primarily address
three classes of fundamental CI/CD activities:</p>

<ol>
  <li>Core <code>git</code> operations: <code>actions/checkout</code></li>
  <li>Core GitHub operations and repository housekeeping: <code>actions/{upload,download}-artifact</code>,
<code>actions/cache</code>, <code>actions/stale</code></li>
  <li>General (but essential) configuration: <code>actions/setup-python</code>, <code>actions/setup-node</code></li>
</ol>

<p>These classes are somewhat distinct from “higher-level” workflows (like the kind
I write): because of their centrality and universal demand, they benefit from
singular, high-quality, <em>officially maintained</em> implementations.</p>

<p>And so, the question: <strong><em>why are there so few of them</em></strong>?</p>

<p>Here is just a smattering of the official actions that <em>don’t</em> exist:</p>

<ol>
  <li><em>Programmatically adding a pull request to a merge queue</em>. GitHub <em>has</em> the machinery to
support this: <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/merging-a-pull-request-with-a-merge-queue?tool=cli"><code>gh pr merge</code> already exists</a>.
It just isn’t exposed as an action; users are (presumably) expected
to piece it together themselves.</li>
</ol>

<p>Even worse, there are actions that <em>did</em> exist but were deprecated (generally
for unclear reasons<sup id="fnref:time" role="doc-noteref"><a href="#fn:time" rel="footnote">12</a></sup>):</p>

<ol>
  <li><a href="https://github.com/actions/create-release"><code>actions/create-release</code></a>:
<a href="https://github.com/actions/create-release/issues/119">unmaintained as of March 2021</a>. Users
encouraged to switch to various community maintained workflows, most notably<sup id="fnref:notably" role="doc-noteref"><a href="#fn:notably" rel="footnote">13</a></sup>
<a href="https://github.com/softprops/action-gh-release"><code>softprops/action-gh-release</code></a>.</li>
  <li><a href="https://github.com/actions/upload-release-asset"><code>actions/upload-release-asset</code></a>: marked
as unmaintained at the same time as <code>actions/create-release</code>.</li>
  <li><a href="https://github.com/actions/setup-ruby"><code>actions/setup-ruby</code></a>:
<a href="https://github.com/actions/setup-ruby/issues/97">unmaintained as of February 2021</a>. Users
encouraged to switch to <a href="https://github.com/ruby/setup-ruby"><code>ruby/setup-ruby</code></a>.</li>
</ol>

<p>I’m sympathetic to the individual maintainers here and, in each case, the transition
to a “recommended” third-party action was relatively painless.</p>

<p>Still, the overall impression given here is unmistakable: that GitHub does not see <em>official</em>
actions for its own platform features (or key ecosystem users, like Ruby) as priorities,
and would rather have the community develop and choose unofficial favorites. This is
<strong>not unreasonable</strong> on a strategic level (it induces third-party development
in their ecosystem), but has a <em>deleterious effect</em> on trust in the platform. I’d like
to be able to write workflows and know that they’ll run (with minimal changes) 5 years from
now, and not worry that GitHub has abandoned core pieces underneath me!</p>

<p>Apart from imparting a general feeling of shabbiness, this compounds with GitHub Action’s
poor security story (<a href="#some-security-would-be-nice">per above</a>): not providing official high-quality actions for their own
API surfaces means that users will <em>continue</em> to make exploitable security mistakes in
their workflows. Nobody wins<sup id="fnref:pentesting" role="doc-noteref"><a href="#fn:pentesting" rel="footnote">14</a></sup>.</p>

<h3 id="fixing-this-3">Fixing this</h3>

<ul>
  <li>
    <p>Give us more official actions. As a <em>very</em> rough rule of thumb: if a thing
directly ties different pieces of GitHub infrastructure together <em>and</em> currently
needs to be done manually (with REST API calls, <code>gh</code> invocations, or whatever else),
it probably deserves a full official action!</p>
  </li>
  <li>
    <p>Give us more <em>pseudo-official</em> actions. Work with the biggest third-party actions<sup id="fnref:biggest" role="doc-noteref"><a href="#fn:biggest" rel="footnote">15</a></sup>
to form a <code>community-actions</code> (or whatever) org, with the expectation that actions homed under
that org have been reviewed (at some point) by GitHub, are forced to adhere to best practices
for repository security, receive semantically versioned updates, &amp;c &amp;c.</p>
  </li>
</ul>

<h2 id="wrap-up">Wrap-up</h2>

<p>This is a long and meandering post, and many parts are in conflict: security and stability
(in the form of more official actions that break less often), for example, are in eternal
conflict with each other.</p>

<p>I’m just one user, and I don’t expect my interests or frustrations to be overriding ones.
Still, I hope that the problems (and potential fixes) above aren’t unique to me, and that there are
engineers at GitHub who (again, selfishly!) share these concerns and would like to see
them fixed.</p>

<hr>




<hr>




  






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve is a wonderful upstream contributor to Linux and the open-source community (776 pts)]]></title>
            <link>https://www.phoronix.com/news/Valve-Upstream-Everything-OSS</link>
            <guid>37612127</guid>
            <pubDate>Fri, 22 Sep 2023 13:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Valve-Upstream-Everything-OSS">https://www.phoronix.com/news/Valve-Upstream-Everything-OSS</a>, See on <a href="https://news.ycombinator.com/item?id=37612127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="VALVE" src="https://www.phoronix.com/assets/categories/valve.webp" width="100" height="100"></p><p>
This shouldn't come as any surprise to any longtime Phoronix readers and dedicated open-source/Linux enthusiasts, but Valve with their work on the Steam Deck and SteamOS have been lifting the open-source ecosystem as a whole. A talk this week at the Linux Foundation Europe's Open-Source Summit highlighted some of the great and ongoing contributions by Valve and their partners.
</p><p>
Alberto Garcia of the open-source consulting firm Igalia, which continues to collaborate with Valve on some of these Linux ecosystem improvements, talked at length around how SteamOS is contributing to the Linux ecosystem.
</p><p>
SteamOS is built atop Arch Linux with a GNU user-space and systemd, the desktop mode features KDE Plasma to which Valve has funded some improvements there, Valve's Steam Play / Proton that leverages Wine has been immensely valuable to Linux gamers and enthusiasts along with related open-source projects like DXVK / VKD3D-Proton, and then there's also they work they are doing around AMD color management / HDR. Igalia engineers have been involved with Valve on the AMD color management work as well as other areas like enabling new Linux kernel features for enabling better Steam Play support. 
</p><p>
The elephant in the room meanwhile is the countless improvements Valve engineers have made to the the Mesa OpenGL and Vulkan drivers as well as to the kernel graphics driver components. Not just to the AMD graphics drivers for benefiting the Steam Deck's hardware but also to Zink OpenGL-on-Vulkan and then other common infrastructure. But in this area of the Linux graphics driver support, Valve's contributions and those of their partners have been incredibly beneficial to the Linux desktop ecosystem even outside gaming.
</p><p><img src="https://www.phoronix.net/image.php?id=steamos-35-benchmarks&amp;image=steamos_35_4_med" alt="Steam Deck"></p>
<p>There has also been other efforts Valve has been involved in such on expanding case insensitive file-system support on Linux, various other kernel features, their Gamescope Wayland compositor, immutable software updates, and Flatpak. Igalia says it's part of Valve's policy to "upstream everything" they are working on.
</p><p>
So for those questioning Valve's contributions to the Linux and open-source ecosystems or for helping to convince any friends/colleagues about Valve's open-source software work, check out Alberto Garcia's <a href="https://osseu2023.sched.com/event/1Qv8y/how-steamos-is-contributing-to-the-linux-ecosystem-alberto-garcia-igalia?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no">OSS EU 2023 presentation</a> for the more comprehensive look at all of the great Valve / SteamOS upstream contributions.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Sites like HN on other topics? (116 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37611708</link>
            <guid>37611708</guid>
            <pubDate>Fri, 22 Sep 2023 13:22:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37611708">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37612051"><td></td></tr>
                <tr id="37613034"><td></td></tr>
            <tr id="37612784"><td></td></tr>
                <tr id="37612920"><td></td></tr>
                <tr id="37616009"><td></td></tr>
                  <tr id="37613303"><td></td></tr>
                  <tr id="37613008"><td></td></tr>
                  <tr id="37612834"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612834" href="https://news.ycombinator.com/vote?id=37612834&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>Honestly, this is the wrong question, but I understand the intent. What you really should be asking is "Where is the online community for your hobby/interest?"<p>The thing with HN, is it works for the startup/hacker/tech community, and that community has evolved here over time. If you want a link/topic sharing site for other interests, the answer is almost always going to be a subreddit.</p><p>But if what you really want is to be immersed in a community, those communities will be in weird corners that you have to discover. The reason for this is that a community almost always wants to be semi-private: discoverable by others that share the same interest, but just far enough out of the public eye to not get bombarded with bots/trolls/obnoxious people</p><p>Some examples I know of:</p><pre><code>  - Voron (3d printer)? You want to be on their discord
  - BigGreenEgg (kamado smoker)? eggheadforum.com
  - MMA? sherdog
  - Electronics/Robotics diy style hacky projects? hackaday and go to the comment secion
</code></pre>
The cool part about finding these true pockets of a community, is you can learn a ton about what truly makes them tick. What do they get excited about, what do they have an irrational hatred for, etc. If you've ever asked yourself "how do I market my app/website/saas", it's almost certainly because you haven't yet encountered where that niche community resides, and are falling back to more generic strategies. Find the community, and they'll tell you what they want in spades</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37612978"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612978" href="https://news.ycombinator.com/vote?id=37612978&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>Yes and no. Yes, you're spot on.<p>And yet I find that my favorite posts &amp; comment threads here are the ones that *aren't* about tech, startups, etc. And I'm always amazed at how no matter what the topic there's always someone who seems to know what they're talking about. Of course there are always many people who think they know what they're talking about :)</p><p>Perhaps it's less "Community about X" but rather "Community of people who are interested in X" as there's more likelihood that you'd have other shared interests? Not sure.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37612022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612022" href="https://news.ycombinator.com/vote?id=37612022&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>I've stopped following pretty much everything except HN. Which is unfortunate (though sorta nice for my mental health), but my interests right now are also narrow.<p>I'd suggest Lemmy? It still feels small, and has decent activity for self-hosting and homelab stuff after the whole reddit API debacle. That's where I plan to put more effort once I stand up my own ActivityPub server. Id like to try Mastodon again too but I bounced off last time
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37612559"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612559" href="https://news.ycombinator.com/vote?id=37612559&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>I only visit two sites without question each day: BBC News and HN. And that already occupies too much of my day.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37612196"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612196" href="https://news.ycombinator.com/vote?id=37612196&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>Tildes is pretty good. Still small and text focused.<p>I was enjoying Discuit but then they added images, and now it's just another stream of memes.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37612974"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612974" href="https://news.ycombinator.com/vote?id=37612974&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>I used to follow a lot of communities, but many of them have died out over the years for various reasons.<p>One of the things I've been trying to do recently is to get out and attend local meetups. The resolution and depth of the discussion IRL can be much deeper. And it makes me realize that most people aren't on Hacker News!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37612348"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612348" href="https://news.ycombinator.com/vote?id=37612348&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>Not exactly what you're asking for, but a few people appreciated when I shared this last time a similar post came around:<p><a href="https://brutalist.report/" rel="nofollow noreferrer">https://brutalist.report/</a></p><p>Nice conglomeration of headlines/links from lots of different sources. Lots of tech and science, but also a good mix of global news. Includes HN actually.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37614908"><td></td></tr>
                  <tr id="37612528"><td></td></tr>
            <tr id="37612715"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612715" href="https://news.ycombinator.com/vote?id=37612715&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>Twitter is actually pretty cool if you find the right people to follow in the topics you’re interested in.<p>You can also make lists of such people so they’re separate from your main feed.</p><p>It does feel harder to comment on stuff though. Not sure why?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37615454"><td></td></tr>
            <tr id="37612807"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612807" href="https://news.ycombinator.com/vote?id=37612807&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>I’ve friends say this but I haven’t figured out how to make it work. It takes so much time to sort and filter and there’s no way to get a proper “feed” without constantly scrolling through crap that’s injected.<p>I asked a friend to show me and he walked me through their workflow and it boiled down to “spend hours on twitter everyday and some decent percent is manually filtering.”</p><p>HN has an easy interface -front page. I can hide things I’ve read and I don’t see them again, or I can leave them to read later. And comments are threaded. Twitter doesn’t seem to have these things.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37613709"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37613709" href="https://news.ycombinator.com/vote?id=37613709&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>It's kind of like that one friend who will pontificate on the merits of smoked cinnamon sticks, bitters, candied pecans, and the $10,000 worth of liquor in their living room bar - they've fooled themselves into thinking they're &lt;drinking/using twitter&gt; <i>responsibly</i> and <i>nobly</i>, but it's really just a bunch of mental gymnastics to justify doing &lt;unhealthy thing which feels good&gt;.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37612820"><td></td></tr>
                  <tr id="37612602"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612602" href="https://news.ycombinator.com/vote?id=37612602&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>There are some excellent topic-specific subreddits IMO. Like:<pre><code>  - /r/ProgrammingLanguages
  - /r/Compilers
  - /r/EmuDev
  - /r/DatabaseDevelopment (disclosure, I run this one)</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37614658"><td></td></tr>
            <tr id="37612849"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612849" href="https://news.ycombinator.com/vote?id=37612849&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>Are there really any? HN has the quality it does thanks to dang's tireless efforts, and I don't know of any other site with similar moderation.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37612208"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612208" href="https://news.ycombinator.com/vote?id=37612208&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>Fishbowl if you are into consulting.<p>Kind of an old gen site and isnt without its issues, but I feel like metafilter gives me a few good links every day still so I keep it around. They are good at spotlighting stuff that is lesser known.</p><p>lowtechmagazine if you are interested in very practical sustainability. Their solar powered website (a 1-board computer on somebody's roof in barcelona) has been a recurring HN topic. <a href="https://news.ycombinator.com/item?id=20038619">https://news.ycombinator.com/item?id=20038619</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37612358"><td></td></tr>
            <tr id="37612827"><td></td></tr>
                      <tr id="37612191"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612191" href="https://news.ycombinator.com/vote?id=37612191&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>I really like Tildes<p><a href="https://tildes.net/" rel="nofollow noreferrer">https://tildes.net/</a></p><p>which is less focused,  more about everything (god I wish I could frontpage an article about sports on HN) but has a much higher ratio of discussions to links (e.g. Ask HN is a joke)</p><p>I have invites.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37612406"><td></td></tr>
            <tr id="37612558"><td></td></tr>
            <tr id="37612808"><td></td></tr>
            <tr id="37612829"><td></td></tr>
            <tr id="37612752"><td></td></tr>
                <tr id="37612793"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37612793" href="https://news.ycombinator.com/vote?id=37612793&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>Anyone who wants one should look up my profile and send me an email.  I have three right now and they will go to whoever sends me the first three emails.  I will get around to it this evening.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37612939"><td></td></tr>
                        <tr id="37612705"><td></td></tr>
            <tr id="37612349"><td></td></tr>
                  <tr id="37612951"><td></td></tr>
            <tr id="37612947"><td></td></tr>
            <tr id="37612937"><td></td></tr>
            <tr id="37613252"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37613252" href="https://news.ycombinator.com/vote?id=37613252&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>I am making digglu.com, which is supposed to be old-digg-like link aggregator site. I would have it out by now if only I could stop procrastinating T_T</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37612689"><td></td></tr>
            <tr id="37612217"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37612217" href="https://news.ycombinator.com/vote?id=37612217&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>Not identical as a "users create all content" but hackaday.com (and .io) is excellent for tech stuff. Think DIY meets crazy scientist.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37612201"><td></td></tr>
            <tr id="37613001"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37613001" href="https://news.ycombinator.com/vote?id=37613001&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>Not sure if you can really find something like HN for other topics. HN works as it works exactly because of the topic and audience. Same structure might not work for other things</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37616102"><td></td></tr>
                  <tr id="37612941"><td></td></tr>
            <tr id="37612739"><td></td></tr>
            <tr id="37612148"><td></td></tr>
                <tr id="37612411"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37612411" href="https://news.ycombinator.com/vote?id=37612411&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><br><div>
                  <p><span>I'm torn between two sides when thinking about lobste.rs
On one side they have imho high quality content. But on the other side they can keep this just because their registration is invite-only.
Which leads to fewer/smaller discussions.
On HN I don't like everything, but I read a lot more discussions even about topics which I'm not interested in.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37612899"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37612899" href="https://news.ycombinator.com/vote?id=37612899&amp;how=up&amp;goto=item%3Fid%3D37611708"></a></center>    </td><td><p><span>I liked lobste.rs but found they were too restrictive on topics.<p>I kind of like the hacker-adjacent discussions here (notebooks and journaling and reading and whatnot) and lobste.rs seems to block and remove that and there’s no outlet for that type of discussion.</p><p>I like they HN, for me, is 80% tech/startup/programming, 10% adjacent and interesting and 10% stuff I just go past.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tailscale Kubernetes Operator (140 pts)]]></title>
            <link>https://tailscale.com/kb/1236/kubernetes-operator/</link>
            <guid>37611213</guid>
            <pubDate>Fri, 22 Sep 2023 12:39:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/kb/1236/kubernetes-operator/">https://tailscale.com/kb/1236/kubernetes-operator/</a>, See on <a href="https://news.ycombinator.com/item?id=37611213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The <a href="https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/manifests/operator.yaml">Tailscale Kubernetes operator</a> allows you to:</p>
<ul>
<li>Expose <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services</a> in your
Kubernetes cluster to your Tailscale network</li>
<li>Securely connect to the <a href="https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver">Kubernetes control plane (kube-apiserver)</a> via an API server proxy, with or without authentication</li>
<li>Egress from a Kubernetes cluster to an external service on your Tailscale network</li>
</ul>


<p>
    Kubernetes operator is currently
    <a href="https://tailscale.com/kb/1167/release-stages/#alpha">in private alpha</a>. Therefore, this topic is currently hidden.
    
    To try it, follow the steps below to enable it for your network using Tailscale v1.37.40 or
    later.
  </p>

<h3 id="setting-up-the-kubernetes-operator">
  <a href="#setting-up-the-kubernetes-operator">
    Setting up the Kubernetes operator
    
  </a>
</h3>

<ol>
<li>
<p>In your <a href="https://tailscale.com/kb/1018/acls/">tailnet policy file</a>, create the <a href="https://tailscale.com/kb/1068/acl-tags/">ACL
tags</a> <code>tag:k8s-operator</code> and <code>tag:k8s</code>, and make <code>tag:k8s-operator</code>
an owner of <code>tag:k8s</code>. If you want your services to be exposed with tags
other than the default <code>tag:k8s</code>, create those as well and make
<code>tag:k8s-operator</code> an owner.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span><span>"tagOwners"</span><span>:</span> <span>{</span>
</span></span><span><span>   <span>"tag:k8s-operator"</span><span>:</span> <span>[],</span>
</span></span><span><span>   <span>"tag:k8s"</span><span>:</span> <span>[</span><span>"tag:k8s-operator"</span><span>],</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div></li>
<li>
<p><a href="https://tailscale.com/kb/1215/oauth-clients/#setting-up-an-oauth-client">Create an OAuth client</a> in the <a href="https://login.tailscale.com/admin/settings/oauth"><strong>OAuth clients</strong></a>
page of the admin console. Create the client with <code>Devices</code> write scope and
the tag <code>tag:k8s-operator</code>.</p>
</li>
<li>
<p>Download the Tailscale Kubernetes operator <a href="https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/manifests/operator.yaml">manifest file</a>
from the <a href="https://github.com/tailscale/tailscale">tailscale/tailscale</a> repo.</p>
</li>
<li>
<p>Edit your version of the manifest file:</p>
<ol>
<li>Find <code># SET CLIENT ID HERE</code> and replace it with your OAuth client ID.</li>
<li>Find <code># SET CLIENT SECRET HERE</code> and replace it with your OAuth client secret.</li>
</ol>
<p>For both the client ID and secret, quote the value, to avoid any potential yaml misinterpretation of
unquoted strings. For example, use:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>client_id</span><span>:</span><span> </span><span>"k123456CNTRL"</span><span>
</span></span></span><span><span><span></span><span>client_secret</span><span>:</span><span> </span><span>"tskey-client-k123456CNTRL-abcdef"</span><span>
</span></span></span></code></pre></div><p>instead of:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>client_id</span><span>:</span><span> </span><span>k123456CNTRL</span><span>
</span></span></span><span><span><span></span><span>client_secret</span><span>:</span><span> </span><span>tskey-client-k123456CNTRL-abcdef</span><span>
</span></span></span></code></pre></div></li>
<li>
<p>Apply the edited file to your Kubernetes cluster:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>kubectl apply -f manifest.yaml
</span></span></code></pre></div><p>This creates the “tailscale” namespace in your cluster, and deploys the
Tailscale operator within it.</p>
</li>
<li>
<p>Verify that the Tailscale operator has joined your tailnet. Open the
<a href="https://login.tailscale.com/admin/machines"><strong>Machines</strong></a> page of the admin console and look for a node
named <strong>tailscale-operator</strong>, tagged with the <code>tag:k8s-operator</code> tag. It may
take a minute or two for the operator to join your tailnet, due to the time
required to download and start the container image in Kubernetes.</p>
</li>
</ol>
<h3 id="cluster-ingress">
  <a href="#cluster-ingress">
    Exposing a service to your tailnet (cluster ingress)
    
  </a>
</h3>

<p>You can use the Tailscale Kubernetes operator to expose a Kubernetes service to your Tailscale network in three ways: by making it a <code>LoadBalancer</code> type with the <code>tailscale</code> <code>loadBalancerClass</code>, by annotating an existing service, or by creating an ingress resource fronting a service.</p>
<h4 id="exposing-a-service-using-loadbalancerclass">
  <a href="#exposing-a-service-using-loadbalancerclass">
    Exposing a service using <code>loadBalancerClass</code>
    
  </a>
</h4>

<p>Edit the service you want to expose and make it a load balancer:</p>
<ol>
<li>Set <code>spec.type</code> to <code>LoadBalancer</code>.</li>
<li>Set <code>spec.loadBalancerClass</code> to <code>tailscale</code>.</li>
</ol>
<p>Once provisioning is complete, the service’s status will show the
<a href="https://tailscale.com/kb/1081/magicdns/">fully-qualified domain name</a> of the service in your tailnet. You can view the
service’s status by running <code>kubectl get service &lt;service name&gt;</code>.</p>
<p>You should also see a new node with that name appear in the
<a href="https://login.tailscale.com/admin/machines"><strong>Machines</strong></a> page of the admin console.</p>
<h4 id="exposing-a-service-using-annotations">
  <a href="#exposing-a-service-using-annotations">
    Exposing a service using annotations
    
  </a>
</h4>

<p>If the service you want to expose already exists, you can
expose it to Tailscale using <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">object annotations</a>.</p>
<p>Edit the service and under <code>metadata.annotations</code>, add the annotation
<code>tailscale.com/expose</code> with the value <code>"true"</code>. Note that <code>"true"</code> is quoted
because annotation values are strings, and an unquoted <code>true</code> will be
incorrectly interpreted as a boolean.</p>
<p>In this mode, Kubernetes doesn’t tell you the Tailscale machine name. You can look
up the node in the <a href="https://login.tailscale.com/admin/machines"><strong>Machines</strong></a> of the admin console to learn
its machine name. By default, the machine name of an exposed service is
<code>&lt;k8s-namespace&gt;-&lt;k8s-servicename&gt;</code>.</p>
<h5 id="using-a-custom-machine-name">
  <a href="#using-a-custom-machine-name">
    Using a custom machine name
    
  </a>
</h5>

<p>If you want the service to have a machine name other than the default
<code>&lt;k8s-namespace&gt;-&lt;k8s-servicename&gt;</code>, you can provide your own machine name by
setting the <code>tailscale.com/hostname</code> annotation on the service, with your
desired machine name as the value.</p>
<p>Machine names are subject to the constraints of DNS: they can be up to 63 characters
long, must start and end with a letter, and consist of only letters, numbers,
and <code>-</code>.</p>
<h5 id="customizing-acl-tags">
  <a href="#customizing-acl-tags">
    Customizing ACL tags
    
  </a>
</h5>

<p>By default, services join your tailnet tagged with the <a href="https://tailscale.com/kb/1068/acl-tags/">ACL tag</a> <code>tag:k8s</code>. You can use a
different tag or tags by setting the <code>tailscale.com/tags</code> annotation on the
service, with a comma-separated list of the desired tags.</p>
<p>For example, setting <code>tailscale.com/tags = tag:foo,tag:bar</code> will result in the
tailnet node having the tags <code>tag:foo</code> and <code>tag:bar</code>.</p>
<p>The Tailscale operator must be a <a href="https://tailscale.com/kb/1018/acls/#tag-owners">tag owner</a> of all the specified tags: if you want
to expose a service with <code>tag:foo,tag:bar</code>, the <code>tagOwners</code> section of the
<a href="https://tailscale.com/kb/1018/acls/">tailnet policy file</a> must list <code>tag:k8s-operator</code> as one
of the owners of both <code>tag:foo</code> and <code>tag:bar</code>.</p>
<h4 id="exposing-a-service-using-ingress">
  <a href="#exposing-a-service-using-ingress">
    Exposing a service using ingress
    
  </a>
</h4>

<p>You can use the Tailscale Kubernetes operator to expose an ingress resource in your Kubernetes cluster to your tailnet.</p>
<p>Ingress resources only support TLS, and are only exposed over HTTPS. You must <a href="https://tailscale.com/kb/1153/enabling-https/">enable HTTPS</a> on your tailnet.</p>
<p>Edit the ingress resource you want to expose to use the ingress class <code>tailscale</code>:</p>
<ol>
<li>Set <code>spec.ingressClassName</code> to <code>tailscale</code>.</li>
<li>Set <code>tls.hosts</code> to the desired host name of the Tailscale node. Only the first label is used.</li>
</ol>
<p>For example, to expose an ingress resource <code>nginx</code> to your tailnet:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>networking.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Ingress</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>nginx</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>defaultBackend</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>service</span><span>:</span><span>
</span></span></span><span><span><span>      </span><span>name</span><span>:</span><span> </span><span>nginx</span><span>
</span></span></span><span><span><span>      </span><span>port</span><span>:</span><span>
</span></span></span><span><span><span>        </span><span>number</span><span>:</span><span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span><span>:</span><span> </span><span>tailscale</span><span>
</span></span></span><span><span><span>  </span><span>tls</span><span>:</span><span>
</span></span></span><span><span><span>  </span>- <span>hosts</span><span>:</span><span>
</span></span></span><span><span><span>    </span>- <span>nginx</span><span>
</span></span></span></code></pre></div><p>The backend is HTTP by default. To use HTTPS on the backend, either set the port name to <code>https</code> or the port number to <code>443</code>:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>networking.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Ingress</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>nginx</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>defaultBackend</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>service</span><span>:</span><span>
</span></span></span><span><span><span>      </span><span>name</span><span>:</span><span> </span><span>nginx</span><span>
</span></span></span><span><span><span>      </span><span>port</span><span>:</span><span>
</span></span></span><span><span><span>        </span><span>name</span><span>:</span><span> </span><span>https</span><span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span><span>:</span><span> </span><span>tailscale</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span><span>:</span><span> </span><span>v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Service</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>nginx</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>ports</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>name</span><span>:</span><span> </span><span>https</span><span>
</span></span></span><span><span><span>    </span><span>port</span><span>:</span><span> </span><span>443</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span><span>:</span><span> </span><span>443</span><span>
</span></span></span><span><span><span>  </span><span>type</span><span>:</span><span> </span><span>ClusterIP</span><span>
</span></span></span></code></pre></div><p>A single ingress resource can be used to front multiple backend services:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>networking.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Ingress</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>ingress</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span><span>:</span><span> </span><span>tailscale</span><span>
</span></span></span><span><span><span>  </span><span>rules</span><span>:</span><span>
</span></span></span><span><span><span>  </span>- <span>http</span><span>:</span><span>
</span></span></span><span><span><span>      </span><span>paths</span><span>:</span><span>
</span></span></span><span><span><span>      </span>- <span>path</span><span>:</span><span> </span><span>/</span><span>
</span></span></span><span><span><span>        </span><span>pathType</span><span>:</span><span> </span><span>Prefix</span><span>
</span></span></span><span><span><span>        </span><span>backend</span><span>:</span><span>
</span></span></span><span><span><span>          </span><span>service</span><span>:</span><span>
</span></span></span><span><span><span>            </span><span>name</span><span>:</span><span> </span><span>ui-svc</span><span>
</span></span></span><span><span><span>            </span><span>port</span><span>:</span><span>
</span></span></span><span><span><span>              </span><span>number</span><span>:</span><span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span>- <span>path</span><span>:</span><span> </span><span>/api</span><span>
</span></span></span><span><span><span>        </span><span>pathType</span><span>:</span><span> </span><span>Prefix</span><span>
</span></span></span><span><span><span>        </span><span>backend</span><span>:</span><span>
</span></span></span><span><span><span>          </span><span>service</span><span>:</span><span>
</span></span></span><span><span><span>            </span><span>name</span><span>:</span><span> </span><span>api-svc</span><span>
</span></span></span><span><span><span>            </span><span>port</span><span>:</span><span>
</span></span></span><span><span><span>              </span><span>number</span><span>:</span><span> </span><span>80</span><span>
</span></span></span></code></pre></div><h5 id="exposing-a-service-to-the-public-internet-using-ingress-and-tailscale-funnel">
  <a href="#exposing-a-service-to-the-public-internet-using-ingress-and-tailscale-funnel">
    Exposing a service to the public internet using ingress and Tailscale Funnel
    
  </a>
</h5>

<p>You can also use the Tailscale Kubernetes operator to expose an ingress resource in your Kubernetes cluster to the public internet using <a href="https://tailscale.com/kb/1223/tailscale-funnel/">Taiscale Funnel</a>. To do so, add a <code> tailscale.com/funnel: "true"</code> annotation:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>networking.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Ingress</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>funnel</span><span>
</span></span></span><span><span><span>  </span><span>annotations</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>tailscale.com/funnel</span><span>:</span><span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>defaultBackend</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>service</span><span>:</span><span>
</span></span></span><span><span><span>      </span><span>name</span><span>:</span><span> </span><span>funnel</span><span>
</span></span></span><span><span><span>      </span><span>port</span><span>:</span><span>
</span></span></span><span><span><span>        </span><span>number</span><span>:</span><span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span><span>:</span><span> </span><span>tailscale</span><span>
</span></span></span><span><span><span>  </span><span>tls</span><span>:</span><span>
</span></span></span><span><span><span>  </span>- <span>hosts</span><span>:</span><span>
</span></span></span><span><span><span>    </span>- <span>funnel</span><span>
</span></span></span></code></pre></div><h4 id="removing-a-service">
  <a href="#removing-a-service">
    Removing a service
    
  </a>
</h4>

<p>Any of the following actions remove a Kubernetes service you exposed from your tailnet:</p>
<ul>
<li>Delete the service entirely.</li>
<li>If you are using <code>type=LoadBalancer</code>, remove <code>loadBalancerClass=tailscale</code> or set <code>type</code> to <code>ClusterIP</code>.</li>
<li>If you are using the <code>tailscale.com/expose</code> annotation, remove the annotation.</li>
<li>If you are using an ingress resource, remove the ingress resource or remove the service from the ingress resource.</li>
</ul>
<p>Deleting a service’s Tailscale node
in the <a href="https://login.tailscale.com/admin/machines">admin console</a> does not clean up the Kubernetes state
associated with that service.</p>

<h3 id="accessing-the-kubernetes-control-plane-using-an-api-server-proxy">
  <a href="#accessing-the-kubernetes-control-plane-using-an-api-server-proxy">
    Accessing the Kubernetes control plane using an API server proxy
    
  </a>
</h3>

<p>You can use the Tailscale Kubernetes operator to expose and access the Kubernetes control plane (kube-apiserver) over Tailscale.</p>
<p>You can use the API server proxy with or without authentication headers. With authentication headers, when a user tries to access the Kubernetes control plane over Tailscale using an API server proxy, they will hit the kube-apiserver with the same user identity that they have in Tailscale. This is done by injecting an authentication header in the request. For example, <code>alice@example.com</code> will have the user <code>alice@example.com</code> in an API server proxy.</p>
<p>If you do not want to use Tailscale for authentication, but use an existing authentication mechanism instead, you can disable the use of authentication headers. This allows you to access the Kubernetes control plane over Tailscale, without using Tailscale for authentication.</p>
<h4 id="configuring-the-api-server-proxy">
  <a href="#configuring-the-api-server-proxy">
    Configuring the API server proxy
    
  </a>
</h4>

<p>To use a Tailscale Kubernetes API server proxy, you need to <a href="https://tailscale.com/kb/1153/enabling-https/#configure-https">enable HTTPS</a> for your tailnet.</p>
<p>To configure the API server proxy:</p>
<p>1. In your Tailscale Kubernetes operator’s <a href="https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/manifests/operator.yaml">manifest file</a>, add the following lines to the <code>env</code> section:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>name</span><span>:</span><span> </span><span>APISERVER_PROXY</span><span>
</span></span></span><span><span><span></span><span>value</span><span>:</span><span> </span><span>"true"</span><span>
</span></span></span></code></pre></div><p>The kube-apiserver is automatically discovered by the operator.</p>
<p>2. Apply the changes from the example to your operator’s manifest file.</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>rbac.authorization.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>ClusterRole</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>tailscale-auth-proxy</span><span>
</span></span></span><span><span><span></span><span>rules</span><span>:</span><span>
</span></span></span><span><span><span></span>- <span>apiGroups</span><span>:</span><span> </span><span>[</span><span>""</span><span>]</span><span>
</span></span></span><span><span><span>  </span><span>resources</span><span>:</span><span> </span><span>[</span><span>"users"</span><span>]</span><span>
</span></span></span><span><span><span>  </span><span>verbs</span><span>:</span><span> </span><span>[</span><span>"impersonate"</span><span>]</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span><span>:</span><span> </span><span>rbac.authorization.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>ClusterRoleBinding</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>tailscale-auth-proxy</span><span>
</span></span></span><span><span><span></span><span>subjects</span><span>:</span><span>
</span></span></span><span><span><span></span>- <span>kind</span><span>:</span><span> </span><span>ServiceAccount</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>operator</span><span>
</span></span></span><span><span><span>  </span><span>namespace</span><span>:</span><span> </span><span>tailscale</span><span>
</span></span></span><span><span><span></span><span>roleRef</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>kind</span><span>:</span><span> </span><span>ClusterRole</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>tailscale-auth-proxy</span><span>
</span></span></span><span><span><span>  </span><span>apiGroup</span><span>:</span><span> </span><span>rbac.authorization.k8s.io</span><span>
</span></span></span></code></pre></div><p>3. Add an <a href="https://tailscale.com/kb/1018/acls/">access rule</a> in your tailnet’s policy file to grant access to the API server proxy over Tailscale.</p>
<p>4. Run the following command to grant a user the <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles">Kubernetes cluster-admin role</a> in your cluster.</p>
<pre tabindex="0"><code>kubectl create clusterrolebinding --clusterrole cluster-admin --user alice@example alice@example
</code></pre><p>5. Use the <code>tailscale configure kubeconfig &lt;hostname-or-fqdn&gt;</code> <a href="https://tailscale.com/kb/1080/cli/#configure-alpha">CLI</a> command to configure your local <code>kubeconfig</code> file to manage how to authenticate to kubectl as the Tailscale Kubernetes API server proxy.</p>
<p>To validate that API server proxy allows you to access the kube-apiserver over Tailscale:</p>
<p>6. Run <code>kubectl config current-context</code> to verify that kubectl commands will
now use Tailscale context.</p>
<p>7. Run the <code>kubectl get pods -A</code> command to run a test and verify that you have authorization.</p>
<pre tabindex="0"><code>NAMESPACE     NAME                               READY   STATUS    RESTARTS      AGE
kube-system   cilium-6b2x8                       1/1     Running   0             28d
kube-system   cilium-operator-759999b555-qbsrk   1/1     Running   3 (21d ago)   29d
kube-system   coredns-7697897646-4vh2l           1/1     Running   0             29d
kube-system   coredns-7697897646-rshwm           1/1     Running   0             29d
kube-system   cpc-bridge-proxy-xksns             1/1     Running   0             29d
kube-system   csi-do-node-k5snn                  2/2     Running   0             29d
kube-system   do-node-agent-n8nrr                1/1     Running   0             29d
kube-system   konnectivity-agent-k846g           1/1     Running   0             29d
kube-system   kube-proxy-lgzr9                   1/1     Running   0             29d
tailscale     operator-6b94c54478-n6tmc          1/1     Running   0             14d
</code></pre><h5 id="enabling-tagged-nodes-to-authenticate-using-the-api-server-proxy">
  <a href="#enabling-tagged-nodes-to-authenticate-using-the-api-server-proxy">
    Enabling tagged nodes to authenticate using the API server proxy
    
  </a>
</h5>

<p>The API server proxy allows users to use their Tailscale identities to authenticate to the Kubernetes control plane.</p>
<p>Tagged nodes authenticate as the node name instead of the user who created the node.</p>

<p>To enable tagged nodes to authenticate to the Kubernetes control plane, create a Kubernetes RoleBinding for a group, and give the group cluster access.</p>
<p>For example, create a RoleBinding for the group <code>tag:ci</code>:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>subjects</span><span>:</span><span>
</span></span></span><span><span><span></span>- <span>kind</span><span>:</span><span> </span><span>Group</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>"tag:ci"</span><span>
</span></span></span><span><span><span>  </span><span>apiGroup</span><span>:</span><span> </span><span>rbac.authorization.k8s.io</span><span>
</span></span></span></code></pre></div><p>Then, grant the group <code>tag:ci</code> a <code>ClusterRole</code> using a Kubernetes ClusterRoleBinding:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>kubectl create clusterrolebinding --clusterrole cluster-admin --group <span>"tag:ci"</span> tag-ci
</span></span></code></pre></div>

<p>To use the API server proxy without authentication headers, in the <code>env</code> section of your Kubernetes operator.yml file, set the value <code>"noauth"</code> for the <code>APISERVER_PROXY</code>:</p>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>   </span><span>name</span><span>:</span><span> </span><span>APISERVER_PROXY</span><span>
</span></span></span><span><span><span>   </span><span>value</span><span>:</span><span> </span><span>"noauth"</span><span> </span><span># instead of true</span><span>
</span></span></span></code></pre></div><h3 id="cluster-egress">
  <a href="#cluster-egress">
    Exposing a service to your cluster (cluster egress)
    
  </a>
</h3>

<p>You can use the Tailscale Kubernetes operator to advertise a service external to your cluster which is on your Tailscale network.</p>
<p>This is done by deploying a proxy in the cluster, setting the service’s <code>spec.externalName</code> to point to the proxy, and setting iptables rules for the proxy to direct incoming traffic to the DNS entry for the tailnet service. When a cluster workload attempts to reach the service, it is first directed to the proxy, which then redirects the traffic to the external service.</p>
<h4 id="exposing-a-tailnet-service-using-annotations">
  <a href="#exposing-a-tailnet-service-using-annotations">
    Exposing a tailnet service using annotations
    
  </a>
</h4>

<p>You can expose a tailnet service to your cluster workloads using annotations and an <a href="https://kubernetes.io/docs/concepts/services-networking/service/#externalname">external name</a>. You can currently only expose tailnet services that use HTTP.</p>
<p>To expose a tailnet service to your cluster workloads:</p>
<ol>
<li>Create a Kubernetes Service of type <a href="https://kubernetes.io/docs/concepts/services-networking/service/#externalname">ExternalName</a> annotated with the Tailscale IP address of the service you want to make available:</li>
</ol>
<div><pre tabindex="0"><code data-lang="yaml"><span><span><span>apiVersion</span><span>:</span><span> </span><span>v1</span><span>
</span></span></span><span><span><span></span><span>kind</span><span>:</span><span> </span><span>Service</span><span>
</span></span></span><span><span><span></span><span>metadata</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>annotations</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>tailscale.com/tailnet-ip</span><span>:</span><span> </span><span>100.68.29.93</span><span> </span><span>// Tailscale IP address</span><span>
</span></span></span><span><span><span>  </span><span>name</span><span>:</span><span> </span><span>nginx // service name</span><span>
</span></span></span><span><span><span></span><span>spec</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>externalName</span><span>:</span><span> </span><span>unused // any value - will be overwritten by operator</span><span>
</span></span></span></code></pre></div><p>Under <code>metadata.annotations</code>, add the annotation
<code>tailscale.com/tailnet-ip</code> with the Tailscale IP address for the tailnet service.
This can be either an IPv4 or IPv6 address, for either a Tailscale node or a route in a Tailscale subnet. This does not support IP ranges or Tailscale node names.</p>
<p>Under <code>spec.externalName</code>, add any value. This needs to be set to pass Kubernetes validation, but can hold any placeholder value. It will be overwritten by the Tailscale Kubernetes operator with a DNS name that cluster workloads can use to reach the proxy.</p>
<ol start="2">
<li>Wait for the Tailscale Kubernetes operator to deploy the proxy in the <code>tailscale</code> namespace, and to update the <code>spec.externalName</code> of the Kubernetes Service. It will get set to the DNS name of the egress proxy that Tailscale operator creates.</li>
</ol>
<p>Any cluster workload can now access the exposed Tailscale service using the annotated Kubernetes Service — calls to it will be routed to the proxy and forwarded to the right Tailscale node.</p>
<h5 id="validate-the-proxy-is-properly-deployed">
  <a href="#validate-the-proxy-is-properly-deployed">
    Validate the proxy is properly deployed
    
  </a>
</h5>

<p>The proxy pod is deployed in the <code>tailscale</code> namespace, and will have a name of the form <code>ts-&lt;annotated-service-name&gt;-&lt;random-string&gt;</code>.</p>
<p>If there are issues reaching the external service, verify the proxy pod is properly deployed:</p>
<ul>
<li>Review the logs of the proxy pod.</li>
<li>Review the logs of the operator. You can do this by running <code>kubectl logs deploy/operator --namespace tailscale</code>. The log level can be configured using the <code>OPERATOR_LOGGING</code> environment variable in the operator’s <a href="https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/manifests/operator.yaml">manifest file</a>.</li>
<li>Verify that the cluster workload is able to send traffic to the proxy pod in the <code>tailscale</code> namespace.</li>
</ul>
<h3 id="exposing-a-service-in-one-cluster-to-another-cluster-cross-cluster-connectivity">
  <a href="#exposing-a-service-in-one-cluster-to-another-cluster-cross-cluster-connectivity">
    Exposing a service in one cluster to another cluster (cross-cluster connectivity)
    
  </a>
</h3>

<p>You can use the Tailscale Kubernetes operator to expose a service in one cluster to another cluster. This is done by exposing the service on destination cluster A to the tailnet (<a href="#cluster-ingress">cluster ingress</a>), and connecting from a source service in cluster B to the tailnet (<a href="#cluster-egress">cluster egress</a>) in order to access the service running in cluster A.</p>
<p>This will need to be configured for each ingress and egress pair of services. To set this up for access via ingress to a service in cluster A and routing via egrees from a service in cluster B:</p>
<ol>
<li>Set up <a href="#exposing-a-service-using-ingress">ingress</a> in cluster A for the service you wish to access.</li>
<li>Expose the external service (running in cluster A) using its Tailscale IP address in cluster B with an <a href="#exposing-an-external-service-using-annotations">annotation on the external service</a></li>
</ol>
<h3 id="limitations">
  <a href="#limitations">
    Limitations
    
  </a>
</h3>

<ul>
<li>Only development (“unstable”) builds are usable for now, as the operator
depends on some changes that happened after the release of Tailscale v1.36.</li>
<li>There are no deployment options other than applying the
<a href="https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/manifests/operator.yaml">manifest file</a>.</li>
<li>There are no automated updates. The operator and proxy pods will not update
automatically to newer Tailscale releases as they become available.</li>
<li>There are no dashboards or metrics.</li>
</ul>
<p>Cluster ingress</p>
<ul>
<li>Tags are only considered during initial provisioning. That is, editing
<code>tailscale.com/tags</code> on an already exposed service doesn’t update the tags
until you clean up and re-expose the service.</li>
<li>The requested machine name is only considered during initial provisioning. That
is, editing <code>tailscale.com/hostname</code> on an already exposed service doesn’t
update the machine name until you clean up and re-expose the service.</li>
<li>Ingress to cluster services currently only supports hosts where netfilter can be configured via iptables.</li>
</ul>
<p>API server proxy</p>
<ul>
<li>The API server proxy runs inside of the cluster. If your cluster is non-functional or is unable to schedule pods, you may lose access to the API server proxy.</li>
</ul>
<p>Cluster egress</p>
<ul>
<li>When exposing a service to your cluster, any associated MagicDNS name will not resolve in-cluster. Instead, the name of the ExternalName service should be used to connect to the resource. As a result of this, if you use Tailscale to provision certificates you may see certificate name mismatch errors. <em>We are working on this.</em></li>
<li>Egress to external services supports using an IPv4 or IPv6 address for a single route in the <code>tailscale.com/tailnet-ip</code> annotation, but not IP ranges or node names.</li>
<li>Egress to external services currently only supports hosts where netfilter can be configured via iptables.</li>
<li>Egress to external services currently only supports clusters where privileged pods are permitted (i.e., GKE Autopilot is not supported).</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Prime Video content to start including ads next year (148 pts)]]></title>
            <link>https://www.bbc.co.uk/news/business-66887717</link>
            <guid>37611191</guid>
            <pubDate>Fri, 22 Sep 2023 12:36:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/business-66887717">https://www.bbc.co.uk/news/business-66887717</a>, See on <a href="https://news.ycombinator.com/item?id=37611191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/7ED8/production/_131227423_lotr.png.webp 976w" type="image/webp"><img alt="Galadriel in the Lord of the Rings: Rings of Power TV series" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/7ED8/production/_131227423_lotr.png 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/7ED8/production/_131227423_lotr.png 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/7ED8/production/_131227423_lotr.png 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/7ED8/production/_131227423_lotr.png 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/7ED8/production/_131227423_lotr.png 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/7ED8/production/_131227423_lotr.png 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/7ED8/production/_131227423_lotr.png" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Amazon Studios</span></p><figcaption><span>Image caption, </span><p>TV shows like The Lord of the Rings: Rings of Power and The Marvelous Mrs Maisel have proven huge hits for Amazon</p></figcaption></figure></div><div data-component="text-block"><p><b>Amazon is set to introduce adverts to its Prime Video streaming service in 2024 as it seeks to put more cash into creating TV shows and films.</b></p></div><div data-component="text-block"><p>UK Prime customers, along with those in the US, Germany and Canada, will see ads early next year unless they subscribe for an "ad-free" option at an additional cost.</p></div><div data-component="text-block"><p><a href="https://www.aboutamazon.co.uk/news/entertainment/an-update-on-prime-video">In a statement</a>, Amazon said Prime Video still offered "very compelling value".</p></div><div data-component="text-block"><p>It follows similar moves by rivals including Disney+ and Netflix.</p></div><div data-component="text-block"><p>Amazon said that the ads would be introduced across France, Italy, Spain, Mexico and Australia later in 2024.</p></div><div data-component="text-block"><p>It will roll out the "ad-free" subscription tier for an extra $2.99 (£2.44) per month for Prime subscribers in the United States.</p></div><div data-component="text-block"><p>Pricing for other countries will be announced at a later date, Amazon said.</p></div><div data-component="text-block"><p>At the moment, a Prime subscription, which includes free one-day delivery on goods as well as access to its streaming service, costs £8.99 per month, or £95 a year, in the UK.</p></div><div data-component="text-block"><p>"To continue investing in compelling content and keep increasing that investment over a long period of time, starting in 2024, Prime Video shows and movies will include limited advertisements in the UK," Amazon said.</p></div><div data-component="text-block"><p>But in the wake of similar announcements by other streaming companies, customers have expressed their disappointment.</p></div><div data-component="text-block"><p>Disney+ announced plans for an ad-supported service in August, while Netflix introduced its "basic with ads" streaming plan last year.</p></div><div data-component="text-block"><p>It marked a massive change for Netflix, which pioneered the world of ad-free, subscription-based, streaming.</p></div><div data-component="text-block"><p>In its announcement on Friday, Amazon said that it would aim "to have meaningfully fewer ads than linear TV and other streaming TV providers".</p></div><div data-component="text-block"><p>The company said it would get in touch with Prime members a few weeks before ads are introduced to show how to sign-up for the ad-free option if they wish to.</p></div><div data-component="text-block"><p>Live event broadcasts, like sports matches, will still include adverts even for those who sign up to the ad-free option.</p></div><div data-component="text-block"><p><a href="https://www.bbc.co.uk/news/business-64450202">Data previously released</a> by analysts Kantar showed that people cut back on video streaming services in their droves last year as they sought out different ways to deal with the spike in the cost of living.</p></div><div data-component="text-block"><p>It found that the number of paid-for video streaming subscriptions in the UK fell by two million, from 30.5 million to 28.5 million.</p></div><div data-component="text-block"><p>Although demand picked up around Christmas, Kantar said, people quickly looked to cut back again afterwards.</p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Principles for building and scaling feature flag systems (112 pts)]]></title>
            <link>https://docs.getunleash.io/topics/feature-flags/feature-flag-best-practices</link>
            <guid>37611136</guid>
            <pubDate>Fri, 22 Sep 2023 12:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.getunleash.io/topics/feature-flags/feature-flag-best-practices">https://docs.getunleash.io/topics/feature-flags/feature-flag-best-practices</a>, See on <a href="https://news.ycombinator.com/item?id=37611136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Feature flags, sometimes called feature toggles or feature switches, are a software development technique that allows engineering teams to decouple the release of new functionality from software deployments. With feature flags, developers can turn specific features or code segments on or off at runtime, without the need for a code deployment or rollback. Organizations who adopt feature flags see improvements in all key operational metrics for DevOps: Lead time to changes, mean-time-to-recovery, deployment frequency, and change failure rate.</p><p>There are 11 principles for building a large-scale feature flag system. These principles have their roots in distributed systems architecture and pay particular attention to security, privacy, and scale that is required by most enterprise systems. If you follow these principles, your feature flag system is less likely to break under load and will be easier to evolve and maintain.</p><p>These principles are:</p><ol><li><a href="https://docs.getunleash.io/topics/feature-flags/runtime-control">Enable run-time control. Control flags dynamically, not using config files.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/never-expose-pii">Never expose PII. Follow the principle of least privilege.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/evaluate-flags-close-to-user">Evaluate flags as close to the user as possible. Reduce latency.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/scale-horizontally">Scale Horizontally. Decouple reading and writing flags.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/limit-payloads">Limit payloads. Feature flag payload should be as small as possible.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/availability-over-consistency">Design for failure. Favor availability over consistency.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/short-lived-feature-flags">Make feature flags short-lived. Do not confuse flags with application configuration.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/unique-names">Use unique names across all applications. Enforce naming conventions.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/democratize-feature-flag-access">Choose open by default. Democratize feature flag access.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/prioritize-ux">Do no harm. Prioritize consistent user experience.</a></li><li><a href="https://docs.getunleash.io/topics/feature-flags/enable-traceability">Enable traceability. Make it easy to understand flag evaluation</a></li></ol><h2 id="background">Background<a href="#background" aria-label="Direct link to Background" title="Direct link to Background">​</a></h2><p>Feature flags have become a central part of the DevOps toolbox along with Git, CI/CD and microservices. You can write modern software without all of these things, but it sure is a lot harder, and a lot less fun.  </p><p>And just like the wrong Git repo design can cause interminable headaches, getting the details wrong when first building a feature flag system can be very costly.</p><p>This set of principles for building a large-scale feature management platform is the result of thousands of hours of work building and scaling Unleash, an open-source feature management solution used by thousands of organizations.  </p><p>Before Unleash was a community and a company, it was an internal project, started by <a href="https://github.com/ivarconr" target="_blank" rel="noopener noreferrer">one dev</a>, for one company. As the community behind Unleash grew, patterns and anti-patterns of large-scale feature flag systems emerged. Our community quickly discovered that these are important principles for anyone who wanted to avoid spending weekends debugging the production system that is supposed to make debugging in production easier.</p><p>“Large scale” means the ability to support millions of flags served to end-users with minimal latency or impact on application uptime or performance. That is the type of system most large enterprises are building today and the type of feature flag system that this guide focuses on.</p><p>Our motivation for writing these principles is to share what we’ve learned building a large-scale feature flag solution with other architects and engineers solving similar challenges. Unleash is open-source, and so are these principles. Have something to contribute? <a href="https://github.com/Unleash/unleash/pulls" target="_blank" rel="noopener noreferrer">Open a PR</a> or <a href="https://github.com/orgs/Unleash/discussions" target="_blank" rel="noopener noreferrer">discussion</a> on our Github.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn Rust by building real-world examples (107 pts)]]></title>
            <link>https://www.shuttle.rs/launchpad</link>
            <guid>37611080</guid>
            <pubDate>Fri, 22 Sep 2023 12:25:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shuttle.rs/launchpad">https://www.shuttle.rs/launchpad</a>, See on <a href="https://news.ycombinator.com/item?id=37611080">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><nav><a href="https://www.shuttle.rs/"><svg width="124" height="30" viewBox="0 0 124 30" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M45.6895 21.2534C46.3587 21.2534 46.832 21.1886 47.1096 21.059C47.3872 20.9294 47.5259 20.6784 47.5259 20.3058C47.5259 20.0145 47.3463 19.7635 46.9873 19.5529C46.628 19.3261 46.0814 19.0751 45.3468 18.8C44.7755 18.5894 44.253 18.3708 43.7797 18.144C43.3228 17.9174 42.9309 17.6503 42.6045 17.3425C42.278 17.0187 42.025 16.6381 41.8456 16.2008C41.666 15.7636 41.5763 15.2373 41.5763 14.6219C41.5763 13.4236 42.025 12.4763 42.9228 11.78C43.8207 11.0836 45.0529 10.7355 46.62 10.7355C47.4035 10.7355 48.1542 10.8083 48.8724 10.9541C49.5906 11.0836 50.1619 11.2294 50.5864 11.3913L49.9499 14.2089C49.5253 14.0633 49.0603 13.9337 48.5541 13.8204C48.0645 13.707 47.5096 13.6503 46.8893 13.6503C45.7467 13.6503 45.1755 13.9661 45.1755 14.5977C45.1755 14.7435 45.1999 14.8729 45.2488 14.9863C45.2978 15.0997 45.3958 15.2131 45.5425 15.3265C45.6895 15.4234 45.8854 15.5368 46.1304 15.6664C46.3914 15.7798 46.718 15.9094 47.1096 16.0552C47.9095 16.3466 48.5705 16.6381 49.093 16.9295C49.6152 17.2049 50.0232 17.5125 50.3171 17.8526C50.6272 18.1764 50.8394 18.5408 50.9537 18.9458C51.0841 19.3505 51.1495 19.8201 51.1495 20.3544C51.1495 21.6175 50.6681 22.573 49.7049 23.2208C48.7583 23.8685 47.4116 24.1923 45.6651 24.1923C44.5226 24.1923 43.5675 24.0953 42.8005 23.9009C42.0496 23.7066 41.5273 23.5446 41.2334 23.4152L41.8456 20.476C42.4659 20.7188 43.1024 20.9132 43.7554 21.059C44.4083 21.1886 45.0529 21.2534 45.6895 21.2534ZM53.8758 23.8523V5.5859L57.524 5.00293V11.1241C57.7689 11.0431 58.0789 10.9703 58.4545 10.9055C58.8461 10.8245 59.2217 10.784 59.5807 10.784C60.6253 10.784 61.4905 10.9298 62.176 11.2213C62.8779 11.4966 63.4329 11.8933 63.8411 12.4115C64.2654 12.9297 64.5591 13.5451 64.7224 14.2575C64.902 14.9701 64.9917 15.7636 64.9917 16.6381V23.8523H61.3435V17.0753C61.3435 15.9094 61.1885 15.0835 60.8785 14.5977C60.5846 14.112 60.0296 13.869 59.2134 13.869C58.8871 13.869 58.5768 13.9013 58.2831 13.9661C58.0056 14.0147 57.7526 14.0714 57.524 14.1362V23.8523H53.8758ZM79.2873 23.4152C78.667 23.5932 77.8672 23.7551 76.8879 23.9009C75.9084 24.0629 74.8802 24.1439 73.8027 24.1439C72.7092 24.1439 71.7952 23.9981 71.0607 23.7066C70.3424 23.4152 69.7712 23.0102 69.3466 22.4921C68.9223 21.9577 68.6203 21.3262 68.4407 20.5974C68.2611 19.8687 68.1714 19.0671 68.1714 18.1926V11.0755H71.8196V17.7554C71.8196 18.9214 71.9746 19.7635 72.2848 20.2816C72.5949 20.7998 73.1744 21.059 74.0233 21.059C74.2843 21.059 74.5619 21.0508 74.8556 21.0346C75.1495 21.0022 75.4107 20.9698 75.6391 20.9374V11.0755H79.2873V23.4152ZM82.5894 7.89349L86.2375 7.31052V11.0755H90.6203V14.0876H86.2375V18.5814C86.2375 19.3423 86.3682 19.9497 86.6294 20.403C86.9068 20.8566 87.4537 21.0832 88.2699 21.0832C88.6615 21.0832 89.0615 21.0508 89.4695 20.986C89.894 20.9052 90.2774 20.7998 90.6203 20.6702L91.1345 23.488C90.6936 23.6662 90.204 23.8199 89.6654 23.9495C89.1268 24.0791 88.4656 24.1439 87.6821 24.1439C86.6865 24.1439 85.8622 24.0143 85.2093 23.7551C84.5564 23.48 84.0339 23.1074 83.6422 22.6378C83.2506 22.1521 82.973 21.5691 82.8097 20.889C82.663 20.2088 82.5894 19.4557 82.5894 18.63V7.89349ZM93.4686 7.89349L97.1167 7.31052V11.0755H101.499V14.0876H97.1167V18.5814C97.1167 19.3423 97.2474 19.9497 97.5086 20.403C97.786 20.8566 98.3329 21.0832 99.1491 21.0832C99.5407 21.0832 99.9407 21.0508 100.349 20.986C100.773 20.9052 101.157 20.7998 101.499 20.6702L102.014 23.488C101.573 23.6662 101.083 23.8199 100.545 23.9495C100.006 24.0791 99.3448 24.1439 98.5615 24.1439C97.5657 24.1439 96.7414 24.0143 96.0885 23.7551C95.4356 23.48 94.9133 23.1074 94.5214 22.6378C94.1298 22.1521 93.8522 21.5691 93.6891 20.889C93.5422 20.2088 93.4686 19.4557 93.4686 18.63V7.89349ZM109.612 24.0953C108.551 24.0791 107.686 23.9657 107.017 23.7551C106.364 23.5446 105.841 23.2532 105.45 22.8806C105.074 22.4921 104.813 22.0305 104.666 21.4961C104.536 20.9456 104.47 20.3302 104.47 19.6501V5.5859L108.119 5.00293V18.9214C108.119 19.2453 108.143 19.5367 108.192 19.7959C108.241 20.0549 108.331 20.2736 108.461 20.4516C108.608 20.6298 108.812 20.7756 109.073 20.889C109.334 21.0022 109.685 21.0752 110.126 21.1076L109.612 24.0953ZM111.605 17.5611C111.605 16.4276 111.777 15.4396 112.119 14.5977C112.479 13.7394 112.944 13.0269 113.515 12.4601C114.086 11.8933 114.739 11.4642 115.474 11.1727C116.224 10.8812 116.992 10.7355 117.775 10.7355C119.603 10.7355 121.048 11.2942 122.109 12.4115C123.17 13.5127 123.7 15.1401 123.7 17.2939C123.7 17.5045 123.692 17.7393 123.676 17.9984C123.66 18.2412 123.643 18.4598 123.627 18.6542H115.351C115.433 19.3991 115.784 19.9901 116.404 20.4274C117.024 20.8646 117.857 21.0832 118.901 21.0832C119.571 21.0832 120.224 21.0266 120.86 20.9132C121.513 20.7836 122.044 20.6298 122.452 20.4516L122.941 23.3908C122.746 23.488 122.484 23.5852 122.158 23.6824C121.831 23.7796 121.464 23.8603 121.056 23.9251C120.664 24.0061 120.24 24.0709 119.783 24.1195C119.326 24.1681 118.869 24.1923 118.412 24.1923C117.253 24.1923 116.241 24.0223 115.376 23.6824C114.527 23.3422 113.817 22.8806 113.246 22.2977C112.691 21.6985 112.274 20.9942 111.997 20.1844C111.736 19.3747 111.605 18.5004 111.605 17.5611ZM120.175 16.1766C120.158 15.8688 120.101 15.5692 120.003 15.2779C119.922 14.9863 119.783 14.7273 119.587 14.5005C119.407 14.2737 119.171 14.0876 118.877 13.9418C118.599 13.7961 118.249 13.7232 117.824 13.7232C117.416 13.7232 117.065 13.7961 116.771 13.9418C116.477 14.0714 116.233 14.2495 116.037 14.4761C115.841 14.7029 115.686 14.9701 115.572 15.2779C115.474 15.5692 115.4 15.8688 115.351 16.1766H120.175Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.5276 0.927734L14.3516 7.9646L7.17585 7.9646L0 15.0013H7.17585L7.17585 22.0375H0V29.0743H7.17585L7.17585 22.0383H14.3516V29.0751L21.5276 22.0383V15.0013L28.7034 7.9646V0.927734H21.5276Z"></path></svg></a></nav><main><section><h2>Embark on your Rust learning journey with<!-- --> <span>Shuttle Launchpad</span></h2><p>Master Rust easily with our engaging, tutorial-style lessons and real-world examples in our Launchpad newsletter.</p><p>Join hundreds of developers on their journey</p></section><section><img alt="background" sizes="100vw" srcset="https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=640&amp;q=75 640w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=750&amp;q=75 750w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=828&amp;q=75 828w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1080&amp;q=75 1080w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1200&amp;q=75 1200w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1920&amp;q=75 1920w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=2048&amp;q=75 2048w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=3840&amp;q=75 3840w" src="https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><div><h2>Let’s face it - learning Rust can be a daunting task 😵‍💫</h2><p>And while there are plenty of resources out there, they can often be overwhelming, difficult to follow, or simply not engaging enough to hold your attention.</p><p>The Rust Book and various YouTube videos, for example, are comprehensive but can leave you feeling lost in the details, while other tutorials may not provide enough context or real-world examples to make the concepts stick, especially with the rapid developments of the Rust ecosystem.</p></div><div><h2>That’s where Shuttle Launchpad comes in 🚀</h2><p>Our unique approach to teaching Rust tutorial-style with real-world applications, in the form of a newsletter, sets us apart from other learning resources.</p><p>We make it easy for busy developers to learn Rust by breaking each concept down into manageable<span> one-hour chunks</span> that can be completed at your own pace, providing a focused and engaging way to learn that ensures you retain the material.</p><p>Advanced learners will also benefit from our additional resources, including tasks, links, and videos that allow you to dive deeper into the subject matter.</p></div><div><h2>Teaming up with Stefan Baumgartner 🦀</h2><p>We have teamed up with none other than <a href="https://fettblog.eu/">Stefan Baumgartner</a>, an architect, developer and author from Linz, Austria who, with his extensive experience of educating developers, brings a wealth of knowledge and practical insights to the Launchpad.</p><p>Stefan is, among other things, the organizer of the biggest Rust meet up in Europe ━<!-- --> <span><a href="https://rust-linz.at/">Rust Linz</a></span> <!-- -->and he was the lecturer at our<!-- --> <span><a href="https://www.youtube.com/watch?v=-N8AKKCE9L8">Rust for Javascript Developers</a></span> <!-- -->workshop!</p></div><div><h2>So, to sum it up</h2><p><span>→</span> Learning Rust tutorial-style with real-world applications</p><p><span>→</span> Suitable for beginners and experienced developers</p><p><span>→</span> Contents of each newsletter issue take only 1 hour to implement</p><p><span>→</span> Flexible pacing for busy developers</p><p><span>→</span> Additional resources for advanced learners</p><p><span>→</span> Learning one Rust concept per issue</p></div></section><section><h2>Past issues</h2><p>For our upcoming issues, we have some exciting project ideas lined up for you, such as an e-commerce application that performs safe transactions with a payment provider and an Activity Pub Client. Each project takes you from basics to advanced concepts.</p><a href="https://www.shuttle.rs/launchpad/issues"><p>See More <span>→</span></p></a></section><section><img alt="background" sizes="100vw" srcset="https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=640&amp;q=75 640w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=750&amp;q=75 750w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=828&amp;q=75 828w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1080&amp;q=75 1080w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1200&amp;q=75 1200w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=1920&amp;q=75 1920w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=2048&amp;q=75 2048w, https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=3840&amp;q=75 3840w" src="https://www.shuttle.rs/_next/image?url=%2Fimages%2Fsections%2Fbatch%2Fbg-batch.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"><h2>Start your journey now</h2><p>The Shuttle Launchpad newsletter is meant to run for multiple issues with no defined end, ensuring you always have new material to keep learning, staying up to date with the ever-changing Rust ecosystem.</p><p>Join hundreds of developers on their journey</p></section></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All the ways to capture changes in Postgres (235 pts)]]></title>
            <link>https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/</link>
            <guid>37610899</guid>
            <pubDate>Fri, 22 Sep 2023 12:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/">https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/</a>, See on <a href="https://news.ycombinator.com/item?id=37610899">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <!--kg-card-begin: markdown--><p>Working with data at rest is where Postgres shines. But what about when you need data in motion? What about when you need to trigger a workflow based on changes to a table? Or you need to stream the data in Postgres to another data store, system, or service in real-time?</p>
<p>Fortunately, Postgres comes with a lot of options to make this happen. In this post, I’ll lay them all out. I’ll also give you an idea of which are easy to do, which are more robust, and how to make the right choice for you.</p>
<h2 id="listennotify">Listen/Notify</h2>
<p>Perhaps the simplest approach is to use Postgres' interprocess communication feature, Listen/Notify. Listen/Notify is an implementation of the publish-subscribe pattern.</p>
<p>With Listen/Notify, a Postgres session (or connection) can "listen" to a particular channel for notifications. Activity in the database or other sessions can "notify" that channel. Whenever a notification is sent to a channel, all sessions listening to that channel receive the notification instantly.</p>
<p>You can see Listen/Notify for yourself by opening two <code>psql</code> sessions.</p>
<p>In session 1, you can setup your listener:</p>
<pre><code>&gt; listen my_channel;
LISTEN
</code></pre>
<p>And in session 2, you can publish to that channel with a message:</p>
<pre><code>&gt; notify my_channel, 'hey there!';
NOTIFY
&gt; notify my_channel, 'is this thing on?';
NOTIFY
</code></pre>
<p>While the listener process received the message right away, <code>psql</code> won't print the message automatically. To get it to print out the messages it's received so far, you just need to run any query. For example, you can just send an empty query like this:</p>
<pre><code>&gt; listen my_channel;
LISTEN
&gt; ;
Asynchronous notification "my_channel" with payload "hey there!" received from server process with PID 80019.
Asynchronous notification "my_channel" with payload "is this thing on?" received from server process with PID 80019.
</code></pre>
<p>(Naturally, this isn't how the Postgres client library in your preferred programming language will work. Libraries will deliver messages to your subscriber immediately without requiring a query.)</p>
<p>To use Listen/Notify to capture changes, you can set up a trigger. For example, here's an <code>after</code> trigger that sends along the payload of the record that changed as JSON via Notify:</p>
<pre><code>create or replace function notify_trigger() returns trigger as $$
declare
  payload json;
begin
  payload := json_build_object('table', TG_TABLE_NAME, 'id', NEW.id, 'action', TG_OP);
  perform pg_notify('table_changes', payload::text);
  return new;
end;
$$ language plpgsql;

create trigger my_trigger
after insert or update or delete on my_table
for each row execute function notify_trigger();
</code></pre>
<h3 id="downsides">Downsides</h3>
<p>Listen/Notify is simple and powerful, but has some notable downsides.</p>
<p>First, as a pub-sub mechanism, it has "at most once" delivery semantics. Notifications are transient; a listener needs to be listening to a channel when notifications are published. When a listener subscribes to a channel, it will only receive notifications from that moment forward. This also means that if there are network issues that cause a listening session to disconnect even briefly, it won't receive the notification.</p>
<p>Second, the payload size limit is 8000 bytes. If the message exceeds this size, the <code>notify</code> command will fail. <sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>As such, Listen/Notify is solid for basic change detection needs, but you'll probably find it does not serve more sophisticated needs well. However, it can complement other strategies (like "poll the table") nicely.</p>
<h2 id="poll-the-table">Poll the table</h2>
<p>The simplest <em>robust</em> way to capture changes is to poll the table directly. Here, you need each table to have an <code>updated_at</code> column or similar that updates whenever the row updates. (You can use a trigger for this.) A combination of <code>updated_at</code> <a href="https://blog.sequin.io/whats-changed-in-your-api/">and <code>id</code></a> serve as your cursor. In this setup, your application logic that polls the table handles storing and maintaining the cursor.</p>
<p>In addition to polling the table, you can use a Notify subscription to inform your application that a record has been inserted or modified. Postgres' notifications are ephemeral, so this should only serve as an optimization on top of polling.</p>
<h3 id="downsides">Downsides</h3>
<p>This approach has two downsides.</p>
<p>The first is that you can't detect when a row is deleted. There's no way to "see" the missing row in the table.</p>
<p>One remediation is to have a Postgres trigger fire on deletes, and store the <code>id</code> (and whatever other columns you want) in a separate table: e.g. <code>deleted_contacts</code>. Then, your application can poll that table to discover deletes instead.</p>
<p>The second downside is that you don't get diffs. You know this record was updated since you last polled the table, but you don't know <em>what</em> was updated on the record.</p>
<p>Maybe deletes aren't a big deal for your use case or you don't care about diffs. If so, polling the table is a reasonable and simple solution for tracking changes.</p>
<h2 id="replication-wal">Replication (WAL)</h2>
<p>Postgres supports streaming replication to other Postgres databases. In streaming replication, Postgres sends the WAL stream over a network connection from the primary to a replica. The standby servers pull these WAL records and replay them to keep their database in sync with the primary database.</p>
<p>Streaming replication was built for streaming changes to other Postgres servers. But you can use it to capture changes for your application too.</p>
<p>You first create a replication slot, like this:</p>
<pre><code>select * from
pg_create_logical_replication_slot('&lt;your_slot_name&gt;', '&lt;output_plugin&gt;');
</code></pre>
<p><code>output_plugin</code> is a parameter which specifies which plugin Postgres should use to decode WAL changes. Postgres comes with a few built-in plugins. <code>pgoutput</code> is the default. It formats the output in the binary expected by client servers. <code>test_decoding</code> is a simple output plugin that provides human-readable output of the changes to the WAL.</p>
<p>The most popular output plugin not built-in to Postgres is <code>wal2json</code>. It does what it says on the tin. JSON will be a lot easier for you to consume from an application than Postgres' binary format.</p>
<p>After creating your replication slot, you can start it and consume from it. Working with replication slots uses a different part of the Postgres protocol than standard queries. But many client libraries have functions that help you work with replication slots.</p>
<p>For example, this is how you consume WAL messages in the <code>psycopg2</code> library:</p>
<pre><code>cursor.start_replication(slot_name='your_slot_name', decode=True)
cursor.consume_stream(lambda msg: acknowledge_to_server(cursor, msg))

def acknowledge_to_server(cursor, msg):
    # Process the message (msg) here
    # ...
    # Acknowledge the message
    cursor.send_feedback(flush_lsn=msg.wal_end)
</code></pre>
<p>Note that the client is responsible for ack'ing WAL messages that it has received. So the replication slot behaves like event buses such as SQS.</p>
<p>Instead of consuming from the WAL directly, you can use tools like Debezium to do this for you. Debezium will consume the WAL from Postgres and stream those changes to a variety of sinks, including Kafka or NATS.</p>
<h3 id="downsides">Downsides</h3>
<p>Using Postgres' replication facilities to capture changes is a robust solution. The biggest downside is complexity. Replication slots and the replication protocol are less familiar to most developers than the "standard" parts (i.e. tables and queries).</p>
<p>Along with this complexity is a decrease in clarity. If something with replication breaks or if there's a lag or things aren't working as expected, it can be a bit trickier to debug than the other solutions outlined here.</p>
<p>Another aspect worth mentioning is that replication slots may require tweaking <code>postgresql.conf</code>. For example, you may need to tweak parameters like <code>max_wal_senders</code> and <code>max_replication_slots</code>. So you'll need total access to the database to implement this solution.</p>
<h2 id="capture-changes-in-an-audit-table">Capture changes in an audit table</h2>
<p>In this approach, you set up a separate table for logging changes, e.g. <code>changelog</code>. That table contains column related to the record's modification, such as:</p>
<ul>
<li><code>action</code>: Was this an <code>insert</code>, <code>update</code>, or <code>delete</code>?</li>
<li><code>old</code>: A jsonb of the record before the mutation. Blank for inserts.</li>
<li><code>values</code>: A jsonb of the change fields. Blank for deletes.</li>
<li><code>inserted_at</code>: Time the change occurred.</li>
</ul>
<p>To set this up, you need to create a trigger function that inserts into this table every time a change occurs. Then, you need to create triggers on all the tables you care about to invoke that trigger function.</p>
<p>Here's an example of what that trigger function might look like:</p>
<pre><code>create or replace function changelog_trigger() returns trigger as $$
declare
  action text;
  table_name text;
  transaction_id bigint;
  timestamp timestamp;
  old_data jsonb;
  new_data jsonb;
begin
  action := lower(TG_OP::text);
  table_name := TG_TABLE_NAME::text;
  transaction_id := txid_current();
  timestamp := current_timestamp;

  if TG_OP = 'DELETE' then
    old_data := to_jsonb(OLD.*);
  elseif TG_OP = 'INSERT' then
    new_data := to_jsonb(NEW.*);
  elseif TG_OP = 'UPDATE' then
    old_data := to_jsonb(OLD.*);
    new_data := to_jsonb(NEW.*);
  end if;

  insert into changelog (action, table_name, transaction_id, timestamp, old_data, new_data) 
  values (action, table_name, transaction_id, timestamp, old_data, new_data);

  return null;
end;
$$ language plpgsql;
</code></pre>
<p>After setting up a way to capture changes, you need to figure out how to consume them.</p>
<p>There's a lot of different ways you can do this. One way is to treat the <code>changelog</code> as a queue. Your application workers can pull changes from this table. You'll probably want to ensure that changes are processed ~exactly once. You can use the <code>for update skip locked</code> feature in Postgres to do this. For example, your workers can open a transaction and grab a chunk of <code>changelog</code> entries:</p>
<pre><code>begin;

select * 
from changelog 
order by timestamp 
limit 100 
for update skip locked;
</code></pre>
<p>Now, other workers running that query will not receive this "locked" block of rows. After your worker processes the records, it can delete them:</p>
<pre><code>delete from changelog 
where id in (list_of_processed_record_ids);

commit;
</code></pre>
<h3 id="downsides">Downsides</h3>
<p>This approach is similar to using a replication slot, but more manual. The trigger function and table design I've outlined might work to start. But you'd likely need to make tweaks before deploying at scale in production. <sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>The advantage over replication slots is that it's all "standard" Postgres. Instead of an opaque replication slot, you have an easy to query Postgres table. And you don't need access to <code>postgresql.conf</code> to make this work.</p>
<h2 id="foreign-data-wrappers">Foreign data wrappers</h2>
<p>Foreign data wrappers (FDWs) are a Postgres feature that allow you to both read from and write to external data sources from your Postgres database.</p>
<p>The most notable and widely supported extension built on FDWs is <code>postgres_fdw</code>. With <code>postgres_fdw</code>, you can connect two Postgres databases and create something like a <a href="https://www.postgresql.org/docs/current/sql-createview.html?ref=blog.sequin.io">view</a> in one Postgres database that references a table in another Postgres database. Under the hood, you're turning one Postgres database into a client and the other into a server. When you make queries against foreign tables, the client database sends the queries to the server database via Postgres' <a href="https://www.postgresql.org/docs/current/protocol-flow.html?ref=blog.sequin.io">wire protocol</a>.</p>
<p>Using FDWs to capture changes is an unusual strategy. I wouldn't recommend it outside very specific situations.</p>
<p>One situation where FDWs could make sense is if you're capturing changes in one Postgres database in order to write them to another Postgres database. Perhaps you use one database for accounting and another for your application. You can skip the intermediary change capture steps and use <code>postgres_fdw</code> to go from database to database.</p>
<p>Here's an example trigger that ensures the status for a given account (identified by <code>email</code>) is in-sync across two databases. This assumes the foreign table has already been declared as <code>foreign_app_database</code>:</p>
<pre><code>create or replace function cancel_subscription()
  returns trigger as $$
declare
  account_status text;
begin
  if (new.status = 'cancelled' or new.status = 'suspended') then
    account_status := 'cancelled';

    update foreign_app_database.account
    set status = account_status
    where email = new.email;
  end if;

  return new;
end;
$$ language plpgsql;
</code></pre>
<p>In addition to <code>postgres_fdw</code>, you can create and load your own foreign data wrappers into your Postgres database.</p>
<p>That means you could create a foreign data wrapper that posts changes to an internal API. Unlike the other change detection strategies in this list, because you'd write to the API inside your commit, your API would have the ability to reject the change and roll back the commit.</p>
<h3 id="downsides">Downsides</h3>
<p>Foreign data wrappers are a fun and powerful Postgres feature. But they'll rarely be your best option for capturing changes. You're probably not trying to replicate changes from one Postgres database to another. And while writing your own foreign data wrapper from scratch <a href="https://github.com/supabase/wrappers?ref=blog.sequin.io">has gotten easier</a>, writing your own FDW is probably the biggest lift in this list for capturing changes.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There are lots of options for capturing changes in Postgres. Depending on your use case, some options are clearly better than others. In sum:</p>
<ul>
<li>Listen/Notify is great for non-critical event capture, prototyping, or optimizing polling.</li>
<li>Polling for changes is a fine, straightforward solution for simple use cases.</li>
<li>Replication is probably your best bet for a robust solution. If that’s too difficult or opaque, then perhaps the audit table is a good middle-ground.</li>
<li>Finally, foreign data wrappers solve a need you’re unlikely to have.</li>
</ul>
<p>We examined all of these options for our own change capture requirements, and unfortunately none of them met our complex (and niche) needs. So, we ended up needing to build a Postgres proxy 😅 You can <a href="https://blog.sequin.io/we-had-no-choice-but-to-build-a-postgres-proxy/">read more about that here</a>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>Note the payload size includes the channel name, which like all Postgres identifiers can be up to 64 bytes in size. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>One example issue that comes to mind: should there be a timeout for how long workers can have changes checked out? <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal: The Pqxdh Key Agreement Protocol (198 pts)]]></title>
            <link>https://signal.org/docs/specifications/pqxdh/</link>
            <guid>37610447</guid>
            <pubDate>Fri, 22 Sep 2023 11:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://signal.org/docs/specifications/pqxdh/">https://signal.org/docs/specifications/pqxdh/</a>, See on <a href="https://news.ycombinator.com/item?id=37610447">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><img src="https://signal.org/assets/body/spaceship-e722e5c700acd0e0ac8b003fb8269174a00e6190021f47ea233bb4713189818e.png"></p><h2> The PQXDH Key Agreement Protocol</h2><h3><p>Revision 1, 2023-05-24 [<a href="https://signal.org/docs/specifications/pqxdh/pqxdh.pdf">PDF</a>]</p><p>Ehren Kret, Rolfe Schmidt</p></h3></div><div><div id="TOC"><h2>Table of Contents</h2><ul><li><a href="#introduction" id="toc-introduction">1. Introduction</a></li><li><a href="#preliminaries" id="toc-preliminaries">2. Preliminaries</a><ul><li><a href="#pqxdh-parameters" id="toc-pqxdh-parameters">2.1. PQXDH parameters</a></li><li><a href="#cryptographic-notation" id="toc-cryptographic-notation">2.2. Cryptographic notation</a></li><li><a href="#roles" id="toc-roles">2.3. Roles</a></li><li><a href="#elliptic-curve-keys" id="toc-elliptic-curve-keys">2.4. Elliptic Curve Keys</a></li><li><a href="#post-quantum-key-encapsulation-keys" id="toc-post-quantum-key-encapsulation-keys">2.5. Post-Quantum Key Encapsulation Keys</a></li></ul></li><li><a href="#the-pqxdh-protocol" id="toc-the-pqxdh-protocol">3. The PQXDH protocol</a><ul><li><a href="#overview" id="toc-overview">3.1. Overview</a></li><li><a href="#publishing-keys" id="toc-publishing-keys">3.2. Publishing keys</a></li><li><a href="#sending-the-initial-message" id="toc-sending-the-initial-message">3.3. Sending the initial message</a></li><li><a href="#receiving-the-initial-message" id="toc-receiving-the-initial-message">3.4. Receiving the initial message</a></li></ul></li><li><a href="#security-considerations" id="toc-security-considerations">4. Security considerations</a><ul><li><a href="#authentication" id="toc-authentication">4.1. Authentication</a></li><li><a href="#protocol-replay" id="toc-protocol-replay">4.2. Protocol replay</a></li><li><a href="#replay-and-key-reuse" id="toc-replay-and-key-reuse">4.3. Replay and key reuse</a></li><li><a href="#deniability" id="toc-deniability">4.4. Deniability</a></li><li><a href="#signatures" id="toc-signatures">4.5. Signatures</a></li><li><a href="#key-compromise" id="toc-key-compromise">4.6. Key compromise</a></li><li><a href="#passive-quantum-adversaries" id="toc-passive-quantum-adversaries">4.7. Passive quantum adversaries</a></li><li><a href="#active-quantum-adversaries" id="toc-active-quantum-adversaries">4.8. Active quantum adversaries</a></li><li><a href="#server-trust" id="toc-server-trust">4.9. Server trust</a></li><li><a href="#identity-binding" id="toc-identity-binding">4.10. Identity binding</a></li><li><a href="#risks-of-weak-randomness-sources" id="toc-risks-of-weak-randomness-sources">4.11. Risks of weak randomness sources</a></li></ul></li><li><a href="#ipr" id="toc-ipr">5. IPR</a></li><li><a href="#acknowledgements" id="toc-acknowledgements">6. Acknowledgements</a></li><li><a href="#references" id="toc-references">7. References</a></li></ul></div><h2 id="introduction">1. Introduction</h2><p>This document describes the “PQXDH” (or “Post-Quantum Extended Diffie-Hellman”) key agreement protocol. PQXDH establishes a shared secret key between two parties who mutually authenticate each other based on public keys. PQXDH provides post-quantum forward secrecy and a form of cryptographic deniability but still relies on the hardness of the discrete log problem for mutual authentication in this revision of the protocol.</p><p>PQXDH is designed for asynchronous settings where one user (“Bob”) is offline but has published some information to a server. Another user (“Alice”) wants to use that information to send encrypted data to Bob, and also establish a shared secret key for future communication.</p><h2 id="preliminaries">2. Preliminaries</h2><h2 id="pqxdh-parameters">2.1. PQXDH parameters</h2><p>An application using PQXDH must decide on several parameters:</p><table><colgroup><col><col></colgroup><thead><tr><th>Name</th><th>Definition</th></tr></thead><tbody><tr><td><em>curve</em></td><td>A Montgomery curve for which XEdDSA <span data-cites="xeddsa"><a href="#ref-xeddsa" role="doc-biblioref">[1]</a></span> is specified, at present this is one of curve25519 or curve448</td></tr><tr><td><em>hash</em></td><td>A 256 or 512-bit hash function (e.g.&nbsp;SHA-256 or SHA-512)</td></tr><tr><td><em>info</em></td><td>An ASCII string identifying the application with a minimum length of 8 bytes</td></tr><tr><td><em>pqkem</em></td><td>A post-quantum key encapsulation mechanism (e.g. Crystals-Kyber-1024 <span data-cites="kyberfips203"><a href="#ref-kyberfips203" role="doc-biblioref">[2]</a></span>)</td></tr><tr><td><em>EncodeEC</em></td><td>A function that encodes a <em>curve</em> public key into a byte sequence</td></tr><tr><td><em>DecodeEC</em></td><td>A function that decodes a byte sequence into a <em>curve</em> public key and is the inverse of <em>EncodeEC</em></td></tr><tr><td><em>EncodeKEM</em></td><td>A function that encodes a <em>pqkem</em> public key into a byte sequence</td></tr><tr><td><em>DecodeKEM</em></td><td>A function that decodes a byte sequence into a <em>pqkem</em> public key and is the inverse of <em>EncodeKEM</em></td></tr></tbody></table><p>For example, an application could choose <em>curve</em> as curve25519, <em>hash</em> as SHA-512, <em>info</em> as “MyProtocol”, and <em>pqkem</em> as CRYSTALS-KYBER-1024.</p><p>The recommended implementation of <em>EncodeEC</em> consists of a single-byte constant representation of <em>curve</em> followed by little-endian encoding of the u-coordinate as specified in <span data-cites="rfc7748"><a href="#ref-rfc7748" role="doc-biblioref">[3]</a></span>. The single-byte representation of <em>curve</em> is defined by the implementer. Similarly the recommended implementation of <em>DecodeEC</em> reads the first byte to determine the parameter <em>curve</em>. If the first byte does not represent a recognized curve, the function fails. Otherwise it applies the little-endian decoding of the u-coordinate for <em>curve</em> as specified in <span data-cites="rfc7748"><a href="#ref-rfc7748" role="doc-biblioref">[3]</a></span>.</p><p>The recommended implementation of <em>EncodeKEM</em> consists of a single-byte constant representation of <em>pqkem</em> followed by the encoding of <strong><em>PQKPK</em></strong> specified by <em>pqkem</em>. The single-byte representation of <em>pqkem</em> is defined by the implementer. Similarly the recommended implementation of <em>DecodeKEM</em> reads the first byte to determine the parameter <em>pqkem</em>. If the first byte does not represent a recognized key encapsulation mechanism, the function fails. Otherwise it applies the decoding specified by the selected key encapsulation mechanism.</p><h2 id="cryptographic-notation">2.2. Cryptographic notation</h2><p>Throughout this document, all public keys have a corresponding private key, but to simplify descriptions we will identify key pairs by the public key and assume that the corresponding private key can be accessed by the key owner.</p><p>This document will use the following notation:</p><ul><li><p>The concatenation of byte sequences <strong><em>X</em></strong> and <strong><em>Y</em></strong> is <strong><em>X</em></strong> || <strong><em>Y</em></strong>.</p></li><li><p><strong><em>DH(PK1, PK2)</em></strong> represents a byte sequence which is the shared secret output from an Elliptic Curve Diffie-Hellman function involving the key pairs represented by public keys <em>PK1</em> and <em>PK2</em>. The Elliptic Curve Diffie-Hellman function will be either the X25519 or X448 function from <span data-cites="rfc7748"><a href="#ref-rfc7748" role="doc-biblioref">[3]</a></span>, depending on the <em>curve</em> parameter.</p></li><li><p><strong><em>Sig(PK, M, Z)</em></strong> represents the byte sequence that is a <em>curve</em> XEdDSA signature on the byte sequence <em>M</em> which was created by signing <em>M</em> with <em>PK</em>’s corresponding private key and using 64 bytes of randomness <em>Z</em>. This signature verifies with public key <em>PK</em>. The signing and verification functions for XEdDSA are specified in <span data-cites="xeddsa"><a href="#ref-xeddsa" role="doc-biblioref">[1]</a></span>.</p></li><li><p><strong><em>KDF(KM)</em></strong> represents 32 bytes of output from the HKDF algorithm <span data-cites="rfc5869"><a href="#ref-rfc5869" role="doc-biblioref">[4]</a></span> using <em>hash</em> with inputs:</p><ul><li><em>HKDF input key material</em> = <em>F</em> || <em>KM</em>, where <em>KM</em> is an input byte sequence containing secret key material, and <em>F</em> is a byte sequence containing 32 0xFF bytes if <em>curve</em> is curve25519, and 57 0xFF bytes if <em>curve</em> is curve448. As in in XEdDSA <span data-cites="xeddsa"><a href="#ref-xeddsa" role="doc-biblioref">[1]</a></span>, <em>F</em> ensures that the first bits of the HKDF input key material are never a valid encoding of a scalar or elliptic curve point.</li><li><em>HKDF salt</em> = A zero-filled byte sequence with length equal to the <em>hash</em> output length, in bytes.</li><li><em>HKDF info</em> = The concatenation of string representations of the 4 PQXDH parameters <em>info</em>, <em>curve</em>, <em>hash</em>, and <em>pqkem</em> into a single string separated with ‘<code>_</code>’ such as “<code>MyProtocol_CURVE25519_SHA-512_CRYSTALS-KYBER-1024</code>”. The string representations of the PQXDH parameters are defined by the implementer.</li></ul></li><li><p><strong><em>(CT, SS) = PQKEM-ENC(PK)</em></strong> represents a tuple of the byte sequence that is the KEM ciphertext, <em>CT</em>, output by the algorithm <em>pqkem</em> together with the shared secret byte sequence <em>SS</em> encapsulated by the ciphertext using the public key <em>PK</em>.</p></li><li><p><strong><em>PQKEM-DEC(PK, CT)</em></strong> represents the shared secret byte sequence <em>SS</em> decapsulated from a <em>pqkem</em> ciphertext using the private key counterpart of the public key <em>PK</em> used to encapsulate the ciphertext CT.</p></li></ul><h2 id="roles">2.3. Roles</h2><p>The PQXDH protocol involves three parties: <strong>Alice</strong>, <strong>Bob</strong>, and a <strong>server</strong>.</p><ul><li><p><strong>Alice</strong> wants to send <strong>Bob</strong> some initial data using encryption, and also establish a shared secret key which may be used for bidirectional communication.</p></li><li><p><strong>Bob</strong> wants to allow parties like <strong>Alice</strong> to establish a shared key with him and send encrypted data. However, <strong>Bob</strong> might be offline when <strong>Alice</strong> attempts to do this. To enable this, <strong>Bob</strong> has a relationship with some <strong>server</strong>.</p></li><li><p>The <strong>server</strong> can store messages from <strong>Alice</strong> to <strong>Bob</strong> which <strong>Bob</strong> can later retrieve. The <strong>server</strong> also lets <strong>Bob</strong> publish some data which the server will provide to parties like <strong>Alice</strong>. The amount of trust placed in the server is discussed in <a href="#server-trust">Section 4.9</a>.</p></li></ul><p>In some systems the <strong>server</strong> role might be divided between multiple entities, but for simplicity we assume a single server that provides the above functions for <strong>Alice</strong> and <strong>Bob</strong>.</p><h2 id="elliptic-curve-keys">2.4. Elliptic Curve Keys</h2><p>PQXDH uses the following elliptic curve public keys:</p><table><colgroup><col><col></colgroup><thead><tr><th>Name</th><th>Definition</th></tr></thead><tbody><tr><td><em>IK<sub>A</sub></em></td><td>Alice’s identity key</td></tr><tr><td><em>IK<sub>B</sub></em></td><td>Bob’s identity key</td></tr><tr><td><em>EK<sub>A</sub></em></td><td>Alice’s ephemeral key</td></tr><tr><td><em>SPK<sub>B</sub></em></td><td>Bob’s signed prekey</td></tr><tr><td>(<em>OPK<sub>B</sub><sup>1</sup></em>, <em>OPK<sub>B</sub><sup>2</sup></em>, …)</td><td>Bob’s set of one-time prekeys</td></tr></tbody></table><p>The elliptic curve public keys used within a PQXDH protocol run must either all be in curve25519 form, or they must all be in curve448 form, depending on the <em>curve</em> parameter <span data-cites="rfc7748"><a href="#ref-rfc7748" role="doc-biblioref">[3]</a></span>.</p><p>Each party has a long-term identity elliptic curve public key (<em>IK<sub>A</sub></em> for Alice, <em>IK<sub>B</sub></em> for Bob).</p><p>Bob also has a signed prekey <em>SPK<sub>B</sub></em>, which he changes periodically and signs each time with <em>IK<sub>B</sub></em>, and a set of one-time prekeys (<em>OPK<sub>B</sub><sup>1</sup></em>, <em>OPK<sub>B</sub><sup>2</sup></em>, …), which are each used in a single PQXDH protocol run. (“Prekeys” are so named because they are essentially protocol messages which Bob publishes to the server prior to Alice beginning the protocol run.) These keys will be uploaded to the <strong>server</strong> as described in <a href="#publishing-keys">Section 3.2</a>.</p><p>During each protocol run, Alice generates a new ephemeral key pair with public key <em>EK<sub>A</sub></em>.</p><h2 id="post-quantum-key-encapsulation-keys">2.5. Post-Quantum Key Encapsulation Keys</h2><p>PQXDH uses the following post-quantum key encapsulation public keys:</p><table><colgroup><col><col></colgroup><thead><tr><th>Name</th><th>Definition</th></tr></thead><tbody><tr><td><em>PQSPK<sub>B</sub></em></td><td>Bob’s signed last-resort <em>pqkem</em> prekey</td></tr><tr><td>(<em>PQOPK<sub>B</sub><sup>1</sup></em>, <em>PQOPK<sub>B</sub><sup>2</sup></em>, …)</td><td>Bob’s set of signed one-time <em>pqkem</em> prekeys</td></tr></tbody></table><p>The <em>pqkem</em> public keys used within a PQXDH protocol run must all use the same <em>pqkem</em> parameter.</p><p>Bob has a signed last-resort post-quantum prekey <em>PQSPK<sub>B</sub></em>, which he changes periodically and signs each time with <em>IK<sub>B</sub></em>, and a set of signed one-time prekeys (<em>PQOPK<sub>B</sub><sup>1</sup></em>, <em>PQOPK<sub>B</sub><sup>2</sup></em>, …) which are also signed with <em>IK<sub>B</sub></em> and each used in a single PQXDH protocol run. These keys will be uploaded to the <strong>server</strong> as described in <a href="#publishing-keys">Section 3.2</a>. The name “last-resort” refers to the fact that the last-resort prekey is only used when one-time <em>pqkem</em> prekeys are not available. This can happen when the number of prekey bundles downloaded for Bob exceeds the number of one-time <em>pqkem</em> prekeys Bob has uploaded (see <a href="#the-pqxdh-protocol">Section 3</a> for details about the role of the server).</p><h2 id="the-pqxdh-protocol">3. The PQXDH protocol</h2><h2 id="overview">3.1. Overview</h2><p>PQXDH has three phases:</p><ol type="1"><li><p>Bob publishes his elliptic curve identity key, elliptic curve prekeys, and <em>pqkem</em> prekeys to a server.</p></li><li><p>Alice fetches a “prekey bundle” from the server, and uses it to send an initial message to Bob.</p></li><li><p>Bob receives and processes Alice’s initial message.</p></li></ol><p>The following sections explain these phases.</p><h2 id="publishing-keys">3.2. Publishing keys</h2><p>Bob generates a sequence of 64-byte random values <em>Z<sub>SPK</sub>, Z<sub>PQSPK</sub>, Z<sub>1</sub>, Z<sub>2</sub>, …</em> and publishes a set of keys to the server containing:</p><ul><li>Bob’s <em>curve</em> identity key <em>IK<sub>B</sub></em></li><li>Bob’s signed <em>curve</em> prekey <em>SPK<sub>B</sub></em></li><li>Bob’s signature on the <em>curve</em> prekey <em>Sig(IK<sub>B</sub>, EncodeEC(SPK<sub>B</sub>), Z<sub>SPK</sub>)</em></li><li>Bob’s signed last-resort <em>pqkem</em> prekey <em>PQSPK<sub>B</sub></em></li><li>Bob’s signature on the <em>pqkem</em> prekey <em>Sig(IK<sub>B</sub>, EncodeKEM(PQSPK<sub>B</sub>), Z<sub>PQSPK</sub>)</em></li><li>A set of Bob’s one-time <em>curve</em> prekeys <em>(OPK<sub>B</sub><sup>1</sup>, OPK<sub>B</sub><sup>2</sup>, OPK<sub>B</sub><sup>3</sup>, …)</em></li><li>A set of Bob’s signed one-time <em>pqkem</em> prekeys <em>(PQOPK<sub>B</sub><sup>1</sup>, PQOPK<sub>B</sub><sup>2</sup>, PQOPK<sub>B</sub><sup>3</sup>, …)</em></li><li>The set of Bob’s signatures on the signed one-time <em>pqkem</em> prekeys <em>(Sig(IK<sub>B</sub>, EncodeKEM(PQOPK<sub>B</sub><sup>1</sup>), Z<sub>1</sub>), Sig(IK<sub>B</sub>, EncodeKEM(PQOPK<sub>B</sub><sup>2</sup>), Z<sub>2</sub>), Sig(IK<sub>B</sub>, EncodeKEM(PQOPK<sub>B</sub><sup>3</sup>), Z<sub>3</sub>), …)</em></li></ul><p>Bob only needs to upload his identity key to the server once. However, Bob may upload new one-time prekeys at other times (e.g.&nbsp;when the server informs Bob that the server’s store of one-time prekeys is getting low).</p><p>For both the signed <em>curve</em> prekey and the signed last-resort <em>pqkem</em> prekey, Bob will upload a new prekey along with its signature using <em>IK<sub>B</sub></em> at some interval (e.g.&nbsp;once a week or once a month). The new signed prekey and its signatures will replace the previous values.</p><p>After uploading a new pair of signed <em>curve</em> and signed last-resort <em>pqkem</em> prekeys, Bob may keep the private key corresponding to the previous pair around for some period of time to handle messages using it that may have been delayed in transit. Eventually, Bob should delete this private key for forward secrecy (one-time prekey private keys will be deleted as Bob receives messages using them; see <a href="#receiving-the-initial-message">Section 3.4</a>).</p><h2 id="sending-the-initial-message">3.3. Sending the initial message</h2><p>To perform a PQXDH key agreement with Bob, Alice contacts the server and fetches a “prekey bundle” containing the following values:</p><ul><li>Bob’s <em>curve</em> identity key <em>IK<sub>B</sub></em></li><li>Bob’s signed <em>curve</em> prekey <em>SPK<sub>B</sub></em></li><li>Bob’s signature on the <em>curve</em> prekey <em>Sig(IK<sub>B</sub>, EncodeEC(SPK<sub>B</sub>), Z<sub>SPK</sub>)</em></li><li>One of either Bob’s signed one-time <em>pqkem</em> prekey <em>PQOPK<sub>B</sub><sup>n</sup></em> or Bob’s last-resort signed <em>pqkem</em> prekey <em>PQSPK<sub>B</sub></em> if no signed one-time <em>pqkem</em> prekey remains. Call this key <em>PQPK<sub>B</sub></em>.</li><li>Bob’s signature on the <em>pqkem</em> prekey <em>Sig(IK<sub>B</sub>, EncodeKEM(PQPK<sub>B</sub>), Z<sub>PQPK</sub>)</em></li><li>(Optionally) Bob’s one-time <em>curve</em> prekey <em>OPK<sub>B</sub><sup>n</sup></em></li></ul><p>The server should provide one of Bob’s <em>curve</em> one-time prekeys if one exists and then delete it. If all of Bob’s <em>curve</em> one-time prekeys on the server have been deleted, the bundle will not contain a one-time <em>curve</em> prekey element.</p><p>The server should prefer to provide one of Bob’s <em>pqkem</em> one-time signed prekeys <em>PQOPK<sub>B</sub><sup>n</sup></em> if one exists and then delete it. If all of Bob’s <em>pqkem</em> one-time signed prekeys on the server have been deleted, the bundle will instead contain Bob’s <em>pqkem</em> last-resort signed prekey <em>PQSPK<sub>B</sub></em>.</p><p>Alice verifies the signatures on the prekeys. If any signature check fails, Alice aborts the protocol. Otherwise, if all signature checks pass, Alice then generates an ephemeral <em>curve</em> key pair with public key EK<sub>A</sub>. Alice additionally generates a <em>pqkem</em> encapsulated shared secret:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;(CT, SS) = PQKEM-ENC(PQPK<sub>B</sub>)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shared secret SS<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ciphertext CT</p><p>If the bundle does not contain a <em>curve</em> one-time prekey, she calculates:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;DH<sub>1</sub> = DH(IK<sub>A</sub>, SPK<sub>B</sub>)<br> &nbsp;&nbsp;&nbsp;&nbsp;DH<sub>2</sub> = DH(EK<sub>A</sub>, IK<sub>B</sub>)<br> &nbsp;&nbsp;&nbsp;&nbsp;DH<sub>3</sub> = DH(EK<sub>A</sub>, SPK<sub>B</sub>)<br> &nbsp;&nbsp;&nbsp;&nbsp;SK = KDF(DH<sub>1</sub> || DH<sub>2</sub> || DH<sub>3</sub> || SS)</p><p>If the bundle does contain a <em>curve</em> one-time prekey, the calculation is modified to include an additional <em>DH</em>:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;DH<sub>4</sub> = DH(EK<sub>A</sub>, OPK<sub>B</sub>)<br> &nbsp;&nbsp;&nbsp;&nbsp;SK = KDF(DH<sub>1</sub> || DH<sub>2</sub> || DH<sub>3</sub> || DH<sub>4</sub> || SS)</p><p>After calculating <em>SK</em>, Alice deletes her ephemeral private key, the <em>DH</em> outputs, the shared secret <em>SS</em>, and the ciphertext <em>CT</em>.</p><p>Alice then calculates an “associated data” byte sequence <em>AD</em> that contains identity information for both parties:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;AD = EncodeEC(IK<sub>A</sub>) || EncodeEC(IK<sub>B</sub>)</p><p>Alice may optionally append additional information to <em>AD</em>, such as Alice and Bob’s usernames, certificates, or other identifying information.</p><p>Alice then sends Bob an initial message containing:</p><ul><li>Alice’s identity key <em>IK<sub>A</sub></em></li><li>Alice’s ephemeral key <em>EK<sub>A</sub></em></li><li>The <em>pqkem</em> ciphertext <em>CT</em> encapsulating <em>SS</em> for <em>PQPK<sub>B</sub></em></li><li>Identifiers stating which of Bob’s prekeys Alice used</li><li>An initial ciphertext encrypted with some AEAD encryption scheme <span data-cites="aead"><a href="#ref-aead" role="doc-biblioref">[5]</a></span> using <em>AD</em> as associated data and using an encryption key which is either <em>SK</em> or the output from some cryptographic PRF keyed by <em>SK</em>.</li></ul><p>The initial ciphertext is typically the first message in some post-PQXDH communication protocol. In other words, this ciphertext typically has two roles, serving as the first message within some post-PQXDH protocol, and as part of Alice’s PQXDH initial message.</p><p>The initial message must be encoded in an unambiguous format to avoid confusion of the message items by the recipient.</p><p>After sending this, Alice may continue using <em>SK</em> or keys derived from <em>SK</em> within the post-PQXDH protocol for communication with Bob, subject to the security considerations discussed in <a href="#security-considerations">Section 4</a>.</p><h2 id="receiving-the-initial-message">3.4. Receiving the initial message</h2><p>Upon receiving Alice’s initial message, Bob retrieves Alice’s identity key and ephemeral key from the message. Bob also loads his identity private key and the private key(s) corresponding to the signed prekeys and one-time prekeys Alice used.</p><p>Using these keys, Bob calculates <em>PQKEM-DEC(PQPK<sub>B</sub>, CT)</em> as the shared secret <em>SS</em> and repeats the <em>DH</em> and <em>KDF</em> calculations from the previous section to derive <em>SK</em>, and then deletes the <em>DH</em> values and <em>SS</em> values.</p><p>Bob then constructs the <em>AD</em> byte sequence using <em>IK<sub>A</sub></em> and <em>IK<sub>B</sub></em> as described in the previous section. Finally, Bob attempts to decrypt the initial ciphertext using <em>SK</em> and <em>AD</em>. If the initial ciphertext fails to decrypt, then Bob aborts the protocol and deletes <em>SK</em>.</p><p>If the initial ciphertext decrypts successfully, the protocol is complete for Bob. For forward secrecy, Bob deletes the ciphertext and any one-time prekey private key that was used. Bob may then continue using <em>SK</em> or keys derived from <em>SK</em> within the post-PQXDH protocol for communication with Alice subject to the security considerations discussed in <a href="#security-considerations">Section 4</a>.</p><h2 id="security-considerations">4. Security considerations</h2><p>The security of the composition of X3DH <span data-cites="x3dh"><a href="#ref-x3dh" role="doc-biblioref">[6]</a></span> with the Double Ratchet <span data-cites="doubleratchet"><a href="#ref-doubleratchet" role="doc-biblioref">[7]</a></span> was formally studied in <span data-cites="CCDGS20"><a href="#ref-CCDGS20" role="doc-biblioref">[8]</a></span> and proven secure under the Gap Diffie-Hellman assumption (GDH)<span data-cites="OP01"><a href="#ref-OP01" role="doc-biblioref">[9]</a></span>. PQXDH composed with the Double Ratchet retains this security against an adversary without access to a quantum computer, but strengthens the security of the initial handshake to require the solution of both GDH and Module-LWE <span data-cites="LS15"><a href="#ref-LS15" role="doc-biblioref">[10]</a></span>. The remainder of this section discusses an incomplete list of further security considerations.</p><h2 id="authentication">4.1. Authentication</h2><p>Before or after a PQXDH key agreement, the parties may compare their identity public keys <em>IK<sub>A</sub></em> and <em>IK<sub>B</sub></em> through some authenticated channel. For example, they may compare public key fingerprints manually, or by scanning a QR code. Methods for doing this are outside the scope of this document.</p><p>Authentication in PQXDH is not quantum-secure. In the presence of an active quantum adversary, the parties receive no cryptographic guarantees as to who they are communicating with. Post-quantum secure deniable mutual authentication is an open research problem which we hope to address with a future revision of this protocol.</p><p>If authentication is not performed, the parties receive no cryptographic guarantee as to who they are communicating with.</p><h2 id="protocol-replay">4.2. Protocol replay</h2><p>If Alice’s initial message doesn’t use a one-time prekey, it may be replayed to Bob and he will accept it. This could cause Bob to think Alice had sent him the same message (or messages) repeatedly.</p><p>To mitigate this, a post-PQXDH protocol may wish to quickly negotiate a new encryption key for Alice based on fresh random input from Bob. This is the typical behavior of Diffie-Hellman-based ratcheting protocols <span data-cites="doubleratchet"><a href="#ref-doubleratchet" role="doc-biblioref">[7]</a></span>.</p><p>Bob could attempt other mitigations, such as maintaining a blacklist of observed messages, or replacing old signed prekeys more rapidly. Analyzing these mitigations is beyond the scope of this document.</p><h2 id="replay-and-key-reuse">4.3. Replay and key reuse</h2><p>Another consequence of the replays discussed in the previous section is that a successfully replayed initial message would cause Bob to derive the same <em>SK</em> in different protocol runs.</p><p>For this reason, any post-PQXDH protocol that uses <em>SK</em> to derive encryption keys MUST take measures to prevent catastrophic key reuse. For example, Bob could use a DH-based ratcheting protocol to combine <em>SK</em> with a freshly generated <em>DH</em> output to get a randomized encryption key <span data-cites="doubleratchet"><a href="#ref-doubleratchet" role="doc-biblioref">[7]</a></span>.</p><h2 id="deniability">4.4. Deniability</h2><p>Informally, cryptographic deniability means that a protocol neither gives its participants a publishable cryptographic proof of the contents of their communication nor proof of the fact that they communicated. PQXDH, like X3DH, aims to provide both Alice and Bob deniablilty that they communicated with each other in a context where a “judge” who may have access to one or more party’s secret keys is presented with a transcript allegedly created by communication between Alice and Bob.</p><p>We focus on offline deniability because if either party is collaborating with a third party during protocol execution, they will be able to provide proof of their communication to such a third party. This limitation on “online” deniability appears to be intrinsic to the asynchronous setting <span data-cites="unger"><a href="#ref-unger" role="doc-biblioref">[11]</a></span>.</p><p>PQXDH has some forms of cryptographic deniability. Motivated by the goals of X3DH, Brendel et al. <span data-cites="bfgjs21"><a href="#ref-bfgjs21" role="doc-biblioref">[12]</a></span> introduce a notion of 1-out-of-2 deniability for semi-honest parties and a “big brother” judge with access to all parties’ secret keys. Since either Alice or Bob can create a fake transcript using only their own secret keys, PQXDH has this deniability property. Vatandas, et al. <span data-cites="vgik21"><a href="#ref-vgik21" role="doc-biblioref">[13]</a></span> prove that X3DH is deniable in a different sense subject to certain “Knowledge of Diffie-Hellman Assumptions”. PQXDH is deniable in this sense for Alice, subject to the same assumptions, and we conjecture that it is deniable for Bob subject to an additional Plaintext Awareness (PA) assumption for <em>pqkem</em>. We note that Kyber uses a variant of the Fujisaki-Okamoto transform with implicit rejection <span data-cites="hhk17"><a href="#ref-hhk17" role="doc-biblioref">[14]</a></span> and is therefore not PA as is. However, in PQXDH, an AEAD ciphertext encrypted with the session key is always sent along with the Kyber ciphertext. This should offer the same guarantees as PA. We encourage the community to investigate the precise deniability properties of PQXDH.</p><p>These assertions all pertain to deniability in the classical setting. As discussed in <span data-cites="hkkp21"><a href="#ref-hkkp21" role="doc-biblioref">[15]</a></span> we expect that for future revisions of this protocol (that provide post-quantum mutual authentication) assertions about deniability against semi-honest quantum advsersaries will hold. Deniability in the face of malicious quantum adversaries requires further research.</p><h2 id="signatures">4.5. Signatures</h2><p>It might be tempting to omit the prekey signature after observing that mutual authentication and forward secrecy are achieved by the <em>DH</em> calculations. However, this would allow a “weak forward secrecy” attack: A malicious server could provide Alice a prekey bundle with forged prekeys, and later compromise Bob’s <em>IK<sub>B</sub></em> to calculate <em>SK</em>.</p><p>Alternatively, it might be tempting to replace the DH-based mutual authentication (i.e.&nbsp;<em>DH<sub>1</sub></em> and <em>DH<sub>2</sub></em>) with signatures from the identity keys. However, this reduces deniability, increases the size of initial messages, and increases the damage done if ephemeral or prekey private keys are compromised, or if the signature scheme is broken.</p><h2 id="key-compromise">4.6. Key compromise</h2><p>Compromise of a party’s private keys has a disastrous effect on security, though the use of ephemeral keys and prekeys provides some mitigation.</p><p>Compromise of a party’s identity private key allows impersonation of that party to others. Compromise of a party’s prekey private keys may affect the security of older or newer <em>SK</em> values, depending on many considerations.</p><p>A full analysis of all possible compromise scenarios is outside the scope of this document, however a partial analysis of some plausible scenarios is below:</p><ul><li><p>If either an elliptic curve one-time prekey (<em>OPK<sub>B</sub></em>) or a post-quantum key encapsulation one-time prekey (<em>PQOPK<sub>B</sub></em>) are used for a protocol run and deleted as specified, then a compromise of Bob’s identity key and prekey private keys at some future time will not compromise the older <em>SK</em>.</p></li><li><p>If one-time prekeys were not used for a protocol run, then a compromise of the private keys for <em>IK<sub>B</sub></em>, <em>SPK<sub>B</sub></em>, and <em>PQSPK<sub>B</sub></em> from that protocol run would compromise the <em>SK</em> that was calculated earlier. Frequent replacement of signed prekeys mitigates this, as does using a post-PQXDH ratcheting protocol which rapidly replaces <em>SK</em> with new keys to provide fresh forward secrecy <span data-cites="doubleratchet"><a href="#ref-doubleratchet" role="doc-biblioref">[7]</a></span>.</p></li><li><p>Compromise of prekey private keys may enable attacks that extend into the future, such as passive calculation of <em>SK</em> values, and impersonation of arbitrary other parties to the compromised party (“key-compromise impersonation”). These attacks are possible until the compromised party replaces his compromised prekeys on the server (in the case of passive attack); or deletes his compromised signed prekey’s private key (in the case of key-compromise impersonation).</p></li></ul><h2 id="passive-quantum-adversaries">4.7. Passive quantum adversaries</h2><p>PQXDH is designed to prevent “harvest now, decrypt later” attacks by adversaries with access to a quantum computer capable of computing discrete logarithms in <em>curve</em>.</p><ul><li><p>If an attacker has recorded the public information and the message from Alice to Bob, even access to a quantum computer will not compromise <em>SK</em>.</p></li><li><p>If a post-quantum key encapsulation one-time prekey (<em>PQOPK<sub>B</sub></em>) is used for a protocol run and deleted as specified then compromise after deletion and access to a quantum computer at some future time will not compromise the older <em>SK</em>.</p></li><li><p>If post-quantum one-time prekeys were not used for a protocol run, then access to a quantum computer and a compromise of the private key for <em>PQSPK<sub>B</sub></em> from that protocol run would compromise the <em>SK</em> that was calculated earlier. Frequent replacement of signed prekeys mitigates this, as does using a post-PQXDH ratcheting protocol which rapidly replaces <em>SK</em> with new keys to provide fresh forward secrecy <span data-cites="doubleratchet"><a href="#ref-doubleratchet" role="doc-biblioref">[7]</a></span>.</p></li></ul><h2 id="active-quantum-adversaries">4.8. Active quantum adversaries</h2><p>PQXDH is not designed to provide protection against active quantum attackers. An active attacker with access to a quantum computer capable of computing discrete logarithms in <em>curve</em> can compute <em>DH(PK<sub>1</sub>, PK<sub>2</sub>)</em> and <em>Sig(PK, M, Z)</em> for all elliptic <em>curve</em> keys <em>PK<sub>1</sub></em>, <em>PK<sub>2</sub></em>, and <em>PK</em>. This allows an attacker to impersonate Alice by using the quantum computer to compute the secret key corresponding to <em>PK<sub>A</sub></em> then continuing with the protocol. A malicious server with access to such a quantum computer could impersonate Bob by generating new key pairs <em>PQSPK’<sub>B</sub></em> and <em>PQOPK’<sub>B</sub></em>, computing the secret key corresponding to <em>PK<sub>B</sub></em>, then using <em>PK<sub>B</sub></em> to sign the newly generated post-quantum KEM keys and delivering these attacker-generated keys in place of Bob’s post-quantum KEM key when Alice requests a prekey bundle.</p><p>It is tempting to consider adding a post-quantum identity key that Bob could use to sign the post-quantum prekeys. This would prevent the malicious server attack described above and provide Alice a cryptographic guarantee that she is communicating with Bob, but it does not provide mutual authentication. Bob does not have any cryptographic guarantee about who he is communicating with. The post-quantum KEM and signature schemes being standardized by NIST <span data-cites="nistpostquantum"><a href="#ref-nistpostquantum" role="doc-biblioref">[16]</a></span> do not provide a mechanism for post-quantum deniable mutual authentication, although this can be achieved through the use of a post-quantum ring signature or designated verifier signature <span data-cites="bfgjs21"><a href="#ref-bfgjs21" role="doc-biblioref">[12]</a></span>, <span data-cites="hkkp21"><a href="#ref-hkkp21" role="doc-biblioref">[15]</a></span>. We urge the community to work toward standardization of these or other mechanisms that will allow deniable mutual authentication.</p><h2 id="server-trust">4.9. Server trust</h2><p>A malicious server could cause communication between Alice and Bob to fail (e.g.&nbsp;by refusing to deliver messages).</p><p>If Alice and Bob authenticate each other as in <a href="#authentication">Section 4.1</a>, then the only additional attack available to the server is to refuse to hand out one-time prekeys, causing forward secrecy for <em>SK</em> to depend on the signed prekey’s lifetime (as analyzed in <a href="#key-compromise">Section 4.6</a>).</p><p>This reduction in initial forward secrecy could also happen if one party maliciously drains another party’s one-time prekeys, so the server should attempt to prevent this (e.g.&nbsp;with rate limits on fetching prekey bundles).</p><h2 id="identity-binding">4.10. Identity binding</h2><p>Authentication as in <a href="#authentication">Section 4.1</a> does not necessarily prevent an “identity misbinding” or “unknown key share” attack.</p><p>This results when an attacker (“Charlie”) falsely presents Bob’s identity key fingerprint to Alice as his (Charlie’s) own, and then either forwards Alice’s initial message to Bob, or falsely presents Bob’s contact information as his own. The effect of this is that Alice thinks she is sending an initial message to Charlie when she is actually sending it to Bob.</p><p>To make this more difficult the parties can include more identifying information into <em>AD</em>, or hash more identifying information into the fingerprint, such as usernames, phone numbers, real names, or other identifying information. Charlie would be forced to lie about these additional values, which might be difficult.</p><p>However, there is no way to reliably prevent Charlie from lying about additional values, and including more identity information into the protocol often brings trade-offs in terms of privacy, flexibility, and user interface. A detailed analysis of these trade-offs is beyond the scope of this document.</p><h2 id="risks-of-weak-randomness-sources">4.11. Risks of weak randomness sources</h2><p>In addition to concerns about the generation of the keys themselves, the security of the PQKEM shared secret relies on the random source available to Alice’s machine at the time of running the <strong><em>PQKEM-ENC</em></strong> operation. This leads to a situation similar to what we face with a Diffie-Hellman exchange. For both Diffie-Hellman and Kyber, if Alice has weak entropy then the resulting shared secret will have low entropy when conditioned on Bob’s public key. Thus both the classical and post-quantum security of <em>SK</em> depend on the strength of Alice’s random source.</p><p>Kyber hashes Bob’s public key with Alice’s random bits to generate the shared secret, making Bob’s key contributory, as it is with a Diffie-Hellman key exchange. This does not reduce the dependence on Alice’s entropy source, as described above, but it does limit Alice’s ability to control the post-quantum shared secret. Not all KEMs make Bob’s key contributory and this is a property to consider when selecting <em>pqkem</em>.</p><h2 id="ipr">5. IPR</h2><p>This document is hereby placed in the public domain.</p><h2 id="acknowledgements">6. Acknowledgements</h2><p>The PQXDH protocol was developed by Ehren Kret and Rolfe Schmidt as an extension of the X3DH protocol <span data-cites="x3dh"><a href="#ref-x3dh" role="doc-biblioref">[6]</a></span> by Moxie Marlinspike and Trevor Perrin. Thanks to Trevor Perrin for discussions on the design of this protocol.</p><p>Thanks to Bas Westerbaan, Chris Peikert, Daniel Collins, Deirdre Connolly, John Schanck, Jon Millican, Jordan Rose, Karthik Bhargavan, Loïs Huguenin-Dumittan, Peter Schwabe, Rune Fiedler, Shuichi Katsumata, Sofía Celi, and Yo’av Rieck for helpful discussions and editorial feedback.</p><p>Thanks to the Kyber team <span data-cites="kyberweb"><a href="#ref-kyberweb" role="doc-biblioref">[17]</a></span> for their work on the Kyber key encapsulation mechanism.</p><h2 id="references">7. References</h2><div id="refs" role="list"><div id="ref-rfc7748" role="listitem"><p>[3]</p><p>A. Langley, M. Hamburg, and S. Turner, <span>“<span>Elliptic Curves for Security</span>.”</span> Internet Engineering Task Force; RFC 7748 (Informational); IETF, Jan-2016. <a href="http://www.ietf.org/rfc/rfc7748.txt">http://www.ietf.org/rfc/rfc7748.txt</a></p></div><div id="ref-rfc5869" role="listitem"><p>[4]</p><p>H. Krawczyk and P. Eronen, <span>“<span>HMAC-based Extract-and-Expand Key Derivation Function (HKDF)</span>.”</span> Internet Engineering Task Force; RFC 5869 (Informational); IETF, May-2010. <a href="http://www.ietf.org/rfc/rfc5869.txt">http://www.ietf.org/rfc/rfc5869.txt</a></p></div><div id="ref-aead" role="listitem"><p>[5]</p><p>P. Rogaway, <span>“<span>Authenticated-encryption with Associated-data</span>,”</span> in <span>Proceedings of the 9th <span>ACM</span> Conference on Computer and Communications Security</span>, 2002. <a href="http://web.cs.ucdavis.edu/~rogaway/papers/ad.pdf">http://web.cs.ucdavis.edu/~rogaway/papers/ad.pdf</a></p></div><div id="ref-CCDGS20" role="listitem"><p>[8]</p><p>K. Cohn-Gordon, C. Cremers, B. Dowling, L. Garratt, and D. Stebila, <span>“A formal security analysis of the signal messaging protocol,”</span> J. Cryptol., vol. 33, no. 4, 2020. <a href="https://doi.org/10.1007/s00145-020-09360-1">https://doi.org/10.1007/s00145-020-09360-1</a></p></div><div id="ref-OP01" role="listitem"><p>[9]</p><p>T. Okamoto and D. Pointcheval, <span>“The gap-problems: A new class of problems for the security of cryptographic schemes,”</span> in Proceedings of the 4th international workshop on practice and theory in public key cryptography: Public key cryptography, 2001.</p></div><div id="ref-LS15" role="listitem"><p>[10]</p><p>A. Langlois and D. Stehlé, <span>“Worst-case to average-case reductions for module lattices,”</span> Des. Codes Cryptography, vol. 75, no. 3, Jun. 2015. <a href="https://doi.org/10.1007/s10623-014-9938-4">https://doi.org/10.1007/s10623-014-9938-4</a></p></div><div id="ref-unger" role="listitem"><p>[11]</p><p>N. Unger and I. Goldberg, <span>“<span>Deniable Key Exchanges for Secure Messaging</span>,”</span> in <span>Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</span>, 2015. <a href="https://cypherpunks.ca/~iang/pubs/dake-ccs15.pdf">https://cypherpunks.ca/~iang/pubs/dake-ccs15.pdf</a></p></div><div id="ref-bfgjs21" role="listitem"><p>[12]</p><p>J. Brendel, R. Fiedler, F. Günther, C. Janson, and D. Stebila, <span>“Post-quantum asynchronous deniable key exchange and the signal handshake,”</span> in Public-key cryptography - <span>PKC</span> 2022 - 25th <span>IACR</span> international conference on practice and theory of public-key cryptography, virtual event, march 8-11, 2022, proceedings, part <span>II</span>, 2022, vol. 13178. <a href="https://doi.org/10.1007/978-3-030-97131-1_1">https://doi.org/10.1007/978-3-030-97131-1_1</a></p></div><div id="ref-vgik21" role="listitem"><p>[13]</p><p>N. Vatandas, R. Gennaro, B. Ithurburn, and H. Krawczyk, <span>“On the cryptographic deniability of the signal protocol,”</span> in Applied cryptography and network security - 18th international conference, <span>ACNS</span> 2020, rome, italy, october 19-22, 2020, proceedings, part <span>II</span>, 2020, vol. 12147. <a href="https://doi.org/10.1007/978-3-030-57878-7_10">https://doi.org/10.1007/978-3-030-57878-7_10</a></p></div><div id="ref-hhk17" role="listitem"><p>[14]</p><p>D. Hofheinz, K. Hövelmanns, and E. Kiltz, <span>“A modular analysis of the fujisaki-okamoto transformation,”</span> in Theory of cryptography - 15th international conference, <span>TCC</span> 2017, baltimore, MD, USA, november 12-15, 2017, proceedings, part <span>I</span>, 2017, vol. 10677. <a href="https://doi.org/10.1007/978-3-319-70500-2_12">https://doi.org/10.1007/978-3-319-70500-2_12</a></p></div><div id="ref-hkkp21" role="listitem"><p>[15]</p><p>K. Hashimoto, S. Katsumata, K. Kwiatkowski, and T. Prest, <span>“An efficient and generic construction for signal’s handshake <span>(X3DH):</span> Post-quantum, state leakage secure, and deniable,”</span> J. Cryptol., vol. 35, no. 3, 2022. <a href="https://doi.org/10.1007/s00145-022-09427-1">https://doi.org/10.1007/s00145-022-09427-1</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using enums to represent state in Rust (105 pts)]]></title>
            <link>https://corrode.dev/blog/enums/</link>
            <guid>37610375</guid>
            <pubDate>Fri, 22 Sep 2023 10:48:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corrode.dev/blog/enums/">https://corrode.dev/blog/enums/</a>, See on <a href="https://news.ycombinator.com/item?id=37610375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Many Rust beginners with a background in systems programming tend to use <code>bool</code>
(or even <code>u8</code> — an 8-bit unsigned integer type) to represent <em>"state"</em>.</p>
<p>For example, how about a <code>bool</code> to indicate whether a user is active or not?</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>User</span> </span><span><span><span>{</span>
    <span> ...
</span>    <span>active</span><span>:</span> <span>bool</span>,
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Initially, this might seem fine, but as your codebase grows,
you'll find that "active" is not a binary state. There are many
different states that a user can be in. For example, a user
might be suspended or deleted. However, extending the user struct
can get problematic, because other parts of the code might
rely on the fact that <code>active</code> is a <code>bool</code>.</p>
<p>Another problem is that <code>bool</code> is not self-documenting. What does
<code>active = false</code> mean? Is the user inactive? Or is the user
deleted? Or is the user suspended? We don't know!</p>
<p>Alternatively, you could use an unsigned integer to represent state:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>User</span> </span><span><span><span>{</span>
    <span> ...
</span>    <span>status</span><span>:</span> <span>u8</span>,
</span><span><span>}</span></span></span>
</span></code></pre>
<p>This is <em>slightly</em> better, because we can now use different values
to represent more states:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>const</span> <span>ACTIVE</span><span>:</span> <span>u8</span> <span>=</span> <span>0</span><span>;</span>
<span>const</span> <span>INACTIVE</span><span>:</span> <span>u8</span> <span>=</span> <span>1</span><span>;</span>
<span>const</span> <span>SUSPENDED</span><span>:</span> <span>u8</span> <span>=</span> <span>2</span><span>;</span>
<span>const</span> <span>DELETED</span><span>:</span> <span>u8</span> <span>=</span> <span>3</span><span>;</span>

<span>let</span> user <span>=</span> User <span><span>{</span>
    <span><span>//</span> ...
</span>    status<span>:</span> <span>ACTIVE</span><span>,</span>
</span><span><span>}</span></span><span>;</span>
</span></code></pre>
<p>A common use-case for <code>u8</code> is when you interface with C code.
In that case, using <code>u8</code> might seemingly be the only option.
However, we could still wrap that <code>u8</code> in a
<a href="https://doc.rust-lang.org/rust-by-example/generics/new_types.html">newtype</a>!</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>User</span> </span><span><span><span>{</span>
    <span> ...
</span>    <span>status</span><span>:</span> UserStatus,
</span><span><span>}</span></span></span>

<span><span>struct</span> </span><span><span>UserStatus</span></span><span><span><span>(</span><span>u8</span></span><span>)</span></span><span>;</span>

<span>const</span> <span>ACTIVE</span><span>:</span> UserStatus <span>=</span> UserStatus<span><span>(</span><span>0</span></span><span><span>)</span></span><span>;</span>
<span>const</span> <span>INACTIVE</span><span>:</span> UserStatus <span>=</span> UserStatus<span><span>(</span><span>1</span></span><span><span>)</span></span><span>;</span>
<span>const</span> <span>SUSPENDED</span><span>:</span> UserStatus <span>=</span> UserStatus<span><span>(</span><span>2</span></span><span><span>)</span></span><span>;</span>
<span>const</span> <span>DELETED</span><span>:</span> UserStatus <span>=</span> UserStatus<span><span>(</span><span>3</span></span><span><span>)</span></span><span>;</span>

<span>let</span> user <span>=</span> User <span><span>{</span>
    <span><span>//</span> ...
</span>    status<span>:</span> <span>ACTIVE</span><span>,</span>
</span><span><span>}</span></span><span>;</span>
</span></code></pre>
<p>This way, we can still use <code>u8</code> to represent state, but we can
now also put the type system to work (a common pattern in idiomatic Rust). For
example, we can define methods on <code>UserStatus</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>UserStatus</span> </span><span><span><span>{</span>
    <span><span><span>fn</span> </span><span>is_active</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>bool</span></span> </span><span><span><span>{</span>
        <span>self</span><span>.</span><span>0</span> <span>==</span> <span>ACTIVE</span><span>.</span><span>0</span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>And we can even define a constructor that validates the input:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>UserStatus</span> </span><span><span><span>{</span>
    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>status</span><span>:</span> <span>u8</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>Self</span>, <span>&amp;</span><span>'static</span> <span>str</span><span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span>match</span> status <span><span>{</span>
            <span>ACTIVE</span><span>.</span><span>0</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>ACTIVE</span></span><span><span>)</span></span><span>,</span>
            <span>INACTIVE</span><span>.</span><span>0</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>INACTIVE</span></span><span><span>)</span></span><span>,</span>
            <span>SUSPENDED</span><span>.</span><span>0</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>SUSPENDED</span></span><span><span>)</span></span><span>,</span>
            <span>DELETED</span><span>.</span><span>0</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>DELETED</span></span><span><span>)</span></span><span>,</span>
            <span>_</span> <span>=&gt;</span> <span>Err</span><span><span>(</span><span><span>"</span>Invalid status<span>"</span></span></span><span><span>)</span></span><span>,</span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>It's still not ideal, however! Not even if you interface with C code, as we will
see in a bit. But first, let's look at the recommended way to represent state in Rust.</p>
<h2 id="use-enums-instead">Use Enums Instead!</h2>
<p><strong>Enums are a great way to model state inside your domain.</strong> They allow you to
express your intent in a very concise way. </p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Debug</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>pub</span> <span>enum</span> <span>UserStatus</span> <span><span>{</span>
    <span> The user is active and has full access
</span>    <span> to their account and any associated features.
</span>    Active<span>,</span>
    
    <span> The user's account is inactive.
</span>    <span> This state can be reverted to active by
</span>    <span> the user or an administrator.
</span>    Inactive<span>,</span>
    
    <span> The user's account has been temporarily suspended, 
</span>    <span> possibly due to suspicious activity or policy violations.
</span>    <span> During this state, the user cannot access their account,
</span>    <span> and an administrator's intervention might
</span>    <span> be required to restore the account.
</span>    Suspended<span>,</span>
    
    <span> The user's account has been permanently 
</span>    <span> deleted and cannot be restored.
</span>    <span> All associated data with the account might be 
</span>    <span> removed, and the user would need to create a new account
</span>    <span> to use the service again.
</span>    Deleted<span>,</span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>We can plug this enum into our <code>User</code> struct:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>User</span> </span><span><span><span>{</span>
    <span> ...
</span>    <span>status</span><span>:</span> UserStatus,
</span><span><span>}</span></span></span>
</span></code></pre>
<p>But that's not all; in Rust, enums are much more powerful than in many other
languages. For example, we can add data to our enum variants:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Debug</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>pub</span> <span>enum</span> <span>UserStatus</span> <span><span>{</span>
    Active<span>,</span>
    Inactive<span>,</span>
    Suspended <span><span>{</span> until<span>:</span> <span>DateTime<span>&lt;</span>Utc<span>&gt;</span></span> </span><span><span>}</span></span><span>,</span>
    Deleted <span><span>{</span> deleted_at<span>:</span> <span>DateTime<span>&lt;</span>Utc<span>&gt;</span></span> </span><span><span>}</span></span><span>,</span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>We can even represent <strong>state transitions</strong>:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>use</span> <span>chrono<span>::</span></span><span><span>{</span>DateTime<span>,</span> Utc</span><span><span>}</span></span><span>;</span>

<span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Debug</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>pub</span> <span>enum</span> <span>UserStatus</span> <span><span>{</span>
    Active<span>,</span>
    Inactive<span>,</span>
    Suspended <span><span>{</span> until<span>:</span> <span>DateTime<span>&lt;</span>Utc<span>&gt;</span></span> </span><span><span>}</span></span><span>,</span>
    Deleted <span><span>{</span> deleted_at<span>:</span> <span>DateTime<span>&lt;</span>Utc<span>&gt;</span></span> </span><span><span>}</span></span><span>,</span>
</span><span><span>}</span></span></span>

<span><span>impl</span> </span><span><span>UserStatus</span> </span><span><span><span>{</span>
    <span> Suspend the user until the given date.
</span>    <span><span><span>fn</span> </span><span>suspend</span></span><span><span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span>, <span>until</span><span>:</span> <span>DateTime<span>&lt;</span>Utc<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
        <span>match</span> <span>self</span> <span><span>{</span>
            <span>UserStatus<span>::</span></span>Active <span>=&gt;</span> <span>*</span><span>self</span> <span>=</span> <span>UserStatus<span>::</span></span>Suspended <span><span>{</span> until </span><span><span>}</span></span><span>,</span>
            <span>_</span> <span>=&gt;</span> <span><span>{</span></span><span><span>}</span></span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span></span>

    <span> Activate the user.
</span>    <span><span><span>fn</span> </span><span>activate</span></span><span><span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>(</span><span>)</span>, <span>&amp;</span><span>'static</span> <span>str</span><span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span>match</span> <span>self</span> <span><span>{</span>
                        <span>UserStatus<span>::</span></span>Deleted <span><span>{</span> <span>..</span> </span><span><span>}</span></span> <span>=&gt;</span> <span>return</span> <span>Err</span><span><span>(</span><span><span>"</span>can't activate a deleted user<span>"</span></span></span><span><span>)</span></span><span>,</span>
            <span>_</span> <span>=&gt;</span> <span>*</span><span>self</span> <span>=</span> <span>UserStatus<span>::</span></span>Active
        </span><span><span>}</span></span>
        <span>Ok</span><span><span>(</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span>

    <span> Delete the user. This is a permanent action!
</span>    <span><span><span>fn</span> </span><span>delete</span></span><span><span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
        <span>if</span> <span>let</span> <span>UserStatus<span>::</span></span>Deleted <span><span>{</span> <span>..</span> </span><span><span>}</span></span> <span>=</span> <span>self</span> <span><span>{</span>
                        <span>return</span><span>;</span>
        </span><span><span>}</span></span>
        <span>*</span><span>self</span> <span>=</span> <span>UserStatus<span>::</span></span>Deleted <span><span>{</span>
            deleted_at<span>:</span> <span>Utc<span>::</span></span>now<span><span>(</span></span><span><span>)</span></span><span>,</span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span></span>

    <span><span><span>fn</span> </span><span>is_active</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>bool</span></span> </span><span><span><span>{</span>
        <span>matches!</span><span><span>(</span><span>self</span><span>,</span> <span>UserStatus<span>::</span></span>Active</span><span><span>)</span></span>
    </span><span><span>}</span></span></span>

    <span><span><span>fn</span> </span><span>is_suspended</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>bool</span></span> </span><span><span><span>{</span>
        <span>matches!</span><span><span>(</span><span>self</span><span>,</span> <span>UserStatus<span>::</span></span>Suspended <span><span>{</span> <span>..</span> </span><span><span>}</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span>

    <span><span><span>fn</span> </span><span>is_deleted</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>bool</span></span> </span><span><span><span>{</span>
        <span>matches!</span><span><span>(</span><span>self</span><span>,</span> <span>UserStatus<span>::</span></span>Deleted <span><span>{</span> <span>..</span> </span><span><span>}</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>

<span><span>#</span><span>[</span><span>cfg</span><span><span><span>(</span></span></span><span><span>test</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>mod</span> <span>tests</span> <span><span>{</span>
    <span>use</span> <span>chrono<span>::</span></span>Duration<span>;</span>
    <span>use</span> <span><span>super</span><span>::</span></span><span>*</span><span>;</span>

    <span><span>#</span><span>[</span><span>test</span><span>]</span></span>
    <span><span><span>fn</span> </span><span>test_user_status</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>(</span><span>)</span>, <span>&amp;</span><span>'static</span> <span>str</span><span>&gt;</span></span></span></span><span><span><span>{</span>
        <span>let</span> <span>mut</span> status <span>=</span> <span>UserStatus<span>::</span></span>Active<span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_active</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
                status<span>.</span><span>suspend</span><span><span>(</span><span>Utc<span>::</span></span>now<span><span>(</span></span><span><span>)</span></span> <span>+</span> <span>Duration<span>::</span></span>days<span><span>(</span><span>1</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_suspended</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
        status<span>.</span><span>activate</span><span><span>(</span></span><span><span>)</span></span><span>?</span><span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_active</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
        status<span>.</span><span>delete</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_deleted</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
        <span>Ok</span><span><span>(</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span>

    <span><span>#</span><span>[</span><span>test</span><span>]</span></span>
    <span><span><span>fn</span> </span><span>test_user_status_transition</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
        <span>let</span> <span>mut</span> status <span>=</span> <span>UserStatus<span>::</span></span>Active<span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_active</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
        status<span>.</span><span>delete</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
        <span>assert!</span><span><span>(</span>status<span>.</span><span>is_deleted</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
                <span>assert!</span><span><span>(</span>status<span>.</span><span>activate</span><span><span>(</span></span><span><span>)</span></span><span>.</span><span>is_err</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Look how much ground we've covered with just a few lines of code!
We can extend the application with confidence, knowing that
we can't accidentally delete a user twice or re-activate a deleted user.
<a href="https://corrode.dev/blog/illegal-state">Illegal state transitions are now impossible!</a></p>
<h2 id="using-enums-to-interact-with-c-code">Using Enums to Interact with C Code</h2>
<p>Earlier, I promised that you can still use enums, even if you have to interact
with C code.</p>
<p>Suppose you have a C library with a user status type (I've omitted the other
fields for brevity).</p>
<pre data-lang="c"><code data-lang="c"><span><span>typedef</span> <span>struct</span> <span><span>{</span>
    <span>uint8_t</span> status<span>;</span>
<span>}</span></span> <span>User</span><span>;</span>

User <span>*</span><span><span>create_user</span></span><span><span><span>(</span></span></span><span><span><span>uint8_t</span> <span>status</span><span>)</span></span></span><span>;</span>
</span></code></pre>
<p>You can write a Rust enum to represent the status:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>repr</span><span><span><span>(</span></span></span><span><span>u8</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Debug<span>,</span> PartialEq</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>pub</span> <span>enum</span> <span>UserStatus</span> <span><span>{</span>
    Active <span>=</span> <span>0</span><span>,</span>
    Inactive<span>,</span>
    Suspended<span>,</span>
    Deleted<span>,</span>
</span><span><span>}</span></span></span>

<span><span>impl</span> </span><span><span>TryFrom<span>&lt;</span><span>u8</span><span>&gt;</span></span> <span>for</span></span><span> <span>UserStatus</span> </span><span><span><span>{</span>
    <span>type</span> <span>Error</span> <span>=</span> <span><span>(</span></span><span><span>)</span></span><span>;</span>

    <span><span><span>fn</span> </span><span>try_from</span></span><span><span><span>(</span><span>value</span><span>:</span> <span>u8</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>Self</span>, <span><span><span>Self</span><span>::</span></span></span>Error<span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span>match</span> value <span><span>{</span>
            <span>0</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>UserStatus<span>::</span></span>Active</span><span><span>)</span></span><span>,</span>
            <span>1</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>UserStatus<span>::</span></span>Inactive</span><span><span>)</span></span><span>,</span>
            <span>2</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>UserStatus<span>::</span></span>Suspended</span><span><span>)</span></span><span>,</span>
            <span>3</span> <span>=&gt;</span> <span>Ok</span><span><span>(</span><span>UserStatus<span>::</span></span>Deleted</span><span><span>)</span></span><span>,</span>
            <span>_</span> <span>=&gt;</span> <span>Err</span><span><span>(</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Noticed that <code>#[repr(u8)]</code> attribute? It tells the compiler to represent this
enum as an unsigned 8-bit integer. This is critical for compatibility with the C
code.</p>
<p>Now, let's wrap the C function in a safe Rust wrapper:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>extern</span> <span><span>"</span>C<span>"</span></span> <span><span>{</span>
    <span><span><span>fn</span> </span><span>create_user</span></span><span><span><span>(</span><span>status</span><span>:</span> <span>u8</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>*mut</span> User</span></span><span>;</span>
</span><span><span>}</span></span>

<span><span><span>pub</span> <span>fn</span> </span><span>create_user_wrapper</span></span><span><span><span>(</span><span>status</span><span>:</span> UserStatus</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span>User, <span>&amp;</span><span>'static</span> <span>str</span><span>&gt;</span></span></span> </span><span><span><span>{</span>
    <span>let</span> user <span>=</span> <span>unsafe</span> <span><span>{</span> <span>create_user</span><span><span>(</span>status <span>as</span> <span>u8</span></span><span><span>)</span></span> </span><span><span>}</span></span><span>;</span>
    <span>if</span> user<span>.</span><span>is_null</span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
        <span>Err</span><span><span>(</span><span><span>"</span>Failed to create user<span>"</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span> <span>else</span> <span><span>{</span>
        <span>Ok</span><span><span>(</span><span>unsafe</span> <span><span>{</span> <span>*</span><span>Box</span><span><span>::</span></span>from_raw<span><span>(</span>user</span><span><span>)</span></span> </span><span><span>}</span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>The Rust code now communicates with the C code using a rich enum type, allowing
for more expressive and type-safe code.</p>
<p>If you want, you can play around with the code on the <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=8973e5e655c92c725f0b2b00f7830385">Rust
playground</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Enums in Rust are more powerful than in most other languages.
They can be used to elegantly represent state transitions —
even across language boundaries.</p>
<p>You should consider using enums whenever you need to represent a set of possible
values, like when representing the state of an object.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Age of the Grift Shift (233 pts)]]></title>
            <link>https://tante.cc/2023/09/21/the-age-of-the-grift-shift/</link>
            <guid>37610238</guid>
            <pubDate>Fri, 22 Sep 2023 10:25:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tante.cc/2023/09/21/the-age-of-the-grift-shift/">https://tante.cc/2023/09/21/the-age-of-the-grift-shift/</a>, See on <a href="https://news.ycombinator.com/item?id=37610238">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-4631">

	<div>

			<p>For a book proposal I am currently working on (German, no proposal isn’t done yet because I keep reworking stuff, my agent hates me) I am thinking a lot about late stage capitalism and technologies, about how the kinda terminal economic system shapes the technologies it brings forward etc. And there are of course a lot of ways one can tackle that topic: You can do very abstract theoretical approaches or rather argue from examples or mix and match etc.</p>



<p>So a few weeks ago while going through my “to read pile” I stumbled on <a href="https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift">an article</a> that itself isn’t super relevant or special but it gave me a very useful phrase to chew on for the last weeks: The “grift shift”. In that article the term is used in reference to a whole bunch of VC firms who – after crypto/web3 had imploded – seamlessly rebranded themselves as “AI” firms. And if you’ve heard me speak in the last few months you know me often opening with jokingly referencing the same dynamic on Linkedin where the crypto influencers turned into Metaverse influencers and now have turned into “AI” influencers.</p>



<p>Very obviously I agree with the analysis: There is a massive amount of people and institutions jumping from one grift to the next to the next. But that’s not necessarily new: Especially when it comes to investments it’s just natural to follow the hype, to try to ride the wave wherever it leads you to get the next payday. </p>



<p>But I think there’s more going on here. Because it’s not just investors or their little helpers from the consulting firms: The whole tech discourse has shifted a bit in my subjective observation. So let’s try to define the “Grift Shift” in a more structural way. </p>



<blockquote>
<p>The Grift Shift is a new paradigm of debating technologies within a society that is based a lot less on the actual realistic use cases or properties of a certain technology but a surface level fascination with technologies but even more their narratives of future deliverance. Within the Grift Shift paradigm the topics and technologies addressed are mere material for public personalities to continuously claim expertise and “thought leadership” in every cycle of the shift regardless of what specific technologies are being talked about.</p>
</blockquote>



<p>Everybody can probably name a few people who have embraced that paradigm fully. People who effortlessly shift from “web3 is the future” to “I will explain to you why ‘AI’ will replace you”, people who get fame by talking about self driving cars and jump to superconductors the next week depending on whatever is sticky in the news. Again, it’s not like we never had these people but their relevance, their position of power, their influence has changed.</p>



<p>So I kept thinking about why that is. Like: Are these people just willing or clueless PR people for whatever capital wants to push on people (usually through their employers or sometimes even governments)? But while that is a function they have, I think that’s not the “why”. And I think the “why” is to a certain degree YouTube’s fault.</p>



<h3>Content</h3>



<p>A few years ago YouTube (of course others joined in and followed but I think YouTube was a leading force here) established the term “content”. It was no longer about the actual qualities of the medium, not about videos or music or stories or essays etc. Everything one made was just <em>content</em>.</p>



<p>This was an interesting development that underlined how strong our culture is already shaped by technological terminology: “Content” is an absolute abstraction, shedding everything specific about the objects it references. All meaning (and everything that goes beyond the most literal output, the most simplistic view on a created artifact) is stripped. No longer are people writers or filmmakers or musicians or whatever: They are “content creators”. Lumped together in spite of having radically different processes, subcultures, communities, values, traditions, etc. </p>



<p>“Content” is how programmers think about media when designing software systems to manage it. WordPress, the system I use to write and publish this website, is a so-called “content management system”. Which is technically correct but strips all the process of writing, editing, formatting from the narrative. But that is how you build a system to manage “articles”. You don’t care too much about how articles emerge from experience, thinking and sometimes pure accident. You care about the words and paragraphs. The long string of characters you need to store in a database. And that string could also be the binary representation of an image or a movie or whatnot. Who gives a shit?</p>



<p>“Content” is less about the creation. It’s about delivery and managing audience attention. YouTube’s algorithms reward certain behaviors (and when we look at how YouTube works, that seems to be mostly “be a right wing shithead yelling in the microphone about trans people”) and certain formal structures (as in length, structure of the preview image, title) but they don’t really care about anything. Just that whenever a person is on the site they get something to look at. And something else afterwards. And something else afterwards. It doesn’t matter. Just don’t leave or at least let the tab open so whe can autoplay some more ads.</p>



<p>There was a somewhat big scandal recently about a popular YouTube tech show doing shoddy work, abusing their workers and after trying to ignore it one excuse was the murderous schedule: “We can’t do good work because we need to put out a video every day.” Whether that explanation holds any water (in my experience people who abuse their workers and coworkers do that regardless of the stress) it’s telling how putting anything out every day has priority over what it is or means. “I need to say something every day, but I don’t have something to say every day” is a shitty position that pretty mush crushes a human soul after a while but the millions that channel made probably help to dry some tears. </p>



<p>Why am I talking about YouTube when we were talking about the grift shift? It’s not because everyone on YouTube is a grifter or that it is worse than many other platforms (TBH I feel like Linkedin is a bit worse and let’s not even talk about Twitter), it is because YouTube’s way of thinking about “content” has reshaped our tech discourse structurally. Which is a bit of the problem given how especially in western countries we basically outsourced “Future” to tech instead of also thinking about maybe for example political visions.</p>



<p>I think the Grift Shift is a symptom of the shift from actual media to content.</p>



<h3>Why so unserious?</h3>



<p>Our lives, our economies everything is increasingly shaped by digital technologies. Even activities within the physical realm require digital tools to participate. So talking about technology, explaining new technologies to the public who needs to form an opinion on them is gravely important. </p>



<p>But we’ve seemingly lost a bit of our ability to be  serious. Now I don’t mean that talking about technology (or politics or social issues for that matter) cannot be entertaining or have entertaining aspects to it. But we are not taking the subject matter at hand serious. </p>



<p>Technological development, potential innovations are published through advertising / PR into the world and as soon as a bit of momentum forms they get reduced to materials to generate content. Because there needs to be a video every day. And everyone else says that <s>crypto</s> <s>metaverse</s> AI is the future and you need to ride that wave. Click subscribe please.</p>



<p>The Grift Shift keeps going because we are not taking technological developments seriously as political forces. Which they are or are at least very aligned with. You can’t talk about “AI” and the wonders of whatever the newest paper or startup promises without looking at environmental impacts, about deskilling, about how these tools affect the real economy that we’re relying on to get people the money to pay for food and housing (granted: I don’t think that that basic idea is kinda dumb because every human being has a right to food, shelter, healthcare, community and participation, has a right to have their needs met in order for them to thrive but that is a whole different article I guess).</p>



<p>And that is what is so sad about it. The grift shift and all the people and organizations who embody it are just a sign that we’ve all developed “morbus contentcreatoris” or “content creator brain”. And that is very depressing.</p>



<p>Because basically nobody doing anything interesting is “creating content”. The things we write, the paintings we draw, the music we play isn’t just about what the <em>output </em>is. Every creative activity is in itself a value, with the output sometimes being almost secondary. This article isn’t about the words I typed or even the process of typing them. It’s about me thinking through things I see in the world through the lense of my understanding of it, my values, my feelings and wishes. This isn’t content.</p>



<p>I keep thinking about the grift shifters. But maybe I shouldn’t. Maybe even that is taking them to seriously. And why should I? Because they surely don’t seem to be very serious about anything. </p>
<div><p>Liked it? Take a second to support tante on Patreon!</p><p><a rel="nofollow" href="https://www.patreon.com/tante?utm_content=post_button&amp;utm_medium=patron_button_and_widgets_plugin&amp;utm_campaign=&amp;utm_term=&amp;utm_source=https://tante.cc/2023/09/21/the-age-of-the-grift-shift/" aria-label="Click to become a patron at Patreon!"><img decoding="async" src="https://tante.cc/wp-content/plugins/patron-button-and-widgets-by-codebard/images/become_a_patron_button.png" alt="Become a patron at Patreon!"></a></p></div>
			
		

	
		</div><!-- .container.container-small -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Turning an old car into a powerful generator (193 pts)]]></title>
            <link>https://blog.arduino.cc/2023/09/20/turning-an-old-car-into-a-powerful-generator/</link>
            <guid>37610230</guid>
            <pubDate>Fri, 22 Sep 2023 10:25:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.arduino.cc/2023/09/20/turning-an-old-car-into-a-powerful-generator/">https://blog.arduino.cc/2023/09/20/turning-an-old-car-into-a-powerful-generator/</a>, See on <a href="https://news.ycombinator.com/item?id=37610230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-35891">
<h3>Turning an old car into a powerful generator</h3>
<p> — <span>September 20th, 2023</span>
</p>
<div>
<figure><p><img loading="lazy" width="1024" height="609" src="https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-1024x609.jpg" alt="" srcset="https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-1024x609.jpg 1024w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-300x179.jpg 300w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-768x457.jpg 768w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-1536x914.jpg 1536w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-3-2048x1219.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></p></figure>
<p>Generators are expensive pieces of equipment. You can get a small low-quality model for a few hundred dollars, but powerful high-quality generators cost thousands or even tens of thousands of dollars. Old cars, on the other hand, can be very cheap — especially if they aren’t roadworthy anymore. Jake von Slatt has a video series explaining how you can convert an old car with a working engine into a powerful generator.</p>
<p>Most of the cost of a generator is from the engine, and alternator or dynamo with inverter. In this case, the engine is in a Toyota Sienna minivan. The vehicle isn’t worth keeping on the road, but the engine still runs well. And that engine has plenty of power for a generator. The alternator came from a Harbor Freight generator that had a bad engine. To keep the AC voltage output at the steady 60Hz needed for household appliances and tools, von Slatt utilized an Arduino.</p>
<figure><p><img loading="lazy" width="1024" height="646" src="https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1-1024x646.jpg" alt="" srcset="https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1-1024x646.jpg 1024w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1-300x189.jpg 300w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1-768x484.jpg 768w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1-1536x969.jpg 1536w, https://blog.arduino.cc/wp-content/uploads/2023/09/Car-Generator-1.jpg 1972w" sizes="(max-width: 1024px) 100vw, 1024px"></p></figure>
<p>The Sienna has a cruise control system that actuates the throttle in an attempt to keep wheel speed consistent. But in this case, von Slatt needed it to keep the engine stable at 3600rpm to maintain 60Hz. So he built a simple circuit around an <a href="https://store.arduino.cc/products/arduino-nano-every">Arduino Nano Every board</a> and an H-bridge. The Arduino controls the cruise control actuator’s servo motor through the H-bridge while monitoring the alternator output voltage (stepped down to 5V) frequency. If the frequency is too low, the Arduino rotates the cruise control actuator to increase engine speed until the frequency is exactly 60Hz. If the frequency is too low, it does the opposite.</p>
<figure><p>
<iframe loading="lazy" title="Car Generator Powered by Arduino! - Part 3" width="500" height="281" src="https://www.youtube.com/embed/1PO0-g3e-sE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>
<p>This is a simple and effective way to keep both an old car and an old generator out of the scrap yard while providing off-grid power.</p>
<section>


<p>
<small>

You can follow any responses to this entry through the <a href="https://blog.arduino.cc/2023/09/20/turning-an-old-car-into-a-powerful-generator/feed/">RSS 2.0</a> feed.
You can <a href="#respond">leave a response</a>, or <a href="https://blog.arduino.cc/2023/09/20/turning-an-old-car-into-a-powerful-generator/trackback/" rel="trackback">trackback</a> from your own site.
</small>
</p>
</section>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS Customers Cannot Escape IPv4 (212 pts)]]></title>
            <link>https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html</link>
            <guid>37608900</guid>
            <pubDate>Fri, 22 Sep 2023 07:15:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html">https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html</a>, See on <a href="https://news.ycombinator.com/item?id=37608900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<hr>

<p>This is the first part of a blog series. Other pieces will be linked here when published.</p>

<hr>

<h2 id="introduction">Introduction</h2>

<p>AWS has announced that they
<a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/">will charge for public IPv4 addresses</a>
soon. Surprisingly, most reactions to this announcement were welcoming, as it’s believed to bring
a big push for IPv6 adoption.</p>

<p>The AWS blog had this to say:</p>

<blockquote>
  <p>This change … is also intended to encourage you to be a bit more frugal with your use of public IPv4 addresses and to think about accelerating your adoption of IPv6 as a modernization and conservation measure.</p>
</blockquote>

<p>I will explain why this charge will not affect IPv6 adoption in any meaningful way.</p>

<p>I will also show that it is pretty bold for AWS to use IPv6 in trying to soften the blow of this charge: AWS IPv6 support has
<em>huge</em> gaps. AWS makes it impossible for customers to move on from IPv4.</p>

<p>But first, a quick summary, and some costly anti-patterns to watch out for.</p>

<h2 id="whats-going-to-change">What’s Going to Change</h2>

<p>Every <em>dedicated</em> public AWS IPv4 address<sup id="fnref:byoip" role="doc-noteref"><a href="#fn:byoip" rel="footnote">1</a></sup> will be charged with $0.005 per hour (almost $4 per month /
more than $40 per year), starting February 2024.</p>

<p>Some examples of affected services:</p>
<ul>
  <li>Elastic Load Balancers (one address per availability zone, <em>at least</em> two – and IPv4 cannot be turned off)</li>
  <li>EC2 instances &amp; Elastic IPs</li>
  <li>ECS Fargate<sup id="fnref:awsvpc" role="doc-noteref"><a href="#fn:awsvpc" rel="footnote">2</a></sup> tasks configured with public IPs</li>
  <li>Global Accelerator IPs</li>
  <li>Site-to-site VPN IPs</li>
  <li>RDS (nine in ten customers have public IP on RDS by accident)</li>
  <li>Managed NAT Gateways</li>
</ul>

<p>So for a public Elastic Load Balancer configured in three availability zones (as recommended),
this is a base price increase of over 50%.</p>

<p>For Managed NAT Gateway, this is a base price increase of ~10%. I didn’t think those could become more expensive,
but here we are. By the way, this is a good time to remember that a NAT Gateway does <em>not</em> provide zonal redundancy –
a NAT Gateway in every availability zone is required when outbound connections are critical.</p>

<p>All <em>shared</em> IPv4 addresses, like those used for Cloudfront distributions or API Gateway endpoints, are <em>not</em> affected. Public IPv4
addresses used by non-VPC Lambda functions aren’t affected either.</p>

<h2 id="wasteful-patterns">Wasteful Patterns</h2>

<p>I have observed two patterns that waste quite a lot of addresses and are heavily impacted by the upcoming charges.</p>

<p>The first pattern is having multiple Load Balancers (per VPC); this is often the result of using several
readily available Cloudformation templates / Terraform modules, or somehow using Kubernetes ingress controllers that create
a Load Balancer <em>for every service</em>. This is fixed by not doing that! A single Load Balancer can handle many URLs
and services.</p>

<p>The second pattern is intentionally configuring public IPv4 on EC2 instances and Fargate tasks just to avoid Managed NAT Gateway
(commendable until now!). Ironically, this will still be cheaper until hitting around ten public IPv4 addresses per availability zone.</p>

<h2 id="how-to-check">How to Check</h2>

<p>The usage data for public IPv4 addresses is already being accounted, so it’s easy to check.
Usage is currently listed with $0 but will start to cost $0.005 per hour starting in February 2024.</p>

<ul>
  <li>In <em>Billing Dashboard</em> &gt; <em>Bills</em>, usage is displayed as <code>PublicIPv4:InUseAddress</code>
in the <em>Virtual Private Cloud</em> section</li>
  <li>In <em>Cost Explorer</em>, it’s easy to see usage hours by filtering to only include <code>PublicIPv4:InUseAddress</code>;
this is nice because Cost Explorer shows usage per day/hour and it’s easy to group e.g. by member account</li>
  <li><em>VPC</em> &gt; <em>VPC IP Address Manager</em> (at the very bottom) &gt; <em>Public IP insights</em> is a nice way to get an overview of
public IPv4 address usage; this part of IPAM is free, but it shows data only per account and region, and it’s
incomplete data – it doesn’t capture transient address usage.</li>
</ul>

<h2 id="accelerating-your-ipv6-adoption">Accelerating Your IPv6 Adoption</h2>

<p>So it does sound pretty obvious: IPv4 addresses will soon cost a few bucks, while IPv6 addresses are free of charge. Even better:
There is no concept of private addresses in IPv6, which means farewell to the Managed NAT Gateway and its magnificent pricing.
It seems like the perfect time to finally adopt IPv6.</p>

<p>There are two approaches to adopting IPv6. Either by going dual-stack, which means configuring IPv4 <em>and</em> IPv6 addresses on all systems.
Or by going IPv6-only internally, with IPv4 connectivity only where facing the general public – for example at the very edge, on the CDN,
which takes care of serving IPv4 clients.</p>

<p>Running IPv6-only is the optimal solution for a future-proof network. Even the U.S. Government, of all places,
recognizes this. A <a href="https://www.whitehouse.gov/wp-content/uploads/2020/11/M-21-07.pdf">memorandum from the Executive Office of the President</a> states:</p>

<blockquote>
  <p>In recent years it has become clear that [running dual-stack] is overly complex to maintain and unnecessary. As a result, standards bodies and leading technology companies began migrating toward IPv6-only deployments, thereby eliminating complexity, operational cost, and threat vectors associated with operating two network protocols.</p>
</blockquote>

<p>That memorandum was issued three years ago.</p>

<p>The underlying IPv6 support on AWS is excellent. It’s possible to configure IPv6-only EC2 instances in IPv6-only subnets
and everything works fine, including local DNS, time service, host configuration, security
and all the stuff to be expected in an IPv6-enabled network. Components like VPN, Transit Gateway,
Direct Connect and Network Firewall also support IPv6.</p>

<h2 id="cannot-escape-ipv4">Cannot Escape IPv4<sup id="fnref:title" role="doc-noteref"><a href="#fn:title" rel="footnote">3</a></sup></h2>

<p>The problem is actually using all the (other) Amazon Web Services.</p>

<p>It’s practically impossible to run IPv6-only on AWS. Trying to configure core services like API Gateway, Lambda,
ECS or App Runner into IPv6-only subnets makes them either sternly refuse those subnets or throw comically sad error messages like
<code>Not enough IP space available</code> (Elastic Load Balancer).</p>

<p>And it gets worse: Not even running dual-stack internally helps. Most AWS services, when configured into a dual-stack
subnet, happily ignore that the subnet has IPv6 configured; they can connect to IPv4 targets only. This
<a href="https://twitter.com/tim_nolet/status/1696206569090789416">actually hurts customers</a>.</p>

<p>Almost no AWS API can be used from a VPC without public IPv4 addresses<sup id="fnref:privatelink" role="doc-noteref"><a href="#fn:privatelink" rel="footnote">4</a></sup>.
Running an innocent <code>aws s3 ls</code> from an EC2 instance will fail. Using Systems Manager – the recommended way to
connect to and manage an instance – does not work. Accessing SQS from an ECS Task is impossible. And so on.
This is because <a href="https://awsipv6.neveragain.de/">more than 90% of all AWS service API endpoints do not support IPv6</a>.
Other endpoints like SES SMTP or ECR repository endpoints<sup id="fnref:dockerhub" role="doc-noteref"><a href="#fn:dockerhub" rel="footnote">5</a></sup> don’t work either. Cloudfront cannot connect
to IPv6 origins.</p>

<p>And then AWS frames this new charge as an “encouragement” to adopt IPv6. <em>This</em> is what ticked me off:
There is no escape, no way to avoid these charges – because AWS has significantly neglected IPv6 for years.</p>

<h2 id="charges-without-changes">Charges Without Changes</h2>

<p>For most customers, nothing will change. Which is exactly my point.</p>

<p>For AWS customers of any relevant size, this IPv4 charge is a drop in the bucket; they won’t even notice it on their bill.
In my experience so far, this will usually be very well below 1%. Nobody will invest significant engineering effort for that alone.</p>

<p>For many SMBs, hobbyists and startups though, this charge can easily amount to 10-30% of the bill. These customers certainly
would adopt IPv6 to avoid such a price increase.</p>

<p>So that’s where we are: The larger customers do not care about this charge; and those that will feel the impact have
no way to avoid it.</p>

<h2 id="conclusion-so-far">Conclusion So Far</h2>

<p>I think the IPv4 charge by itself is fine – maybe even necessary, given the IPv4 acquisition costs lately.</p>

<p>I would like AWS to be a role model in IPv6 adoption, like they are in many other areas.
If the AWS service landscape had mature IPv6 support, many customers could indeed migrate to IPv6-only environments.
Larger customers still would not care – but it would create a lot of
first movers within the AWS community, setting examples to follow.</p>

<p>But IPv6 is clearly not a first-class citizen on AWS today.</p>

<p>Given those circumstances, this charge will do nothing to drive IPv6 adoption. It will stiffle innovation and make it
<a href="https://www.lastweekinaws.com/blog/aws-has-a-moral-responsibility-to-fix-the-free-tier/">even less</a> attractive to have a
personal AWS account – for example, for learning AWS, to study for certifications (to invest in AWS as a career path) or to
support open-source work. AWS talks big about democratizing cloud; this isn’t it.</p>

<h2 id="upcoming-posts">Upcoming Posts</h2>

<p>This is the first part of a blog series. Upcoming posts will take in-depth looks at options for ingress and intra-VPC traffic
(IPv6 support in Cloudfront, API Gateway, App Runner etc.), egress traffic (avoiding public IPv4 addresses and NAT Gateways),
application programming issues (accessing AWS service endpoints and peculiarities of the AWS CLI and SDKs in dealing with
IPv6), other obscure details and a list of helpful resources.</p>

<hr>

<p><a href="https://twitter.com/apparentorder/status/1704628704876388472">Discuss and/or follow on Twitter!</a></p>

<hr>

<h2 id="links">Links</h2>

<ul>
  <li><a href="https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/IPv6-reference-architectures-for-AWS-and-hybrid-networks-ra.pdf">Dual Stack and IPv6-only Amazon VPC Reference Architectures</a> (PDF)</li>
  <li>AWS Workshop: <a href="https://catalog.workshops.aws/ipv6-on-aws/en-US">Get hands-on with IPv6</a></li>
  <li>AWS blog
    <ul>
      <li><a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/">AWS Public IPv4 Address Charge + Public IP Insights</a></li>
      <li><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/identify-and-optimize-public-ipv4-address-usage-on-aws/">Identify and optimize public IPv4 address usage on AWS</a></li>
      <li>Dual-stack IPv6 architectures for AWS and hybrid networks
<a href="https://aws.amazon.com/blogs/networking-and-content-delivery/dual-stack-ipv6-architectures-for-aws-and-hybrid-networks/">Part 1</a>
and <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/dual-stack-architectures-for-aws-and-hybrid-networks-part-2/">Part 2</a></li>
    </ul>
  </li>
  <li>AWS whitepaper
    <ul>
      <li><a href="https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/designing-an-ipv6-aws-cloud-network.html">Designing an IPv6 AWS Cloud network</a></li>
      <li><a href="https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/IPv6-on-AWS.html">IPv6 on AWS</a></li>
    </ul>
  </li>
  <li>AWS VPC documentation
    <ul>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/aws-ipv6-support.html">AWS services that support IPv6</a>
(includes a list of ~30 services that have at least some IPv6 support; not listed at all are the ~200 other services)</li>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html">Migrate your VPC from IPv4 to IPv6</a></li>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-nat64-dns64.html">DNS64 and NAT64</a></li>
    </ul>
  </li>
</ul>

<hr>

<p><em>Update 2023-09-21</em> – added “Links” section</p>

<p><em>Update 2023-09-21</em> – affected services: added RDS; made it clear that those are examples
(list is not exhaustive)</p>

<hr>



	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm fed up with it, so I'm writing a browser (655 pts)]]></title>
            <link>https://adayinthelifeof.nl/2023/09/22/browsers.html</link>
            <guid>37608580</guid>
            <pubDate>Fri, 22 Sep 2023 06:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adayinthelifeof.nl/2023/09/22/browsers.html">https://adayinthelifeof.nl/2023/09/22/browsers.html</a>, See on <a href="https://news.ycombinator.com/item?id=37608580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<article>
			<header>
				
				
			</header>

			

			









<img src="https://www.gravatar.com/avatar/1761ecd7fe763583553dde43e62c47bd?s=50">

Posted on 22 Sep 2023<br>

Tagged with:

    
    [ <a href="https://adayinthelifeof.nl/archive/tags/rustprogrammingbrowsers">rust, programming, browsers</a> ]&nbsp;


 <p>This blogpost starts with me switching of my car radio, and ends with me writing a browser. There is some stuff 
in between as well.</p>

<!--more-->

<p>Some ten years ago, I used to travel a lot by car to customers. I listened to news radio to keep myself occupied during the daily commute. In the Netherlands, I would listen to Business News Radio (BNR)
and/or Radio 1. I liked having people talk and discuss the daily news and not being interrupted by the latest music I didn’t like anyway. I think I still had a car with MP3 CDs.</p>

<p>All was well until one day, I decided to switch off my radio. I didn’t want to hear the news anymore. And that evening on the way back, the radio stayed off. I still didn’t want to listen to the news. In fact, I didn’t want to listen to the news like ever again!</p>

<p>Why? I don’t know exactly, but I don’t think it’s a coincidence that this was also around the time my wife
got pregnant (although I cannot say for sure if this was before or after we knew). But when I thought about it, the
more apparent it got: the ONLY thing you hear on the news.. any news.. are bad things(tm). War, corruption, you name it.
We still should be thankful we still call these things news because often it feels like it would be more newsworthy if
politicians weren’t corrupt, or not kept their promises, or screwed another group of people over. And the problem is
that I - as a single person - have no ways or means to do anything about it. Nothing. Not even if I tried.</p>

<p>Basically, my thinking was: if listening to the news makes me feel bad, I should stop listening to the news. Makes sense
I guess. But I went a bit further: i do not listen to the news, I also do not watch the news or browse
the internet for news. But I also actively try to get the news out of the way. We don’t watch news channels; we switch
channels when the 8 o’clock news starts (that was a thing before streaming), and I leave a room where there is news on
which I cannot control.</p>

<p>Fast forward to almost the present: I still do not watch, listen or browse the news, but I read Reddit for some subreddits that
sometimes posts news facts here and there. Any “important” news that I should need to know (do I really?) comes to me anyway because
of people. So all is well. I’m not getting saddened by the news. Job done.</p>

<p>That was until a year ago, but it’s probably way earlier that the next thing started. So although I don’t follow the classic world and political news, I do like to follow tech news, and for me, this is done 98% through Ycombinator’s hacker news. But I started to get the same feeling I had ten years ago: it’s all negative.
Companies are pushing unwanted updates, breaking all promises, raising subscriptions, and buying up companies while doing
everything in their power to figure out how much more personal data they can take (away) from us. Governments are banning
encryption because of child molesters (they also use vans and candy. Let’s also ban them!). People in power with
absolutely no idea of how the modern world works and people in power who DO know how it works, want to make it even worse for everybody else. Meanwhile, the enshitification keeps speeding up, and I’m afraid
to update my printer drivers in case my printer tells me it doesn’t like my ink and stops printing. And then there is the golden goose: the internet browser. The one place where almost everybody in the world spend their time. How long will it be until browser developers decide that ads could also shown by the browser itself, rather than from the
rendered sites that adblockers can block? We already had those.. they WILL come back. And how about browser-specific extensions that make sites unusable from other browsers (sorry, you cannot view your gmail with Firefox. Please install google chrome)? This has happened, and as soon as lawyers find a way out of monopoly issues, it will happen
again. Things like this make me sad, and again, I’m just one person out of many with little to no influence.</p>

<p>But that’s not quite right this time. This time, I DO have some influence because I am a programmer. I can develop software and I can share this software and code with others so they do not need to use it from companies that only serve their shareholders and pockets.</p>

<p>So we come to the point that I’ve decided that I am going to write a browser. For two reasons: I want this to be a way to push back. Just a tiny amount. And reason two is that I always wanted to write a browser.</p>

<p>I do not expect ANYTHING to come from this project. I do not
expect to finish this project. And I do expect that - if nobody will follow me into helping the project - the project will be dead quite soon. I do not expect this project to become the dominant browser that will topple all the big players in the market. But I do want the project to be open for non-commercial purposes. I want others to be inspired by it and make their project. And those projects will inspire others, and so on, until we DO reach a point where we have a browser that can topple the big players.
I can’t fly. I don’t have x-ray eyes. I don’t have a heavy hammer. But I can develop software. And I can share with as many people that I can. Either as an inspiration for others to write their own code, or as an example on how not to write code.</p>

<p>I’m not an idyllic person. I’m not an activist fighting for a better world or anything. But I get angry about the corporations screwing over people, software, planets. I’m not in a position to solve that. I’m just a
developer that will attempt to write a browser.</p>

<p>Details about the progress of the project can be found here: <a href="https://codemusings.nl/@jaytaph/p/MQpHToAx8c1KXyU98Auip4">https://codemusings.nl/@jaytaph/p/MQpHToAx8c1KXyU98Auip4</a></p>

<p>The project itself can be found on GitHub: <a href="https://github.com/jaytaph/gosub-browser">https://github.com/jaytaph/gosub-browser</a></p>




<hr>



		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is ClickHouse Moving Away from Open Source? (109 pts)]]></title>
            <link>https://altinity.com/blog/is-clickhouse-moving-away-from-open-source</link>
            <guid>37608186</guid>
            <pubDate>Fri, 22 Sep 2023 05:20:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://altinity.com/blog/is-clickhouse-moving-away-from-open-source">https://altinity.com/blog/is-clickhouse-moving-away-from-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=37608186">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-clarity-region="article" id="post-15138">
	<div>
	
<p>ClickHouse is one of the best open source databases with a very active community. Released in 2016 with permissive Apache 2.0 license, it now counts more than 30,000 GitHub stargazers, hundreds of code contributors, a rich ecosystem, and thousands of businesses using ClickHouse in production. All are excellent indicators of the outstanding success of this open source technology.&nbsp;</p>



<p>In 2021 ClickHouse Inc. emerged in order to commercialize ClickHouse. Two years later there are now signs that ClickHouse might be moving away from its open source origins – important new features are available only in ClickHouse Cloud. That raises a lot of questions about ClickHouse’s future.</p>



<h2 id="h-closing-open-source">Closing Open Source?</h2>



<div><p>ClickHouse Cloud service was started in September 2022, and since then we can see evidence that ClickHouse Inc. started to make certain features in the private version only. For a long time there were only minor ones around the Replicated database engine. They had little impact on open source use.</p><p>But the recent <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates" target="_blank" rel="noreferrer noopener">announcement of SharedMergeTree and lightweight updates</a>, which are available only in ClickHouse Cloud, shows that this strategy may be changing. It’s now apparent that important features are not going to be available in the open source version.&nbsp;</p></div>



<p>Community members of course raised questions to the ClickHouse team about their plans. As Alexey Milovidov, ClickHouse Inc. CTO, <a href="https://github.com/ClickHouse/ClickHouse/issues/44767#issuecomment-1689746194" target="_blank" rel="noreferrer noopener">responded</a>:</p>



<p><em>“It’s good to have a small, limited number of modifications exclusive to ClickHouse Cloud, but only those that do not compromise the features or operation in self-managed usages, but in the same way, are crucial and distinguishing for the Cloud.”</em></p>



<p>Unfortunately, this statement is incomplete. The current closed source features include not only those that are essential for cloud operation at scale (like new SharedMergeTree storage engine, allowing true separation of storage from compute), but also generic features like <a href="https://clickhouse.com/docs/en/guides/developer/lightweight-update" target="_blank" rel="noreferrer noopener">lightweight UPDATE</a> that any ClickHouse user would use, whether in the cloud or on prem, or <a href="https://clickhouse.com/docs/en/cloud/manage/security/secure-s3#access-your-s3-bucket-with-the-clickhouseaccess-role" target="_blank" rel="noreferrer noopener">S3-role based access</a> which is a key security feature in public clouds.&nbsp;</p>



<p>Moreover, the current object storage implementation in open source ClickHouse looks neglected. Significant improvements were planned in <a href="https://github.com/ClickHouse/ClickHouse/issues/44767" target="_blank" rel="noreferrer noopener">the 2023 Roadmap</a>, and some, like shared metadata, were a part of <a href="https://github.com/ClickHouse/ClickHouse/issues/32513" target="_blank" rel="noreferrer noopener">the 2022 Roadmap</a> and even earlier, so the community expected those to be available in ClickHouse. Now it is clear that those improvements were implemented in ClickHouse internal fork as a part of SharedMergeTree, and are not going to be available in open source at all. This is a disappointment for the community, which has been waiting several years for those features.</p>



<h2 id="h-so-is-clickhouse-now-open-core">So is ClickHouse now Open Core?</h2>



<p>Open core is the common name for an open source project with a set of advanced features that are closed source. It is one of several models to monetize open source projects, an increasing concern for software businesses looking for a big return on investment. It also means tectonic changes for the ClickHouse open source community.</p>







<figure><img decoding="async" loading="lazy" width="1000" height="603" src="https://altinity.com/wp-content/uploads/2023/09/Open-core-model.jpg" alt="" srcset="https://altinity.com/wp-content/uploads/2023/09/Open-core-model.jpg 1000w, https://altinity.com/wp-content/uploads/2023/09/Open-core-model-480x289.jpg 480w, https://altinity.com/wp-content/uploads/2023/09/Open-core-model-768x463.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201000%20603'%3E%3C/svg%3E" data-lazy-srcset="https://altinity.com/wp-content/uploads/2023/09/Open-core-model.jpg 1000w, https://altinity.com/wp-content/uploads/2023/09/Open-core-model-480x289.jpg 480w, https://altinity.com/wp-content/uploads/2023/09/Open-core-model-768x463.jpg 768w" data-lazy-src="https://altinity.com/wp-content/uploads/2023/09/Open-core-model.jpg"><figcaption><em>Diagram from </em><a href="https://www.linux.com/news/how-make-money-open-source-platforms/" target="_blank" rel="noreferrer noopener"><em>“How to Make Money from Open Source Platforms – Linux.com”</em></a></figcaption></figure>



<p>In the true open source model that ClickHouse has followed for years, the community is the main development driver. The community helps to define the roadmap, submits new features or feature requests, develops the ecosystem, and starts new businesses. The core development team develops strategic features, and acts as a moderator and the housekeeper, trying to ensure sustainable community growth. The more active the community, the more successful applications emerge, and the more popular the project becomes. This is a positive feedback loop.</p>



<p>In a full open core project, it is quite the opposite. The business and product of the project owner are the main drivers. The core development team focuses on product development. The open source community, to the extent it exists, is valued as a lead generation source. One can start developing an application using an open source version, but in order to run it in production at scale, one has to switch to the closed source product.</p>



<p>What we can see now in ClickHouse is a shift to the latter model. Using object storage efficiently is crucial for big data analytics, yet this feature is not fully available in open source. The Roadmap execution is successful in closed source cloud features only, others are abandoned, and so on.&nbsp;</p>



<h2>What’s Next?</h2>



<p>ClickHouse is a great database that has proven its value many times over. It runs everywhere–from edge devices to huge server farms–with outstanding results. Its extreme performance, flexibility, and portability were the main reasons for its success. It could not happen without an outstanding open source community that has grown for years – thanks to Apache 2.0 licensing and a caring approach to users.</p>



<p>Unfortunately, a switch to an open core model undermines several factors that made ClickHouse successful. The focus on the product does not allow the core team to maintain the same level of community support as before. The focus on ClickHouse Cloud features ignores the needs of users that want to use ClickHouse elsewhere. Those and other factors hurt the trust with the community that made ClickHouse so popular. The community will have to adapt to continue the amazing growth of the past few years.&nbsp;</p>



<p>First, it is necessary to distinguish the open source roadmap from the ClickHouse cloud roadmap. Users need to know what is going to be available in open source and when.</p>



<p>Second, the community must step up to drive development of strategic features. We recently submitted an <a href="https://github.com/ClickHouse/ClickHouse/issues/54644" target="_blank" rel="noreferrer noopener">RFC for object storage support improvements</a>. It is based on abundant feedback from other open source users and is truly a community effort. We hope this work can be done and will be merged to upstream even if it overlaps with ClickHouse cloud-only features.&nbsp;</p>



<p>Third, the ClickHouse team will have to delegate more authority to community contributors. The ClickHouse team is emerging as a bottleneck for reviewing and merging pull requests, which is understandable given their focus on the product.</p>



<p>We’ve always dreamed that ClickHouse will move eventually to a foundation with independent governance. This would make many users happy. We may still dream, right?</p>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Bogus CVE Problem (232 pts)]]></title>
            <link>https://lwn.net/Articles/944209/</link>
            <guid>37608110</guid>
            <pubDate>Fri, 22 Sep 2023 05:08:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/944209/">https://lwn.net/Articles/944209/</a>, See on <a href="https://news.ycombinator.com/item?id=37608110">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>LWN.net needs you!</b><p>Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing</p></div>
           </center>
           
<p>
The "<a href="https://cve.mitre.org/">Common Vulnerabilities and
Exposures</a>" (CVE) system was launched late 
in the previous century (September&nbsp;1999) to track vulnerabilities in
software.  Over the years since, it has had a <a href="https://lwn.net/Articles/679315/">somewhat checkered
reputation</a>, along with some <a href="https://lwn.net/Articles/851849/">some attempts to
replace it</a>, but CVE numbers are still the only effective way to track
vulnerabilities.  While that can certainly be useful, the
CVE-assignment (and severity scoring) process is not without its problems.
The prominence of CVE numbers, and the consequent increase in 
"reputation" for a reporter, have combined to create a system that can
be—and is—actively gamed.  Meanwhile, the organizations that oversee the
system are ultimately not doing a particularly stellar job.
</p>

<p>
A recent incident highlights some of the problems inherent in the system. <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-19909">CVE-2020-19909</a>,
which is an integer-overflow bug in
the <a href="https://curl.se/">curl tool and library for URL-based data
transfers</a> that was only reported
to the project in&nbsp;2023.  In a <a href="https://daniel.haxx.se/blog/2023/08/26/cve-2020-19909-is-everything-that-is-wrong-with-cves/">blog
post describing the mess</a>, curl maintainer Daniel
Stenberg said that a <a href="https://curl.se/mail/lib-2023-08/0031.html">message to the
curl-library mailing list</a> on August&nbsp;25 alerted the project that the CVE
had become public the week before.
</p>

<p>
The year in the CVE number (2020 in this case) is meant to indicate when
the bug was 
reported to one of the <a href="https://www.cve.org/ProgramOrganization/CNAs">more than&nbsp;300 CVE
numbering authorities</a> (CNAs) that hand out CVE numbers.  Under normal
circumstances, a new bug showing up with a CVE number would have&nbsp;2023 in
it, but sometimes CVEs are given out for older bugs that somehow
slipped through the cracks.  That appears to be what happened in this case,
as Stenberg was able to track the problem back to a <a href="https://hackerone.com/reports/661847">bug report</a> from Jason Lee
in mid-2019.
</p>

<p>
The report was for a legitimate bug, where the
<tt>‑‑retry‑delay</tt> option value was being multiplied
by&nbsp;1000 (to milliseconds) without an overflow check.  But what it was
<i>not</i> was a security 
bug, Stenberg said; giving insanely large values for the
option might result in incorrect 
delays—far shorter than requested—but it is not a security problem to make
multiple requests in a short time span.  If it were, "<q>then a browser
makes a DOS [denial of service] every time you visit a website — and curl
does it when you give it two URLs on the same command line</q>", he said in
a <a href="https://daniel.haxx.se/blog/2023/09/05/bogus-cve-follow-ups/">followup
post</a>. 
</p>

<p>
The problem was <a href="https://github.com/curl/curl/pull/4166">duly
fixed</a>, a test case was added, and Lee was credited with the report in
the commit message.  In September&nbsp;2019, curl&nbsp;7.66.0 was <a href="https://curl.se/mail/archive-2019-09/0002.html">released</a> with
fix, which was mentioned in the announcement; also notable are the two CVEs
mentioned at the top of the bug fixes listed.  As Stenberg noted, the curl
project 
works hard to ensure that it fully documents the (real) CVEs that get filed
for
it; his exasperation with CVE-2020-19909 coming out of the blue is evident:
</p><blockquote>
In the curl project we work hard and fierce on security and we always work
with security researchers who report problems. We file our own CVEs, we
document them and we make sure to tell the world about them. <a href="https://curl.se/docs/security.html">We list over&nbsp;140 of them</a> with every imaginable detail about them provided. We aim at
providing gold-level documentation for <i>everything</i> and that includes our
past security vulnerabilities. 
<p>
That someone else suddenly has submitted a CVE for curl is a surprise. We
have not been told about this and we would <i>really</i> have liked to. [...]
</p></blockquote>


<p>
The <a href="https://nvd.nist.gov/">National Vulnerability Database</a>
(NVD) tracks CVEs and "scores" them using the <a href="https://en.wikipedia.org/wiki/Common_Vulnerability_Scoring_System">Common
Vulnerability Scoring System</a> (CVSS), which is a ten-point scale that is
meant to give an indication of the severity of a vulnerability.  For the
curl bug, which should probably not be scored at all, NVD initially came up
with a "9.8 critical", scoring an integer overflow in a delay parameter
as one of the most severe types of vulnerability possible.  Stenberg, who
has tangled with NVD over scoring before, is even further exasperated:
</p><blockquote>
It was obvious already before that NVD really does not try very hard to
actually understand or figure out the problem they grade. In this case it
is quite impossible for me to understand how they could come up with this
severity level. It's like they saw "integer overflow" and figure that <i>wow,
yeah that is the most horrible flaw we can imagine</i>, but clearly nobody at
NVD engaged their brains nor looked at the "vulnerable" code or the patch
that fixed the bug. Anyone that looks can see that this is not a security
problem. 
</blockquote>


<p>
In fact, the pull request for the fix was attached to the report, but that
apparently made little difference in the assessment from NVD.  Back in
March, 
Stenberg <a href="https://daniel.haxx.se/blog/2023/03/06/nvd-makes-up-vulnerability-severity-levels/">decried</a>
the NVD scoring process and, in particular, the NVD practice of re-scoring
CVEs that have already had severity levels attached to them.  NVD uses
CVSS, but the curl project long ago rejected that scoring system:
</p><blockquote>
In the curl project we decided to abandon CVSS years ago because of its
inherent problems. Instead we use only the four severity names: <b>Low</b>,
<b>Medium</b>, <b>High</b>, and <b>Critical</b> and we work out the
severity together in the 
curl security team as we work on the vulnerability. We make sure we
understand the problem, the risks, its prevalence and more. We take all
factors into account and then we set a severity level we think helps the
world understand it. 
</blockquote>


<p>
His example in that case is <a href="https://curl.se/docs/CVE-2022-42915.html">a double-free in curl</a>
that the project determined had a "medium" severity, while NVD scored it as
"9.8 critical", as can be seen in the <a href="https://github.com/advisories/GHSA-98w6-hw73-ph8m">GitHub advisory
database</a>. Since then, NVD apparently had a change of heart after
Stenberg's complaint as the CVE is <a href="https://nvd.nist.gov/vuln/detail/CVE-2022-42915">now scored "8.1
high"</a>.  In a <a href="https://daniel.haxx.se/blog/2023/06/12/nvd-damage-continued/">followup
on NVD "brokenness"</a>, Stenberg gave another example of a CVE that was
initially scored "9.8 critical", but eventually was reduced to "5.9 medium"
after complaints—though the curl project rates it as "low".  He also noted
that there is a set of projects that never report low or medium CVEs that
they find, in order to avoid these kinds of scoring woes.
</p>

<p>
One could perhaps wonder if this is all just a problem for the curl project
and not more widespread, but there is a fair amount of evidence of a
variety of problems in CVE-land.  For example, the PostgreSQL project had a
similar problem with a <a href="https://www.postgresql.org/about/news/cve-2020-21469-is-not-a-security-vulnerability-2701/">CVE
"from"&nbsp;2020</a> that appeared recently—and is not a security vulnerability
at all, according to the project.  In June, the <a href="https://keepassxc.org/">KeePassXC</a> password manager project
<a href="https://keepassxc.org/blog/2023-06-20-cve-202335866/">had a bogus
CVE filed</a> for the tool; there are other examples as well.
</p>

<p>
Each of these bogus CVE filings, which can apparently be made anonymously and
without much in the way of backing information, require that the project
notice its existence, analyze the problem (or "problem"), and, if
necessary, dispute the existence or score of the CVE.
As noted, several curl CVEs have had their scores reduced rather
substantially due to requests from the project.  The delay parameter
overflow that was initially scored&nbsp;9.8 has been reduced to "3.3
low", marked as "disputed", and had a link to Stenberg's blog post
added to the NVD entry.

</p><p>
Keeping up with all of that is a lot of
work, which Stenberg said he is going to try to avoid in the future by
applying to become the CNA for curl.  Several other open-source projects are
CNAs, which gives them some notification of a reported problem along with
ways to try to ensure that the problem is handled sanely.  He mentioned
Apache, OpenSSL, and Python as some of the projects that are already
CNAs; Python was just <a href="https://pyfound.blogspot.com/2023/08/psf-authorized-as-cna.html">granted
CNA status</a> at the end of August.
</p>

<p>
Meanwhile, though, CVEs are used in ways that elevate their importance well
beyond the level that makes sense given the amount of scrutiny that is
apparently applied to them.
Service-level contracts and governmental requirements mean that a critical
CVE needs to be addressed in short order, so non-critical bugs that get
marked that way can cause real problems.  It does provide incentives for
companies and others to try to downplay the severity of bugs, as well, of
course, which makes for something of a "<a href="https://en.wikipedia.org/wiki/List_of_Doctor_Dolittle_characters#Pushmi-Pullyu">Pushmi-Pullyu</a>"
in CVE-land.
</p>

<p>
As was alluded to in our mid-August <a href="https://lwn.net/Articles/941745/">look at
kernel security reporting for distributions</a>, the CVE system is
generally included in the "security circus" that kernel developers
largely disdain.  A <a href="https://lwn.net/Articles/801157/">2019 talk by Greg
Kroah-Hartman</a> described multiple problems that he sees with the
system as well.
</p>

<p>
All in all, the CVE system seems to be broken in various ways.  It also
seems to be getting more and more entrenched into "cybersecurity" handling
at various levels.  Given that it is effectively run by—and now
for—governmental agencies, the ability to replace it with something more
sensible has likely already passed us by.  CVE, warts and all, will be with
us for a long time to come; FOSS projects and organizations are simply
going to have to figure out how to coexist with it.
</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/944209/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modular forms, the ‘fifth fundamental operation’ of math (141 pts)]]></title>
            <link>https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/</link>
            <guid>37608061</guid>
            <pubDate>Fri, 22 Sep 2023 04:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/">https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/</a>, See on <a href="https://news.ycombinator.com/item?id=37608061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><p>Modular forms are one of the most beautiful and mysterious objects in mathematics. What are they?</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/ModularForms-Courtesy-of-SamuelJinglianLi-Lede-1-scaled.webp"></p></div><figcaption></figcaption></figure><div><h2>Introduction</h2><div><p>“There are five fundamental operations in mathematics,” the German mathematician Martin Eichler supposedly said. “Addition, subtraction, multiplication, division and modular forms.”</p>
<p>Part of the joke, of course, is that one of those is not like the others. Modular forms are much more complicated and enigmatic functions, and students don’t typically encounter them until graduate school. But “there are probably fewer areas of math where they don’t have applications than where they do,” said <a href="https://people.mpim-bonn.mpg.de/zagier/">Don Zagier</a>, a mathematician at the Max Planck Institute for Mathematics in Bonn, Germany. Every week, new papers extend their reach into number theory, geometry, combinatorics, topology, cryptography and even string theory.</p>
<p>They are often described as functions that satisfy symmetries so striking and elaborate that they shouldn’t be possible. The properties that come with those symmetries make modular forms immensely powerful. It’s what made them key players in the landmark 1994 proof of Fermat’s Last Theorem. It’s what made them central to <a href="https://www.quantamagazine.org/ukrainian-mathematician-maryna-viazovska-wins-fields-medal-20220705/">more recent work on sphere packing</a>. And it’s what now makes them crucial to the ongoing development of a “mathematical theory of everything” called the <a href="https://www.quantamagazine.org/what-is-the-langlands-program-20220601/">Langlands program</a>.</p>
<p>But what are they?</p>
<h2><strong>Infinite Symmetries</strong></h2>
<p>To understand a modular form, it helps to first think about more familiar symmetries.</p>
<p>In general, a shape is said to have symmetry when there is some transformation that leaves it the same.</p>
</div></div><figure></figure><div><h2>Introduction</h2><div><p>A function can also exhibit symmetries. Consider the parabola defined by the equation $latex f(x) = x^2$. It satisfies one symmetry: It can be reflected over the <em>y</em>-axis. For instance, $latex f(3) = f(−3) = 9$. More generally, if you shift any input $latex x$ to $latex -x$, then $latex x^2$ outputs the same value.</p>
<p>Infinitely many functions satisfy this symmetry. Here are just a few:</p>

<p>The last example is the cosine function from trigonometry. It exhibits reflection symmetry, but it also has other symmetries. If you shift $latex x$<i>&nbsp;</i>by integer multiples of $latex 2\pi$, the function always returns the same value — meaning that there are infinitely many transformations that can leave the function unchanged.</p>

<p>This additional symmetry makes functions like cosine incredibly useful. “Much of basic physics begins with understanding the full implications of the trigonometric functions,” said <a href="https://uva.theopenscholar.com/ken-ono">Ken Ono</a>, a mathematician at the University of Virginia.</p>
<p>“Modular forms are something like trigonometric functions, but on steroids,” he added. They satisfy infinitely many “hidden” symmetries.</p>
<h2><strong>The Complex Universe</strong></h2>
<p>Functions can only do so much when they’re defined in terms of the real numbers — values that can be expressed as a conventional decimal. As a result, mathematicians often turn to the complex numbers, which can be thought of as pairs of real numbers. Any complex number is described in terms of two values — a “real” component and an “imaginary” one, which is a real number multiplied by the square root of −1 (which mathematicians write as $latex i$).</p>
<p>Any complex number can therefore be represented as a point in a two-dimensional plane.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/RepresentingComplexNunmbers-v5_Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><p>It is hard to visualize functions of complex numbers, so mathematicians often turn to color. For example, you can color the complex plane so that it looks like a rainbow wheel. The color of each point corresponds to its angle in polar coordinates. Directly to the right of center, where points have an angle of 0 degrees, you get red. At 90 degrees, or straight up, points are colored bright green. And so on. Finally, contour lines mark changes in size, or magnitude, as on a topographical map.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/graphic5-one-scaled.jpg"></p></div><figcaption><div><p>The complex function&nbsp;<i data-stringify-type="italic">f</i>(<i data-stringify-type="italic">z</i>) =&nbsp;<i data-stringify-type="italic">z</i>&nbsp;depicted as a rainbow wheel of color. It can be used as a reference to illustrate other functions.</p></div></figcaption></figure><div><h2>Introduction</h2><p>You can now use this as a reference graph to illustrate complex functions. A point’s position on the plane represents the input, and you’ll assign that point a color based on the reference graph. For instance, consider the function $latex f(z) = z^2$. When $latex z = 1 + i$, $latex f(z) = 2i$, since&nbsp; $latex (1 + i)^2 = 2i$. Because $latex 2i$ is colored bright green on the reference graph, on your new graph you’ll color the point $latex 1 + i$ bright green.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/graphic5-two-scaled.jpg"></p></div><figcaption><div dir="auto" data-qa="message-text">
<p>This graph of the complex function <i data-stringify-type="italic">f</i>(z) = $latex z^2$&nbsp;shows outputs via colors chosen using<b data-stringify-type="bold">&nbsp;</b>the reference graph&nbsp;<i data-stringify-type="italic">f</i>(<i data-stringify-type="italic">z</i>) =&nbsp;<i data-stringify-type="italic">z</i>.</p>
</div></figcaption></figure><div><h2>Introduction</h2><div><p>The graph of $latex f(z) = z^2$ runs through the colors twice, because squaring a complex number doubles its angle. It also has more contour lines, because the outputs grow in size more quickly.</p>
<p>More generally, the graph looks the same when you reflect points over a diagonal line drawn through the center (or origin).</p>
<p>This is one symmetry of a complex-valued function. Modular forms exhibit a bewildering variety of such symmetries. But it can be tough to make sense of the actual function those colors and contour lines represent.</p>
<h2><strong>The Fundamental Domain</strong></h2>
<p>To do so, it helps to try to simplify the way we look at these complicated functions.</p>
<p>Because of the modular form’s symmetries, you can compute the entire function based on just a narrow sliver of inputs, located in a region of the plane called the fundamental domain. This region looks like a strip going up from the horizontal axis with a semicircular hole cut out of its bottom.</p>
<p>If you know how the function behaves there, you’ll know what it does everywhere else.</p>
<p>Here’s how:</p>
</div></div><figure></figure><div><h2>Introduction</h2><div><p>Two kinds of transformations copy the fundamental domain to the right and left, as well as to a series of ever-shrinking semicircles along the horizontal axis. These copies fill the entire upper half of the complex plane.</p>
<p>A modular form relates the copies to each other in a very particular way. That’s where its symmetries enter the picture.</p>
<p>If you can move from a point in one copy to a point in another through the first kind of transformation — by shifting one unit to the left or right — then the modular form assigns the same value to those two points. Just as the values of the cosine function repeat in intervals of $latex 2\pi$, a modular form is periodic in one-unit intervals.</p>

<p>Meanwhile, you can get from a point in one copy to a point in another through the second type of transformation — by reflecting over the boundary of the circle with radius 1 centered at the origin. In this case, the modular form doesn’t necessarily assign those points the same value. However, the values at the two points relate to each other in a regular way that also gives rise to symmetry.</p>
<p>You can combine these transformations in infinitely many ways, which gives you the infinitely many symmetry conditions that the modular form must satisfy.</p>
<p>“That doesn’t necessarily sound very exciting,” said <a href="https://math.dartmouth.edu/~jvoight/">John Voight</a>, a mathematician at Dartmouth College. “I mean, carving up the upper half-plane and putting numbers on various places — who cares?”</p>
<p>“But they’re very elemental,” he added. And there’s a reason why that’s the case.</p>
<h2><strong>Controlled Spaces </strong></h2>
<p>In the 1920s and ’30s, the German mathematician Erich Hecke developed a deeper theory around modular forms. Crucially, he realized that they exist in certain spaces — spaces with specific dimensions and other properties. He figured out how to describe these spaces concretely and use them to relate different modular forms to one another.</p>
<p>This realization has driven a lot of 20th- and 21st-century mathematics.</p>
<p>To understand how, first consider an old question: How many ways can you write a given integer as the sum of four squares? There is only one way to write zero, for instance, while there are eight ways to express 1, 24 ways to express 2, and 32 ways to express 3. To study this sequence — 1, 8, 24, 32 and so on — mathematicians encoded it in an infinite sum called a generating function:</p>
<p>$latex 1 + 8q + {{24q}^2} + {{32q}^3} + {{24q}^4} + {{48q}^5} + …$</p>
<p>There wasn’t necessarily a way to know what the coefficient of, say, $latex q^{174}$ should be — that was precisely the question they were trying to answer. But by converting the sequence into a generating function, mathematicians could apply tools from calculus and other fields to infer information about it. They might, for instance, be able to come up with a way to approximate the value of any coefficient.</p>

<p>But it turns out that if the generating function is a modular form, you can do much better: You can get your hands on an exact formula for every coefficient.</p>
<p>“If you know it’s a modular form, then you know everything,” said <a href="https://www.mathematik.tu-darmstadt.de/fb/personal/details/jan_hendrik_bruinier.en.jsp">Jan Bruinier</a> of the Technical University of Darmstadt in Germany.</p>
<p>That’s because the infinitely many symmetries of the modular form aren’t just beautiful to look at — “they’re so constraining,” said <a href="https://math.vanderbilt.edu/rolenl/index.html">Larry Rolen</a> of Vanderbilt University, that they can be made into “a tool for automatically proving congruences and identities between things.”</p>
<p>Mathematicians and physicists often encode questions of interest in generating functions. They might want to count the number of points on special curves, or the number of states in certain physical systems. “If we are lucky, then it is a modular form,” said <a href="https://www.claudia-alfes.de/">Claudia Alfes-Neumann</a>, a mathematician at Bielefeld University in Germany. That can be very difficult to prove, but if you can, then “the theory of modular forms is so rich that it gives you tons of possibilities to investigate these [series] coefficients.”</p>
<h2><strong>Building Blocks</strong></h2>
<p>Any modular form is going to look very complicated. Some of the simplest — which are used as building blocks for other modular forms —&nbsp;are called Eisenstein series.</p>
<p>You can think of an Eisenstein series as an infinite sum of functions. To determine each of those functions, use the points on an infinite 2D grid:</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/MeetaModularFormbyMerrillSherman-v6_Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><p>When you add the functions associated to just four points in the grid near the origin, you can see how distinct symmetries begin to emerge.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/7C-01.webp"></p></div><figcaption><div><p>The sum of the four simple functions above, shown in the upper half of the complex plane.</p></div></figcaption></figure><div><h2>Introduction</h2><p>If you take the full sum of the grid’s infinitely many functions, you get an Eisenstein series that’s arguably the easiest modular form to write down. The patterns reflect the form’s defining symmetries — repeating endlessly to the left and right, and transforming in more complicated ways closer to the horizontal axis.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/7C-02.webp"></p></div><figcaption><div><p>The full Eisenstein series is the sum of an infinite number of functions.</p></div></figcaption></figure><div><h2>Introduction</h2><div><h2><strong>The Game Continues</strong></h2>
<p>The study of modular forms has led to a flood of mathematical triumphs. For instance, recent work on sphere packing, for which the Ukrainian mathematician Maryna Viazovska <a href="https://www.quantamagazine.org/ukrainian-mathematician-maryna-viazovska-wins-fields-medal-20220705/">won the Fields Medal last year</a>, used modular forms. “When I saw that, I was quite surprised,” Bruinier said. “But it somehow works.”</p>
<p>Modular forms have turned out to be connected to an important algebraic object called the <a href="https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/">monster group</a>. They’ve been used to construct special kinds of networks called <a href="https://www.quantamagazine.org/new-proof-shows-that-expander-graphs-synchronize-20230724/">expander graphs</a>, which show up in computer science, communications theory and other applications. They’ve made it possible to study potential models of particle interactions in string theory and quantum physics.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/ApplicationsofModularFormsbyMerrillSherman-v4_1300-Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Perhaps most famously, the 1994 proof of Fermat’s Last Theorem hinged on modular forms. The theorem, widely considered one of the most important problems in number theory, states that there are no three nonzero integers <em>a</em>, <em>b</em> and <em>c</em> that satisfy the equation $latex {a^n} + {b^n} = {c^n}$ when $latex n$ is an integer greater than 2. The mathematician Andrew Wiles proved it true by assuming the opposite — that a solution to the equation does exist — and then using modular forms to show that such an assumption must lead to a contradiction.</p>
<p>First he used his assumed solution to construct a mathematical object called an elliptic curve. He then showed that you can always associate a unique modular form to such a curve. However, the theory of modular forms dictated that in this case, that modular form couldn’t exist. “It’s too good to be true,” Voight said. Which meant, in turn, that the assumed solution couldn’t exist — thus confirming Fermat’s Last Theorem.</p>
<p>Not only did this resolve a centuries-old problem; it also provided a better understanding of elliptic curves, which can be difficult to study directly (and which play an important role in cryptography and error-correcting codes).</p>

<p>The proof also illuminated a bridge between geometry and number theory. That bridge has since been broadened into the <a href="https://www.quantamagazine.org/what-is-the-langlands-program-20220601/">Langlands program</a>, a greater set of connections between the two fields — and the subject of one of the central research efforts of contemporary mathematics. Modular forms have also been generalized in other areas, where their potential applications are just starting to be recognized.</p>
<p>They continue to turn up all over the place in math and physics, sometimes quite mysteriously. “I look in a paper about black holes,” said <a href="http://www.math.toronto.edu/~skudla/">Steve Kudla</a> of the University of Toronto, “and I find modular forms that are friends of mine. But I don’t know why they’re there.”</p>
<p>“Somehow,” he added, “modular forms capture some of the most fundamental symmetries of the world.”</p>
</div></div></div><div><h2>Next article</h2><p>The Experimental Cosmologist Hunting for the First Sunrise</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suppressing negative thoughts may be good for mental health after all (150 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</link>
            <guid>37607203</guid>
            <pubDate>Fri, 22 Sep 2023 02:35:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests">https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</a>, See on <a href="https://news.ycombinator.com/item?id=37607203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Researchers at the Medical Research Council (MRC) Cognition and Brain Sciences Unit trained 120 volunteers worldwide to suppress thoughts about negative events that worried them, and found that not only did these become less vivid, but that the participants’ mental health also improved.</p>

<p>“We’re all familiar with the Freudian idea that if we suppress our feelings or thoughts, then these thoughts remain in our unconscious, influencing our behaviour and wellbeing perniciously,” said Professor Michael Anderson.</p>

<p>“The whole point of psychotherapy is to dredge up these thoughts so one can deal with them and rob them of their power. In more recent years, we’ve been told that suppressing thoughts is intrinsically ineffective and that it actually causes people to think the thought more – it’s the classic idea of ‘Don’t think about a pink elephant’."</p>

<p>These ideas have become dogma in the clinical treatment realm, said Anderson, with national guidelines talking about thought avoidance as a major maladaptive coping behaviour to be eliminated and overcome in depression, anxiety, PTSD, for example.</p>

<p>When COVID-19 appeared in 2020, like many researchers, Professor Anderson wanted to see how his own research could be used to help people through the pandemic. His interest lay in a brain mechanism known as inhibitory control – the ability to override our reflexive responses – and how it might be applied to memory retrieval, and in particular to stopping the retrieval of negative thoughts when confronted with potent reminders to them.</p>

<p>Dr Zulkayda Mamat – at the time a PhD student in Professor Anderson’s lab and at Trinity College, Cambridge – believed that inhibitory control was critical in overcoming trauma in experiences occurring to herself and many others she has encountered in life. She had wanted to investigate whether this was an innate ability or something that was learnt – and hence could be taught.</p>

<p>Dr Mamat said: “Because of the pandemic, we were seeing a need in the community to help people cope with surging anxiety. There was already a mental health crisis, a hidden epidemic of mental health problems, and this was getting worse. So with that backdrop, we decided to see if we could help people cope better.”</p>

<p>Professor Anderson and Dr Mamat recruited 120 people across 16 countries to test whether it might in fact be possible – and beneficial – for people to practice suppressing their fearful thoughts. Their findings are published today in <em>Science Advances</em>.</p>

<p>In the study, each participant was asked to think of a number of scenarios that might plausibly occur in their lives over the next two years – 20 negative ‘fears and worries’ that they were afraid might happen, 20 positive ‘hopes and dreams’, and 36 routine and mundane neutral events. The fears had to be worries of current concern to them, that have repeatedly intruded in their thoughts.</p>

<p>Each event had to be specific to them and something they had vividly imagined occurring. For each scenario, they were to provide a cue word (an obvious reminder that could be used to evoke the event during training) and a key detail (a single word expressing a central event detail). For example:</p>

<ul>
	<li>Negative – visiting one’s parents at the hospital as a result of COVID-19, with the cue ‘Hospital’ and the detail ‘Breathing’.</li>
	<li>Neutral – a visit to the opticians, with the cue ‘Optician’ and the detail ‘Cambridge’.</li>
	<li>Positive – seeing one’s sister get married, with the cue ‘Wedding’ and the detail ‘Dress’.</li>
</ul>

<p>Participants were asked to rate each event on a number of points: vividness, likelihood of occurrence, distance in the future, level of anxiety about the event (or level of joy for positive events), frequency of thought, degree of current concern, long-term impact, and emotional intensity.</p>

<p>Participants also completed questionnaires to assess their mental health, though no one was excluded, allowing the researchers to look at a broad range of participants, including many with serious depression, anxiety, and pandemic-related post-traumatic stress.</p>

<p>Then, over Zoom, Dr Mamat took each participant through the 20-minute training, which involved 12 ‘No-imagine’ and 12 ‘Imagine’ repetitions for events, each day for three days.</p>

<p>For No-imagine trials, participants were given one of their cue words, asked to first acknowledge the event in their mind.&nbsp; Then, while continuing to stare directly at the reminder cue, they were asked to stop thinking about the event – they should not try to imagine the event itself or use diversionary thoughts to distract themselves, but rather should try to block any images or thoughts that the reminder might evoke. &nbsp;For this part of the trial, one group of participants was given their negative events to suppress and the other given their neutral ones.</p>

<p>For Imagine trials, participants were given a cue word and asked to imagine the event as vividly as possible, thinking what it would be like and imagining how they would feel at the event. For ethical reasons, no participant was given a negative event to imagine, but only positive or neutral ones.</p>

<p>At the end of the third day and again three months later, participants were once again asked to rate each event on vividness, level of anxiety, emotional intensity, etc., and completed questionnaires to assess changes in depression, anxiety, worry, affect, and wellbeing, key facets of mental health.</p>

<p>Dr Mamat said: “It was very clear that those events that participants practiced suppressing were less vivid, less emotionally anxiety-inducing, than the other events and that overall, participants improved in terms of their mental health. But we saw the biggest effect among those participants who were given practice at suppressing fearful, rather than neutral, thoughts.”&nbsp;</p>

<p>Following training – both immediately and after three months – participants reported that suppressed events were less vivid and less fearful. They also found themselves thinking about these events less.</p>

<p>Suppressing thoughts even improved mental health amongst participants with likely post-traumatic stress disorder. Among participants with post-traumatic stress who suppressed negative thoughts, their negative mental health indices scores fell on average by 16% (compared to a 5% fall for similar participants suppressing neutral events), whereas positive mental health indices scores increased by almost 10% (compared to a 1% fall in the second group).</p>

<p>In general, people with worse mental health symptoms at the outset of the study improved more after suppression training, but only if they suppressed their fears. This finding directly contradicts the notion that suppression is a maladaptive coping process.</p>

<p>Suppressing negative thoughts did not lead to a ‘rebound’, where a participant recalled these events more vividly. Only one person out of 120 showed higher detail recall for suppressed items post-training, and just six of the 61 participants that suppressed fears reported increased vividness for No-Imagine items post-training, but this was in line with the baseline rate of vividness increases that occurred for events that were not suppressed at all. &nbsp;</p>

<p>“What we found runs counter to the accepted narrative,” said Professor Anderson. “Although more work will be needed to confirm the findings, it seems like it is possible and could even be potentially beneficial to actively suppress our fearful thoughts.”</p>

<p>Although participants were not asked to continue practising the technique, many of them chose to do so spontaneously. When Dr Mamat contacted the participants after three months, she found that the benefits in terms of reduced levels of depression and negative emotions, continued for all participants, but were most pronounced among those participants who continued to use the technique in their daily lives.</p>

<p>“The follow up was my favourite time of my entire PhD, because every day was just joyful,” she said. “I didn’t have a single participant who told me ‘Oh, I feel bad’ or ‘This was useless’. I didn't prompt them or ask ‘Did you find this helpful?’ They were just automatically telling me how helpful they found it.”</p>

<p>One participant was so impressed by the technique that she taught her daughter and her own mother how to do it. Another reported how she had moved home just prior to COVID-19 and so felt very isolated during the pandemic.</p>

<p>“She said this study had come exactly at the time she needed it because she was having all these negative thoughts, all these worries and anxiety about the future, and this really, really helped her,” said Dr Mamat. “My heart literally just melted, I could feel goosebumps all over me. I said to her ‘If everyone else hated this experiment, I would not care because of how much this benefited you!’.”</p>

<p>The research was funded by the Medical Research Council and the Mind Science Foundation.</p>

<p><em><strong>Reference</strong><br>
Mamat, Z, and Anderson, MC. <a href="https://doi.org/10.1126/sciadv.adh5292">Improving Mental Health by Training the Suppression of Unwanted Thoughts.</a> Sci Adv; 20 Sept 2023; DOI: 10.1126/sciadv.adh5292</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Luiz André Barroso has died (187 pts)]]></title>
            <link>https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</link>
            <guid>37606775</guid>
            <pubDate>Fri, 22 Sep 2023 01:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/">https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</a>, See on <a href="https://news.ycombinator.com/item?id=37606775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Luiz André Barroso</span> had never designed a data center before Google asked him to do it in the early 2000s. By the time he finished his first, he had overturned many conventions of the computing industry, laying the foundations for Silicon Valley’s development of cloud computing.</p><p>Barroso, a 22-year veteran of Google who unexpectedly died on September 16 at age 59, <a href="https://www.wired.com/2012/10/ff-inside-google-data-center/">built his data centers</a> with low-cost components instead of expensive specialized hardware. He reimagined how they worked together to develop the concept of “the data center as a computer,” which now underpins the web, mobile apps, and other internet services.</p><p>Jen Fitzpatrick, senior vice president of Google’s infrastructure organization, says Barroso left an indelible imprint at the company whose contributions to the industry are countless. “We lost a beloved friend, colleague and respected leader,” she writes in a statement on behalf of the company.&nbsp;</p><p>Barroso continued to lead major projects at Google, including development of <a href="https://www.wired.com/story/covid-exposure-apps-are-headed-for-a-mass-extinction-event/">its Covid exposure notifications app</a>, for which he served as a mediator across teams within the company and with outside partners. In an email Fitzpatrick sent to Google staff seen by WIRED, she wrote that it's understood Barroso died from natural causes.</p><div data-testid="GenericCallout"><figure><p><span>Luiz André Barroso</span><span>Photograph: Sebastian Kennerknecht</span></p></figure></div><p>Fitzpatrick says Barroso’s family, which includes his wife Catherine Warner, a singer for whom he sometimes <a data-offer-url="https://www.beforebossa.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.beforebossa.com/&quot;}" href="https://www.beforebossa.com/" rel="nofollow noopener" target="_blank">played guitar</a>, is seeking privacy. &nbsp;The cause of death could take weeks to determine, according to the medical examiner’s office of Santa Clara County, in Silicon Valley.</p><p>Barroso had wanted to be an electrical engineer since his childhood days in Brazil, where he got into amateur radio with his grandfather and earned bachelor’s and master’s degrees in electrical engineering from the Pontifical Catholic University of Rio de Janeiro. He came to the US for a doctorate in computer architecture from the University of Southern California and worked on chips at Compaq and Digital Equipment Corporation. But he came to Google in 2001 wanting to focus on software engineering.</p><p>Barroso wasn’t a coder for long—the then small startup’s few employees had to pitch in wherever help was needed. Three years after joining Google, <a href="https://www.wired.com/2017/06/google-copes-even-cant-afford-enough-gear/">Urs Hölzle</a>, the company’s first vice president of engineering, tasked Barroso with rebuilding the company's infrastructure. “I was the closest thing we had to a hardware person,” Barroso <a href="https://www.wired.com/2012/01/google-man/">recalled to WIRED in 2012</a>.</p><div><p>When he took on the infrastructure gig, internet businesses such as Google typically hosted their websites on servers in data centers maintained by another company. But these vendors couldn’t handle the surging search startup’s growing needs.</p><p>Barroso’s inexperience in data center design helped lead him to reinventing it, he wrote in <a data-offer-url="https://barroso.org/publications/IEEEMicro2021.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://barroso.org/publications/IEEEMicro2021.pdf&quot;}" href="https://barroso.org/publications/IEEEMicro2021.pdf" rel="nofollow noopener" target="_blank">an essay</a> and <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">recalled during a podcast</a> in 2021. He found himself asking “Wait, wait, wait, but why are we doing it this way?” Barroso said on the podcast. “And it just turns out that the people who had been living in that area hadn't really thought about questioning that. And sometimes it's something that was based on a good reason three years ago, and that reason had a sell-by date, and it's time to do something else.”</p></div></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Google’s first data center consisted of 40-foot, server-filled shipping containers, which enabled advanced cooling and fewer construction headaches. It opened its own data center campus in Oregon in 2006, resembling the conventional bland, boxy, and massive buildings that now dot the world. But Barroso’s ideas made the insides exceptional.</p><p>He and his Google colleagues <a data-offer-url="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf&quot;}" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" rel="nofollow noopener" target="_blank">turned away from the then standard approach</a> of centralizing key software in a data center on a few expensive and powerful machines. Instead they began distributing Google’s programs across thousands of cheaper, mid-grade servers. That saved money spent on pricey hardware while also saving energy and allowing software to run more nimbly.</p><p>Barroso laid out his new philosophy in <a data-offer-url="https://research.google/pubs/pub41606/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://research.google/pubs/pub41606/&quot;}" href="https://research.google/pubs/pub41606/" rel="nofollow noopener" target="_blank"><em>The Datacenter as a Computer</em></a>, a book he coauthored with Hölzle that became a seminal text on modern computing infrastructure. “We must treat the data center itself as one massive warehouse-scale computer,” the book says.</p><p>The efforts of Barroso’s “speed-up” team, as he liked to call it, paid off for Google and helped establish its reputation as not just a neat search engine but also a place that broke new ground in computing. By <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">customizing nearly every inch of Google’s data centers and the hardware within them</a>, including power supplies and cooling kits, the search giant could deliver results, emails, and other services faster—even as the “slow-down” teams integrated more algorithms and features.</p><p>“It’s easy to forget just how crazy the amount of computational data is required to be able to give you a new result every 20 milliseconds or something,” he told WIRED’s Steven Levy in 2012. “We’re essentially searching our web corpus, our images corpus, you name it, every time you do a keystroke.”</p><p>Barroso’s ideas spread quickly across Silicon Valley. Meta and other internet giants adopted an approach similar to Google's for their data centers. The architecture Barroso devised became the basis for Google’s cloud computing unit, which now accounts for about 10 percent of the company’s overall revenue.</p><p>Over the past decade, Barroso helped start the team that designed <a href="https://www.wired.com/2017/04/building-ai-chip-saved-google-building-dozen-new-data-centers/">Google’s AI chips known as TPUs</a>; led engineering for Google’s “geo” services, including the infusion of augmented reality and machine learning into Maps; and founded Google’s core unit, which manages software and other tools used across the company. He held the title of Google fellow, the company’s highest rank for technical staff. In 2020, he received the Eckert Mauchly award from the Association for Computing Machinery and the Institute of Electrical and Electronics Engineers for his contributions to computer architecture.</p><p>Barroso recently joined the board of Stone, an ecommerce company in Brazil, where the engineer was born and where he successfully pushed Google to hire more. Stone wrote in a disclosure to investors this week that Barroso “made significant contributions to our technology team and overall strategy” and that “our hearts and thoughts are with [Barroso’s] family, friends, and colleagues.” A spokesperson for the company declined further comment.</p><p>Barroso was also active in environmental projects. He served on the board of Rainforest Trust, a nonprofit for whom he organized and led a weeklong trip to Brazil's Pantanal wetlands last month. He also <a data-offer-url="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/&quot;}" href="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" rel="nofollow noopener" target="_blank">expressed concern</a> about <a href="https://www.wired.com/story/bitcoin-mining-guzzles-energyand-its-carbon-footprint-just-keeps-growing/">the cryptocurrency industry’s thirst for electricity</a>. Barroso had been executive sponsor for Google’s Hispanic and Latinx employee group and a program awarding fellowships to doctoral students in Latin America.</p><p>Despite all his technical achievements, Barroso told WIRED in 2012 that mentoring interns was “probably the thing I’m best at.” Google chief scientist Jeff Dean, who brought Barroso to Google in 2001 with interviews over crème brûlée, tweeted on Monday without naming his onetime research partner, “Sometimes close friends and colleagues leave us altogether too soon.”</p><p><em>Additional reporting by Steven Levy.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo 3DS Architecture (316 pts)]]></title>
            <link>https://www.copetti.org/writings/consoles/nintendo-3ds/</link>
            <guid>37606380</guid>
            <pubDate>Fri, 22 Sep 2023 00:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.copetti.org/writings/consoles/nintendo-3ds/">https://www.copetti.org/writings/consoles/nintendo-3ds/</a>, See on <a href="https://news.ycombinator.com/item?id=37606380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="imagery">Supporting imagery</h2><section><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul></section><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>As smartphones surge in adoption, the videogame market is experiencing an unusual growth led by discount App Stores and affordable development licenses. With this, one can only wonder when kids will prefer an iPhone 4 over a Nintendo DSi.</p><p>In the midst of finding out the answer, Nintendo conceives a thrilling successor to its triumphant portable system. In it, users will find old, present and unfamiliar technology - many of which can’t be replicated by smartphones.</p><p>And so, this new production of the Architecture of Consoles series will give you a profound description of how this new console works, both internally and externally.</p><h3 id="recommended-reading">Recommended reading</h3><p>If you are new to this <a href="https://www.copetti.org/writings/consoles/">article series</a>, I strongly suggest reading the <a href="https://www.copetti.org/writings/consoles/gamecube/">GameCube</a>, <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/">Nintendo DS</a> articles beforehand, as they will explain various terms and concepts referenced in this one.</p><hr><h2 id="models-and-variants">Models and variants</h2><p>Throughout the lifecycle (and struggles) of this console, Nintendo released numerous revisions in an attempt to correct its target audience and recover loyal customers.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_n3ds.305bcbf7b0cd255a64549d2027f9f6478a7b9777009c85e229f6f93b4c025cc5.webp"><picture><img alt="Image" width="1111" height="506" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_huc15dcfe92f3446e20d07697c5f45f979_25336_2089e178d0cad5b37d8d0bc91ff41544.png" loading="lazy"></picture></a><figcaption>An original Nintendo 3DS (the first generation, from 2011) next to a New Nintendo 3DS XL (the last generation, from 2015).</figcaption></figure><p>From the architectural point of view, there were a total of six different models:</p><ul><li><strong>Nintendo 3DS</strong> (2011) and <strong>Nintendo 3DS XL</strong> (2012): The debuting series featuring the original architecture. The only relevant difference between the XL and non-XL models is the screen size.</li><li><strong>Nintendo 2DS</strong> (2013): A cheaper alternative to the original Nintendo 3DS by removing the stereoscopic screen and featuring a <a href="https://www.copetti.org/writings/consoles/game-boy/">Game Boy</a>-inspired shape.</li><li><strong>New Nintendo 3DS</strong> (2014) and <strong>New Nintendo 3DS XL</strong> (2015): A re-engineering of the standard 3DS models. The ‘New’ variants exhibit an incremental hardware upgrade, an NFC reader, a larger button set and an improved stereoscopic system.</li><li><strong>New Nintendo 2DS XL</strong> (2017): The ‘New’ affordable alternative to the New Nintendo 3DS XL by omitting stereoscopic functionality.</li></ul><p>Now, for this article, the focus will be on the original Nintendo 3DS (after all, it’s the lowest common denominator for games). However, since the architectural differences of the ‘New’ series are worth studying, these will receive a dedicated section.</p><hr><h2 id="displays">Displays</h2><p>There’s only one company that keeps altering the standard structure of all my analyses, and that’s Nintendo. This time, I’ll start with the <strong>stereoscopic screens</strong> (a.k.a. ‘3D without glasses’).</p><p>First things first, the Nintendo 3DS, as a successor of the Nintendo DS, includes two LCD screens. The upper screen has a resolution of <strong>800 x 240 pixels</strong> and somehow can display images with a sense of depth. When I first read this, only questions popped into my head:</p><ul><li>What optics principles are they applying?</li><li>How is the screen designed?</li><li>How do games comply with this system?</li></ul><p>Well, here are the answers!</p><h3 id="principles">Principles</h3><p>Liked or not, the fundamentals are not so different from the <a href="https://www.copetti.org/writings/consoles/virtual-boy/">Virtual Boy</a>, which I’ve happened to analyse two years before. To recall, the Virtual Boy displays two images, one to each eye, and shows objects individually shifted from the centre. By looking at the two pictures at the same time, they are perceived as some objects are behind others (sense of depth). This is the basis of <strong>Stereoscopic Parallax</strong>.</p><figure><ul><li id="tab-1-1-left-link"><a href="#tab-1-1-left">Left</a></li><li id="tab-1-2-right-link"><a href="#tab-1-2-right">Right</a></li></ul><figure id="tab-1-1-left"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png" loading="lazy"></picture></a><figcaption>Left display.</figcaption></figure><figure id="tab-1-2-right"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png" loading="lazy"></picture></a><figcaption>Right display.</figcaption></figure><figcaption>Demonstration of how the Virtual Boy displayed stereoscopic imagery.<br>Mario’s Tennis (1995).</figcaption></figure><p>Now, the way the Virtual Boy executed this was a bit cumbersome: it required users to place their heads close to the eyepiece and then adjust the focal length and inter-pupil distance. 15 years later, Nintendo rightly said ‘No’ to all of that nuisance, and designed a new system where users could enjoy 3D-looking scenery without <em>considerable</em> intervention.</p><figure><ul><li id="tab-2-1-left-link"><a href="#tab-2-1-left">Left</a></li><li id="tab-2-2-right-link"><a href="#tab-2-2-right">Right</a></li></ul><figure id="tab-2-1-left"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png" loading="lazy"></picture></a><figcaption>Top screen, left eye.</figcaption></figure><figure id="tab-2-2-right"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png" loading="lazy"></picture></a><figcaption>Top screen, right eye.</figcaption></figure><figcaption>An example of two frames the Nintendo 3DS shows on its top screen at the same time. Looks like the fish is going to hit you. The same principle applies 15 years later.<br>Luigi’s Mansion (2018).</figcaption></figure><p>This brings us to our next question.</p><h3 id="the-special-screen">The special screen</h3><p>Take a look again at the resolution of the upper LCD screen. On paper, it says it’s <strong>800 x 240 pixels</strong> wide, which results in a ludicrous aspect ratio.</p><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_da7038c582d606c66574ccd907cc6aac.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_a8b4b2a43322fb984cdf1b2394d5a99e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_bc329dbcc83d74e2fc1cf5fae9dfa34e.webp 1000w"><img alt="Image" width="1000" height="907" src="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS again <sup id="bibref:1"><a href="#bib:photography-amos" role="doc-biblioref">[1]</a></sup>, take a closer look at its screens.</figcaption></figure><p>In reality, the physical screen is made of <strong>half-width pixels</strong> and operates in two modes:</p><ul><li><strong>Traditional/2D mode</strong>: When the stereoscopic function is disabled, groups of two horizontal pixel pairs are treated as a single one.<ul><li>Truth be told, the screen can still display a frame of 800 x 240 px, although no commercial game ever used this.</li></ul></li><li><strong>Stereoscopic/3D mode</strong>: All pixels are treated individually, and with it, the screen displays <strong>two frames</strong> of <strong>400 x 240 pixels</strong> at the same time.</li></ul><p>Moreover, to perform stereoscopic parallax, this particular LCD houses an extra layer called <strong>Parallax Barrier</strong> <sup id="bibref:2"><a href="#bib:graphics-display_teardown" role="doc-biblioref">[2]</a></sup>. These opaque shutters deviate the backlight beamed behind the pixels of the LCD, so each eye will receive the light of a different subset of pixels <sup id="bibref:3"><a href="#bib:graphics-display_howworks" role="doc-biblioref">[3]</a></sup>. The half-width pixels will also appear to be wider, thereby giving the feeling they have the traditional aspect ratio.</p><p>All in all, this recreates the original effect of the Virtual Boy without requiring controls for adjustment.</p></div><p>The technology is not perfect, however, as there are a few caveats:</p><ul><li>The parallax barrier requires extra brightness, thereby impacting the battery life.</li><li>The user must not hold the screen in a tilted position (compared to the user’s eyes). Otherwise, the user will end up seeing a confusing mix of the two parallax frames, which can be a disorienting experience. Not to mention the eyes won’t enjoy the extra fatigue.</li><li>Combining the fact the user must maintain a fixed posture while playing, and that stereoscopic parallax can tire the eyes quicker. The 3D feature, as a whole, can be an unnecessary hassle for most.</li></ul><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/tilt.a90f13873f8d4fb7e34e72734a026aa48c63063ed1cfdcb7be0259533b637534.webp"><picture><img alt="Image" width="1000" height="750" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu68de68eb39eeb5477ffb3e92dc8b5860_29218_11a22cc1e2532e946c972f3f59bcc0ba.png" loading="lazy"></picture></a><figcaption>My attempt to capture the tilt effect of the original 3DS. The 3D depth slider (at the right side of the screen) is all the way up, and by looking at the screen from one side, a ghosting effect appears on the top screen. This is quite eye-straining to look at in reality!</figcaption></figure><p>To remediate things, Nintendo added a slider control (called <strong>3D depth slider</strong>) to adjust the level of depth between objects. In doing so, it either increases or decreases the difference between the two frames. This was done to reduce the depth effect for people who didn’t find it enjoyable or too fatiguing.</p><p>Setting the 3D slider to the max can be disorienting at first. In my experience, my eyes eventually got focused, at which point I perceived the top LCD screen as if I were looking through a window. The main problem is that users will need to continuously shift their eyes to see the bottom screen, and the repeated action can be very straining.</p></div><p>As a side note, one can’t help but find it amusing how the graphics pipeline has gone full circle when rendering stereoscopic frames. During rendering, 3D data is projected into a 2D space, and now with the stereoscopic screen, that 2D space is displayed again as 3D. At this point, let’s just use holograms and skip the 3D projection stage altogether.</p><h4 id="a-small-update">A small update</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/new3ds.1ab0c98b61b0946592b63bc6732fcf95f0b6c312e1ce7814df84ec2cb1a8470f.webp"><picture><img alt="Image" width="900" height="158" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu025c37239d25a6aa4098cc524d1fdd78_10338_753f29eb89554cf724e8ee8dd01456e8.png" loading="lazy"></picture></a><figcaption>Top part of the New 3DS XL. At its centre, there’s a front camera and an infrared LED, both used for head tracking.</figcaption></figure><p>With the advent of the ‘New 3DS’ model, Nintendo revisioned their stereoscopic screen in an effort to reach enjoyability levels. In the new model, the console incorporates a face-tracking mechanism to tackle the tilting effect, so issues don’t need to worry about keeping a good head-console posture anymore.</p><h3 id="the-special-games">The special games</h3><p>Now for this system to work, games must play along (pun intended). Just like they traditionally interact with the GPU to draw frames on the display, they must now broadcast two frames of the scenery but with objects slightly shifted.</p><p>To make life easier for developers, there are official APIs that assist in this, especially for those games with 3D sceneries. These APIs help by providing routines that construct two projection matrices, the graphics pipeline then uses them to render the two slightly-shifted frames.</p><hr><h2 id="cpu">CPU</h2><p>Now that we know how the display works, let’s look at the internals of this console. If you get a hold of the motherboard, you’ll see three big chips, one being the <strong>CPU CTR</strong>. That’s the big System-On-Chip (SoC) that houses the entire system (aside from storage and RAM).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/cpu_photo.f1977dfae04cf532272eac2f256cafc4571f4bd76d90ba92e509f80065d69b41.webp"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7aff8078eb7c7ec1542a270f9f966adc.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_c5342c8a4cc85707ed9853e78e1d98bf.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7bba4f72e3e2d7084e976dc626153693.webp 1000w"><img alt="Image" width="1000" height="534" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_eac66c08da1275ec4651225b62bc7d48.png" loading="lazy"></picture></a><figcaption>CPU CTR next to some FCRAM</figcaption></figure><p>CPU CTR follows the design methods of previous portable consoles from Nintendo. That is, squash all your engineering into a single block. In doing so, it will reduce the production of counterfeits, protect sensible components and improve heat dissipation.</p><p>In terms of the actual CPU, Nintendo partnered again with their old friend, <strong>ARM</strong>, to produce their next-generation core. ARM’s licensing model happens to be favourable to Nintendo as they have always offered synthesisable designs, which allows Nintendo to mould to their needs (including, fitting them into a big SoC). In the end, ARM gave them a relatively antiquated with substantial upgrades. Their choice was the <strong>ARM11</strong> core, a successor of the ARM9 (featured with the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a>). More specifically, the <strong>MPCore</strong> variant, ARM’s first <strong>homogenous multi-core</strong> solution.</p><p>Using ARM’s designs, Nintendo crafted an ARM11 MPCore cluster housing <strong>two</strong> ARM11 cores <sup id="bibref:4"><a href="#bib:cpu-lioncash" role="doc-biblioref">[4]</a></sup>. Three years later, with the arrival of the ‘New’ 3DS, the SoC was expanded to contain <strong>four</strong> ARM11 cores. The effects of this will be explained in due time so, before anything else, let’s analyse what the new CPU cores offered to this console.</p><h3 id="an-iconic-industry">An iconic industry</h3><p>The ARM11 series originates from 2002, as a successor of the popular ARM9 and the short-lived ARM10.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/devices.b32e1b8964b560bbeed11ec8ae0e743efb6b9cf4163964bc086ba8aa1b804a5c.webp"><picture><img alt="Image" width="1000" height="384" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hucd71f9346efedab899883028b66e7ae5_21546_4af778c28e8044da94eb7237b76f5f62.png" loading="lazy"></picture></a><figcaption>A Nokia 5230 (2009), a red 3DS (2011) and a Raspberry Pi Model B (2012), all carrying an ARM11.</figcaption></figure><p>In case you haven’t heard about them before, ARM11s are best known for powering the 2006-2008 generation of smartphones (back when many of them featured a keypad or a clamshell design). If you owned a Nokia N95, 5230 or the first iPhone, you’ve used an ARM11. This also applied to many high-end cameras, GPS or similar peripherals. If you wonder, other manufacturers like RIM and Samsung held onto Intel XScale (the continuation of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/##tab-2-2-a-question-about-the-hardware-choice">StrongARM</a>, implementing the ARMv5TE instruction set) until 2009, when they made the switch to ARM11 (this is a bit ironic, considering the iPhone’s CPU was supplied by Samsung!). Last but not least, the ARM11 was the choice of the CPU for the first model of the Raspberry Pi.</p><p>Now, by the time Nintendo adopted the ARM11, its creator had already succeeded it with the Cortex-A series. This is nothing but expected, as Nintendo’s model favours cost-effectiveness over avant-garde CPUs. Look at it from another way, saving in CPU costs allows them to focus their budget on other aspects of the console, you’ll soon see.</p><h4 id="new-dialects">New dialects…</h4><p>Along with the new shiny CPUs, a new instruction set arrived, the <strong>ARMv6</strong>.</p><p>From a programmer’s perspective, the ARMv6 ISA innovates with a new set of vector instructions and multi-core support <sup id="bibref:5"><a href="#bib:cpu-thomas" role="doc-biblioref">[5]</a></sup>. The new vector set provides SIMD instructions that operate groups of <strong>four 8-bit values</strong> or <strong>two 16-bit values</strong> at the same time (using the existing 32-bit registers) <sup id="bibref:6"><a href="#bib:cpu-armcc" role="doc-biblioref">[6]</a></sup>. The new multi-core instructions consist of <code>Store</code> and <code>Load</code> opcodes with special care for synchronisation (crucial for an environment of multiple CPUs using the same memory locations) <sup id="bibref:7"><a href="#bib:cpu-sync" role="doc-biblioref">[7]</a></sup>.</p><p>All in all, this may not seem that thriving for a new chip series, but remember that ARM’s CPUs speak many ‘languages’. In the case of an ARM11-based core, you are provided with:</p><ul><li>The main 32-bit ISA, called <strong>ARMv6</strong>.</li><li>A compressed alternative called <strong>Thumb</strong>. Its instructions fit in 16-bit words instead. If you’d like to know more, I go over it in the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#whats-new">Game Boy Advance article</a>, as it weighs significant importance in that console.</li><li><strong>Jazelle</strong>, a Java bytecode interpreter, mostly forgotten and left unused. I’ve mentioned a bit of it in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii article</a>.</li><li>Any extension bundled into the core. For instance, the MPCore includes a <strong>Vector Floating-point Coprocessor</strong> with additional instructions to control said coprocessor <sup id="bibref:8"><a href="#bib:cpu-vfp" role="doc-biblioref">[8]</a></sup>.</li></ul><p>To make matters less confusing, ARM tends to package all of these with a single nomenclature. For instance, in the case of the ARM11 MPCore opcodes, ARM refers to them as the <strong>ARMv6k</strong> instruction set.</p><h4 id="and-a-fragmented-distribution">… and a fragmented distribution</h4><p>The adoption of extensions and alternative instruction sets eventually made things very convoluted for developers targeting generic ARM hardware, you only have to look at the uncountable ARM ports devised for Linux distributions.</p><p>Debian, one of the most popular distributions, tried to tackle the disparities by developing two ports in parallel:</p><ul><li><code>armel</code>: unoptimized, compatible with ARMv4T onwards.</li><li><code>armhf</code>: accelerated with VFP, but only compatible with ARMv7 onwards.</li></ul><p>Yet, with the arrival of the Raspberry Pi (powered by ARMv6 and accelerated with VFP), neither of them was deemed acceptable. Thus, an unofficial port called ‘Raspbian’ was developed to provide a VFP-accelerated version for ARMv6 CPUs <sup id="bibref:9"><a href="#bib:cpu-armhf" role="doc-biblioref">[9]</a></sup>. Even so, the trend continued: years later, with the arrival of ARMv8 and AArch64, Debian spawned yet-another port, <code>arm64</code>, optimised for the new 64-bits ISA.</p><p>I don’t remember seeing this labyrinth with x86, but at least things are now getting more orderly. AArch64 has unified many extensions and dropped alternative modes (<em>farewell, Thumb and Jazelle</em>).</p><h3 id="core-functionality">Core functionality</h3><p>That was a big deviation. Let’s go back to the 3DS CPU, the ARM11, and check what’s inside.</p><p>For this study, we can divide the ARM11 MPCore into two areas:</p><ul><li>The <strong>MP11 cores</strong> that make up the cluster.</li><li>The <strong>Advanced eXtensible Interface (AXI)</strong> bus, a new invention that interconnects the cores and interfaces with the outside world.</li></ul><p>Let’s start with the cores now and then we’ll check the AXI bus.</p><div><ul><li id="tab-3-1-the-original-mpcore-link"><a href="#tab-3-1-the-original-mpcore">The original MPCore</a></li><li id="tab-3-2-the-new-mpcore-link"><a href="#tab-3-2-the-new-mpcore">The ‘New’ MPCore</a></li><li id="tab-3-3-the-axi-bus-link"><a href="#tab-3-3-the-axi-bus">The AXI bus</a></li></ul><div><div id="tab-3-1-the-original-mpcore"><h4 id="tab-3-1-the-original-mpcore">The original MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png"><picture><img alt="Image" width="513" height="542" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png" loading="lazy"></picture></a><figcaption>Overview of the ARM11 MPCore CPU cluster</figcaption></figure><p>The first ARM11 MPCore variant, which debuted with the original 3DS, includes two cores. Each is called <strong>MP11</strong> and runs at <strong>268 MHz</strong> <sup id="bibref:10"><a href="#bib:cpu-lioncash" role="doc-biblioref">[10]</a></sup>.</p><p>Apart from implementing the ARMv6k instruction set, the CPU features an <strong>8-stage pipeline</strong> <sup id="bibref:11"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[11]</a></sup>. Furthermore, the core provides <strong>two levels of branch prediction</strong>, ‘dynamic’ (based on previous executions) and ‘static’ (based on the current instruction alone). Overall, both enhancements will be quickly noticed, considering the 5-stage ARM9 couldn’t predict a thing!</p><p>Additionally, since the ARM946E-S CPU, ARM has been fitting a <strong>System Control Coprocessor</strong> called <strong>CP15</strong>. This time, it provides <strong>Memory-Management</strong> (MMU functions) and registers that output information about the MPCore cluster.</p><p>Now, there’s no more <strong>Tightly-Coupled Memory</strong> (TCM). There are however <strong>16 KB of instruction cache</strong> and <strong>16 KB of data cache</strong>, this change of model resembles other systems of the same generation. If you are curious, this L1 cache is 4-way set associative.</p><p>Finally, each core houses a co-processor called <strong>Vector Floating-point Coprocessor</strong> (also known as ‘VFP11’). This accelerates arithmetic operations with floating-point numbers, both 32-bit single-precision (a.k.a. <code>float</code>) and 64-bit double-precision (a.k.a. <code>double</code>) ones <sup id="bibref:12"><a href="#bib:cpu-vfp" role="doc-biblioref">[12]</a></sup>. It’s not a big coprocessor though, as its register file is composed of 32 32-bit registers, so doubles will consume two registers. In any case, this processor implements the <strong>VFPv2 instruction set</strong> and follows the <strong>IEEE 754</strong> standard. The latter is a welcomed decision, considering the architecture of <a href="https://www.copetti.org/writings/consoles/playstation-2/#the-leader">previous generations</a>.</p></div><div id="tab-3-2-the-new-mpcore"><h4 id="tab-3-2-the-new-mpcore">The ‘New’ MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png"><picture><img alt="Image" width="960" height="624" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png" loading="lazy"></picture></a><figcaption>Overview of the ‘New’ CPU cluster</figcaption></figure><p>With the arrival of the New 3DS in 2014, a new SoC was included (<strong>CPU LGR</strong>) and with it, a luxurious CPU upgrade.</p><p>The most apparent change is that we have now <strong>four MP11 cores</strong> instead of two. The consequences of this, however, are not simple to disseminate, but we’ll see them in due time.</p><p>The second change is that the CPU incorporates <strong>2 MB of L2 cache</strong> shared between the four cores. This type of cache is 16-way associative, which anticipates four cores accessing it at the same time. If you’d like to know more, I went over associative caches with the <a href="https://www.copetti.org/writings/consoles/xbox-360/#shared-cache">Xbox 360 article</a>.</p><p>Moving on, all cores now run at <strong>804 MHz</strong> (three times the original speed, which will certainly raise a few eyebrows).</p></div><div id="tab-3-3-the-axi-bus"><h4 id="tab-3-3-the-axi-bus">The AXI bus</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png"><picture><img alt="Image" width="510" height="600" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png" loading="lazy"></picture></a><figcaption>Example of how the AXI protocol interconnects different types of components</figcaption></figure><p>Whether there are two or four cores, all of these are connected using a specialised bus, proudly authored by ARM, called the <strong>Advanced eXtensible Interface</strong> (AXI). This protocol is part of the AMBA3 model, a successor of the original AMBA revision that we’ve seen in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#internal-interfaces">Wii U</a> (both housing an ARM9 CPU).</p><p>Generally speaking, the AMBA model provides a set of protocols for connecting components with distinct bandwidth requirements using a <strong>bus topology</strong>. Compare this to the token-ring model of the <a href="https://www.copetti.org/writings/consoles/playstation-3/#inside-cell-the-heart">PlayStation 3</a> or the mesh solution made for the <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-the-messenger">Xbox 360</a>. All of these consoles shared the same problem, but each came up with different solutions, neither better nor worse, just different.</p><p>Following AMBA’s methodologies for interconnecting components, there will be a master-slave hierarchy imposed to maintain order. The master components (typically, the CPU cores) will be the ones sending commands to the slaves (i.e.&nbsp;memory and I/O blocks).</p><p>Now, as part of the AMBA3 specification, ARM offered the AXI model as a critical ingredient for building <strong>System On Chips</strong> (SoC). Instead of using a single bus, AXI uses a dedicated block (called <strong>AXI interconnect</strong>) acting as a <strong>bus matrix</strong> <sup id="bibref:13"><a href="#bib:cpu-axi" role="doc-biblioref">[13]</a></sup>, this is connected to every single component using <strong>64-bit dedicated buses</strong> <sup id="bibref:14"><a href="#bib:cpu-arm11_overview" role="doc-biblioref">[14]</a></sup>. In doing so, AXI overcomes the limitations of high-bandwidth components sharing the same bus (as it happened with the <a href="https://www.copetti.org/writings/consoles/playstation-2/#cpu">PlayStation 2</a>). Moreover, multiple master devices can communicate with slave nodes using separate channels to avoid waiting for other masters to finish. Finally, traditional enhancements like <a href="https://www.copetti.org/writings/consoles/gamecube/#ibms-enhancements">burst transactions</a> are implemented, from which the MP11 cores take advantage.</p><p>In the case of the 3DS, the AXI interconnect is housed in a bigger block called <strong>Snoop Control Unit</strong> (SCU) that also takes care of automatically maintaining L1 cache coherency between the MP11 cores.</p></div></div></div><h3 id="any-other-cpus">Any other CPUs?</h3><p>Up to this moment, I’ve been talking about the MPCore as if it were the only CPU in this system, the reason being mixing up distinct CPUs for this analysis can turn it into an incomprehensible essay. That is, until now.</p><p>The truth is, Nintendo had extra requirements for this console. They wanted a proper security system, but also the possibility to turn the console into a <strong>Nintendo DSi or a GBA</strong> on-the-fly. So, for all of that, they ended up bundling <strong>three distinct CPU packages</strong> - one being the mentioned ARM11. The other two are well hidden, in the sense that games are completely unaware of them. In fact, 3DS emulators like Citra don’t care about them either <sup id="bibref:15"><a href="#bib:cpu-citra_cpu" role="doc-biblioref">[15]</a></sup>.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_ds.3eba2a2f3a4631917db480bd5069986d1298ec9182c0806ba209e2b55dd0eb09.webp"><picture><img alt="Image" width="1111" height="530" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hufe4dbc5506e4ce358fb18d5a09cafc51_22792_04c9dc378631b6c66caa6384f2d190a3.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS next to a predecessor (a Nintendo DS Lite), the latter has become a common denominator.</figcaption></figure><p>But we do! Here’s the complete list of CPUs this system houses:</p><ul><li>The <strong>ARM11 MPCore</strong> we’ve just seen.</li><li>An <strong>ARM946E-S</strong> from the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS days</a>. It’s treated as a secret co-processor and it’s only managed by the operating system. Alternatively, it becomes the main processor whenever a DS or DSi game is executed.<ul><li>Thanks to its bundled CP15 co-processor, there’s a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU) in place. This will protect the CPU from arbitrarily executing code from any location in memory.</li></ul></li><li>An <strong>ARM7TDMI</strong> from the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance days</a>. It’s a relatively ignored CPU, unless a DS or DSi game is being played, in which case it acts as a co-processor. However, on the special occasion when a Game Boy Advanced game is running, the main execution falls into this CPU.</li></ul><p>Unfortunately, or for obvious reasons, the three CPUs are never usable at the same time. Instead, the console has three modes of operation:</p><ul><li><strong>Native 3DS mode</strong>: The ARM11 MPCore executes a 3DS game while the ARM946E-S deals with I/O and security. The ARM7, on the other side, is switched off.</li><li><strong>Nintendo DSi mode</strong>: The ARM946E-S and ARM7TDMI operate in a multi-processor configuration to execute a Nintendo DS or DSi game. Just like with its predecessor, the ARM7TDMI has greater access to I/O. Meanwhile, the ARM11 MPCore will be working in the background to replicate missing and re-located DS hardware (real-time clock, power management, keypad, GBA/DS PPU display and so forth).</li><li><strong>Game Boy Advance mode</strong>: The ARM7TDMI is the only CPU executing instructions (in 99% of cases, that will come from a GBA game). The ARM11 MPCore and ARM9, both still operating within the capacities of ‘Native 3DS mode’, will be working in the background.</li></ul><p>If you stop to think about it, the Nintendo 3DS ends up housing four processors in total (Two MP11 cores + one ARM9 + one ARM7), or the absurd amount of six in the case of the New 3DS. How convoluted is that? Luckily, this system didn’t suffer the complications of the <a href="https://www.copetti.org/writings/consoles/sega-saturn/#cpu">Sega Saturn</a> and you can thank Nintendo and ARM’s engineering for that. After all, 3DS developers only had to deal with the MPCore.</p><p>Since the ARM9 and ARM7 are predominantly for I/O, security and backwards compatibility (neither of which require the developer’s awareness), I discuss them in later sections of this article. But if you’d like to know more about the design of the ARM7 and ARM9, I wrote about them in previous articles (the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a> ones, respectively).</p><h4 id="multi-core-communication">Multi-core communication</h4><p>I guess the question now is, how can CPUs and cores talk to each other? Well, the easiest way is to share RAM… but you could also try a more efficient approach, depending on the cores trying to communicate:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png"><picture><img alt="Image" width="1281" height="288" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png" loading="lazy"></picture></a><figcaption>Representation of the communication channels each CPU is provided with.</figcaption></figure><ul><li>With <strong>inter-core ARM11 communication</strong>, a core can send interrupts to another core by writing on its <code>Software Interrupt Register</code> <sup id="bibref:16"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[16]</a></sup>.</li><li>In the case of <strong>ARM11↔︎ARM9</strong> or <strong>ARM9↔︎ARM7 communication</strong>, the same <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#interconnection">FIFO model</a> from the Nintendo DS is implemented. Plus, the ARM11↔︎ARM9 FIFO is also called ‘PXI’ <sup id="bibref:17"><a href="#bib:cpu-korth" role="doc-biblioref">[17]</a></sup>.</li></ul><h3 id="memory-available">Memory available</h3><p>Having three different CPUs also means the memory layout will not be simple, especially if you care about security.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png"><picture><img alt="Image" width="621" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png" loading="lazy"></picture></a><figcaption>Overview of memory organisation on the Nintendo 3DS.</figcaption></figure><p>To make a long story short, we’ve got the following blocks:</p><ul><li>From the developer’s perspective, the system provides <strong>128 MB of FCRAM</strong>. The New 3DS increased this to <strong>256 MB</strong>. The rest is redundant for games.</li><li>For predominantly security reasons, the ARM11 is also provided with a fast block of <strong>512 KB of SRAM</strong>. The ARM9 is also given a block of <strong>1 MB of SRAM</strong> (<strong>1.5 MB</strong> in the case of the New 3DS).</li><li>By inheriting the model of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-1-2-arm946e-s">Nintendo DS</a>, the ARM9 also houses <strong>Tightly-Coupled Memory</strong> (TCM). Particularly, there’s <strong>32 KB for instructions</strong> and <strong>16 KB for data</strong>.</li></ul><h4 id="a-new-type-of-memory-spotted">A new type of memory spotted</h4><p>It’s all jolly that the Nintendo 3DS includes 32 times the general-purpose memory of its predecessor, but what about that ‘FCRAM’? Is it any different from the other standards?</p><p>Well, <strong>Fast Cycle DRAM</strong> (FCRAM) is yet another RAM invention, this time authored in 2002 by Fujitsu and Toshiba. Presented as an alternative to DRAM-based technology (i.e.&nbsp;<a href="https://www.copetti.org/writings/consoles/xbox/#memory-layout">SDRAM</a>, <a href="https://www.copetti.org/writings/consoles/playstation/#the-offering">EDO DRAM</a>, <a href="https://www.copetti.org/writings/consoles/nintendo-64/#memory-design">RDRAM</a>, etc.), FCRAM excels on non-continuous reads, where it exhibits a lower latency than DRAM <sup id="bibref:18"><a href="#bib:cpu-fcram1" role="doc-biblioref">[18]</a></sup>. This was done to replicate the performance offered by the more expensive SRAM.</p><p>FCRAM competes directly with DDR DRAM by offering a revamped design of the memory arrays. In place of adding more circuitry on top of it, arrays are split into smaller subblocks, which are then accessed using a 3-stage pipeline <sup id="bibref:19"><a href="#bib:cpu-fcram2" role="doc-biblioref">[19]</a></sup>. In doing so, reading and writing on random locations become faster. These changes are still designed with backwards compatibility in mind. Thus, FCRAM is compatible with DDR DRAM controllers (hence, its full name is ‘DDR FCRAM’).</p><h3 id="faster-memory-transfers">Faster memory transfers</h3><p>The inventors of the MPCore and the AMBA bus happen to also offer a brand of <a href="https://www.copetti.org/writings/consoles/playstation/#taking-over-the-cpu">DMA controllers</a> called <strong>CoreLink</strong>, with Nintendo being a loyal client. So, it’s no mystery as to why the 3DS bundles multiple blocks of <strong>CoreLink DMA-330</strong> into their SoC <sup id="bibref:20"><a href="#bib:cpu-korth" role="doc-biblioref">[20]</a></sup>.</p><p>These DMAs in particular are attached to an AXI bus and act as master devices. They can transfer data between two slaves interconnected with the AMBA protocol (either AXI or the slower APB) with the following advantages:</p><ul><li>Faster transfer rates compared to either CPU.</li><li>Support of up to eight channels (eight transfers at the same time) <sup id="bibref:21"><a href="#bib:cpu-corelink" role="doc-biblioref">[21]</a></sup>.</li></ul><p>To be precise, Nintendo fitted one CoreLink DMA next to the ARM9, this is referred to as <strong>XDMA</strong> and provides <strong>up to four channels</strong>. There’s another DMA next to the ARM11 block, this time called <strong>CDMA</strong>, which provides <strong>up to eight channels</strong>. With the arrival of the New 3DS, another CoreLink DMA-330 is fitted next to the ARM11 block (now a quad-core cluster).</p><h3 id="programming">Programming</h3><p>With all being said, how do you program a system featuring this unorthodox CPU arrangement? To be fair, unusual systems are no strangers to videogame developers. But in this case, <strong>3DS programmers only have access to the ARM11 MPCore</strong>. Furthermore, once you reach the ‘Operating System’ section, you’ll learn the abilities with this cluster are further restricted.</p><p>In any case, no matter the console revision, programmers base their algorithms on the <strong>multi-threading model</strong>: the program groups sequences of instructions using <strong>threads</strong>, these are then dispatched by the operating system to the physical cores, as the former deems fit. Once a novelty for <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-programming-styles">Xbox 360 software</a>, this standard provides a layer of abstraction that blinds the developers from writing software only compatible with a fixed number and type of CPU cores.</p><h4 id="dealing-with-the-new-hardware">Dealing with the ‘New’ hardware</h4><p>Since the New 3DS diverts considerably from the original specification, Nintendo set up a thin compatibility layer to enable old 3DS games to work with the new hardware without manual intervention.</p><p>In essence, when a game is launched from a New 3DS console, the game’s code specifies if it’s specifically targeting the new models or not <sup id="bibref:22"><a href="#bib:cpu-applet_manager" role="doc-biblioref">[22]</a></sup>. If it is, the operating system will proceed to activate all the novelties (faster clock speed, extra RAM and use of L2 cache) for that game to enjoy. If it’s not, the operating system will keep its exclusive hardware deactivated until the user exits the game, so the game can safely assume it’s running on the old hardware and will do so without issue.</p><p>To keep supporting the old 3DS, games can be packaged with two codebases (one for the ‘New’ model and the other for the ‘Old’ one). It’s up to the game studios to decide whether to support the old and new 3DS, or only the new 3DS.</p><p>You may be wondering what happens with the rest of the exclusive hardware the New 3DS houses (i.e.&nbsp;extra ARM11 cores and DMA). Well, to properly understand the rationale, I explain this once you reach the ‘Operating System’ section, but I’m afraid you won’t like the answer!</p><hr><h2 id="graphics">Graphics</h2><p>Next to a new CPU is always a modern GPU. So, what kind of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Picture Processing Unit</a> did Nintendo build this time? To tell the truth, none. For the first time in their portable line, <strong>they resorted to a GPU supplier</strong>.</p><p>Nevertheless, the requirements of Nintendo haven’t shifted. The company still wanted a chip with acceptable performance… and the <strong>intellectual property</strong>. This will allow them to embed the GPU into their SoC, in the same way they did with the ARM CPUs.</p><figure><ul><li id="tab-4-1-kart-link"><a href="#tab-4-1-kart">Kart</a></li><li id="tab-4-2-sonic-link"><a href="#tab-4-2-sonic">Sonic</a></li><li id="tab-4-3-mario-link"><a href="#tab-4-3-mario">Mario</a></li><li id="tab-4-4-animal-link"><a href="#tab-4-4-animal">Animal</a></li><li id="tab-4-5-zelda-link"><a href="#tab-4-5-zelda">Zelda</a></li></ul><figure id="tab-4-1-kart"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_hu5324041165f0a097655cc1fd8a33a74b_77731_31339cf3bb9271013ab1a183d3738ff6.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png" loading="lazy"></picture></a><figcaption>Mario Kart 7 (2011)</figcaption></figure><figure id="tab-4-2-sonic"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png" loading="lazy"></picture></a><figcaption>Sonic Generations (2011)</figcaption></figure><figure id="tab-4-3-mario"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png" loading="lazy"></picture></a><figcaption>New Super Mario Bros.&nbsp;2 (2012)</figcaption></figure><figure id="tab-4-4-animal"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png" loading="lazy"></picture></a><figcaption>Animal Crossing: New Leaf (2012)</figcaption></figure><figure id="tab-4-5-zelda"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_huc7d8aa9f2330731c61256ecd3c86f738_72234_d95f355bd8ccd2a1150852bb8d302ac0.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png" loading="lazy"></picture></a><figcaption>The Legend of Zelda: Majora’s Mask 3D (2015)</figcaption></figure><figcaption>Example of Nintendo 3DS games. All render two frames of 400 x 240 pixels and one frame of 320 x 240 pixels.</figcaption></figure><p>Meanwhile, a potential candidate just finished unveiling their new invention at SIGGRAPH 2006 <sup id="bibref:23"><a href="#bib:graphics-dmp_insight" role="doc-biblioref">[23]</a></sup>. For some time, <strong>Digital Media Professionals Inc.</strong> (also known as ‘DMP’) have been building affordable GPUs for the embedded market and, while their chips are nothing out of the ordinary, they guarantee decent OpenGL ES support. Furthermore, their licensing framework offers <strong>synthesisable GPUs</strong>.</p><p>This seemed enough for Nintendo, who happily negotiated a license for DMP’s latest core, the <strong>PICA200</strong> and subsequently bundled it on CTR CPU (the Nintendo 3DS’ SoC). The GPU runs at <strong>268 MHz</strong>.</p><h3 id="architecture-of-the-pica200">Architecture of the PICA200</h3><p>If I had to summarise it in one sentence, the PICA200 is a budget low-power 3D processor that combines a pre-<a href="https://www.copetti.org/writings/consoles/xbox-360/#a-new-foundation-on-the-way">unified architecture</a> with a modernised API. The underlying architecture of the PICA200 is called <strong>Maestro 2G</strong> <sup id="bibref:24"><a href="#bib:graphics-siggraph" role="doc-biblioref">[24]</a></sup> and its design is compliant with <strong>OpenGL ES 1.1</strong>, but extended with elements from <strong>OpenGL ES 2.0</strong> <sup id="bibref:25"><a href="#bib:graphics-nintendo_gl" role="doc-biblioref">[25]</a></sup>. However, the PICA200’s APIs are not limited to either standard.</p><p>You see, even though the pipeline is segregated and the pixel stage is fixed-function (ala <a href="https://www.copetti.org/writings/consoles/playstation-2/#graphics">PlayStation 2</a>), DMP expanded the limited circuitry with a set of <strong>Maestro functions</strong> that provide capabilities beyond the expectations of the embedded market <sup id="bibref:26"><a href="#bib:graphics-ocp" role="doc-biblioref">[26]</a></sup>. This includes fragment lighting, multiple shadowing algorithms, polygon subdivision, bump mapping, procedural textures and many fog effects.</p><p>Additionally and in contrast to the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-5-3-result">Nintendo DS</a>, the PICA200 <strong>only works with framebuffers</strong>. That’s it. The <a href="https://www.copetti.org/writings/consoles/nes/#graphics">sprite engine</a>, a popular workaround to tackle unaffordable memory requirements, is now a thing of the past. This also includes <a href="https://www.copetti.org/writings/consoles/nes/#secrets-and-limitations">scan-line tricks</a>, as contemporary GPUs work way faster than the refresh rate of a CRT.</p><h4 id="organising-the-content">Organising the content</h4><p>Now that we know that this console can draw 3D shapes, the question now is: where does it store its materials? There are two locations, the large <strong>FCRAM</strong> block and the smaller but faster <strong>VRAM</strong>.</p><p>Nintendo only provided <strong>6 MB of VRAM</strong> exclusively for the GPU. Ideally, programmers would fit as much as they can there, but since it will fill up pretty quickly, it is expected to be used to store data that needs instant access (i.e.&nbsp;commands, buffers and recurrent textures) while placing the rest on FCRAM. The PICA200 comes with a <strong>DMA unit</strong> that can transfer data between FCRAM and VRAM. So, at the end of the day, it’s the responsibility of the programmer to come up with an efficient placement to avoid bottlenecks.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png"><picture><img alt="Image" width="939" height="605" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png" loading="lazy"></picture></a><figcaption>Example of how data is organised across the memory available.</figcaption></figure><p>During rendering, programmers allocate dedicated render buffers (i.e.&nbsp;frame, stencil, depth, etc.) for many operations. That’s always been the case. With the 3DS, alongside these buffers, programmers are also expected to reserve extra space for <strong>Display buffers</strong>, these are bound to the physical screens. The 3DS requires to allocate <strong>three Display buffers</strong> (two for the stereoscopic upper screen and one for the bottom one). To give you an idea, the display process works as follows:</p><ol><li>The LCD continuously displays the content of the front (active) Display buffer, as instructed by the value of the buffer index.</li><li>Meanwhile, the GPU finishes rendering geometry in a framebuffer.</li><li>The framebuffer is exported to the back (inactive) Display buffer.</li><li>The GPU swaps the index of the front Display buffer.<ul><li>For practical reasons, the index swap should happen at the end of <a href="https://www.copetti.org/writings/consoles/nes/#tab-5-5-result">Vertical Sync</a> to avoid tearing down the picture <sup id="bibref:27"><a href="#bib:graphics-opengl_swap" role="doc-biblioref">[27]</a></sup>. The official APIs provide synchronisation functions to keep all operations at the correct pace.</li></ul></li><li>The LCD will now be scanning the recently updated Display buffer from now on.</li></ol><h4 id="adopting-open-standards">Adopting open standards</h4><p>On an interesting note, just like the ARM11 MPCore adopts ARM’s AXI protocol for interconnecting its cores, DMP adopted a less-proprietary option called <strong>Open Core Protocol</strong> (OCP) <sup id="bibref:28"><a href="#bib:graphics-ocp" role="doc-biblioref">[28]</a></sup>. As its name indicates, the Open Core protocol does not impose any licensing restrictions on its users, something that vendors using the PICA200 may find advantageous. For comparison purposes, AXI was released in 2003 (along with the AMBA 3 specification) while OCP was published in 2001. It does make me wonder what kind of technology Nintendo fitted to adapt the OCP signal coming from the PICA200 into an AXI-compliant signal, so the rest of the SoC understands it. I assume that there’s a bridge between the PICA200 and the AXI bus.</p><p>Interestingly enough, the predecessor of the PICA200, the ULTRAY2000, shares many similarities with its successor. The most notable difference, however, is that the data interfaces use the PCI and DDR-SDRAM protocols instead <sup id="bibref:29"><a href="#bib:graphics-hardware" role="doc-biblioref">[29]</a></sup>.</p><h3 id="constructing-the-frame">Constructing the frame</h3><p>Naturally, the GPU is not aware of the stereoscopic or dual-screen nature of the displays, it will only be tasked with rendering three screens during gameplay:</p><ul><li><strong>Top stereoscopic-left</strong>: 400 x 240 pixels wide.</li><li><strong>Top stereoscopic-right</strong>: 400 x 240 pixels wide.</li><li><strong>Bottom</strong>: 320 x 240 pixels wide.</li></ul><p>All of them can display 8-bit RGB colours, which equates to up to 16.78 million colours.</p><p>Considering players will expect acceptable frame rates on all three screens (especially on the first two), the single PICA200 will be subject to high amounts of workload throughout its operation, an important aspect to remember when judging its performance.</p><p>That being said, here is an overview of how data travels to draw a single frame:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png"><picture><img alt="Image" width="548" height="479" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png" loading="lazy"></picture></a><figcaption>Overview of the graphics pipeline in PICA200.</figcaption></figure><p>… and as customary in this series of articles, we’ll now take a look at what happens at each stage.</p><div><ul><li id="tab-5-1-commands-link"><a href="#tab-5-1-commands">Commands</a></li><li id="tab-5-2-vertex-link"><a href="#tab-5-2-vertex">Vertex</a></li><li id="tab-5-3-geometry-link"><a href="#tab-5-3-geometry">Geometry</a></li><li id="tab-5-4-rasteriser-link"><a href="#tab-5-4-rasteriser">Rasteriser</a></li><li id="tab-5-5-fragment-link"><a href="#tab-5-5-fragment">Fragment</a></li><li id="tab-5-6-post-processing-link"><a href="#tab-5-6-post-processing">Post-processing</a></li></ul><div><div id="tab-5-1-commands"><h4 id="tab-5-1-commands">Commands</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png"><picture><img alt="Image" width="548" height="392" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png" loading="lazy"></picture></a><figcaption>Overview of the command stage.</figcaption></figure><p>This is Nintendo’s first portable console to finally draw triangles in ‘the usual way’. That is, with the use of commands. But it’s not a surprising factor, as the PICA200 is expected to abide by the teachings of OpenGL ES.</p><p>In essence, the PICA200 draws polygons by reading a <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-1-commands">command buffer</a> <sup id="bibref:30"><a href="#bib:graphics-nintendo_gpu_reg" role="doc-biblioref">[30]</a></sup>. Furthermore, the vertex data can either be embedded within the command or stored in a separate buffer in VRAM, with the latter being the most efficient.</p></div><div id="tab-5-2-vertex"><h4 id="tab-5-2-vertex">Vertex</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png" loading="lazy"></picture></a><figcaption>Overview of the vertex stage.</figcaption></figure><p>The PICA200 provides <strong>four Vertex Processors</strong> (VP) that operate in parallel. However, if the geometry shader (the next pipeline stage) is activated, only <strong>three</strong> processors can be utilised.</p><p>Each core computes 96-bit vectors made of four 24-bit floating-point values <sup id="bibref:31"><a href="#bib:graphics-picasso" role="doc-biblioref">[31]</a></sup>, but unlike the ARM11’s VFP, they don’t comply with IEEE-754 <sup id="bibref:32"><a href="#bib:graphics-shader_isa" role="doc-biblioref">[32]</a></sup>. The vertex processors are programmed using assembly language specific to the PICA200 (reminiscent of the days of the <a href="https://www.copetti.org/writings/consoles/xbox/#graphics">Nvidia NV30</a>) and are operated as follows <sup id="bibref:33"><a href="#bib:graphics-game-vertex" role="doc-biblioref">[33]</a></sup>:</p><ol><li>Developers write the vertex shader using PICA200 assembly. For reference, the instruction set is very similar to Microsoft’s <code>vs_2_0</code> <sup id="bibref:34"><a href="#bib:graphics-vs2" role="doc-biblioref">[34]</a></sup>.</li><li>The shader is compiled using a proprietary assembler.</li><li>The 3DS program must copy the compiled binary to memory (either FCRAM or VRAM).</li><li>Then, the 3DS program issues a GPU command to load the binary and connect it with the program.</li></ol><p>Once the vertex cores finish processing, they output the results to the <strong>Sync Control</strong> block, which acts as a vertex cache and buffer. It has a capacity of <strong>384 Bytes</strong>, enabling it to hold up to 32 96-bit vectors. Finally, the next stage reads from this block.</p></div><div id="tab-5-3-geometry"><h4 id="tab-5-3-geometry">Geometry</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png" loading="lazy"></picture></a><figcaption>Overview of the geometry stage.</figcaption></figure><p>The geometry stage is a signature feature of 8th-generation consoles, allowing developers to spawn complex geometry out of simple vertex data.</p><p>In this case, the PICA200’s geometry stage is implemented by <strong>stealing one of the four Vertex Processors</strong>. Then, the ‘geometry’ vertex core is loaded with a different vertex shader. Finally, it receives the vertex data from the three other processors.</p><p>Even though the geometry shader is programmable, in practice, <strong>Nintendo doesn’t allow this</strong>. Thus, game developers can only choose from a pre-programmed set of geometry shader programs (found in the SDK). Examples of available geometry shaders include square and line generation (using point primitives), geometry subdivision, silhouette edge rendering; and random particle generation.</p></div><div id="tab-5-4-rasteriser"><h4 id="tab-5-4-rasteriser">Rasteriser</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png"><picture><img alt="Image" width="428" height="345" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png" loading="lazy"></picture></a><figcaption>Overview of the rasteriser stage.</figcaption></figure><p>At this stage, all primitives are converted into pixels.</p><p>The rasterizer unit on the PICA200 is very simple, it just generates triangles out of primitives, then applies culling and clipping to remove unseen triangles (hidden behind others and/or outside the view area, respectively). This is all very similar to OpenGL ES’ modus operandi, albeit developers have to watch out for some coordinate systems that are inverted when working with the PICA200.</p></div><div id="tab-5-5-fragment"><h4 id="tab-5-5-fragment">Fragment</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png"><picture><img alt="Image" width="705" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png" loading="lazy"></picture></a><figcaption>Overview of the fragment stage.</figcaption></figure><p>The fragment stage is made of two areas: the <strong>texture units</strong>, which can fetch textures in memory and process them. And the <strong>shading unit</strong>, which can perform extra operations on the texture data.</p><p>The PICA200 contains <strong>four</strong> texture units <sup id="bibref:35"><a href="#bib:graphics-fragment" role="doc-biblioref">[35]</a></sup>, each houses <strong>256 Bytes of L1</strong> cache and all of them share <strong>8 KB of L2</strong> cache. However, the units are not homogenous. Instead, the range of services varies between each unit <sup id="bibref:36"><a href="#bib:graphics-pica_pipeline_diagram" role="doc-biblioref">[36]</a></sup>:</p><ul><li>Only three units can process 2D textures.</li><li>Only one unit can perform shadow, cube and <a href="https://www.copetti.org/writings/consoles/playstation-portable/#tab-2-4-textures">projective texture</a> mapping.</li><li>The last unit is more of a noise generator, meaning it only outputs <strong>random textures</strong>. It uses a combination of a random number generator and a colour lookup table. This is a slender yet efficient way of implementing <a href="https://www.copetti.org/writings/consoles/playstation-2/#infinite-worlds">procedure generation</a> with textures, saving bandwidth along the way.</li></ul><p>Afterwards, it’s the job of the shading unit to creatively fiddle with the textures coming in. However - and something unexpected considering we’re talking about an 8th-generation console - is that the PICA200’s unit is <strong>not programmable with <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-3-pixel">pixel shaders</a></strong> <sup id="bibref:37"><a href="#bib:graphics-kazakov" role="doc-biblioref">[37]</a></sup>. Instead, we find six <strong>configurable colour combiners</strong>, each combiner receives three RGB or Alpha values and performs a logical operation on them. The result is passed to the next combiner and so forth. Each colour combiner can get its input from the previous combiner (except the first), a texture unit or a constant value.</p><p>All in all, a modern reflection of the <a href="https://www.copetti.org/writings/consoles/gamecube/#tab-1-3-texture">Flipper era</a> (while abiding by the OpenGL specification <sup id="bibref:38"><a href="#bib:graphics-glTexEnv" role="doc-biblioref">[38]</a></sup>), but don’t forget developers may also combine this with the aforementioned Maestro functions.</p></div><div id="tab-5-6-post-processing"><h4 id="tab-5-6-post-processing">Post-processing</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png"><picture><img alt="Image" width="956" height="527" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png" loading="lazy"></picture></a><figcaption>Overview of the post-processing stage.</figcaption></figure><p>After the frame is processed and ready to be written into the framebuffer (or <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-4-pixel-shader">render targets</a>), it goes through a sequence of final ‘corrections’. This is similar to the OpenGL ES 2.0’s pipeline.</p><p>That being said, the frame goes through <strong>alpha</strong>, <strong>stencil</strong> and <strong>depth</strong> testing. Afterwards, the result can be mixed with an existing frame (in the framebuffer) using the colour blender or logical operators (AND, XOR, etc.). Finally, the frame is written into the assigned buffer in memory either as a whole or through a stencil filter (for masking).</p><p>For additional smoothing of the edges, the PICA200 can render the framebuffer at twice the selected dimensions, and then average it with antialiasing 2x2. This is an <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-4-post-processing">old technique</a> known as <strong>supersampling</strong>.</p><p>Once the framebuffer is ready to be displayed, it must be copied into another block in memory called <strong>Display Buffer</strong> (whose format is better aligned to the scan-line procedure of the LCD screen) and then transferred to the LCD in the form of scan-lines.</p></div></div></div><h3 id="interactive-comparison">Interactive comparison</h3><p>Now that you’ve seen how the PICA200 draws its triangles on the screen, it’s time for some practical examples. Here I’ve gathered two Marios from Smash Bros games, the Wii and 3DS one. Notice how the level of detail of ‘angry Mario’ hasn’t changed that much, considering we’re comparing a 2006 home console with its 2011 portable.</p><p>It’s worth reminding again that, in practice, the PICA200 will be rendering three screens at the same time, something that the <a href="https://www.copetti.org/writings/consoles/wii/#graphics">Wii’s GPU</a> wasn’t subjected to.</p><h3 id="nostalgic-rendering">Nostalgic rendering</h3><p>After all that’s been explained, there’s one question left unanswered: How does the PICA200 render Nintendo DS and Game Boy Advanced games? You may remember that the DS and GBA’s GPU exhibit completely different modus operandi for <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#the-3d-accelerator">rendering and broadcasting</a> frames.</p><figure><a href="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png"><picture><img alt="Image" width="256" height="192" src="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png" loading="lazy"></picture></a><figcaption>A frame rendered by the Nintendo DS’ <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Graphics Engine</a>, whose pipeline segregates between 2D and 3D data. That’s something the OpenGL-compliant PICA200 doesn’t understand.</figcaption></figure><p>The explanation is that the <strong>DS and GBA PPUs are housed in the SoC</strong> and DSi/DS/GBA games will operate them as they originally did on previous consoles. The PPUs output (scanlines) is delivered to a block called <strong>LgyFB</strong>, which may optionally upscale the frame, and then forwarded to the framebuffer, where the PICA200 will take care of displaying it. It’s the job of the ARM11 and its DMA to take care of all memory transfers during this process.</p><p>Naturally, this arrangement will add some delay (a.k.a. lag), albeit negligible to the user.</p><hr><h2 id="audio">Audio</h2><p>Overall, the SoC houses <strong>two audio blocks</strong>:</p><ul><li>A proprietary <strong>DSP</strong> exclusively programmed for sound operations. This is used by 3DS games.</li><li>A variant of the <a href="">Nintendo DS audio block</a> named <strong>CSND</strong>. 3DS, DS and GBA games use it.</li></ul><h3 id="the-3ds-only-hardware">The 3DS-only hardware</h3><p>You may know that this same DSP was previously bundled with the Nintendo DSi, but treated as an optional accelerator instead. With the 3DS, it’s become the designated audio processor, so it’s no longer a voluntary component.</p><p>The DSP is called <strong>CEVA TeakLite II</strong> <sup id="bibref:39"><a href="#bib:audio-teakra" role="doc-biblioref">[39]</a></sup> and operates at <strong>~134 MHz</strong> <sup id="bibref:40"><a href="#bib:audio-teakra_arch" role="doc-biblioref">[40]</a></sup>. It’s manufactured by ParthusCeva, a company that provides synthesisable cores for audio processing <sup id="bibref:41"><a href="#bib:audio-dsp_press" role="doc-biblioref">[41]</a></sup>, and I guess ‘synthesisable’ was the keyword Nintendo was looking for when they partnered.</p><p>Moving on, the DSP outputs stereo samples (<strong>2 channels</strong>) of up to <strong>32 kHz</strong> of sampling rate and <strong>16-bit</strong> resolution.</p><p>Next to this component, we can find <strong>512 KB of RAM</strong> that is used by the DSP as a working area. It’s double-buffered (256 KB per buffer), so both the CPU and DSP can read and write without interruption <sup id="bibref:42"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[42]</a></sup>. Apart from that, the DSP comes with a dedicated DMA that can transfer data in and out of those 512 KB.</p><h4 id="operation">Operation</h4><p>For all intents and purposes, games treat this as an opaque DSP. Thus, only Nintendo knows how to program it.</p><p>3DS programs, as a consequence of being developed using the official SDK, bundle a DSP firmware (solely authored by Nintendo) which is then uploaded to the DSP chip at runtime <sup id="bibref:43"><a href="#bib:audio-dsp_binary" role="doc-biblioref">[43]</a></sup>. Afterwards, programs rely on that firmware to execute audio-related routines. Furthermore, the audio services provided by the operating system further abstract the communication between the program and the DSP’s firmware <sup id="bibref:44"><a href="#bib:audio-dsp_services" role="doc-biblioref">[44]</a></sup>.</p><p>In any case, while the DSP firmware may change over the years, some capabilities have remained the same. For instance, the DSP can mix <strong>ADPCM</strong> and <strong>PCM</strong> samples. with support of up to <strong>24 channels</strong> <sup id="bibref:45"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[45]</a></sup>. There’s also functionality for filtering and sequencing, including the generation of <a href="https://www.copetti.org/writings/consoles/nes/#audio">PSG</a>-like sounds.</p><p>Interestingly enough, the steps followed for hacking the 3DS (so it can execute homebrew application) optionally involve extracting the HOME Menu’s DSP firmware, so homebrew may use it to provide audio output <sup id="bibref:46"><a href="#bib:audio-dsp_dump" role="doc-biblioref">[46]</a></sup>.</p><h3 id="the-backwards-compatible-block">The backwards-compatible block</h3><p>At the other end of the spectrum, we find the CSND block. 3DS may use it as an extension of the DSP and DS/DSi/GBA games rely on it to replicate their hardware.</p><p>In terms of functionality, the CSND features <strong>32 channels</strong> <sup id="bibref:47"><a href="#bib:audio-3ds_sound" role="doc-biblioref">[47]</a></sup>, which is twice the amount of the Nintendo DS counterpart.</p><p>Curiously enough, early homebrew defaulted to this block for providing sounds, while waiting for the DSP to be reverse-engineered.</p><h3 id="pipeline">Pipeline</h3><p>Both DSP and CSND work independently and separately output their audio to the speaker.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png"><picture><img alt="Image" width="665" height="303" src="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png" loading="lazy"></picture></a><figcaption>Overview of the audio pipeline.</figcaption></figure><p>As a curious note, the original Nintendo 3DS didn’t play well with the speaker’s capabilities, as Nintendo ended up providing troubleshooting guides for cases of buzzing noises and fluctuations with 3D slider <sup id="bibref:48"><a href="#bib:audio-buzzing" role="doc-biblioref">[48]</a></sup>, all caused by the design of the case.</p><hr><h2 id="io">I/O</h2><p>This section tends to be very rich in technologies considering Nintendo’s consoles favour generous I/O before state-of-the-art CPUs and GPUs. Let’s see what the Nintendo 3DS offers.</p><h3 id="external-interfaces-and-peripherals">External interfaces and peripherals</h3><p>The Nintendo DS had tons of modules built-in and the Nintendo DSi added more on top of it (after removing the GBA Slot). Now we found ourselves with a new console combining interfaces from two decades (the 2000s and 2010s).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_01c078a07c7bbb5949a953c8edcc6b9f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_2c4c532e83c7d9e10248df14d45e089e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_678c74a178b4bb3822d17fa443a083bc.webp 2252w"><img alt="Image" width="2252" height="971" src="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png" loading="lazy"></picture></a><figcaption>Main diagram of the console’s architecture. You can sense that the I/O area on the left side was a strong selling point of this console.</figcaption></figure><p>To be fair, we still don’t have a standard like USB, but that may be expendable considering the Nintendo 3DS bundles the following:</p><ul><li>A keypad composed of <strong>digital buttons</strong>, an <strong>analogue circle pad</strong>.</li><li>3D and volume <strong>sliders</strong>.</li><li>A Wi-Fi <strong>switch</strong>.</li><li>A <strong>resistive touch sensor</strong> on the bottom screen.</li><li>A <strong>gyroscope</strong> measuring the console’s rotation changes.</li><li>An <strong>accelerometer</strong> to measure the console’s motion.</li><li><strong>One front camera</strong> and <strong>two back cameras</strong>, the latter allowing to take stereoscopic pictures.</li><li>An <strong>infrared transceiver</strong>, used to transfer data between external accessories.</li><li>An <strong>SD card</strong> slot, serving as external storage.</li><li>A standard <strong>3.5 mm jack socket</strong> for headphones.</li><li>A <strong>game card reader</strong>, where 3DS, DSi and DS retail games are read from.</li></ul><h4 id="the-new-enhancements">The ‘New’ enhancements</h4><p>If that wasn’t enough, the New 3DS came with more modules on top. This includes:</p><ul><li>Two <strong>extra digital buttons</strong> and an extra analogue circle pad (called <strong>‘C-Stick’</strong>).</li><li>An <strong>NFC Reader</strong> on the bottom screen.</li><li>An <strong>infrared LED</strong>, reserved for head tracking.</li><li>The SD slot is replaced with a <strong>microSD slot</strong>.</li><li>The Wi-Fi switch has been removed, now it’s only controlled through software.</li></ul><p>Now, to prevent leaving ‘old’ users behind, Nintendo provided external accessories to enhance the old models, although most of them relied on the single infrared transceiver to connect. Thus, only one accessory could be connected at the same time.</p><p>Not all the exclusive features of the New 3DS can be replicated, however. For instance, the New 3DS’ head tracking mechanism depends on the extra ARM11 core.</p><h3 id="internal-interfaces">Internal interfaces</h3><p>Now it’s time to check how are these interfaces - and others - internally wired up.</p><p>Firstly, a large subset is interconnected with the standard <strong>Serial Peripheral Interface</strong> (SPI) protocol. There are four SPI buses and all of them are accessed by the ARM9 (which I assume also includes the ARM7). The ARM11 only has access to <em>most</em> of them <sup id="bibref:49"><a href="#bib:io-spi_registers" role="doc-biblioref">[49]</a></sup>. In any case, the SPI buses connect the following modules <sup id="bibref:50"><a href="#bib:io-spi_devices" role="doc-biblioref">[50]</a></sup>:</p><ul><li>The flash memory found inside 3DS game cards, for storing save data.</li><li>DS’ Power Management.</li><li>Parts of the Wi-Fi chip.</li><li>Touch screen.</li><li>Sound.</li><li>Microphone.</li><li>Circle Pad.</li></ul><p>Curiously enough, some peripherals are interfaced twice to replicate the old DS/DSi’s I/O layout and also provide extended capabilities for 3DS software.</p><p>Secondly, there’s a <strong>Human-interface device</strong> (HID) module connected to both ARM11 and ARM9 data buses. This is how the digital keypad is accessed. The data is read through a 16-bit register.</p><p>Moving on, we got an <strong>I²C</strong> block which uses a more sophisticated serial protocol. This is connected to the following <sup id="bibref:51"><a href="#bib:io-i2c_devices" role="doc-biblioref">[51]</a></sup>:</p><ul><li>Front camera, also works in DSi mode.</li><li>Two back cameras, the right camera is accessible in DSi mode as well.</li><li>Infrared transceiver.</li><li>The NFC interface, in the case of the New 3DS.</li><li>The ‘QTM’ module, used for head-tracking (New 3DS only).</li><li>Gyroscope.</li><li>MCU chip, a separate controller that interfaces more components (explained in the next section).</li></ul><p>Finally, there are various <strong>registers</strong> interfacing FIFO blocks which, in turn, connect to two relatively high-speed (16 MB/s) peripherals <sup id="bibref:52"><a href="#bib:io-misc" role="doc-biblioref">[52]</a></sup>:</p><ul><li>Internal eMMC memory.</li><li>SD card slot.</li></ul><p>As confusing as it may sound, there’s more hardware left to discuss. The rest is handled by a middle-man chip called <strong>Auxiliary Microcontroller</strong> (MCU) <sup id="bibref:53"><a href="#bib:graphics-hardware" role="doc-biblioref">[53]</a></sup>. This is just a microcontroller designed by NEC and manufactured by Renesas. Particularly, the <strong>model 78K0R</strong>, which bundles a proprietary (yet low-power and relatively modern) processor and a ROM <sup id="bibref:54"><a href="#bib:io-renesas" role="doc-biblioref">[54]</a></sup>. The 78K0R stores a firmware handled by the console’s operating system, both ARM9 and ARM11 can interact with it but so do other peripherals.</p><p>The MCU chip exclusively controls the following <sup id="bibref:55"><a href="#bib:graphics-hardware" role="doc-biblioref">[55]</a></sup> <sup id="bibref:56"><a href="#bib:io-i2c_mcu" role="doc-biblioref">[56]</a></sup>:</p><ul><li>Accelerometer.</li><li>LCD screens.</li><li>LED indicators.</li><li>Power Management.</li><li>Battery fuel gauge and rejection (whether to enable the charging circuitry or not).</li><li>Wi-Fi’s EEPROM.</li><li>Real-Time Clock (RTC).</li><li>3D slider and Wi-Fi switch, the latter is only found on old 3DS models.</li><li>HOME and power buttons.</li></ul><p>A subset of this group is already accessible by the main CPUs. This is because the MCU also perform monitoring tasks, thereby saving resources from the ARM11 or ARM9.</p><h3 id="ready-for-trends">Ready for trends</h3><p>With such a heavy list of I/O hardware, you can now see how Nintendo tried to compete against the smartphone market. This led to interesting services deployed throughout the console’s lifecycle:</p><ul><li>A <strong>QR Reader</strong> bundled with the camera app.</li><li><strong>AR Games</strong>: Nintendo shipped ‘AR Cards’ that could be scanned with the 3DS camera using an app called ‘AR Games’. This would make static Nintendo characters pop up in your room, like any traditional augmented-reality-based application.</li><li><strong>Face Riders</strong>: Another camera-based app, but in this case, takes a photo of the player to compose the game’s characters. The player must then use the gyroscope and microphone to battle his/her evil clones.</li><li><strong>Amiibos</strong>: Uses the NFC reader to scan figurines and unlock game bonuses, the same service was also <a href="https://www.copetti.org/writings/consoles/wiiu/#the-supplemental-interface">implemented in the Wii U</a>.</li><li><strong>SpotPass</strong>: The continuation of <a href="https://www.copetti.org/writings/consoles/wii/#games">WiiConnect24</a>, now automatically connects to unsecured Wi-Fi access points.</li><li><strong>StreetPass</strong>: Automatically exchanges data between nearby 3DS systems. Nintendo marketed it as a way of connecting random 3DS players on the street.</li><li><strong>Play Coins</strong>: Unlocks game content by doing some exercise (walking).</li></ul><hr><h2 id="operating-system">Operating System</h2><p>Having a large number of CPUs eventually impacts the overall complexity of the operating system. Not only that, but this console also stores more than one OS. This originates as a mechanism for providing large services (i.e.&nbsp;DSi/DS/GBA backwards compatible, rescue mode, etc.).</p><p>So, to avoid making this section any more confusing, let’s go by steps.</p><h3 id="architecture">Architecture</h3><p>The Nintendo 3DS, as a whole, comes with four firmware <sup id="bibref:57"><a href="#bib:operating_system-firm" role="doc-biblioref">[57]</a></sup>:</p><ul><li><strong>NATIVE_FIRM</strong>: Operates the console in ‘native’ mode (with the functionality exclusive to the Nintendo 3DS). Here, the ARM11 executes the main program.<ul><li>Curiously enough, there are two instances of NATIVE_FIRM installed (named <strong>FIRM0</strong> and <strong>FIRM1</strong>, respectively) in case the first one gets corrupted, for some reason.</li><li>This firmware is often referred to as ‘Horizon’ as well.</li></ul></li><li><strong>TWL_FIRM</strong>: It commands the Nintendo 3DS to behave like a Nintendo DSi. It does come at the expense of disabling all the exclusive features, but considering how the CPU, GPU, sound and I/O are intertwined; TWL_FIRM is truly a work of art. Consequently, the ARM9 and ARM7 are placed in the foreground (they execute the main program) <sup id="bibref:58"><a href="#bib:operating_system-gbatek_firm" role="doc-biblioref">[58]</a></sup>.<ul><li>The name ‘TWL’ comes from the codename of the Nintendo DSi.</li></ul></li><li><strong>AGB_FIRM</strong>: Similarly to TWL_FIRM but the 3DS now becomes a Game Boy Advance. Here, the ARM7 executes the main program.</li><li><strong>SAFE_FIRM</strong>: Used solely for maintenance-related tasks, such as system updates. This firmware is basically an early revision of NATIVE_FIRM (doesn’t go beyond version <code>3.0</code> on the old 3DS and version <code>8.1</code> on the New 3DS <sup id="bibref:59"><a href="#bib:operating_system-safehax" role="doc-biblioref">[59]</a></sup>).</li></ul><p>All of these firmware come with separate binaries for the ARM11, ARM9 and ARM7 CPUs. The only exception is that the ARM7 won’t be active under NATIVE_FIRM and SAFE_FIRM.</p><p>Generally speaking, the Nintendo 3DS will first launch a Boot ROM and then bootstrap NATIVE_FIRM. Afterwards, the running operating system may choose to reboot to another firmware based on the user’s actions (i.e.&nbsp;load a Nintendo DS game or run the firmware update assistant).</p><p>Let’s take a look now at how each CPU behaves in NATIVE_FIRM mode.</p><h4 id="the-security-processor">The security processor</h4><p>Once NATIVE_FIRM is bootstrapped, the ARM9 runs its own operating system made of a kernel called <strong>Kernel9</strong> and a single program called <strong>Process9</strong> <sup id="bibref:60"><a href="#bib:operating_system-overview" role="doc-biblioref">[60]</a></sup>.</p><p>Kernel9’s design follows the <strong>microkernel</strong> model, meaning it only provides essential abstraction with the hardware, including:</p><ul><li>Memory management.</li><li>Process scheduling.</li><li>Inter-Process Communication.</li></ul><p>On the other side, Process9 is a userland application that implements these services:</p><ul><li>Communication with the ARM11, called ‘PXI’.</li><li>Cryptography-related functions. This involves AES, RSA, SHA and ECDSA.</li><li>I/O management.</li><li>File System.</li><li>Title (3DS software) verification and installation.</li></ul><p>Both Kernel and Process9 reside on an ARM9-only block of 1 MB of SRAM (1.5 MB in the case of the New 3DS).</p><p>In terms of security, there’s no privilege distinction between Kernel9 and Process9, since the latter has unconditional access to a system call that runs arbitrary code in kernel mode.</p><p>In summary, combined with the exclusive I/O hardwired into the ARM9, this CPU has the role of a <strong>security processor</strong>, much like what the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii and Wii U’s ARM9</a> also did, and unlike the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#design">co-processor architecture</a> of the Nintendo DS, where its second processor just offloaded I/O and audio tasks.</p><h4 id="the-user-processor">The user processor</h4><p>Likewise, the ARM11 runs a kernel of similar architecture to the ARM9 one (now called <strong>Kernel11</strong>). The big difference is that the ARM11 will be running multiple userland processes, and in doing so they provide services like:</p><ul><li>Communication with the ARM9, called ‘PXI’.</li><li>Multi-core processing.</li><li>Networking, HTTP and SSL.</li><li>Connection with Nintendo online infrastructure.</li><li>The graphical shell (called ‘HOME Menu’).</li><li>The ability to launch applications.</li><li>A layer of abstraction for apps called <strong>Services</strong>, which games must call to access hardware resources. Some components like the GPU are interfaced by a very thin API, nonetheless.<ul><li>Furthermore, services are implemented in a layered manner. Games only access a subset of these, and the latter in turn invokes greater privileged and specialised services.</li></ul></li></ul><p>The ARM11’s kernel resides on a dedicated block 512 KB of SRAM <sup id="bibref:61"><a href="#bib:operating_system-glossary" role="doc-biblioref">[61]</a></sup>, also called ‘AXI Work RAM’ or ‘AXI WRAM’, because it’s connected to the ARM11 using the AXI protocol.</p><h4 id="imposed-behaviour">Imposed behaviour</h4><p>Now for the bitter news, NATIVE_FIRM also enforces unusual restrictions on user programs.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png"><picture><img alt="Image" width="761" height="519" src="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png" loading="lazy"></picture></a><figcaption>Overview of the privilege levels in the Nintendo 3DS running NATIVE_FIRM, after combining both operating systems.</figcaption></figure><p>To start with, the ARM11’s scheduler is hard-coded with specific behaviour for each core (as opposed to treating every core as general-purpose units):</p><ul><li>The first core, called <strong>appcore</strong>, is for the game to use. Yet, its thread scheduling policy is FIFO, meaning that the game can deadlock itself if multi-threading is improperly used <sup id="bibref:62"><a href="#bib:operating_system-multithreading" role="doc-biblioref">[62]</a></sup>.</li><li>The second core, called <strong>syscore</strong>, gets assigned system-related tasks.</li></ul><p>Conversely, syscore can lend 30% of its execution time to user applications, which may be helpful to offload some operations, although not every routine may work under syscore (especially time-sensitive ones) <sup id="bibref:63"><a href="#bib:operating_system-dsx86" role="doc-biblioref">[63]</a></sup>.</p><p>The curtailment is further extended with the New 3DS, whose ARM11 MPCore now comes with four cores, namely:</p><ul><li><strong>The fourth MP11 is solely used for face-tracking</strong>. Instead of adding circuitry, Nintendo engineers implemented face-tracking through pure software. Thus, it reserves one MP11 for this. I’m guessing this was a cost-effective solution for Nintendo.</li><li><strong>The third MP11 core is permanently idle</strong>, as the scheduler is never instructed to dispatch threads there. Game meta-data does include a flag to enable thread scheduling on this core, albeit its usage on any commercial game is yet to be confirmed. It’s highly possible that at one point Nintendo considered it, but ultimately rendered it unfeasible for battery consumption reasons, or maybe because single-core games would suffer some compatibility issue.<ul><li>Considering the New Nintendo 2DS XL is a New Nintendo 3DS without the stereoscopic screen, that means half of its quad-core CPU is wasted!</li></ul></li><li>CDMA, the ‘New’ DMA unit, is only accessible during the console’s boot <sup id="bibref:64"><a href="#bib:operating_system-dma" role="doc-biblioref">[64]</a></sup>. After the boot process finishes, <strong>CDMA is never used again</strong>.</li></ul><p>Moving on, in terms of usable RAM for games, we know that the Nintendo 3DS and New Nintendo 3DS come with 128 MB and 256 MB of FCRAM, respectively. What you need to know now is that the available RAM for apps is only <strong>64 MB</strong> and <strong>124 MB</strong> <sup id="bibref:65"><a href="#bib:operating_system-memory" role="doc-biblioref">[65]</a></sup>, respectively. This means that the OS consumes ~50% of the console’s main memory, not a particularly pleasant quality! To alleviate this, games also have the option to set a flag in their metadata (called <code>APPMEMTYPE</code>) to claim more FCRAM from the system, up to <strong>96 MB</strong> and <strong>176 MB</strong>, respectively. Behind the scenes, that flag instructs the system to reboot the console and boot the game without launching the HOME Menu beforehand, saving memory in the way.</p><p>All things considered, you can now sense how not all extra hardware in the New 3DS will automatically imply faster software. It’s a shame, and it gives me the feeling that the New 3DS was a rushed product, from the software perspective. But to be fair, Nintendo never planned the ‘New’ 3DS to be a full successor of the original 3DS. The ‘New’ brand was a clear move to refresh the 3DS line, considering the sales number wasn’t satisfying, to say the least.</p><h3 id="storage-medium">Storage Medium</h3><p>Now that we know how the operating system is designed, let’s look at where and how data is stored in this console.</p><div><ul><li id="tab-6-1-boot-roms-link"><a href="#tab-6-1-boot-roms">Boot ROMs</a></li><li id="tab-6-2-otp-memory-link"><a href="#tab-6-2-otp-memory">OTP memory</a></li><li id="tab-6-3-emmc-nand-link"><a href="#tab-6-3-emmc-nand">eMMC NAND</a></li><li id="tab-6-4-sdmicrosd-link"><a href="#tab-6-4-sdmicrosd">SD/microSD</a></li></ul><div><div id="tab-6-1-boot-roms"><h4 id="tab-6-1-boot-roms">Boot ROMs</h4><p>Following its long ancestor, the <a href="https://www.copetti.org/writings/consoles/game-boy/#cpu">Game Boy</a>, the SoC stores a series of unencrypted ROMs containing the programs used for booting up NATIVE_FIRM <sup id="bibref:66"><a href="#bib:operating_system-bootloader" role="doc-biblioref">[66]</a></sup>. These bootstrappers are called <strong>Boot9</strong> and <strong>Boot11</strong>; and are executed by the ARM9 and ARM11, respectively. Likewise, they are physically and virtually kept hidden for security reasons. To give you an example, Boot9 stores AES decryption keys, which are not something to carelessly leave anywhere.</p><p>Interestingly enough, Boot9’s code has revealed that it’s more capable than just bootstrapping NATIVE_FIRM from eMMC NAND. However, due to certain routines having hardcoded directories and security layers added on top, the only firmware the Boot ROMs can ultimately load is NATIVE_FIRM from eMMC NAND.</p><p>Moreover, while multiple components have changed with the arrival of the New 3DS, the BootROMs have not changed a bit <sup id="bibref:67"><a href="#bib:operating_system-boot" role="doc-biblioref">[67]</a></sup>.</p></div><div id="tab-6-2-otp-memory"><h4 id="tab-6-2-otp-memory">OTP memory</h4><p>To further increase the level of security, the console stores a series of console-unique information in <strong>One-Time-Programmable</strong> (OTP) memory <sup id="bibref:68"><a href="#bib:operating_system-otp" role="doc-biblioref">[68]</a></sup>. Similarly to the <a href="https://www.copetti.org/writings/consoles/wii/#tab-7-1-shared-encryption">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#tab-7-1-dedicated-hardware">Wii U</a>, this information also includes encryption keys.</p><p>OTP is written once during manufacturing, so the keys differ between each console. Hence, one hacked console won’t necessarily be able to compromise the rest. This is a significant milestone for a portable console, considering a certain <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-10-1-encryption-system">previous implementation</a> included global keys.</p></div><div id="tab-6-3-emmc-nand"><h4 id="tab-6-3-emmc-nand">eMMC NAND</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/emmc.8c2a1c478ffded9127a1389a411ec8ff8eb6b16c39a7ebc62f693480261828e4.webp"><picture><img alt="Image" width="900" height="699" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hua5f2cd62267f437d12e49c5b8a374e56_58072_86c90e2811a715b2aa2a3980f340716e.png" loading="lazy"></picture></a><figcaption>Samsung-made eMMC chip on the original 3DS.</figcaption></figure><p>Next to the big SoC, there’s an eMMC NAND Flash chip. However, its size is slightly different depending on the manufacturer <sup id="bibref:69"><a href="#bib:operating_system-fs" role="doc-biblioref">[69]</a></sup>. For instance, Toshiba supplied <strong>943 MB</strong> and <strong>1,888 MB</strong> chips, while Samsung provided ones with <strong>954 MB</strong> and <strong>1,240 MB</strong>.</p><p>To tackle this disparity, Nintendo defined the 3DS partition table using a common size: <strong>943 MB</strong> for Old 3DS (Toshiba-size) and <strong>1,240 MB</strong> (Samsung-size) for New 3DS. So, if your console came with a larger eMMC chip, the extra space is unfortunately left unemployed.</p><p>In any case, the console relies on eMMC for storing its system data, including the multiple firmware, and user data (saves and configs within 3DS and DSi mode).</p></div><div id="tab-6-4-sdmicrosd"><h4 id="tab-6-4-sdmicrosd">SD/microSD</h4><p>Once an optional (and sometimes, symbolic) medium, SD cards now enjoy similar responsibilities to internal storage, as the 3DS is dependent on it to download software from the eShop and store user data (game saves, camera pictures and microphone recordings) <sup id="bibref:70"><a href="#bib:operating_system-sd" role="doc-biblioref">[70]</a></sup>.</p><p>Software and user data stored here are protected with AES-128-CTR encryption.</p></div></div></div><h3 id="boot-process">Boot process</h3><p>Now that we know how the operating system is structured and where data is stored, let’s see how the Nintendo 3DS goes from a glossy powered-off brick to becoming an operating console offering multiple services.</p><h4 id="multi-core-chaos">Multi-core chaos</h4><p>Considering the 3DS must be able to manage four processors (2-core ARM11 + ARM9 + ARM7) in its SoC - or six, if you look at the New 3DS - one can only wonder how these ‘central’ processors suddenly become exceptionally coordinated during the console’s startup. Well, it’s all about implementing a master-slave hierarchy.</p><p>With the ARM9 and ARM7, there’s not a lot of room for doubt, both can be powered on separately and load different binaries. So the challenge is mainly focused on disseminating the homogenous multi-core ARM11.</p><p>In the ARM11 MPCore cluster, all cores start execution at vector <code>0x00000000</code> <sup id="bibref:71"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[71]</a></sup>. However, CP15 (the System Control co-processor) provides a register called <code>CPU ID</code> which, among other things, serves to identify the core currently executing instructions. Thus, programmers can query this register to decide whether the current CPU core should give orders (master) or wait for commands (slave). ARM later improved this technique by supplying a dedicated register called <code>mpidr</code>, found in ARMv7 CPUs.</p><p>Thanks to this, Nintendo engineers were able to identify any CPU core within the 3DS cluster and implement a bootloader where all cores become coordinated, and then carry out the necessary functions to bring the console to life.</p><h4 id="boot-procedure">Boot procedure</h4><p>Time to dive into the boot process. As with any other console of its generation, security is of great importance, which will have an impact on the boot stage. To avoid making this section too dense, I’ve simplified the stages where the security system is set up, but you’ll find more information in the ‘Anti-piracy’ section.</p><p>Having said that, once the console is powered on, the following sequence of events takes place <sup id="bibref:72"><a href="#bib:operating_system-boot" role="doc-biblioref">[72]</a></sup> <sup id="bibref:73"><a href="#bib:operating_system-bootrom" role="doc-biblioref">[73]</a></sup>:</p><ol><li>The ARM9 and ARM11 power up.</li><li>The ARM9’s reset vector is at address <code>0xFFFF0000</code>, which points to Boot9 <sup id="bibref:74"><a href="#bib:operating_system-memory" role="doc-biblioref">[74]</a></sup>. The ARM11 is induced in an infinite reset until its reset pin is lifted.</li><li>Boot9 clears ARM11’s reset pin and then initialises the ARM9’s MPU.</li></ol><p>The ARM11 MPCore will now start execution of Boot11 in parallel:</p><ol><li>ARM11’s reset vector is at address <code>0x00000000</code> <sup id="bibref:75"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[75]</a></sup>, which happens to be in the same place as Boot11.</li><li>Boot11 will branch depending on which core is it being executed on. If it’s greater than core 2, it hangs indefinitely.</li><li>Wait until ARM9 is finished bootstrapping a firmware</li></ol><p>Meanwhile, the ARM9 will be busy continuig with Boot9 execution:</p><ol start="4"><li>The AES and RSA public keys are exported to the AES and RSA engines (these will be explained in the ‘Anti-piracy’ section).</li><li>Boot9 will try to boot from NAND.<ol><li>In NAND, there’s a partition at location <code>0x0</code> called ‘NCSD header’, this states that there are eight partitions, each with a firmware to boot from.</li><li>For each firmware partition listed, Boot9 will fetch its header, validate the SHA-256 hash and RSA-2048 signature (using a set of keys previously loaded from BootROM) and repeat this process until one validation succeeds. Then, it will boot from there.</li></ol></li><li>If all validations in NAND fail, Boot9 will try to boot from a Flash memory in the Wi-Fi module. If that also fails, the console will display an error screen.</li><li>The first partition validated happens to contain NATIVE_FIRM. Boot9 will proceed to copy the firmware to different memory areas based on the header’s parameters.</li><li>Disable half of Boot9 and Boot11. In doing so, FCRAM will be accessible.</li><li>Redirect ARM9 and ARM11’s execution to the firmware’s entry points.</li></ol><p>Now that NATIVE_FIRM is bootstrapped:</p><ol><li>The ARM9 will:<ol><li>Load Kernel9.</li><li>Kernel9 hides OTP memory and loads Process9.</li><li>The ARM9 CPU is now up and running Process9.</li></ol></li><li>Whilst the ARM11 does the following:<ol><li>Load Kernel11.</li><li>In the case of the New 3DS, Kernel11 will write to a new register called <code>CFG11_BOOTROM_OVERLAY_CNT</code> to overlay Boot11 code <sup id="bibref:76"><a href="#bib:operating_system-pdn" role="doc-biblioref">[76]</a></sup>. This will allow to redirect execution of the new ARM11 cores (core 3 and core 4) away from Boot11 to arbitrary functions in Kernel11, thereby taking control of them.</li><li>Kernel11 will start various system processes, including PM (Process Manager).</li><li>PM will start the ‘NS’ (Nintendo User Interface Shell) system module.</li><li>NS will either launch a game or the HOME Menu application.</li><li>The user is now in control.</li></ol></li></ol><h4 id="alternative-boot-processes">Alternative boot processes</h4><p>All of the previous explanations have been focused on booting up NATIVE_FIRM, which results in the traditional native 3DS mode. For other firmware such as TWL_FIRM, AGB_FIRM and SAFE_FIRM, it’s a bit more complicated. Turns out the previous boot process is still needed because only NATIVE_FIRM can be booted from a power cycle. But once this is running, it can bootstrap any of those firmware, and each will program the ARM9 differently. In either case, the security set-up during Boot9 will still be enforced.</p><p>TWL_FIRM and AGB_FIRM, in particular, operate a special set of registers that mould the 3DS hardware and memory layout in accordance with what DS, DSi or GBA games expect to find. FCRAM can still be accessed, allowing to boot a game ROM from those places as well (apart from the NTR card reader). However, FCRAM will be reconfigured to follow the DS and GBA bus specification (16-bit wide, instead of 32-bit).</p><p>A big difference about the backwards compatible firmware is that, at last, the ARM7 will be active (as Nintendo DSi/DS and GBA software require it).</p><p>To exit either mode, non-NATIVE firmware contain a routine that reboots the system and consequently returns it to NATIVE_FIRM. Thus, 3DS mode.</p><h3 id="interactive-shell">Interactive shell</h3><p>The 7th generation of console interfaces has landed on the Nintendo 3DS. A clear indication is that users don’t need a retail game to make the most out of their console, just navigate through the shell and you’ll find numerous apps and services bundled. This includes the special offering of this console (3D camera, stereoscopic view and augmented reality). The pressure to compete against smartphones couldn’t be clearer.</p><p>In terms of user interface design, I’m inclined to say there are many patterns borrowed from the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System Menu</a>, yet ported to a dual-screen portable system. The <strong>HOME Menu</strong> (name of the interactive shell) uses a 1-page navigation system where every installed application is shown on a scrollable grid. Except for a few shortcuts here and there, every service is an application to be launched.</p><p>Now, being a Nintendo product, you can expect a special focus on creativity and attention to detail. Families are the target audience, nevertheless, adults are the ones paying, and Nintendo knows that.</p><h4 id="maintaining-consistency">Maintaining consistency</h4><p>The NS module is not only responsible for launching the interactive shell, it also offers 3DS software with the ability to invoke routines to handle certain interactions. One example is the ‘Back to HOME Menu’ overlay, which must be shown whenever the user presses the ‘HOME’ button.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png" loading="lazy"></picture></a><figcaption>Users can press the ‘HOME’ button midgame, this will reinstate the HOME Menu without closing the current application. This event is handled by the running application but the routines are provided by the NS service.</figcaption></figure><p>Furthermore, 3DS software may also invoke ‘mini applications’ for attending other events (i.e.&nbsp;show the virtual keyboard), these are known as <strong>Applets</strong> <sup id="bibref:77"><a href="#bib:operating_system-ns" role="doc-biblioref">[77]</a></sup>.</p><p>Both sets are a crucial dependency for all applications, as they are responsible for properly reacting to external events consistently. Interestingly enough, since Applets and NS routines are not part of the game itself, in the case of New 3DS systems, even if a game is running in compatibility mode (that is, with all the ‘New’ hardware disabled), they will still be executed using the full extend of hardware, giving a small performance boost to unoptimised 3DS games.</p><h4 id="the-legacy-shell">The legacy shell</h4><p>Whilst the special firmware includes the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#operating-system">old BIOS routines</a> DS/GBA games will expect, there’s no DS or DSi shell in sight.</p><p>The old <strong>Wi-Fi setup screens</strong> (invoked by DSi and DS games) are the only exceptions. Interestingly enough, while the original DS Wi-Fi settings are useless (as they can only connect to WEP-protected access points), the DSi counterpart (accessed from DSi and ‘DSi enhanced’ DS games) can alter the 3DS’ Wi-Fi settings. Yet, these games bundle an old Wi-Fi driver that only worked with the real DSi (the 3DS contains an Atheros AR6014 while the DSi came with an Atheros AR6002 or AR6013). So, to tackle this, both Wi-Fi settings are automatically synced when the firmware boots up <sup id="bibref:78"><a href="#bib:operating_system-firm" role="doc-biblioref">[78]</a></sup>.</p><h4 id="updatability">Updatability</h4><p>Well, of course, an updatable system is pretty much a requirement, not only for providing new functionality but also from a security perspective.</p><p>You can update the system software either online or through a game cartridge. Confusingly enough, both contain different update packages. Game cartridges only bundle system updates without the updated user apps, whereas network updates include everything <sup id="bibref:79"><a href="#bib:operating_system-home" role="doc-biblioref">[79]</a></sup>. Consequently, version names encode a mix of the two, in case the user used both channels.</p><p>To install updates, the NS service reboots into <code>SAFE_FIRM</code>, where the <strong>System Updater</strong> takes care of this process <sup id="bibref:80"><a href="#bib:operating_system-settings" role="doc-biblioref">[80]</a></sup>.</p><hr><h2 id="games">Games</h2><p>It’s time to check how game development and distribution were carried out. Additionally, we’ll see some exclusive services Nintendo prepared for this console.</p><h3 id="development-ecosystem">Development ecosystem</h3><p>Before the Nintendo 3DS arrived, developing for embedded system involved monumental efforts and high levels of patience. Compared to desktop applications, the tooling wasn’t standardised and sometimes it didn’t converge well with each other (ActiveSync is the clearest example I remember). The range of documentation didn’t usually go beyond what the manufacturer provided, the same applied for technical support.</p><p>Enter the 2010s decade, coinciding with the influx of an ARM-based smartphone industry and more efficient compilers, development for those platforms was no longer a complicated endeavour. Consequently, game studios developing for the Nintendo 3DS were able to enjoy this evolution. Now, Nintendo was not providing a standard toolchain yet, but they were on the right track (finally reached with the Nintendo Switch).</p><p>Curiously enough (and this is an interesting contrast), back in 2011, Apple offered Clang/LLVM 1.3 and OpenGL ES 3.0 for developing iOS apps, this was considered state-of-the-art for mobile projects. Well, you’ll see throughout this section that this wasn’t the case for Nintendo. Yet, at present, if you grab an old iPhone 4s and try to install any app on the App Store (its only official medium), it will tell you your system is too old. Whereas you can still play any retail game on your 3DS. Food for thought.</p><div><ul><li id="tab-8-1-hardware-kits-link"><a href="#tab-8-1-hardware-kits">Hardware Kits</a></li><li id="tab-8-2-software-kits-link"><a href="#tab-8-2-software-kits">Software Kits</a></li></ul><div><div id="tab-8-1-hardware-kits"><h4 id="tab-8-1-hardware-kits">Hardware Kits</h4><p>Nintendo partnered with two suppliers to produce development kits <sup id="bibref:81"><a href="#bib:games-hardware" role="doc-biblioref">[81]</a></sup>. The first supplier was the well-known <strong>Intelligent Systems</strong> and the other was <strong>Kyoto MicroComputers</strong>.</p><p>Among the many options, studios could rent a general-purpose ‘CTR-BOX’. This is a metallic box housing the 3DS hardware, and connected to it is a ‘dummy’ 3DS case that serves as a controller and display. With it, developers could deploy, test and debug their code.</p><p>For more single-purpose tools, studios could get official <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-9-1-the-hardware">flashcards</a> to distribute game prototypes to external testers. These flashcards still only run on non-retail equipment, though this included cheaper options (with reduced functionality) than the fully-fledged CTR-BOX.</p><p>With the arrival of the New 3DS, IS and Partner offered the ‘SNAKE’ kits with updated hardware.</p></div><div id="tab-8-2-software-kits"><h4 id="tab-8-2-software-kits">Software Kits</h4><p>As always, licensed studios would also get an SDK package from Nintendo which included <sup id="bibref:82"><a href="#bib:games-software" role="doc-biblioref">[82]</a></sup>:</p><ul><li>A variant of <code>armcc</code> (ARM’s <strong>C</strong> and <strong>C++ compiler</strong>) modelled for the Nintendo 3DS and the ARM11 MPCore.<ul><li>This will just produce code for the ARM11 MPCore. Both ARM9 and ARM7 are out of the equation, as 3DS games only run within the ARM11 cluster.</li></ul></li><li><strong>Debuggers</strong> made for IS and Partner’s development kits.</li><li><strong>APIs</strong> to communicate with the hardware and operating system’s services.</li><li>Four <strong>graphics libraries</strong>:<ul><li><strong>GL</strong>: a simpler but slower OpenGL ES API.</li><li><strong>GD</strong>: a faster alternative to GL that generates PICA200 commands.</li><li><strong>GR</strong>: the closest-to-metal PICA200 command API, albeit with the steepest learning curve.</li><li><strong>GX</strong>: the general-purpose library used for PICA200 management.</li></ul></li><li>A 3DS <strong>app packager</strong>, so an executable can be created.</li><li>Further libraries to ease common development tasks, such as implementing network protocols, online gaming and audio/video decoding and processing.</li><li>A plugin for <strong>Visual Studio 2010</strong>, so it can be adopted as the main IDE.</li><li><strong>Assistant tools</strong> for the PICA200.</li><li><strong>Profilers</strong>, for measuring and optimising performance.</li></ul><p>If that wasn’t enough, developers also had access to NintendoWare to download code samples, libraries and further tools designed for Nintendo 3DS development. Furthermore, with the arrival of the New 3DS, game engines like Unity lent their support to this (once ignored) platform <sup id="bibref:83"><a href="#bib:games-unity" role="doc-biblioref">[83]</a></sup>.</p></div></div></div><h3 id="medium">Medium</h3><p>The Nintendo 3DS can run software from three different mediums.</p><div><ul><li id="tab-9-1-gamecards-link"><a href="#tab-9-1-gamecards">Gamecards</a></li><li id="tab-9-2-eshopsd-card-link"><a href="#tab-9-2-eshopsd-card">eShop/SD Card</a></li><li id="tab-9-3-local-wireless-link"><a href="#tab-9-3-local-wireless">Local wireless</a></li></ul><div><div id="tab-9-1-gamecards"><h4 id="tab-9-1-gamecards">Gamecards</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/gamecard.7848bd8ebfd8dcfe983d8f2883ce66afafff32cbd45fd9742c0a8161b400692c.webp"><picture><img alt="Image" width="815" height="571" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu73e8e42e37f2e81f9775ed553ee0b728_42910_ac83099bc32b8ab4125c6c805b799620.png" loading="lazy"></picture></a><figcaption>Example of a retail game. Notice the creative touch with Luigi holding onto the 3DS banner.</figcaption></figure><p>This is the distribution channel for retail software. Internally called ‘CTR cards’, they’re just another proprietary card/cartridge designed by Nintendo. To be fair, they’re not very different from <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#games">NTR Cards</a> (used by the Nintendo DS), aside from a cosmetic notch at the top right. Inside them, there is read-only and/or writable storage.</p><p>The variants of CTR cards range drastically. Their PCB can bundle a ROM chip ranging from <strong>128 MB</strong> to <strong>4 GB</strong> in size, while also including some ‘backup memory’ to store saves, this can be either <strong>128 KB or 512 KB</strong>. In other variants, the whole CTR storage is instead filled with <strong>Flash</strong> (up to 2 GB), and it’s partitioned to store both the game and saves in the same physical chip.</p><p>Moreover, since games will be bundling the official SDK as well, the usable capacity of ROM/RAM allowed to the game depends on the revision of the SDK linked to.</p><p>Internally, the ROM chip is connected to an 8-bit data bus <sup id="bibref:84"><a href="#bib:games-gamecards" role="doc-biblioref">[84]</a></sup>, while the backup memory relies on a serial bus. Both are connected to a 16.6 MHz clock. When the card is inserted into the console, the 3DS first queries it using NTR (Nintendo DS) commands <sup id="bibref:85"><a href="#bib:games-card_registers" role="doc-biblioref">[85]</a></sup>, and then switches to ‘CTR mode’ once it detects it’s a 3DS card.</p></div><div id="tab-9-2-eshopsd-card"><h4 id="tab-9-2-eshopsd-card">eShop/SD Card</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png" loading="lazy"></picture></a><figcaption>The Nintendo eShop store for the Nintendo 3DS.</figcaption></figure><p>Expandable storage, an icon of the 8th generation of consoles. The Nintendo 3DS now enjoys installing and launching software from the SD (or microSD) card; and with it, retail cards are no longer the only medium for games. In fact, SD storage is the result of an emerging distribution channel: The Nintendo online store, bundled into every Nintendo 3DS.</p><p>Thanks to the eShop, there were new distribution techniques: users could <strong>pre-order</strong> games and DLCs before the shipping date <sup id="bibref:86"><a href="#bib:games-preorder" role="doc-biblioref">[86]</a></sup>. These would get downloaded ahead of time, but can only be played once the release date arrived.</p><p>When it comes to storing the respective saves, Nintendo allows its downloaded software to request up to <strong>1 MB</strong> of SD card storage. However, this rule is waived if the retail counterpart already requires more space, in which case the system will allocate as much ‘backup memory’ as the respective CTR card already provides.</p></div><div id="tab-9-3-local-wireless"><h4 id="tab-9-3-local-wireless">Local wireless</h4><p><strong>Download Play</strong>, a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#medium">debuting feature</a> of the Nintendo DS that enabled players to transfer small games between their consoles, has been pushed into the Nintendo 3DS. It now comes with a revamped protocol that relies on a thick security layer, in line with the rest of the software executed in the new console.</p><p>I’d say the big difference now is that transferred games (or ‘demos’, for it’s worth), are now installed into NAND (as any other installable package) <sup id="bibref:87"><a href="#bib:games-download_play" role="doc-biblioref">[87]</a></sup>. There’s only one slot reserved for them, so the installed program is replaced whenever a new game is transferred in.</p></div></div></div><h3 id="virtual-consoles">Virtual console(s)</h3><p>If, after all explained, users still got bored, Nintendo prepared another offering for them: <strong>Virtual Console</strong>.</p><p>Once again, thanks to the bundled eShop app, Nintendo also expanded its 3DS-only catalogue by incorporating games originally published for the following consoles:</p><ul><li>Nintendo DSi (DSiWare only).</li><li>NES/Famicom.</li><li><em>Sega</em> Game Gear.</li><li>Game Boy.</li><li>Game Boy Color.</li></ul><p>Virtual Console games behave as any other application installed into the console. Except for DSiWare software, the application package includes a ROM and emulator. The latter implements interesting capabilities, such as Download Play (for some games) and save states.</p><p>Once again, things were different for New 3DS users, as they could also access the <a href="https://www.copetti.org/writings/consoles/super-nintendo/">Super Nintendo</a> catalogue. This particularly strikes me as odd, as I remember a time when (homebrew) SNES emulators were developed for the original Nintendo DS (with its mere ARM9-ARM7 and a couple of megs of RAM).</p><p>Now, here’s another peculiarity of Virtual console games: this console can also play <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> games, officially. Yet, they’re not available for everyone. Only those who purchased a Nintendo 3DS before August 2011 (right before the console received an $80 price cut), became members of the ‘Ambassador Program’ <sup id="bibref:88"><a href="#bib:games-ambassador" role="doc-biblioref">[88]</a></sup>. One of the perks included access to a selection of GBA games which, for one reason or another, were kept exclusive until date.</p><p>Even more puzzling, GBA titles don’t run on the ARM11 using an emulator (albeit there’s one installed, but never been used!). Instead, they kickstart the third firmware, AGB_FIRM, to run natively on top of the ARM7. What makes it puzzling, is that these GBA games, only offered through the Ambassador Program, remained the only purpose of AGB_FIRM, as if Nintendo planned for something bigger in the future, but never materialised. This is another example of how the Nintendo 3DS possessed more hardware than the software ever took advantage of.</p><p>If you’re curious, GBA titles make use of the bundled ARM7 core instead. Thus, they don’t allow for the extra features that emulators (running on ARM11 cores) provide. Although, this happens at the exchange of running at full speed and precision. Be as it may, since the 3DS doesn’t contain a GBA cartridge slot, the GBA game is instead copied into FCRAM before the system reboots into AGB_FIRM, and then lets the ARM7 take control (while the ARM11 and ARM9 provide basic support tasks) <sup id="bibref:89"><a href="#bib:io-misc" role="doc-biblioref">[89]</a></sup>.</p><p>With all these capabilities, and on top of being a portable console, one can’t help but wonder why Nintendo didn’t distribute GBA and Nintendo DS/DSi games on the eShop as well. Most probably a marketing and licensing issue, I sense.</p><h3 id="game-updates">Game updates</h3><p>Another requirement of the 8th generation of consoles, games can now receive patches after the shipping date. That’s right, no more need to quality control the game before selling it!</p><p>Leaving irony aside, game updates are distributed through the eShop as well <sup id="bibref:90"><a href="#bib:games-updates" role="doc-biblioref">[90]</a></sup>, which applies to all types of games (except Download Play). All updates are downloaded onto the SD card and eShop games get their updates applied altogether (along with the game itself).</p><h3 id="network-service">Network service</h3><p>Out with the old (<a href="https://www.copetti.org/writings/consoles/nintendo-ds/#network-service">Nintendo Wi-Fi Connection</a>), in with the new (<a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Nintendo Network</a>)! The same service offered with the Wii U is also implemented on the Nintendo 3DS, and they’re admirably unified, so I recommend checking the <a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Wii U article</a> where it’s been explained in detail.</p><hr><h2 id="anti-piracy-and-homebrew">Anti-Piracy and Homebrew</h2><p>The history of hacking this console is a long and interesting sequence of events. At first, interests focused on cracking game card readers (in an attempt to replicate the success of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#defeat">Nintendo DS</a>) and then shifted towards more sophisticated approaches, only involving the operating system.</p><h3 id="main-targets">Main targets</h3><p>First things first, let’s start by describing the two main targets of this system:</p><ul><li>The <strong>Game/CTR card reader</strong>: This is where physical games are loaded. Historically, the Nintendo DS implemented a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#security-mechanisms">weak security mechanism</a> that was eventually cracked and subsequently led to an influx of <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#flashcarts">Flashcards</a>.</li><li>The <strong>Operating System</strong>: This area is responsible for verifying the authenticity and integrity of every single program before execution (aside from the Boot ROM). Disabling said mechanisms would grant the execution of Homebrew (unauthorised applications) without any restrictions, in theory.</li></ul><p>They may look like two independent fronts (similar to the <a href="https://www.copetti.org/writings/consoles/xbox-360/#main-targets">Xbox 360</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#main-targets">Wii U</a>), but in the case of this console, both are intertwined. You’ll see it in a bit.</p><h3 id="the-card-reader-front">The card reader front</h3><p>The card reader is the interface between the CPUs and the Gamecards’ memory chip. Its only job is to simplify the communication with the use of commands.</p><p>Inside the ROM/Flash of CTR Carts, the system will find a block of data in a secured format called <strong>NCSD</strong>, this will be handled by the operating system, who will be in charge of authenticating, validating and decrypting it.</p><p>In conclusion, it seems the OS is solely responsible for the communication with the card reader, so let’s move on to the next front.</p><h3 id="the-operating-system-front">The Operating System front</h3><p>Before we continue, if you’re not familiar with symmetric and asymmetric encryption systems, I recommend reading previous articles of this series. They will also explain why systems like this rely so much on asymmetric encryption systems (such as RSA and ECDSA).</p><div><ul><li id="tab-10-1-dedicated-hardware-link"><a href="#tab-10-1-dedicated-hardware">Dedicated hardware</a></li><li id="tab-10-2-chain-of-trust-link"><a href="#tab-10-2-chain-of-trust">Chain of trust</a></li><li id="tab-10-3-operating-system-functions-link"><a href="#tab-10-3-operating-system-functions">Operating system functions</a></li></ul><div><div id="tab-10-1-dedicated-hardware"><h4 id="tab-10-1-dedicated-hardware">Dedicated hardware</h4><p>You’d be right if you suspected that the ARM11 lacks the powerhouse to protect the whole system. Nintendo knew that too, so they took extra care and bundled extra components to compensate:</p><ul><li>The ARM11 cores implement the <strong>XN flag</strong> and the ARM9 bundles a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU), meaning the CPUs won’t execute code from any location in memory just because the current program tells it to.</li><li>As said before, the ARM9 acts as a <strong>dedicated processor</strong> to handle all security-related tasks while the ARM11 MPCore executes the game. Additionally, the ARM9 is exclusively wired to a few hidden <strong>cryptographic accelerators</strong>:<ul><li>An <strong>AES engine</strong> that performs AES-128 encryption/decryption without consuming (and exposing) CPU resources. This was inherited from the Nintendo DSi, but can now store up to 64 keys and can operate in numerous block cipher modes, including CTR, CCM, CBC and ECB <sup id="bibref:91"><a href="#bib:anti_piracy-aes" role="doc-biblioref">[91]</a></sup>. Each key slot also features its own <strong>key-scrambler</strong>, meaning that two arbitrary keys can be used to generate the final AES key. Moreover, the key-scrambler won’t allow anyone to read the generated key, only to treat it as a blackbox to encrypt/decrypt data.</li><li>An <strong>RSA engine</strong>. By contrast, this performs RSA encryption/decryption using a given RSA public key. This time, it only contains four key slots and there’s no key-scrambler <sup id="bibref:92"><a href="#bib:anti_piracy-crypto" role="doc-biblioref">[92]</a></sup>. However, it’s still a write-only space, meaning no one will be able to read the keys stored there. You’ll soon see that this system is filled with RSA-2048 and RSA-4096 signatures, which explains why this component is as crucial as the AES engine.</li><li>A <strong>Pseudo Random Number Generator</strong> (PRNG): These are registers that return a different value every time they’re read from.</li></ul></li><li><strong>OTP</strong> (one-time programmable) memory that stores console-unique keys, information about the console and <strong>CTCert</strong> (an ECDSA private key to authenticate with Nintendo’s servers). To complicate things further, these keys will be encrypted with an AES-CBC key found in Boot9. Finally, this region includes a flag to disable its access once it’s not needed anymore.</li><li>Last but not least, the eMMC memory contains a register called <strong>CID</strong> (Card Identification) and stores unique information about the eMMC’s manufacturing, which will be fed to the AES’ key-scrambler for further obfuscation.</li></ul><p>To top it off, everything is sealed in an SoC, including the <strong>two boot ROMs</strong> (Boot9 and Boot11). These are unencrypted, but since they’re inaccessible, they don’t represent a concern.</p></div><div id="tab-10-2-chain-of-trust"><h4 id="tab-10-2-chain-of-trust">Chain of trust</h4><p>This should come as no surprise considering we’ve already introduced RSA, AES and the boot ROM as part of the security system. To give you an overview of the Nintendo 3DS’ change of trust:</p><ol><li>ARM9’s boot ROM (Boot9) bundles the public key for decrypting and validating the contents of the NAND. The AES engine will be initialised with the keys stored in Boot9. With this, the contents of OTP memory will be accessed.</li><li>The contents of eMMC are decrypted using Boot9’s AES keys combined with the eMMC CID.</li><li>NAND and CTR cards are formatted using the <strong>NCSD</strong> format <sup id="bibref:93"><a href="#bib:anti_piracy-ncsd" role="doc-biblioref">[93]</a></sup>. NCSD stores a header and a collection of up to eight partitions. The NCSD header contains a signature using RSA-2048 and SHA-258, which is quite strong. To decrypt this signature, the system finds its public RSA key in the boot ROM or ITCM memory (the latter was previously decrypted and copied from OTP). The choice depends on where the NCSD block came from (NAND or CTR card).</li><li>Once the NCSD block is validated, the system accesses each partition. These are structured using the <strong>NCCH</strong> (Nintendo Content Container Header) format. Independently whether the data was pulled from NAND, the CTR card or the SD card, the NCCH block also contains an RSA-2048 + SHA-258 signature <sup id="bibref:94"><a href="#bib:anti_piracy-ncch" role="doc-biblioref">[94]</a></sup>, and its payload is encrypted with AES-128 CTR.</li><li>Furthermore, installed software is catalogued in the form of <strong>Titles</strong> (similar to the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System</a>). In this case, all titles are signed with either RSA-2048, RSA-4096 or ECDSA; plus SHA256 <sup id="bibref:95"><a href="#bib:anti_piracy-titles" role="doc-biblioref">[95]</a></sup>. The public keys are stored in <code>NATIVE_FIRM</code>.<ul><li>It does surprise me that some signatures are in the form of ECDSA, considering there’s no hardware accelerator installed for it.</li></ul></li><li>Once the payload is verified and decrypted, the system will find either an executable, library or asset (i.e.&nbsp;manual, icon or banner) that the ARM11 can read.</li></ol><p>Please note, this explanation focuses on the main 3DS firmware (<code>NATIVE_FIRM</code>). Yet, <code>TWL_FIRM</code> and <code>AGB_FIRM</code> will also have their share of cryptography implemented.</p><p>As time passed by and hackers got the handle on how this console was protected, Nintendo shuffled the chain of trust further to deter the decryption of NCCH data. In some ways, it achieved its purpose, but in others, Nintendo ended up revealing too much. You’ll see it in the following sections.</p></div><div id="tab-10-3-operating-system-functions"><h4 id="tab-10-3-operating-system-functions">Operating system functions</h4><p>Once <code>NATIVE_FIRM</code> is up and running, in addition to the aforementioned chain of trust, the following security mechanisms are present:</p><ul><li>User programs only access hardware functions through system calls, authorised at Kernel11’s discretion. Depending on the hardware, it will also involve Kernel9.</li><li>From an architecture perspective, ARM11 user programs are completely unaware of the ARM9 and its neighbouring components.</li><li>User applications are <strong>sandboxed</strong>, meaning they can’t access each other’s space.</li><li>Last but not least, with the increase in online services, users will require a legitimate game card and an updated firmware to access the new functions. This will deter users who may consider keeping their console on a vulnerable firmware.</li><li>Software downloaded from the eShop also comes with its quirks. In this scenario, the license of a Title is encoded in the form of a <strong>Ticket</strong> which, again, is signed with RSA-2048 and SHA-256 <sup id="bibref:96"><a href="#bib:anti_piracy-cdn" role="doc-biblioref">[96]</a></sup>. Tickets are either linked to a single console ID and the eShop’s user account; or made global for any console. Furthermore, Nintendo uses additional RSA certificates in the downloaded Title’s metadata to further enlarge the chain of trust <sup id="bibref:97"><a href="#bib:anti_piracy-tmd" role="doc-biblioref">[97]</a></sup>.</li></ul></div></div></div><h4 id="flaws">Flaws</h4><p>Even though the Nintendo 3DS enjoyed modern protection techniques, such as asymmetric cryptography and lots of hardware at its disposal, there were some fundamental flaws in its implementation. Take a look at the following findings discovered by the hacking community:</p><ul><li>While the XN flag in the ARM11 works without problems, Kernel11 sets up the page table in AXI WRAM (where Kernel11 resides) in a way that it grants Read, Write and Execute permissions to the whole memory block <sup id="bibref:98"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[98]</a></sup>, rendering the capabilities of XN a bit useless (at least for protecting Kernel11).</li><li>Before system version <code>3.0.0</code>, OTP memory was never hidden <sup id="bibref:99"><a href="#bib:operating_system-otp" role="doc-biblioref">[99]</a></sup>, meaning that with the help of any exploit, the OTP keys could be extracted without problem.</li><li>There’s no separation between Process9 and Kernel9, as Kernel9 provides a system call that allows Process9 to perform any function with Kernel9 privileges <sup id="bibref:100"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[100]</a></sup>.</li><li>There’s no <strong>ASLR</strong> (Address space layout randomization) implemented <sup id="bibref:101"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[101]</a></sup>, enabling Return-oriented programming (ROP) for exploitation purposes.</li><li>Similarly, there’s no protection against system <strong>downgrading</strong>.</li><li>Once again, this system also comes with a <strong>Web browser based on Webkit</strong>, which is under constant attack (especially if the fork is old).</li></ul><p>This will not only pave the way to the first exploitation attempts, but will also act as a constraint for Nintendo when they try to patch their system.</p><h3 id="defeat">Defeat</h3><p>The history of the Nintendo 3DS and Homebrew is a successful one. Tons of video tutorials can attest to that. Yet, the passage exposes very clever discoveries, which evolved from initially requiring proprietary and expensive equipment to just a couple of clicks on your computer.</p><h4 id="the-ds-flashcard-era-2011-2013">The DS flashcard era (2011-2013)</h4><p>Where to begin? Well, from where the Nintendo DSi left it off: <strong>Flashcards</strong>.</p><p>After the release of the Nintendo DSi in 2008, Nintendo incorporated a new element to fight against Flashcards: A <strong>whitelist file</strong> listing every single licensed card and thereby blocking the ‘unauthorised ones’ <sup id="bibref:102"><a href="#bib:anti_piracy-card_whitelist" role="doc-biblioref">[102]</a></sup>. By no means Flashcard manufacturers ceased their production, they just shipped new variants of their old Flashcards that allowed the user to re-program the cartridge header, enabling the card to identify as a different authorised game whilst Nintendo kept amending the list (through software updates).</p><p>This method encompassed the Nintendo 3DS as well, following the same process as the Nintendo DSi. On no account they would get access to the exclusive 3DS hardware, yet this is how Homebrew started in this console.</p><h4 id="the-3ds-flashcard-era-2013-2016">The 3DS flashcard era (2013-2016)</h4><p>There was much progress during the first two years of this console (a big achievement for Nintendo!). Yet, things took a turn in August 2013…</p><div><ul><li id="tab-11-1-the-first-real-3ds-flashcard-link"><a href="#tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</a></li><li id="tab-11-2-inside-the-gateway3ds-link"><a href="#tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</a></li><li id="tab-11-3-subsequent-anecdotes-link"><a href="#tab-11-3-subsequent-anecdotes">Subsequent anecdotes</a></li></ul><div><div id="tab-11-1-the-first-real-3ds-flashcard"><h5 id="tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg"><picture><img alt="Image" width="650" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg" loading="lazy"></picture></a><figcaption>The Gateway3DS package <sup id="bibref:103"><a href="#bib:anti_piracy-gateway_review" role="doc-biblioref">[103]</a></sup>.</figcaption></figure><p>Ignoring teasers of ‘3DS Flascards’ that never appeared <sup id="bibref:104"><a href="#bib:anti_piracy-crown3ds" role="doc-biblioref">[104]</a></sup>. <strong>Gateway3DS</strong> can be considered the first 3DS Flashcard to reach the stores. The instructions were not as simple as DS Flashcards, however. You can sense this by looking at the contents of the box:</p><ul><li>A whitelisted <strong>DS Flashcard</strong> (known as <em>Blue Gateway</em>) whose only purpose is to run a Nintendo DS ROM crafted by Gateway. As part of the ‘installation’ process, users were first required to run this ‘game’ and follow the instructions.</li><li>A <strong>Launcher.dat</strong> to be placed in the 3DS’ SD card.</li><li>A <strong>3DS Flashcard</strong> (known as <em>Red Gateway</em>) where the 3DS game is loaded from. Like any other Flashcard, it also features a microSD slot where the 3DS game is stored. The big difference, however, is that the 3DS game image is flashed into the microSD card, meaning that only one 3DS game can be stored at a time.<ul><li>This makes sense, as RSA signatures can’t be faked (at least, that’s computationally unfeasible). Yet, replicating an exact clone of the game (NCSD block) worked. Forget about Homebrew, for now.</li></ul></li></ul><p>After completing the installation process, users would have to follow these instructions to run any game:</p><ol><li>Insert the <em>Red</em> Gateway card. Nothing will appear, yet.</li><li>Open the 3DS settings app and navigate to the DS profile editor screen.</li><li>For some reason, the 3DS will restart and the flashed 3DS game will show up.</li><li>After finishing playing a game, returning to the HOME Menu will create a savefile in the 3DS’ SD card.</li></ol><p>And just like that, users were now able to download 3DS ROMs from the net and run them on their consoles… but how was all of this possible? How did anyone manage to extract decrypted games? What exploits did Gateway3DS employ (or even discover)?</p><p>Truth is, there’s a lot of hidden functionality within this product. Let’s analyse it step by step.</p></div><div id="tab-11-2-inside-the-gateway3ds"><h5 id="tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png" loading="lazy"></picture></a><figcaption>The DS Message editor found on the 3DS settings app. The character limit rule depends solely on the graphical interface.</figcaption></figure><p>Sometime in 2012, hacker ‘ichfly’ discovered interesting behaviour in the Nintendo DS’ old profile editor, found on both <code>NATIVE_FIRM</code> and <code>TWL_FIRM</code>. In one of its text fields, you can enter a ‘Message’ value, which will then be displayed as a greeting on PictoChat rooms <sup id="bibref:105"><a href="#bib:anti_piracy-profile" role="doc-biblioref">[105]</a></sup>. The 3DS’ settings app won’t allow you to enter more characters than allowed. Yet, nothing prevents a Nintendo DS game from doing so. When that happens, opening the 3DS’ System Settings app (called <strong>MSET</strong>) will crash, and what makes it interesting is that this is caused by <strong>stack overflow</strong> <sup id="bibref:106"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[106]</a></sup>. Does this remind you of <a href="https://www.copetti.org/writings/consoles/wii/#the-dawn-of-homebrew">a certain horse name</a>?</p><p>Now, the mysterious Launcher.dat by Gateway is a configuration file that the Settings app normally reads. What happened is that Gateway crafted their own Launcher.dat to embed data used for the next stages of their exploit. Curiously enough, Launcher.dat is stored in NAND (not in the SD), so the initial exploit chain also alters where the Settings app loads this from.</p><p>If you combine this with a Process9/Kernel9 exploit, you get full execution privileges on this console and can start fiddling with system services. Some hardware like OTP and the boot ROMs will still be out of reach. Yet, this is a significant milestone.</p><p>So far so good? Let’s now connect this information with Gateway’s package:</p><ul><li>The DS/Blue flashcard is just an entry point to install the corrupted DS profile (which will trigger the MSET exploit).</li><li>The 3DS/Red flashcard houses a ProASIC3 FPGA programmed with a firmware (distributed by Gateway). The FPGA and the microSD card are combined to replicate a retail game.</li><li>Launcher.dat is the payload of the MSET exploit. It bundles a Kernel exploit and a collection of system patches. In other words, a <strong>Custom Firmware</strong> (CFW). A console running Gateway’s CFW can extract games or load a 3DS game using the red flashcard. Surprisingly, Gateway also crafted their CFW so it requires the red flashcard inserted to work (<em>a DRM mechanism in a Flashcard, have the tables turned?</em>).</li></ul></div><div id="tab-11-3-subsequent-anecdotes"><h5 id="tab-11-3-subsequent-anecdotes">Subsequent anecdotes</h5><p>All seemed jolly for Gateway until November 2013, when a stream of clones of their card landed. ‘R4i Gold 3DS Deluxe’ came for some healthy competition, albeit by using some of Gateway’s firmware code. In retaliation, Gateway3DS took drastic measures: Subsequent firmware updates of Gateway3DS corrupted the 3DS NAND if a clone was detected. <em>The irony!</em></p><p>In October 2013, hacker ‘Smealum’ published a video showing his own MSET-based implementation that instead booted a copy of <code>NATIVE_FIRM</code> stored in the 3DS’ SD <sup id="bibref:107"><a href="#bib:anti_piracy-rednand" role="doc-biblioref">[107]</a></sup>. This meant that consoles stuck on system <code>4.5.0</code> could boot newer system versions without losing the ability to run exploits. Smealum called this function <strong>redNAND</strong> (from ‘redirected NAND’) and, while it wasn’t publicly released, Gateway later incorporated this functionality (now referred to as <strong>emuNAND</strong>) with their CFW released in December 2013 <sup id="bibref:108"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[108]</a></sup>. This became a strong selling point for Gateway3DS.</p><p>It’s not known what Process9/Kernel9 exploit Gateway employed. Yet, in December 2013, Fierce_Waffle, Xerpi and Megazig reversed engineered and open-sourced Gateway’s payload in the form of a tool called ‘3DS Toolkit’ <sup id="bibref:109"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[109]</a></sup> <sup id="bibref:110"><a href="#bib:anti_piracy-ramdump" role="doc-biblioref">[110]</a></sup>.</p><p>In the following years, a second generation of 3DS flashcards will appear in the market. Examples include <strong>Stargate</strong>, <strong>Sky3DS</strong> and dozens of clones. This time, they didn’t rely on an operating system exploit to work and could load multiple games from their microSD. However, their utility will be entirely based on replicating retail 3DS games (including their signatures), in other words, for solely piracy purposes.</p></div></div></div><h4 id="nintendo-acts-fast">Nintendo acts fast</h4><p>Having an updatable system software meant Nintendo didn’t have to stand there and watch how its system got cracked:</p><ul><li>In March 2013, system update <code>5.0.0-11</code> updated the settings app, provisionally fixing the MSET exploit <sup id="bibref:111"><a href="#bib:anti_piracy-neko" role="doc-biblioref">[111]</a></sup>. If you check the timeline, this was before Gateway3DS shipped their card! Hence, it was a prerequisite for users to stay on older versions.<ul><li>It won’t be until 2015 when the Gateway team released a notable firmware update. From then on, the flashcard relied on a new Web Browser exploit (called <strong>spider exploit</strong>, discovered by MathewE) as the entry point. This method lasted until the end of Gateway3DS’ lifespan.</li></ul></li><li>In December 2013, system update <code>7.0.0-13</code> fixed the kernel exploits used in combination with MSET and, most importantly, added the RSA module into the chain of trust to decrypt NCCH blocks (where the game data is found) <sup id="bibref:112"><a href="#bib:anti_piracy-70013" role="doc-biblioref">[112]</a></sup>. RSA keys are cleared once Kernel9 finishes loading, meaning existing exploits won’t be able to decrypt games that adopted the new <code>7.0.0</code> encryption system (unless a vulnerability is used before Kernel9 boots).</li><li>As Gateway3DS’ Launcher.dat file contained copyrighted code by Nintendo, the latter company sent Cease &amp; Desist letters to many forums, including GBATemp, which in turn blocked the distribution of those files.</li></ul><p>As always, this marked the start of another cat-and-mouse game. Though, to make a long story short, system update <code>9.3.0</code> (released in December 2014) finally put an end to Gateway3DS by patching their private Kernel exploit <sup id="bibref:113"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[113]</a></sup>. Since then, Gateway3DS’ firmware updates only improved emuNAND support with the latest system versions (for those who didn’t update past the breaking update). In 2016, Gateway’s last update was released. Meanwhile, Sky3DS enjoyed support until system software <code>11.0</code> (released in May 2016) <sup id="bibref:114"><a href="#bib:anti_piracy-sky3ds" role="doc-biblioref">[114]</a></sup>, when Nintendo blacklisted it for good.</p><p>I think now it’s fair to say that the 3DS flashcard market ended up being too turbulent and unreliable for the average user, compare this to the ‘plug &amp; play’ experience Nintendo DS flashcard offered. Finally some good news for Nintendo, so far.</p><h4 id="the-dawn-of-homebrew-2014">The dawn of homebrew (2014)</h4><p>2014 saw an emergence of homebrew-focused solutions in a circle populated by piracy-oriented developments <sup id="bibref:115"><a href="#bib:anti_piracy-proto_homebrew" role="doc-biblioref">[115]</a></sup>. Hacking a 3DS still required an old system version, a Gateway3DS card and emuNAND - but that would slowly shift once alternative tools gained traction.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg"><picture><img alt="Image" width="854" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg" loading="lazy"></picture></a><figcaption>Plutoo, Derrek and Smealum presenting their findings at the 32nd Chaos Communication Congress (2015) <sup id="bibref:116"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[116]</a></sup>, the following paragraphs will explain most of them.</figcaption></figure><div><ul><li id="tab-12-1-open-source-sdks-link"><a href="#tab-12-1-open-source-sdks">Open-source SDKs</a></li><li id="tab-12-2-ninjhax-chain-link"><a href="#tab-12-2-ninjhax-chain">Ninjhax chain</a></li><li id="tab-12-3-gaining-kernel11-access-link"><a href="#tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</a></li></ul><div><div id="tab-12-1-open-source-sdks"><h5 id="tab-12-1-open-source-sdks">Open-source SDKs</h5><p>Initial Homebrew appeared in the form of Laucher.dat files, these were produced with the help of devkitARM (a general-purpose toolchain for ARM-based CPUs) and a set of scripts. Fierce Waffle provided ‘ROP Loader’, a toolkit that included a DS program to install the MSET exploit; and a Launcher.dat that triggered a Kernel11 exploit. It’s worth pointing out that there wasn’t any tool available, yet, that helped access the 3DS’ exclusive hardware.</p><p>At the start of 2014, Smealum, with the collaboration of yellows8, ichfly, WinterMute, fincs, mtheall and plutoo, released <strong>ctrulib</strong>, an open-source C library to facilitate Homebrew development <sup id="bibref:117"><a href="#bib:anti_piracy-libctru" role="doc-biblioref">[117]</a></sup>. This is now known as <strong>libctru</strong> and maintained by the devkitPro group, who have incorporated it into their toolchain.</p><p>A year later, neobrain released <strong>nihstro</strong> <sup id="bibref:118"><a href="#bib:anti_piracy-nihstro" role="doc-biblioref">[118]</a></sup>, a PICA200 shader assembler a disassembler, making the job of programming the PICA200 a bit more enjoyable.</p><p>To run Homebrew, users had the option to flash a homebrew binary into a microSD, and then use the Gateway3DS to boot it (as their CFW already disabled signature checks) <sup id="bibref:119"><a href="#bib:anti_piracy-gateway_homebrew" role="doc-biblioref">[119]</a></sup>.</p></div><div id="tab-12-2-ninjhax-chain"><h5 id="tab-12-2-ninjhax-chain">Ninjhax chain</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png" loading="lazy"></picture></a><figcaption>The Homebrew Launcher, inspired by the <a href="https://www.copetti.org/writings/consoles/wii/#a-permanent-state">iconic Wii counterpart</a>. Its arrival marked the sophistication of 3DS Homebrew.</figcaption></figure><p>The poisoned updates of Gateway left a bitter mark on their users. The time had come to look for nonproprietary alternatives.</p><p>Thankfully, people were working on this. During the second half of 2014, a new milestone awaited for the Homebrew community: Smealum published <strong>Ninjhax</strong>, a package composed of the following components <sup id="bibref:120"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[120]</a></sup>:</p><ol><li>A crafted <strong>QR code</strong> to be scanned by ‘Cubic Ninja’, a game that allows to share user-designed levels using QR codes. This served as a new entry point exploit.</li><li><strong>GSPWN</strong>: A userland vulnerability where the GPU’s DMA is used to write over the HOME Menu’s heap. Furthermore, the combination with ROP leads to privilege escalation. This resulted in the ability to create &amp; kill processes, SD card access, decrypt &amp; dump titles and override executable data.<ul><li><a href="https://www.copetti.org/writings/consoles/xbox-360/#graphics">Other GPUs</a> were also known for <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-20-3-king-kong-exploit">intruding</a> into the system’s RAM.</li></ul></li><li><strong>Homebrew launcher</strong>: A new service running under the HOME Menu process thanks to GSPWN. It provides a graphical user interface to load unsigned Homebrew apps (using a new portable .3dsx format) and take over processes. The launcher loads homebrew by opening an official application with enough privileges and then hijacks it with GSPWN, replaces the code with Homebrew code and finally executes it.<ul><li>With its ability to alter user data, the Homebrew launcher can also be used to install alternative entry points as they’re discovered (i.e.&nbsp;OotHax, Ironhax and so forth). Thus, reducing its dependency on Cubic Ninja. A notable aftermarket exploit was <strong>MenuHax</strong>, which exploited a vulnerability in the HOME Menu theme engine and was triggered at boot, <strong>making it a permanent solution to launch a payload</strong>.</li><li>If you are curious, the Wii U also experienced <a href="https://www.copetti.org/writings/consoles/wiiu/#fooling-iosu">similar methodologies</a> as early attempts to run Homebrew.</li></ul></li></ol><p>Notice how Gateway3DS is, for once, out of the equation. Be as it may, the Homebrew Launcher was still under the scope of userland (meaning homebrew apps could only access 64 MB of RAM and had no access to the audio DSP <sup id="bibref:121"><a href="#bib:anti_piracy-hbl_limitations" role="doc-biblioref">[121]</a></sup>).</p></div><div id="tab-12-3-gaining-kernel11-access"><h5 id="tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</h5><p>Turns out that before the publication of Ninjhax, in February 2014, yellows8 made a very important discovery: An exploit leading to Kernel11 privileges.</p><p>Kernel11 keeps track of the unused memory pages in FCRAM using a structure called <code>memchunk header</code>. This data is stored as a linked list, where each header contains the address of the previous and next header. Well, it so happens <code>memchunk headers</code> <strong>are stored in FCRAM</strong>, which may be overwritten thanks to other exploits like GSPWN. Consequently, memchunk headers can be modified to grant userland access to AXI WRAM. In doing so, the attacker can eventually modify the Kernel11’s page table to grant all FCRAM access to user-space, leading to arbitrary control of Kernel11. This discovery was called <strong>memchunkhax</strong>.</p><p>Nevertheless, Nintendo patched it in December 2014 <sup id="bibref:122"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[122]</a></sup>. However, another hacker by the name of derrek found a race condition where the ‘next’ pointer of a <code>memchunk header</code> may be replaced with the location of a crafted one. So, when Kernel11 tries to access the crafted <code>memchunk header</code>, it will end up executing arbitrary code with Kernel11 privileges. Ipso facto, <strong>memchunkhax2</strong> came into existence.</p><p>Thanks to the new privilege escalation, Homebrew software gained complete control of the system up to the ARM9 area… but why stop there?</p></div></div></div><h4 id="most-wanted-tools">Most-wanted tools</h4><p>Considering the availability of Gateway3DS’ emuNAND, Ninjhax, CTRLib and the new Kernel exploits, the flood of new software was too great to ignore. To mention a few:</p><ul><li><strong>CtrBootManager</strong> by cpasjuste: An extra stage in HomeMenuHax’s chain that acts as a boot manager, enabling the selection of various payloads <sup id="bibref:123"><a href="#bib:anti_piracy-ctrbootmanager" role="doc-biblioref">[123]</a></sup>.<ul><li>Shortly after, a new implementation with extended functionality emerged: <strong>BootCtr</strong> by m45t3r <sup id="bibref:124"><a href="#bib:anti_piracy-bootctr" role="doc-biblioref">[124]</a></sup>.</li></ul></li><li><strong>RxTools</strong> by Roxas75: A Swiss knife for Gateway3DS users <sup id="bibref:125"><a href="#bib:anti_piracy-rxtools" role="doc-biblioref">[125]</a></sup>. This was offered as a replacement for Gateway3DS’ binaries. Among many things, it includes a CFW called <strong>RXMode</strong>. This alternative and open-source solution disables signature checks on 3DS binaries, provides emuNAND and removes <code>TWL_FIRM</code>’s whitelist checks, to mention a few.<ul><li>Other CFWs will soon make their appearance, like CakesFW, ReiNand and Pasta CFW <sup id="bibref:126"><a href="#bib:anti_piracy-cfw_old_list" role="doc-biblioref">[126]</a></sup>. These serve different purposes and include their own set of modifications.</li></ul></li><li><strong>Custom HomeMenu Manager</strong> (CHMM) by Rinnegatamante: Allows to install HOME Menu themes from the SD card <sup id="bibref:127"><a href="#bib:anti_piracy-chmm" role="doc-biblioref">[127]</a></sup>.</li><li><strong>AGB_FIRM Signature patcher</strong> by Riku. Loads arbitrary Game Boy Advance ROMs into AGB_FIRM <sup id="bibref:128"><a href="#bib:anti_piracy-agb_converter" role="doc-biblioref">[128]</a></sup>, finally expanding the abandoned catalogue of Nintendo Ambassador games.</li><li><strong>Ftpbrony</strong> by mtheall (later known as <strong>ftpd</strong>): A simple FTP server <sup id="bibref:129"><a href="#bib:anti_piracy-ftpbrony" role="doc-biblioref">[129]</a></sup>.</li><li><strong>DevMenu</strong>: Not exactly a homebrew app, but a <em>stolen</em> Nintendo-authored app from development units, enabling users to install app packages (in the form of ‘CIA’ files) into the system, just like the eShop did behind the scenes.<ul><li>Months later, <strong>BigBlueMenu</strong> was used instead, which also came from Nintendo’s development kit.</li><li>It wasn’t until a real open-source solution was brought forward some months after. <strong>FBI</strong> by Steveice10 became the standard dilemma-free tool for installing CIA files (notice the pun in the names) <sup id="bibref:130"><a href="#bib:anti_piracy-fbi" role="doc-biblioref">[130]</a></sup>.</li></ul></li></ul><h4 id="new-console-permanent-mods-2015">New console, permanent mods (2015)</h4><p>While homebrew developers were busy fiddling with their system, Nintendo released a <em>new</em> product to the surprise of everyone: The <strong>New 3DS</strong>.</p><p>Apart from the extra hardware (already mentioned throughout this article), a new stage was added to the boot process: <strong>arm9loader</strong>. With this, Nintendo enhanced their chain of trust by adding new keys, which must be decrypted with the help of a hash of OTP memory (therefore, using console-unique values) <sup id="bibref:131"><a href="#bib:anti_piracy-arm9loader" role="doc-biblioref">[131]</a></sup>. However, arm9loader and the new keys are still stored in NAND, meaning that the contents may be overwritten. This led to one of the most disrupting vulnerabilities of 2015, involving Plutoo, Yellows8 and Delebile.</p><h5 id="arm9loaderhax">arm9loaderhax</h5><p>The first implementation of arm9loader was flawed: the decryption key for the ARM9 system was never removed from the AES engine. So, with the help of additional exploitation, one could reconstruct part of the encryption keys <sup id="bibref:132"><a href="#bib:anti_piracy-arm9loaderhax" role="doc-biblioref">[132]</a></sup>. Consequently, Nintendo quickly tried again with <code>arm9loader v1.1</code> (found on system update <code>9.6.0</code>). As luck would have it, this led to a more powerful exploit: Plutoo discovered that the key used to decrypt the ARM9 system was never verified. Hence, arm9Loader will boot <code>NATIVE_FIRM</code> even if the decrypted data is wrong (a.k.a. garbage). Plus, if Firm0 (the first copy of <code>NATIVE_FIRM</code>) fails to boot, Boot9 will try to load Firm1 while the remains of Firm0 stay in the ARM9’s RAM.</p><p>All in all, if:</p><ul><li>NAND is modified (somehow) so the encrypted Firm0 contains extra crafted code at the end.</li><li>The ARM9 OS key is mangled in a way that the decrypted Firm1 will contain a jump instruction to Firm0’s crafted code.</li></ul><p>… you got yourself <strong>arm9loaderhax</strong>, a permanent exploit that provides <strong>arbitrary code execution</strong> with <strong>Kernel9 privileges</strong> at <strong>boot time</strong>!</p><p>Since Kernel9 access was now possible, albeit through difficult means, work was put into simplifying the process (i.e.&nbsp;developing an automated installer).</p><h5 id="the-effects-of-arm9loaderhax">The effects of arm9loaderhax</h5><p>New discoveries meant new developments. Over the following months, more advanced tools will become part of the ‘must have’ list of every homebrew user.</p><p>To start with, a new CFW to-rule-them-all shipped: <strong>Luma3DS</strong> <sup id="bibref:133"><a href="#bib:anti_piracy-luma3ds" role="doc-biblioref">[133]</a></sup>. Among many features, Luma3DS provides:</p><ul><li>The removal of signature and region checks.</li><li>A layered filesystem to redirect file operations to the SD card (enabling game modifications).</li><li>Rosalina Menu, an in-game menu overlay where many utilities can be accessed without closing any application.</li></ul><p>Initially, Luma3DS was bootstrapped with BootCtr, but that changed once arm9loaderhax became the de-facto hack for any 3DS. Thus, the arm9loaderhax + Luma3DS combination became part of any hacking tutorial.</p><p>Along it, other software appeared:</p><ul><li><strong>Godmode9</strong> by d0k3: A next-generation Swiss knife that takes advantage of the permissions granted by ARM9 exploits <sup id="bibref:134"><a href="#bib:anti_piracy-godmode" role="doc-biblioref">[134]</a></sup>, enabling the user to read and modify every corner of the console. It can be loaded by arm9loaderhax, Luma3DS or any other compatible hack. Now, the more powerful the exploit, the more functionality is provided. Examples of functionality include a file browser and NAND backup. Plus it’s further extended with scripts.</li><li><strong>Anemone3DS</strong> by astronautlevel: With a multitude of features, it soon became <em>the app</em> for managing HOME Menu themes <sup id="bibref:135"><a href="#bib:anti_piracy-anemone3ds" role="doc-biblioref">[135]</a></sup>.</li><li><strong>nds-bootstrap</strong> by Rocket Robz: As the name indicates, it loads Nintendo DS software (ROMs and homebrew) from the SD card <sup id="bibref:136"><a href="#bib:anti_piracy-nds_bootstrap" role="doc-biblioref">[136]</a></sup>. While it’s designed to support the three portable consoles (the Nintendo 3DS, DSi and DS, the latter requiring a flashcard), loading it from the 3DS will kickstart <code>TWL_FIRM</code>, meaning there’s no emulation at all. It’s most commonly used through ‘TWLMenu’ (now ‘TWiLight Menu++’), the front-end of nds-bootstrap.</li></ul><p>It’s worth mentioning that, at the time of this writing, these are the most popular utilities to install on a hacked 3DS.</p><h4 id="the-golden-age-2016-2017">The Golden Age (2016-2017)</h4><p>While a universal and powerful solution, installing arm9loaderhax was still considered a complicated and dangerous activity. Not only does this require dumping the console’s OTP memory beforehand (using other exploits), but neglecting any step could potentially turn a working Nintendo 3DS into a rock.</p><p>But fear not as new developments were in the works (a mighty effort considering Nintendo was still battling to protect their console).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg"><picture><img alt="Image" width="1280" height="720" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg" loading="lazy"></picture></a><figcaption>naehrwert, nedwill and derrek presenting a new set of findings at the 33nd Chaos Communication Congress (December 2016) <sup id="bibref:137"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[137]</a></sup>.</figcaption></figure><p>At the 33C3 conference, derrek unveiled two major discoveries <sup id="bibref:138"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[138]</a></sup>, which led to subsequent milestones.</p><div><ul><li id="tab-13-2-sighax-link"><a href="#tab-13-2-sighax">Sighax</a></li><li id="tab-13-3-boot9strap-link"><a href="#tab-13-3-boot9strap">Boot9strap</a></li><li id="tab-13-4-ntrboot-link"><a href="#tab-13-4-ntrboot">Ntrboot</a></li></ul><div><div id="tab-13-1-extracting-boot9"><p>For some reason, the contents of ARM9’s RAM are not cleared upon reset. Thus, Derrek discovered that, with the use of external hardware, he could override the exception vectors from ARM9’s RAM (previously copied from Boot9) with arbitrary code. Then, reset the system, glitch it at very precise timing (also using external hardware) to trigger an exception and hope for the ARM9 to have executed the new code. This will have included something like ‘Copy all contents of Boot9 to X location in RAM’.</p><p>Lo and behold, this did work. With this, Derrek and others managed to analyse the contents of the Boot9 ROM, allowing new vulnerabilities to be found.</p></div><div id="tab-13-2-sighax"><h5 id="tab-13-2-sighax">Sighax</h5><p>One vulnerability from Boot9 was <strong>Sighax</strong> <sup id="bibref:139"><a href="#bib:anti_piracy-sighax" role="doc-biblioref">[139]</a></sup>, a flaw in Boot9’s RSA-2048 signature verification. RSA signatures of type ‘PKCS #1 v1.5’ (adopted by this system) contain an area called <strong>padding</strong> to prevent being reversed. Additionally, they store an SHA-256 hash encoded with a model called ‘ASN.1’, this <strong>guarantees the authenticity of the data</strong> being decrypted.</p><p>Now, the respective parser found in Boot9 <strong>lacks several protections</strong>, including bounds checking. In the end, this allowed Derrek to produce a crafted RSA signature (through brute-forcing) that will always succeed on any data. In doing so, the <strong>entirety of the chain of trust was nullified</strong>.</p><p>For the curious, I recommend reading a comprehensive post in GBATemp describing the theory more calmly <sup id="bibref:140"><a href="#bib:anti_piracy-mrjason" role="doc-biblioref">[140]</a></sup>.</p><p>With this, one would now be allowed to craft a firmware for the ARM9 core, sign it with a crafted RSA signature, install it on NAND and Boot9 will ‘just run it’. The question now is, how can the average user do this using an unmodified console?</p></div><div id="tab-13-3-boot9strap"><h5 id="tab-13-3-boot9strap">Boot9strap</h5><p>The year is 2017. Most know about the existence of Sighax but only a handful can apply it, all because the new method requires a crafted RSA signature and writing access to the NAND, none of which is easy to come by (and let’s not forget Nintendo was still clamping down hard on user-land exploits through system updates). Luckily, Sighax was in the process of being democratised.</p><p>Even though Derrek’s announcement didn’t include a suitable RSA signature or a copy of Boot9 (due to copyright reasons, I’m guessing), that didn’t stop hackers SciresM and Myria from finding alternative resources that would enable them to craft an RSA signature.</p><p>In summary, they discovered that system versions before <code>1.0.0</code> shared similar flaws to those previously exposed with Sighax <sup id="bibref:141"><a href="#bib:anti_piracy-sighax_pres" role="doc-biblioref">[141]</a></sup> and, thanks to this, they were able to begin brute-forcing RSA signatures. The result was a success, a match was eventually found with the help of plenty of Nvidia GPUs <sup id="bibref:142"><a href="#bib:anti_piracy-sighax_math" role="doc-biblioref">[142]</a></sup>.</p><p>Now that they could craft an alternative firmware that Boot9 would accept, they needed to find a way to redirect Boot9 to their payload. The challenge was to redirect execution before Boot9 hides its Boot ROM. To tackle this, the duo found a route through the ARM9’s exception handlers. The ARM9 can’t override these, but the NDMA can - and the CPU can command the NDMA to do so.</p><p>All in all, the team were able to use the NDMA to fill the exception handlers with a jump to arbitrary code, and then instruct the ARM9 to copy to <code>NULL</code>, resulting in an exception that would execute the payload with unrestricted access. In the end, this was packaged in a solution called <strong>boot9strap</strong> and served as an alternative bootloader that could either load a payload from the SD card or continue to boot normally. Consequently, Godmode9 was extended to backup OTP and the Boot ROMs, if needed.</p><p>And so, boot9strap quickly displaced arm9loaderhax as the de facto solution for loading arbitrary code with maximum privileges.</p></div><div id="tab-13-4-ntrboot"><h5 id="tab-13-4-ntrboot">Ntrboot</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg"><picture><img alt="Image" width="947" height="900" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg" loading="lazy"></picture></a><figcaption>Some DS Flashcards sold after the discovery of ntrboot came with a switch to enable a ‘3DS mode’ (see the top corner of the photo), this enables trigger ntrboot.</figcaption></figure><p>At this point, there was only one question left: How could users install boot9strap?</p><p>Well, the team didn’t stop there. By taking a look at their recent Boot ROM dumps, they found an interesting routine: During boot, Boot9 will query if a specific <strong>key combination is pressed</strong> and the <strong>lid is closed</strong>. If so, Boot9 will redirect execution to the inserted Nintendo DS card (with full privileges).</p><p>Thus, <strong>ntrboot</strong> came to fruition: Flash a sighax-signed payload into a Nintendo DS flashcard, use a magnet to simulate a closed shell and press the required key combination. Instant Boot9 privileges.</p><p>If this wasn’t enough, Nintendo couldn’t fix any of these vulnerabilities through software updates, as they’re hardwired into the Boot ROM. A possible solution would’ve been to ship new hardware revisions, yet, none ever appeared.</p></div></div></div><h4 id="the-remaining-years-2018-present">The remaining years (2018-present)</h4><p>Now that the homebrew community has achieved its magnum opus, the remaining years of the Nintendo 3DS will only see the streamlining of hacking methods, all of which share the same objective: Install boot9strap.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_67144305ea863bc41e118057e27ecc2f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_15f7a7c7977b5b7d951c8f22169b0417.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_60452d1ade267dfc9302be1e561295f6.webp 985w"><img alt="Image" width="985" height="640" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png" loading="lazy"></picture></a><figcaption>As the methodologies used to hack a 3DS drastically evolve, sometimes too quickly for new users, community-maintained websites like 3ds.hacks.guide currently holds a reputation as the most reliable and updated set of tutorials.</figcaption></figure><p>By this point in time, there were many exploits in the wild: ‘SoundHax’, ‘Safehax’, ‘Browserhax’… too many to mention here. For the curious, 3DBrew provides a comprehensive list <sup id="bibref:143"><a href="#bib:anti_piracy-user_flaws" role="doc-biblioref">[143]</a></sup>.</p><p>To give you an idea of how elegant exploitation became by 2023, let me show you a common method users relied on and didn’t require extra hardware. This process was called ‘seedminer + BannerBomb3’ and combined the following vulnerabilities, the majority of them authored by zoogie:</p><ol><li><strong>seedminer</strong>: User data installed in the 3DS’ SD card is encrypted using AES-128-CTR. Its key is constructed from other keys found in a file called <code>movable.sed</code> (console-unique, stored in NAND). Well, it was discovered that this file can be re-constructed by using the console’s Friend Code, subdirectory names in the SD card (generated by the console) and short-term brute-forcing. Once extracted, the keys allowed to tamper with DSiWare data in the SD card.</li><li><strong>BannerBomb3</strong>: An exploit that overflows the stack of the Settings app while it tries to parse the banner of an installed DSiWare title <sup id="bibref:144"><a href="#bib:anti_piracy-bannerbomb" role="doc-biblioref">[144]</a></sup>. Combined with seedminer, this serves as an entry-level exploit with Kernel11 privileges.</li><li>Now, how to take advantage of BannerBomb3 (i.e.&nbsp;which payload to use) depended on the tutorial the user was following at the time. For simplicity purposes, there were two routes:</li></ol><div><ul><li id="tab-14-1-the-safe-mode-route-link"><a href="#tab-14-1-the-safe-mode-route">The Safe Mode route</a></li></ul><div><div id="tab-14-1-the-safe-mode-route"><h5 id="tab-14-1-the-safe-mode-route">The Safe Mode route</h5><p>This route consisted of exploiting <code>SAFE_FIRM</code> and was described in earlier tutorials:</p><ol><li><strong>unSAFE_MODE</strong>: Users can boot into Safe Mode by pressing a combination of buttons during the console’s boot, the alternative firmware then enables the user to perform a system update, which is useful for repairing the console. Well, zoogie discovered that the proxy settings can be overflowed <sup id="bibref:145"><a href="#bib:anti_piracy-unsafe_mode" role="doc-biblioref">[145]</a></sup>. Hence, providing user-land execution within Safe Mode.</li><li><strong>safehax</strong>: a port of ‘firmlaunch-hax’ to work under SAFE_FIRM. Nintendo originally patched it with system update <code>9.5.0</code> released in February 2015 <sup id="bibref:146"><a href="#bib:anti_piracy-9_5" role="doc-biblioref">[146]</a></sup>. Yet, SAFE_FIRM is an immutable replica of the factory firmware, and thus it features old exploits NATIVE_FIRM once <em>enjoyed</em>.<ol><li><strong>firmlaunch-hax</strong>: When the firmware is booting, the ARM9 stores the firmware’s header in FCRAM for verifying and then parsing. With the help of a race condition, execution can take control of the ARM9, so the boot9strap installer can be launched. Nintendo fixed this by keeping the header in ARM9 RAM instead, although this stayed unpatched on SAFE_FIRM.</li></ol></li></ol></div><div id="tab-14-2-the-home-menu-route"><p>Sometime later, a new route was proposed by new tutorials. This exploited the HOME Menu with a new Menuhax-style hack:</p><ol><li><strong>menuhax67</strong>: The screen brightness configuration value can be overflown <sup id="bibref:147"><a href="#bib:anti_piracy-menuhax67" role="doc-biblioref">[147]</a></sup>, leading to user-land control from the HOME Menu.</li><li><strong>nimdsphax</strong>: An modern exploit chain combining ‘ctr-httpwn’, ‘nimhax’ and ‘dsp pwn’ <sup id="bibref:148"><a href="#bib:anti_piracy-nimdsphax" role="doc-biblioref">[148]</a></sup>.<ol><li><strong>ctr-httpwn</strong> by yellows8: The HTTP service used for network connections can be controlled by overriding its heap memory (using the old GPU DMA exploit) <sup id="bibref:149"><a href="#bib:anti_piracy-httpwn" role="doc-biblioref">[149]</a></sup>.</li><li><strong>nimhax</strong> by luigoalma: Uses ctr-httpwn to escalate and take over the services that control the user file system, console configuration and application management <sup id="bibref:150"><a href="#bib:anti_piracy-nimhax" role="doc-biblioref">[150]</a></sup>.</li><li><strong>dsp pwn</strong> by luigoalma: Uses nimhax to take control of the DSP, which in turn uses the GPU’s DMA to override the Kernel9 memory space. Thus, obtaining ARM9 privileges.</li></ol></li></ol></div></div></div><h5 id="post-2023-and-conclusions">Post-2023 and conclusions</h5><p>Be as it may, at the time of this writing, Nintendo hasn’t quite surrendered to the cat-and-mouse game. In May 2023, system update <code>11.17.0</code> patched BannerBomb3, nullifying one of the last entry points that didn’t require additional materials <sup id="bibref:151"><a href="#bib:anti_piracy-11_17" role="doc-biblioref">[151]</a></sup>. This means users will now need to either obtain a legitimate 3DS game (which can then be exploited), an ntrboot-compatible DS flashcard; or wait for a WebKit exploit (there’s one only left for the New 3DS browser <sup id="bibref:152"><a href="#bib:anti_piracy-skaterhax" role="doc-biblioref">[152]</a></sup>).</p><hr><h2 id="thats-all-folks">That’s all folks</h2><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/my3dss.291f0c00616803ac9f9b570af267a17901a729074ded39c03576c16d4cd6917f.webp"><picture><img alt="Image" width="1037" height="500" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu8ac0a8a08ff3edfea9a6be84b3a66ace_53002_245d657b9cd3a8518579bd5017c67086.png" loading="lazy"></picture></a><figcaption>My Nintendo 3DS(s). Apart from the XL one on the left, I bought two extra for this article: The red one you see on the right (originally listed as ‘broken’, turned out the power socket is just flaky) and another one (<em>correctly</em> listed ‘for parts’) to take the motherboard photos.</figcaption></figure><p>Phew, that was another one of those long articles. I’m glad you managed to keep up and reach the end!</p><p>If you are curious, this article took me almost a year to finish, mainly due to a combination of multiple factors, but the important thing is that I ultimately managed to complete it.</p><p>Looking back, it’s hard to admit this console didn’t enjoy the same degree of success as the Nintendo DS. Considering all of its offerings analysed here, I think many external factors hindered its marketing. For starters, the timing was unfortunate and the price tag wasn’t exactly tempting. From my perspective, back when it launched in 2011, the ‘08 financial crisis was hitting hard (I was living in Spain back then), so the Nintendo 3DS wasn’t exactly on adults’ (and kids’) priorities. I eventually got mine in 2018, by then living in the UK.</p><p>In any case, I want to thank the #ReSwitched and #Godmode9 for spotting lots of mistakes in my initial drafts. This has been the most intricate console I’ve written about (to this date!), nevertheless, I’m very grateful to find communities willing to help out.</p><p>Until next time!<br>Rodrigo</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Outperforming larger language models with less training data and smaller models (275 pts)]]></title>
            <link>https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</link>
            <guid>37606352</guid>
            <pubDate>Fri, 22 Sep 2023 00:29:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</a>, See on <a href="https://news.ycombinator.com/item?id=37606352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2762083676649407668">
<p><span>Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team
</span>

</p><p>
Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via <a href="https://arxiv.org/abs/2005.14165">zero-shot or few-shot prompting</a>. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using <a href="https://arxiv.org/abs/2201.12023">specialized infrastructure</a>, not to mention that today's state-of-the-art LLMs are composed of over <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">500 billion parameters</a>. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance.
</p>

<p>
To circumvent these deployment challenges, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: <a href="https://arxiv.org/abs/1801.06146">fine-tuning</a> or <a href="https://arxiv.org/abs/1503.02531">distillation</a>. Fine-tuning updates a pre-trained smaller model (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a> or <a href="https://arxiv.org/abs/1910.10683">T5</a>) using downstream manually-annotated data. Distillation trains the same smaller models with labels generated by a larger LLM. Unfortunately, to achieve comparable performance to LLMs, fine-tuning methods require human-generated labels, which are expensive and tedious to obtain, while distillation requires large amounts of unlabeled data, which can also be hard to collect.
</p>

<p>
In “<a href="https://arxiv.org/abs/2305.02301">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</a>”, presented at <a href="https://2023.aclweb.org/">ACL2023</a>, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs’ performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png"><img data-original-height="788" data-original-width="1570" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png"></a></td></tr><tr><td>While LLMs offer strong zero and few-shot performance, they are challenging to serve in practice. On the other hand, traditional ways of training small task-specific models require a large amount of training data. Distilling step-by-step provides a new paradigm that reduces both the deployed model size as well as the number of data required for training.</td></tr></tbody></table>


<br>


<h2>Distilling step-by-step</h2>


<p>
The key idea of distilling step-by-step is to extract informative <em>natural language</em> <em>rationales (i.e., </em>intermediate reasoning steps)<em> </em>from LLMs, which can in turn be used to train small models in a more data-efficient way. Specifically, natural language rationales explain the connections between the input questions and their corresponding outputs. For example, when asked, “<em>Jesse's room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet, how much more carpet does she need to cover the whole floor?</em>”, an LLM can be prompted by the few-shot <a href="https://blog.research.google/2022/05/language-models-perform-reasoning-via.html">chain-of-thought</a> (CoT) prompting technique to provide intermediate rationales, such as, “<em>Area = length * width. Jesse’s room has 11 * 15 square feet.</em>” That better explains the connection from the input to the final answer, “<em>(11 * 15 ) - 16</em>”. These rationales can contain relevant task knowledge, such as “<em>Area = length * width”</em>, that may originally require many data for small models to learn. We utilize these extracted rationales as additional, richer supervision to train small models, in addition to the standard task labels.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s1999/image4.png"><img data-original-height="932" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png"></a></td></tr><tr><td>Overview on distilling step-by-step: First, we utilize CoT prompting to extract rationales from an LLM. We then use the generated rationales to train small task-specific models within a multi-task learning framework, where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix.</td></tr></tbody></table>


<p>
Distilling step-by-step consists of two main stages. In the first stage, we leverage few-shot CoT prompting to extract rationales from LLMs. Specifically, given a task, we prepare few-shot exemplars in the LLM input prompt where each example is composed of a triplet containing: (1) input, (2) rationale, and (3) output. Given the prompt, an LLM is able to mimic the triplet demonstration to generate the rationale for any new input. For instance, in a <a href="https://arxiv.org/abs/1811.00937">commonsense question answering task</a>, given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”, paired with the rationale that provides better connection from the question to the answer, “The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people.” By providing CoT examples paired with rationales in the prompt, the <a href="https://arxiv.org/abs/2005.14165">in-context learning ability</a> allows LLMs to output corresponding rationales for future unseen inputs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999/image2.png"><img data-original-height="1175" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png"></a></td></tr><tr><td>We use the few-shot CoT prompting, which contains both an example rationale (<strong>highlighted in green</strong>) and a label (<strong>highlighted in blue</strong>), to elicit rationales from an LLM on new input examples. The example is from a commonsense question answering task.</td></tr></tbody></table>


<p>
After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel <em>rationale generation task</em> in addition to the standard <em><a href="https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1">label prediction task</a></em>. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend <a href="https://arxiv.org/abs/1910.10683">task prefixes</a> (i.e., [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks.
</p>




<h2>Experimental setup</h2>


<p>
In the experiments, we consider a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model as the LLM. For task-specific downstream models, we use <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5 models</a>. For CoT prompting, we use the <a href="https://arxiv.org/abs/2201.11903">original CoT prompts</a> when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: <a href="https://arxiv.org/abs/1812.01193">e-SNLI</a> and <a href="https://arxiv.org/abs/1910.14599">ANLI</a> for<a href="https://arxiv.org/abs/1508.05326"> natural language inference</a>; <a href="https://arxiv.org/abs/1811.00937">CQA</a> for commonsense question answering; and <a href="https://arxiv.org/abs/2103.07191">SVAMP</a> for <a href="https://aclanthology.org/N16-1136/">arithmetic math word problems</a>. We include two sets of baseline methods. For comparison to <a href="https://arxiv.org/abs/2005.14165">few-shot prompted LLMs</a>, we compare to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompting</a> with a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model. In the <a href="https://arxiv.org/abs/2305.02301">paper</a>, we also compare standard task-specific model training to both <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a> and <a href="https://arxiv.org/abs/1503.02531">standard distillation</a>. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes.
</p>



<h3>Less training data</h3>


<p>
Compared to <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a>, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png"><img data-original-height="1477" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png"></a></td></tr><tr><td>Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.</td></tr></tbody></table>

<br>


<h3>Smaller deployed model size</h3>


<p>
Compared to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompted LLMs</a>, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM’s performance using standard fine-tuning.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png"><img data-original-height="1384" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png"></a></td></tr><tr><td>We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM’s performance using the same model size.</td></tr></tbody></table>


<br>


<h3>Distilling step-by-step outperforms few-shot LLMs with smaller models using less data</h3>


<p>
Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM’s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM’s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png"><img data-original-height="1400" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png"></a></td></tr><tr><td>We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM’s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM’s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required.
</p>




<h2>Availability on Google Cloud Platform</h2>


<p>
Distilling step-by-step is available for private preview on <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>. If you are interested in trying it out, please contact <a href="mailto:vertex-llm-tuning-preview@google.com">vertex-llm-tuning-preview@google.com</a> with your Google Cloud Project number and a summary of your use case.
</p>




<h2>Acknowledgements</h2>


<p>
<em>This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.</em>
</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>