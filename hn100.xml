<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 29 Jul 2024 17:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Movable tree CRDTs and Loro's implementation (127 pts)]]></title>
            <link>https://loro.dev/blog/movable-tree</link>
            <guid>41099901</guid>
            <pubDate>Mon, 29 Jul 2024 12:24:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loro.dev/blog/movable-tree">https://loro.dev/blog/movable-tree</a>, See on <a href="https://news.ycombinator.com/item?id=41099901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><main>
<!-- -->

<p><img loading="lazy" width="1792" height="1024" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmovable-tree-cover.f2c64ff7.png&amp;w=1920&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmovable-tree-cover.f2c64ff7.png&amp;w=3840&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmovable-tree-cover.f2c64ff7.png&amp;w=3840&amp;q=75"></p>
<p>This article introduces the implementation difficulties and challenges of Movable Tree CRDTs when collaboration, and how Loro implements it and sorts child nodes. The algorithm has high performance and can be used in production.</p>
<h2>Background<a href="#background" id="background" aria-label="Permalink for this section"></a></h2>
<p>In distributed systems and collaborative software, managing hierarchical relationships is difficult and complex. Challenges arise in resolving conflicts and meeting user expectations when working with the data structure that models movement by combining deletion and insertion. For instance, if a node is concurrently moved to different parents in replicas, it may lead to the unintended creation of duplicate nodes with the same content. Because the node is deleted twice and created under two parents.</p>
<p>Currently, many software solutions offer different levels of support and functionality for managing hierarchical data structures in distributed environments. The key variation among these solutions lies in their approaches to handling potential conflicts.</p>
<h3>Conflicts in Movable Trees<a href="#conflicts-in-movable-trees" id="conflicts-in-movable-trees" aria-label="Permalink for this section"></a></h3>
<p>A movable tree has 3 primary operations: creation, deletion, and movement. Consider a scenario where two peers independently execute various operations on their respective replicas of the same movable tree. Synchronizing these operations can lead to potential conflicts, such as:</p>
<ul>
<li>The same node was deleted and moved</li>
<li>The same node was moved under different nodes</li>
<li>Different nodes were moved, resulting in a cycle</li>
<li>The ancestor node is deleted while the descendant node is moved</li>
</ul>
<h4>Deletion and Movement of the Same Node<a href="#deletion-and-movement-of-the-same-node" id="deletion-and-movement-of-the-same-node" aria-label="Permalink for this section"></a></h4>
<p><img alt="Deletion and Movement of the Same Node" loading="lazy" width="3054" height="1394" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove-delete-dark.17378273.png&amp;w=3840&amp;q=75 1x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove-delete-dark.17378273.png&amp;w=3840&amp;q=75"></p>
<p>This situation is relatively easy to resolve. It can be addressed by applying one of the operations while ignoring the other based on the timestamp in the distributed system or the application's specific requirements. Either approach yields an acceptable outcome.</p>
<h4>Moving the Same Node Under Different Parents<a href="#moving-the-same-node-under-different-parents" id="moving-the-same-node-under-different-parents" aria-label="Permalink for this section"></a></h4>
<p><img alt="Moving the Same Node Under Different Parents" loading="lazy" width="3070" height="1398" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove-same-node-dark.fc82da02.png&amp;w=3840&amp;q=75 1x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove-same-node-dark.fc82da02.png&amp;w=3840&amp;q=75"></p>
<p>Merging concurrent movement operations of the same node is slightly more complex. Different approaches can be adopted depending on the application:</p>
<ul>
<li>Delete the node and create copies of nodes under different parent nodes. Subsequent operations then treat these nodes independently. This approach is acceptable when node uniqueness is not critical.</li>
<li>Allow the node have two edges pointing to different parents. However, this approach breaks the fundamental tree structure and is generally not considered acceptable.</li>
<li>Sort all operations, then apply them one by one. The order can be determined by timestamps in a distributed system. Providing the system maintains a consistent operation sequence, it ensures uniform results across all peers.</li>
</ul>
<h4>Movement of Different Nodes Resulting in a Cycle<a href="#movement-of-different-nodes-resulting-in-a-cycle" id="movement-of-different-nodes-resulting-in-a-cycle" aria-label="Permalink for this section"></a></h4>
<p><img alt="cycle" loading="lazy" width="3024" height="1442" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcycle-dark.267300d7.png&amp;w=3840&amp;q=75 1x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcycle-dark.267300d7.png&amp;w=3840&amp;q=75"></p>
<p>Concurrent movement operations that cause cycles make the conflict resolution of movable trees complex. Matthew Weidner listed several solutions to resolve cycles in his <a href="https://mattweidner.com/2023/09/26/crdt-survey-2.html#forests-and-trees" target="_blank" rel="noreferrer">blog<span> (opens in a new tab)</span></a>.</p>
<blockquote>
<ol>
<li>Error. Some desktop file sync apps do this in practice (<a href="https://doi.org/10.1109/TPDS.2021.3118603" target="_blank" rel="noreferrer">Martin Kleppmann et al. (2022)<span> (opens in a new tab)</span></a> give an example).</li>
<li>Render the cycle nodes (and their descendants) in a special “time-out” zone. They will stay there until some user manually fixes the cycle.</li>
<li>Use a server to process move ops. When the server receives an op, if it would create a cycle in the server’s own state, the server rejects it and tells users to do likewise. This is&nbsp;<a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-trees-of-objects" target="_blank" rel="noreferrer">what Figma does<span> (opens in a new tab)</span></a>. Users can still process move ops optimistically, but they are tentative until confirmed by the server. (Optimistic updates can cause temporary cycles for users; in that case, Figma uses strategy (2): it hides the cycle nodes.)</li>
<li>Similar, but use a&nbsp;<a href="https://mattweidner.com/2023/09/26/crdt-survey-2.html#topological-sort" target="_blank" rel="noreferrer">topological sort<span> (opens in a new tab)</span></a>&nbsp;(below) instead of a server’s receipt order. When processing ops in the sort order, if an op would create a cycle, skip it&nbsp;<a href="https://doi.org/10.1109/TPDS.2021.3118603" target="_blank" rel="noreferrer">(Martin Kleppmann et al. 2022)<span> (opens in a new tab)</span></a>.</li>
<li>For forests: Within each cycle, let&nbsp;<code dir="ltr">B.parent = A</code>&nbsp;be the edge whose&nbsp;<code dir="ltr">set</code>&nbsp;operation has the largest LWW timestamp. At render time, “hide” that edge, instead rendering&nbsp;<code dir="ltr">B.parent = "none"</code>, but don’t change the actual CRDT state. This hides one of the concurrent edges that created the cycle.
• To prevent future surprises, users’ apps should follow the rule: before performing any operation that would create or destroy a cycle involving a hidden edge, first “affirm” that hidden edge, by performing an op that sets&nbsp;<code dir="ltr">B.parent = "none"</code>.</li>
<li>For trees: Similar, except instead of rendering&nbsp;<code dir="ltr">B.parent = "none"</code>, render the previous parent for&nbsp;<code dir="ltr">B</code>&nbsp;- as if the bad operation never happened. More generally, you might have to backtrack several operations. Both&nbsp;<a href="http://dx.doi.org/10.1145/3209280.3229110" target="_blank" rel="noreferrer">Hall et al. (2018)<span> (opens in a new tab)</span></a>&nbsp;and&nbsp;<a href="https://arxiv.org/abs/2103.04828" target="_blank" rel="noreferrer">Nair et al. (2022)<span> (opens in a new tab)</span></a>&nbsp;describe strategies along these lines.</li>
</ol>
</blockquote>
<h4>Ancestor Node Deletion and Descendant Node Movement<a href="#ancestor-node-deletion-and-descendant-node-movement" id="ancestor-node-deletion-and-descendant-node-movement" aria-label="Permalink for this section"></a></h4>
<p><img alt="Ancestor Node Deletion and Descendant Node Movement" loading="lazy" width="2064" height="1136" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove_chlid_delete_parent_dark.4422d913.png&amp;w=3840&amp;q=75 1x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmove_chlid_delete_parent_dark.4422d913.png&amp;w=3840&amp;q=75"></p>
<p>The most easily overlooked scenario is moving descendant nodes when deleting an ancestor node. If all descendant nodes of the ancestor are deleted directly, users may easily misunderstand that their data has been lost.</p>
<h3>How Popular Applications Handle Conflicts<a href="#how-popular-applications-handle-conflicts" id="how-popular-applications-handle-conflicts" aria-label="Permalink for this section"></a></h3>
<p>Dropbox is a file data synchronization software. Initially, Dropbox treated file movement as a two-step process: deletion from the original location followed by creation at a new location. However, this method risked data loss, especially if a power outage or system crash occurred between the delete and create operations.</p>
<p>Today, when multiple people move the same file concurrently and attempt to save their changes, Dropbox detects a conflict. In this scenario, it typically saves one version of the original file and creates a new <a href="https://help.dropbox.com/organize/conflicted-copy" target="_blank" rel="noreferrer">"conflicted copy"<span> (opens in a new tab)</span></a> for the changes made by one of the users.</p>
<p><img alt="Solution for conflicts when moving files with Dropbox" loading="lazy" width="852" height="311" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdropbox_move.467b7931.gif&amp;w=1080&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdropbox_move.467b7931.gif&amp;w=1920&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdropbox_move.467b7931.gif&amp;w=1920&amp;q=75"></p>
<p>The image shows the conflict that occurs when A is moved to the B folder and B
is moved to the A folder concurrently.</p>
<p>Figma is a real-time collaborative prototyping tool. They consider tree structures as the most complex part of the collaborative system, as detailed in their <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-trees-of-objects" target="_blank" rel="noreferrer">blog post about multiplayer technology<span> (opens in a new tab)</span></a>. To maintain consistency, each element in Figma has a "parent" attribute. The centralized server plays a crucial role in ensuring the integrity of these structures. It monitors updates from various users and checks if any operation would result in a cycle. If a potential cycle is detected, the server rejects the operation.</p>
<p>However, due to network delays and similar issues, there can be instances where updates from users temporarily create a cycle before the server has the chance to reject them. Figma acknowledges that this situation is uncommon. Their <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-trees-of-objects" target="_blank" rel="noreferrer">solution<span> (opens in a new tab)</span></a> is straightforward yet effective: they temporarily preserve this state and hide the elements involved in the cycle. This approach lasts until the server formally rejects the operation, ensuring both the stability of the system and a seamless user experience.</p>
<p><img alt="An animation that demonstrates how Figma resolves conflicts." loading="lazy" width="1248" height="824" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffigma-tree.8521d43a.gif&amp;w=1920&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffigma-tree.8521d43a.gif&amp;w=3840&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffigma-tree.8521d43a.gif&amp;w=3840&amp;q=75"></p>
<div><p>An animation that demonstrates how
<a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-trees-of-objects" target="_blank" rel="noreferrer">Figma<span> (opens in a new tab)</span></a>
resolves conflicts.</p></div>
<h2>Movable Tree CRDTs<a href="#movable-tree-crdts" id="movable-tree-crdts" aria-label="Permalink for this section"></a></h2>
<p>The applications mentioned above use movable trees and resolve conflicts based on centralized solutions. Another alternative approach to collaborative tree structures is using Conflict-free Replicated Data Types (CRDTs). While initial CRDT-based algorithms were challenging to implement and incurred significant storage overhead as noted in prior research, such as <a href="https://arxiv.org/pdf/1201.1784.pdf" target="_blank" rel="noreferrer">Abstract unordered and
ordered trees CRDT<span> (opens in a new tab)</span></a> or <a href="https://arxiv.org/pdf/1207.5990.pdf" target="_blank" rel="noreferrer">File system on CRDT<span> (opens in a new tab)</span></a>, but continual optimization and improvement have made several CRDT-based tree synchronization algorithms suitable for certain production environments. This article highlights two innovative CRDT-based approaches for movable trees. The first is presented by Martin Kleppmann et al. in their work <strong><em><a href="https://martin.kleppmann.com/2021/10/07/crdt-tree-move-operation.html" target="_blank" rel="noreferrer">A highly-available move operation for replicated trees<span> (opens in a new tab)</span></a></em></strong> and the second by Evan Wallace in his <strong><em><a href="https://madebyevan.com/algos/crdt-mutable-tree-hierarchy/" target="_blank" rel="noreferrer">CRDT: Mutable Tree Hierarchy<span> (opens in a new tab)</span></a></em></strong>.</p>
<h3>A highly-available move operation for replicated trees<a href="#a-highly-available-move-operation-for-replicated-trees" id="a-highly-available-move-operation-for-replicated-trees" aria-label="Permalink for this section"></a></h3>
<p>This paper unifies the three operations used in trees (creating, deleting, and moving nodes) into a move operation. The move operation is defined as a four-tuple <code dir="ltr">Move t p m c</code>, where <code dir="ltr">t</code> is the operation's unique and ordered timestamp such as <a href="https://en.wikipedia.org/wiki/Lamport_timestamp" target="_blank" rel="noreferrer"><code dir="ltr">Lamport timestamp</code><span> (opens in a new tab)</span></a>, <code dir="ltr">p</code> is the parent node ID, <code dir="ltr">m</code> is the metadata associated with the node, and <code dir="ltr">c</code> is the child node ID.</p>
<p>If all nodes of the tree do not contain <code dir="ltr">c</code>, this is a <strong>creation</strong> operation that creates a child node <code dir="ltr">c</code> under parent node <code dir="ltr">p</code>. Otherwise, it is a <strong>move</strong> operation that moves <code dir="ltr">c</code> from its original parent to the new parent <code dir="ltr">p</code>. Additionally, node deletion is elegantly handled by introducing a designated <code dir="ltr">TRASH</code> node; moving a node to <code dir="ltr">TRASH</code> implies its deletion, with all descendants of <code dir="ltr">TRASH</code> considered deleted. But they remain in memory to prevent concurrent editing from moving them to other nodes. In order to handle the previously mentioned situation of deleting ancestor nodes and moving descendant nodes concurrently.</p>
<p>In the three potential conflicts mentioned earlier, since deletion is also defined as a move operation, <strong>deleting and moving the same node</strong> is transformed into two move operations, leaving only two remaining problems:</p>
<ul>
<li><strong>Moving the same node under different parents</strong></li>
<li><strong>Moving different nodes, creating a cycle</strong></li>
</ul>
<p>Logical timestamps are added so that all operations can be linearly ordered, thus the first conflict can be avoided as they can be expressed as two operations in sequence rather than concurrently for the same node. Therefore, in modeling a Tree using only move operations, the only exceptional case in concurrent editing would be creating a cycle, and operations causing a cycle are termed <strong>unsafe operations</strong>.</p>
<p>This algorithm sorts all move operations according to their timestamps. It can then sequentially apply each operation. Before applying, the algorithm detects cycles to determine whether an operation is safe. If the operation creates a cycle, we ignore the unsafe operation to ensure the correct structure of the tree.</p>
<p>Based on the above approach, the consistency problem of movable trees becomes the following two questions:</p>
<ol>
<li>How to introduce global order to operations</li>
<li>How to apply a remote operation that should be inserted in the middle of an existing sorted sequence of operations</li>
</ol>
<h4>Globally Ordered Logical Timestamps<a href="#globally-ordered-logical-timestamps" id="globally-ordered-logical-timestamps" aria-label="Permalink for this section"></a></h4>
<p><a href="https://en.wikipedia.org/wiki/Lamport_timestamp" target="_blank" rel="noreferrer">Lamport Timestamp<span> (opens in a new tab)</span></a> can determine the causal order of events in a distributed system. Here's how they work: each peer starts with a counter initialized to <code dir="ltr">0</code>. When a local event occurs, the counter is increased by <code dir="ltr">1</code>, and this value becomes the event's Lamport Timestamp. When peer <code dir="ltr">A</code> sends a message to peer <code dir="ltr">B</code>, <code dir="ltr">A</code> attaches its Lamport Timestamp to the message. Upon receiving the message, peer <code dir="ltr">B</code> compares its current logical clock value with the timestamp in the message and updates its logical clock to the larger value.</p>
<p>To globally sort events, we first look at the Lamport Timestamps: smaller numbers mean earlier events. If two events have the same timestamp, we use the unique ID of the peer serves as a tiebreaker.</p>
<h4>Apply a Remote Operation<a href="#apply-a-remote-operation" id="apply-a-remote-operation" aria-label="Permalink for this section"></a></h4>
<p>An op's safety depends on the tree's state when applied, avoiding cycles. Insertion requires evaluating the state formed by all preceding ops. For remote updates, we may need to:</p>
<ol>
<li>Undo recent ops</li>
<li>Insert the new op</li>
<li>Reapply undone ops</li>
</ol>
<p>This ensures proper integration of new ops into the existing sequence.</p>
<h5>Undo Recent Ops<a href="#undo-recent-ops" id="undo-recent-ops" aria-label="Permalink for this section"></a></h5>
<p>Since we've modeled all operations on the tree as move operations, undoing a move operation involves either moving the node back to its old parent or undoing the operation that created this node. To enable quick undoing, we cache and record the <strong>old parent</strong> of the node before applying each move operation.</p>
<h5>Apply the Remote Op<a href="#apply-the-remote-op" id="apply-the-remote-op" aria-label="Permalink for this section"></a></h5>
<p>Upon encountering an unsafe operation, disregarding its effects prevents the creation of a cycle. Nevertheless, it's essential to record the operation, as the safety of an operation is determined <strong>dynamically</strong>. For instance, if we receive and sort an update that deletes another node causing the cycle prior to this operation, the operation that was initially unsafe becomes safe. Additionally, we need to mark this unsafe operation as ineffective, since during undo operations, it's necessary to query the <strong>old parent</strong> node, which is the target parent of the last effective operation in the sequence targeting this node.</p>
<h5>Reapply Undone Ops<a href="#reapply-undone-ops" id="reapply-undone-ops" aria-label="Permalink for this section"></a></h5>
<p>Cycles only occur when receiving updates from other peers, so the undo-do-redo process is also needed at this time. When receiving a new op:</p>

<ul>
<li>If the new operation depends on an op that has not been encountered locally, indicating that some inter-version updates are still missing, it is necessary to temporarily cache the new op and wait to apply it until the missing updates are received.</li>
<li>Compare the new operation with all existing operations. If the <code dir="ltr">opId</code> of the new operation is greater than that of all existing operations, it can be directly applied. If the new operation is safe, record the parent node of the target node as the old parent node, then apply the move operation to change the current state. If it is not safe, mark this operation as ineffective and ignore the operation's impact.</li>
<li>If the new opId is sorted in the middle of the existing sequence, it is necessary to pop the operations that are sorted later from the sequence one by one, and undo the impact of this operation, which means moving back to the child of the old parent node, until the new operation can be applied. After applying the new operation, reapply the undone nodes in sequence order, ensuring that all operations are applied in order.</li>
</ul>
<p>The following animated GIF demonstrates the process executed by <code dir="ltr">Peer1</code>:</p>
<ol>
<li>Received <code dir="ltr">Peer0</code> creating node <code dir="ltr">A</code> with the <code dir="ltr">root</code> node as its parent.</li>
<li>Received <code dir="ltr">Peer0</code> creating node <code dir="ltr">B</code> with <code dir="ltr">A</code> as its parent.</li>
<li>Created node <code dir="ltr">C</code> with <code dir="ltr">A</code> as its parent and synchronized it with <code dir="ltr">Peer0</code>.</li>
<li>Moved <code dir="ltr">C</code> to have <code dir="ltr">B</code> as its parent.</li>
<li>Received <code dir="ltr">Peer0</code>'s moving <code dir="ltr">B</code> to have <code dir="ltr">C</code> as its parent.</li>
</ol>
<p><img loading="lazy" width="1440" height="810" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fundo-do-redo.213bb232.gif&amp;w=1920&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fundo-do-redo.213bb232.gif&amp;w=3840&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fundo-do-redo.213bb232.gif&amp;w=3840&amp;q=75"></p>
<p>The queue at the top right of the animation represents the order of local operations and newly received updates. The interpretation of each element in each <code dir="ltr">Block</code> is as follows:</p>
<p><img loading="lazy" width="891" height="465" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fexplain.ba7477d6.png&amp;w=1080&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fexplain.ba7477d6.png&amp;w=1920&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fexplain.ba7477d6.png&amp;w=1920&amp;q=75"></p>
<p>A particular part of this process to note is the two operations with <code dir="ltr">lamport timestamps</code> of <code dir="ltr">0:3</code> and <code dir="ltr">1:3</code>. Initially, the <code dir="ltr">1:3</code> operation moving <code dir="ltr">C</code> to <code dir="ltr">B</code> was created and applied locally, followed by receiving <code dir="ltr">Peer0</code>'s <code dir="ltr">0:3</code> operation moving <code dir="ltr">B</code> to <code dir="ltr">C</code>. In <code dir="ltr">lamport timestamp</code> order, <code dir="ltr">0:3</code> is less than <code dir="ltr">1:3</code> but greater than <code dir="ltr">1:2</code> (with peer as the tiebreaker when counters are equal). To apply the new op, the <code dir="ltr">1:3</code> operation is undone first, moving <code dir="ltr">C</code> back to its old parent <code dir="ltr">A</code>, then <code dir="ltr">0:3</code> moving <code dir="ltr">B</code> to <code dir="ltr">C</code> is applied. After that, <code dir="ltr">1:3</code> is redone, attempting to move <code dir="ltr">C</code> to <code dir="ltr">B</code> again (the old parent remains <code dir="ltr">A</code>, omitted in the animation). However, a cycle is detected during this attempt, preventing the operation from taking effect, and the state of the tree remains unchanged. This completes an <code dir="ltr">undo-do-redo</code> process.</p>
<h3>CRDT: Mutable Tree Hierarchy<a href="#crdt-mutable-tree-hierarchy" id="crdt-mutable-tree-hierarchy" aria-label="Permalink for this section"></a></h3>
<p>Evan Wallace has developed an innovative algorithm that enables each node to track all its historical parent nodes, attaching a counter to each recorded parent. The count value of a new parent node is 1 higher than that of all the node's historical parents, indicating the update sequence of the node's parents. The parent with the highest count is considered the current parent node.</p>
<p>During synchronization, this parent node information is also synced. If a cycle occurs, a heuristic algorithm reattaches the nodes causing the cycle back to the nearest historical parent node that won't cause a cycle and is connected to the root node, thus updating the parent node record. This process is repeated until all nodes causing cycles are reattached to the tree, achieving all replica synchronization of the tree structure. The demo in <a href="https://madebyevan.com/algos/crdt-mutable-tree-hierarchy/" target="_blank" rel="noreferrer">Evan's blog<span> (opens in a new tab)</span></a> clearly illustrates this process.</p>
<p>As Evan summarized at the end of the article, this algorithm does not require the expensive <code dir="ltr">undo-do-redo</code> process. However, each time a remote move is received, the algorithm needs to determine if all nodes are connected to the root node and reattach the nodes causing cycles back to the tree, which can perform poorly when there are too many nodes.</p>
<p>I established a <a href="https://github.com/Leeeon233/movable-tree-crdt" target="_blank" rel="noreferrer">benchmark<span> (opens in a new tab)</span></a> to compare the performance of the movable tree algorithms.</p>
<h2>Movable Tree CRDTs implementation in Loro<a href="#movable-tree-crdts-implementation-in-loro" id="movable-tree-crdts-implementation-in-loro" aria-label="Permalink for this section"></a></h2>
<p>Loro implements the algorithm proposed by Martin Kleppmann et al., <strong><em><a href="https://martin.kleppmann.com/2021/10/07/crdt-tree-move-operation.html" target="_blank" rel="noreferrer">A highly-available move operation for replicated trees<span> (opens in a new tab)</span></a></em></strong>. On one hand, this algorithm has high performance in most real world scenarios. On the other hand, the core <code dir="ltr">undo-do-redo</code> process of the algorithm is highly similar to how REG (Replayable Event Graph) applies remote updates in Loro. Introduction about <strong>REG</strong> can be found in our previous <a href="https://www.loro.dev/blog/loro-richtext#brief-introduction-to-replayable-event-graph" target="_blank" rel="noreferrer">blog<span> (opens in a new tab)</span></a>.</p>
<p>Movable tree has been introduced in detail, but there is still another problem of tree structure that has not been solved. For movable tree, in some real use cases, we still need the capability to sort child nodes. This is necessary for outline notes or layer management in graphic design softwares. Users need to adjust node order and sync it to other collaborators or devices.</p>
<p>We integrated the <code dir="ltr">Fractional Index</code> algorithm into Loro and combined it with the movable tree, making the child nodes of the movable tree sortable.</p>
<p>There are many introductions to <code dir="ltr">Fractional Index</code> on the web, You can read more about <code dir="ltr">Fractional Index</code> in the <a href="https://www.figma.com/blog/realtime-editing-of-ordered-sequences" target="_blank" rel="noreferrer">Figma blog<span> (opens in a new tab)</span></a> or <a href="https://madebyevan.com/algos/crdt-fractional-indexing/" target="_blank" rel="noreferrer">Evan blog<span> (opens in a new tab)</span></a>. In simple terms, <code dir="ltr">Fractional Index</code> assigns a sortable value to each object, and if a new insertion occurs between two objects, the <code dir="ltr">Fractional Index</code> of the new object will be between the left and right values. What we want to speak about more here is how to deal with potential conflicts brought by <code dir="ltr">Fractional Index</code> in CRDTs systems.</p>
<h3>Potential Conflicts in Child Node Sorting<a href="#potential-conflicts-in-child-node-sorting" id="potential-conflicts-in-child-node-sorting" aria-label="Permalink for this section"></a></h3>
<p>As our applications are in a distributive condition, when multiple peers insert new nodes in the same position, the same <code dir="ltr">Fractional Index</code> would be assigned to these differing content but same position nodes. When updates from the remote are applied to local, conflicts arise as the same <code dir="ltr">Fractional Index</code> is encountered.</p>
<p>In Loro, we retain these identical <code dir="ltr">Fractional Index</code> and use <code dir="ltr">PeerID</code> (unique ID of every Peer) as the tie-breaker for the relative order judgment of the same <code dir="ltr">Fractional Index</code>.</p>
<p><img loading="lazy" width="1096" height="465" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FFI-and-PeerID-dark.82febfcc.png&amp;w=1200&amp;q=75 1x, https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FFI-and-PeerID-dark.82febfcc.png&amp;w=3840&amp;q=75 2x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FFI-and-PeerID-dark.82febfcc.png&amp;w=3840&amp;q=75"></p>
<p>Although this solved the sorting problem among the same <code dir="ltr">Fractional Index</code> nodes from different peers, it impacted the generation of new <code dir="ltr">Fractional Index</code> as we cannot generate a new <code dir="ltr">Fractional Index</code> between two same ones. We use two methods to solve this problem:</p>
<ol>
<li>The first method, as stated in Evan's blog, we could add a certain amount of jitter to each generated <code dir="ltr">Fractional Index</code>, (for the ease of explanation, all examples below take decimal fraction as the <code dir="ltr">Fractional Index</code>) for example, when generating a new <code dir="ltr">Fractional Index</code> between 0 and 1, it should have been 0.5, but through random jitters, it could be <code dir="ltr">0.52712</code>, <code dir="ltr">0.58312</code>, <code dir="ltr">0.52834</code>, etc., thus significantly reducing the chance of same <code dir="ltr">Fractional Index</code> appearing.</li>
<li>If the situation arises where the same <code dir="ltr">Fractional Index</code> is present on both sides, we can handle this problem by resetting these <code dir="ltr">Fractional Index</code>. For example, if we need to insert a new node between <code dir="ltr">0.7@A</code> and <code dir="ltr">0.7@B</code> (which indicates <code dir="ltr">Fractional Index</code> @ <code dir="ltr">PeerID</code>), instead of generating a new <code dir="ltr">Fractional Index</code> between 0.7 and 0.7, we could assign two new <code dir="ltr">Fractional Index</code> respectively for the new node and the <code dir="ltr">0.7@B</code> node between 0.7 and 1, which could be understood as an extra move operations.</li>
</ol>
<p><img loading="lazy" width="2592" height="1354" decoding="async" data-nimg="1" srcset="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fsame-FI-dark.79d4bd5a.png&amp;w=3840&amp;q=75 1x" src="https://loro.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fsame-FI-dark.79d4bd5a.png&amp;w=3840&amp;q=75"></p>
<h3>Implementation and Encoding Size<a href="#implementation-and-encoding-size" id="implementation-and-encoding-size" aria-label="Permalink for this section"></a></h3>
<p>Introducing <code dir="ltr">Fractional Index</code> brings the advantage of node sequence. What about encoding size?</p>
<p>Loro uses <a href="https://github.com/drifting-in-space/fractional_index" target="_blank" rel="noreferrer">drifting-in-space<span> (opens in a new tab)</span></a> <code dir="ltr">Fractional Index</code> implementation based on <code dir="ltr">Vec&lt;u8&gt;</code>, which is base 256. In other words, you need to continuously insert 128 values forward or backward from the default value to increase the byte size of the <code dir="ltr">Fractional Index</code> by 1. The worst storage overhead case, such as inserting new values alternately each time. For example, the initial sequence is <code dir="ltr">ab</code>, insert <code dir="ltr">c</code> between <code dir="ltr">a</code> and <code dir="ltr">b</code>, then insert <code dir="ltr">d</code> between <code dir="ltr">c</code> and <code dir="ltr">b</code>, then <code dir="ltr">e</code> between <code dir="ltr">c</code> and <code dir="ltr">d</code>, like:</p>

<p>a new operation would cause an additional byte to be needed. But such a situation is very rare.</p>
<p>Considering that potential conflicts wouldn't appear frequently in most applications, Loro simply extended the implementation, the original implementation produced new <code dir="ltr">Fractional Index</code> in <code dir="ltr">Vec&lt;u8&gt;</code> by only increasing or decreasing 1 in certain index to achieve relative sorting. The simple jitter solution was added, by appending random bytes in length of jitter value to <code dir="ltr">Fractional Index</code>. To enable jitter in js, you can use <code dir="ltr">doc.setFractionalIndexJitter(number)</code> with a positive value. But this will increase the encoding size slightly, but each <code dir="ltr">Fractional Index</code> only adds <code dir="ltr">jitter</code> bytes. If you want to generate <code dir="ltr">Fractional Index</code> at the same position with 99% probability without conflict, the relationship between <code dir="ltr">jitter</code> settings and the maximum number of concurrent edits <code dir="ltr">n</code> will be:</p>
<table><thead><tr><th><p>jitter</p></th><th><p>max num of concurrent edits</p></th></tr></thead><tbody><tr><td><p>1</p></td><td><p>3</p></td></tr><tr><td><p>2</p></td><td><p>37</p></td></tr><tr><td><p>3</p></td><td><p>582</p></td></tr></tbody></table>
<p>When there are numerous <code dir="ltr">Fractional Indexes</code>, there will be many common prefixes after being sorted, when Loro encodes these <code dir="ltr">Fractional Indexes</code>, prefix optimization would be implemented. Each <code dir="ltr">Fractional Index</code> only saves the amount of same prefix bits and remaining bytes with the previous one, which further downsizes the overall encoding size.</p>
<h3>Related work<a href="#related-work" id="related-work" aria-label="Permalink for this section"></a></h3>
<p>Other than using Fractional Index, there are other movable list CRDT that can make sibling nodes of the tree in order. One of these algorithms is Martin Kleppmann's <a href="https://martin.kleppmann.com/2020/04/27/papoc-list-move.html" target="_blank" rel="noreferrer">Moving Elements in List CRDTs<span> (opens in a new tab)</span></a>, which has been used in Loro's <a href="https://www.loro.dev/docs/tutorial/list" target="_blank" rel="noreferrer">Movable List<span> (opens in a new tab)</span></a>.</p>
<p>In comparison, the implementation of <code dir="ltr">Fractional Index</code> solution is simpler, and no stable position representation is provided for child nodes when modeling nodes in a tree, otherwise, the overall tree structure would be too complex. However, the <code dir="ltr">Fractional Index</code> has the problem of <a href="https://vlcn.io/blog/fractional-indexing#interleaving" target="_blank" rel="noreferrer">interleaving<span> (opens in a new tab)</span></a>, but this is acceptable when some only need relative order and do not require strict sequential semantics, such as figma layer items, multi-level bookmarks, etc.</p>
<h2>Benchmark<a href="#benchmark" id="benchmark" aria-label="Permalink for this section"></a></h2>
<p>We conducted performance benchmarks on the Movable Tree implementation by Loro, including scenarios of random node movement, switching to historical versions, and performance under extreme conditions with significantly deep tree structures. The results indicate that it is capable of supporting real-time collaboration and enabling seamless historical version checkouts.</p>

<div><p>Test environment: M2 Max CPU, you can find the bench code
<a href="https://github.com/loro-dev/loro/blob/main/crates/loro-internal/benches/tree.rs" target="_blank" rel="noreferrer">here<span> (opens in a new tab)</span></a>.</p></div>
<h2>Usage<a href="#usage" id="usage" aria-label="Permalink for this section"></a></h2>

<h3>Demo<a href="#demo" id="demo" aria-label="Permalink for this section"></a></h3>
<p>We developed a simulated Todo app with data synchronization among multiple peers using Loro, including the use of <code dir="ltr">Movable Tree</code> to represent subtask relationships, <code dir="ltr">Map</code> to represent various attributes of tasks, and <code dir="ltr">Text</code> to represent task titles, etc. In addition to basic creation, moving, modification, and deletion, we also implemented version switching based on Loro. You can drag the scrollbar to switch between all the historical versions that have been operated on.</p>

<h2>Summary<a href="#summary" id="summary" aria-label="Permalink for this section"></a></h2>
<p>This article discusses why implementing Movable Tree CRDTs is difficult, and presents two innovative algorithms for movable trees.</p>
<p>For implementation, Loro has integrated <strong><em><a href="https://martin.kleppmann.com/2021/10/07/crdt-tree-move-operation.html" target="_blank" rel="noreferrer">A highly-available move operation for replicated trees<span> (opens in a new tab)</span></a></em></strong> to implement the hierarchical movement of the Tree, and integrated the <code dir="ltr">Fractional Index</code> implementation by <a href="https://github.com/drifting-in-space/fractional_index" target="_blank" rel="noreferrer">drifting-in-space<span> (opens in a new tab)</span></a> to achieve the movement between child nodes. This can meet the needs of various application scenarios.</p>
<p>If you are developing collaborative applications or are interested in CRDT algorithms, you are welcome to join <a href="https://discord.gg/tUsBSVfqzf" target="_blank" rel="noreferrer">our community<span> (opens in a new tab)</span></a>.</p></main></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Children should be allowed to get bored, expert says (2013) (111 pts)]]></title>
            <link>https://www.bbc.com/news/education-21895704</link>
            <guid>41098488</guid>
            <pubDate>Mon, 29 Jul 2024 07:17:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/education-21895704">https://www.bbc.com/news/education-21895704</a>, See on <a href="https://news.ycombinator.com/item?id=41098488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Children should be allowed to get bored so they can develop their innate ability to be creative, an education expert says.</b></p><p>Dr Teresa Belton told the BBC cultural expectations that children should be constantly active could hamper the development of their imagination</p><p>She quizzed author Meera Syal and artist Grayson Perry about how boredom had aided their creativity as children.</p><p>Syal said boredom made her write, while Perry said it was a "creative state".</p><p>The senior researcher at the University of East Anglia's School of Education and Lifelong Learning interviewed a number of authors, artists and scientists in her exploration of the effects of boredom.</p><p>She heard Syal's memories of the small mining village, with few distractions, where she grew up.</p><p>Dr Belton said: "Lack of things to do spurred her to talk to people she would not otherwise have engaged with and to try activities she would not, under other circumstances, have experienced, such as talking to elderly neighbours and learning to bake cakes.</p><p>"Boredom is often associated with solitude and Syal spent hours of her early life staring out of the window across fields and woods, watching the changing weather and seasons.</p><p>"But importantly boredom made her write. She kept a diary from a young age, filling it with observations, short stories, poems, and diatribe. And she attributes these early beginnings to becoming a writer late in life."</p></div><div data-component="text-block"><p>The comedienne turned writer said: "Enforced solitude alone with a blank page is a wonderful spur."</p><p>While Perry said boredom was also beneficial for adults: "As I get older, I appreciate reflection and boredom. Boredom is a very creative state."</p><p>And neuroscientist and expert on brain deterioration Prof Susan Greenfield, who also spoke to the academic, recalled a childhood in a family with little money and no siblings until she was 13.</p><p>"She happily entertained herself with making up stories, drawing pictures of her stories and going to the library."</p><p>Dr Belton, who is an expert in the impact of emotions on behaviour and learning, said boredom could be an "uncomfortable feeling" and that society had "developed an expectation of being constantly occupied and constantly stimulated".</p><p>But she warned that being creative "involves being able to develop internal stimulus".</p><p>"Nature abhors a vacuum and we try to fill it," she said. "Some young people who do not have the interior resources or the responses to deal with that boredom creatively then sometimes end up smashing up bus shelters or taking cars out for a joyride."</p></div><div data-component="text-block"><p>The academic, who has previously studied the impact of television and videos on children's writing, said: "When children have nothing to do now, they immediately switch on the TV, the computer, the phone or some kind of screen. The time they spend on these things has increased. </p><p>"But children need to have stand-and-stare time, time imagining and pursuing their own thinking processes or assimilating their experiences through play or just observing the world around them."</p><p>It is this sort of thing that stimulates the imagination, she said, while the screen "tends to short circuit that process and the development of creative capacity".</p><p>Syal adds: "You begin to write because there is nothing to prove, nothing to lose, nothing else to do.</p><p>"It's very freeing being creative for no other reason other than you freewheel and fill time."</p><p>Dr Belton concluded: "For the sake of creativity perhaps we need to slow down and stay offline from time to time."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding the design of the the Super Nintendo video system (111 pts)]]></title>
            <link>https://fabiensanglard.net/snes_video/index.html</link>
            <guid>41098141</guid>
            <pubDate>Mon, 29 Jul 2024 05:36:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/snes_video/index.html">https://fabiensanglard.net/snes_video/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41098141">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    

</center><p>
July 29, 2024</p>
<p>Designing the Super Nintendo Video System</p><hr>




<p>Last time, I explored the <a href="https://fabiensanglard.net/snes_carts">inside of the Super Nintendo cartridges</a>. Today I am going through  its video system.</p>

<p>I put myself in the shoes of a Nintendo engineer working in Masayuki Uemura (上村雅之)'s team<a name="back_1" href="#footnote_1"><sup>[1]</sup></a><a name="back_2" href="#footnote_2"><sup>[2]</sup></a> by studying what was available in 1989, namely a TV set, to understand what decisions had to be made while designing the SNES video system.</p>

<p>Here is the summary of what I learned. Perhaps you will enjoy tagging along.</p>



<p>What is inside an early 90s TV?</p><hr><p>The screen upon which the SNES outputs video is a standard TV set. Usually it is used to watch Captain Tsubasa, Cobra, Astro Boy, Captain Herlock, Saint Seiya, or Dragon Ball.</p>

<p>There is an antenna on the roof of the house which catches the analog TV broadcast (NTSC), a cable bringing the signal to a tuner, and finally the part where the image is displayed, called a cathode ray tube (CRT).</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/tv.svg" width="178.47072mm" height="68.211235mm">

<p>More importantly for the topic at hand, there are auxiliary (AUX) inputs. A basic TV set would have a composite connector (in yellow) which carries a video signal. The auxiliary stereo audio signals are carried over dedicated jacks (in white and red on my ugly drawing).</p>

<p>How a CRT works</p><hr><p>The CRT is a super line drawing machine. At the time, they were rated at 15kHz which means they could draw in the vicinity of 15,000 lines per second.</p>

<p>Inside the CRT is a gun with three electron cannons. The cannons always shoot straight in front of them, and two sets of magnets (one vertical and one horizontal) route them up/down and left/right.
</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/shadow_mask.svg" width="10.246995in" height="5.2401996in">
<p>In the drawing above, I colored the rays from the cannons but only so the reader can follow. Electrons have no color. There is a mask in front of the phosphor strips to make sure the electrons from each cannon land in the appropriate color strip.</p>
 
<p>There are no pixels in the world of CRTs. A slot is <b>not a pixel</b>. The drawing below zoom into a scanline where various parts of slots are hit. The one guarantee is that electrons from a cannon always land in the correct color strip.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/triad_slots.svg" width="210.54335mm" height="27.952505mm">

<p>A HD TV has smaller slots, better able to render the color signal. In the drawing below, the same line is rendered horizontally with more fidelity thanks to the higher density of slots.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/triad_slots_hd.svg" width="210.54335mm" height="27.952505mm">


<p>How a CRT is controlled</p><hr><p>A CRT consumes five signals, carried over four wires. There is one wire for each of the Red, Green, and Blue signals. They are directly connected to the cannons of the gun. The higher the signal, the more electrons are shot and the more bright the phosphor strips are. No signal on all three wires means no electrons being shot, resulting in black being displayed on that line.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/rgb_wires.svg" width="210.30133mm" height="30.451578mm">
<p>The white wire in the drawing above carries the synchronization signals. There are two, named Horizontal Sync (HSYNC) and Vertical Sync (VSYNC). The two signals use the same wire so it is called Composite Sync (CSYNC).</p>

<p>With my PC programming background, I was used to "Wait for VSYNC" which carried the false idea the CRT emitted it. That is wrong. A CRT emits nothing, it only consumes signals and tries to synchronize the cannon with them.</p>

<p>How a CRT draws an image</p><hr><p>The CRT draws a line (a.k.a raster) from left to right. When it receives a HSYNC event, it "returns" to the left of the screen (X=0). When it receives a VSYNC event, it goes back to the top of the screen (Y=0). 
</p>

<p>Observant readers will see a problem with these events. There is no way to go down. The system driving the CRT can issue as many HSYNC and VSYNC as it wants, the same line at the top of the screen will end up being drawn over and over.
</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/one_line.svg" width="208.88399mm" height="82.920204mm">

<p>The key to understand CRTs</p><hr><p>The key to understanding CRTs is to assimilate that the cannon moves towards the right of the screen with a <b>downward slope</b><a name="back_3" href="#footnote_3"><sup>[3]</sup></a>. Upon HSYNC, the CRT returns to X = 0 but because the cannon will have aimed downward, the next line will be drawn below the previous one.</p>


<img loading="lazy" src="https://fabiensanglard.net/snes_video/p_scan.svg" width="208.88399mm" height="82.920204mm">
<p>This opens the door to cool tricks. The drawing above shows a signal where VSYNC is issued at the same time as the last HSYNC. The lines are always drawn at the same location on the screen. But look below what happens if a VSYNC is issued between two HSYNC.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/i_scan.svg" width="208.88399mm" height="82.920204mm">
<p>Because it only drew half a line at the bottom, the CRT starts drawing the next line at the top of the screen at the same X position. The next set of lines will be interlaced with the previous set.</p>

<p>Lines sets are called "fields". The mode where fields are drawn at the same location is called "progressive" scan ("p"). The mode where fields are interlaced is abbreviated "i". In i mode, the tradeoff is that the vertical resolution is doubled but the refresh rate of each line is halved.</p> 

<p>NTSC issues two fields at 30-ish Hz. Therefore all CRTs provisioned enough space between lines for interlacing. When drawing in progressive, scanline spacing is visible. It results in black space between lines<a name="back_4" href="#footnote_4"><sup>[4]</sup></a> which are characteristic of CRT rasterization.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_video/zelda3.webp" width="1154" height="866"><span><i><small>
Visible scanlines gaps. Photo source: <a href="https://www.retrogameboards.com/t/scanline-screenshot-thread-because-240p-is-all-the-ps-i-need-56k-warning/71/83?page=5">retrogameboards.com</a></small></i></span>

<p>What is inside a line?</p><hr><p>The CRT is numeric when it comes to drawing lines but analog when it comes to what is inside a line<a name="back_5" href="#footnote_5"><sup>[5]</sup></a>. As seen in the drawing, the three cannons are directly connected to the three RGB wires. A system is free to change the color signal as much as it wants (hence use any horizontal resolution). The only limit is signal propagation and the slot mask density.
</p>

 

<p>Dealing with what exists</p><hr><p>While the SNES designers could issue what they wanted on the wires, they still had to make sure the CRT would be able to deal with it. Since the hardware is designed to display a NTSC signal, whatever they decided on had to be close to these specifications<a name="back_6" href="#footnote_6"><sup>[6]</sup></a><a name="back_7" href="#footnote_7"><sup>[7]</sup></a>.
</p>

<ul>
  <li>4:3 aspect ratio</li>
  <li>Number of lines: 262.5 per field</li>
  <li>Number of dots on a line: 341.25</li>
  <li>Field frequency: 59.94Hz</li>
</ul>

<p>59.94Hz is such a weird number. Isn't the power grid running at 60Hz and TVs used that AC frequency directly? Black and White NTSC used to be 60Hz. When broadcast engineers had to find a way to add color to the NTSC signal without breaking backward compatibility they decided to reduce frequency by 0.1% to avoid artifacts<a name="back_8" href="#footnote_8"><sup>[8]</sup></a><a name="back_9" href="#footnote_9"><sup>[9]</sup></a>.</p>



<p>Being a Nintendo engineer</p><hr> 
<p>Now that we know how a CRT works, it is time to play at being a Nintendo engineer and craft a video system.</p>

<p>The first choice to make it how many lines we want. NTSC uses 262.5 lines per field but the half-line is to interlace fields. We can use 262 to make it progressive. With a target framerate of 59.94, that should require 15,734.26 lines per second which is within 4% of the 15KHz rating.
</p>

<p>The CRT screen has an aspect ratio of 4:3. If we use 350 dots horizontally, we will match exactly that aspect ratio and there will be no distortion when the console image is converted into scanlines.</p>
 
<p>262 lines at 59.94Hz, each with 350 dots means we need a dot clock pulsing at 262 * 350 * 59.94 = 5,496,498Hz. We can craft an ASIC which counts dot ticks. Every 350 ticks it issues a HSYNC. Every 350*262 = 91,700 ticks, it issues a VSYNC<a name="back_10" href="#footnote_10"><sup>[10]</sup></a>. I guess we are done?</p>







<p>We are not done. We are just getting started</p><hr>
<p>There are two issues with this naive design.</p>

<ol>
<li>It requires a dot clock of 5,496,498Hz we don't have. The SNES cost constraints prevent the video system from getting its own oscillator. There is a Master oscillator which sub-systems must use via dividers<a name="back_11" href="#footnote_11"><sup>[11]</sup></a>.
</li>

<li>You can't draw color all the time. This is called overscan. And it deserves its own section.</li>
</ol>



<p>Introducing overscan</p><hr>

<p>When the gun position is reset horizontally or vertically, it continues to shoot electrons. If it was to keep on shooting, it would create visible artifacts.</p>

<p>Another thing to consider is that TVs tend to over-scan their screen area<a name="back_12" href="#footnote_12"><sup>[12]</sup></a>, which means the picture on the screen is a little larger than the display. How much the TV over-scans varies from TV to TV. This happens to hide wobbling.</p>

<p>When the gun vertical position is reset to Y=0 (after VSYNC), it is going to undulate up and down for a while. You only get straight lines after a few µs. The same problem happens horizontally after HSYNC.</p>




<p>The solution to all these problems is to "stop" the CRT cannon a little bit after VSYNC and after HSYNC. These time spans during which no electrons are shot are called respectively VBLANK and HBLANK.</p>

<div><p>All gaming systems of that era used blanking. Here is a summary of the SNES competitors.</p><table>

<tbody><tr>  
  <th>Machine</th>
  <th>Year</th>
  <th>Lines</th>
  <th>VBLANK lines</th>
  <th>Visible lines</th>
  <th>Lines per second</th>
  <th>Framerate</th>
</tr>
  <tr>
  <td>Capcom arcade CPS-1</td>
  <td>1989</td>
  <td>262</td>
  <td>38</td>
  <td>224</td>
  <td>15,622</td>
  <td>59.6294<a name="back_13" href="#footnote_13"><sup>[13]</sup></a></td>
</tr>  
<tr>
  <td>Sega Genesis</td>
  <td>1989</td>
  <td>262</td>
  <td>38</td>
  <td>224</td>
  <td>15,700</td>
  <td>59.9227<a name="back_14" href="#footnote_14"><sup>[14]</sup></a></td>
</tr>  
<tr>
  <td>Neo-Geo AES<a name="back_15" href="#footnote_15"><sup>[15]</sup></a></td>
  <td>1990</td>
  <td>264</td>
  <td>40</td>
  <td>224</td>
  <td>15,734</td>
  <td>59.18&nbsp;&nbsp;<a name="back_16" href="#footnote_16"><sup>[16]</sup></a></td>
</tr>  
</tbody></table>
</div>






<p>Picking the SNES vertical resolution</p><hr><img loading="lazy" src="https://fabiensanglard.net/snes_video/super-mario.png" width="256" height="262">
<p>If we look closely at the recap table above, we see that all the competing systems, namely the Megadrive, the Neo-Geo, and Capcom's CPS-1 used 224 visible lines.</p>

<p>
They probably did not pick that number at random. 224 is a number evenly divisible by 16 (224/16 = 14) which means it plays nicely with the graphic rendering pipeline tilemaps.</p>

<p>My best guess is that Nintendo did not want to reinvent the wheel. They did not need higher resolution but better graphics. What made the system stand apart was its PPUs.</p>

<div><p>In the end, they went the safe way and split their 262 lines per frame into 224 visible + 38 blanks (as the drawing on the right shows).</p></div> 

<div>




<div><p>Arcade games could afford to be as peculiar as they wanted on a per-title basis. 
The designers of R-Type at Irem were unsatisfied with the default ”standard” 224 active lines of a CRT.</p><p>

They calibrated their M72-System registers to draw 284 lines, 512 dots, and used an 8 Mhz dot-clock. Leaving 128 dots to HBLANK and 28 lines to VBLANK resulted in an active resolution of 384x256 which was higher than other arcade titles at the time.</p><p>
The trade-off was a vertical refresh rate of 55.017605 Hz which was visually less pleasing and dangerously 10% off from the CRT recommended values. This refresh rate is difficult to replicate for ”modern” emulators but what an impressive feat for a 1987 system!</p><p>



<img loading="lazy" src="https://fabiensanglard.net/snes_video/rtype.png" width="384" height="256"><span><i><small>
R-Type (1984) has a whopping 256 visible lines (photo credit: <a href="https://en.wikipedia.org/wiki/R-Type">wikipedia</a>)!</small></i></span></p></div>

<p>Picking the SNES horizontal resolution</p><hr><p>So far we have picked a number of lines per frame (262). We also know we won't be able to pick a dotclock. We have to use the Master clock (21.47727MHz) and use a divider to end up close to NTSC dotclock. That leaves us with using a 21.47727 Mhz / 4 = 5.3693175 MHz dot clock.</p>

<p>Lines, dots, dot clock and refresh rate are inter-connected via the framerate equation.</p>

<center>
<p>refresh rate = <span>
    <span>lines * dots</span>
    <span>/</span>
    <span>dot clock</span>
</span></p>
</center>

<p>Given that our target refresh rate is 59.94Hz, we don't have much of a choice for the number of dots per line.</p>

<center>
<p>dots = <span>
  <span>5369317.5 (dot clock)</span>
    <span>/</span>
    <span>262 (lines) * 59.94 (rate)</span>
</span> ≃ 342 </p>
</center>

<p>Except that for gory reasons involving carrier artifact when using composite outputs, Nintendo engineers had to use 341 dot per lines instead of 342. This leaves the SNES with a framerate of:</p>

<center>
<p>refresh rate = <span>
    <span>5369317.5</span>
    <span>/</span>
    <span>(341 * 262)</span>
</span> = <b>60.098Hz</b></p>
</center>

<p>60.098Hz is not NTSC's 59.94 Hz but since, as seen previously with R-Type, CRTs have tolerance it works. If you enjoyed this part, Nerdy pleasure has plenty more<a name="back_17" href="#footnote_17"><sup>[17]</sup></a></p>



<p>Picking the SNES horizontal Overscan</p><hr>
<p><img loading="lazy" src="https://fabiensanglard.net/snes_video/super-mario-overscan-res.png" width="342" height="262"></p><p>Of these 341 dots, all of them are not usable for the same wobbling, artifact hiding, and TV overscan  reasons. The SNES needs an horizontal overscan during which it issues a blank signal.</p>

<p>The constraints are:
  </p><ul>
    <li>Result in an aspect ratio close to 4:3. This would mean 224*(4/3) = 298 visible dots.</li>
    <li>Play nice with the graphic pipeline tilemaps which uses 16x16 tiles. That leaves values 304 (16x19), 288 (16x18), 272 (16x17), 256 (16x16), 240 (16*15), and so on. The best value, the one resulting in next to no distortion on the screen would be 304 dots.</li>
  </ul>

<p>A third constraint was to allow enough time for the PPU to populate its sprite line buffer during HBLANK. My guess is that up to 128 sprites was a lot of data to retrieve and the PPU needed more than the 7µs granted by 37 dots of HBLANK if 304 visible dots was to be picked as horizontal resolution.</p>

<p>In the end, Nintendo decided on <b>256 visible dots</b> per line with <b>85 dots of HBLANK</b>. This means the PPU has 16µs to retrieve sprite data during HBLANK. This also means the aspect ratio was not 4:3 but 8:7 which results in slight distortion when the CRT displayed what the PPU generated.</p>


<p>High vertical Resolution mode: Interlacing</p><hr>
<p>So far we have designed the SNES video system with only progressive mode in mind.

</p><pre>Overscan resolution: 341x262
Visible resolution:  256x224
Framerate:           60.098Hz
</pre>

<p>Even though this is what 99% of games ended up using, the SNES also had high-resolution modes. I can double its resolution vertically and/or horizontally.</p>


<p>Doubling the resolution vertically to 448 lines is easy. We can just change the counter to issue a VSYNC half a line after the latest HSYNC to interlace frames. That means drawing 262.5 lines per frame but each line is now refreshed at only 60.098/2=30.049Hz. It will cause flickering and it won't be very pleasant but the vertical resolution will be higher<a name="back_18" href="#footnote_18"><sup>[18]</sup></a>.</p>


<p>High horizontal Resolution mode: The hack</p><hr>
<p>Doubling the horizontal resolution however is much more difficult since the console doesn't have the dotclock for it.</p>

<p>The hack is that the SNES shifts every second field horizontally a bit, so the dots of the field end up between the dots of the previous field. You end up with something running at half the framerate and massive color bleeding. Quite a few titles used it, mainly for menu screens as detailed in <a href="https://problemkaputt.de/fullsnes.txt">fullsnes.txt</a>.</p>


 <pre>Hires Software
  Air Strike Patrol (mission overview)       (whatever mode? with Interlace)
  Bishoujo Wrestler Retsuden (some text)     (512x448, BgMode5+Interlace)
  Ball Bullet Gun (in lower screen half)     (512x224, BgMode5)
  Battle Cross (in game) (but isn't hires?)  (512x224, BgMode1+PseudoH)(Bug?)
  BS Radical Dreamers (user name input only) (512x224, BgMode5)
  Chrono Trigger (crash into Lavos sequence) (whatever mode? with Interlace)
  Donkey Kong Country 1 (Nintendo logo)      (512x224, BgMode5)
  G.O.D. (intro &amp; lower screen half)         (512x224, BgMode5)
  Jurassic Park (score text)                 (512x224, BgMode1+PseudoH+Math)
  Kirby's Dream Land 3 (leaves in 1st door)  (512x224, BgMode1+PseudoH)
  Lufia 2 (credits screen at end of game)    (whatever mode?)
  Moryo Senki Madara 2 (text)                (512x224, BgMode5)
  Power Drive (in intro)                     (512x448, BgMode5+Interlace)
  Ranma 1/2: Chounai Gekitou Hen             (256x448, BgMode1+InterlaceBug)
  RPM Racing (in intro and in game)          (512x448, BgMode5+Interlace)
  Rudra no Hihou (RnH/Treasure of the Rudras)(512x224, BgMode5)
  Seiken Densetsu 2 (Secret of Mana) (setup) (512x224, BgMode5)
  Seiken Densetsu 3                          (512x224, BgMode5)
  Shock Issue 1 &amp; 2 (homebrew eZine)         (512x224, BgMode5)
  SNES Test Program (by Nintendo) (Character Test includes BgMode5/BgMode6)
  Super Play Action Football (text)          (512x224, BgMode5)
  World Cup Striker (intro/menu)             (512x224, BgMode5)

Notes: Ranma is actually only 256x224 (but does accidentally have interlace enabled,
which causes some totally useless flickering).


</pre>


<p>PAL vs NTSC</p><hr><p>We are still not done. In Europe, TVs don't use NTSC but PAL and the French even use SECAM. The framerate expected is exactly 50Hz and there are 312.5 lines per field.</p>

<p>That is actually a simple problem to solve. These versions of the SNES ship with a Master clock running at 17.7344750MHz. The same divider gives a dot clock of 5.32034250MHz. The overscan resolution is 312 lines by 341 dots. The visible resolution is 224 lines by 256 dots. And the framerate is 50.00697891Hz.
</p>

<p>The problem is that only 224 lines of graphics is going to result in big black bands above and below the active zone. This is solved via an "Overscan mode" which increased the number of visible lines to 240 (that is 16 lines which is one tile tall).</p>

<p>What a blessing for game developers willing to port a game to the European market you may say. In practice, "overscan mode" was never used. Most titles were tailor made for 224 lines so the developers did not know what to put in these 16 extra lines. In total, only twelve titles ever used it<a name="back_19" href="#footnote_19"><sup>[19]</sup></a>. Nintendo still managed to do something awesome with their flagship title Super Mario World by increasing the vertical view range.</p>




<p>Note that both NTSC and PAL screen use the same 4:3 aspect ratio so the PAL image is a little bit more compressed vertically than the NTSC one.</p>



<p>Besides the annoying black band, the game code was also rarely revised to account for the VSYNC which occurred at 50.00697891Hz instead of 60.098Hz. This resulted in game running 17% slower than intended. European gaming was a real dumpster fire. But luckily without the internet we did not know about it.</p>

<p>Outputs</p><hr><p>So far we have only considered the "pure" signals needed to drive a CRT. However, few TVs set allowed to directly feed the CRT. Most sets only had a yellow composite jack input in the back while some high-end models had S-Video inputs. </p>

<p>The SNES does something pretty cool to handle this diversity. It converts the CRT signals to both composite and S-Video<a name="back_20" href="#footnote_20"><sup>[20]</sup></a>.</p>


<p><img loading="lazy" src="https://fabiensanglard.net/snes_video/multi-out.svg" width="244.45126mm" height="65.154602mm"></p><p>The AV connector</p><hr><p><img loading="lazy" src="https://fabiensanglard.net/snes_video/av.svg" width="88mm" height="55mm"></p><p>None of the signals are discarded. Thanks to the design of its AV output, gamers get a la carte access to the pure "RGB/CSync" signal, the "Composite" signal, and the S-Video.</p>

<pre>
1. Red                7. Luminance (S-Video)
2. Green              8. Chrominance (S-Video)
3. C-Sync             9. Composite Video
4. Blue              10. +5V DC
5. Ground            11. Left Audio 
6. Ground            12. Right Audio
</pre>

<p>European TVs, especially those in France, came with SCART connectors (a.k.a Prise peritel). This allowed them to craft cables feeding the CRT directly<a name="back_21" href="#footnote_21"><sup>[21]</sup></a>.</p>


<p><img loading="lazy" src="https://fabiensanglard.net/snes_video/scart.webp" width="974" height="553"></p><p>That way we could enjoy our 17% slower, black-banded games at the highest level of visual fidelity.</p>
<p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [ 1]</td><td><a href="https://www.youtube.com/watch?v=FlOAd81a1aI">Inside 1990 Nintendo Headquarters in Kyoto</a></td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [ 2]</td><td><a href="https://www.youtube.com/watch?v=zt2i51CZZ5M">Nintendo Headquarters in 1993</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [ 3]</td><td><a href="https://www.analog.com/en/resources/technical-articles/basics-of-analog-video.html">Video Basics</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [ 4]</td><td><a href="https://www.youtube.com/watch?v=3GJUM6pCpew">Why is TV 29.97 frames per second?</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [ 5]</td><td><a href="https://www.youtube.com/watch?v=puksg4iD3RY">How Games Used to Look: Why Retro Gaming on a CRT Looks WAY Different</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [ 6]</td><td><a href="https://fabiensanglard.net/snes_video/NTSCspecifications.pdf">NTSC specs by jsgil</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [ 7]</td><td><a href="https://forums.nesdev.org/viewtopic.php?t=7265">NTSC specs</a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [ 8]</td><td><a href="https://en.wikipedia.org/wiki/NTSC#:~:text=Resolution%20and%20refresh%20rate">NTSC (wikipedia)</a></td></tr><tr><td><a name="footnote_9"></a><a href="#back_9">^</a></td><td> [ 9]</td><td><a href="https://www.youtube.com/watch?v=CBFlhj2UMEk">Tektronix explains analog video color, 1979</a></td></tr><tr><td><a name="footnote_10"></a><a href="#back_10">^</a></td><td> [10]</td><td><a href="https://digilent.com/reference/learn/programmable-logic/tutorials/vga-display-congroller/start">VGA Display Controller</a></td></tr><tr><td><a name="footnote_11"></a><a href="#back_11">^</a></td><td> [11]</td><td><a href="https://fabiensanglard.net/snes_video/snes_hearts">The hearts of the Super Nintendo</a></td></tr><tr><td><a name="footnote_12"></a><a href="#back_12">^</a></td><td> [12]</td><td><a href="http://scanline.ca/overscan/">Overscan and broadcast television</a></td></tr><tr><td><a name="footnote_13"></a><a href="#back_13">^</a></td><td> [13]</td><td><a href="https://gamicus.fandom.com/wiki/CP_System#cite_note-cps1drv-7">CPS-1 Framerate</a></td></tr><tr><td><a name="footnote_14"></a><a href="#back_14">^</a></td><td> [14]</td><td><a href="https://assets.analogue.co/pdf/cb0e551939f5a31cbe617382f3178d47/Analogue+Mega+Sg+User+Manual+1.3.pdf">Analogue Mega Sg User Manual</a></td></tr><tr><td><a name="footnote_15"></a><a href="#back_15">^</a></td><td> [15]</td><td><a href="https://www.reddit.com/r/gamecollecting/comments/pe8ex/neo_geo_mvsaes_guide/">Neo Geo MVS/AES Guide</a></td></tr><tr><td><a name="footnote_16"></a><a href="#back_16">^</a></td><td> [16]</td><td><a href="https://wiki.neogeodev.org/index.php?title=Framerate">Neo Geo framerate</a></td></tr><tr><td><a name="footnote_17"></a><a href="#back_17">^</a></td><td> [17]</td><td><a href="https://nerdlypleasures.blogspot.com/2017/01/classic-systems-true-framerate.html">Classic Systems - The True Framerate</a></td></tr><tr><td><a name="footnote_18"></a><a href="#back_18">^</a></td><td> [18]</td><td><a href="https://nerdlypleasures.blogspot.com/2017/10/the-rise-of-interlacing-in-video-game.html">The Rise of Interlacing in Video Game Consoles</a></td></tr><tr><td><a name="footnote_19"></a><a href="#back_19">^</a></td><td> [19]</td><td><a href="https://snes.nesdev.org/wiki/Uncommon_graphics_mode_games#:~:text=The%20launch%20PAL%20release%20did%20not%20use%20overscan%2C%20but%20a%20later%20PAL%20revision%20did.">SNES games using Overscan (239 lines)</a></td></tr><tr><td><a name="footnote_20"></a><a href="#back_20">^</a></td><td> [20]</td><td><a href="https://fabiensanglard.net/snes_video/snes_schematic.pdf">SNES schematic</a></td></tr><tr><td><a name="footnote_21"></a><a href="#back_21">^</a></td><td> [21]</td><td><a href="https://fabiensanglard.net/snes_video/nintendo-snes-pal-manual.pdf">SNES Pal manual</a></td></tr></tbody></table></p> <hr>
 <center>*</center></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ps aux written in bash without forking (201 pts)]]></title>
            <link>https://github.com/izabera/ps</link>
            <guid>41097241</guid>
            <pubDate>Mon, 29 Jul 2024 01:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/izabera/ps">https://github.com/izabera/ps</a>, See on <a href="https://news.ycombinator.com/item?id=41097241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>ps aux</code> written entirely in bash without ever forking</h2><a id="user-content-ps-aux-written-entirely-in-bash-without-ever-forking" aria-label="Permalink: ps aux written entirely in bash without ever forking" href="#ps-aux-written-entirely-in-bash-without-ever-forking"></a></p>
<p dir="auto">An interview question for a position that requires knowledge of bash/linux/stuff could be:</p>
<blockquote>
<p dir="auto">What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken.  What do you do?</p>
</blockquote>
<p dir="auto">And if that's what you're facing, this might be the tool for you!  Now you can kinda sorta pretend that you have access to a working <code>ps aux</code>.</p>
<p dir="auto">It definitely totally works on 100% of the machines in every situation ever, guaranteed.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LeanDojo: Theorem Proving in Lean Using LLMs (154 pts)]]></title>
            <link>https://leandojo.org/</link>
            <guid>41096486</guid>
            <pubDate>Sun, 28 Jul 2024 22:34:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leandojo.org/">https://leandojo.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41096486">Hacker News</a></p>
<div id="readability-page-1" class="page">


<div>
                    <h2>LeanDojo: Theorem Proving in Lean using Language Models</h2>
<!--                     <h3 class="title is-4 conference-authors"><a target="_blank" href="https://nips.cc/">NeurIPS 2023 (Datasets and Benchmarks Track), Oral presentation</a></h3> -->
                    <!--
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a target="_blank" href="https://yangky11.github.io/">Kaiyu&#160;Yang</a> <a href="mailto:kaiyuy@caltech.edu"><i class="fas fa-envelope"></i></a><sup>1</sup>,
                            <a target="_blank" href="https://aidanswope.com/about">Aidan&#160;Swope</a><sup>2</sup>,
                            <a target="_blank"
                            href="https://minimario.github.io/">Alex&#160;Gu</a><sup>3</sup>,
                            <a target="_blank" href="https://rchalamala.github.io/">Rahul&#160;Chalamala</a><sup>1</sup>,
                            <a target="_blank" href="https://peiyang-song.github.io/">Peiyang&#160;Song</a><sup>4</sup>,
                            <a target="_blank"
                            href="https://billysx.github.io/">Shixing&#160;Yu</a><sup>5</sup>,
                            <br>
                            <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad&#160;Godil</a><sup>2</sup>,
                            <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan&#160;Prenger</a><sup>2</sup>,
                            <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima&#160;Anandkumar</a><sup>1 2</sup>
                
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Caltech; </span>
                        <span class="author-block"><sup>2</sup>NVIDIA; </span>
                        <span class="author-block"><sup>3</sup>MIT; </span>
                        <span class="author-block"><sup>4</sup>UC Santa Barbara; </span>
                        <span class="author-block"><sup>5</sup>UT Austin </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><i class="fas fa-envelope"></i> Correspondence to: kaiyuy@caltech.edu</span>
                    </div>
                    -->

                    
                </div>


<div>
                    <h2><span>LLMs as Copilots for Theorem Proving</span></h2>
                    <video autoplay="" controls="">
            		  <source src="https://leandojo.org/images/Lean%20Copilot%20demo.mp4" type="video/mp4">
            		  Your browser does not support HTML video.
            		</video>
                    
                    <p>We introduce <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> for LLMs to act as copilots in Lean for proof automation, e.g., suggesting tactics/premises and searching for proofs. Users can use our model or bring their own models that run either locally (w/ or w/o GPUs) or on the cloud.</p>
                </div>
    
<div>
                    <h2><span>Overview of LeanDojo</span></h2>
                    <p><img src="https://leandojo.org/images/LeanDojo.jpg" alt=""></p><p><b>Top right</b>: LeanDojo extracts proofs in <a target="_blank" href="https://leanprover.github.io/">Lean</a> into datasets for training machine learning models. It also enables the trained model to prove theorems by interacting with Lean's proof environment.</p>
                      
                    <p><b>Top left</b>: The proof tree of a Lean theorem <a target="_blank" href="https://github.com/leanprover-community/lean/blob/cce7990ea86a78bdb383e38ed7f9b5ba93c60ce0/library/init/data/nat/gcd.lean#L36">\(\forall n \in \mathbb{N},~\texttt{gcd n n = n}\)</a>, where <span>\(\texttt{gcd}\)</span> is the greatest common divisor. When proving the theorem, we start from the original theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such as <span>\(\texttt{mod_self}\)</span> and  <span>\(\texttt{gcd_zero_left}\)</span> defined in a large math library. E.g., <span>\(\texttt{mod_self}\)</span> is an existing theorem <a target="_blank" href="https://github.com/leanprover-community/lean/blob/cce7990ea86a78bdb383e38ed7f9b5ba93c60ce0/library/init/data/nat/lemmas.lean#L861">\(\forall n \in \mathbb{N},~\texttt{n % n = 0}\)</a> used in the proof to simplify the goal.</p>
                    
                    <p><b>Bottom</b>: Our ReProver model. Given a state, it retrieves premises from the math library, which are concatenated with the state and fed into an encoder-decoder Transformer to generate the next tactic.</p>
                </div>
<!--
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. 
                        However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute 
                        requirements. This has created substantial barriers to research on machine learning methods for theorem proving. 
                        This paper removes these barriers by introducing <em>LeanDojo</em>: an open-source Lean playground consisting of toolkits, 
                        data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. 
                        It contains fine-grained annotations of premises in proofs, providing valuable data for <em>premise selection</em>&mdash;
                        a key bottleneck in theorem proving. Using this data, we develop <em>ReProver</em> (<u>Re</u>trieval-Augmented <u>Prover</u>): 
                        an LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. 
                        It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis 
                        capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. 
                        Furthermore, we construct a new benchmark consisting of 98,641 theorems and proofs extracted from Lean's math library. 
                        It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never 
                        used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness 
                        of ReProver over non-retrieval baselines and GPT-4. <b>We thus provide the first set of open-source LLM-based theorem provers without 
                        any proprietary datasets and release it under a permissive MIT license to facilitate further research.</b>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
-->


<div>
                    <h2><span>Benchmarks</span></h2>
                    <ul>
                            <li><a href="https://doi.org/10.5281/zenodo.8016385"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.8016385.svg" alt="DOI"></a> <b>LeanDojo Benchmark</b>: 98,734 theorems/proofs, 217,776 tactics, and 130,262 premises from <a target="_blank" href="https://github.com/leanprover-community/mathlib/tree/19c869efa56bbb8b500f2724c0b77261edbfa28c">mathlib</a>.</li>
                            <li><a href="https://doi.org/10.5281/zenodo.8040109"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.8040109.svg" alt="DOI"></a> <b>LeanDojo Benchmark 4</b>: 122,517 theorems/proofs, 259,580 tactics, and 167,779 premises from <a target="_blank" href="https://github.com/leanprover-community/mathlib4/tree/29dcec074de168ac2bf835a77ef68bbe069194c5">mathlib4</a>.</li>
                        </ul>
                    
                    
                    <p>LeanDojo can extract data from any GitHub repos in Lean (supporting both Lean 3 and Lean 4). The data contains rich information not directly visible in the raw Lean code, including file dependencies, abstract syntax trees (ASTs), proof states, tactics, and premises.</p>
                    
                    <p><b>Key feature 1: Premise information</b>: LeanDojo Benchmark contains fine-grained annotations of premises (where they are used in proofs and where they are defined in the library), providing valuable data for premise selection—a key bottleneck in theorem proving.</p>
                    
                    <p><b>Key feature 2: Challenging data split</b>: Splitting theorems randomly into training/testing leads to overestimated performance. LLMs can prove seemingly difficult theorems simply by memorizing the proofs of similar theorems during training. We mitigate this issue by designing challenging data split requiring the model to generalize to theorems relying on novel premises that are never used in training.</p>
                </div>

<div>
                    <h2><span>Interacting with Lean</span></h2>
                    <p><img src="https://leandojo.org/images/interaction.jpg" width="50%" alt=""></p><p>LeanDojo turns Lean into a <a target="_blank" href="https://github.com/openai/gym">gym-like</a> environment, in which the prover can observe the proof state, run tactics to change the state, and receive feedback on errors or on proof completion. This environment is indispensable for evaluating/deploying the prover or training it through RL.</p>
                </div>

<!--
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">LeanDojo Benchmark</span></h2>
                
                    <img src="images/quaternion.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">ReProver: Retrieval-Augmented Theorem Prover</span></h2>
                </div>
            </div>
        </div>
    </div>
</section>
-->

<div>
                    <h2><span>Experiments</span></h2>
                    <p><img src="https://leandojo.org/images/results.png" width="40%" alt=""></p><p>We use LeanDojo Benchmark to train and evaluate ReProver. During testing, the tactic generator is combined with best-first search to search for complete proofs. The table shows the percentage of theorem proved within 10 minutes. The \(\texttt{novel_premises}\) spilt is much more challenging than the \(\texttt{random}\) split. On both data splits, ReProver outperforms Lean's built-in proof automation tactic (tidy), a baseline that generates tactics directly without retrieval, and another baseline using GPT-4 to generate tactics in a zero-shot manner.</p>
                </div>

<div>
                    <h2><span>Discovering New Proofs and Uncovering Formalization Bugs</span></h2>
                    <p>We evaluate ReProver on theorems in miniF2F and ProofNet. It discovers <a target="_blank" href="https://github.com/facebookresearch/miniF2F/pull/13">33 proofs in miniF2F</a> and <a target="_blank" href="https://github.com/zhangir-azerbayev/ProofNet/pull/14">39 proofs in ProofNet</a> of theorems that did not have existing proofs in Lean. Our proofs have helped ProofNet uncover multiple bugs in the formalization of theorem statements.</p>
                </div>

<div>
                    <h2><span id="chatgpt-for-theorem-proving">ChatGPT for Theorem Proving</span></h2>
                    
                    
                    <p>We build a <a target="_blank" href="https://github.com/lean-dojo/LeanDojoChatGPT">LeanDojo ChatGPT plugin</a> that enables ChatGPT to prove theorems by interacting with Lean. Compared to specialized LLMs finetuned for theorem proving (e.g., ReProver), ChatGPT can interleave informal mathematics with formal proof steps, similar to how humans interact with proof assistants. It can explain error messages from Lean and is more steerable (by prompt engineering) than specialized provers. However, it struggles to find correct proofs in most cases, due to weakness in search and planning.</p>
                </div>
    

<div>
                    <h2><span>Team</span></h2>
                    
                    <br>
                    

                </div>
    
<div id="BibTeX">
        <h2>BibTeX</h2>
        <pre><code>@inproceedings{yang2023leandojo,
    title={{LeanDojo}: Theorem Proving with Retrieval-Augmented Language Models},
    author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima},
    booktitle={Neural Information Processing Systems (NeurIPS)},
    year={2023}
}

@article{song2024towards,
    title={Towards Large Language Models as Copilots for Theorem Proving in {Lean}},
    author={Peiyang Song and Kaiyu Yang and Anima Anandkumar},
    year={2024},
    journal = {arXiv preprint arXiv: Arxiv-2404.12534}
}</code></pre>
    </div>





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Higher-kinded bounded polymorphism in OCaml (2021) (131 pts)]]></title>
            <link>https://okmij.org/ftp/ML/higher-kind-poly.html</link>
            <guid>41096187</guid>
            <pubDate>Sun, 28 Jul 2024 21:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://okmij.org/ftp/ML/higher-kind-poly.html">https://okmij.org/ftp/ML/higher-kind-poly.html</a>, See on <a href="https://news.ycombinator.com/item?id=41096187">Hacker News</a></p>
<div id="readability-page-1" class="page">



<div><p>Higher-kinded polymorphism&nbsp;-- the abstraction over a type
<em>constructor</em> to be later supplied with arguments&nbsp;-- is often needed,
for expressing generic operations over collections or embedding typed
DSLs, particularly in tagless-final style. Typically, the abstracted type
constructor is not arbitrary, but must implement a particular
interface (e.g., an abstract sequence)&nbsp;-- so-called bounded
polymorphism. 
</p><p>OCaml does not support higher-kinded polymorphism directly: OCaml type
variables range over types rather than type constructors, and type
constructors may not appear in type expressions without being applied
to the right number of arguments.  Nevertheless, higher-kinded
polymorphism is expressible in OCaml&nbsp;-- in fact, in several, more or
less cumbersome ways. The less cumbersome ways are particularly less
known, and kept being rediscovered. This page summarizes the different
ways of expressing, and occasionally avoiding, higher-kinded
polymorphism. They are collected from academic papers and messages on
the caml-list spread over the years&nbsp;-- and adjusted to fit the story
and differently explained.
</p></div>
<ul>
<li><a href="#intro">Introduction</a>
</li>
<li><a href="#complex">Why higher-kinded polymorphism is not supported directly in OCaml</a>
</li>
<li><a href="#functors">Higher-kinded functions as Functors</a>
</li>
<li><a href="#yw">Yallop and White's Lightweight higher-kinded polymorphism</a>
</li>
<li><a href="#avoid">Sidestepping higher-kinded polymorphism</a>
</li>
<li><a href="#algebra">Algebras</a>
</li>
<li><a href="#hkname">What's in a higher-kinded type name</a>
</li>
<li><a href="#conc">Conclusions</a>
</li></ul>
<hr>


<h2><a name="intro">Introduction</a></h2>
<dl>
<dd>``Polymorphism abstracts types, just as functions abstract
values. Higher-kinded polymorphism takes things a step further,
abstracting both types and types constructors, just as higher-order
functions abstract both first-order values and functions.''&nbsp;-- write
Yallop and White (FLOPS 2014).
<p>This remarkably concise summary is worth expounding upon, to
demonstrate how (bounded) higher-kinded polymorphism tends to
arise. The example introduced here is used all throughout the page.
</p><p>Summing up numbers frequently occurs in practice; abstracting from
concrete numbers leads to a function&nbsp;-- an operation that can be
uniformly performed on any collection (list) of numbers:

</p><pre>    let rec sumi : int list -&gt; int = function [] -&gt; 0 | h::t -&gt; h + sumi t
</pre>We may further abstract over <code>0</code> and the operation
<code>+</code>, which itself is a function (a parameterized value, so to
speak). The result is a higher-order function:

<pre>    let rec foldi (f: int-&gt;int-&gt;int) (z: int) : int list -&gt; int =
        function [] -&gt; z | h::t -&gt; f h (foldi f z t)
</pre>Folding over a list, say, of floating-point numbers proceeds similarly, so we
may abstract yet again&nbsp;-- this time not over values but over the type
<code>int</code>, replacing it with a type variable:

<pre>    let rec fold (f: 'a-&gt;'a-&gt;'a) (z: 'a) : 'a list -&gt; 'a =
        function [] -&gt; z | h::t -&gt; f h (fold f z t)
</pre>thus giving us the polymorphic function: the function that describes an
operation performed over lists of various types, uniformly.
The operation <code>f</code> and the value <code>z</code> can be collected into a 
parameterized record

<pre>    type 'a monoid = {op: 'a-&gt;'a-&gt;'a; unit: 'a}
</pre>The earlier <code>fold</code> then takes the form

<pre>    let rec foldm (m: 'a monoid) : 'a list -&gt; 'a =
        function [] -&gt; m.unit | h::t -&gt; m.op h (foldm m t)
</pre>When using <code>foldm</code> on a concrete list of the type <code>t list</code>, 
the type variable <code>'a</code> gets
instantiated to the type <code>t</code> of the elements of this list. The type is not
completely arbitrary, however: there must exist the value 
<code>t monoid</code>, to be passed to <code>foldm</code> as the argument. We say the type
<code>t</code> must (at least) implement/support the <code>'a monoid</code> interface; the <code>t monoid</code> value is then the witness that <code>t</code> indeed does so. Hence the
polymorphism in <code>foldm</code> is <em>bounded</em>.
<p><em>Exercise:</em> if <code>'a monoid</code> really describes a monoid, 
<code>op x unit = x</code> holds. Write a more optimal version of <code>foldm</code> (and
its subsequent variants) taking advantage of this identity.
</p><p>A file, a string, an array, a sequence&nbsp;-- all can be folded over in the
same way. Any collection is foldable so long as it supports the
deconstruction operation, which tells if the collection is empty, or
gives its element and the rest of the sequence. One is tempted to
abstract again&nbsp;-- this time not over a mere type like <code>int</code> or <code>int list</code>, but over a type constructor such as <code>list</code>, and introduce

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>This is a hypothetical OCaml: the type variable 'F (with the
upper-case name) is to be instantiated not with types but one-argument
type constructors: technically, one says it has the <em>higher-kind</em>
<code>* -&gt; *</code> rather than the ordinary kind <code>*</code> of types and ordinary type variables
such as <code>'a</code>. The record <code>seq</code> is, hence, higher-kind polymorphic. The
function <code>foldm</code> then generalizes to

<pre>    let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>Again, <code>'F</code> is instantiated not with just any type constructor, but only that
for which we can find the value <code>('a,'F) seq</code>; thus <code>folds</code> exhibits 
<em>bounded higher-kinded</em> polymorphism.
<p>Alas, higher-kind type variables are not possible in OCaml. The
next section explains why. The following sections tell what we can do in
OCaml instead. There are several alternatives. In some, the end result
ends up looking almost exactly as the above imagined
higher-kind--polymorphic code.</p></dd></dl>


<h2><a name="complex">Why higher-kinded polymorphism is not supported directly in OCaml</a></h2>
<dl>
<dd>Higher-kind type variables are not supported in OCaml.  Yallop and
White's FLOPS 2014 paper (Sec. 1.1) explains why: in a word, type
aliasing. For completeness, we recount their explanation here, with
modifications.
<p>Consider the following two modules:

</p><pre>    module Tree = struct
      type 'a t = Leaf | Branch of 'a t * 'a * 'a t
    end
    
    module TreeA : dcont = struct
      type 'a t = ('a * 'a) Tree.t
    end
</pre>Here, <code>'a Tree.t</code> is a data type: a fresh type, distinct from
all other existing types. On the other hand, <code>'a TreeA.t</code> is an alias:
as its declaration says, it is equal to an existing type, viz. <code>('a * 'a) Tree.t</code>.
<p>Suppose OCaml had higher-kind <code>* -&gt; *</code> type variables, such as <code>'F</code>
hypothesized in the previous section. Type checking is, in the end,
solving/checking type equalities, such as <code>'a 'F = 'b 'G</code>. If
higher-kind type variables ranged only over <em>data type</em> constructors,
the solution is easy: <code>'a = 'b</code> and <code>'F = 'G</code>: a data type is fresh, hence
equal only to itself. This is the situation in Haskell. To ensure that
only data type constructors can be substituted for higher-kind type
variables, a Haskell compiler keeps track of type aliases, even across
module boundaries. Module system in Haskell is rather simple, so such
tracking is unproblematic.
</p><p>Module system of ML is, in contrast, sophisticated. It has functors,
signatures, etc., and extensively relies on type aliases, for example:

</p><pre>    module F(T: sig type 'a t  val empty: 'a t end) = struct
      type 'a ft = 'a T.t
    end
</pre>If we preclude substitution of type aliases for higher-kind type
variables, we severely restrict expressiveness. For example, <code>'a ft</code>
above is a type alias; hence <code>F(TRee).ft</code> cannot be substituted for a
higher-kind type variable, even though one may feel <code>F(TRee).ft</code> is
the same as <code>Tree.t</code>, which is substitutable.
<p>On the other hand, if we allow type aliases to be substituted for
higher-kind type variables, the equivalence of <code>'a 'F = 'b 'G</code> and <code>'a = 'b, 'F = 'G</code> breaks down. Indeed, consider <code>(int*int) 'F = int 'G</code>. This equation now has the solution: <code>'F = Tree.t</code> and <code>'G = TreeA.t</code>. Parameterized type aliases like <code>'a TreeA.t</code> are type
functions, and type expressions like <code>int TreeA.t</code> are applications of
those functions, expanding to the right-hand-side of the alias
declaration with <code>'a</code> substituted for <code>int</code>. Thus, with type aliases,
the type equality problem becomes the higher-order unification problem,
which is not decidable.</p></dd>
<dt><strong>References</strong></dt>
<dd>Jeremy Yallop and Leo White: Lightweight higher-kinded
polymorphism. FLOPS 2014.
</dd></dl>


<h2><a name="functors">Higher-kinded functions as Functors</a></h2>
<dl>
<dd>Although OCaml does not support higher-kind type variables, higher-kinded
polymorphism is not out of the question. There are other ways of
parameterizing by a type constructor: the module system (functor)
abstraction is the first to come to mind. 
It is however rather verbose and cumbersome. Let us see.
<p>We now re-write the hypothetical higher-kind--polymorphic OCaml code
at the end of [<a href="#intro">Introduction</a>] in the real OCaml&nbsp;-- by raising the level, so to
speak, from term-level to module-level. The hypothetical record

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>becomes the module signature

<pre>    module type seq_i = sig
      type 'a t                                (* sequence type *)
      val decon : 'a t -&gt; ('a * 'a t) option
    end
</pre>which represents the higher-kind type variable <code>'F</code>, not supported in
OCaml, with an ordinary type constructor <code>t</code> (type
constant). Different implementations of <code>seq_i</code> (see, e.g., <code>ListS</code>
below) instantiate <code>'a t</code> in their own ways; hence
<code>t</code> does in effect act like a variable. The hypothetical
higher-kind--polymorphic function

<pre>    let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>becomes the functor, parameterized by the <code>seq_i</code> signature:

<pre>    module FoldS(S:seq_i) = struct
      let rec fold (m: 'a monoid) : 'a S.t -&gt; 'a = fun c -&gt;
        match S.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (fold m t)
    end
</pre>We got what we wanted: abstraction over a sequence. To use
it to define other higher-kinded polymorphic functions, such as
<code>sums</code> to sum up a sequence, we also
need functors. Functors are infectious, one may say.

<pre>    module SumS(S:seq_i) = struct
      open S
      open FoldS(S)
      let sum : int t -&gt; int = fold monoid_plus
    end
</pre>
<p>Finally, an example of instantiating and using
higher-kind--polymorphic functions: summing a list. First we need an
instance of <code>seq_i</code> for a list: the witness that a list is a sequence.

</p><pre>    module ListS = struct
      type 'a t = 'a list
      let decon = function [] -&gt; None | h::t -&gt; Some (h,t)
    end
</pre>which we pass to the <code>SumS</code> functor:

<pre>    let 6 =
      let module M = SumS(ListS) in M.sum [1;2;3]
</pre>The accompanying code shows another example: using the same <code>SumS</code> to
sum up an array, which also can be made a sequence.
<p>Thus in this approach, all higher-kind--polymorphic functions are
functors, which leads to verbosity, awkwardness and boilerplate. For
example, we cannot even write a <code>SumS</code> application as <code>SumS(ListS).sum [1;2;3]</code>; we have to use the verbose expression above.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</dd></dl>


<h2><a name="yw">Yallop and White's Lightweight higher-kinded polymorphism</a></h2>
<dl>
<dd>Perhaps surprisingly, higher-kinded polymorphism can always be reduced
to the ordinary polymorphism, as Yallop and White's FLOPS 2014 paper
cleverly demonstrated. They explained their approach as
defunctionalization. Here we recap it and explain in a different way.
<p>Consider the type <code>'a list</code> again. It is a parameterized type: <code>'a</code> is
the type of elements, and <code>list</code> is the name of the collection: `the
base name', so to speak. The combination of the element type and the
base name can be expressed differently, for example, as
<code>('a,list_name) app</code>, where <code>('a,'b) app</code> is some fixed type, and
<code>list_name</code> is the ordinary type that tells the base name.  The fact
that the two representations are equivalent is witnessed by the
bijection:

</p><pre>    inj: 'a list -&gt; ('a,list_name) app
    prj: ('a,list_name) app -&gt; 'a list
</pre>
<p>Here is a way to implement it. First, we introduce the dedicated
`pairing' data type. It is extensible, to let us define as many
pairings as needed.

</p><pre>    type ('a,'b) app = ..
</pre>For <code>'a list</code>, we have:

<pre>    type list_name
    type ('a,'b) app += List_name : 'a list -&gt; ('a,list_name) app
</pre>In this case the bijection <code>'a list &lt;-&gt; ('a,list_name) app</code> is:

<pre>    let inj x = List_name x and
    let prj (List_name x) = x
</pre>and the two functions are indeed inverses of each other.
<p><em>Exercise:</em> Actually, that the above <code>inj</code> and <code>prj</code> are
the inverses of each other is not as straightforward. It requires a
side-condition, which is satisfied in our case. State it.
</p><p>In this new representation of the polymorphic list as <code>('a,list_name) app</code>, the base name <code>list_name</code> is the ordinary (kind <code>*</code>)
type. Abstraction over it is straightforward: replacing with a
type variable. The base-name-polymorphism is, hence, the ordinary
polymorphism. We can then write the desired sequence-polymorphic
<code>folds</code> almost literally as the hypothetical code at the end of [<a href="#intro">Introduction</a>]:

</p><pre>    type ('a,'n) seq = {decon: ('a,'n) app -&gt; ('a * ('a,'n) app) option}
    
    let rec folds (m: 'a monoid) (s: ('a,'n) seq) : ('a,'n) app -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>Instead of <code>'a 'F</code> we write <code>('a,'n) app</code>. That's it. Using <code>folds</code> in
other higher-kinded functions is straightforward, as if it were
a regular polymorphic function (which it actually is):

<pre>    let sums s c = folds monoid_plus s c
     (* val sums : (int, 'a) seq -&gt; (int, 'a) app -&gt; int = &lt;fun&gt; *)
</pre>Type annotations are not necessary: the type inference works. Here is a
usage example, summing a list:

<pre>    let list_seq : ('a,list_name) seq =
      {decon = fun (List_name l) -&gt;
       match l with [] -&gt; None | h::t -&gt; Some (h,List_name t)}
    
    let 6 = sums list_seq (List_name [1;2;3])
</pre>
<p>There is still a bit of awkwardness remains: the user have to think up
the base name like <code>list_name</code> and the tag like <code>List_name</code>, and
ensure uniqueness.  Yallop and White automate using the module system,
see the code accompanying this page, or Yallop and White's paper (and
the Opam package `higher').
</p><p>We shall return to Yallop and White's approach later on this page,
with another perspective and implementation.</p></dd>
<dt><strong>References</strong></dt>
<dd>Jeremy Yallop and Leo White: Lightweight higher-kinded
polymorphism. FLOPS 2014.

<p><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</p></dd></dl>


<h2><a name="avoid">Sidestepping higher-kinded polymorphism</a></h2>
<dl>
<dd>At times, higher-kinded polymorphism can be avoided altogether: upon
close inspection it may turn out that the problem at hand does not
actually require higher-kinded polymorphism. In fact, our running
example is such a problem.
<p>Let us examine the sequence interface,
parameterized both by the type of the sequence elements and the sequence
itself. The definition that first comes to mind, which cannot be written
as such in OCaml, is (from <a href="#intro">Introduction</a>):

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>It has a peculiarity: the sole operation <code>decon</code> consumes and produces
sequences of the same type <code>'a 'F</code> (i.e., the same sort of sequence
with the elements of the same type). That is, <code>'F</code>
always occurs as the type <code>'a 'F</code>, where <code>'a</code> is <code>seq</code>'s parameter: <code>'a</code> and
<code>'F</code> do not vary independently. Therefore, there is actually
no higher-kinded polymorphism here. The sequence interface can be written
simply as 

<pre>    type ('a,'t) seq = {decon: 't -&gt; ('a * 't) option}
</pre>with <code>folds</code> taking <em>exactly</em> the desired form:

<pre>    let rec folds (m: 'a monoid) (s: ('a,'t) seq) : 't -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>It is the ordinary polymorphic function. There is no problem in using
it to define other such sequence-polymorphic functions, e.g.:

<pre>    let sums s c = folds monoid_plus s c
    (* val folds : 'a monoid -&gt; ('a, 't) seq -&gt; 't -&gt; 'a = &lt;fun&gt; *)
</pre>and applying it, say, to a list:

<pre>    let list_seq : ('a,'a list) seq =
      {decon = function [] -&gt; None | h::t -&gt; Some (h,t)}
    
    let 6 = sums list_seq [1;2;3]
</pre>
<p><em>Exercise:</em> Consider the interface of collections that may be
`mapped', in the hypothetical OCaml with higher-kind type variables:

</p><pre>    type ('a,'b,'F) ftor = {map: ('a-&gt;'b) -&gt; ('a 'F -&gt; 'b 'F)}
</pre>Now <code>'F</code> is applied to different types. Can this interface be still
expressed using the ordinary polymorphism, or higher-kinded
polymorphism is really needed here?
<p>Looking very closely at the higher-kinded polymorphic interface
<code>('a,'F) seq</code> and the ordinary polymorphic <code>('a,'t) seq</code>, one may
notice that the latter is larger. The higher-kinded interface
describes only polymorphic sequences such as <code>'a list</code>, whereas
<code>('a,'t) seq</code> applies also to files, strings, buffers, etc. Such an
enlargement is welcome here: we can apply the same <code>folds</code> to
sequences whose structure is optimized for the type of their
elements. In Haskell terms, <code>('a,'t) seq</code> corresponds to `data
families', a later Haskell extension. Here is an
example, of applying <code>folds</code> to a string, which is not a polymorphic
sequence:

</p><pre>    let string_seq : (char,int*string) seq =
      {decon = fun (i,s) -&gt;
        if i &gt;= String.length s || i &lt; 0 then None else Some (s.[i], (i+1, s))}
    
    let 'c' = folds monoid_maxchar string_seq (0,"bca")
</pre>We can hence use <code>folds</code> with any collection, polymorphic or not, for
which there is an implementation of the <code>('a,'t) seq</code> interface. We have
encountered the old, and very useful trick: enlarging the type but
restricting the set of its values by having to be able to define `witnesses'
such as <code>('a,'t) seq</code>.
<p><em>Exercise:</em> Yallop and White's approach can also deal with
non-polymorphic collections. Use it to implement <code>string_seq</code>.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</dd></dl>


<h2><a name="algebra">Algebras</a></h2>
<dl>
<dd>The running example, the <code>seq</code> interface, was about deconstruction of
sequences: technically, about co-algebras. Let us now turn to
construction: building of values using a fixed set of operations,
which can be considered an embedded DSL. The abstraction over a DSL
implementation gives rise to polymorphism. If the embedded
DSL is typed, the polymorphism becomes higher-kinded&nbsp;-- as commonly seen
in DSL embeddings in tagless-final style.
<p>Here we briefly recount how the higher-kinded polymorphism arises in DSL
embeddings, and how it can be hidden away. The key idea is initial
algebra, which is, by definition, the abstraction over any concrete
algebra of the same signature, i.e., the abstraction over DSL implementations.
</p><p>Our running example in this section is a simple programming language
with integers and booleans: a dialect of the language used in Chap. 3
of Pierce's `Types and Programming Languages' (TAPL). Here is the familiar
tagless-final embedding in OCaml. The grammar of the language is
represented as an OCaml signature:

</p><pre>    module type sym = sig
      type 'a repr
      val int    : int -&gt; int repr
      val add    : int repr -&gt; int repr -&gt; int repr
      val iszero : int repr -&gt; bool repr
      val if_    : bool repr -&gt; 'a repr -&gt; 'a repr -&gt; 'a repr
    end
</pre>The language is typed; therefore, the type <code>'a repr</code>, which represents DSL
terms, is indexed by the term's type: an <code>int</code> or a <code>bool</code>. The
signature <code>sym</code> also defines the type system of the DSL: almost like
in TAPL, but with the typing rules written in a
vertical-space--economic way.
<p>Here is a sample term of the DSL:

</p><pre>    module SymEx1(I:sym) = struct
      open I
      let t1 = add (add (int 1) (int 2)) (int 3) (* intermediate binding *)
      let res = if_ (iszero t1) (int 0) (add t1 (int 1))
    end
</pre>It is written as a functor parameterized by <code>sym</code>: a DSL
implementation is abstracted out. The term is  polymorphic over
<code>sym</code> and, hence, may be evaluated in any implementation of the DSL. Since
<code>sym</code> contains a higher-kinded type <code>repr</code>, the polymorphism is
higher-kinded.
<p>The just presented (tagless-final) DSL embedding followed the
approach described in [<a href="#functors">Higher-kinded functions as Functors</a>]. Let us move away from functors
to ordinary terms. Actually, we never quite escape functors, but we
hide them in terms, relying on first-class modules.
As we have seen, a DSL term of the type <code>int</code> such as <code>SymEx1</code> is the functor

</p><pre>    functor (I:sym) -&gt; sig val res : int I.repr end
</pre>To abstract over <code>int</code>, we wrap it into a module

<pre>    module type symF = sig
      type a
      module Term(I:sym) : sig val res : a I.repr end
    end
</pre>which can then be turned into ordinary polymorphic type:

<pre>    type 'a sym_term = (module (symF with type a = 'a))
</pre>which lets us represent the functor <code>SymEx1</code> as an ordinary OCaml value:

<pre>    let sym_ex1 : _ sym_term = 
      (module struct type a = int module Term = SymEx1 end)
</pre>Here, the type annotation is needed. However, we let the type of the term
to be <code>_</code>,&nbsp;as a schematic variable. OCaml infers it as <code>int</code>.
If we have an implementation of <code>sym</code>, say, module <code>R</code>, we can use it
to run the example (and obtain the <code>sym_ex1</code>'s value in <code>R</code>'s interpretation):

<pre>    let _ = let module N = (val sym_ex1) in 
            let module M = N.Term(R) in M.res
</pre>
<p>The type <code>'a sym_term</code> can itself implement the <code>sym</code> signature, in 
a `tautological' sort of way:

</p><pre>    module SymSelf : (sym with type 'a repr = 'a sym_term) = struct
      type 'a repr = 'a sym_term
      let int : int -&gt; int repr = fun n -&gt;
        let module M(I:sym) = struct let res = I.int n end in
        (module struct type a = int module Term = M end)
      let add : int repr -&gt; int repr -&gt; int repr = fun (module E1) (module E2) -&gt;
        let module M(I:sym) = 
          struct module E1T = E1.Term(I) module E2T = E2.Term(I)
                 let res = I.add (E1T.res) (E2T.res) 
          end in
        (module struct type a = int module Term = M end)
       ...
    end
</pre>That was a mouthful. But writing <code>sym</code> DSL terms becomes much
easier, with no functors and no type annotations. The earlier <code>sym_ex1</code>
can now be written as

<pre>    let sym_ex1 = 
      let open SymSelf in
      let t1 = add (add (int 1) (int 2)) (int 3) in (* intermediate binding *)
      if_ (iszero t1) (int 0) (add t1 (int 1))
</pre>It can be evaluated in <code>R</code> or other implementation as shown before.
<p>Technically, <code>SymSelf</code> is the initial algebra: an implementation of
the DSL that can be mapped to any other implementation, and in a
unique way. That means its terms like <code>sym_ex1</code> can be evaluated in
any <code>sym</code> DSL implementation: they are polymorphic over DSL implementation.
</p><p>On the down-side, we have <code>SymSelf</code>, which is the epitome of
boilerplate: utterly trivial and voluminous code that has to be
written. On the up side, writing DSL terms cannot be easier: no type
annotations, no functors, no implementation passing&nbsp;-- and no overt
polymorphism, higher-kind or even the ordinary kind. Still, the terms
can be evaluated in any implementation of the DSL.
</p><p><em>Exercise:</em> Apply Yallop and White's method to this DSL
example. Hint: the first example in Yallop and White's paper, monad
representations, is an example of a DSL embedding in tagless-final
style.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_tf.ml">HKPoly_tf.ml</a>&nbsp;[13K]<br>

The complete code with tests and detailed development

<p><a href="https://okmij.org/ftp/tagless-final/Algebra.html#init">Initial Algebra</a><br>

The initial algebra construction using first-class functors,
in the case of one-sorted algebras (corresponding to untyped DSLs)

</p><p>Stephen Dolan: phantom type. Message on the caml-list
posted on Mon, 27 Apr 2015 12:51:11 +0100
</p></dd></dl>


<h2><a name="hkname">What's in a higher-kinded type name</a></h2>
<dl>
<dd>We now look back at Yallop and White's approach of reducing
higher-kinded polymorphism to the ordinary polymorphism, from a
different perspective. It gives if not a new insight, at least new
implementations.
<p>A polymorphic type like <code>'a list</code> represents a family of types,
indexed by a type (of list elements, in this example). A higher-kinded
type abstraction such as <code>'a 'F</code> with the hypothetical (in OCaml)
higher-kind type variable <code>'F</code> is the abstraction over a family name,
so to speak, while still keeping track of the index. Here is another
way of accomplishing such an abstraction. 
</p><p>Consider the existential type <code>exists a. a list</code> (realizable in OCaml
in several ways, although not in the shown notation. We will keep the
notation for clarity). The existential is now the ordinary, rank <code>*</code>
type and can be abstracted in a type variable, e.g., <code>'d</code>. The `family
name' is, hence, the family type with the hidden index. We have
lost track of the index, however. Therefore, we tack it back, ending
up with the type <code>('a,'d) hk</code>. Thus <code>(t,exists a. a list) hk</code> is meant
to be the same as <code>t list</code> (for any type <code>t</code>).
</p><p>There is a problem however: <code>('a,'d) hk</code> is a much bigger type. We
need the condition that in <code>(t,exists a. a list) hk</code>, the index <code>t</code> is
exactly the one that we hid in the existential quantification&nbsp;-- we
need dependent pairs, not supported in OCaml. Remember the old trick,
however: we may have a bigger type so long as we control the producers
of its values and ensure only the values satisfying the condition are
built. To be concrete, we must make certain that the only way to
produce <code>('a,'d) hk</code> values is by using functions like <code>inj: 'a list -&gt; ('a, exists a. a list) hk</code> that expose the same index they hide.
At some point the type checker will demand a
proof: when implementing the
inverse mapping <code>('a, exists a. a list) hk -&gt; 'a list</code> and extracting
the list out of the existential. There are several ways of going about
the proof.
</p><p>The simplest is to give our word&nbsp;-- that the condition always holds for
all <code>('a,'d) hk</code> values actually produced, and we have a proof of that
on some piece of paper or in a <code>.v</code> file. This leads to the exceptionally
simple implementation, which does nothing at all
(all of its operations are the identity).

</p><pre>    module HK : sig
      type ('a,'d) hk                       (* abstract *)
      module MakeHK : functor (S: sig type 'a t end) -&gt; sig
        type anyt                           (* also abstract *)
        val inj : 'a S.t -&gt; ('a,anyt) hk
        val prj : ('a,anyt) hk -&gt; 'a S.t
      end
     end = struct
      type ('a,'d) hk = 'd
      module MakeHK(S:sig type 'a t end) = struct
        type anyt = Obj.t
        let inj : 'a S.t -&gt; ('a,anyt) hk = Obj.repr
        let prj : ('a,anyt) hk -&gt; 'a S.t = Obj.obj
      end
    end
</pre>The accompanying code shows a different, also quite simple
implementation without any <code>Obj</code> magic.
<p>After enriching the <code>sym</code> signature of the DSL from the previous
section with fake higher-kinded types:

</p><pre>    module type sym_hk = sig
      include sym
      include module type of HK.MakeHK(struct type 'a t = 'a repr end)
    end
</pre>we can write the earlier <code>SymEx1</code> example as a function (a term)
rather than a functor:

<pre>    let sym_ex1 (type d) (module I:(sym_hk with type anyt=d)) : (_,d) HK.hk =
      let open I in
      let t1 = add (add (int 1) (int 2)) (int 3) |&gt; inj in (* intermediate term *)
      let res = if_ (iszero t1) (int 0) (add t1 (int 1)) in
      inj res
</pre>It can be evaluated simply as

<pre>    sym_ex1 (module RHK) |&gt; RHK.prj
</pre>where <code>RHK</code> is a module implementing <code>sym_hk</code>. Incidentally, if <code>SHK</code>
is another module implementing <code>sym_hk</code> and we attempt
<code>sym_ex1 (module RHK) |&gt; SHK.prj</code>, we discover that
<code>(int,RHK.anyt) bk</code> and <code>(int,SHK.anyt) bk</code> are actually different
types. Although <code>HK</code> does not do anything (at runtime), it does
maintain safety and soundness.</dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_tf.ml">HKPoly_tf.ml</a>&nbsp;[13K]<br>

The complete code with tests and detailed development
</dd></dl>


<h2><a name="conc">Conclusions</a></h2>
<dl>
<dd>We have surveyed various ways of abstracting over a type
constructor&nbsp;-- or, writing interface-parameterized terms
when the interface involves a polymorphic type. Even if the
language does not support type-constructor--polymorphism directly,
such interface parameterization can still be realized, as:

<ul>
<li>interface abstraction as a functor abstraction
</li>
<li>reducing higher-kinded polymorphism to ordinary polymorphism,
by establishing a bijection between type constructors and
ordinary types
</li>
<li>hiding the polymorphism over DSL implementations
behind initial algebra
(if the interface is algebraic, as often happens in tagless-final
DSL embeddings)
</li>
<li>and in some problems, higher-kinded polymorphism is not actually
needed, on close inspection
</li></ul></dd></dl>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A football/soccer pass visualizer made with Three.js (174 pts)]]></title>
            <link>https://statsbomb-3d-viz.vercel.app/</link>
            <guid>41095839</guid>
            <pubDate>Sun, 28 Jul 2024 20:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statsbomb-3d-viz.vercel.app/">https://statsbomb-3d-viz.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=41095839">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Don't blindly prefer `emplace_back` to `push_back` (132 pts)]]></title>
            <link>https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/</link>
            <guid>41095814</guid>
            <pubDate>Sun, 28 Jul 2024 20:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/">https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/</a>, See on <a href="https://news.ycombinator.com/item?id=41095814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>In one of my recent training courses, a student informed me that both clang-tidy
and PVS-Studio were complaining about some code of the form</p>

<div><pre><code>std::vector&lt;Widget&gt; widgets;
~~~
widgets.push_back(Widget(foo, bar, baz));
</code></pre></div>

<p>Both tools flagged this line as “bad style.”
clang-tidy even offered a (SARCASM ALERT) helpful fixit:</p>

<div><pre><code>warning: use emplace_back instead of push_back [modernize-use-emplace]
    widgets.push_back(Widget(foo, bar, baz));
            ^~~~~~~~~~~~~~~~~             ~
            emplace_back(
</code></pre></div>

<p>The student dutifully changed the line, and both tools reported their
satisfaction with the replacement:</p>

<div><pre><code>widgets.emplace_back(Widget(foo, bar, baz));
</code></pre></div>

<p>The original line materializes a temporary <code>Widget</code> object on the stack;
takes an rvalue reference to it; and passes that reference to
<code>vector&lt;Widget&gt;::push_back(Widget&amp;&amp;)</code>, which move-constructs a <code>Widget</code>
into the vector. Then we destroy the temporary.</p>

<p>The student’s replacement materializes a temporary <code>Widget</code> object on the stack;
takes an rvalue reference to it; and passes that reference to
<code>vector&lt;Widget&gt;::emplace_back&lt;Widget&gt;(Widget&amp;&amp;)</code>, which move-constructs
a <code>Widget</code> into the vector. Then we destroy the temporary.</p>

<p><em>Absolutely no difference.</em></p>

<p>The change clang-tidy meant to suggest — and in fact <em>did</em> suggest,
if you pay very close attention to the underlining in the fixit — was actually this:</p>

<div><pre><code>widgets.emplace_back(foo, bar, baz);
</code></pre></div>

<p>This version does <em>not</em> materialize any <code>Widget</code> temporaries. It simply
passes <code>foo, bar, baz</code> to <code>vector&lt;Widget&gt;::emplace_back&lt;Foo&amp;, Bar&amp;, Baz&amp;&gt;(Foo&amp;, Bar&amp;, Baz&amp;)</code>,
which constructs a <code>Widget</code> into the vector using whatever
constructor of <code>Widget</code> best matches that bunch of arguments.</p>

<h2 id="emplace_back-is-not-magic-c11-pixie-dust"><code>emplace_back</code> is not magic C++11 pixie dust</h2>

<p>Even a decade after C++11 was released, I still sometimes see programmers assume
that <code>emplace_back</code> is somehow related to move semantics. (In the same way that
some programmers assume lambdas are somehow the same thing as <code>std::function</code>,
you know?) For example, they’ll rightly observe that this code makes an
unnecessary copy:</p>

<div><pre><code>void example() {
    auto w = Widget(1,2,3);
    widgets.push_back(w);  // Copy-constructor alert!
}
</code></pre></div>

<p>So they’ll change it to this:</p>

<div><pre><code>void example() {
    auto w = Widget(1,2,3);
    widgets.emplace_back(w);  // Fixed? Nope!
}
</code></pre></div>

<p>The original line constructs a <code>Widget</code> object into <code>w</code>, then
passes <code>w</code> by reference to <code>vector&lt;Widget&gt;::push_back(const Widget&amp;)</code>,
which copy-constructs a <code>Widget</code> into the vector.</p>

<p>The replacement constructs a <code>Widget</code> object into <code>w</code>, then
passes <code>w</code> by reference to <code>vector&lt;Widget&gt;::emplace_back&lt;Widget&amp;&gt;(Widget&amp;)</code>,
which copy-constructs a <code>Widget</code> into the vector.</p>

<p><em>Absolutely no difference.</em></p>

<p>What the student should have done is ask the compiler to make an
<em>rvalue</em> reference to <code>w</code>, by saying either</p>

<div><pre><code>widgets.push_back(std::move(w));
</code></pre></div>

<p>or</p>

<div><pre><code>widgets.emplace_back(std::move(w));
</code></pre></div>

<p>It doesn’t matter which verb you use; what matters is the value category of
<code>w</code>. You must explicitly mention <code>std::move</code>, so that the language (and the
human reader) understand that you’re done using <code>w</code> and it’s okay for
<code>widgets</code> to pilfer its guts.</p>

<p><code>emplace_back</code> was added to the language at the same time as <code>std::move</code> — just
like lambdas were added at the same time as <code>std::function</code> — but that doesn’t
make them the same thing. <code>emplace_back</code> may “look more C++11-ish,” but it’s
not magic move-enabling pixie dust and it will never insert a move in a place
you don’t explicitly request one.</p>

<h2 id="when-all-else-is-equal-prefer-push_back-to-emplace_back">When all else is equal, prefer <code>push_back</code> to <code>emplace_back</code></h2>

<p>So, given that these two lines do the same thing and are equally efficient
at runtime, which should I prefer, stylistically?</p>

<div><pre><code>widgets.push_back(std::move(w));
widgets.emplace_back(std::move(w));
</code></pre></div>

<p>I recommend sticking with <code>push_back</code> for day-to-day use. You should definitely
use <code>emplace_back</code> when you need its particular set of skills — for example, <code>emplace_back</code>
is your only option when dealing with a <code>deque&lt;mutex&gt;</code> or other non-movable type —
but <code>push_back</code> is the appropriate default.</p>

<p>One reason is that <code>emplace_back</code> is more work for the compiler.
<code>push_back</code> is an overload set of two non-template member functions.
<code>emplace_back</code> is a single variadic template.</p>

<div><pre><code>void push_back(const Widget&amp;);
void push_back(Widget&amp;&amp;);

template&lt;class... Ts&gt;
reference emplace_back(Ts&amp;&amp;...);
</code></pre></div>

<p>When you call <code>push_back</code>, the compiler must do overload resolution, but that’s all.
When you call <code>emplace_back</code>, the compiler must do template type deduction, followed
by (easy-peasy) overload resolution, followed by function template instantiation and
code generation. That’s a much larger amount of work for the compiler.</p>

<h2 id="the-benchmark-program">The benchmark program</h2>

<p>I wrote a simple test program to demonstrate the difference in compiler workload.
Of course <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a> applies:
my benchmark displays a massive difference because it’s doing <em>nothing but</em>
instantiating <code>emplace_back</code>, whereas any production codebase will be doing vastly
more other stuff relative to the number of times it instantiates <code>emplace_back</code>.
Still, I hope this benchmark gives you a sense of why I recommend “<code>push_back</code> over
<code>emplace_back</code>” and not vice versa.</p>

<p>This Python 3 script generates the benchmark:</p>

<div><pre><code>import sys
print('#include &lt;vector&gt;')
print('#include &lt;string&gt;')
print('extern std::vector&lt;std::string&gt; v;')
for i in range(1000):
    print('void test%d() {' % i)
    print('    v.%s_back("%s");' % (sys.argv[1], 'A' * i))
    print('}')
</code></pre></div>

<p>Generate like this:</p>

<div><pre><code>python generate.py push &gt;push.cpp
python generate.py emplace &gt;emplace.cpp
time g++ -c push.cpp
time g++ -c emplace.cpp
</code></pre></div>

<p>With Clang trunk on my laptop, I get consistently about 1.0s for the <code>push</code> version,
and 4.2s for the <code>emplace</code> version. This big difference is due to the fact that the
<code>push</code> version is merely code-generating a thousand <code>test</code> functions, whereas
the <code>emplace</code> version is code-generating that same thousand <code>test</code> functions <em>and</em>
another thousand template instantiations of <code>emplace_back</code> with different parameter
types:</p>

<div><pre><code>vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[1]&gt;(const char (&amp;)[1])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[2]&gt;(const char (&amp;)[2])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[3]&gt;(const char (&amp;)[3])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[4]&gt;(const char (&amp;)[4])
~~~
</code></pre></div>

<p>See, <code>push_back</code> knows that it expects a <code>string&amp;&amp;</code>, and so it knows to call the
non-explicit constructor <code>string(const char *)</code> on the caller’s side. The same
constructor is called in each case, and the temporary <code>string</code> is passed to
the same overload of <code>push_back</code> in each case. <code>emplace_back</code>, on the other hand,
is a dumb perfect-forwarding template: it doesn’t know that the relevant constructor
overload will end up being <code>string(const char *)</code> in each case. So it takes
an lvalue reference to the specific <em>array type</em> being passed by the caller.
Perfect-forwarding has no special cases for <code>const char *</code>!</p>

<p>If we change <code>vector&lt;string&gt;</code> to <code>vector&lt;const char *&gt;</code>, the compile-time-performance
gap widens: now it’s 0.7s for <code>push</code>, 3.8s for <code>emplace</code>. This is because we’ve cut
out some of the work that was common to both versions (constructing <code>std::string</code> objects)
without affecting the source of the gap (that one version instantiates a
thousand copies of <code>emplace_back</code> and the other doesn’t). Amdahl’s Law in action!</p>

<p>My conclusions:</p>

<blockquote>
  <p>Use <code>push_back</code> by default.</p>
</blockquote>

<blockquote>
  <p>Use <code>emplace_back</code> where it is semantically significant to your algorithm
(such as when the element type’s move-constructor is absent or has been
benchmarked as expensive).</p>
</blockquote>

<blockquote>
  <p>Avoid mixing string literals and perfect-forwarding templates,
especially in repetitive machine-generated code.</p>
</blockquote>

<hr>

<p>Previously on this blog:</p>

<ul>
  <li><a href="https://quuxplusone.github.io/blog/2018/06/26/cost-of-static-lifetime-constructors/">“The surprisingly high cost of static-lifetime constructors”</a> (2018-06-26)</li>
</ul>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[tolower() with AVX-512 (236 pts)]]></title>
            <link>https://dotat.at/@/2024-07-28-tolower-avx512.html</link>
            <guid>41095790</guid>
            <pubDate>Sun, 28 Jul 2024 20:38:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-07-28-tolower-avx512.html">https://dotat.at/@/2024-07-28-tolower-avx512.html</a>, See on <a href="https://news.ycombinator.com/item?id=41095790">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>A couple of years ago I wrote about <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">tolower() in bulk at speed using
SWAR tricks</a>. A couple of days ago I was interested by Olivier
Giniaux’s article about <a href="https://ogxd.github.io/articles/unsafe-read-beyond-of-death/">unsafe read beyond of death</a>, an
optimization for handling small strings with SIMD instructions, for a
fast hash function written in Rust.</p>
<p>I’ve long been annoyed that SIMD instructions can easily eat short
strings whole, but it’s irritatingly difficult to transfer short
strings between memory and vector registers. Oliver’s post caught my
eye because it seemed like a fun way to avoid the problem, at least
for loads. (Stores remain awkward!)</p>
<p>Actually, to be frank, Olivier nerdsniped me.</p>
<ul>
<li><a href="#signs-of-hope">signs of hope</a></li>
<li><a href="#tolower64-">tolower64()</a></li>
<li><a href="#bulk-load-and-store">bulk load and store</a></li>
<li><a href="#masked-load-and-store">masked load and store</a></li>
<li><a href="#benchmarking">benchmarking</a></li>
<li><a href="#conclusion">conclusion</a></li>
</ul>
<h2><a name="signs-of-hope" href="#signs-of-hope">signs of hope</a></h2>
<p>Reading more around the topic, I learned that <em>some</em> SIMD instruction
sets do, in fact, have useful masked loads and stores that are
suitable for string processing, that is, they have byte granularity.
They are:</p>
<ul>
<li>
<p>ARM SVE, which is available on recent big-ARM Neoverse cores, such
as Amazon Graviton, but not Apple Silicon.</p>
</li>
<li>
<p>AVX-512-BW, the bytes and words extension, which is available on
recent AMD Zen processors. AVX-512 is a complicated mess of
extensions that might or might not be available; support on Intel
is particularly random.</p>
</li>
</ul>
<p>I have an AMD Zen 4 box, so I thought I would try a little AVX-512-BW.</p>
<h2><a name="tolower64-" href="#tolower64-">tolower64()</a></h2>
<p>Using the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel intrinsics guide</a> I wrote a basic <code>tolower()</code>
function that can munch 64 bytes at once.</p>
<p>Top tip: You can use <code>*</code> as a wildcard in the search box, so I made
heavy use of <code>mm512*epi8</code> to find byte-wise AVX-512 functions (<code>epi8</code>
is an obscure alias for byte).</p>
<p>First, we fill a few registers with 64 copies of some handy bytes.</p>
<p>We need the letters A and Z:</p>
<pre><code>    __m512i A = _mm512_set1_epi8('A');
    __m512i Z = _mm512_set1_epi8('Z');
</code></pre>
<p>We need a number to add to uppercase letters to make them lowercase:</p>
<pre><code>    __m512i to_upper = _mm512_set1_epi8('a' - 'A');
</code></pre>
<p>We compare our input characters <code>c</code> with A and Z. The result of each
comparison is a 64 bit mask which has bits set for the bytes where
the comparison is true:</p>
<pre><code>    __mmask64 ge_A = _mm512_cmpge_epi8_mask(c, A);
    __mmask64 le_Z = _mm512_cmple_epi8_mask(c, Z);
</code></pre>
<p>If it’s greater than or equal to A, <em>and</em> less than or equal to Z,
then it is upper case. (AVX mask registers have names beginning with
<code>k</code>.)</p>
<pre><code>    __mmask64 is_upper = _kand_mask64(ge_A, le_Z);
</code></pre>
<p>Finally, we do a masked add. We pass <code>c</code> twice: bytes from the first
<code>c</code> are copied to the result when <code>is_upper</code> is false, and when
<code>is_upper</code> is true the result is <code>c + to_upper</code>.</p>
<pre><code>    return  _mm512_mask_add_epi8(c, is_upper, c, to_upper);
</code></pre>
<h2><a name="bulk-load-and-store" href="#bulk-load-and-store">bulk load and store</a></h2>
<p>The <code>tolower64()</code> kernel in the previous section needs to be wrapped
up in more convenient functions such as copying a string while
converting it to lower case.</p>
<p>For long strings, the bulk of the work uses unaligned vector load and
store instructions:</p>
<pre><code>	__m512i src_vec = _mm512_loadu_epi8(src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_storeu_epi8(dst_ptr, dst_vec);
</code></pre>
<h2><a name="masked-load-and-store" href="#masked-load-and-store">masked load and store</a></h2>
<p>Small strings and the stub end of long strings use masked unaligned
loads and stores.</p>
<p><strong>This is the magic!</strong> Here is the reason I wrote this blog post!</p>
<p>The mask has its lowest <code>len</code> bits set (its first <code>len</code> bits in
little-endian order). I wrote these two lines with perhaps more
ceremony than required, but I thought it was helpful to indicate that
the mask is not any old 64 bit integer: it has to be loaded into one
of the SIMD unit’s mask registers.</p>
<pre><code>	uint64_t len_bits = (~0ULL) &gt;&gt; (64 - len);
	__mmask64 len_mask =  _cvtu64_mask64(len_bits);
</code></pre>
<p>The load and store look fairly similar to the full-width versions, but
with the mask stuff added. The <code>z</code> in <code>maskz</code> means zero the
destination register when the mask is clear, as opposed to copying
from another register (like in <code>mask_add</code> above).</p>
<pre><code>	__m512i src_vec = _mm512_maskz_loadu_epi8(len_mask, src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_mask_storeu_epi8(dst_ptr, len_mask, dst_vec);
</code></pre>
<p>That’s the essence of it: you can <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c">see the complete version of
<code>copytolower64()</code></a> in my git repository.</p>
<h2><a name="benchmarking" href="#benchmarking">benchmarking</a></h2>
<p>To see how well it works, I benchmarked several similar functions.
Here’s a chart of the results, compiled with Clang 16 on Debian 11,
and run on an AMD Ryzen 9 7950X.</p>
<p>The benchmark measures the time to copy about 1 MiByte, in chunks of
various lengths from 1 byte to 1 kilobyte. I wanted to take into
account differences in alignment in the source and destination
strings, so there are a few bytes between each source and destination
string, which are not counted as part of the megabyte.</p>
<p>On this CPU the L2 cache is 1 MiB per core, so I expect each run of
the test spills into the L3 cache.</p>
<p>To be sure I was measuring what I thought I was, I compiled each
function separately to avoid interference from inlining and code
motion. In real code it’s more likely that you would want to encourage
inlining, not prevent it!</p>
<p><img src="https://dotat.at/@/2024-07-vectolower.svg" alt="benchmark results"></p>
<ul>
<li>
<p>The pink <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c"><code>tolower64</code></a> line is the code described in this blog post.
It is consistently near the fastest of all the functions under
test. (It drops a little at 65 bytes long, where it spills into a
second vector.)</p>
<p>The interesting feature of the line for my purposes is that it
rises fast and lacks deep troughs, showing that the masked loads
and stores were effective at handling small string fragments
quickly.</p>
</li>
<li>
<p>The green <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes64.c"><code>copybytes64</code></a> line is a version of <code>memcpy</code> using
AVX-512 in a similar manner to <code>tolower64</code>. It is (maybe
surprisingly) not much faster. I had to compile <code>copybytes64</code> with
Clang 11 because more recent versions are able to recognise what
the function does and rewrite it completely.</p>
</li>
<li>
<p>The orange <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes1.c"><code>copybytes1</code></a> line is a byte-by-byte version of <code>memcpy</code>
again compiled using Clang 11. It illustrates that Clang 11 had
relatively poor autovectorizer heuristics and was pretty bad for
string fragments less than 256 bytes long.</p>
</li>
<li>
<p>The very slow red <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower.c"><code>tolower</code></a> line calls the standard
<code>tolower()</code> from <code>&lt;ctype.h&gt;</code> to provide a baseline.</p>
</li>
<li>
<p>The purple <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower1.c"><code>tolower1</code></a> line is a simple byte-by-byte version of
<code>tolower()</code> compiled with Clang 16. It shows that Clang 16 has a
much better autovectorizer than Clang 11, but it is slower and
<a href="https://godbolt.org/z/9dPMo8h8a">much more complicated</a> than my
hand-written version. It is very spiky because the autovectorizer
did not handle short string fragments as well as <code>tolower64</code> does.</p>
</li>
<li>
<p>The brown <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower8.c"><code>tolower8</code></a> line is <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">the SWAR <code>tolower()</code> from my
previous blog post</a>. Clang valiantly tries to autovectorize
it, but the result is not great because the function is too
complicated. (It has the Clang-11-style 256-byte performance
cliffs despite being compiled with Clang 16.)</p>
</li>
<li>
<p>The blue <code>memcpy</code> line calls glibc’s <code>memcpy</code>. There’s something
curious going on here: it starts off fast but drops off to about
half the speed of <code>copybytes64</code>. Dunno why!</p>
</li>
</ul>
<h2><a name="conclusion" href="#conclusion">conclusion</a></h2>
<p>So, AVX-512-BW is <em>very nice indeed</em> for working with strings,
especially short strings. On Zen 4 it’s very fast, and the intrinsic
functions are reasonably easy to use.</p>
<p>The most notable thing is AVX-512-BW’s smooth performance: there’s
very little sign of the performance troughs that the autovectorizer
suffers from as it shifts to scalar code for small string fragments.</p>
<p>I don’t have convenient access to an ARM box with SVE support, so I
have not investigated it in detail. It’ll be interesting to see how
well SVE works for short strings.</p>
<p>I would like both of these instruction set extensions to be much more
widely available. They should improve the performance of string
handling tremendously.</p>
<p><a href="https://dotat.at/cgi/git/vectolower.git">The code for this blog post is available from my web site.</a></p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft technical breakdown of CrowdStrike incident (368 pts)]]></title>
            <link>https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/</link>
            <guid>41095530</guid>
            <pubDate>Sun, 28 Jul 2024 19:55:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/">https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=41095530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
<p>Windows is an open and flexible platform used by many of the world’s top businesses for high availability use cases where security and availability are non-negotiable.</p>



<p>To meet those needs:</p>



<ol>
<li>Windows provides a range of operating modes that customers can choose from. This includes the ability to limit what can run to only approved software and drivers. This can increase security and reliability by making Windows operate in a mode closer to mobile phones or appliances.</li>



<li>Customers can choose integrated security monitoring and detection capabilities that are included with Windows. Or they can choose to replace or supplement this security with a wide variety of choices from a vibrant open ecosystem of vendors.</li>
</ol>



<p>In this blog post, we examine the recent CrowdStrike outage and provide a technical overview of the root cause. We also explain why security products use kernel-mode drivers today and the safety measures Windows provides for third-party solutions. In addition, we share how customers and security vendors can better leverage the integrated security capabilities of Windows for increased security and reliability. Lastly, we provide a look into how Windows will enhance extensibility for future security products.</p>



<p>CrowdStrike recently published a <a href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/" target="_blank" rel="noreferrer noopener">Preliminary Post Incident Review</a> analyzing their outage. In their blog post, CrowdStrike describes the root cause as a memory safety issue—specifically a read out-of-bounds access violation in the CSagent driver. We leverage the Microsoft <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debugger/debugger-download-tools" target="_blank" rel="noreferrer noopener">WinDBG Kernel Debugger</a> and <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debuggercmds/-reg" target="_blank" rel="noreferrer noopener">several extensions</a> that are available free to anyone to perform this analysis. Customers with crash dumps can reproduce our steps with these tools.</p>



<p>Based on Microsoft’s analysis of the Windows Error Reporting (<a href="https://learn.microsoft.com/en-us/windows/win32/wer/windows-error-reporting" target="_blank" rel="noreferrer noopener">WER</a>) kernel crash dumps related to the incident, we observe global crash patterns that reflect this:</p>


<div><pre title="">FAULTING_THREAD:  ffffe402fe868040

READ_ADDRESS:  ffff840500000074 Paged pool

MM_INTERNAL_CODE:  2

IMAGE_NAME:  csagent.sys

MODULE_NAME: csagent

FAULTING_MODULE: fffff80671430000 csagent

PROCESS_NAME:  System

TRAP_FRAME:  ffff94058305ec20 -- (.trap 0xffff94058305ec20)
.trap 0xffff94058305ec20
NOTE: The trap frame does not contain all registers.
Some register values may be zeroed or incorrect.
rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003
rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000
rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0
 r8=ffff840500000074  r9=0000000000000000 r10=0000000000000000
r11=0000000000000014 r12=0000000000000000 r13=0000000000000000
r14=0000000000000000 r15=0000000000000000
iopl=0         nv up ei ng nz na po nc
csagent+0xe14ed:
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8] ds:ffff8405`00000074=????????
.trap
Resetting default scope

STACK_TEXT:  
ffff9405`8305e9f8 fffff806`5388c1e4     : 00000000`00000050 ffff8405`00000074 00000000`00000000 ffff9405`8305ec20 : nt!KeBugCheckEx 
ffff9405`8305ea00 fffff806`53662d8c     : 00000000`00000000 00000000`00000000 00000000`00000000 ffff8405`00000074 : nt!MiSystemFault+0x1fcf94  
ffff9405`8305eb00 fffff806`53827529     : ffffffff`00000030 ffff8405`af8351a2 ffff9405`8305f020 ffff9405`8305f020 : nt!MmAccessFault+0x29c 
ffff9405`8305ec20 fffff806`715114ed     : 00000000`00000000 ffff9405`8305eeb0 ffff8405`b0bcd00c ffff8405`b0bc505c : nt!KiPageFault+0x369 
ffff9405`8305edb0 fffff806`714e709e     : 00000000`00000000 00000000`e01f008d ffff9405`8305f102 fffff806`716baaf8 : csagent+0xe14ed
ffff9405`8305ef50 fffff806`714e8335     : 00000000`00000000 00000000`00000010 00000000`00000002 ffff8405`b0bc501c : csagent+0xb709e
ffff9405`8305f080 fffff806`717220c7     : 00000000`00000000 00000000`00000000 ffff9405`8305f382 00000000`00000000 : csagent+0xb8335
ffff9405`8305f1b0 fffff806`7171ec44     : ffff9405`8305f668 fffff806`53eac2b0 ffff8405`afad4ac0 00000000`00000003 : csagent+0x2f20c7
ffff9405`8305f430 fffff806`71497a31     : 00000000`0000303b ffff9405`8305f6f0 ffff8405`afb1d140 ffffe402`ff251098 : csagent+0x2eec44
ffff9405`8305f5f0 fffff806`71496aee     : ffff8405`afb1d140 fffff806`71541e7e 00000000`000067a0 fffff806`7168f8f0 : csagent+0x67a31
ffff9405`8305f760 fffff806`7149685b     : ffff9405`8305f9d8 ffff8405`afb1d230 ffff8405`afb1d140 ffffe402`fe8644f8 : csagent+0x66aee
ffff9405`8305f7d0 fffff806`715399ea     : 00000000`4a8415aa ffff8eee`1c68ca4f 00000000`00000000 ffff8405`9e95fc30 : csagent+0x6685b
ffff9405`8305f850 fffff806`7148efbb     : 00000000`00000000 ffff9405`8305fa59 ffffe402`fe864050 ffffe402`fede62c0 : csagent+0x1099ea
ffff9405`8305f980 fffff806`7148edd7     : ffffffff`ffffffa1 fffff806`7152e5c1 ffffe402`fe864050 00000000`00000001 : csagent+0x5efbb
ffff9405`8305fac0 fffff806`7152e681     : 00000000`00000000 fffff806`53789272 00000000`00000002 ffffe402`fede62c0 : csagent+0x5edd7
ffff9405`8305faf0 fffff806`53707287     : ffffe402`fe868040 00000000`00000080 fffff806`7152e510 006fe47f`b19bbdff : csagent+0xfe681
ffff9405`8305fb30 fffff806`5381b8e4     : ffff9680`37651180 ffffe402`fe868040 fffff806`53707230 00000000`00000000 : nt!PspSystemThreadStartup+0x57 
ffff9405`8305fb80 00000000`00000000     : ffff9405`83060000 ffff9405`83059000 00000000`00000000 00000000`00000000 : nt!KiStartSystemThread+0x34 

</pre></div>


<p>Digging in more to this crash dump, we can restore the stack frame at the time of the access violation to learn more about its origin. Unfortunately, with WER data we only receive a compressed version of state and thus we cannot disassemble backwards to see a larger set of instructions prior to the crash, but we can see in the disassembly that there is a check for NULL before performing a read at the address specified in the R8 register:</p>


<div><pre title="">6: kd&gt; .trap 0xffff94058305ec20
.trap 0xffff94058305ec20
NOTE: The trap frame does not contain all registers.
Some register values may be zeroed or incorrect.
rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003
rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000
rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0
 r8=ffff840500000074  r9=0000000000000000 r10=0000000000000000
r11=0000000000000014 r12=0000000000000000 r13=0000000000000000
r14=0000000000000000 r15=000000000000
000
iopl=0         nv up ei ng nz na po nc
csagent+0xe14ed:
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8] ds:ffff8405`00000074=????????
6: kd&gt; !pte ffff840500000074
!pte ffff840500000074
                                           VA ffff840500000074
PXE at FFFFABD5EAF57840    PPE at FFFFABD5EAF080A0    PDE at FFFFABD5E1014000    PTE at FFFFABC202800000
contains 0A00000277200863  contains 0000000000000000
pfn 277200    ---DA--KWEV  contains 0000000000000000
not valid

6: kd&gt; ub fffff806`715114ed
ub fffff806`715114ed
csagent+0xe14d9:
fffff806`715114d9 04d8            add     al,0D8h
fffff806`715114db 750b            jne     csagent+0xe14e8 (fffff806`715114e8)
fffff806`715114dd 4d85c0          test    r8,r8
fffff806`715114e0 7412            je      csagent+0xe14f4 (fffff806`715114f4)
fffff806`715114e2 450fb708        movzx   r9d,word ptr [r8]
fffff806`715114e6 eb08            jmp     csagent+0xe14f0 (fffff806`715114f0)
fffff806`715114e8 4d85c0          test    r8,r8
fffff806`715114eb 7407            je      csagent+0xe14f4 (fffff806`715114f4)
6: kd&gt; ub fffff806`715114d9
ub fffff806`715114d9
                          ^ Unable to find valid previous instruction for 'ub fffff806`715114d9'
6: kd&gt; u fffff806`715114eb
u fffff806`715114eb
csagent+0xe14eb:
fffff806`715114eb 7407            je      csagent+0xe14f4 (fffff806`715114f4)
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8]
fffff806`715114f0 4d8b5008        mov     r10,qword ptr [r8+8]
fffff806`715114f4 4d8bc2          mov     r8,r10
fffff806`715114f7 488d4d90        lea     rcx,[rbp-70h]
fffff806`715114fb 488bd6          mov     rdx,rsi
fffff806`715114fe e8212c0000      call    csagent+0xe4124 (fffff806`71514124)
fffff806`71511503 4533d2          xor     r10d,r10d

6: kd&gt; db ffff840500000074
db ffff840500000074
ffff8405`00000074  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`00000084  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`00000094  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000a4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000b4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000c4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000d4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000e4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????

</pre></div>


<p>Our observations confirm CrowdStrike’s analysis that this was a read-out-of-bounds memory safety error in the CrowdStrike developed CSagent.sys driver.</p>



<p>We can also see that the csagent.sys module is registered as a <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/ifs/flt-parameters-for-irp-mj-create-named-pipe" target="_blank" rel="noreferrer noopener">file system filter driver</a> commonly used by anti-malware agents to receive notifications about file operations such as the creation or modification of a file. This is often used by security products to scan any new file saved to disk, such as downloading a file via the browser.</p>



<p>File System filters can also be used as a signal for security solutions attempting to monitor the behavior of the system. CrowdStrike noted in their blog that part of their content update was changing the sensor’s logic relating to data around named pipe creation. The File System filter driver API allows the driver to receive a call when named pipe activity (e.g., named pipe creation) occurs on the system that could enable the detection of malicious behavior. The general function of the driver correlates to the information shared by CrowdStrike.</p>


<div><pre title="">6: kd&gt;!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f67f9c

[SubKeyAddr]         [SubKeyName]
ffff8405a6f683ac     Instances
ffff8405a6f6854c     Sim

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          2
REG_DWORD           Start                         1
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     \??\C:\Windows\system32\drivers\CrowdStrike\csagent.sys
REG_SZ              DisplayName                   CrowdStrike Falcon
REG_SZ              Group                         FSFilter Activity Monitor
REG_MULTI_SZ        DependOnService               FltMgr\0
REG_SZ              CNFG                          Config.sys
REG_DWORD           SupportedFeatures             f

</pre></div>


<p>We can see the control channel file version 291 specified in the CrowdStrike analysis is also present in the crash indicating the file was read.</p>



<p>Determining how the file itself correlates to the access violation observed in the crash dump would require additional debugging of the driver using these tools but is outside of the scope of this blog post.</p>


<div><pre title="">!ca ffffde8a870a8290

ControlArea  @ ffffde8a870a8290
  Segment      ffff880ce0689c10  Flink      ffffde8a87267718  Blink        ffffde8a870a7d98
  Section Ref                 0  Pfn Ref                   b  Mapped Views                0
  User Ref                    0  WaitForDel                0  Flush Count                 0
  File Object  ffffde8a879b29a0  ModWriteCount             0  System Views                0
  WritableRefs                0  PartitionId                0  
  Flags (8008080) File WasPurged OnUnusedList 

      \Windows\System32\drivers\CrowdStrike\C-00000291-00000000-00000032.sys

1: kd&gt; !ntfskd.ccb ffff880ce06f6970
!ntfskd.ccb ffff880ce06f6970

   Ccb: ffff880c`e06f6970
 Flags: 00008003 Cleanup OpenAsFile IgnoreCase
Flags2: 00000841 OpenComplete AccessAffectsOplocks SegmentObjectReferenced
  Type: UserFileOpen
FileObj: ffffde8a879b29a0

(018)  ffff880c`db937370  FullFileName [\Windows\System32\drivers\CrowdStrike\C-00000291-00000000-00000032.sys]
(020) 000000000000004C  LastFileNameOffset 
(022) 0000000000000000  EaModificationCount 
(024) 0000000000000000  NextEaOffset 
(048) FFFF880CE06F69F8  Lcb 
(058) 0000000000000002  TypeOfOpen 

</pre></div>


<p>We can leverage the crash dump to determine if any other drivers supplied by CrowdStrike may exist on the running system during the crash.</p>


<div><pre title="">6: kd&gt; lmDvmCSFirmwareAnalysis
lmDvmCSFirmwareAnalysis
Browse full module list
start             end                 module name
fffff806`58920000 fffff806`5893c000   CSFirmwareAnalysis   (deferred)             
    Image path: \SystemRoot\system32\DRIVERS\CSFirmwareAnalysis.sys
    Image name: CSFirmwareAnalysis.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Mar 18 11:32:14 2024 (65F888AE)
    CheckSum:         0002020E
    ImageSize:        0001C000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; lmDvmcspcm4
lmDvmcspcm4
Browse full module list
start             end                 module name
fffff806`71870000 fffff806`7187d000   cspcm4     (deferred)             
    Image path: \??\C:\Windows\system32\drivers\CrowdStrike\cspcm4.sys
    Image name: cspcm4.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Jul  8 18:33:22 2024 (668C9362)
    CheckSum:         00012F69
    ImageSize:        0000D000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; lmDvmcsboot.sys
lmDvmcsboot.sys
Browse full module list
start             end                 module name

Unloaded modules:
fffff806`587d0000 fffff806`587dc000   CSBoot.sys
    Timestamp: unavailable (00000000)
    Checksum:  00000000
    ImageSize:  0000C000

6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csboot
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csboot

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f68924

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         0
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     system32\drivers\CrowdStrike\CSBoot.sys
REG_SZ              DisplayName                   CrowdStrike Falcon Sensor Boot Driver
REG_SZ              Group                         Early-Launch
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csdevicecontrol
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csdevicecontrol

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f694ac

[SubKeyAddr]         [VolatileSubKeyName]
ffff84059ce196c4     Enum

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         3
REG_DWORD           ErrorControl                  1
REG_DWORD           Tag                           1f
REG_EXPAND_SZ       ImagePath                     \SystemRoot\System32\drivers\CSDeviceControl.sys
REG_SZ              DisplayName                   @oem40.inf,%DeviceControl.SVCDESC%;CrowdStrike Device Control Service
REG_SZ              Group                         Base
REG_MULTI_SZ        Owners                        oem40.inf\0!csdevicecontrol.inf_amd64_b6725a84d4688d5a\0!csdevicecontrol.inf_amd64_016e965488e83578\0
REG_DWORD           BootFlags                     14
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f67f9c

[SubKeyAddr]         [SubKeyName]
ffff8405a6f683ac     Instances
ffff8405a6f6854c     Sim

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          2
REG_DWORD           Start                         1
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     \??\C:\Windows\system32\drivers\CrowdStrike\csagent.sys
REG_SZ              DisplayName                   CrowdStrike Falcon
REG_SZ              Group                         FSFilter Activity Monitor
REG_MULTI_SZ        DependOnService               FltMgr\0
REG_SZ              CNFG                          Config.sys
REG_DWORD           SupportedFeatures             f

6: kd&gt; lmDvmCSFirmwareAnalysis
lmDvmCSFirmwareAnalysis
Browse full module list
start             end                 module name
fffff806`58920000 fffff806`5893c000   CSFirmwareAnalysis   (deferred)             
    Image path: \SystemRoot\system32\DRIVERS\CSFirmwareAnalysis.sys
    Image name: CSFirmwareAnalysis.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Mar 18 11:32:14 2024 (65F888AE)
    CheckSum:         0002020E
    ImageSize:        0001C000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csfirmwareanalysis
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csfirmwareanalysis

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f69d9c

[SubKeyAddr]         [VolatileSubKeyName]
ffff84059ce197cc     Enum

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         0
REG_DWORD           ErrorControl                  1
REG_DWORD           Tag                           6
REG_EXPAND_SZ       ImagePath                     system32\DRIVERS\CSFirmwareAnalysis.sys
REG_SZ              DisplayName                   @oem43.inf,%FirmwareAnalysis.SVCDESC%;CrowdStrike Firmware Analysis Service
REG_SZ              Group                         Boot Bus Extender
REG_MULTI_SZ        Owners                        oem43.inf\0!csfirmwareanalysis.inf_amd64_12861fc608fb1440\0
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\Controlset001\control\earlylaunch
!reg querykey \REGISTRY\MACHINE\system\Controlset001\control\earlylaunch
</pre></div>


<p>As we can see from the above analysis, CrowdStrike loads four driver modules. One of those modules receives dynamic control and content updates frequently based on the CrowdStrike Preliminary Post-incident-review timeline.</p>



<p>We can leverage the unique stack and attributes of this crash to identify the Windows crash reports generated by this specific CrowdStrike programming error. It’s worth noting the number of devices which generated crash reports is a subset of the number of impacted devices previously shared by <a href="https://blogs.microsoft.com/blog/2024/07/20/helping-our-customers-through-the-crowdstrike-outage/" target="_blank" rel="noreferrer noopener">Microsoft in our blog post</a>, because crash reports are sampled and collected only from customers who choose to upload their crashes to Microsoft. Customers who <a href="https://learn.microsoft.com/en-us/windows/privacy/configure-windows-diagnostic-data-in-your-organization" target="_blank" rel="noreferrer noopener">choose to enable crash dump sharing</a> help both driver vendors and Microsoft to identify and remediate quality issues and crashes.</p>


<figure><img decoding="async" src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture1-3.webp" alt="" srcset="" data-orig-src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture1-3.webp"><figcaption>Figure 1 CrowdStrike driver associated crash dump reports over time</figcaption></figure>



<p>We make this information available to driver owners so they can assess their own reliability via the <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/dashboard/hardware-program-register" target="_blank" rel="noreferrer noopener">Hardware Dev Center analytics</a> dashboard. As we can see from the above, any reliability problem like this invalid memory access issue can lead to widespread availability issues when not combined with safe deployment practices. Let’s dig into why security solutions leverage kernel drivers on Windows.</p>



<h2 id="why-do-security-solutions-leverage-kernel-drivers">Why do security solutions leverage kernel drivers?</h2>



<p>Many security vendors such as CrowdStrike and Microsoft leverage a kernel driver architecture and there are several reasons for this.</p>



<h3 id="visibility-and-enforcement-of-security-related-events">Visibility and enforcement of security related events</h3>



<p>Kernel drivers allow for system wide visibility, and the capability to load in early boot to detect threats like <a href="https://learn.microsoft.com/en-us/defender-endpoint/malware/rootkits-malware" target="_blank" rel="noreferrer noopener">boot kits and root kits</a> which can load before user-mode applications. In addition, Microsoft provides a rich set of capabilities such as system event callbacks for process and thread creation and filter drivers which can watch for events like file creation, deletion, or modification. Kernel activity can also trigger call backs for drivers to decide when to block activities like file or process creations. Many vendors also use drivers to collect a variety of network information in the kernel using the <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/ndis-drivers" target="_blank" rel="noreferrer noopener">NDIS driver class</a>.</p>



<h3 id="performance">Performance</h3>



<p>Kernel drivers are often utilized by security vendors for potential performance benefits. For example, analysis or data collection for high throughput network activity may benefit from a kernel driver. There are many scenarios where data collection and analysis can be optimized for operation outside of kernel mode and Microsoft continues to partner with the ecosystem to improve performance and provide best practices to achieve parity outside of kernel mode.</p>



<h3 id="tamper-resistance">Tamper resistance</h3>



<p>A second benefit of loading into kernel mode is tamper resistance. Security products want to ensure that their software cannot be disabled by malware, targeted attacks, or malicious insiders, even when those attackers have admin-level privileges. They also want to ensure that their drivers load as early as possible so that they can observe system events at the earliest possible time. Windows provides a mechanism to launch drivers marked as <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/install/early-launch-antimalware" target="_blank" rel="noreferrer noopener">Early Launch Antimalware</a> (ELAM) early in the boot process for this reason. CrowdStrike signs the above CSboot driver as ELAM, enabling it to load early in the boot sequence.</p>



<p>In the general case, there is a tradeoff that security vendors must rationalize when it comes to kernel drivers. Kernel drivers provide the above properties at the cost of resilience. Since kernel drivers run at the most trusted level of Windows, where containment and recovery capabilities are by nature constrained, security vendors must carefully balance needs like visibility and tamper resistance with the risk of operating within kernel mode.</p>



<p>All code operating at kernel level requires extensive validation because it cannot fail and restart like a normal user application. This is universal across all operating systems. Internally at Microsoft, we have invested in moving complex Windows core services from kernel to user mode, such as font file parsing from <a href="https://techcommunity.microsoft.com/t5/microsoft-security-baselines/dropping-the-quot-untrusted-font-blocking-quot-setting/ba-p/701068" target="_blank" rel="noreferrer noopener">kernel to user mode</a>.</p>



<p>It is possible today for security tools to balance security and reliability. For example, security vendors can use minimal sensors that run in kernel mode for data collection and enforcement limiting exposure to availability issues. The remainder of the key product functionality includes managing updates, parsing content, and other operations can occur isolated within user mode where recoverability is possible. This demonstrates the best practice of minimizing kernel usage while still maintaining a robust security posture and strong visibility.</p>


<figure><img decoding="async" src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture2-2.webp" alt="" srcset="" data-orig-src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture2-2.webp"><figcaption>Figure 2 Example security product architecture which balances security and reliability</figcaption></figure>



<p>Windows provides several user mode protection approaches for anti-tampering, like Virtualization-based security <a href="https://learn.microsoft.com/en-us/windows/win32/trusted-execution/vbs-enclaves" target="_blank" rel="noreferrer noopener">(VBS) Enclaves</a> and <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/install/early-launch-antimalware" target="_blank" rel="noreferrer noopener">Protected Processes</a> that vendors can use to protect their key security processes. Windows also provides <a href="https://techcommunity.microsoft.com/t5/windows-it-pro-blog/new-security-capabilities-in-event-tracing-for-windows/ba-p/3949941" target="_blank" rel="noreferrer noopener">ETW events</a> and user-mode interfaces like <a href="https://learn.microsoft.com/en-us/windows/win32/amsi/antimalware-scan-interface-portal" target="_blank" rel="noreferrer noopener">Antimalware Scan Interface</a> for event visibility. These robust mechanisms can be used to reduce the amount of kernel code needed to create a security solution, which balances security and robustness.</p>







<p>Microsoft engages with third-party security vendors through an industry forum called the <a href="https://learn.microsoft.com/en-us/defender-xdr/virus-initiative-criteria" target="_blank" rel="noreferrer noopener">Microsoft Virus Initiative</a> (MVI). This group consists of Microsoft and Security Industry and was created to establish a dialogue and collaboration across the Windows security ecosystem to improve robustness in the way security products use the platform. With MVI, Microsoft and vendors collaborate on the Windows platform to define reliable extension points and platform improvements, as well as share information about how to best protect our customers.</p>



<p>Microsoft works with members of MVI to ensure compatibility with Windows updates, improve performance, and address reliability issues. MVI partners actively participating in the program contribute to making the ecosystem more resilient and gain benefits including technical briefings, feedback loops with Microsoft product teams, and access to antimalware platform features such as ELAM and Protected Processes. Microsoft also provides runtime protection such as <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/driver-x64-restrictions" target="_blank" rel="noreferrer noopener">Patch Guard</a> to prevent disruptive behavior from kernel driver types like anti-malware.</p>



<p>In addition, all drivers signed by the Microsoft Windows Hardware Quality Labs (WHQL) must run a series of tests and attest to a number of quality checks, including using <a href="https://learn.microsoft.com/en-us/windows-hardware/test/hlk/testref/236b8ad5-0ba1-4075-80a6-ae9dafb71c94" target="_blank" rel="noreferrer noopener">fuzzers</a>, running <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/prefast-for-drivers-warnings" target="_blank" rel="noreferrer noopener">static code analysis</a> and testing under <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/driver-verifier" target="_blank" rel="noreferrer noopener">runtime driver verification</a>, among other techniques. These tests have been developed to ensure that best practices around security and reliability are followed. Microsoft includes all these tools in the Windows Driver Kit used by all driver developers. A list of the resources and tools is <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/driversecurity/driver-security-checklist" target="_blank" rel="noreferrer noopener">available here</a>.</p>



<p>All WHQL signed drivers are run through Microsoft’s ingestion checks and malware scans and must pass before being approved for signing. Additionally, if a third-party vendor chooses to distribute their driver via Windows Update (WU), the driver also goes through Microsoft’s flighting and gradual rollout processes to observe quality and ensure the driver meets the necessary quality criteria for a broad release.</p>



<h2 id="can-customers-deploy-windows-in-a-higher-security-mode-to-increase-reliability">Can customers deploy Windows in a higher security mode to increase reliability?</h2>



<p>Windows at its core is an open and versatile OS, and it can easily be locked down for increased security using integrated tools. In addition, Windows is constantly increasing security defaults, including dozens of new security features enabled by default in Windows 11.</p>



<h3 id="security-features-enabled-by-default-in-windows-11">Security features enabled by default in Windows 11</h3>



<figure><table><tbody><tr><td><strong>Area</strong></td><td><strong>Feature</strong></td></tr><tr><td><strong>Hardware Security Baseline</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/tpm/tpm-fundamentals" target="_blank" rel="noreferrer noopener">TPM2.0</a><br><a href="https://learn.microsoft.com/en-us/mem/intune/user-help/you-need-to-enable-secure-boot-windows" target="_blank" rel="noreferrer noopener">Secure boot</a><br><a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/oem-vbs" target="_blank" rel="noreferrer noopener">Virtualization-based security (VBS)<br></a><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/enable-virtualization-based-protection-of-code-integrity" target="_blank" rel="noreferrer noopener">Memory integrity (Hypervisor-protected Code Integrity (HVCI))<br></a><a href="https://techcommunity.microsoft.com/t5/windows-os-platform-blog/understanding-hardware-enforced-stack-protection/ba-p/1247815" target="_blank" rel="noreferrer noopener">Hardware-enforced stack protection<br></a><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/kernel-dma-protection-for-thunderbolt" target="_blank" rel="noreferrer noopener">Kernel Direct Memory Access (DMA) protection<br></a>HW-based kernel protection (HLAT)<br><a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security" target="_blank" rel="noreferrer noopener">Enhanced sign-in security (ESS) for built-in biometric sensors</a></td></tr><tr><td><strong>Encryption</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/" target="_blank" rel="noreferrer noopener">BitLocker</a> (commercial)<br><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/#device-encryption" target="_blank" rel="noreferrer noopener">Device Encryption</a> (consumer)</td></tr><tr><td><strong>Identity Management</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/identity-protection/credential-guard/" target="_blank" rel="noreferrer noopener">Credential Guard<br></a><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-token-protection" target="_blank" rel="noreferrer noopener">Entra primary refresh token (PRT) hardware protected<br></a>MDM deployed SCEP certs hardware protected<br>MDM enrollment certs hardware protected<br>Local Security Authority (LSA) PPL prevents token/credential dumping<br>Account lockout policy (for 10 failed sign-ins)<br>Enhanced phishing protection with Microsoft Defender<br><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/virus-and-threat-protection/microsoft-defender-smartscreen/" target="_blank" rel="noreferrer noopener">Microsoft Defender SmartScreen<br></a>NPLogonNotification doesn’t include password<br>WDigest SSO removed to reduce password disclosure<br>AD Device Account protected by CredGuard*</td></tr><tr><td><strong>Multi-Factor Authentication<br>(Passwordless)</strong></td><td>MSA &amp; Entra users lead through Hello enablement by default<br>MSA password automatically removed from Windows if never used<br>Hello container VSM protected<br>Peripheral biometric sensors blocked for ESS enabled devices<br>Lock on leave integrated into Hello</td></tr><tr><td><strong>Security Incident Reduction</strong></td><td>Common Log File Systems run from trusted source<br>Move tool-tip APIs from kernel to user mode<br>Modernize print stack by removing untrusted drivers<br>DPAPI moved from 3DES to AES<br>TLS 1.3 default with TLS 1.0/1.1 disabled by default<br>NTLM-less*</td></tr><tr><td><strong>OS lockdown</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/microsoft-recommended-driver-block-rules#microsoft-vulnerable-driver-blocklist" target="_blank" rel="noreferrer noopener">Microsoft Vulnerable Driver Blocklist<br></a>3P driver security baseline enforced via WHCP<br>Smart App Control*</td></tr></tbody></table><figcaption>*Feature available in the Windows Insider Program or currently off by default and on a path for default enablement</figcaption></figure>



<p>Windows has integrated security features to self-defend. This includes key anti-malware features enabled by default, such as:</p>



<ol>
<li><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/system-security/secure-the-windows-10-boot-process" target="_blank" rel="noreferrer noopener">Secure Boot</a>, which helps prevent early boot malware and rootkits by enforcing signing consistently across Windows boots.</li>



<li><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/system-security/secure-the-windows-10-boot-process" target="_blank" rel="noreferrer noopener">Measured Boot</a>, which provides TPM-based hardware cryptographic measurements on boot-time properties available through integrated attestation services such as <a href="https://learn.microsoft.com/en-us/windows-server/security/device-health-attestation" target="_blank" rel="noreferrer noopener">Device Health Attestation</a>.</li>



<li><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/bringup/device-guard-and-credential-guard" target="_blank" rel="noreferrer noopener">Memory integrity</a> (also known as hypervisor-protected code integrity or HVCI), which prevents runtime generation of dynamic code in the kernel and helps ensure control flow integrity.</li>



<li><a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/microsoft-recommended-driver-block-rules#microsoft-vulnerable-driver-blocklist" target="_blank" rel="noreferrer noopener">Vulnerable driver blocklist</a>, which is on by default, integrated into the OS, and managed by Microsoft. This complements the malicious driver block list.</li>



<li><a href="https://learn.microsoft.com/en-us/windows-server/security/credentials-protection-and-management/configuring-additional-lsa-protection" target="_blank" rel="noreferrer noopener">Protected Local Security Authority</a> is on by default in Windows 11 to protect a range of credentials. <a href="https://learn.microsoft.com/en-us/windows/security/identity-protection/credential-guard/?toc=%2Fwindows-server%2Fsecurity%2Ftoc.json&amp;bc=%2Fwindows-server%2Fbreadcrumbs%2Ftoc.json" target="_blank" rel="noreferrer noopener">Hardware-based credential protection</a> is on by default for enterprise versions of Windows.</li>



<li><a href="https://learn.microsoft.com/en-us/defender-endpoint/microsoft-defender-antivirus-windows" target="_blank" rel="noreferrer noopener">Microsoft Defender Antivirus</a> is enabled by default in Windows and offers anti-malware capabilities across the OS.</li>
</ol>



<p>These security capabilities provide layers of protection against malware and exploitation attempts in modern Windows. Many Windows customers have leveraged our security baseline and Windows security technologies to harden their systems and these capabilities collectively have reduced the attack surface significantly.</p>



<p>Using the integrated security features of Windows to prevent adversary attacks such as those displayed in the <a href="https://attack.mitre.org/" target="_blank" rel="noreferrer noopener">MITRE ATT&amp;CK® framework</a> increases security while reducing cost and complexity. It leverages best practices to achieve maximum security and reliability. These best practices include:</p>



<ol>
<li>Using <a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/" target="_blank" rel="noreferrer noopener">App Control for Business</a> (formerly Windows Defender Application Control), you can author a security policy to allow only trusted and/or business-critical apps. Your policy can be crafted to deterministically and durably prevent nearly all malware and “living off the land” style attacks. It can also specify which kernel drivers are allowed by your organization to durably guarantee that only those drivers will load on your managed endpoints.</li>



<li>Use <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/bringup/device-guard-and-credential-guard" target="_blank" rel="noreferrer noopener">Memory integrity</a> with a <a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/wdac-wizard" target="_blank" rel="noreferrer noopener">specific allow list policy</a> to further protect the Windows kernel using <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/oem-vbs" target="_blank" rel="noreferrer noopener">Virtualization-based security</a> (VBS). Combined with App Control for Business, memory integrity can reduce the attack surface for kernel malware or boot kits. This can also be used to limit any drivers that might impact reliability on systems.</li>



<li>Running as <a href="https://learn.microsoft.com/en-us/windows-server/remote/multipoint-services/create-a-standard-user-account" target="_blank" rel="noreferrer noopener">Standard User</a> and elevating only as necessary. Companies that follow the best practices to run as standard user and reduce privileges mitigate many of the <a href="https://attack.mitre.org/" target="_blank" rel="noreferrer noopener">MITRE ATT&amp;CK®</a> techniques.</li>



<li>Use <a href="https://learn.microsoft.com/en-us/windows-server/security/device-health-attestation" target="_blank" rel="noreferrer noopener">Device Health Attestation</a> (DHA) to monitor devices for the right security policy, including hardware-based measurements for the security posture of the machine. This is a modern and exceptionally durable approach to ensure security for high availability scenarios and uses Microsoft’s <a href="https://www.microsoft.com/en-us/security/business/security-101/what-is-zero-trust-architecture?msockid=04462a6256e861da2e753a3d57346023" target="_blank" rel="noreferrer noopener">Zero Trust architecture</a>.</li>
</ol>



<h2 id="what-is-next">What is next?</h2>



<p>Windows is a self-protecting operating system that has produced dozens of new security features and architectural changes <a href="https://www.microsoft.com/en-us/security/blog/2024/05/20/new-windows-11-features-strengthen-security-to-address-evolving-cyberthreat-landscape/" target="_blank" rel="noreferrer noopener">in recent versions</a>. We plan to work with the anti-malware ecosystem to take advantage of these integrated features to modernize their approach, helping to support and even increase security along with reliability.</p>



<p>This includes helping the ecosystem by:</p>



<ol>
<li>Providing safe rollout guidance, best practices, and technologies to make it safer to perform updates to security products.</li>



<li>Reducing the need for kernel drivers to access important security data.</li>



<li>Providing enhanced isolation and anti-tampering capabilities with technologies like our recently <a href="https://techcommunity.microsoft.com/t5/windows-os-platform-blog/securely-design-your-applications-and-protect-your-sensitive/ba-p/4179543" target="_blank" rel="noreferrer noopener">announced VBS enclaves</a>.</li>



<li>Enabling zero trust approaches like <a href="https://learn.microsoft.com/en-us/azure/attestation/overview" target="_blank" rel="noreferrer noopener">high integrity attestation</a> which provides a method to determine the security state of the machine based on the health of Windows native security features.</li>
</ol>



<p>As we move forward, Windows is continuing to innovate and offer new ways for security tools to detect and respond to emerging threats safely and securely. Windows has <a href="https://www.microsoft.com/en-us/security/blog/2024/03/06/enhancing-protection-updates-on-microsofts-secure-future-initiative/" target="_blank" rel="noreferrer noopener">announced a commitment around the Rust programming language</a> as part of Microsoft’s <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative" target="_blank" rel="noreferrer noopener">Secure Future Initiative</a> (SFI) and has recently expanded the <a href="https://www.youtube.com/watch?v=8T6ClX-y2AE" target="_blank" rel="noreferrer noopener">Windows kernel to support Rust</a>.</p>



<p>The information in this blog post is provided as part of our commitment to communicate learnings and next steps after the CrowdStrike incident. We will continue to share ongoing guidance on security best practices for Windows and work across our broad ecosystem of customers and partners to develop new security capabilities based on your feedback.</p>
			</div></div>]]></description>
        </item>
    </channel>
</rss>